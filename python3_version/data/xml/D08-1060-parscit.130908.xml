<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.820561">
Generalizing Local and Non-Local Word-Reordering Patterns for
Syntax-Based Machine Translation
</title>
<author confidence="0.706101">
Bing Zhao Yaser Al-onaizan
</author>
<affiliation confidence="0.460321">
IBM T.J. Watson Research IBM T.J. Watson Research
</affiliation>
<address confidence="0.535117">
Yorktown Heights, NY-10598 Yorktown Heights, NY-10598
</address>
<email confidence="0.966296">
zhaob@us.ibm.com onaizan@us.ibm.com
</email>
<sectionHeader confidence="0.983293" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999033722222222">
Syntactic word reordering is essential for
translations across different grammar struc-
tures between syntactically distant language-
pairs. In this paper, we propose to em-
bed local and non-local word reordering de-
cisions in a synchronous context free gram-
mar, and leverages the grammar in a chart-
based decoder. Local word-reordering is ef-
fectively encoded in Hiero-like rules; whereas
non-local word-reordering, which allows for
long-range movements of syntactic chunks,
is represented in tree-based reordering rules,
which contain variables correspond to source-
side syntactic constituents. We demonstrate
how these rules are learned from parallel cor-
pora. Our proposed shallow Tree-to-String
rules show significant improvements in trans-
lation quality across different test sets.
</bodyText>
<sectionHeader confidence="0.992496" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867541666667">
One of the main issues that a translator (human or
machine) must address during the translation pro-
cess is how to match the different word orders be-
tween the source language and the target language.
Different language-pairs require different levels of
word reordering. For example, when we translate
between English and Spanish (or other Romance
languages), most of the word reordering needed
is local because of the shared syntactical features
(e.g., Spanish noun modifier constructs are written
in English as modifier noun). However, for syn-
tactically distant language-pairs such as Chinese-
English, long-range reordering is required where
whole phrases are moved across the sentence.
The idea of “syntactic cohesion” (Fox, 2002) is
characterized by its simplicity, which has attracted
researchers for years. Previous works include sev-
eral approaches of incorporating syntactic informa-
tion to preprocess the source sentences to make them
more like the target language in structure. Xia and
McCord (2004) (Niessen and Ney, 2004; Collins et
al., 2005) described approaches applied to language-
pairs such as French-English and German-English.
Later, Wang et al. (2007) presented specific rules
to pre-order long-range movements of words, and
improved the translations for Chinese-to-English.
Overall, these works are similar, in that they design
a few language-specific and linguistically motivated
reordering rules, which are generally simple. The
eleven rules described in Wang et al. (2007) are ap-
pealing, as they have rather simple structure, mod-
eling only NP, VP and LCP via one-level sub-tree
structure with two children, in the source parse-tree
(a special case of ITG (Wu, 1997)). It effectively en-
hances the quality of the phrase-based translation of
Chinese-to-English. One major weakness is that the
reordering decisions were done in the preprocessing
step, therefore rendering the decoding process un-
able to recover the reordering errors from the rules if
incorrectly applied to. Also the reordering decisions
are made without the benefits of additional models
(e.g., the language models) that are typically used
during decoding.
Another method to address the re-ordering prob-
lem in translation is the Hiero model proposed by
Chiang (2005), in which a probabilistic synchronous
context free grammar (PSCFG) was applied to guide
the decoding. Hiero rules generalize phrase-pairs
</bodyText>
<note confidence="0.878892333333333">
572
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 572–581,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958292134831">
by introducing a single generic nonterminal (i.e., a
variable) [X]. The combination of variables and lex-
icalized words in a Hiero rule nicely captures local
word and phrase reordering (modeling an implicit
reordering window of max-phrase length). These
rules are then applied in a CYK-style decoder. In
Hiero rules, any nested phrase-pair can be general-
ized as variables [X]. This usually leads to too many
redundant translations, which worsens the spurious
ambiguities (Chiang, 2005) problems for both de-
coding and optimization (i.e., parameter tuning). We
found thatvariables (nonterminal [X]) in Hiero rules
offer a generalization too coarse to improve the ef-
fectiveness of hierarchical models’ performance.
We propose to enrich the variables in Hiero rules
with additional source syntactic reordering informa-
tion, in the form of shallow Tree-to-String syntactic
structures. The syntactic information is represented
by flat one-level sub-tree structures, with Hiero-like
nonterminal variables at the leaf nodes. The syntac-
tic rules, proposed in this paper, are composed of
(possibly lexicalized) source treelets and target sur-
face strings, with one or more variables that help
capture local-reordering similar to the Hiero rules.
Variables in a given rule are derived not only from
the embedded aligned blocks (phrase-pairs), but also
from the aligned source syntactic constituents. The
aligned constituents, as in our empirical observa-
tions for Chinese-English, tend to move together in
translations. The decoder is guided by these rules to
reduce spurious derivations; the rules also constrain
the exploration of the search space toward better
translation quality and sometime improved speed by
breaking long sentences into pieces. Overall, what
we want is to enable the long-range reordering deci-
sions to be local in a chart-based decoder.
To be more specific, we think the simple shal-
low syntactic structure is powerful enough for cap-
turing the major structure-reordering patterns, such
as NP, VP and LCP structures. We also use sim-
ple frequency-based feature functions, similar to the
blocks used in phrase-based decoder, to further im-
prove the rules’ representation power. Overall, this
enables us to avoid either a complex decoding pro-
cess to generate the source parse tree, or difficult
combinatorial optimizations for the feature func-
tions associated with rules.
In Marton and Resnik (2008), hiero variables
were disambiguated with additional binary feature
functions, with their weights optimized in standard
MER training. The combinatorial effects of the
added feature functions can make the feature se-
lection and optimization of the weights rather dif-
ficult. Since the grammar is essentially the same
as the Hiero ones, a standard CYK decoder can be
simply applied in their work. Word reordering can
also be addressed via distortion models. Work in
(Al-Onaizan and Kishore, 2006; Xiong et al., 2006;
Zens et al., 2004; Kumar and Byrne, 2005; Tillmann
and Zhang, 2005) modeled the limited information
available at phrase-boundaries. Syntax-based ap-
proaches such as (Yamada and Knight, 2001; Graehl
and Knight, 2004; Liu et al., 2006) heavily rely on
the parse-tree to constrain the search space by as-
suming a strong mapping of structures across distant
language-pairs. Their algorithms are also subject to
parsers’ performances to a larger extent, and have
high complexity and less scalability in reality. In Liu
et al. (2007), multi-level tree-structured rules were
designed, which made the decoding process very
complex, and auxiliary rules have to be designed
and incorporated to shrink multiple source nonter-
minals into one target nonterminal. From our em-
pirical observations, most of the time, however, the
multi-level tree-structure is broken in the translation
process, and POS tags are frequently distorted. In-
deed, strictly following the source parse tree is usu-
ally not necessary, and maybe too expensive for the
translation process.
The remainder of this paper is structured as fol-
lows: in section § 2, we define the notations in our
synchronous context free grammar, in section § 3,
the rule extractions are illustrated in details, in sec-
tion § 4, the decoding process of applying these rules
is described. Experiments in § 5 were carried out
using GALE Dev07 datasets. Improved translation
qualities were obtained by applying the proposed
Tree-to-String rules. Conclusions and discussions
are given in § 6.
</bodyText>
<sectionHeader confidence="0.635254" genericHeader="method">
2 Shallow Tree-to-String Rules
</sectionHeader>
<bodyText confidence="0.99990825">
Our proposed rules are in the form of probabilis-
tic synchronous context free grammar (PSCFG). We
adopt the notations used in (Chiang, 2005). Let N
be a set of nonterminals, a rule has the following
</bodyText>
<equation confidence="0.850206666666667">
573
form:
X —*&lt; E; -y; α; w &gt;, (1)
</equation>
<bodyText confidence="0.99802862962963">
where X abstracts nonterminal symbols in N; -y E
[N, VS]+ is a sequence of one or more source 1
words (as in the vocabulary of VS) and nonterminal
symbols in N; α E [N, VT]+ is a sequence of one
or more target words (in VT) and nonterminals in N
. — is the one-to-one alignment of the nonterminals
between -y and α; w contains non-negative weights
associated with each rule; E is a label-symbol speci-
fying the root node of the source span covering -y. In
our grammar, E is one of the labels (e.g., NP) defined
in the source treebank tagset (in our case UPenn
Chinese tagset) indicating that the source span -y is
rooted at E. Additionally, a NULL tag 0 in E denotes
a flat structure of -y, in which no constituent structure
was found to cover the span, and we need to back
off to the normal Hiero-style rules. Our nonterminal
symbols include the labels and the POS tags in the
source parse trees.
In the following, we will illustrate the Tree-to-
String rules we are proposing. At the same time, we
will describe the extraction algorithm, with which
we derive our rules from the word-aligned source-
parsed parallel text. Our nonterminal set N is a re-
duced set of the treebank tagset (Xue et al., 2005). It
consists of 17 unique labels.
The rules we extract belong to one of the follow-
ing categories:
</bodyText>
<listItem confidence="0.996152785714286">
• -y contains only words, and E is NULL; this cor-
responds to the general blocks used in phrase-
based decoder (Och and Ney, 2004);
• -y contains words and variables of [X,0] and
[X,1], and E is NULL; this corresponds to the
Hiero rules as in Chiang (2005);
• -y contains words and variables in the form
of [X,TAG2], in which TAG is from the LDC
tagset; this defines a well formed subtree, in
which at least one child (constituent) is aligned
to continuous target ngrams. If -y contains only
variables from LDC tag set, this indicates all
the constituents (children) in the subtree are
aligned. This is a superset of rules generalizing
</listItem>
<footnote confidence="0.6531375">
1we use end-user terminologies for source and target.
2we index the tags for multiple occurrences in one rule
</footnote>
<bodyText confidence="0.999116466666667">
those in Wang et al. (2007). If -y contains vari-
ables from POS tags, this essentially produces
a superset of the monolingual side POS-based
reordering rules explored in Tillmann (2008).
We focus on the third category — a syntactic label
E over the span of -y, indicating the covered source
words consist of a linguistically well-defined phrase.
E together with -y define a tree-like structure: the root
node is E, and the aligned children are nonterminals
in -y. The structure information is encoded in (E,
-y) pair-wise connections, and the variables keep the
generalizations over atomic translation-pairs similar
to Hiero models. When the rule is applied during
decoding time, the labels, the tree-structure and the
lexical items need to be all matched.
</bodyText>
<sectionHeader confidence="0.83413" genericHeader="method">
3 Learning and Applying Rules
</sectionHeader>
<bodyText confidence="0.9999215">
A parser is assumed for the source language in the
parallel data. In our case, a Chinese parser is applied
for training and test data. A word alignment model is
used to align the source words with the target words.
</bodyText>
<subsectionHeader confidence="0.998716">
3.1 Extractions
</subsectionHeader>
<bodyText confidence="0.999860695652174">
Our rule extraction is a three-step process. First, tra-
ditional blocks (phrase-pairs) extraction is carried
out. Secondly, Tree-to-String rules, are then ex-
tracted from the aligned blocks, of which the source
side is covered by a complete subtree, with different
permutations of the embedded aligned constituents,
or partially lexicalized constituents. Otherwise, the
Hiero-like rules will be extracted when there is no
sub-tree structure identified, in our final step. Fre-
quencies of extracted rules were counted to compute
feature functions.
Figure 1-(a) shows that a subtree (with root at
VP) is aligned to the English string. Considering the
huge quantity of all the permutations of the aligned
constituents under the tree, only part of the Tree-to-
String rules extracted are shown in Figure 1-(c). The
variables incorporate linguistic information in the
assigned tag by the parser. When there is no aligned
constituent for further generalization, the variables,
defined in our grammar, back off to the Hiero-like
ones without any label-identity information. One
such example is in the rule “在 [X,0] 前 [X,VP] --�
[X,VP] before the [X,0]”, in which the Hiero-style
</bodyText>
<figure confidence="0.9863954">
574
VP
PP VP
ߎথ
೼ 咢ᯢ ࠡ
೼ 咢ᯢ ࠡ before the sunrise
ߎথ March
咢ᯢ sunrise
೼ 咢ᯢ ࠡ ߎথ
March before the sunrise
[X,PP] [X,VP]
[X,PP] ߎথ
೼ 咢ᯢ ࠡ [X,VP]
೼ [X,0] ࠡ[X,VP]
[X,VP] [X,PP]
March [X,PP]
[X,VP] before the sunrise
[X,VP ] before the [X,0]
March before the sunrise
(a) Parse-Tree Alignment (b) Blocks Alignment (c) Tree-to-String rules with root of VP
</figure>
<figureCaption confidence="0.842505">
Figure 1: Example rules extracted. (a) the aligned source parse tree with target string; (b) general blocks alignment;
(c) Tree-to-String rules, with root of VP. The tree structure is aligned with target strings
</figureCaption>
<figure confidence="0.907458">
ℸ Ḝ ೼ ഄ ᓩথ Ꮌ
</figure>
<figureCaption confidence="0.869183333333333">
Figure 2: Subtree of “VP(PP,VP)” triggered a reordering pattern of swapping the order of the two children PP and VP
in the source parse tree. This will move the translation “in the local” after the translation of “triggered a huge shock”,
to form the preferred translation in the highlighted cell: “triggered a huge shock in the local”.
</figureCaption>
<figure confidence="0.863956">
This case locally triggered a huge a great shock
The case in the local a huge shock great shocks
This case was
This local in local triggered an enormous shock
The In the trigger shocks
case In thelocal a huge
cases
ℸ Ḝ ೼ ᔧഄ ᓩথ Ꮌ໻ 䳛ࡼ
this
case in the locals triggered enormous shock
Translations of “೼ ᔧഄ ᓩথ Ꮌ໻ 䳛ࡼ”:
triggered a huge shock in the local
locally triggered an enormous
shock
DT
໻
DP NP
NN
IP
NP VP
P
NN
PP
NP
VV NP
ADJP
JJ
VP
NP
NN
䳛ࡼ
locally
</figure>
<bodyText confidence="0.999445714285714">
variable [X,0] and the label-based variable [X,VP]
co-exist in our proposed rule.
We illustrate several special cases of our extracted
Tree-to-String rules in the following. We index the
variables with their positions to indicate the align-
ment —, and skip the feature function w to simplify
the notations.
</bodyText>
<equation confidence="0.995185">
X →&lt; [X, IP]; [X, NP0] [X, V P0]; (2)
[X, NP0] is [X, V P0] &gt; .
</equation>
<bodyText confidence="0.984853166666667">
The rule in Eqn. 2 shows that a source tree rooted
at IP, with two children of NP and VP generalized
into variables [X,NP] and [X,VP]; they are rewritten
into “[X,NP] is [X,VP]”, with the spontaneous word
is inserted. Such rules are not allowed in Hiero-style
models, as there is no lexical item between the two
variables (Chiang, 2005) in the source side. This
tremendous
shocked
rule will generate a spontaneous word “is” from the
given subtree structure. Usually, it is very hard to
align the spontaneous word correctly, and the rules
we proposed indicate that spontaneous words are
generated directly from the source sub-tree struc-
ture, and they might not necessarily get aligned to
some particular source words.
A second example is shown in Eqn. 3, which is
similar to the Hiero rules:
</bodyText>
<equation confidence="0.860888">
X →&lt; 0; [X, 0] zhiyi; (3)
</equation>
<bodyText confidence="0.983222428571429">
one of the [X, 0] &gt; .
The rule in Eqn. 3 shows that when there is
no linguistically-motivated root covering the span,
([X,NULL] is then assigned), we simply back
off to the Hiero rules. In this case, the source
span of [X, 0] zhiyi is rewritten into the target
“one of the [X, 0]”, without considering the map-
</bodyText>
<page confidence="0.671222">
575
</page>
<bodyText confidence="0.9997176">
ping of the root of the span. In this way, the repre-
sentation power is kept in the variables in our rules,
even if the source subtree is aligned to a discontin-
uous sequence on the target side. This is important
for Chinese-to-English, because the grammar struc-
ture is so different that more than 40% of the subtree
structures were not kept during the translation in our
study on hand-aligned data. Following strictly the
source side syntax will derail from these informative
translation patterns.
</bodyText>
<equation confidence="0.9988985">
X →&lt; [X, NP]; [X, NN1][X, NN2][X, NN3];
[X, NN3][X, NN1][X, NN2] &gt; . (4)
</equation>
<bodyText confidence="0.993144555555555">
Eqn. 4. is a POS-based rule — a special case in
our proposed rules. This rule shows the reorder-
ing patterns for three adjacent NN’s. POS based
rules can be very informative for some language-
pairs such as Arabic-to-English, where the ADJ is
usually moved before NN during the translations.
As also shown in Eqn. 4 for POS sequences, in the
UPenn treebank-style parse trees, a root usually have
more than two variables. Our rule set for subtree,
therefore, contain more than two variables: “X →&lt;
[X, IP]; [X, ADV P0][X, NP0][X, V P0]; [X, NP0]
[X, ADV P0][X, V P0] &gt;”. A CYK-style decoder
has to rely on binarization to preprocess the
grammar as did in (Zhang et al., 2006) to handle
multi-nonterminal rules. We adopt the so-called
dotted-rule or dotted-production, similar to the
Early-style algorithm (Earley, 1970), to handle the
multi-nonterminal rules in our chart-based decoder.
</bodyText>
<subsectionHeader confidence="0.999287">
3.2 Feature Functions
</subsectionHeader>
<bodyText confidence="0.999915076923077">
As used in most of the SMT decoders for a phrase-
pair, a set of standard feature functions are applied
in our decoder, including IBM Model-1 like scores
in both directions, relative frequencies in both direc-
tions. In addition to these features, a counter is as-
sociated to each rule to collect how many rules were
applied so far to generate a hypothesis. The stan-
dard Minimum Error Rate training (Och, 2003) was
applied to tune the weights for all feature types.
The number of extracted rules from the GALE
data is generally large. We pruned the rules accord-
ing to their frequencies, and only keep at most the
top-50 frequent candidates for each source side.
</bodyText>
<sectionHeader confidence="0.994617" genericHeader="method">
4 Chart-based Decoder
</sectionHeader>
<bodyText confidence="0.999976333333333">
Given the source sentence, with constituent parse-
trees, the decoder is to find the best derivation D*
which yield the English string e*:
</bodyText>
<equation confidence="0.988496">
e* = arg max {O(D)O(e)O(f|e)}, (5)
D*
</equation>
<bodyText confidence="0.999981315789474">
where O(D) is the cost for each of the derivations
that lead to e from a given source-parsed f; O(e)
is for cost functions from the standard n-gram lan-
guage models; O(f|e) is the cost for the standard
translation models, including general blocks. We
separate the costs for normal blocks and the general-
ized rules explicitly here, because the blocks contain
stronger lexical evidences observed directly from
data, and we assign them with less cost penalties
via a different weight factor visible for optimization,
and prefer the lexical match over the derived paths
during the decoding.
Our decoder is a chart-based parser with beam-
search for each cell in a chart. Because the tree-
structure can have more than two children, there-
fore, the Tree-to-String rules extracted usually con-
tain more than two variables. Slightly different from
the decoder in (Chiang, 2005), we implemented
the dotted-rule in Early-style parser to handle rules
containing more than two variables. Our cube-
expansion, implemented the cube-pruning in Chiang
(2007), and integrated piece-wise cost computations
for language models via LM states. The intermedi-
ate hypotheses were merged (recombined) accord-
ing to their LM states and other cost model states.
We use MER (Och, 2003) to tune the decoder’s pa-
rameters using a development data set.
Figure 2 shows an example of a tree-based rule
fired at the subtree of VP covering the highlighted
cell. When a rule is applied at a certain cell in the
chart, the covered source ngram should match not
only the lexical items in the rules, but also the tree-
structures as well. The two children under the sub-
tree root VP are PP (“在当地”: in the local) and VP
(“引发巨大震动”: triggered a huge shock ). This
rule triggered a swap of these children to generate
the correct word order in the translation: “triggered
a huge shock in the local”.
</bodyText>
<page confidence="0.89867">
576
</page>
<sectionHeader confidence="0.997894" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99997408">
Our training data consists of two corpora: the GALE
Chinese-English parallel corpus and the LDC hand-
aligned corpus1. The Chinese side of these two cor-
pora were parsed using a constituency parser (Luo,
2003). The average labeled F-measure of the parser
is 81.4%.
Parallel sentences were first word-aligned using
a MaxEnt aligner (Ittycheriah and Roukos, 2005).
Then, phrase-pairs that overlap with our develop-
ment and test set were extracted from the word
alignments (from both hand alignments and auto-
matically aligned GALE corpora) based on the pro-
jection principle (Tillmann, 2003). Besides the regu-
lar phrase-pairs, we also extracted the Tree-to-String
rules from the two corpora. The detailed statistics
are shown in Table 1. Our re-implementation of Hi-
ero system is the baseline. We integrated the eleven
reordering rules described in (Wang et al., 2007),
in our chart-based decoder. In addition, we report
the results of using the Tree-to-String rules extracted
from the hand-aligned training data and the automat-
ically aligned training data. We also report the result
of our translation quality in terms of both BLEU (Pa-
pineni et al., 2002) and TER (Snover et al., 2006)
against four human reference translations.
</bodyText>
<subsectionHeader confidence="0.996259">
5.1 The Data
</subsectionHeader>
<bodyText confidence="0.999883">
Table 1 shows the statistics of our training, develop-
ment and test data. As our word aligner (Ittycheriah
and Roukos, 2005) can introduce errors in extracting
Tree-to-String rules, we use a small hand-aligned
data set “CE16K”, which consists of 16K sentence-
pairs, to get relatively clean rules, free from align-
ment errors. A much larger GALE data set, which
consists of 10 million sentence-pairs, is used to in-
vestigate the scalability of our proposed approach.
</bodyText>
<tableCaption confidence="0.999528">
Table 1: Training and Test Data
</tableCaption>
<table confidence="0.909636">
Train/test sentences src words tgt words
CE16K 16379 380103 477801
GALE 10.5M 274M 310M
MT03 919 24099 -
Dev07 2303 61881 -
1LDC2006E93
</table>
<bodyText confidence="0.999585666666667">
The NIST 2003 MT Evaluation (MT03) is used
as our development data set to tune the decoder’s
parameters toward better BLEU score. The text part
of GALE 2007 Chinese-to-English Development set
(GALE DEV07) is used as our test set. MT03 con-
sists of 919 sentences, whereas GALE DEV07 con-
sists of 2303 sentences under two genres: NewsWire
and WebLog. Both have four human reference trans-
lations.
</bodyText>
<subsectionHeader confidence="0.998643">
5.2 Details of Extracted Rules
</subsectionHeader>
<bodyText confidence="0.9998118">
From the hand-aligned data, the rules we extracted
fall into three categories: regular blocks (phrase-
pairs), Hiero-like rules, and Tree-to-String rules.
The statistics of the extracted rules are shown in Ta-
ble 2
</bodyText>
<tableCaption confidence="0.996347">
Table 2: Rules extracted from hand-aligned data
</tableCaption>
<table confidence="0.9413974">
Types Frequency
Block 846965
Hiero 508999
Tree-to-String 409767
Total 1765731
</table>
<bodyText confidence="0.999772833333333">
We focus on Tree-to-String rules. Table 3 shows
the detailed statistics of the Tree-to-String rules ex-
tracted from the Chinese-to-English hand-aligned
training data. The following section provides a de-
tailed analysis of the most frequent subtrees ob-
served in our training data.
</bodyText>
<subsubsectionHeader confidence="0.51518">
5.2.1 Frequent Subtrees: NP, VP, and DNP
</subsubsectionHeader>
<bodyText confidence="0.999833">
The majority of Tree-to-String rules we extracted
are rooted at the following labels: NP (46%),
VP(22.8%), DNP (2.23%), and QP(2.94%).
Wang et al. (2007) covers only subtrees of NP,
VP, and LCP, which are a subset of our proposed
Tree-to-String rules here. They apply these rules as
a pre-processing step to reorder the input sentences
with hard decisions. Our proposed Tree-to-String
rules, on the contrary, are applied during the de-
coding process which allows for considering many
possible competing reordering options for the given
sentences, and the decoder will choose the best one
according to the cost functions.
Table 4 shows the statistics of reordering rules
for subtrees rooted at VP. The statistics suggest that
</bodyText>
<page confidence="0.836453">
577
</page>
<tableCaption confidence="0.998646">
Table 5: Hiero, Tree-Based (eleven rules in Wang et al. (2007)), and Tree-to-String Rules with “DE”
</tableCaption>
<table confidence="0.999914">
Ruleset Root Src Tgt Frequency
NULL [X,0] M [X,1] [X,0] ’s [X,1] 347
Hiero NULL [X,0] M [X,1] [X,1] of [X,0] 306
NULL [X,0] M [X,1] [X,0] of [X,1] 174
NP DNP(NP) NP NP DNP(NP) -
Tree-Based NP DNP(PP) NP NP DNP(PP) -
NP DNP(LCP) NP NP DNP(LCP) -
[X,DNP] [X,NP] [X,DEG] [X,NP] [X,DEG] 580
Tree-to-String [X,DNP] [X,NP] [X,DEG] [X,DEG] [X,NP] 2163
[X,DNP] [X,NP] [X,DEG] [X,NP] , [X,DEG] 4
</table>
<tableCaption confidence="0.957283">
Table 3: Distributions of the NP, VP, QP, LCP rules
</tableCaption>
<table confidence="0.999948846153846">
Root Frequency Percentage (%)
NP 189616 46.2
VP 93535 22.8
IP 68341 16.6
PP 18519 4.51
DNP 9141 2.23
QP 12064 2.94
LCP 4127 1.00
CP 2994 0.73
PRN 2810 0.68
DP 1415 0.34
Others 6879 1.67
Total 409767 -
</table>
<tableCaption confidence="0.989908666666667">
Table 4: Distribution of the reordering rules for subtrees
rooted at VP: [X,VP]; [X,PP] [X,VP]; statistics are col-
lected from GALE training data
</tableCaption>
<table confidence="0.996622333333333">
Root Target Frequency
[X,PP] [X,VP] 126310
[X,VP] [X,PP] 22144
VP [X,PP] , [X,VP] 1524
[X,PP] that [X,VP] 1098
[X,PP] and [X,VP] 831
</table>
<bodyText confidence="0.999919129032258">
it is impossible to come up with a reordering rule
that is always applicable. For instance, (Wang et
al., 2007) will always swap the children of the sub-
tree VP(PP,VP). However, the statistics shown in Ta-
ble 4 suggest that might not be best way. In fact,
due to parser’s performance and word alignment ac-
curacies, the statistics we collected from the GALE
dataset, containing 10 million sentence-pairs, show
that the children in the subtree VP(PP,VP) is trans-
lated monotonically 126310 times, while reordered
of only 22144 times. However, the hand-aligned
data support the swap for 1245 times, and monotoni-
cally for only 168 times. Part of this disagreement is
due to the word segmentation errors, incorrect word
alignments and unreliable parsing results.
Another observations through our extracted Tree-
to-String rules is on the controlled insertion of the
target spontaneous2 (function) words. Instead of hy-
pothesizing spontaneous words based only on the
language model or only on observing in phrase-
pairs, we make use of the Tree-to-String rules to get
suggestion on the insertion of spontaneous words.
In this way, we can make sure that the spontaneous
words are generated from the structure information,
as opposed to those from a pure hypothesis. The ad-
vantage of this method is shown in Table 4. For in-
stance, the word “that” and the punctuation “,” were
generated in the target side of the rule. This proves
that our model can provide a more principled way to
generate spontaneous words needed for fluent trans-
lations.
</bodyText>
<note confidence="0.447033">
5.2.2 DEG and DEC
</note>
<bodyText confidence="0.986480714285714">
An interesting linguistic phenomenon that we in-
vestigated is the Chinese word DE “M”. “M” is an
informative lexical clue that indicates the need for
long range phrasal movements. Table 5 shows a few
2Target spontaneous words are function words that do not
have specific lexical source informants and are needed to make
the target translation fluent.
</bodyText>
<page confidence="0.763798">
578
</page>
<bodyText confidence="0.999896823529412">
high-frequent reordering rules that contain the Chi-
nese word “DE”.
The three type of rules handle “DE” differently. A
major difference is the structure in the source side.
Hiero rules do not consider any structure, and ap-
ply the rule of “[X,0] 的 [X,1]”. Tree-based rules,
as described in Wang et al. (2007) do not handle
的 directly; they are often implicitly taken care of
when reordering DNPs instead. Our proposed Tree-
to-String rules model 的 directly in a subtree con-
taining DEG/DEC, which triggers word reordering
within the structure. Our rule set includes all the
above three rule-types with the associated frequen-
cies, this enriched the reordering choices to be cho-
sen by the chart-based decoder, guided by the statis-
tics collected from the data and the language model
costs.
</bodyText>
<subsectionHeader confidence="0.996937">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.987512019230769">
We tuned the decoding parameters using the MT03
data set, and applied the updated parameters to the
GALE evaluation set. The eleven rules of VP, NP,
and LCP (tree-based) improved the Hiero baseline3
from 32.43 to 33.02 on BLEU. The reason, the tree-
reordering does not gain much over Hiero baseline,
is probably that the reordering patterns covered by
tree-reordering rules, are potentially handled in the
standard Hiero grammar.
A small but noticeable further improvement over
tree-based rules, from 33.02 to 33.26, was ob-
tained on applying Tree-to-String rules extracted
from hand-aligned dataset. We think that the Tree-
based rules covers major reordering patterns for
Chinese-English, and our hand-aligned dataset is
also too small to capture representative statistics and
more reordering patterns. A close check at the rules
we learned from the hand-aligned data shows that
the tree-based rules are simply the subset of the
rules extracted. The Tree-to-String grammar im-
proved the Hiero baseline from 32.43 to 33.26 on
BLEU; considering the effects from the tree-based
rules only, the additional information improved the
BLEU scores from 33.02 to 33.26. Similar pictures
of improvements were observed for the two unseen
tests of newswire and weblog in GALE data.
When applying the rules extracted from the much
3Hiero results are from our own re-implementation.
larger GALE training set with about ten million
sentence-pairs, we achieved significant improve-
ments from both genres (newswire and web data).
The improvements are significant in both BLEU
and TER. BLEU improved from 32.44 to 33.51 on
newswire, and from 25.88 to 27.91 on web data.
Similar improvements were found in TER as shown
in the table. The gain came mostly from the richer
extracted rule set, which not only presents robust
statistics for reordering patterns, but also offers more
target spontaneous words generated from the syntac-
tic structures. Since the top-frequent rules extracted
are NP, VP, and IP as shown in Table 3, our proposed
rules will be able to win the correct word order with
reliable statistics, as long as the parser shows accept-
able performances on these structures. This is espe-
cially important for weblog data, where the parser’s
overall accuracy potentially might not be very good.
Table 7 shows the translations from different
grammars for the same source sentence. Both Tree-
based and Tree-to-String methods get the correct re-
ordering, while the latter can suggest insertions of
target spontaneous words like “a” to allow the trans-
lation to run more fluently.
</bodyText>
<sectionHeader confidence="0.981804" genericHeader="conclusions">
6 Conclusion and Discussions
</sectionHeader>
<bodyText confidence="0.999975523809524">
In this paper, we proposed our approach to model
both local and non-local word-reordering in one
probabilistic synchronous CFG. Our current model
incorporates source-side syntactic information, to
model the observations that the source syntactic con-
stituent tends to move together during translations.
The proposed rule set generalizes over the variables
in Hiero-rules, and we also showed the special cases
of the Tree-based rules and the POS-based rules.
Since the proposed rules has at most one-level tree
structure, they can be easily applied in a chart-based
decoder. We analyzed the statistics of our rules,
qualitatively and quantitatively. Next, we compared
our work with other research, especially with the
work in Wang et al. (2007). Finally, we reported
our empirical results on Chinese-English transla-
tions. Our Tree-to-String rules showed significant
improvements over the Hiero baseline on the GALE
DEV07 test set.
Given the low accuracy of the parsers, and the po-
tential errors from Chinese word-segmentations, and
</bodyText>
<page confidence="0.896999">
579
</page>
<tableCaption confidence="0.745988">
Table 6: Hiero, Tree-Based (NP, VP, LCP), and Tree-to-String rules extracted from hand-aligned data (H) or from
GALE training data (G)
</tableCaption>
<table confidence="0.9999655">
Setup MT03 TER GALE07-NewsWire TER GALE07-Weblog TER
BLEUr4n4 BLEUr4n4 BLEUr4n4
Hiero 32.43 59.75 31.68 61.45 25.99 65.65
Tree-based 33.02 59.84 32.22 61.46 25.67 65.64
Tree-to-String (H) 33.26 61.04 32.44 61.36 25.88 65.54
Tree-to-String (G) 35.51 57.28 33.51 59.71 27.91 62.88
</table>
<tableCaption confidence="0.999275">
Table 7: Hiero, Tree-Based (NP, VP, LCP), Tree-to-String Translations
</tableCaption>
<bodyText confidence="0.9964110625">
Src-Sent k 4Eh�ft�IaC)O X10
Hiero in this case local triggered shock .
Tree-Based the case triggered uproar in the local.
Tree-to-String the case triggered a huge uproar in the local .
word-alignments, our rules learned are still noisy.
Exploring better cost functions associate each rule
might lead to further improvement. Because of
the relative high accuracy of English parsers, many
works such as Zollmann and Venugopal (2006) and
Shen et al. (2008) emphasize on using syntax in tar-
get languages, to directly influence the fluency as-
pect of the translation output. In future, we plan to
incorporate features from target-side syntactic infor-
mation, and connect them with the source informa-
tion explored in this paper, to model long-distance
reordering for better translation quality.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999368">
The authors would like to thank the anonymous
reviewers for their comments to improve this pa-
per. This work was supported by DARPA GALE
program under the contract number HR0011-06-2-
0001.
</bodyText>
<sectionHeader confidence="0.98298" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995387435897436">
Yaser Al-Onaizan and Papineni. Kishore. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of ACL-COLING, pages 529–536.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL’05), pages 263–270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. In Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. In Communications of the ACM., volume 13,
pages 94–102.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Processing,
pages 304–311, Philadelphia, PA, July 6-7.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. NAACL-HLT.
Abraham Ittycheriah and Salim Roukos. 2005. A maxi-
mum entropy word aligner for arabic-english machine
translation. In HLT/EMNLP.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In HLT/EMNLP 2005, Vancouver, B.C., Canada.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In ACL-Coling.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In 45th
Annual Meeting of the Association for Computational
Linguistics.
Xiaoqiang Luo. 2003. A maximum entropy chinese
character-based parser. In Proc. of ACL.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In ACL.
580
Sonja Niessen and Hermann Ney. 2004. Statistical
machine translation with scarce resources using mor-
phosyntactic information. In Computational Linguis-
tics.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
In Computational Linguistics, volume 30, pages 417–
449.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computational
Linguistics, Japan, Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), pages 311–318, Philadelphia, PA,
July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Christoph Tillmann and Tong Zhang. 2005. A localized
prediction model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL’05), pages
557–564, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proc. of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Christoph Tillmann. 2008. A rule-driven dynamic pro-
gramming decoder for statistical mt. In HLT Second
Workshop on Syntax and Structure in Statistical Trans-
lation.
Chao Wang, Michael Collins, and Phillip Koehn. 2007.
Chinese syntactic reordering for statistical machine
translation. In proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora. In
Computational Linguistics, volume 23(3), pages 377–
403.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In the 20th International Conference on
Computational Linguistics (COLING 2004), Geneva,
Switzerland, Aug 22-29.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In ACL-Coling.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering, volume 11, pages 207–238.
K. Yamada and Kevin. Knight. 2001. Syntax-based Sta-
tistical Translation Model. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL-2001).
Richard Zens, E. Matusov, and Hermmann Ney. 2004.
Improved word alignment using a symmetric lexicon
model. In Proceedings of the 20th International Con-
ference on Computational Linguistics (CoLing 2004),
pages 36–42, Geneva, Switzerland, Auguest.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the HLT-NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. of NAACL 2006 - Workshop on statistical ma-
chine translation.
</reference>
<page confidence="0.921776">
581
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.840240">
<title confidence="0.9994115">Generalizing Local and Non-Local Word-Reordering Patterns Syntax-Based Machine Translation</title>
<author confidence="0.999212">Bing Zhao Yaser Al-onaizan</author>
<affiliation confidence="0.991428">IBM T.J. Watson Research IBM T.J. Watson Research</affiliation>
<address confidence="0.867841">Yorktown Heights, NY-10598 Yorktown Heights, NY-10598</address>
<email confidence="0.998278">zhaob@us.ibm.comonaizan@us.ibm.com</email>
<abstract confidence="0.998570947368421">Syntactic word reordering is essential for translations across different grammar structures between syntactically distant languagepairs. In this paper, we propose to embed local and non-local word reordering decisions in a synchronous context free grammar, and leverages the grammar in a chartbased decoder. Local word-reordering is effectively encoded in Hiero-like rules; whereas non-local word-reordering, which allows for long-range movements of syntactic chunks, is represented in tree-based reordering rules, which contain variables correspond to sourceside syntactic constituents. We demonstrate how these rules are learned from parallel corpora. Our proposed shallow Tree-to-String rules show significant improvements in translation quality across different test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kishore</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-COLING,</booktitle>
<pages>529--536</pages>
<contexts>
<context position="6522" citStr="Kishore, 2006" startWordPosition="962" endWordPosition="963">ree, or difficult combinatorial optimizations for the feature functions associated with rules. In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, wh</context>
</contexts>
<marker>Kishore, 2006</marker>
<rawString>Yaser Al-Onaizan and Papineni. Kishore. 2006. Distortion models for statistical machine translation. In Proceedings of ACL-COLING, pages 529–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3311" citStr="Chiang (2005)" startWordPosition="481" endWordPosition="482">rce parse-tree (a special case of ITG (Wu, 1997)). It effectively enhances the quality of the phrase-based translation of Chinese-to-English. One major weakness is that the reordering decisions were done in the preprocessing step, therefore rendering the decoding process unable to recover the reordering errors from the rules if incorrectly applied to. Also the reordering decisions are made without the benefits of additional models (e.g., the language models) that are typically used during decoding. Another method to address the re-ordering problem in translation is the Hiero model proposed by Chiang (2005), in which a probabilistic synchronous context free grammar (PSCFG) was applied to guide the decoding. Hiero rules generalize phrase-pairs 572 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 572–581, Honolulu, October 2008.c�2008 Association for Computational Linguistics by introducing a single generic nonterminal (i.e., a variable) [X]. The combination of variables and lexicalized words in a Hiero rule nicely captures local word and phrase reordering (modeling an implicit reordering window of max-phrase length). These rules are then applied in a C</context>
<context position="8233" citStr="Chiang, 2005" startWordPosition="1232" endWordPosition="1233">ructured as follows: in section § 2, we define the notations in our synchronous context free grammar, in section § 3, the rule extractions are illustrated in details, in section § 4, the decoding process of applying these rules is described. Experiments in § 5 were carried out using GALE Dev07 datasets. Improved translation qualities were obtained by applying the proposed Tree-to-String rules. Conclusions and discussions are given in § 6. 2 Shallow Tree-to-String Rules Our proposed rules are in the form of probabilistic synchronous context free grammar (PSCFG). We adopt the notations used in (Chiang, 2005). Let N be a set of nonterminals, a rule has the following 573 form: X —*&lt; E; -y; α; w &gt;, (1) where X abstracts nonterminal symbols in N; -y E [N, VS]+ is a sequence of one or more source 1 words (as in the vocabulary of VS) and nonterminal symbols in N; α E [N, VT]+ is a sequence of one or more target words (in VT) and nonterminals in N . — is the one-to-one alignment of the nonterminals between -y and α; w contains non-negative weights associated with each rule; E is a label-symbol specifying the root node of the source span covering -y. In our grammar, E is one of the labels (e.g., NP) defi</context>
<context position="9877" citStr="Chiang (2005)" startWordPosition="1544" endWordPosition="1545"> Tree-toString rules we are proposing. At the same time, we will describe the extraction algorithm, with which we derive our rules from the word-aligned sourceparsed parallel text. Our nonterminal set N is a reduced set of the treebank tagset (Xue et al., 2005). It consists of 17 unique labels. The rules we extract belong to one of the following categories: • -y contains only words, and E is NULL; this corresponds to the general blocks used in phrasebased decoder (Och and Ney, 2004); • -y contains words and variables of [X,0] and [X,1], and E is NULL; this corresponds to the Hiero rules as in Chiang (2005); • -y contains words and variables in the form of [X,TAG2], in which TAG is from the LDC tagset; this defines a well formed subtree, in which at least one child (constituent) is aligned to continuous target ngrams. If -y contains only variables from LDC tag set, this indicates all the constituents (children) in the subtree are aligned. This is a superset of rules generalizing 1we use end-user terminologies for source and target. 2we index the tags for multiple occurrences in one rule those in Wang et al. (2007). If -y contains variables from POS tags, this essentially produces a superset of t</context>
<context position="14622" citStr="Chiang, 2005" startWordPosition="2360" endWordPosition="2361">te several special cases of our extracted Tree-to-String rules in the following. We index the variables with their positions to indicate the alignment —, and skip the feature function w to simplify the notations. X →&lt; [X, IP]; [X, NP0] [X, V P0]; (2) [X, NP0] is [X, V P0] &gt; . The rule in Eqn. 2 shows that a source tree rooted at IP, with two children of NP and VP generalized into variables [X,NP] and [X,VP]; they are rewritten into “[X,NP] is [X,VP]”, with the spontaneous word is inserted. Such rules are not allowed in Hiero-style models, as there is no lexical item between the two variables (Chiang, 2005) in the source side. This tremendous shocked rule will generate a spontaneous word “is” from the given subtree structure. Usually, it is very hard to align the spontaneous word correctly, and the rules we proposed indicate that spontaneous words are generated directly from the source sub-tree structure, and they might not necessarily get aligned to some particular source words. A second example is shown in Eqn. 3, which is similar to the Hiero rules: X →&lt; 0; [X, 0] zhiyi; (3) one of the [X, 0] &gt; . The rule in Eqn. 3 shows that when there is no linguistically-motivated root covering the span, (</context>
<context position="18608" citStr="Chiang, 2005" startWordPosition="3043" endWordPosition="3044">separate the costs for normal blocks and the generalized rules explicitly here, because the blocks contain stronger lexical evidences observed directly from data, and we assign them with less cost penalties via a different weight factor visible for optimization, and prefer the lexical match over the derived paths during the decoding. Our decoder is a chart-based parser with beamsearch for each cell in a chart. Because the treestructure can have more than two children, therefore, the Tree-to-String rules extracted usually contain more than two variables. Slightly different from the decoder in (Chiang, 2005), we implemented the dotted-rule in Early-style parser to handle rules containing more than two variables. Our cubeexpansion, implemented the cube-pruning in Chiang (2007), and integrated piece-wise cost computations for language models via LM states. The intermediate hypotheses were merged (recombined) according to their LM states and other cost model states. We use MER (Och, 2003) to tune the decoder’s parameters using a development data set. Figure 2 shows an example of a tree-based rule fired at the subtree of VP covering the highlighted cell. When a rule is applied at a certain cell in th</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="18779" citStr="Chiang (2007)" startWordPosition="3067" endWordPosition="3068">ssign them with less cost penalties via a different weight factor visible for optimization, and prefer the lexical match over the derived paths during the decoding. Our decoder is a chart-based parser with beamsearch for each cell in a chart. Because the treestructure can have more than two children, therefore, the Tree-to-String rules extracted usually contain more than two variables. Slightly different from the decoder in (Chiang, 2005), we implemented the dotted-rule in Early-style parser to handle rules containing more than two variables. Our cubeexpansion, implemented the cube-pruning in Chiang (2007), and integrated piece-wise cost computations for language models via LM states. The intermediate hypotheses were merged (recombined) according to their LM states and other cost model states. We use MER (Och, 2003) to tune the decoder’s parameters using a development data set. Figure 2 shows an example of a tree-based rule fired at the subtree of VP covering the highlighted cell. When a rule is applied at a certain cell in the chart, the covered source ngram should match not only the lexical items in the rules, but also the treestructures as well. The two children under the subtree root VP are</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2113" citStr="Collins et al., 2005" startWordPosition="299" endWordPosition="302">ctical features (e.g., Spanish noun modifier constructs are written in English as modifier noun). However, for syntactically distant language-pairs such as ChineseEnglish, long-range reordering is required where whole phrases are moved across the sentence. The idea of “syntactic cohesion” (Fox, 2002) is characterized by its simplicity, which has attracted researchers for years. Previous works include several approaches of incorporating syntactic information to preprocess the source sentences to make them more like the target language in structure. Xia and McCord (2004) (Niessen and Ney, 2004; Collins et al., 2005) described approaches applied to languagepairs such as French-English and German-English. Later, Wang et al. (2007) presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English. Overall, these works are similar, in that they design a few language-specific and linguistically motivated reordering rules, which are generally simple. The eleven rules described in Wang et al. (2007) are appealing, as they have rather simple structure, modeling only NP, VP and LCP via one-level sub-tree structure with two children, in the source parse-tree </context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>In Communications of the ACM.,</journal>
<volume>13</volume>
<pages>94--102</pages>
<contexts>
<context position="16801" citStr="Earley, 1970" startWordPosition="2740" endWordPosition="2741">s Arabic-to-English, where the ADJ is usually moved before NN during the translations. As also shown in Eqn. 4 for POS sequences, in the UPenn treebank-style parse trees, a root usually have more than two variables. Our rule set for subtree, therefore, contain more than two variables: “X →&lt; [X, IP]; [X, ADV P0][X, NP0][X, V P0]; [X, NP0] [X, ADV P0][X, V P0] &gt;”. A CYK-style decoder has to rely on binarization to preprocess the grammar as did in (Zhang et al., 2006) to handle multi-nonterminal rules. We adopt the so-called dotted-rule or dotted-production, similar to the Early-style algorithm (Earley, 1970), to handle the multi-nonterminal rules in our chart-based decoder. 3.2 Feature Functions As used in most of the SMT decoders for a phrasepair, a set of standard feature functions are applied in our decoder, including IBM Model-1 like scores in both directions, relative frequencies in both directions. In addition to these features, a counter is associated to each rule to collect how many rules were applied so far to generate a hypothesis. The standard Minimum Error Rate training (Och, 2003) was applied to tune the weights for all feature types. The number of extracted rules from the GALE data </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. In Communications of the ACM., volume 13, pages 94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>304--311</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1793" citStr="Fox, 2002" startWordPosition="252" endWordPosition="253">the different word orders between the source language and the target language. Different language-pairs require different levels of word reordering. For example, when we translate between English and Spanish (or other Romance languages), most of the word reordering needed is local because of the shared syntactical features (e.g., Spanish noun modifier constructs are written in English as modifier noun). However, for syntactically distant language-pairs such as ChineseEnglish, long-range reordering is required where whole phrases are moved across the sentence. The idea of “syntactic cohesion” (Fox, 2002) is characterized by its simplicity, which has attracted researchers for years. Previous works include several approaches of incorporating syntactic information to preprocess the source sentences to make them more like the target language in structure. Xia and McCord (2004) (Niessen and Ney, 2004; Collins et al., 2005) described approaches applied to languagepairs such as French-English and German-English. Later, Wang et al. (2007) presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English. Overall, these works are similar, in that</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 304–311, Philadelphia, PA, July 6-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In Proc. NAACL-HLT.</booktitle>
<contexts>
<context position="6757" citStr="Graehl and Knight, 2004" startWordPosition="996" endWordPosition="999">ized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decoding process very complex, and auxiliary rules have to be designed and incorporated to shrink multiple source nonterminals into one target nonterminal. From our empirical observations, most of the time, however, the mu</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Proc. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="19964" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3268" endWordPosition="3271">e two children under the subtree root VP are PP (“在当地”: in the local) and VP (“引发巨大震动”: triggered a huge shock ). This rule triggered a swap of these children to generate the correct word order in the translation: “triggered a huge shock in the local”. 576 5 Experiments Our training data consists of two corpora: the GALE Chinese-English parallel corpus and the LDC handaligned corpus1. The Chinese side of these two corpora were parsed using a constituency parser (Luo, 2003). The average labeled F-measure of the parser is 81.4%. Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005). Then, phrase-pairs that overlap with our development and test set were extracted from the word alignments (from both hand alignments and automatically aligned GALE corpora) based on the projection principle (Tillmann, 2003). Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora. The detailed statistics are shown in Table 1. Our re-implementation of Hiero system is the baseline. We integrated the eleven reordering rules described in (Wang et al., 2007), in our chart-based decoder. In addition, we report the results of using the Tree-to-String rules </context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP 2005,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="6584" citStr="Kumar and Byrne, 2005" startWordPosition="972" endWordPosition="975">feature functions associated with rules. In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decoding process very complex, and auxiliary rule</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In HLT/EMNLP 2005, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In ACL-Coling.</booktitle>
<contexts>
<context position="6776" citStr="Liu et al., 2006" startWordPosition="1000" endWordPosition="1003">ning. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decoding process very complex, and auxiliary rules have to be designed and incorporated to shrink multiple source nonterminals into one target nonterminal. From our empirical observations, most of the time, however, the multi-level tree-stru</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In ACL-Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7069" citStr="Liu et al. (2007)" startWordPosition="1047" endWordPosition="1050">addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decoding process very complex, and auxiliary rules have to be designed and incorporated to shrink multiple source nonterminals into one target nonterminal. From our empirical observations, most of the time, however, the multi-level tree-structure is broken in the translation process, and POS tags are frequently distorted. Indeed, strictly following the source parse tree is usually not necessary, and maybe too expensive for the translation process. The remainder of this paper is structured as follows: in section § 2, we define th</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>A maximum entropy chinese character-based parser.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="19812" citStr="Luo, 2003" startWordPosition="3248" endWordPosition="3249"> in the chart, the covered source ngram should match not only the lexical items in the rules, but also the treestructures as well. The two children under the subtree root VP are PP (“在当地”: in the local) and VP (“引发巨大震动”: triggered a huge shock ). This rule triggered a swap of these children to generate the correct word order in the translation: “triggered a huge shock in the local”. 576 5 Experiments Our training data consists of two corpora: the GALE Chinese-English parallel corpus and the LDC handaligned corpus1. The Chinese side of these two corpora were parsed using a constituency parser (Luo, 2003). The average labeled F-measure of the parser is 81.4%. Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005). Then, phrase-pairs that overlap with our development and test set were extracted from the word alignments (from both hand alignments and automatically aligned GALE corpora) based on the projection principle (Tillmann, 2003). Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora. The detailed statistics are shown in Table 1. Our re-implementation of Hiero system is the baseline. We integrated the ele</context>
</contexts>
<marker>Luo, 2003</marker>
<rawString>Xiaoqiang Luo. 2003. A maximum entropy chinese character-based parser. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6031" citStr="Marton and Resnik (2008)" startWordPosition="884" endWordPosition="887">ng-range reordering decisions to be local in a chart-based decoder. To be more specific, we think the simple shallow syntactic structure is powerful enough for capturing the major structure-reordering patterns, such as NP, VP and LCP structures. We also use simple frequency-based feature functions, similar to the blocks used in phrase-based decoder, to further improve the rules’ representation power. Overall, this enables us to avoid either a complex decoding process to generate the source parse tree, or difficult combinatorial optimizations for the feature functions associated with rules. In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Niessen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation with scarce resources using morphosyntactic information.</title>
<date>2004</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="2090" citStr="Niessen and Ney, 2004" startWordPosition="295" endWordPosition="298">use of the shared syntactical features (e.g., Spanish noun modifier constructs are written in English as modifier noun). However, for syntactically distant language-pairs such as ChineseEnglish, long-range reordering is required where whole phrases are moved across the sentence. The idea of “syntactic cohesion” (Fox, 2002) is characterized by its simplicity, which has attracted researchers for years. Previous works include several approaches of incorporating syntactic information to preprocess the source sentences to make them more like the target language in structure. Xia and McCord (2004) (Niessen and Ney, 2004; Collins et al., 2005) described approaches applied to languagepairs such as French-English and German-English. Later, Wang et al. (2007) presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English. Overall, these works are similar, in that they design a few language-specific and linguistically motivated reordering rules, which are generally simple. The eleven rules described in Wang et al. (2007) are appealing, as they have rather simple structure, modeling only NP, VP and LCP via one-level sub-tree structure with two children, in</context>
</contexts>
<marker>Niessen, Ney, 2004</marker>
<rawString>Sonja Niessen and Hermann Ney. 2004. Statistical machine translation with scarce resources using morphosyntactic information. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>30</volume>
<pages>417--449</pages>
<contexts>
<context position="9751" citStr="Och and Ney, 2004" startWordPosition="1518" endWordPosition="1521">es. Our nonterminal symbols include the labels and the POS tags in the source parse trees. In the following, we will illustrate the Tree-toString rules we are proposing. At the same time, we will describe the extraction algorithm, with which we derive our rules from the word-aligned sourceparsed parallel text. Our nonterminal set N is a reduced set of the treebank tagset (Xue et al., 2005). It consists of 17 unique labels. The rules we extract belong to one of the following categories: • -y contains only words, and E is NULL; this corresponds to the general blocks used in phrasebased decoder (Och and Ney, 2004); • -y contains words and variables of [X,0] and [X,1], and E is NULL; this corresponds to the Hiero rules as in Chiang (2005); • -y contains words and variables in the form of [X,TAG2], in which TAG is from the LDC tagset; this defines a well formed subtree, in which at least one child (constituent) is aligned to continuous target ngrams. If -y contains only variables from LDC tag set, this indicates all the constituents (children) in the subtree are aligned. This is a superset of rules generalizing 1we use end-user terminologies for source and target. 2we index the tags for multiple occurren</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. In Computational Linguistics, volume 30, pages 417– 449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Japan, Sapporo,</location>
<contexts>
<context position="17296" citStr="Och, 2003" startWordPosition="2824" endWordPosition="2825">rules. We adopt the so-called dotted-rule or dotted-production, similar to the Early-style algorithm (Earley, 1970), to handle the multi-nonterminal rules in our chart-based decoder. 3.2 Feature Functions As used in most of the SMT decoders for a phrasepair, a set of standard feature functions are applied in our decoder, including IBM Model-1 like scores in both directions, relative frequencies in both directions. In addition to these features, a counter is associated to each rule to collect how many rules were applied so far to generate a hypothesis. The standard Minimum Error Rate training (Och, 2003) was applied to tune the weights for all feature types. The number of extracted rules from the GALE data is generally large. We pruned the rules according to their frequencies, and only keep at most the top-50 frequent candidates for each source side. 4 Chart-based Decoder Given the source sentence, with constituent parsetrees, the decoder is to find the best derivation D* which yield the English string e*: e* = arg max {O(D)O(e)O(f|e)}, (5) D* where O(D) is the cost for each of the derivations that lead to e from a given source-parsed f; O(e) is for cost functions from the standard n-gram lan</context>
<context position="18993" citStr="Och, 2003" startWordPosition="3101" endWordPosition="3102">for each cell in a chart. Because the treestructure can have more than two children, therefore, the Tree-to-String rules extracted usually contain more than two variables. Slightly different from the decoder in (Chiang, 2005), we implemented the dotted-rule in Early-style parser to handle rules containing more than two variables. Our cubeexpansion, implemented the cube-pruning in Chiang (2007), and integrated piece-wise cost computations for language models via LM states. The intermediate hypotheses were merged (recombined) according to their LM states and other cost model states. We use MER (Och, 2003) to tune the decoder’s parameters using a development data set. Figure 2 shows an example of a tree-based rule fired at the subtree of VP covering the highlighted cell. When a rule is applied at a certain cell in the chart, the covered source ngram should match not only the lexical items in the rules, but also the treestructures as well. The two children under the subtree root VP are PP (“在当地”: in the local) and VP (“引发巨大震动”: triggered a huge shock ). This rule triggered a swap of these children to generate the correct word order in the translation: “triggered a huge shock in the local”. 576 5</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, Japan, Sapporo, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="20753" citStr="Papineni et al., 2002" startWordPosition="3393" endWordPosition="3397">orpora) based on the projection principle (Tillmann, 2003). Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora. The detailed statistics are shown in Table 1. Our re-implementation of Hiero system is the baseline. We integrated the eleven reordering rules described in (Wang et al., 2007), in our chart-based decoder. In addition, we report the results of using the Tree-to-String rules extracted from the hand-aligned training data and the automatically aligned training data. We also report the result of our translation quality in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) against four human reference translations. 5.1 The Data Table 1 shows the statistics of our training, development and test data. As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set “CE16K”, which consists of 16K sentencepairs, to get relatively clean rules, free from alignment errors. A much larger GALE data set, which consists of 10 million sentence-pairs, is used to investigate the scalability of our proposed approach. Table 1: Training and Test Data Train/test sentence</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Conf. of the Association for Computational Linguistics (ACL 02), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="20783" citStr="Snover et al., 2006" startWordPosition="3400" endWordPosition="3403">principle (Tillmann, 2003). Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora. The detailed statistics are shown in Table 1. Our re-implementation of Hiero system is the baseline. We integrated the eleven reordering rules described in (Wang et al., 2007), in our chart-based decoder. In addition, we report the results of using the Tree-to-String rules extracted from the hand-aligned training data and the automatically aligned training data. We also report the result of our translation quality in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) against four human reference translations. 5.1 The Data Table 1 shows the statistics of our training, development and test data. As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set “CE16K”, which consists of 16K sentencepairs, to get relatively clean rules, free from alignment errors. A much larger GALE data set, which consists of 10 million sentence-pairs, is used to investigate the scalability of our proposed approach. Table 1: Training and Test Data Train/test sentences src words tgt words CE16K 16</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A localized prediction model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>557--564</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="6611" citStr="Tillmann and Zhang, 2005" startWordPosition="976" endWordPosition="979">iated with rules. In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decoding process very complex, and auxiliary rules have to be designed and i</context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2005. A localized prediction model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 557–564, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A projection extension algorithm for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="20189" citStr="Tillmann, 2003" startWordPosition="3305" endWordPosition="3306">n the local”. 576 5 Experiments Our training data consists of two corpora: the GALE Chinese-English parallel corpus and the LDC handaligned corpus1. The Chinese side of these two corpora were parsed using a constituency parser (Luo, 2003). The average labeled F-measure of the parser is 81.4%. Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005). Then, phrase-pairs that overlap with our development and test set were extracted from the word alignments (from both hand alignments and automatically aligned GALE corpora) based on the projection principle (Tillmann, 2003). Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora. The detailed statistics are shown in Table 1. Our re-implementation of Hiero system is the baseline. We integrated the eleven reordering rules described in (Wang et al., 2007), in our chart-based decoder. In addition, we report the results of using the Tree-to-String rules extracted from the hand-aligned training data and the automatically aligned training data. We also report the result of our translation quality in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) again</context>
</contexts>
<marker>Tillmann, 2003</marker>
<rawString>Christoph Tillmann. 2003. A projection extension algorithm for statistical machine translation. In Proc. of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A rule-driven dynamic programming decoder for statistical mt.</title>
<date>2008</date>
<booktitle>In HLT Second Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="10551" citStr="Tillmann (2008)" startWordPosition="1657" endWordPosition="1658">, in which TAG is from the LDC tagset; this defines a well formed subtree, in which at least one child (constituent) is aligned to continuous target ngrams. If -y contains only variables from LDC tag set, this indicates all the constituents (children) in the subtree are aligned. This is a superset of rules generalizing 1we use end-user terminologies for source and target. 2we index the tags for multiple occurrences in one rule those in Wang et al. (2007). If -y contains variables from POS tags, this essentially produces a superset of the monolingual side POS-based reordering rules explored in Tillmann (2008). We focus on the third category — a syntactic label E over the span of -y, indicating the covered source words consist of a linguistically well-defined phrase. E together with -y define a tree-like structure: the root node is E, and the aligned children are nonterminals in -y. The structure information is encoded in (E, -y) pair-wise connections, and the variables keep the generalizations over atomic translation-pairs similar to Hiero models. When the rule is applied during decoding time, the labels, the tree-structure and the lexical items need to be all matched. 3 Learning and Applying Rule</context>
</contexts>
<marker>Tillmann, 2008</marker>
<rawString>Christoph Tillmann. 2008. A rule-driven dynamic programming decoder for statistical mt. In HLT Second Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Phillip Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In proceedings of EMNLP.</booktitle>
<contexts>
<context position="2228" citStr="Wang et al. (2007)" startWordPosition="315" endWordPosition="318">ically distant language-pairs such as ChineseEnglish, long-range reordering is required where whole phrases are moved across the sentence. The idea of “syntactic cohesion” (Fox, 2002) is characterized by its simplicity, which has attracted researchers for years. Previous works include several approaches of incorporating syntactic information to preprocess the source sentences to make them more like the target language in structure. Xia and McCord (2004) (Niessen and Ney, 2004; Collins et al., 2005) described approaches applied to languagepairs such as French-English and German-English. Later, Wang et al. (2007) presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English. Overall, these works are similar, in that they design a few language-specific and linguistically motivated reordering rules, which are generally simple. The eleven rules described in Wang et al. (2007) are appealing, as they have rather simple structure, modeling only NP, VP and LCP via one-level sub-tree structure with two children, in the source parse-tree (a special case of ITG (Wu, 1997)). It effectively enhances the quality of the phrase-based translation of Chinese-</context>
<context position="10394" citStr="Wang et al. (2007)" startWordPosition="1631" endWordPosition="1634"> variables of [X,0] and [X,1], and E is NULL; this corresponds to the Hiero rules as in Chiang (2005); • -y contains words and variables in the form of [X,TAG2], in which TAG is from the LDC tagset; this defines a well formed subtree, in which at least one child (constituent) is aligned to continuous target ngrams. If -y contains only variables from LDC tag set, this indicates all the constituents (children) in the subtree are aligned. This is a superset of rules generalizing 1we use end-user terminologies for source and target. 2we index the tags for multiple occurrences in one rule those in Wang et al. (2007). If -y contains variables from POS tags, this essentially produces a superset of the monolingual side POS-based reordering rules explored in Tillmann (2008). We focus on the third category — a syntactic label E over the span of -y, indicating the covered source words consist of a linguistically well-defined phrase. E together with -y define a tree-like structure: the root node is E, and the aligned children are nonterminals in -y. The structure information is encoded in (E, -y) pair-wise connections, and the variables keep the generalizations over atomic translation-pairs similar to Hiero mod</context>
<context position="20465" citStr="Wang et al., 2007" startWordPosition="3347" endWordPosition="3350">the parser is 81.4%. Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005). Then, phrase-pairs that overlap with our development and test set were extracted from the word alignments (from both hand alignments and automatically aligned GALE corpora) based on the projection principle (Tillmann, 2003). Besides the regular phrase-pairs, we also extracted the Tree-to-String rules from the two corpora. The detailed statistics are shown in Table 1. Our re-implementation of Hiero system is the baseline. We integrated the eleven reordering rules described in (Wang et al., 2007), in our chart-based decoder. In addition, we report the results of using the Tree-to-String rules extracted from the hand-aligned training data and the automatically aligned training data. We also report the result of our translation quality in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) against four human reference translations. 5.1 The Data Table 1 shows the statistics of our training, development and test data. As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set “CE16K”</context>
<context position="22703" citStr="Wang et al. (2007)" startWordPosition="3706" endWordPosition="3709">re shown in Table 2 Table 2: Rules extracted from hand-aligned data Types Frequency Block 846965 Hiero 508999 Tree-to-String 409767 Total 1765731 We focus on Tree-to-String rules. Table 3 shows the detailed statistics of the Tree-to-String rules extracted from the Chinese-to-English hand-aligned training data. The following section provides a detailed analysis of the most frequent subtrees observed in our training data. 5.2.1 Frequent Subtrees: NP, VP, and DNP The majority of Tree-to-String rules we extracted are rooted at the following labels: NP (46%), VP(22.8%), DNP (2.23%), and QP(2.94%). Wang et al. (2007) covers only subtrees of NP, VP, and LCP, which are a subset of our proposed Tree-to-String rules here. They apply these rules as a pre-processing step to reorder the input sentences with hard decisions. Our proposed Tree-to-String rules, on the contrary, are applied during the decoding process which allows for considering many possible competing reordering options for the given sentences, and the decoder will choose the best one according to the cost functions. Table 4 shows the statistics of reordering rules for subtrees rooted at VP. The statistics suggest that 577 Table 5: Hiero, Tree-Base</context>
<context position="24407" citStr="Wang et al., 2007" startWordPosition="3998" endWordPosition="4001">f the NP, VP, QP, LCP rules Root Frequency Percentage (%) NP 189616 46.2 VP 93535 22.8 IP 68341 16.6 PP 18519 4.51 DNP 9141 2.23 QP 12064 2.94 LCP 4127 1.00 CP 2994 0.73 PRN 2810 0.68 DP 1415 0.34 Others 6879 1.67 Total 409767 - Table 4: Distribution of the reordering rules for subtrees rooted at VP: [X,VP]; [X,PP] [X,VP]; statistics are collected from GALE training data Root Target Frequency [X,PP] [X,VP] 126310 [X,VP] [X,PP] 22144 VP [X,PP] , [X,VP] 1524 [X,PP] that [X,VP] 1098 [X,PP] and [X,VP] 831 it is impossible to come up with a reordering rule that is always applicable. For instance, (Wang et al., 2007) will always swap the children of the subtree VP(PP,VP). However, the statistics shown in Table 4 suggest that might not be best way. In fact, due to parser’s performance and word alignment accuracies, the statistics we collected from the GALE dataset, containing 10 million sentence-pairs, show that the children in the subtree VP(PP,VP) is translated monotonically 126310 times, while reordered of only 22144 times. However, the hand-aligned data support the swap for 1245 times, and monotonically for only 168 times. Part of this disagreement is due to the word segmentation errors, incorrect word</context>
<context position="26499" citStr="Wang et al. (2007)" startWordPosition="4345" endWordPosition="4348">we investigated is the Chinese word DE “M”. “M” is an informative lexical clue that indicates the need for long range phrasal movements. Table 5 shows a few 2Target spontaneous words are function words that do not have specific lexical source informants and are needed to make the target translation fluent. 578 high-frequent reordering rules that contain the Chinese word “DE”. The three type of rules handle “DE” differently. A major difference is the structure in the source side. Hiero rules do not consider any structure, and apply the rule of “[X,0] 的 [X,1]”. Tree-based rules, as described in Wang et al. (2007) do not handle 的 directly; they are often implicitly taken care of when reordering DNPs instead. Our proposed Treeto-String rules model 的 directly in a subtree containing DEG/DEC, which triggers word reordering within the structure. Our rule set includes all the above three rule-types with the associated frequencies, this enriched the reordering choices to be chosen by the chart-based decoder, guided by the statistics collected from the data and the language model costs. 5.3 Evaluation We tuned the decoding parameters using the MT03 data set, and applied the updated parameters to the GALE eval</context>
<context position="30318" citStr="Wang et al. (2007)" startWordPosition="4950" endWordPosition="4953">FG. Our current model incorporates source-side syntactic information, to model the observations that the source syntactic constituent tends to move together during translations. The proposed rule set generalizes over the variables in Hiero-rules, and we also showed the special cases of the Tree-based rules and the POS-based rules. Since the proposed rules has at most one-level tree structure, they can be easily applied in a chart-based decoder. We analyzed the statistics of our rules, qualitatively and quantitatively. Next, we compared our work with other research, especially with the work in Wang et al. (2007). Finally, we reported our empirical results on Chinese-English translations. Our Tree-to-String rules showed significant improvements over the Hiero baseline on the GALE DEV07 test set. Given the low accuracy of the parsers, and the potential errors from Chinese word-segmentations, and 579 Table 6: Hiero, Tree-Based (NP, VP, LCP), and Tree-to-String rules extracted from hand-aligned data (H) or from GALE training data (G) Setup MT03 TER GALE07-NewsWire TER GALE07-Weblog TER BLEUr4n4 BLEUr4n4 BLEUr4n4 Hiero 32.43 59.75 31.68 61.45 25.99 65.65 Tree-based 33.02 59.84 32.22 61.46 25.67 65.64 Tree</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Phillip Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="2746" citStr="Wu, 1997" startWordPosition="396" endWordPosition="397">plied to languagepairs such as French-English and German-English. Later, Wang et al. (2007) presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English. Overall, these works are similar, in that they design a few language-specific and linguistically motivated reordering rules, which are generally simple. The eleven rules described in Wang et al. (2007) are appealing, as they have rather simple structure, modeling only NP, VP and LCP via one-level sub-tree structure with two children, in the source parse-tree (a special case of ITG (Wu, 1997)). It effectively enhances the quality of the phrase-based translation of Chinese-to-English. One major weakness is that the reordering decisions were done in the preprocessing step, therefore rendering the decoding process unable to recover the reordering errors from the rules if incorrectly applied to. Also the reordering decisions are made without the benefits of additional models (e.g., the language models) that are typically used during decoding. Another method to address the re-ordering problem in translation is the Hiero model proposed by Chiang (2005), in which a probabilistic synchron</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. In Computational Linguistics, volume 23(3), pages 377– 403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical mt system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2067" citStr="Xia and McCord (2004)" startWordPosition="291" endWordPosition="294">ng needed is local because of the shared syntactical features (e.g., Spanish noun modifier constructs are written in English as modifier noun). However, for syntactically distant language-pairs such as ChineseEnglish, long-range reordering is required where whole phrases are moved across the sentence. The idea of “syntactic cohesion” (Fox, 2002) is characterized by its simplicity, which has attracted researchers for years. Previous works include several approaches of incorporating syntactic information to preprocess the source sentences to make them more like the target language in structure. Xia and McCord (2004) (Niessen and Ney, 2004; Collins et al., 2005) described approaches applied to languagepairs such as French-English and German-English. Later, Wang et al. (2007) presented specific rules to pre-order long-range movements of words, and improved the translations for Chinese-to-English. Overall, these works are similar, in that they design a few language-specific and linguistically motivated reordering rules, which are generally simple. The eleven rules described in Wang et al. (2007) are appealing, as they have rather simple structure, modeling only NP, VP and LCP via one-level sub-tree structur</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical mt system with automatically learned rewrite patterns. In the 20th International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland, Aug 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In ACL-Coling.</booktitle>
<contexts>
<context position="6542" citStr="Xiong et al., 2006" startWordPosition="964" endWordPosition="967">lt combinatorial optimizations for the feature functions associated with rules. In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decodin</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In ACL-Coling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering,</journal>
<volume>11</volume>
<pages>207--238</pages>
<contexts>
<context position="9525" citStr="Xue et al., 2005" startWordPosition="1475" endWordPosition="1478">ting that the source span -y is rooted at E. Additionally, a NULL tag 0 in E denotes a flat structure of -y, in which no constituent structure was found to cover the span, and we need to back off to the normal Hiero-style rules. Our nonterminal symbols include the labels and the POS tags in the source parse trees. In the following, we will illustrate the Tree-toString rules we are proposing. At the same time, we will describe the extraction algorithm, with which we derive our rules from the word-aligned sourceparsed parallel text. Our nonterminal set N is a reduced set of the treebank tagset (Xue et al., 2005). It consists of 17 unique labels. The rules we extract belong to one of the following categories: • -y contains only words, and E is NULL; this corresponds to the general blocks used in phrasebased decoder (Och and Ney, 2004); • -y contains words and variables of [X,0] and [X,1], and E is NULL; this corresponds to the Hiero rules as in Chiang (2005); • -y contains words and variables in the form of [X,TAG2], in which TAG is from the LDC tagset; this defines a well formed subtree, in which at least one child (constituent) is aligned to continuous target ngrams. If -y contains only variables fr</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering, volume 11, pages 207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knight</author>
</authors>
<title>Syntax-based Statistical Translation Model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Conference of the Association for Computational Linguistics (ACL-2001).</booktitle>
<contexts>
<context position="6732" citStr="Knight, 2001" startWordPosition="994" endWordPosition="995"> weights optimized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decoding process very complex, and auxiliary rules have to be designed and incorporated to shrink multiple source nonterminals into one target nonterminal. From our empirical observations, most of </context>
</contexts>
<marker>Knight, 2001</marker>
<rawString>K. Yamada and Kevin. Knight. 2001. Syntax-based Statistical Translation Model. In Proceedings of the Conference of the Association for Computational Linguistics (ACL-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>E Matusov</author>
<author>Hermmann Ney</author>
</authors>
<title>Improved word alignment using a symmetric lexicon model.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (CoLing</booktitle>
<pages>36--42</pages>
<location>Geneva, Switzerland, Auguest.</location>
<contexts>
<context position="6561" citStr="Zens et al., 2004" startWordPosition="968" endWordPosition="971">imizations for the feature functions associated with rules. In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. The combinatorial effects of the added feature functions can make the feature selection and optimization of the weights rather difficult. Since the grammar is essentially the same as the Hiero ones, a standard CYK decoder can be simply applied in their work. Word reordering can also be addressed via distortion models. Work in (Al-Onaizan and Kishore, 2006; Xiong et al., 2006; Zens et al., 2004; Kumar and Byrne, 2005; Tillmann and Zhang, 2005) modeled the limited information available at phrase-boundaries. Syntax-based approaches such as (Yamada and Knight, 2001; Graehl and Knight, 2004; Liu et al., 2006) heavily rely on the parse-tree to constrain the search space by assuming a strong mapping of structures across distant language-pairs. Their algorithms are also subject to parsers’ performances to a larger extent, and have high complexity and less scalability in reality. In Liu et al. (2007), multi-level tree-structured rules were designed, which made the decoding process very comp</context>
</contexts>
<marker>Zens, Matusov, Ney, 2004</marker>
<rawString>Richard Zens, E. Matusov, and Hermmann Ney. 2004. Improved word alignment using a symmetric lexicon model. In Proceedings of the 20th International Conference on Computational Linguistics (CoLing 2004), pages 36–42, Geneva, Switzerland, Auguest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL.</booktitle>
<contexts>
<context position="16657" citStr="Zhang et al., 2006" startWordPosition="2720" endWordPosition="2723">proposed rules. This rule shows the reordering patterns for three adjacent NN’s. POS based rules can be very informative for some languagepairs such as Arabic-to-English, where the ADJ is usually moved before NN during the translations. As also shown in Eqn. 4 for POS sequences, in the UPenn treebank-style parse trees, a root usually have more than two variables. Our rule set for subtree, therefore, contain more than two variables: “X →&lt; [X, IP]; [X, ADV P0][X, NP0][X, V P0]; [X, NP0] [X, ADV P0][X, V P0] &gt;”. A CYK-style decoder has to rely on binarization to preprocess the grammar as did in (Zhang et al., 2006) to handle multi-nonterminal rules. We adopt the so-called dotted-rule or dotted-production, similar to the Early-style algorithm (Earley, 1970), to handle the multi-nonterminal rules in our chart-based decoder. 3.2 Feature Functions As used in most of the SMT decoders for a phrasepair, a set of standard feature functions are applied in our decoder, including IBM Model-1 like scores in both directions, relative frequencies in both directions. In addition to these features, a counter is associated to each rule to collect how many rules were applied so far to generate a hypothesis. The standard </context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL</booktitle>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proc. of NAACL 2006 - Workshop on statistical machine translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>