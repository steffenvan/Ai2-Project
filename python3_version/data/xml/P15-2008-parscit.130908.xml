<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013589">
<title confidence="0.997845">
Document Classification by Inversion of
Distributed Language Representations
</title>
<author confidence="0.996153">
Matt Taddy
</author>
<affiliation confidence="0.997282">
University of Chicago Booth School of Business
</affiliation>
<email confidence="0.995233">
taddy@chicagobooth.edu
</email>
<sectionHeader confidence="0.993821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961666666667">
There have been many recent advances
in the structure and measurement of dis-
tributed language models: those that map
from words to a vector-space that is rich in
information about word choice and com-
position. This vector-space is the dis-
tributed language representation.
The goal of this note is to point out
that any distributed representation can be
turned into a classifier through inversion
via Bayes rule. The approach is simple
and modular, in that it will work with
any language representation whose train-
ing can be formulated as optimizing a
probability model. In our application to 2
million sentences from Yelp reviews, we
also find that it performs as well as or bet-
ter than complex purpose-built algorithms.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961666666667">
Distributed, or vector-space, language representa-
tions V consist of a location, or embedding, for
every vocabulary word in IR,K, where K is the di-
mension of the latent representation space. These
locations are learned to optimize, perhaps approx-
imately, an objective function defined on the origi-
nal text such as a likelihood for word occurrences.
A popular example is the Word2Vec machinery
of Mikolov et al. (2013). This trains the distributed
representation to be useful as an input layer for
prediction of words from their neighbors in a Skip-
gram likelihood. That is, to maximize
</bodyText>
<equation confidence="0.946525">
t+bE log pV(wsj  |wst) (1)
jet, j=t−b
</equation>
<bodyText confidence="0.999470054054054">
summed across all words wst in all sentences ws,
where b is the skip-gram window (truncated by the
ends of the sentence) and pV(wsj|wst) is a neural
network classifier that takes vector representations
for wst and wsj as input (see Section 2).
Distributed language representations have been
studied since the early work on neural networks
(Rumelhart et al., 1986) and have long been ap-
plied in natural language processing (Morin and
Bengio, 2005). The models are generating much
recent interest due to the large performance gains
from the newer systems, including Word2Vec and
the Glove model of Pennington et al. (2014), ob-
served in, e.g., word prediction, word analogy
identification, and named entity recognition.
Given the success of these new models, re-
searchers have begun searching for ways to adapt
the representations for use in document classifica-
tion tasks such as sentiment prediction or author
identification. One naive approach is to use ag-
gregated word vectors across a document (e.g., a
document’s average word-vector location) as input
to a standard classifier (e.g., logistic regression).
However, a document is actually an ordered path
of locations through IR,K, and simple averaging de-
stroys much of the available information.
More sophisticated aggregation is proposed in
Socher et al. (2011; 2013), where recursive neu-
ral networks are used to combine the word vectors
through the estimated parse tree for each sentence.
Alternatively, Le and Mikolov’s Doc2Vec (2014)
adds document labels to the conditioning set in (1)
and has them influence the skip-gram likelihood
through a latent input vector location in V. In each
case, the end product is a distributed representa-
tion for every sentence (or document for Doc2Vec)
that can be used as input to a generic classifier.
</bodyText>
<subsectionHeader confidence="0.997681">
1.1 Bayesian Inversion
</subsectionHeader>
<bodyText confidence="0.990766">
These approaches all add considerable model and
estimation complexity to the original underlying
distributed representation. We are proposing a
simple alternative that turns fitted distributed lan-
guage representations into document classifiers
</bodyText>
<page confidence="0.977771">
45
</page>
<bodyText confidence="0.8678717">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 45–49,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
without any additional modeling or estimation.
Write the probability model that the represen-
tation V has been trained to optimize (likeli-
hood maximize) as pV(d), where document d =
{w1, ...wS} is a set of sentences – ordered vectors
of word identities. For example, in Word2Vec the
skip-gram likelihood in (1) yields
</bodyText>
<equation confidence="0.977369">
log pV(d) = E E t+bE log pVy(wsj  |wst).
s t j6=t, j=t−b
</equation>
<bodyText confidence="0.989799692307692">
(2)
Even when such a likelihood is not explicit it will
be implied by the objective function that is opti-
mized during training.
Now suppose that your training documents are
grouped by class label, y E {1... C}. We can
train separate distributed language representations
for each set of documents as partitioned by y;
for example, fit Word2Vec independently on each
sub-corpus Dc = {di : yi = c} and obtain the
labeled distributed representation map Vc. A new
document d has probability pVc(d) if we treat it as
a member of class c, and Bayes rule implies
</bodyText>
<equation confidence="0.99696">
p(y|d) = pVc (d)πc (3)
</equation>
<bodyText confidence="0.999901859649123">
where πc is our prior probability on class label c.
Thus distributed language representations
trained separately for each class label yield
directly a document classification rule via (3).
This approach has a number of attractive qualities.
Simplicity: The inversion strategy works for any
model of language that can (or its training can) be
interpreted as a probabilistic model. This makes
for easy implementation in systems that are al-
ready engineered to fit such language represen-
tations, leading to faster deployment and lower
development costs. The strategy is also inter-
pretable: whatever intuition one has about the dis-
tributed language model can be applied directly to
the inversion-based classification rule. Inversion
adds a plausible model for reader understanding
on top of any given language representation.
Scalability: when working with massive corpora
it is often useful to split the data into blocks as part
of distributed computing strategies. Our model of
classification via inversion provides a convenient
top-level partitioning of the data. An efficient sys-
tem could fit separate by-class language represen-
tations, which will provide for document classi-
fication as in this article as well as class-specific
answers for NLP tasks such as word prediction or
analogy. When one wishes to treat a document as
unlabeled, NLP tasks can be answered through en-
semble aggregation of the class-specific answers.
Performance: We find that, in our examples, in-
version of Word2Vec yields lower misclassifica-
tion rates than both Doc2Vec-based classification
and the multinomial inverse regression (MNIR) of
Taddy (2013b). We did not anticipate such out-
right performance gain. Moreover, we expect that
with calibration (i.e., through cross-validation)
of the many various tuning parameters available
when fitting both Word and Doc 2Vec the perfor-
mance results will change. Indeed, we find that all
methods are often outperformed by phrase-count
logistic regression with rare-feature up-weighting
and carefully chosen regularization. However, the
out-of-the-box performance of Word2Vec inver-
sion argues for its consideration as a simple default
in document classification.
In the remainder, we outline classification
through inversion of a specific Word2Vec model
and illustrate the ideas in classification of Yelp
reviews. The implementation requires only a
small extension of the popular gensim python
library (Rehurek and Sojka, 2010); the ex-
tended library as well as code to reproduce
all of the results in this paper are available
on github. In addition, the yelp data is
publicly available as part of the correspond-
ing data mining contest at kaggle.com. See
github.com/taddylab/deepir fordetail.
</bodyText>
<sectionHeader confidence="0.970431" genericHeader="introduction">
2 Implementation
</sectionHeader>
<bodyText confidence="0.974106714285714">
Word2Vec trains V to maximize the skip-gram
likelihood based on (1). We work with the Huff-
man softmax specification (Mikolov et al., 2013),
which includes a pre-processing step to encode
each vocabulary word in its representation via a
binary Huffman tree (see Figure 1).
Each individual probability is then
</bodyText>
<equation confidence="0.915140333333333">
L(w)−1
( )
σ ch [η(w,j + 1)] u&gt; η(w,j)vwt
</equation>
<bodyText confidence="0.952247428571429">
(4)
where η(w, i) is the ith node in the Huffman tree
path, of length L(w), for word w; σ(x) = 1/(1 +
exp[−x]); and ch(η) E {−1, +1} translates from
whether η is a left or right child to +/- 1. Every
word thus has both input and output vector coor-
dinates, vw and [uη(w,1) · · · uη(w,L(w))]. Typically,
</bodyText>
<equation confidence="0.9823015">
pV(w|wt) = H
j=1
</equation>
<page confidence="0.995991">
46
</page>
<figureCaption confidence="0.856709166666667">
Figure 1: Binary Huffman encoding of a 4 word
vocabulary, based upon 18 total utterances. At
each step proceeding from left to right the two
nodes with lowest count are combined into a par-
ent node. Binary encodings are read back off of
the splits moving from right to left.
</figureCaption>
<bodyText confidence="0.9963481">
only the input space V = [v,,,l · · · v,,,,], for a p-
word vocabulary, is reported as the language rep-
resentation – these vectors are used as input for
NLP tasks. However, the full representation V in-
cludes mapping from each word to both V and U.
We apply the gensim python implementation
of Word2Vec, which fits the model via stochastic
gradient descent (SGD), under default specifica-
tion. This includes a vector space of dimension
K = 100 and a skip-gram window of size b = 5.
</bodyText>
<subsectionHeader confidence="0.919031">
2.1 Word2Vec Inversion
</subsectionHeader>
<bodyText confidence="0.999977888888889">
Given Word2Vec trained on each of C class-
specific corpora D1 ... DC, leading to C distinct
language representations V1 ... VC, classification
for new documents is straightforward. Consider
the 5-sentence document d: each sentence ws is
given a probability under each representation V,
by applying the calculations in (1) and (4). This
leads to the 5xC matrix of sentence probabilities,
pVc(ws), and document probabilities are obtained
</bodyText>
<equation confidence="0.991213333333333">
1 � pVc(ws). (5)
pVc(d) = 5
s
</equation>
<bodyText confidence="0.99795">
Finally, class probabilities are calculated via
Bayes rule as in (3). We use priors π, = 1/C, so
that classification proceeds by assigning the class
</bodyText>
<equation confidence="0.985991">
yˆ = argmax, pVc(d). (6)
</equation>
<sectionHeader confidence="0.999035" genericHeader="method">
3 Illustration
</sectionHeader>
<bodyText confidence="0.992481592592593">
We consider a corpus of reviews provided by Yelp
for a contest on kaggle.com. The text is tok-
enized simply by converting to lowercase before
splitting on punctuation and white-space. The
training data are 230,000 reviews containing more
than 2 million sentences. Each review is marked
by a number of stars, from 1 to 5, and we fit
separate Word2Vec representations V1 ... V5 for
the documents at each star rating. The valida-
tion data consist of 23,000 reviews, and we ap-
ply the inversion technique of Section 2 to score
each validation document d with class probabili-
ties q = [q1 · · · q5], where q, = p(c|d).
The probabilities will be used in three different
classification tasks; for reviews as
a. negative at 1-2 stars, or positive at 3-5 stars;
b. negative 1-2, neutral 3, or positive 4-5 stars;
c. corresponding to each of 1 to 5 stars.
In each case, classification proceeds by sum-
ming across the relevant sub-class probabilities.
For example, in task a, p(positive) = q3 +
q4 + q5. Note that the same five fitted Word2Vec
representations are used for each task.
We consider a set of related comparator tech-
niques. In each case, some document repre-
sentation (e.g., phrase counts or Doc2Vec vec-
tors) is used as input to logistic regression pre-
diction of the associated review rating. The lo-
gistic regressions are fit under L1 regularization
with the penalties weighted by feature standard
deviation (which, e.g., up-weights rare phrases)
and selected according to the corrected AICc cri-
teria (Flynn et al., 2013) via the gamlr R pack-
age of Taddy (2014). For multi-class tasks b-c,
we use distributed Multinomial regression (DMR;
Taddy 2015) via the distrom R package. DMR
fits multinomial logistic regression in a factorized
representation wherein one estimates independent
Poisson linear models for each response category.
Document representations and logistic regressions
are always trained using only the training corpus.
Doc2Vec is also fit via gensim, using the same
latent space specification as for Word2Vec: K =
100 and b = 5. As recommended in the doc-
umentation, we apply repeated SGD over 20 re-
orderings of each corpus (for comparability, this
was also done when fitting Word2Vec). Le and
Mikolov provide two alternative Doc2Vec specifi-
cations: distributed memory (DM) and distributed
bag-of-words (DBOW). We fit both. Vector rep-
resentations for validation documents are trained
without updating the word-vector elements, lead-
ing to 100 dimensional vectors for each docu-
ment for each of DM and DCBOW. We input
</bodyText>
<page confidence="0.997287">
47
</page>
<figure confidence="0.999686308641975">
mnir
0.0 0.2 0.4 0.6 0.8 1.0
●
● ●
● ●
●
● ●
● ●
1 2 3 4 5
word2vec inversion phrase regression doc2vec regression
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
1 2 3 4 5
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
1 2 3 4 5
●
●
●
●
●
●
●
●
●
1 2 3 4 5
0.0 0.2 0.4 0.6 0.8 1.0
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0 0.2 0.4 0.6 0.8 1.0
probability positive
0.0 0.2 0.4 0.6 0.8 1.0
stars
</figure>
<figureCaption confidence="0.741523333333333">
Figure 2: Out-of-Sample fitted probabilities of a review being positive (having greater than 2 stars) as a
function of the true number of review stars. Box widths are proportional to number of observations in
each class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars.
</figureCaption>
<bodyText confidence="0.999076689655172">
each, as well as the combined 200 dimensional
DM+DBOW representation, to logistic regression.
Phrase regression applies logistic regression of re-
sponse classes directly onto counts for short 1-2
word ‘phrases’. The phrases are obtained using
gensim’s phrase builder, which simply combines
highly probable pairings; e.g., first date and
chicken wing are two pairings in this corpus.
MNIR, the multinomial inverse regression of
Taddy (2013a; 2013b; 2015) is applied as im-
plemented in the textir package for R. MNIR
maps from text to the class-space of inter-
est through a multinomial logistic regression of
phrase counts onto variables relevant to the class-
space. We apply MNIR to the same set of 1-2
word phrases used in phrase regression. Here, we
regress phrase counts onto stars expressed numeri-
cally and as a 5-dimensional indicator vector, lead-
ing to a 6-feature multinomial logistic regression.
The MNIR procedure then uses the 6×p matrix of
feature-phrase regression coefficients to map from
phrase-count to feature space, resulting in 6 di-
mensional ‘sufficient reduction’ statistics for each
document. These are input to logistic regression.
Word2Vec aggregation averages fitted word rep-
resentations for a single Word2Vec trained on all
sentences to obtain a fixed-length feature vector
for each review (K = 100, as for inversion). This
vector is then input to logistic regression.
</bodyText>
<subsectionHeader confidence="0.914657">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.998607666666667">
Misclassification rates for each task on the valida-
tion set are reported in Table 1. Simple phrase-
count regression is consistently the strongest per-
former, bested only by Word2Vec inversion on
task b. This is partially due to the relative strengths
of discriminative (e.g., logistic regression) vs gen-
</bodyText>
<table confidence="0.980862875">
a (NP) b (NNP) c (1-5)
.099 .189 .435
.084 .200 .410
.144 .282 .496
.179 .306 .549
.148 . 284 .500
.095 .254 .480
.118 .248 .461
</table>
<tableCaption confidence="0.999746">
Table 1: Out-of-sample misclassification rates.
</tableCaption>
<bodyText confidence="0.99961952">
erative (e.g., all others here) classifiers: given
a large amount of training text, asymptotic effi-
ciency of logistic regression will start to work in
its favor over the finite sample advantages of a
generative classifier (Ng and Jordan, 2002; Taddy,
2013c). However, the comparison is also unfair
to Word2Vec and Doc2Vec: both phrase regres-
sion and MNIR are optimized exactly under AICc
selected penalty, while Word and Doc 2Vec have
only been approximately optimized under a sin-
gle specification. The distributed representations
should improve with some careful engineering.
Word2Vec inversion outperforms the other doc-
ument representation-based alternatives (except,
by a narrow margin, MNIR in task a). Doc2Vec
under DBOW specification and MNIR both do
worse, but not by a large margin. In contrast to
Le and Mikolov, we find here that the Doc2Vec
DM model does much worse than DBOW. Re-
gression onto simple within- document aggrega-
tions of Word2Vec perform slightly better than any
Doc2Vec option (but not as well as the Word2Vec
inversion). This again contrasts the results of Le
and Mikolov and we suspect that the more com-
plex Doc2Vec model would benefit from a careful
</bodyText>
<figure confidence="0.903584428571429">
W2V inversion
Phrase regression
D2V DBOW
D2V DM
D2V combined
MNIR
W2V aggregation
</figure>
<page confidence="0.998152">
48
</page>
<bodyText confidence="0.99985425">
tuning of the SGD optimization routine.1
Looking at the fitted probabilities in detail we
see that Word2Vec inversion provides a more use-
ful document ranking than any comparator (in-
cluding phrase regression). For example, Figure
2 shows the probabilities of a review being ‘pos-
itive’ in task a as a function of the true star rat-
ing for each validation review. Although phrase
regression does slightly better in terms of misclas-
sification rate, it does so at the cost of classifying
many terrible (1 star) reviews as positive. This oc-
curs because 1-2 star reviews are more rare than 3-
5 star reviews and because words of emphasis (e.g.
very, completely, and !!!) are used both
in very bad and in very good reviews. Word2Vec
inversion is the only method that yields positive-
document probabilities that are clearly increasing
in distribution with the true star rating. It is not dif-
ficult to envision a misclassification cost structure
that favors such nicely ordered probabilities.
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999970090909091">
The goal of this note is to point out inversion as an
option for turning distributed language representa-
tions into classification rules. We are not arguing
for the supremacy of Word2Vec inversion in par-
ticular, and the approach should work well with al-
ternative representations (e.g., Glove). Moreover,
we are not even arguing that it will always outper-
form purpose-built classification tools. However,
it is a simple, scalable, interpretable, and effective
option for classification whenever you are working
with such distributed representations.
</bodyText>
<sectionHeader confidence="0.998624" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.917127597222222">
Cheryl Flynn, Clifford Hurvich, and Jefferey Simonoff.
2013. Efficiency for Regularization Parameter Se-
lection in Penalized Likelihood Estimation of Mis-
specified Models. Journal of the American Statisti-
cal Association, 108:1031–1043.
Quoc V. Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
1Note also that the unsupervised document representa-
tions – Doc2Vec or the single Word2Vec used in Word2Vec
aggregation – could be trained on larger unlabeled corpora. A
similar option is available for Word2Vec inversion: one could
take a single Word2Vec model trained on a large unlabeled
corpora as a shared baseline (prior) and update separate mod-
els with additional training on each labeled sub-corpora. The
representations will all be shrunk towards a baseline language
model, but will differ according to distinctions between the
language in each labeled sub-corpora.
ceedings of the 31 st International Conference on
Machine Learning.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the International Workshop on Arti-
ficial Intelligence and Statistics, pages 246–252.
Andrew Y. Ng and Michael I. Jordan. 2002. On Dis-
criminative vs Generative Classifiers: A Compar-
ison of Logistic Regression and naive Bayes. In
Advances in Neural Information Processing Systems
(NIPS).
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
Radim Rehurek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50.
David Rumelhart, Geoffrey Hinton, and Ronald
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323:533–536.
Richard Socher, Cliff C. Lin, Chris Manning, and An-
drew Y. Ng. 2011. Parsing natural scenes and natu-
ral language with recursive neural networks. In Pro-
ceedings of the 28th international conference on ma-
chine learning (ICML-11), pages 129–136.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the conference on
empirical methods in natural language processing
(EMNLP), volume 1631, page 1642.
Matt Taddy. 2013a. Measuring Political Sentiment
on Twitter: Factor Optimal Design for Multinomial
Inverse Regression. Technometrics, 55(4):415–425,
November.
Matt Taddy. 2013b. Multinomial Inverse Regression
for Text Analysis. Journal of the American Statisti-
cal Association, 108:755–770.
Matt Taddy. 2013c. Rejoinder: Efficiency and struc-
ture in MNIR. Journal of the American Statistical
Association, 108:772–774.
Matt Taddy. 2014. One-step estimator paths for con-
cave regularization. arXiv:1308.5623.
Matt Taddy. 2015. Distributed Multinomial Regres-
sion. Annals of Applied Statistics, To appear.
</reference>
<page confidence="0.999545">
49
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963848">
<title confidence="0.9997505">Document Classification by Inversion Distributed Language Representations</title>
<author confidence="0.999848">Matt Taddy</author>
<affiliation confidence="0.996377">University of Chicago Booth School of</affiliation>
<email confidence="0.998786">taddy@chicagobooth.edu</email>
<abstract confidence="0.99833347368421">There have been many recent advances the structure and measurement of dismodels: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cheryl Flynn</author>
<author>Clifford Hurvich</author>
<author>Jefferey Simonoff</author>
</authors>
<title>Efficiency for Regularization Parameter Selection in Penalized Likelihood Estimation of Misspecified Models.</title>
<date>2013</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>108--1031</pages>
<contexts>
<context position="11150" citStr="Flynn et al., 2013" startWordPosition="1805" endWordPosition="1808"> the relevant sub-class probabilities. For example, in task a, p(positive) = q3 + q4 + q5. Note that the same five fitted Word2Vec representations are used for each task. We consider a set of related comparator techniques. In each case, some document representation (e.g., phrase counts or Doc2Vec vectors) is used as input to logistic regression prediction of the associated review rating. The logistic regressions are fit under L1 regularization with the penalties weighted by feature standard deviation (which, e.g., up-weights rare phrases) and selected according to the corrected AICc criteria (Flynn et al., 2013) via the gamlr R package of Taddy (2014). For multi-class tasks b-c, we use distributed Multinomial regression (DMR; Taddy 2015) via the distrom R package. DMR fits multinomial logistic regression in a factorized representation wherein one estimates independent Poisson linear models for each response category. Document representations and logistic regressions are always trained using only the training corpus. Doc2Vec is also fit via gensim, using the same latent space specification as for Word2Vec: K = 100 and b = 5. As recommended in the documentation, we apply repeated SGD over 20 reordering</context>
</contexts>
<marker>Flynn, Hurvich, Simonoff, 2013</marker>
<rawString>Cheryl Flynn, Clifford Hurvich, and Jefferey Simonoff. 2013. Efficiency for Regularization Parameter Selection in Penalized Likelihood Estimation of Misspecified Models. Journal of the American Statistical Association, 108:1031–1043.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents. In Pro1Note also that the unsupervised document representations – Doc2Vec or the single Word2Vec used in Word2Vec aggregation – could be trained on larger unlabeled corpora. A similar option is available for Word2Vec inversion: one could take a single Word2Vec model trained on a large unlabeled corpora as a shared baseline (prior) and update separate models with additional training on each labeled sub-corpora. The representations will all be shrunk towards a baseline language model, but will differ according to distinctions between the language in each labeled sub-corpora.</title>
<date>2014</date>
<booktitle>ceedings of the 31 st International Conference on Machine Learning.</booktitle>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Pro1Note also that the unsupervised document representations – Doc2Vec or the single Word2Vec used in Word2Vec aggregation – could be trained on larger unlabeled corpora. A similar option is available for Word2Vec inversion: one could take a single Word2Vec model trained on a large unlabeled corpora as a shared baseline (prior) and update separate models with additional training on each labeled sub-corpora. The representations will all be shrunk towards a baseline language model, but will differ according to distinctions between the language in each labeled sub-corpora. ceedings of the 31 st International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1318" citStr="Mikolov et al. (2013)" startWordPosition="203" endWordPosition="206">ted as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms. 1 Introduction Distributed, or vector-space, language representations V consist of a location, or embedding, for every vocabulary word in IR,K, where K is the dimension of the latent representation space. These locations are learned to optimize, perhaps approximately, an objective function defined on the original text such as a likelihood for word occurrences. A popular example is the Word2Vec machinery of Mikolov et al. (2013). This trains the distributed representation to be useful as an input layer for prediction of words from their neighbors in a Skipgram likelihood. That is, to maximize t+bE log pV(wsj |wst) (1) jet, j=t−b summed across all words wst in all sentences ws, where b is the skip-gram window (truncated by the ends of the sentence) and pV(wsj|wst) is a neural network classifier that takes vector representations for wst and wsj as input (see Section 2). Distributed language representations have been studied since the early work on neural networks (Rumelhart et al., 1986) and have long been applied in n</context>
<context position="7671" citStr="Mikolov et al., 2013" startWordPosition="1193" endWordPosition="1196">f a specific Word2Vec model and illustrate the ideas in classification of Yelp reviews. The implementation requires only a small extension of the popular gensim python library (Rehurek and Sojka, 2010); the extended library as well as code to reproduce all of the results in this paper are available on github. In addition, the yelp data is publicly available as part of the corresponding data mining contest at kaggle.com. See github.com/taddylab/deepir fordetail. 2 Implementation Word2Vec trains V to maximize the skip-gram likelihood based on (1). We work with the Huffman softmax specification (Mikolov et al., 2013), which includes a pre-processing step to encode each vocabulary word in its representation via a binary Huffman tree (see Figure 1). Each individual probability is then L(w)−1 ( ) σ ch [η(w,j + 1)] u&gt; η(w,j)vwt (4) where η(w, i) is the ith node in the Huffman tree path, of length L(w), for word w; σ(x) = 1/(1 + exp[−x]); and ch(η) E {−1, +1} translates from whether η is a left or right child to +/- 1. Every word thus has both input and output vector coordinates, vw and [uη(w,1) · · · uη(w,L(w))]. Typically, pV(w|wt) = H j=1 46 Figure 1: Binary Huffman encoding of a 4 word vocabulary, based up</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Artificial Intelligence and Statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="1969" citStr="Morin and Bengio, 2005" startWordPosition="311" endWordPosition="314">d representation to be useful as an input layer for prediction of words from their neighbors in a Skipgram likelihood. That is, to maximize t+bE log pV(wsj |wst) (1) jet, j=t−b summed across all words wst in all sentences ws, where b is the skip-gram window (truncated by the ends of the sentence) and pV(wsj|wst) is a neural network classifier that takes vector representations for wst and wsj as input (see Section 2). Distributed language representations have been studied since the early work on neural networks (Rumelhart et al., 1986) and have long been applied in natural language processing (Morin and Bengio, 2005). The models are generating much recent interest due to the large performance gains from the newer systems, including Word2Vec and the Glove model of Pennington et al. (2014), observed in, e.g., word prediction, word analogy identification, and named entity recognition. Given the success of these new models, researchers have begun searching for ways to adapt the representations for use in document classification tasks such as sentiment prediction or author identification. One naive approach is to use aggregated word vectors across a document (e.g., a document’s average word-vector location) as</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the International Workshop on Artificial Intelligence and Statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>On Discriminative vs Generative Classifiers: A Comparison of Logistic Regression and naive Bayes.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="14968" citStr="Ng and Jordan, 2002" startWordPosition="2483" endWordPosition="2486">nt regression is consistently the strongest performer, bested only by Word2Vec inversion on task b. This is partially due to the relative strengths of discriminative (e.g., logistic regression) vs gena (NP) b (NNP) c (1-5) .099 .189 .435 .084 .200 .410 .144 .282 .496 .179 .306 .549 .148 . 284 .500 .095 .254 .480 .118 .248 .461 Table 1: Out-of-sample misclassification rates. erative (e.g., all others here) classifiers: given a large amount of training text, asymptotic efficiency of logistic regression will start to work in its favor over the finite sample advantages of a generative classifier (Ng and Jordan, 2002; Taddy, 2013c). However, the comparison is also unfair to Word2Vec and Doc2Vec: both phrase regression and MNIR are optimized exactly under AICc selected penalty, while Word and Doc 2Vec have only been approximately optimized under a single specification. The distributed representations should improve with some careful engineering. Word2Vec inversion outperforms the other document representation-based alternatives (except, by a narrow margin, MNIR in task a). Doc2Vec under DBOW specification and MNIR both do worse, but not by a large margin. In contrast to Le and Mikolov, we find here that th</context>
</contexts>
<marker>Ng, Jordan, 2002</marker>
<rawString>Andrew Y. Ng and Michael I. Jordan. 2002. On Discriminative vs Generative Classifiers: A Comparison of Logistic Regression and naive Bayes. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="2143" citStr="Pennington et al. (2014)" startWordPosition="339" endWordPosition="342">t−b summed across all words wst in all sentences ws, where b is the skip-gram window (truncated by the ends of the sentence) and pV(wsj|wst) is a neural network classifier that takes vector representations for wst and wsj as input (see Section 2). Distributed language representations have been studied since the early work on neural networks (Rumelhart et al., 1986) and have long been applied in natural language processing (Morin and Bengio, 2005). The models are generating much recent interest due to the large performance gains from the newer systems, including Word2Vec and the Glove model of Pennington et al. (2014), observed in, e.g., word prediction, word analogy identification, and named entity recognition. Given the success of these new models, researchers have begun searching for ways to adapt the representations for use in document classification tasks such as sentiment prediction or author identification. One naive approach is to use aggregated word vectors across a document (e.g., a document’s average word-vector location) as input to a standard classifier (e.g., logistic regression). However, a document is actually an ordered path of locations through IR,K, and simple averaging destroys much of </context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim Rehurek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<contexts>
<context position="7251" citStr="Rehurek and Sojka, 2010" startWordPosition="1125" endWordPosition="1128"> both Word and Doc 2Vec the performance results will change. Indeed, we find that all methods are often outperformed by phrase-count logistic regression with rare-feature up-weighting and carefully chosen regularization. However, the out-of-the-box performance of Word2Vec inversion argues for its consideration as a simple default in document classification. In the remainder, we outline classification through inversion of a specific Word2Vec model and illustrate the ideas in classification of Yelp reviews. The implementation requires only a small extension of the popular gensim python library (Rehurek and Sojka, 2010); the extended library as well as code to reproduce all of the results in this paper are available on github. In addition, the yelp data is publicly available as part of the corresponding data mining contest at kaggle.com. See github.com/taddylab/deepir fordetail. 2 Implementation Word2Vec trains V to maximize the skip-gram likelihood based on (1). We work with the Huffman softmax specification (Mikolov et al., 2013), which includes a pre-processing step to encode each vocabulary word in its representation via a binary Huffman tree (see Figure 1). Each individual probability is then L(w)−1 ( )</context>
</contexts>
<marker>Rehurek, Sojka, 2010</marker>
<rawString>Radim Rehurek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45– 50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rumelhart</author>
<author>Geoffrey Hinton</author>
<author>Ronald Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<pages>323--533</pages>
<contexts>
<context position="1886" citStr="Rumelhart et al., 1986" startWordPosition="297" endWordPosition="300">mple is the Word2Vec machinery of Mikolov et al. (2013). This trains the distributed representation to be useful as an input layer for prediction of words from their neighbors in a Skipgram likelihood. That is, to maximize t+bE log pV(wsj |wst) (1) jet, j=t−b summed across all words wst in all sentences ws, where b is the skip-gram window (truncated by the ends of the sentence) and pV(wsj|wst) is a neural network classifier that takes vector representations for wst and wsj as input (see Section 2). Distributed language representations have been studied since the early work on neural networks (Rumelhart et al., 1986) and have long been applied in natural language processing (Morin and Bengio, 2005). The models are generating much recent interest due to the large performance gains from the newer systems, including Word2Vec and the Glove model of Pennington et al. (2014), observed in, e.g., word prediction, word analogy identification, and named entity recognition. Given the success of these new models, researchers have begun searching for ways to adapt the representations for use in document classification tasks such as sentiment prediction or author identification. One naive approach is to use aggregated </context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David Rumelhart, Geoffrey Hinton, and Ronald Williams. 1986. Learning representations by backpropagating errors. Nature, 323:533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th international conference on machine learning (ICML-11),</booktitle>
<pages>129--136</pages>
<contexts>
<context position="2835" citStr="Socher et al. (2011" startWordPosition="444" endWordPosition="447">med entity recognition. Given the success of these new models, researchers have begun searching for ways to adapt the representations for use in document classification tasks such as sentiment prediction or author identification. One naive approach is to use aggregated word vectors across a document (e.g., a document’s average word-vector location) as input to a standard classifier (e.g., logistic regression). However, a document is actually an ordered path of locations through IR,K, and simple averaging destroys much of the available information. More sophisticated aggregation is proposed in Socher et al. (2011; 2013), where recursive neural networks are used to combine the word vectors through the estimated parse tree for each sentence. Alternatively, Le and Mikolov’s Doc2Vec (2014) adds document labels to the conditioning set in (1) and has them influence the skip-gram likelihood through a latent input vector location in V. In each case, the end product is a distributed representation for every sentence (or document for Doc2Vec) that can be used as input to a generic classifier. 1.1 Bayesian Inversion These approaches all add considerable model and estimation complexity to the original underlying </context>
</contexts>
<marker>Socher, Lin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Cliff C. Lin, Chris Manning, and Andrew Y. Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing (EMNLP),</booktitle>
<volume>volume</volume>
<pages>1631--1642</pages>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Taddy</author>
</authors>
<title>Measuring Political Sentiment on Twitter: Factor Optimal Design for Multinomial Inverse Regression.</title>
<date>2013</date>
<journal>Technometrics,</journal>
<volume>55</volume>
<issue>4</issue>
<contexts>
<context position="6434" citStr="Taddy (2013" startWordPosition="1011" endWordPosition="1012">a convenient top-level partitioning of the data. An efficient system could fit separate by-class language representations, which will provide for document classification as in this article as well as class-specific answers for NLP tasks such as word prediction or analogy. When one wishes to treat a document as unlabeled, NLP tasks can be answered through ensemble aggregation of the class-specific answers. Performance: We find that, in our examples, inversion of Word2Vec yields lower misclassification rates than both Doc2Vec-based classification and the multinomial inverse regression (MNIR) of Taddy (2013b). We did not anticipate such outright performance gain. Moreover, we expect that with calibration (i.e., through cross-validation) of the many various tuning parameters available when fitting both Word and Doc 2Vec the performance results will change. Indeed, we find that all methods are often outperformed by phrase-count logistic regression with rare-feature up-weighting and carefully chosen regularization. However, the out-of-the-box performance of Word2Vec inversion argues for its consideration as a simple default in document classification. In the remainder, we outline classification thr</context>
<context position="13285" citStr="Taddy (2013" startWordPosition="2212" endWordPosition="2213">eview stars. Box widths are proportional to number of observations in each class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars. each, as well as the combined 200 dimensional DM+DBOW representation, to logistic regression. Phrase regression applies logistic regression of response classes directly onto counts for short 1-2 word ‘phrases’. The phrases are obtained using gensim’s phrase builder, which simply combines highly probable pairings; e.g., first date and chicken wing are two pairings in this corpus. MNIR, the multinomial inverse regression of Taddy (2013a; 2013b; 2015) is applied as implemented in the textir package for R. MNIR maps from text to the class-space of interest through a multinomial logistic regression of phrase counts onto variables relevant to the classspace. We apply MNIR to the same set of 1-2 word phrases used in phrase regression. Here, we regress phrase counts onto stars expressed numerically and as a 5-dimensional indicator vector, leading to a 6-feature multinomial logistic regression. The MNIR procedure then uses the 6×p matrix of feature-phrase regression coefficients to map from phrase-count to feature space, resulting</context>
<context position="14981" citStr="Taddy, 2013" startWordPosition="2487" endWordPosition="2488">istently the strongest performer, bested only by Word2Vec inversion on task b. This is partially due to the relative strengths of discriminative (e.g., logistic regression) vs gena (NP) b (NNP) c (1-5) .099 .189 .435 .084 .200 .410 .144 .282 .496 .179 .306 .549 .148 . 284 .500 .095 .254 .480 .118 .248 .461 Table 1: Out-of-sample misclassification rates. erative (e.g., all others here) classifiers: given a large amount of training text, asymptotic efficiency of logistic regression will start to work in its favor over the finite sample advantages of a generative classifier (Ng and Jordan, 2002; Taddy, 2013c). However, the comparison is also unfair to Word2Vec and Doc2Vec: both phrase regression and MNIR are optimized exactly under AICc selected penalty, while Word and Doc 2Vec have only been approximately optimized under a single specification. The distributed representations should improve with some careful engineering. Word2Vec inversion outperforms the other document representation-based alternatives (except, by a narrow margin, MNIR in task a). Doc2Vec under DBOW specification and MNIR both do worse, but not by a large margin. In contrast to Le and Mikolov, we find here that the Doc2Vec DM </context>
</contexts>
<marker>Taddy, 2013</marker>
<rawString>Matt Taddy. 2013a. Measuring Political Sentiment on Twitter: Factor Optimal Design for Multinomial Inverse Regression. Technometrics, 55(4):415–425, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Taddy</author>
</authors>
<title>Multinomial Inverse Regression for Text Analysis.</title>
<date>2013</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>108--755</pages>
<contexts>
<context position="6434" citStr="Taddy (2013" startWordPosition="1011" endWordPosition="1012">a convenient top-level partitioning of the data. An efficient system could fit separate by-class language representations, which will provide for document classification as in this article as well as class-specific answers for NLP tasks such as word prediction or analogy. When one wishes to treat a document as unlabeled, NLP tasks can be answered through ensemble aggregation of the class-specific answers. Performance: We find that, in our examples, inversion of Word2Vec yields lower misclassification rates than both Doc2Vec-based classification and the multinomial inverse regression (MNIR) of Taddy (2013b). We did not anticipate such outright performance gain. Moreover, we expect that with calibration (i.e., through cross-validation) of the many various tuning parameters available when fitting both Word and Doc 2Vec the performance results will change. Indeed, we find that all methods are often outperformed by phrase-count logistic regression with rare-feature up-weighting and carefully chosen regularization. However, the out-of-the-box performance of Word2Vec inversion argues for its consideration as a simple default in document classification. In the remainder, we outline classification thr</context>
<context position="13285" citStr="Taddy (2013" startWordPosition="2212" endWordPosition="2213">eview stars. Box widths are proportional to number of observations in each class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars. each, as well as the combined 200 dimensional DM+DBOW representation, to logistic regression. Phrase regression applies logistic regression of response classes directly onto counts for short 1-2 word ‘phrases’. The phrases are obtained using gensim’s phrase builder, which simply combines highly probable pairings; e.g., first date and chicken wing are two pairings in this corpus. MNIR, the multinomial inverse regression of Taddy (2013a; 2013b; 2015) is applied as implemented in the textir package for R. MNIR maps from text to the class-space of interest through a multinomial logistic regression of phrase counts onto variables relevant to the classspace. We apply MNIR to the same set of 1-2 word phrases used in phrase regression. Here, we regress phrase counts onto stars expressed numerically and as a 5-dimensional indicator vector, leading to a 6-feature multinomial logistic regression. The MNIR procedure then uses the 6×p matrix of feature-phrase regression coefficients to map from phrase-count to feature space, resulting</context>
<context position="14981" citStr="Taddy, 2013" startWordPosition="2487" endWordPosition="2488">istently the strongest performer, bested only by Word2Vec inversion on task b. This is partially due to the relative strengths of discriminative (e.g., logistic regression) vs gena (NP) b (NNP) c (1-5) .099 .189 .435 .084 .200 .410 .144 .282 .496 .179 .306 .549 .148 . 284 .500 .095 .254 .480 .118 .248 .461 Table 1: Out-of-sample misclassification rates. erative (e.g., all others here) classifiers: given a large amount of training text, asymptotic efficiency of logistic regression will start to work in its favor over the finite sample advantages of a generative classifier (Ng and Jordan, 2002; Taddy, 2013c). However, the comparison is also unfair to Word2Vec and Doc2Vec: both phrase regression and MNIR are optimized exactly under AICc selected penalty, while Word and Doc 2Vec have only been approximately optimized under a single specification. The distributed representations should improve with some careful engineering. Word2Vec inversion outperforms the other document representation-based alternatives (except, by a narrow margin, MNIR in task a). Doc2Vec under DBOW specification and MNIR both do worse, but not by a large margin. In contrast to Le and Mikolov, we find here that the Doc2Vec DM </context>
</contexts>
<marker>Taddy, 2013</marker>
<rawString>Matt Taddy. 2013b. Multinomial Inverse Regression for Text Analysis. Journal of the American Statistical Association, 108:755–770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Taddy</author>
</authors>
<title>Rejoinder: Efficiency and structure in MNIR.</title>
<date>2013</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>108--772</pages>
<contexts>
<context position="6434" citStr="Taddy (2013" startWordPosition="1011" endWordPosition="1012">a convenient top-level partitioning of the data. An efficient system could fit separate by-class language representations, which will provide for document classification as in this article as well as class-specific answers for NLP tasks such as word prediction or analogy. When one wishes to treat a document as unlabeled, NLP tasks can be answered through ensemble aggregation of the class-specific answers. Performance: We find that, in our examples, inversion of Word2Vec yields lower misclassification rates than both Doc2Vec-based classification and the multinomial inverse regression (MNIR) of Taddy (2013b). We did not anticipate such outright performance gain. Moreover, we expect that with calibration (i.e., through cross-validation) of the many various tuning parameters available when fitting both Word and Doc 2Vec the performance results will change. Indeed, we find that all methods are often outperformed by phrase-count logistic regression with rare-feature up-weighting and carefully chosen regularization. However, the out-of-the-box performance of Word2Vec inversion argues for its consideration as a simple default in document classification. In the remainder, we outline classification thr</context>
<context position="13285" citStr="Taddy (2013" startWordPosition="2212" endWordPosition="2213">eview stars. Box widths are proportional to number of observations in each class; roughly 10% of reviews have each of 1-3 stars, while 30% have 4 stars and 40% have 5 stars. each, as well as the combined 200 dimensional DM+DBOW representation, to logistic regression. Phrase regression applies logistic regression of response classes directly onto counts for short 1-2 word ‘phrases’. The phrases are obtained using gensim’s phrase builder, which simply combines highly probable pairings; e.g., first date and chicken wing are two pairings in this corpus. MNIR, the multinomial inverse regression of Taddy (2013a; 2013b; 2015) is applied as implemented in the textir package for R. MNIR maps from text to the class-space of interest through a multinomial logistic regression of phrase counts onto variables relevant to the classspace. We apply MNIR to the same set of 1-2 word phrases used in phrase regression. Here, we regress phrase counts onto stars expressed numerically and as a 5-dimensional indicator vector, leading to a 6-feature multinomial logistic regression. The MNIR procedure then uses the 6×p matrix of feature-phrase regression coefficients to map from phrase-count to feature space, resulting</context>
<context position="14981" citStr="Taddy, 2013" startWordPosition="2487" endWordPosition="2488">istently the strongest performer, bested only by Word2Vec inversion on task b. This is partially due to the relative strengths of discriminative (e.g., logistic regression) vs gena (NP) b (NNP) c (1-5) .099 .189 .435 .084 .200 .410 .144 .282 .496 .179 .306 .549 .148 . 284 .500 .095 .254 .480 .118 .248 .461 Table 1: Out-of-sample misclassification rates. erative (e.g., all others here) classifiers: given a large amount of training text, asymptotic efficiency of logistic regression will start to work in its favor over the finite sample advantages of a generative classifier (Ng and Jordan, 2002; Taddy, 2013c). However, the comparison is also unfair to Word2Vec and Doc2Vec: both phrase regression and MNIR are optimized exactly under AICc selected penalty, while Word and Doc 2Vec have only been approximately optimized under a single specification. The distributed representations should improve with some careful engineering. Word2Vec inversion outperforms the other document representation-based alternatives (except, by a narrow margin, MNIR in task a). Doc2Vec under DBOW specification and MNIR both do worse, but not by a large margin. In contrast to Le and Mikolov, we find here that the Doc2Vec DM </context>
</contexts>
<marker>Taddy, 2013</marker>
<rawString>Matt Taddy. 2013c. Rejoinder: Efficiency and structure in MNIR. Journal of the American Statistical Association, 108:772–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Taddy</author>
</authors>
<title>One-step estimator paths for concave regularization.</title>
<date>2014</date>
<pages>1308--5623</pages>
<contexts>
<context position="11190" citStr="Taddy (2014)" startWordPosition="1816" endWordPosition="1817">ple, in task a, p(positive) = q3 + q4 + q5. Note that the same five fitted Word2Vec representations are used for each task. We consider a set of related comparator techniques. In each case, some document representation (e.g., phrase counts or Doc2Vec vectors) is used as input to logistic regression prediction of the associated review rating. The logistic regressions are fit under L1 regularization with the penalties weighted by feature standard deviation (which, e.g., up-weights rare phrases) and selected according to the corrected AICc criteria (Flynn et al., 2013) via the gamlr R package of Taddy (2014). For multi-class tasks b-c, we use distributed Multinomial regression (DMR; Taddy 2015) via the distrom R package. DMR fits multinomial logistic regression in a factorized representation wherein one estimates independent Poisson linear models for each response category. Document representations and logistic regressions are always trained using only the training corpus. Doc2Vec is also fit via gensim, using the same latent space specification as for Word2Vec: K = 100 and b = 5. As recommended in the documentation, we apply repeated SGD over 20 reorderings of each corpus (for comparability, thi</context>
</contexts>
<marker>Taddy, 2014</marker>
<rawString>Matt Taddy. 2014. One-step estimator paths for concave regularization. arXiv:1308.5623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Taddy</author>
</authors>
<title>Distributed Multinomial Regression. Annals of Applied Statistics,</title>
<date>2015</date>
<note>To appear.</note>
<contexts>
<context position="11278" citStr="Taddy 2015" startWordPosition="1828" endWordPosition="1829">sentations are used for each task. We consider a set of related comparator techniques. In each case, some document representation (e.g., phrase counts or Doc2Vec vectors) is used as input to logistic regression prediction of the associated review rating. The logistic regressions are fit under L1 regularization with the penalties weighted by feature standard deviation (which, e.g., up-weights rare phrases) and selected according to the corrected AICc criteria (Flynn et al., 2013) via the gamlr R package of Taddy (2014). For multi-class tasks b-c, we use distributed Multinomial regression (DMR; Taddy 2015) via the distrom R package. DMR fits multinomial logistic regression in a factorized representation wherein one estimates independent Poisson linear models for each response category. Document representations and logistic regressions are always trained using only the training corpus. Doc2Vec is also fit via gensim, using the same latent space specification as for Word2Vec: K = 100 and b = 5. As recommended in the documentation, we apply repeated SGD over 20 reorderings of each corpus (for comparability, this was also done when fitting Word2Vec). Le and Mikolov provide two alternative Doc2Vec s</context>
</contexts>
<marker>Taddy, 2015</marker>
<rawString>Matt Taddy. 2015. Distributed Multinomial Regression. Annals of Applied Statistics, To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>