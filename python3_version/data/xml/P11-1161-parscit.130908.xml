<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000080">
<title confidence="0.818911">
Ranking Class Labels Using Query Sessions
</title>
<author confidence="0.941822">
Marius Pas¸ca
</author>
<affiliation confidence="0.922664">
Google Inc.
</affiliation>
<address confidence="0.7365395">
1600 Amphitheatre Parkway
Mountain View, California 94043
</address>
<email confidence="0.998768">
mars@google.com
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990618125">
The role of search queries, as available within
query sessions or in isolation from one an-
other, in examined in the context of ranking
the class labels (e.g., brazilian cities, busi-
ness centers, hilly sites) extracted from Web
documents for various instances (e.g., rio de
janeiro). The co-occurrence of a class la-
bel and an instance, in the same query or
within the same query session, is used to re-
inforce the estimated relevance of the class la-
bel for the instance. Experiments over eval-
uation sets of instances associated with Web
search queries illustrate the higher quality of
the query-based, re-ranked class labels, rel-
ative to ranking baselines using document-
based counts.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936489795919">
Motivation: The offline acquisition of instances (rio
de janeiro, porsche cayman) and their correspond-
ing class labels (brazilian cities, locations, vehicles,
sports cars) from text has been an active area of re-
search. In order to extract fine-grained classes of
instances, existing methods often apply manually-
created (Banko et al., 2007; Talukdar et al., 2008) or
automatically-learned (Snow et al., 2006) extraction
patterns to text within large document collections.
In Web search, the relative ranking of documents
returned in response to a query directly affects the
outcome of the search. Similarly, the quality of
the relative ranking among class labels extracted for
a given instance influences any applications (e.g.,
query refinements or structured extraction) using the
extracted data. But due to noise in Web data and
limitations of extraction techniques, class labels ac-
quired for a given instance (e.g., oil shale) may fail
to properly capture the semantic classes to which the
instance may belong (Kozareva et al., 2008). In-
evitably, some of the extracted class labels will be
less useful (e.g., sources, mutual concerns) or incor-
rect (e.g., plants for the instance oil shale). In pre-
vious work, the relative ranking of class labels for
an instance is determined mostly based on features
derived from the source Web documents from which
the data has been extracted, such as variations of the
frequency of co-occurrence or diversity of extraction
patterns producing a given pair (Etzioni et al., 2005).
Contributions: This paper explores the role of
Web search queries, rather than Web documents, in
inducing superior ranking among class labels ex-
tracted automatically from documents for various in-
stances. It compares two sources of indirect ranking
evidence available within anonymized query logs:
a) co-occurrence of an instance and its class label
in the same query; and b) co-occurrence of an in-
stance and its class label, as separate queries within
the same query session. The former source is a noisy
attempt to capture queries that narrow the search re-
sults to a particular class of the instance (e.g., jaguar
car maker). In comparison, the latter source nois-
ily identifies searches that specialize from a class
(e.g., car maker) to an instance (e.g., jaguar) or,
conversely, generalize from an instance to a class.
To our knowledge, this is the first study comparing
inherently-noisy queries and query sessions for the
purpose of ranking of open-domain, labeled class in-
stances.
</bodyText>
<page confidence="0.944089">
1607
</page>
<note confidence="0.97928">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1607–1615,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999378363636364">
The remainder of the paper is organized as fol-
lows. Section 2 introduces intuitions behind an
approach using queries for ranking class labels of
various instances, and describes associated ranking
functions. Sections 3 and 4 describe the experi-
mental setting and evaluation results over evaluation
sets of instances associated with Web search queries.
The results illustrate the higher quality of the query-
based, re-ranked lists of class labels, relative to alter-
native ranking methods using only document-based
counts.
</bodyText>
<sectionHeader confidence="0.997405" genericHeader="method">
2 Instance Class Ranking via Query Logs
</sectionHeader>
<bodyText confidence="0.997343142857143">
Ranking Hypotheses: We take advantage of
anonymized query logs, to induce superior ranking
among the class labels associated with various class
instances within an IsA repository acquired from
Web documents. Given a class instance I, the func-
tions used for the ranking of its class labels are cho-
sen following several observations.
</bodyText>
<listItem confidence="0.99941775">
• Hypothesis H1: If C is a prominent class of an
instance I, then C and I are likely to occur in text in
contexts that are indicative of an IsA relation.
• Hypothesis H2: If C is a prominent class of an
instance I, and I is ambiguous, then a fraction of
the queries about I may also refer to and contain C.
• Hypothesis H3: If C is a prominent class of an
instance I, then a fraction of the queries about I
may be followed by queries about C, and vice-versa.
Ranking Functions: The ranking functions follow
directly from the above hypotheses.
• Ranking based on H1 (using documents): The
</listItem>
<bodyText confidence="0.8192634">
first hypothesis H1 is a reformulation of findings
from previous work (Etzioni et al., 2005). In prac-
tice, a class label is deemed more relevant for an in-
stance if the pair is extracted more frequently and by
multiple patterns, with the scoring formula:
</bodyText>
<equation confidence="0.980484">
Scorex1(C, I) = Freq(C,I) x Size({Pattern(C)})2 (1)
</equation>
<bodyText confidence="0.9712450625">
where Freq(C,I) is the frequency of extraction of
C for the instance I, and Size({Pattern(C)}) is the
number of unique patterns extracting the class label
C for the instance I. The patterns are hand-written,
following (Hearst, 1992):
ff..]
where I is a potential instance (e.g., diderot) and C
is a potential class label (e.g., writers). The bound-
aries are approximated from the part-of-speech tags
of the sentence words, for potential class labels C;
and identified by checking that I occurs as an entire
query in query logs, for instances I (Van Durme and
Pas¸ca, 2008).
The application of the scoring formula (1) to can-
didates extracted from the Web produces a ranked
list of class labels LH1(I).
</bodyText>
<listItem confidence="0.751971">
• Ranking based on H2 (using queries): Intu-
itively, Web users searching for information about
I sometimes add some or all terms of C to a search
query already containing I, either to further spec-
ify their query, or in response to being presented
with sets of search results spanning several mean-
ings of an ambiguous instance. Examples of such
queries are happiness emotion and diderot philoso-
pher. Moreover, queries like happiness positive psy-
chology and diderot enlightenment may be consid-
ered to weakly and partially reinforce the relevance
of the class labels positive emotions and enlighten-
ment writers of the instances happiness and diderot
respectively. In practice, a class label is deemed
more relevant if its individual terms occur in pop-
ular queries containing the instance. More precisely,
for each term within any class label from LH1(I),
we compute a score TermQueryScore. The score is
the frequency sum of the term within anonymized
queries containing the instance I as a prefix, and
the term anywhere else in the queries. Terms are
stemmed before the computation.
</listItem>
<bodyText confidence="0.969813666666667">
Each class label C is assigned the geometric mean
of the scores of its N terms T�, after ignoring stop
words:
</bodyText>
<equation confidence="0.990333">
TermQueryScore(T))1/N (2)
</equation>
<bodyText confidence="0.924373">
The geometric mean is preferred to the arithmetic
mean, because the latter is more strongly affected by
outlier values. The class labels are ranked according
to the means, resulting in a ranked list LH2(I). In
case of ties, LH2(I) keeps the relative ranking from
LH1(I).
• Ranking based on H3 (using query sessions):
Given the third hypothesis H3, Web users searching
for information about I may subsequently search for
more general information about one of its classes C.
Conversely, users may specialize their search from
a class C to one of its instances I. Examples of
such queries are happiness followed later by emo-
tions, or diderot followed by philosophers; or emo-
</bodyText>
<equation confidence="0.989092">
C [such as|including] I [and|,|.]),
Scorex2(C, I) = (
N
</equation>
<page confidence="0.9022725">
11
1608
</page>
<bodyText confidence="0.99994094117647">
tions followed later by happiness, or philosophers
followed by diderot. In practice, a class label is
deemed more relevant if its individual terms occur as
part of queries that are in the same query session as a
query containing only the instance. More precisely,
for each term within any class label from LH1(I),
we compute a score TermSessionScore, equal to the
frequency sum of the anonymized queries from the
query sessions that contain the term and are: a) ei-
ther the initial query of the session, with the instance
I being one of the subsequent queries from the same
session; or b) one of the subsequent queries of the
session, with the instance I being the initial query
of the same session. Before computing the frequen-
cies, the class label terms are stemmed.
Each class label C is assigned the geometric mean
of the scores of its terms, after ignoring stop words:
</bodyText>
<equation confidence="0.992341">
TermSessionScore(T))1/N (3)
</equation>
<bodyText confidence="0.999035076923077">
The class labels are ranked according to the geo-
metric means, resulting in a ranked list LH3(I). In
case of ties, LH3(I) preserves the relative ranking
from LH1(I).
Unsupervised Ranking: Given an instance I, the
ranking hypotheses and corresponding functions
LH1(I), LH2(I) and LH3(I) (or any combination
of them) can be used together to generate a merged,
ranked list of class labels per instance I. The score
of a class label in the merged list is determined by
the inverse of the average rank in the lists LH1(I)
and LH2(I) and LH3(I), computed with the follow-
ing formula:
</bodyText>
<equation confidence="0.997042">
ScoreH1+H2+H3 (C, Z) = N N (4)
�i Rank(C, LHi)
</equation>
<bodyText confidence="0.999949590909091">
where N is the number of input lists of class labels
(in this case, 3), and Rank(C, LHZ) is the rank of C
in the input list of class labels LHZ (LH1, LH2 or
LH3). The rank is set to 1000, if C is not present in
the list LHZ. By using only the relative ranks and not
the absolute scores of the class labels within the in-
put lists, the outcome of the merging is less sensitive
to how class labels of a given instance are numeri-
cally scored within the input lists. In case of ties,
the scores of the class labels from LH1(I) serve as a
secondary ranking criterion. Thus, every instance I
from the IsA repository is associated with a ranked
list of class labels computed according to this rank-
ing formula. Conversely, each class label C from
the IsA repository is associated with a ranked list
of class instances computed with the earlier scoring
formula (1) used to generate lists LH1(I).
Note that the ranking formula can also consider
only a subset of the available input lists. For in-
stance, ScoreH1+H2 would use only LH1(I) and
LH2(I) as input lists; ScoreH1+H3 would use only
LH1(I) and LH3(I) as input lists; etc.
</bodyText>
<sectionHeader confidence="0.993814" genericHeader="method">
3 Experimental Setting
</sectionHeader>
<bodyText confidence="0.999376285714286">
Textual Data Sources: The acquisition of the
IsA repository relies on unstructured text available
within Web documents and search queries. The
queries are fully-anonymized queries in English sub-
mitted to Google by Web users in 2009, and are
available in two collections. The first collection is
a random sample of 50 million unique queries that
are independent from one another. The second col-
lection is a random sample of 5 million query ses-
sions. Each session has an initial query and a se-
ries of subsequent queries. A subsequent query is a
query that has been submitted by the same Web user
within no longer than a few minutes after the initial
query. Each subsequent query is accompanied by
its frequency of occurrence in the session, with the
corresponding initial query. The document collec-
tion consists of a sample of 100 million documents
in English.
Experimental Runs: The experimental runs corre-
spond to different methods for extracting and rank-
ing pairs of an instance and a class:
</bodyText>
<listItem confidence="0.998504375">
• from the repository extracted here, with class
labels of an instance ranked based on the frequency
and the number of extraction patterns (ScoreH1
from Equation (1) in Section 2), in run Rd;
• from the repository extracted here, with class
labels of an instance ranked via the rank-based
merging of: ScoreH1+H2 from Section 2, in run
Rp, which corresponds to re-ranking using co-
occurrence of an instance and its class label in
the same query; ScoreH1+H3 from Section 2, in
run R3, which corresponds to re-ranking using co-
occurrence of an instance and its class label, as sep-
arate queries within the same query session; and
ScoreH1+H2+H3 from Section 2, in run R, which
corresponds to re-ranking using both types of co-
occurrences in queries.
</listItem>
<equation confidence="0.7706702">
ScoreH3(C, Z) = (
��
N77
11
ri
</equation>
<page confidence="0.95225">
1609
</page>
<bodyText confidence="0.999798">
Evaluation Procedure: The manual evaluation of
open-domain information extraction output is time
consuming (Banko et al., 2007). A more practi-
cal alternative is an automatic evaluation procedure
for ranked lists of class labels, based on existing re-
sources and systems.
Assume that there is a gold standard, containing
gold class labels that are each associated with a gold
set of their instances. The creation of such gold stan-
dards is discussed later. Based on the gold standard,
the ranked lists of class labels available within an
IsA repository can be automatically evaluated as fol-
lows. First, for each gold label, the ranked lists of
class labels of individual gold instances are retrieved
from the IsA repository. Second, the individual re-
trieved lists are merged into a ranked list of class
labels, associated with the gold label. The merged
list can be computed, e.g., using an extension of the
ScoreH1+H2+H3 formula (Equation (4)) described
earlier in Section 2. Third, the merged list is com-
pared against the gold label, to estimate the accu-
racy of the merged list. Intuitively, a ranked list of
class labels is a better approximation of a gold label,
if class labels situated at better ranks in the list are
closer in meaning to the gold label.
Evaluation Metric: Given a gold label and a list of
class labels, if any, derived from the IsA repository,
the rank of the highest class label that matches the
gold label determines the score assigned to the gold
label, in the form of the reciprocal rank of the match.
Thus, if the gold label matches a class label at rank
1, 2 or 3 in the computed list, the gold label receives
a score of 1, 0.5 or 0.33 respectively. The score is
0 if the gold label does not match any of the top 20
class labels. The overall score over the entire set of
gold labels is the mean reciprocal rank (MRR) score
over all gold labels from the set. Two types of MRR
scores are automatically computed:
</bodyText>
<listItem confidence="0.543552090909091">
• MRRf considers a gold label and a class label
to match, if they are identical;
• MRRp considers a gold label and a class label
to match, if one or more of their tokens that are not
stop words are identical.
During matching, all string comparisons are case-
insensitive, and all tokens are first converted to their
singular form (e.g., european countries to european
country) using WordNet (Fellbaum, 1998). Thus, in-
surance carriers and insurance companies are con-
Query Set: Sample of Queries
</listItem>
<bodyText confidence="0.972745230769231">
Qe (807 queries): 2009 movies, amino acids, asian
countries, bank, board games, buildings, capitals,
chemical functional groups, clothes, computer lan-
guage, dairy farms near modesto ca, disease, egyp-
tian pharaohs, eu countries, fetishes, french presidents,
german islands, hawaiian islands, illegal drugs, irc
clients, lakes, macintosh models, mobile operator in-
dia, nba players, nobel prize winners, orchids, photo
editors, programming languages, renaissance artists,
roller costers, science fiction tv series, slr cameras,
soul singers, states of india, taliban members, thomas
edison inventions, u.s. presidents, us president, water
slides
</bodyText>
<tableCaption confidence="0.700749538461539">
Q,,,, (40 queries): actors, actresses, airlines, ameri-
can presidents, antibiotics, birds, cars, celebrities, col-
ors, computer languages, digital camera, dog breeds,
dogs, drugs, elements, endangered animals, european
countries, flowers, fruits, greek gods, horror movies,
idioms, ipods, movies, names, netbooks, operating
systems, park slope restaurants, planets, presidents,
ps3 games, religions, renaissance artists, rock bands,
romantic movies, states, universities, university, us
cities, vitamins
Table 1: Size and composition of evaluation sets of
queries associated with non-filtered (Qe) or manually-
filtered (Q,..) instances
</tableCaption>
<bodyText confidence="0.999870375">
sidered to not match in MRRf scores, but match in
MRRp scores. On the other hand, MRRp scores may
give credit to less relevant class labels, such as insur-
ance policies for the gold label insurance carriers.
Therefore, MRRp is an optimistic, and MRRf is a
pessimistic estimate of the actual usefulness of the
computed ranked lists of class labels as approxima-
tions of the gold labels.
</bodyText>
<sectionHeader confidence="0.999409" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.995968454545455">
IsA Repository: The IsA repository, extracted from
the document collection, covers a total of 4.04 mil-
lion instances associated with 7.65 million class la-
bels. The number of class labels available per in-
stance and vice-versa follows a long-tail distribu-
tion, indicating that 2.12 million of the instances
each have two or more class labels (with an average
of 19.72 class labels per instance).
Evaluation Sets of Queries: Table 1 shows sam-
ples of two query sets, introduced in (Pas¸ca, 2010)
and used in the evaluation. The first set, denoted Qe,
</bodyText>
<page confidence="0.948902">
1610
</page>
<table confidence="0.942254714285714">
Query Set Min Max Avg Median
Number of Gold Instances:
Qe 10 100 70.4 81
Qm 8 33 16.9 17
Number of Query Tokens:
Qe 1 8 2.0 2
Qm 1 3 1.4 1
</table>
<tableCaption confidence="0.9669984">
Table 2: Number of gold instances (upper part) and num-
ber of query tokens (lower part) available per query, over
the evaluation sets of queries associated with non-filtered
gold instances (Qe) or manually-filtered gold instances
(Qm)
</tableCaption>
<bodyText confidence="0.999781371428571">
is obtained from a random sample of anonymized,
class-seeking queries submitted by Web users to
Google Squared. The set contains 807 queries, each
associated with a ranked list of between 10 and 100
gold instances automatically extracted by Google
Squared.
Since the gold instances available as input for
each query as part of Qe are automatically extracted,
they may or may not be true instances of the respec-
tive queries. As described in (Pas¸ca, 2010), the sec-
ond evaluation set Qm is a subset of 40 queries from
Qe, such that the gold instances available for each
query in Qm are found to be correct after manual
inspection. The 40 queries from Qm are associated
with between 8 and 33 human-validated instances.
As shown in the upper part of Table 2, the queries
from Qe are up to 8 tokens in length, with an average
of 2 tokens per query. Queries from Qm are com-
paratively shorter, both in maximum (3 tokens) and
average (1.4 tokens) length. The lower part of Ta-
ble 2 shows the number of gold instances available
as input, which average around 70 and 17 per query,
for queries from Qe and Qm respectively. To provide
another view on the distribution of the queries from
evaluation sets, Table 3 lists tokens that are not stop
words, which occur in most queries from Qe. Com-
paratively, few query tokens occur in more than one
query in Qm.
Evaluation Procedure: Following the general eval-
uation procedure, each query from the sets Qe and
Qm acts as a gold class label associated with the
corresponding set of instances. Given a query and
its instances Z from the evaluation sets Qe or Qm,
a merged, ranked lists of class labels is computed
out of the ranked lists of class labels available in the
</bodyText>
<figureCaption confidence="0.997605678571429">
Query Cnt. Examples of Queries Containing
Token the Token
countries 22 african countries, eu countries,
poor countries
cities 21 australian cities, cities in califor-
nia, greek cities
presidents 18 american presidents, korean
presidents, presidents of the
south korea
restaurants 15 atlanta restaurants, nova scotia
restaurants, restaurants 10024
companies 14 agriculture companies, gas util-
ity companies, retail companies
states 14 american states, states of india,
united states national parks
prime 11 australian prime ministers, in-
dian prime ministers, prime min-
isters
cameras 10 cameras, digital cameras olym-
pus, nikon cameras
movies 10 2009 movies, movies, romantic
movies
american 9 american authors, american
president, american revolution
battles
ministers 9 australian prime ministers, in-
dian prime ministers, prime min-
isters
</figureCaption>
<tableCaption confidence="0.706248">
Table 3: Query tokens occurring most frequently in
queries from the Qe evaluation set, along with the number
(Cnt) and examples of queries containing the tokens
</tableCaption>
<bodyText confidence="0.999927705882353">
underlying IsA repository for each instance Z. The
evaluation compares the merged lists of class labels,
with the corresponding queries from Qe or Qm.
Accuracy of Lists of Class Labels: Table 4 summa-
rizes results from comparative experiments, quanti-
fying a) horizontally, the impact of alternative pa-
rameter settings on the computed lists of class la-
bels; and b) vertically, the comparative accuracy of
the experimental runs over the query sets. The ex-
perimental parameters are the number of input in-
stances from the evaluation sets that are used for re-
trieving class labels, I-per-Q, set to 3, 5, 10; and the
number of class labels retrieved per input instance,
C-per-I, set to 5, 10, 20.
Four conclusions can be derived from the results.
First, the scores over Qm are higher than those over
Qe, confirming the intuition that the higher-quality
</bodyText>
<page confidence="0.970398">
1611
</page>
<table confidence="0.991981782608696">
Accuracy
I-per-Q 3 5 10
C-per-I 5 10 20 5 10 20 5 10 20
MRRf computed over Qe:
Rd 0.186 0.195 0.198 0.198 0.207 0.210 0.204 0.214 0.218
Rp 0.202 0.211 0.216 0.232 0.238 0.244 0.245 0.255 0.257
Rs 0.258 0.260 0.261 0.278 0.277 0.276 0.279 0.280 0.282
Ru 0.234 0.241 0.244 0.260 0.263 0.270 0.274 0.275 0.278
MRRp computed over Qe:
Rd 0.489 0.495 0.495 0.517 0.528 0.529 0.541 0.553 0.557
Rp 0.520 0.531 0.533 0.564 0.573 0.578 0.590 0.601 0.602
Rs 0.576 0.584 0.583 0.612 0.616 0.614 0.641 0.636 0.628
Ru 0.561 0.570 0.571 0.606 0.614 0.617 0.640 0.641 0.636
MRRf computed over Qm:
Rd 0.406 0.436 0.442 0.431 0.447 0.466 0.467 0.470 0.501
Rp 0.423 0.426 0.429 0.436 0.483 0.508 0.500 0.526 0.530
Rs 0.590 0.601 0.594 0.578 0.604 0.595 0.624 0.612 0.624
Ru 0.481 0.502 0.508 0.531 0.539 0.545 0.572 0.588 0.575
MRRp computed over Qm:
Rd 0.667 0.662 0.660 0.675 0.677 0.699 0.702 0.695 0.716
Rp 0.711 0.703 0.680 0.734 0.731 0.748 0.733 0.797 0.782
Rs 0.841 0.822 0.820 0.835 0.828 0.823 0.850 0.856 0.844
Ru 0.800 0.810 0.781 0.795 0.794 0.779 0.806 0.827 0.816
</table>
<tableCaption confidence="0.988427">
Table 4: Accuracy of instance set labeling, as full-match (MRRf) or partial-match (MRRp) scores over the evaluation
sets of queries associated with non-filtered instances (Qe) or manually-filtered instances (Qm), for various experi-
mental runs (I-per-Q=number of gold instances available in the input evaluation sets that are used for retrieving class
labels; C-per-I=number of class labels retrieved from IsA repository per input instance)
</tableCaption>
<bodyText confidence="0.999938857142857">
input set of instances available in Q„t relative to
Qe should lead to higher-quality class labels for
the corresponding queries. Second, when I-per-Q
is fixed, increasing C-per-I leads to small, if any,
score improvements. Third, when C-per-I is fixed,
even small values of I-per-Q, such as 3 (that is, very
small sets of instances provided as input) produce
scores that are competitive with those obtained with
a higher value like 10. This suggests that useful class
labels can be generated even in extreme scenarios,
where the number of instances available as input is
as small as 3 or 5. Fourth and most importantly, for
most combinations ofparameter settings and on both
query sets, the runs that take advantage of query logs
(Rp, R3, R,,) produce the highest scores. In particu-
lar, when I-per-Q is set to 10 and C-per-I to 20, run
R,, identifies the original query as an exact match
among the top three to four class labels returned
(score 0.278); and as a partial match among the top
one to two class labels returned (score 0.636), as an
average over the Qe set. The corresponding MRRf
score of 0.278 over the Qe set obtained with run R,&apos;
is 27% higher than with run Rd.
In all experiments, the higher scores of Rp, R3 and
R,, can be attributed to higher-quality lists of class
labels, relative to Rd. Among combinations of pa-
rameter settings described in Table 4, values around
10 for I-per-Q and 20 for C-per-I give the highest
scores over both Qe and Q„t.
Among the query-based runs Rp, R3 and R,,, the
highest scores in Table 4 are obtained mostly for run
R3. Thus, between the presence of a class label and
an instance either in the same query, or as separate
queries within the same query session, it is the lat-
ter that provides a more useful signal during the re-
ranking of class labels of each instance.
Table 5 illustrates the top class labels from the
ranked lists generated in run R3 for various queries
from both Qe and Q„t. The table suggests that the
computed class labels are relatively resistant to noise
and variation within the input set of gold instances.
For example, the top elements of the lists of class la-
</bodyText>
<page confidence="0.971009">
1612
</page>
<table confidence="0.999574914285714">
Query Query Gold Instances Top Labels Generated Using Top 10 Gold In-
Set stances
Cnt. Sample from Top Gold In-
stances
actors Qe 100 abe vigoda, ben kingsley, bill actors, stars, favorite actors, celebrities, movie
hickman stars
Q,,,, 28 al pacino, christopher actors, celebrities, favorite actors, movie stars,
walken, danny devito stars
computer Qe 59 acm transactions on math- languages, programming languages, programs,
languages ematical software, apple- standard programming languages, computer pro-
script, c gramming languages
Q,,,, 17 applescript, eiffel, haskell languages, programming languages, computer
languages, modern programming languages,
high-level languages
european Qe 60 abkhazia, armenia, bosnia &amp; countries, european countries, eu countries, for-
countries herzegovina eign countries, western countries
Q,,,, 19 belgium, finland, greece countries, european countries, eu countries, for-
eign countries, western countries
endangered Qe 98 arkive, arabian oryx, species, animals, endangered species, animal
animals bagheera species, endangered animals
Q,,,, 21 arabian oryx, blue whale, gi- animals, endangered species, species, endan-
ant hispaniolan galliwasp gered animals, rare animals
park slope Qe 100 12th street bar &amp; grill, aji bar businesses, departments
restaurants lounge, anthony’s
Q,,,, 18 200 fifth restaurant bar, ap- (none)
plewood restaurant, beet thai
restaurant
renaissance Qe 95 michele da verona, andrea artists, famous artists, great artists, renaissance
artists sansovino, andrea del sarto artists, italian artists
Q,,,, 11 botticelli, filippo lippi, gior- artists, famous artists, renaissance artists, great
gione artists, italian artists
rock bands Qe 65 blood doll, nightmare, rock- songs, hits, films, novels, famous songs
away beach
Q,,,, 15 arcade fire, faith no more, in- bands, rock bands, favorite bands, great bands,
digo girls groups
</table>
<tableCaption confidence="0.990543666666667">
Table 5: Examples of gold instances available in the input, and actual ranked lists of class labels produced by run R3 for
various queries from the evaluation sets of queries associated with non-filtered gold instances (Qe) or manually-filtered
gold instances (Q,,,,)
</tableCaption>
<bodyText confidence="0.999694166666667">
bels generated for computer languages are relevant
and also quite similar for Qe vs. Qm, although the
list of gold instances in Qe may contain incorrect
items (e.g., acm transactions on mathematical soft-
ware). Similarly, the class labels computed for eu-
ropean countries are almost the same for Qe vs. Qm,
although the overlap of the respective lists of 10 gold
instances used as input is not large. The table shows
at least one query (park slope restaurants) for which
the output is less than optimal, either because the
class labels (e.g., businesses) are quite distant se-
mantically from the query (for Qe), or because no
output is produced at all, due to no class labels being
found in the IsA repository for any of the 10 input
gold instances (for Qm). For many queries, how-
ever, the computed class labels arguably capture the
meaning of the original query, although not neces-
sarily in the exact same lexical form, and sometimes
only partially. For example, for the query endan-
gered animals, only the fourth class label from Qm
identifies the query exactly. However, class labels
preceding endangered animals already capture the
notion of animals or species (first and third labels),
or that they are endangered (second label).
</bodyText>
<page confidence="0.971007">
1613
</page>
<figure confidence="0.7817345">
Query evaluation set: Qe
Query evaluation set: Qm
</figure>
<figureCaption confidence="0.931988">
Figure 1: Percentage of queries from the evaluation sets,
for which the earliest class labels from the computed
ranked lists of class labels, which match the queries, oc-
cur at various ranks in the ranked lists returned by run
R3
</figureCaption>
<bodyText confidence="0.998678512820513">
Figure 1 provides a detailed view on the distribu-
tion of queries from the Qe and Qm evaluation sets,
for which the class label that matches the query oc-
curs at a particular rank in the computed list of class
labels. In the first graph of Figure 1, for Qe, the
query matches the automatically-generated class la-
bel at ranks 1, 2, 3, 4 and 5 for 18.9%, 10.3%, 5.7%,
3.7% and 1.2% of the queries respectively, with full
string matching, i.e., corresponding to MRRf; and
for 52.6%, 12.4%, 5.3%, 3.7% and 1.7% respec-
tively, with partial string matching, corresponding to
MRRp. The second graph confirms that higher MRR
scores are obtained for Qm than for Qe. In particu-
lar, the query matches the class label at rank 1 and 2
for 50.0% and 17.5% (or a combined 67.5%) of the
queries from Qm, with full string matching; and for
52.6% and 12.4% (or a combined 67%), with partial
string matching.
Discussion: The quality of lists of items extracted
from documents can benefit from query-driven rank-
ing, particularly for the task of ranking class labels
of instances within IsA repositories. The use of
queries for ranking is generally applicable: it can
be seen as a post-processing stage that enhances the
ranking of the class labels extracted for various in-
stances by any method into any IsA repository.
Open-domain class labels extracted from text and
re-ranked as described in this paper are useful in a
variety of applications. Search tools such as Google
Squared return a set of instances, in response to
class-seeking queries (e.g., insurance companies).
The labeling of the returned set of instances, using
the re-ranked class labels available per instances, al-
lows for the generation of query refinements (e.g.,
insurers). In search over semi-structured data (Ca-
farella et al., 2008), the labeling of column cells is
useful to infer the semantics of a table column, when
the subject row of the table in which the column ap-
pears is either absent or difficult to detect.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999982913043478">
The role of anonymized query logs in Web-based
information extraction has been explored in tasks
such as class attribute extraction (Pas¸ca and Van
Durme, 2007), instance set expansion (Pennacchiotti
and Pantel, 2009) and extraction of sets of similar
entities (Jain and Pennacchiotti, 2010). Our work
compares the usefulness of queries and query ses-
sions for ranking class labels in extracted IsA repos-
itories. It shows that query sessions produce better-
ranked class labels than isolated queries do. A task
complementary to class label ranking is entity rank-
ing (Billerbeck et al., 2010), also referred to as rank-
ing for typed search (Demartini et al., 2009).
The choice of search queries and query substitu-
tions is often influenced by, and indicative of, vari-
ous semantic relations holding among full queries or
query terms (Jones et al., 2006). Semantic relations
may be loosely defined, e.g., by exploring the ac-
quisition of untyped, similarity-based relations from
query logs (Baeza-Yates and Tiberi, 2007). In com-
parison, queries are used here to re-rank class labels
capturing a well-defined type of open-domain rela-
tions, namely IsA relations.
</bodyText>
<sectionHeader confidence="0.999594" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9962405">
In an attempt to bridge the gap between informa-
tion stated in documents and information requested
</bodyText>
<figure confidence="0.998840027027027">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Rank
(not in
top 20)
Percentage of queries
64.000
32.000
16.000
4.000
2.000
0.500
0.250
0.125
0.062
8.000
1.000
Full-match
Partial-match
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Rank
(not in
top 20)
Percentage of queries
64.000
32.000
16.000
4.000
2.000
0.500
0.250
0.125
0.062
8.000
1.000
Full-match
Partial-match
</figure>
<page confidence="0.990238">
1614
</page>
<bodyText confidence="0.999971714285714">
in search queries, this study shows that inherently-
noisy queries are useful in re-ranking class labels ex-
tracted from Web documents for various instances,
with query sessions leading to higher quality than
isolated queries. Current work investigates the im-
pact of ambiguous input instances (Vyas and Pantel,
2009) on the quality of the generated class labels.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999724827586207">
R. Baeza-Yates and A. Tiberi. 2007. Extracting semantic
relations from query logs. In Proceedings of the 13th
ACM Conference on Knowledge Discovery and Data
Mining (KDD-07), pages 76–85, San Jose, California.
M. Banko, Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information ex-
traction from the Web. In Proceedings of the 20th In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-07), pages 2670–2676, Hyderabad, India.
B. Billerbeck, G. Demartini, C. Firan, T. Iofciu, and
R. Krestel. 2010. Ranking entities using Web search
query logs. In Proceedings of the 14th European
Conference on Research and Advanced Technologyfor
Digital Libraries (ECDL-10), pages 273–281, Glas-
gow, Scotland.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. WebTables: Exploring the power of tables on
the Web. In Proceedings of the 34th Conference on
Very Large Data Bases (VLDB-08), pages 538–549,
Auckland, New Zealand.
G. Demartini, T. Iofciu, and A. de Vries. 2009. Overview
of the INEX 2009 Entity Ranking track. In INitiative
for the Evaluation of XML Retrieval Workshop, pages
254–264, Brisbane, Australia.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
Web: an experimental study. Artificial Intelligence,
165(1):91–134.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539–545, Nantes, France.
A. Jain and M. Pennacchiotti. 2010. Open entity ex-
traction from Web search query logs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (COLING-10), pages 510–518,
Beijing, China.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15h
World Wide Web Conference (WWW-06), pages 387–
396, Edinburgh, Scotland.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
class learning from the Web with hyponym pattern
linkage graphs. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL-08), pages 1048–1056, Columbus, Ohio.
M. Pas¸ca and B. Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from
query logs. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI-07),
pages 2832–2837, Hyderabad, India.
M. Pas¸ca. 2010. The role of queries in ranking la-
beled instances extracted from text. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING-10), pages 955–962, Bei-
jing, China.
M. Pennacchiotti and P. Pantel. 2009. Entity extrac-
tion via ensemble semantics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP-09), pages 238–247,
Singapore.
R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics (COLING-
ACL-06), pages 801–808, Sydney, Australia.
P. Talukdar, J. Reisinger, M. Pas¸ca, D. Ravichandran,
R. Bhagat, and F. Pereira. 2008. Weakly-supervised
acquisition of labeled class instances using graph ran-
dom walks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-08), pages 582–590, Honolulu, Hawaii.
B. Van Durme and M. Pas¸ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. In Proceedings of the 23rd National Confer-
ence on Artificial Intelligence (AAAI-08), pages 1243–
1248, Chicago, Illinois.
V. Vyas and P. Pantel. 2009. Semi-automatic entity set
refinement. In Proceedings of the 2009 Conference
of the North American Association for Computational
Linguistics (NAACL-HLT-09), pages 290–298, Boul-
der, Colorado.
</reference>
<page confidence="0.992394">
1615
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.289471">
<title confidence="0.7022375">Ranking Class Labels Using Query Sessions Google</title>
<address confidence="0.883149">1600 Amphitheatre Mountain View, California</address>
<email confidence="0.999793">mars@google.com</email>
<abstract confidence="0.996757647058824">The role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking class labels (e.g., busiextracted from Web for various instances (e.g., de The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>A Tiberi</author>
</authors>
<title>Extracting semantic relations from query logs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM Conference on Knowledge Discovery and Data Mining (KDD-07),</booktitle>
<pages>76--85</pages>
<location>San Jose, California.</location>
<contexts>
<context position="31288" citStr="Baeza-Yates and Tiberi, 2007" startWordPosition="5163" endWordPosition="5166">n extracted IsA repositories. It shows that query sessions produce betterranked class labels than isolated queries do. A task complementary to class label ranking is entity ranking (Billerbeck et al., 2010), also referred to as ranking for typed search (Demartini et al., 2009). The choice of search queries and query substitutions is often influenced by, and indicative of, various semantic relations holding among full queries or query terms (Jones et al., 2006). Semantic relations may be loosely defined, e.g., by exploring the acquisition of untyped, similarity-based relations from query logs (Baeza-Yates and Tiberi, 2007). In comparison, queries are used here to re-rank class labels capturing a well-defined type of open-domain relations, namely IsA relations. 6 Conclusion In an attempt to bridge the gap between information stated in documents and information requested 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Rank (not in top 20) Percentage of queries 64.000 32.000 16.000 4.000 2.000 0.500 0.250 0.125 0.062 8.000 1.000 Full-match Partial-match 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Rank (not in top 20) Percentage of queries 64.000 32.000 16.000 4.000 2.000 0.500 0.250 0.125 0.062 8.000 1.0</context>
</contexts>
<marker>Baeza-Yates, Tiberi, 2007</marker>
<rawString>R. Baeza-Yates and A. Tiberi. 2007. Extracting semantic relations from query logs. In Proceedings of the 13th ACM Conference on Knowledge Discovery and Data Mining (KDD-07), pages 76–85, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>Michael J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07),</booktitle>
<pages>2670--2676</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="1186" citStr="Banko et al., 2007" startWordPosition="181" endWordPosition="184">ated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts. 1 Introduction Motivation: The offline acquisition of instances (rio de janeiro, porsche cayman) and their corresponding class labels (brazilian cities, locations, vehicles, sports cars) from text has been an active area of research. In order to extract fine-grained classes of instances, existing methods often apply manuallycreated (Banko et al., 2007; Talukdar et al., 2008) or automatically-learned (Snow et al., 2006) extraction patterns to text within large document collections. In Web search, the relative ranking of documents returned in response to a query directly affects the outcome of the search. Similarly, the quality of the relative ranking among class labels extracted for a given instance influences any applications (e.g., query refinements or structured extraction) using the extracted data. But due to noise in Web data and limitations of extraction techniques, class labels acquired for a given instance (e.g., oil shale) may fail</context>
<context position="12545" citStr="Banko et al., 2007" startWordPosition="2082" endWordPosition="2085">ed merging of: ScoreH1+H2 from Section 2, in run Rp, which corresponds to re-ranking using cooccurrence of an instance and its class label in the same query; ScoreH1+H3 from Section 2, in run R3, which corresponds to re-ranking using cooccurrence of an instance and its class label, as separate queries within the same query session; and ScoreH1+H2+H3 from Section 2, in run R, which corresponds to re-ranking using both types of cooccurrences in queries. ScoreH3(C, Z) = ( �� N77 11 ri 1609 Evaluation Procedure: The manual evaluation of open-domain information extraction output is time consuming (Banko et al., 2007). A more practical alternative is an automatic evaluation procedure for ranked lists of class labels, based on existing resources and systems. Assume that there is a gold standard, containing gold class labels that are each associated with a gold set of their instances. The creation of such gold standards is discussed later. Based on the gold standard, the ranked lists of class labels available within an IsA repository can be automatically evaluated as follows. First, for each gold label, the ranked lists of class labels of individual gold instances are retrieved from the IsA repository. Secon</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, Michael J Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the Web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2670–2676, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Billerbeck</author>
<author>G Demartini</author>
<author>C Firan</author>
<author>T Iofciu</author>
<author>R Krestel</author>
</authors>
<title>Ranking entities using Web search query logs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th European Conference on Research and Advanced Technologyfor Digital Libraries (ECDL-10),</booktitle>
<pages>273--281</pages>
<location>Glasgow, Scotland.</location>
<contexts>
<context position="30865" citStr="Billerbeck et al., 2010" startWordPosition="5096" endWordPosition="5099">difficult to detect. 5 Related Work The role of anonymized query logs in Web-based information extraction has been explored in tasks such as class attribute extraction (Pas¸ca and Van Durme, 2007), instance set expansion (Pennacchiotti and Pantel, 2009) and extraction of sets of similar entities (Jain and Pennacchiotti, 2010). Our work compares the usefulness of queries and query sessions for ranking class labels in extracted IsA repositories. It shows that query sessions produce betterranked class labels than isolated queries do. A task complementary to class label ranking is entity ranking (Billerbeck et al., 2010), also referred to as ranking for typed search (Demartini et al., 2009). The choice of search queries and query substitutions is often influenced by, and indicative of, various semantic relations holding among full queries or query terms (Jones et al., 2006). Semantic relations may be loosely defined, e.g., by exploring the acquisition of untyped, similarity-based relations from query logs (Baeza-Yates and Tiberi, 2007). In comparison, queries are used here to re-rank class labels capturing a well-defined type of open-domain relations, namely IsA relations. 6 Conclusion In an attempt to bridge</context>
</contexts>
<marker>Billerbeck, Demartini, Firan, Iofciu, Krestel, 2010</marker>
<rawString>B. Billerbeck, G. Demartini, C. Firan, T. Iofciu, and R. Krestel. 2010. Ranking entities using Web search query logs. In Proceedings of the 14th European Conference on Research and Advanced Technologyfor Digital Libraries (ECDL-10), pages 273–281, Glasgow, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cafarella</author>
<author>A Halevy</author>
<author>D Wang</author>
<author>E Wu</author>
<author>Y Zhang</author>
</authors>
<title>WebTables: Exploring the power of tables on the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 34th Conference on Very Large Data Bases (VLDB-08),</booktitle>
<pages>538--549</pages>
<location>Auckland, New Zealand.</location>
<contexts>
<context position="30076" citStr="Cafarella et al., 2008" startWordPosition="4965" endWordPosition="4969">st-processing stage that enhances the ranking of the class labels extracted for various instances by any method into any IsA repository. Open-domain class labels extracted from text and re-ranked as described in this paper are useful in a variety of applications. Search tools such as Google Squared return a set of instances, in response to class-seeking queries (e.g., insurance companies). The labeling of the returned set of instances, using the re-ranked class labels available per instances, allows for the generation of query refinements (e.g., insurers). In search over semi-structured data (Cafarella et al., 2008), the labeling of column cells is useful to infer the semantics of a table column, when the subject row of the table in which the column appears is either absent or difficult to detect. 5 Related Work The role of anonymized query logs in Web-based information extraction has been explored in tasks such as class attribute extraction (Pas¸ca and Van Durme, 2007), instance set expansion (Pennacchiotti and Pantel, 2009) and extraction of sets of similar entities (Jain and Pennacchiotti, 2010). Our work compares the usefulness of queries and query sessions for ranking class labels in extracted IsA r</context>
</contexts>
<marker>Cafarella, Halevy, Wang, Wu, Zhang, 2008</marker>
<rawString>M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang. 2008. WebTables: Exploring the power of tables on the Web. In Proceedings of the 34th Conference on Very Large Data Bases (VLDB-08), pages 538–549, Auckland, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Demartini</author>
<author>T Iofciu</author>
<author>A de Vries</author>
</authors>
<title>Entity Ranking track.</title>
<date>2009</date>
<journal>Overview of the INEX</journal>
<booktitle>In INitiative for the Evaluation of XML Retrieval Workshop,</booktitle>
<pages>254--264</pages>
<location>Brisbane, Australia.</location>
<marker>Demartini, Iofciu, de Vries, 2009</marker>
<rawString>G. Demartini, T. Iofciu, and A. de Vries. 2009. Overview of the INEX 2009 Entity Ranking track. In INitiative for the Evaluation of XML Retrieval Workshop, pages 254–264, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="2364" citStr="Etzioni et al., 2005" startWordPosition="368" endWordPosition="371">a given instance (e.g., oil shale) may fail to properly capture the semantic classes to which the instance may belong (Kozareva et al., 2008). Inevitably, some of the extracted class labels will be less useful (e.g., sources, mutual concerns) or incorrect (e.g., plants for the instance oil shale). In previous work, the relative ranking of class labels for an instance is determined mostly based on features derived from the source Web documents from which the data has been extracted, such as variations of the frequency of co-occurrence or diversity of extraction patterns producing a given pair (Etzioni et al., 2005). Contributions: This paper explores the role of Web search queries, rather than Web documents, in inducing superior ranking among class labels extracted automatically from documents for various instances. It compares two sources of indirect ranking evidence available within anonymized query logs: a) co-occurrence of an instance and its class label in the same query; and b) co-occurrence of an instance and its class label, as separate queries within the same query session. The former source is a noisy attempt to capture queries that narrow the search results to a particular class of the instan</context>
<context position="5119" citStr="Etzioni et al., 2005" startWordPosition="816" endWordPosition="819">, then C and I are likely to occur in text in contexts that are indicative of an IsA relation. • Hypothesis H2: If C is a prominent class of an instance I, and I is ambiguous, then a fraction of the queries about I may also refer to and contain C. • Hypothesis H3: If C is a prominent class of an instance I, then a fraction of the queries about I may be followed by queries about C, and vice-versa. Ranking Functions: The ranking functions follow directly from the above hypotheses. • Ranking based on H1 (using documents): The first hypothesis H1 is a reformulation of findings from previous work (Etzioni et al., 2005). In practice, a class label is deemed more relevant for an instance if the pair is extracted more frequently and by multiple patterns, with the scoring formula: Scorex1(C, I) = Freq(C,I) x Size({Pattern(C)})2 (1) where Freq(C,I) is the frequency of extraction of C for the instance I, and Size({Pattern(C)}) is the number of unique patterns extracting the class label C for the instance I. The patterns are hand-written, following (Hearst, 1992): ff..] where I is a potential instance (e.g., diderot) and C is a potential class label (e.g., writers). The boundaries are approximated from the part-of</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the Web: an experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and Some of its Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and Some of its Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92),</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="5565" citStr="Hearst, 1992" startWordPosition="891" endWordPosition="892">ly from the above hypotheses. • Ranking based on H1 (using documents): The first hypothesis H1 is a reformulation of findings from previous work (Etzioni et al., 2005). In practice, a class label is deemed more relevant for an instance if the pair is extracted more frequently and by multiple patterns, with the scoring formula: Scorex1(C, I) = Freq(C,I) x Size({Pattern(C)})2 (1) where Freq(C,I) is the frequency of extraction of C for the instance I, and Size({Pattern(C)}) is the number of unique patterns extracting the class label C for the instance I. The patterns are hand-written, following (Hearst, 1992): ff..] where I is a potential instance (e.g., diderot) and C is a potential class label (e.g., writers). The boundaries are approximated from the part-of-speech tags of the sentence words, for potential class labels C; and identified by checking that I occurs as an entire query in query logs, for instances I (Van Durme and Pas¸ca, 2008). The application of the scoring formula (1) to candidates extracted from the Web produces a ranked list of class labels LH1(I). • Ranking based on H2 (using queries): Intuitively, Web users searching for information about I sometimes add some or all terms of C</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jain</author>
<author>M Pennacchiotti</author>
</authors>
<title>Open entity extraction from Web search query logs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),</booktitle>
<pages>510--518</pages>
<location>Beijing, China.</location>
<contexts>
<context position="30568" citStr="Jain and Pennacchiotti, 2010" startWordPosition="5047" endWordPosition="5050"> instances, allows for the generation of query refinements (e.g., insurers). In search over semi-structured data (Cafarella et al., 2008), the labeling of column cells is useful to infer the semantics of a table column, when the subject row of the table in which the column appears is either absent or difficult to detect. 5 Related Work The role of anonymized query logs in Web-based information extraction has been explored in tasks such as class attribute extraction (Pas¸ca and Van Durme, 2007), instance set expansion (Pennacchiotti and Pantel, 2009) and extraction of sets of similar entities (Jain and Pennacchiotti, 2010). Our work compares the usefulness of queries and query sessions for ranking class labels in extracted IsA repositories. It shows that query sessions produce betterranked class labels than isolated queries do. A task complementary to class label ranking is entity ranking (Billerbeck et al., 2010), also referred to as ranking for typed search (Demartini et al., 2009). The choice of search queries and query substitutions is often influenced by, and indicative of, various semantic relations holding among full queries or query terms (Jones et al., 2006). Semantic relations may be loosely defined, </context>
</contexts>
<marker>Jain, Pennacchiotti, 2010</marker>
<rawString>A. Jain and M. Pennacchiotti. 2010. Open entity extraction from Web search query logs. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages 510–518, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
<author>B Rey</author>
<author>O Madani</author>
<author>W Greiner</author>
</authors>
<title>Generating query substitutions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15h</booktitle>
<contexts>
<context position="31123" citStr="Jones et al., 2006" startWordPosition="5140" endWordPosition="5143">ction of sets of similar entities (Jain and Pennacchiotti, 2010). Our work compares the usefulness of queries and query sessions for ranking class labels in extracted IsA repositories. It shows that query sessions produce betterranked class labels than isolated queries do. A task complementary to class label ranking is entity ranking (Billerbeck et al., 2010), also referred to as ranking for typed search (Demartini et al., 2009). The choice of search queries and query substitutions is often influenced by, and indicative of, various semantic relations holding among full queries or query terms (Jones et al., 2006). Semantic relations may be loosely defined, e.g., by exploring the acquisition of untyped, similarity-based relations from query logs (Baeza-Yates and Tiberi, 2007). In comparison, queries are used here to re-rank class labels capturing a well-defined type of open-domain relations, namely IsA relations. 6 Conclusion In an attempt to bridge the gap between information stated in documents and information requested 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Rank (not in top 20) Percentage of queries 64.000 32.000 16.000 4.000 2.000 0.500 0.250 0.125 0.062 8.000 1.000 Full-match Partial-m</context>
</contexts>
<marker>Jones, Rey, Madani, Greiner, 2006</marker>
<rawString>R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proceedings of the 15h</rawString>
</citation>
<citation valid="true">
<authors>
<author>World Wide</author>
</authors>
<title>Web Conference</title>
<date></date>
<pages>387--396</pages>
<location>Edinburgh, Scotland.</location>
<marker>Wide, </marker>
<rawString>World Wide Web Conference (WWW-06), pages 387– 396, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic class learning from the Web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>1048--1056</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1884" citStr="Kozareva et al., 2008" startWordPosition="289" endWordPosition="292">tion patterns to text within large document collections. In Web search, the relative ranking of documents returned in response to a query directly affects the outcome of the search. Similarly, the quality of the relative ranking among class labels extracted for a given instance influences any applications (e.g., query refinements or structured extraction) using the extracted data. But due to noise in Web data and limitations of extraction techniques, class labels acquired for a given instance (e.g., oil shale) may fail to properly capture the semantic classes to which the instance may belong (Kozareva et al., 2008). Inevitably, some of the extracted class labels will be less useful (e.g., sources, mutual concerns) or incorrect (e.g., plants for the instance oil shale). In previous work, the relative ranking of class labels for an instance is determined mostly based on features derived from the source Web documents from which the data has been extracted, such as variations of the frequency of co-occurrence or diversity of extraction patterns producing a given pair (Etzioni et al., 2005). Contributions: This paper explores the role of Web search queries, rather than Web documents, in inducing superior ran</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the Web with hyponym pattern linkage graphs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 1048–1056, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>B Van Durme</author>
</authors>
<title>What you seek is what you get: Extraction of class attributes from query logs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07),</booktitle>
<pages>2832--2837</pages>
<location>Hyderabad, India.</location>
<marker>Pas¸ca, Van Durme, 2007</marker>
<rawString>M. Pas¸ca and B. Van Durme. 2007. What you seek is what you get: Extraction of class attributes from query logs. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2832–2837, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>The role of queries in ranking labeled instances extracted from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10),</booktitle>
<pages>955--962</pages>
<location>Beijing, China.</location>
<marker>Pas¸ca, 2010</marker>
<rawString>M. Pas¸ca. 2010. The role of queries in ranking labeled instances extracted from text. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING-10), pages 955–962, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pennacchiotti</author>
<author>P Pantel</author>
</authors>
<title>Entity extraction via ensemble semantics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-09),</booktitle>
<pages>238--247</pages>
<contexts>
<context position="30494" citStr="Pennacchiotti and Pantel, 2009" startWordPosition="5036" endWordPosition="5039">he returned set of instances, using the re-ranked class labels available per instances, allows for the generation of query refinements (e.g., insurers). In search over semi-structured data (Cafarella et al., 2008), the labeling of column cells is useful to infer the semantics of a table column, when the subject row of the table in which the column appears is either absent or difficult to detect. 5 Related Work The role of anonymized query logs in Web-based information extraction has been explored in tasks such as class attribute extraction (Pas¸ca and Van Durme, 2007), instance set expansion (Pennacchiotti and Pantel, 2009) and extraction of sets of similar entities (Jain and Pennacchiotti, 2010). Our work compares the usefulness of queries and query sessions for ranking class labels in extracted IsA repositories. It shows that query sessions produce betterranked class labels than isolated queries do. A task complementary to class label ranking is entity ranking (Billerbeck et al., 2010), also referred to as ranking for typed search (Demartini et al., 2009). The choice of search queries and query substitutions is often influenced by, and indicative of, various semantic relations holding among full queries or que</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2009</marker>
<rawString>M. Pennacchiotti and P. Pantel. 2009. Entity extraction via ensemble semantics. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP-09), pages 238–247, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLINGACL-06),</booktitle>
<pages>801--808</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1255" citStr="Snow et al., 2006" startWordPosition="191" endWordPosition="194">evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts. 1 Introduction Motivation: The offline acquisition of instances (rio de janeiro, porsche cayman) and their corresponding class labels (brazilian cities, locations, vehicles, sports cars) from text has been an active area of research. In order to extract fine-grained classes of instances, existing methods often apply manuallycreated (Banko et al., 2007; Talukdar et al., 2008) or automatically-learned (Snow et al., 2006) extraction patterns to text within large document collections. In Web search, the relative ranking of documents returned in response to a query directly affects the outcome of the search. Similarly, the quality of the relative ranking among class labels extracted for a given instance influences any applications (e.g., query refinements or structured extraction) using the extracted data. But due to noise in Web data and limitations of extraction techniques, class labels acquired for a given instance (e.g., oil shale) may fail to properly capture the semantic classes to which the instance may b</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>R. Snow, D. Jurafsky, and A. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLINGACL-06), pages 801–808, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Talukdar</author>
<author>J Reisinger</author>
<author>M Pas¸ca</author>
<author>D Ravichandran</author>
<author>R Bhagat</author>
<author>F Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<pages>582--590</pages>
<location>Honolulu, Hawaii.</location>
<marker>Talukdar, Reisinger, Pas¸ca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>P. Talukdar, J. Reisinger, M. Pas¸ca, D. Ravichandran, R. Bhagat, and F. Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-08), pages 582–590, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>M Pas¸ca</author>
</authors>
<title>Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08),</booktitle>
<pages>1243--1248</pages>
<location>Chicago, Illinois.</location>
<marker>Van Durme, Pas¸ca, 2008</marker>
<rawString>B. Van Durme and M. Pas¸ca. 2008. Finding cars, goddesses and enzymes: Parametrizable acquisition of labeled instances for open-domain information extraction. In Proceedings of the 23rd National Conference on Artificial Intelligence (AAAI-08), pages 1243– 1248, Chicago, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vyas</author>
<author>P Pantel</author>
</authors>
<title>Semi-automatic entity set refinement.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference of the North American Association for Computational Linguistics (NAACL-HLT-09),</booktitle>
<pages>290--298</pages>
<location>Boulder, Colorado.</location>
<marker>Vyas, Pantel, 2009</marker>
<rawString>V. Vyas and P. Pantel. 2009. Semi-automatic entity set refinement. In Proceedings of the 2009 Conference of the North American Association for Computational Linguistics (NAACL-HLT-09), pages 290–298, Boulder, Colorado.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>