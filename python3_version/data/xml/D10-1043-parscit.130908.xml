<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.975274">
Non-isomorphic Forest Pair Translation
</title>
<author confidence="0.995769">
Hui Zhang1, 2, 3 Min Zhang1 Haizhou Li1 Eng Siong Chng2
</author>
<affiliation confidence="0.915210666666667">
1Institute for Infocomm Research
2Nanyang Technological University
3USC Information Science Institute
</affiliation>
<email confidence="0.991005">
huizhang.fuan@gmail.com {mzhang, hli}@i2r.a-star.edu.sg aseschng@ntu.edu.sg
</email>
<sectionHeader confidence="0.995514" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948904761905">
This paper studies two issues, non-isomorphic
structure translation and target syntactic structure
usage, for statistical machine translation in the
context of forest-based tree to tree sequence trans-
lation. For the first issue, we propose a novel
non-isomorphic translation framework to capture
more non-isomorphic structure mappings than tra-
ditional tree-based and tree-sequence-based trans-
lation methods. For the second issue, we propose a
parallel space searching method to generate hypo-
thesis using tree-to-string model and evaluate its
syntactic goodness using tree-to-tree/tree sequence
model. This not only reduces the search complexity
by merging spurious-ambiguity translation paths
and solves the data sparseness issue in training, but
also serves as a syntax-based target language mod-
el for better grammatical generation. Experiment
results on the benchmark data show our proposed
two solutions are very effective, achieving signifi-
cant performance improvement over baselines
when applying to different translation models.
</bodyText>
<sectionHeader confidence="0.999104" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999460948717949">
Recently syntax-based methods have achieved very
promising results and attracted increasing interests in
statistical machine translation (SMT) research com-
munity due to their ability to provide informative
context structure information and convenience in
carrying out word transformation and sub-span reor-
dering. Fundamentally, syntax-based SMT views
translation as a structural transformation process.
Generally speaking, from modeling viewpoint, a
syntax-based model tries to convert the source struc-
tures into target structures iteratively and recursively
while from decoding viewpoint a syntax-based sys-
tem segments an input tree/forest into many
sub-fragments, translates each of them separately,
combines the translated sub-fragments and then finds
out the best combinations. Therefore, from bilingual
viewpoint, we face two fundamental problems: the
mapping between bilingual structures and the way of
carrying out the target structures combination.
For the first issue, a number of models have been
proposed to model the structure mapping between
tree and string (Galley et al., 2004; Liu et al., 2006;
Yamada and Knight, 2001; DeNeefe and Knight,
2009) and between tree and tree (Eisner, 2003;
Zhang et al., 2007 &amp; 2008; Liu et al., 2009). How-
ever, one of the major challenges is that all the cur-
rent models only allow one-to-one mapping from one
source frontier non-terminal node (Galley et al., 2004)
to one target frontier non-terminal node in a bilingual
translation rule. Therefore, all those translation equi-
valents with one-to-many frontier non-terminal node
mapping cannot be covered by the current
state-of-the-art models. This may largely compro-
mise the modeling ability of translation rules.
For the second problem, currently, the combina-
tion is driven by only the source side (both
tree-to-string model and tree-to-tree model only
check the source span compatibility when combining
different target structures in decoding) or only the
</bodyText>
<page confidence="0.991977">
440
</page>
<note confidence="0.8177775">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 440–450,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999988170731708">
target side (string to tree model). There is no well
study in considering both the source side information
and the compatibility between different target syn-
tactic structures during combination. In addition, it is
well known that the traditional tree-to-tree models
suffer heavily from the data sparseness issue in
training and the spurious-ambiguity translation path
issue (the same translation with different syntactic
structures) in decoding.
In addition, because of the performance limitation
of automatic syntactic parser, researchers propose
using packed forest (Tomita, 1987; Klein and Man-
ning, 2001; Huang, 2008)1 instead of 1-best parse
tree to carry out training (Mi and Huang, 2008) and
decoding (Mi et al., 2008) in order to reduce the side
effect caused by parsing errors of the one-best tree.
However, when we apply the tree-to-tree model to
the bilingual forest structures, both training and de-
coding become very complicated.
In this paper, to address the first issue, we propose
a framework to model the non-isomorphic translation
process from source tree fragment to target tree se-
quence, allowing any one source frontier
non-terminal node to be translated into any number
of target frontier non-terminal nodes. For the second
issue, we propose a technology to model the combi-
nation task by considering both sides’ syntactic
structure information. We evaluate and integrate the
two technologies into forest-based tree to tree se-
quence translation. Experimental results on the
NIST-2003 and NIST-2005 Chinese-English transla-
tion tasks show that our methods significantly out-
perform the forest-based tree to string and previous
tree to tree models as well as the phrase-based model.
The remaining of the paper is organized as fol-
lowing. Section 2 reviews the related work. In sec-
tion 3 and section 4, we discuss the proposed for-
est-based rule extraction (non-isomorphic mapping)
and decoding algorithms (target syntax information
usage). Finally we report the experimental results in
section 5 and conclude the paper in section 6.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.930646740740741">
Much effort has been done in the syntax-based trans-
lation modeling. Yamada and Knight (2001) propose
1 A packed forest is a compact representation of a set of trees
with sharing substructures; formally, it is defined as a triple a
triple&lt; V, E, S &gt;, where V is non-terminal node set, E is hy-
per-edge set and S is leaf node set (i.e. all sentence words).
Every node in V covers a consecutive sequence of leaf, every
hyper-edge in E connect the father node to its children nodes as
in a tree. Figure 8 is a packed forest contains two trees.
a string to tree model. Galley et al. (2004) propose
the GHKM scheme to model the string-to-tree map-
ping. Liu et al. (2006) propose a tree-to-string trans-
lation model. Liu et al. (2007) propose the tree se-
quence to string model to capture rules covered by
continuous sequence of trees. Shieber (2007), De-
Neefe and Knight (2009) and Carreras and Collins
(2009) propose synchronous tree adjoin grammar to
capture more tree-string mapping beyond the GHKM
scheme. Zhang et al. (2009a) propose the concept of
virtual node to reform a tree sequence as a tree, and
design efficient algorithms for tree sequence model
in forest context. All these works only consider either
the source side or the target side syntax information.
To capture both side syntax contexts, Eisner (2003)
studies the bilingual dependency tree-to-tree map-
ping in conceptual level. Zhang et al. (2008) propose
tree sequence-based tree-to-tree modeling. Liu et al.
(2009) propose efficient algorithms for tree-to-tree
model in the forest-based training and decoding
scheme. One common limitation of the above works
is they only allow the one-to-one mapping between
each non-terminal frontier node, and thus they suffer
from the issue of rule coverage. On the other hand,
due to the data sparseness issue and model coverage
issue, previous tree-to-tree (Zhang et al., 2008; Liu et
al., 2009) decoder has to rely solely on the span in-
formation or source side information to combine the
target syntactic structures, without checking the
compatibility of the merging nodes, in order not to
fail many translation paths. Thus, this solution fails
to effectively utilize the target structure information.
To address this issue, tree sequence (Liu et al.,
2007; Zhang et al., 2008) and virtual node (Zhang et
al., 2009a) are two concepts with promising results
reported. In this paper, with the help of these two
concepts, we propose a novel framework to solve the
one-to-many non-isomorphic mapping issue. In addi-
tion, our proposed solution of using target syntax
information enables our forest-based tree-to-tree se-
quence translation decoding algorithm to not only
capture bilingual forest information but also have
almost the same complexity as forest-based
tree-to-string translation. This reduces the time/space
complexity exponentially.
</bodyText>
<sectionHeader confidence="0.973044" genericHeader="method">
3 Tree to Tree Sequence Rules
</sectionHeader>
<bodyText confidence="0.9995822">
The motivation of introducing tree to tree sequence
rules is to add target syntax information to
tree-to-string rules. Following, we first briefly review
the definition of tree-to-string rules, and then de-
scribe the tree-to-tree sequence rules.
</bodyText>
<page confidence="0.997626">
441
</page>
<subsectionHeader confidence="0.680879">
3.1 Tree to String Rules
</subsectionHeader>
<figure confidence="0.8597725">
AD VV
努力 学习
(try hard to) (study)
try hard to study
</figure>
<figureCaption confidence="0.91608">
Fig. 1. A word-aligned sentence pair with source tree
Fig. 2 Examples of tree to string rules
Fig. 2 illustrates the examples of tree to string rules
extracted from Fig. 1. The tree-to-string rule is very
simple. Its source side is a sub-tree of source parse
tree and its target side is a string with only one varia-
ble/non-terminal X. The source side and the target
side is translation of each other with the constraint of
word alignments. Please note that there is no any
target syntactic or linguistic information used in the
tree-to-string model.
</figureCaption>
<subsectionHeader confidence="0.99974">
3.2 Tree to Tree Sequence Rules
</subsectionHeader>
<bodyText confidence="0.9994654">
It is more challenging when extracting rules with
target tree structure as constraint. Fig. 3 extends Fig.
1 with target tree structure. The problem is that, giv-
en a source tree node, we are able to find its target
string translation, but these target string may not
form a linguistic sub-tree. For example, in Fig. 3, the
source tree node “ADVP” in solid eclipse is trans-
lated to “try hard to” in the target sentence, but there
is no corresponding sub-tree covering and only cov-
ering it in the target side.
Given the example rules in Fig. 2, what are their
corresponding rules with target syntax information?
The answer is that the previous tree or tree se-
quence-based models fail to model the Rule 1 and
Rule 2 at Fig. 2, since at frontier node level they only
allow one-to-one node mapping but the solution is
one-to-many non-terminal frontier node mapping.
The concept of “virtual node” (Zhang et al. 2009a) is
a solution to this issue. To facilitate discussion, we
first introduce three concepts.
</bodyText>
<figureCaption confidence="0.9929855">
Fig. 3. A word-aligned bi-parsed tree
Fig. 4. A restructured tree with a virtual span root
</figureCaption>
<listItem confidence="0.9900066">
• Def. 1. The “node sequence” is a sequence of
nodes (either leaf or internal nodes) covering a
consecutive span. For example, in Fig 3, “VBP
RB TO” and “VBP ADVP TO” are two “node
sequence” covering the same span “try hard to”.
</listItem>
<figure confidence="0.784073666666667">
VP
ADVP
VP
</figure>
<page confidence="0.978592">
442
</page>
<listItem confidence="0.984910666666667">
• Def. 2. The “root node sequence” of a span is
such a node sequence that any node in this se-
quence could not be a child of a node in other
node sequence of the span. Intuitively, the “root
node sequence” of a span is the node sequence
with the highest topology level. For example,
“VBP ADVP TO” is the “root node sequence”
of the span of “try hard to”. It is easy to prove
that given any span, there exist one and only one
“root node sequence”.
• Def. 3. The “span root” of a span is such a node
that if the “root node sequence” contains only
</listItem>
<bodyText confidence="0.94039075">
one tree node, then the “span root” is this tree
node; otherwise, the “span root” is the virtual
father node (Zhang et al., 2009a) of the “root
node sequence”. Fig. 4 illustrates the reformed
Fig. 3 by introducing the virtual node
“VBP+ADVP+TO” as the “span root” of the
span of “try hard to”.
The “span root” facilitates us to extract rules with
target side structure information. Given a sub-tree of
the source tree, we have a set of non-terminal frontier
nodes. For each such frontier node, we can find its
corresponding target “span root”. If the “span root”
is a virtual node, then we add it into the target tree as
a virtual segmentation joint point. After adding the
“span root” as joint point, we are able to ensure that
each frontier source node has only one corresponding
target node, then we can use any traditional rule ex-
traction algorithm to extract rules, including those
rules with one-to-many non-terminal frontier map-
pings.
</bodyText>
<figureCaption confidence="0.927981">
Fig. 5. Tree-to-tree sequence rules
</figureCaption>
<bodyText confidence="0.998334263157895">
Fig. 5 lists the corresponding rules with target
structure information of the tree-to-string rules in Fig
2. All the three rules cannot be extracted by previous
tree-to-tree mapping methods (Liu et al., 2009). The
previous tree-sequence-based methods (Zhang et al.,
2008; Zhang et al., 2009a) can extracted rule 3 since
they allow one-to-many mapping in root node level.
But they cannot extract rule 1 and rule 2. Therefore,
for any tree-to-string rule, our method can always
find the corresponding tree-to-tree sequence rule. As
a result, our rule coverage is the same as
tree-to-string framework while our rules contain
more informative target syntax information. Later we
will show that using our decoding algorithm the
tree-to-tree sequence search space is exponentially
reduced to the same as tree-to-string search space.
That is to say, we do not need to worry about the ex-
ponential search space issue of tree-to-tree sequence
model existing in previous work.
</bodyText>
<subsectionHeader confidence="0.999734">
3.3 Rule Extraction in Tree Context
</subsectionHeader>
<bodyText confidence="0.999106714285714">
Given a word aligned tree pair, we first extract the
set of minimum tree to string rules (Galley et al.
2004), then for each tree-to-string rule, we can easily
extract its corresponding tree-to-tree sequence rule
by introducing the virtual span root node. After that,
we generate the composite rules by iteratively com-
bining small rules.
</bodyText>
<figureCaption confidence="0.524971">
Fig. 6. Rule combination and virtual node removing
</figureCaption>
<page confidence="0.995934">
443
</page>
<bodyText confidence="0.999927428571429">
Please note that in generating composite rules, if
the joint node is a virtual node, we have to recover
the original link and remove this virtual node to
avoid unnecessary ambiguity. Fig. 6 illustrates the
combination process of rule 2 and rule 3 in Fig. 5. As
a result, all of our extract rules do not contain any
internal virtual nodes.
</bodyText>
<subsectionHeader confidence="0.999376">
3.4 Rule Extraction in Forest Context
</subsectionHeader>
<bodyText confidence="0.99361936">
In forest pair context, we also first generate the
minimum tree-to-string rule set as Mi et al. (2008),
and for each tree-to-string rule, we find its corres-
ponding tree-to-tree sequence rules, and then do rule
composition.
In tree pair context, given a tree-to-string rule,
there is one and only one corresponding tree-to-tree
sequence rule. But in forest pair context, given one
such tree-to-string rule, there are many correspond-
ing tree-to-tree sequence rules. All these sub-trees
form one or more sub-forests2 of the entire big target
forest. If we can identify the sub-forests, i.e., all of
the hyper-edges of the sub-forests, we can retrieve all
the sub-trees from the sub-forests as the target sides
of the corresponding tree-to-tree sequence rules.
Given a source sub-tree, we can obtain the target
root span where the target sub-forests start and the
frontier spans where the target sub-forests stop. To
indentify all the hyper-edges in the sub-forests, we
start from every node covering the root span, traverse
from top to down, mark all the hyper-edges visited
and stop at the node if its span is a sub-span of one of
the forest frontier spans or if it is a word node. The
reason we stop at the node once it fell into a frontier
span (i.e. the span of the node is a sub-span of the
frontier span) is to guarantee that given any frontier
span, we could stop at the “root node sequence” of
this span by Def. 2.
For example, Fig. 7 is a source sub-tree of rule 2
in Fig. 5 and the circled part in Fig. 8 is one of its
corresponding target sub-forests. Its corresponding
target root span is [1,4] (corresponding to source root
“VP” ) and its corresponding target frontier span is
{[1,3], study[4,4]}. Now given the target forest, we
start from node VP[1,4] and traverse from top to
down, finally stop at following nodes: VBP[1,1],
ADVP[2,2], TO[3,3], study .
2 All the sub-forests cover the same span. But their roots have
different grammar tags as the roots’ names. The root may be a
virtual span root node in the case of the one-to-many frontier
non-terminal node mappings.
Please note that the starting root node must be a
single node, being either a normal forest node or a
virtual “span root” node. The virtual “span root”
node serves as the frontier node of upper rules and
root node of the currently being extracted rules. Be-
cause we extract rules in a top-to-down manner, the
necessary virtual “span root” node for current
sub-forest has already been added into the global
forest when extracting upper level rules.
</bodyText>
<figureCaption confidence="0.999945333333333">
Figure 7. A source sub-tree in rule 2
Fig. 8. The corresponding target sub-forest for the tree of
Figure 7.
</figureCaption>
<subsectionHeader confidence="0.924432">
3.5 Fractional Count of Rule
</subsectionHeader>
<bodyText confidence="0.999358833333333">
Following Mi and Huang (2008) and Liu et al.
(2009), we assign a fractional count to a rule to
measure how likely it appears given the context of
the forest pair. In following equation, “S” means
source sub-tree, “T” means target sub-tree, “SF” is
source forest and “TF” is the target forest.
</bodyText>
<equation confidence="0.5315095">
P(S, T |SF,TF) - P(S|SF, TF) * P(T|SF, TF)
- P(S|SF) * P(T|TF)
</equation>
<page confidence="0.985989">
444
</page>
<bodyText confidence="0.9485077">
The above equation means the fractional count of
a source-target tree pair is just the product of each of
their fractional count in corresponding forest context
in following equation.
ߙߚሺ݂݋ݎ݁ݏݐ ݎ݋݋ݐሻ
where ߙ and ߚ are the outside and inside probabil-
ities. In addition, if a sub-tree root is a virtual node
(formed by a root node sequence), then we use fol-
lowing equation to approximate the outside probabil-
ity of the virtual node.
</bodyText>
<equation confidence="0.995583333333333">
# ೚೑ ೙೚೏೐ೞ ೔೙ ೙ೞ
ߙሺ݊݋݀݁ ݏ݁ݍݑ݁݊ܿ݁ ݊ݏሻ ൌ ඨ ෑ ߙሺ݊ሻ
௡ א ௡௦
</equation>
<sectionHeader confidence="0.999126" genericHeader="method">
4 Decoding
</sectionHeader>
<subsectionHeader confidence="0.962623">
4.1 Traditional Forest-based Decoding
</subsectionHeader>
<bodyText confidence="0.997910181818182">
A typical translation process of a forest-based system
is to first convert the source packed forest into a tar-
get translation forest, and then apply search algo-
rithm to find the best translation result from this tar-
get translation forest (Mi et al., 2008).
For the tree-to-string model, the forest conversion
process is as following: given an input packed forest,
we do pattern matching (Zhang et al., 2009b) with
the source side structures in the rule set. For each
matched rule, we establish its target side as a hy-
per-edge in the target forest.
</bodyText>
<figureCaption confidence="0.996607">
Fig. 9. A forest conversion step in a tree to string model
</figureCaption>
<bodyText confidence="0.935917692307692">
Fig. 9 exemplifies a conversion step in the tree to
string model. A sub-tree structure with two hy-
per-edge “VP[2,4] =&gt; ADVP[2,2] VP[3,4]” and
“VP[3,4] =&gt; ADVP[3,4] VP[4,4]” is converted into
a target hyper-edge “X-VP[2,4] =&gt; X-ADVP[3,3]
X-ADVP[2,2] X-VP[4,4] ”. The node “X-VP[4,4]”
in the target forest means that its syntactic label in
target forest is “X” and it is translated from the
source node “VP[4,4]” in the source forest. In this
target hyper-edge, “X-ADVP[3,3] X-ADVP[2,2]”
means the translation from source node “ADVP[3,3]”
is put before the translation from “ADVP[2,2]”,
representing a structure reordering.
</bodyText>
<subsectionHeader confidence="0.97822">
4.2 Toward Bilingual Syntax-aware Trans-
lation Generation
</subsectionHeader>
<bodyText confidence="0.999982780487805">
As we could see in section 4.1, there is only one kind
of non-terminal symbol “X” in the target side. It is a
big challenge to rely on such a coarse label to gener-
ate a translation with fine syntactic quality. For ex-
ample, a source node may be translated into a “NP”
(noun phrase) in target side. However, in this rule set
with the only symbol “X”, it may be merged with
upper structure as a “VP” (verb phrase) instead, be-
cause there is no way to favor one over another. In
this case, the target tree does not well model the
translation syntactically. In addition, all of the inter-
nal structure information in the target side is ignored
by the tree-to-string rules.
One natural solution to the above issue is to use
the tree to tree/tree sequence model, which have
richer target syntax structures for more discrimina-
tive probability and finer labels to guide the combi-
nation process. However, the tree to tree/tree se-
quence model may face very severe computational
problem and so-called “spurious ambiguities” issue.
Theoretically, if in the tree-to-tree sequence mod-
el-based decoding, we just give a penalty to the in-
compatible-node combinations instead of pruning out
the translation paths, then the set of sentences gener-
ated by the tree-to-tree sequence model is identical to
that of the tree-to-string model since every
tree-to-tree sequence rule can be projected into a
tree-to-string rule. Motivated by this, we propose a
solution call parallel hypothesis spaces searching to
solve the computational and “spurious ambiguities”
issues mentioned above. In the meanwhile, we can
fully utilize the target structure information to guide
translation.
We restructure the tree-to-tree sequence rule set by
grouping all the rules according to their correspond-
ing tree-to-string rules. This behaves like a
“tree-to-forest” rule. The “forest” encodes all the tree
sequences with same corresponding string. With the
re-constructed rule set, during decoding, we generate
two target translation hypothesis spaces (in the form
of packed forests) synchronously by the tree-to-string
</bodyText>
<equation confidence="0.97325975">
ܲሺݏݑܾݐݎ݁݁ |ܨ݋ݎ݁ݏݐሻ ൌ ߙߚሺ݂݋ݎ݁ݏݐ ݎ݋݋ݐሻ
ߙሺݏݑܾݐݎ݁݁ ݎ݋݋ݐሻ כ ∏௛א௦௨௕௧௥௘௘ ܲሺ݄ሻ כ ∏௩א௟௘௔௩௘௦ሺ௦௨௕௧௥௘௘ሻ ߚሺݒሻ
ൌ
ߙߚሺݏݑܾݐݎ݁݁ሻ
</equation>
<page confidence="0.994313">
445
</page>
<bodyText confidence="0.999844">
rules and tree-to-tree sequence rules, and maintain
the projection between them. In other words, we
generate hypothesis (searching) from the
tree-to-string forest and calculate the probability
(evaluating syntax goodness) for each hypothesis by
the hyper-edges in the tree-to-tree sequence forest.
</bodyText>
<subsectionHeader confidence="0.989034">
4.3 Parallel Hypothesis Spaces
</subsectionHeader>
<figureCaption confidence="0.560995">
Fig. 10. Mapping from tree-to-tree sequence into
tree-to-string rule
</figureCaption>
<bodyText confidence="0.999981769230769">
In this subsection, we describe what the parallel
search spaces are and how to construct them. As
shown at Fig. 10, given a tree-to-tree sequence rule,
it is easy to find its corresponding tree-to-string rule
by simply ignoring the target inside structure and
renaming the root and leaves non-terminal labels into
“X”. We iterate through the tree-to-tree sequence rule
set, find its corresponding tree-to-string rule and then
group those rules with the same tree-to-string projec-
tion. After that, the original tree-to-tree sequence rule
set becomes a set of smaller rule sets. Each of them is
indexed by a unique tree-to-string rule.
We apply the tree-to-string rules to generate an
explicit target translation forest to represent the target
sentences space. At the same time, whenever a
tree-to-string rule is applied, we also retrieve its cor-
responding tree-to-tree sequence rule set and gener-
ate a set of latent hyper-edges with fine-grained syn-
tax information. In this case, we have two parallel
forests, one with coarse explicit hyper-edges and the
other fine and latent. Given a hyper-edge (or a node)
in the coarse forest, there are a group of correspond-
ing latent hyper-edges (or nodes) with finer syntax
labels in the fine forest. Accordingly, given a tree in
the coarse forest, there is a corresponding sub-forest
in the latent fine forest. We can view the latent fine
forest as imbedded inside the explicit coarse forest. If
an explicit hyper-edge is viewed as a big cable, then
the group of its corresponding latent hyper-edges is
the small wires inside it.
We rely on the explicit hyper-edges to enumerate
possible hypothesis while using the latent hy-
per-edges to measure its translation probability and
syntax goodness. Thus, the complexity of the search
space is reduced into the tree-to-string model level,
while keeping the target language generation syntac-
tic aware. More importantly, we thoroughly avoid
those spurious ambiguities introduced by the
tree-to-tree sequence rules.
</bodyText>
<subsectionHeader confidence="0.986659">
4.4 Decoding with Parallel Hypothesis
Spaces
</subsectionHeader>
<figureCaption confidence="0.88239">
Fig. 11. Derivation path and derivation forest
</figureCaption>
<bodyText confidence="0.999885538461538">
In this subsection, we show exactly how our decoder
finds the best result from the parallel spaces. We
generate hypothesis by traversing the coarse forest in
the parallel spaces with cube-pruning (Huang and
Chiang, 2007). Given a newly generated hypothesis,
it is affiliated with a derivation path (tree) in the
coarse forest and a group of derivation paths
(sub-forest) in the finer forest. As shown in Fig. 11,
the left part is the derivation path formed by a coarse
hyper-edge, consisting the newly-generated sub-tree
“X =&gt; X X X” connecting with three previous-
ly-generated sub paths while the right part is the de-
rivation forest formed by newly-generated finer hy-
per-edges rooted at “VP” and “S”, and previous-
ly-generated sub-forests.
In this paper, we use the sum of probabilities of all
the derivation paths in the finer forest to measure the
quality of the candidate translation suggested by the
hypothesis. From Fig. 11, we can see there may be
more than one corresponding finer forests, it is easy
to understand that the sum of all the trees’ probabili-
ties in these finer forests is equal to the sum of the
inside probability of all these root nodes of these fin-
er forests. We adopt the dynamic programming to
compute the probability of the finer forest: whenever
we generate a new hypothesis by concatenating a
</bodyText>
<page confidence="0.998337">
446
</page>
<bodyText confidence="0.999976">
coarse hyper-edge and its sub-path, we find its cor-
responding finer hyper-edges and sub-forests, do the
combination and accumulate probabilities from bot-
tom to up. For the coarse hyper-edge, because there
is only one label “X”, any sub-path could be easily
concatenated with upper structure covering the same
sub-span without the need of checking label compa-
tibility. While for the finer hyper-edges, we only link
the root nodes of sub-forests to upper hyper-edges
with the same linking node label. This is to guarantee
syntactic goodness. In case there are some leaf nodes
of the upper hyper-edges fail to find corresponding
sub-forest roots with the same label (e.g. the “NP” in
red color in the rightmost of Fig 11), we simply link
it into the nodes with the least inside probability
(among these sub-forests), and at the same time give
a penalty score to this combination. If some root
nodes of some sub-forest still cannot find upper leaf
nodes to concatenate (e.g. the “CP” in red color in
Fig. 11), we simply ignore them. After the combina-
tion process, it is straightforward to accumulate the
inside probability dynamically from bottom up.
</bodyText>
<sectionHeader confidence="0.998655" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.985815">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.99903424137931">
We evaluate our method on the Chinese-English
translation task. We first carry out a series empirical
study on a set of parallel data with 30K sentence
pairs, and then do experiment on a larger data set to
ensure that the effectiveness of our method is consis-
tent across data set of different size. We use the
NIST 2002 test set as our dev set, and NIST 2003
and NIST 2005 test sets as our test set. A 3-gram
language model is trained on the target side of the
training data by the SRILM Toolkits (Stolcke, 2002)
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995). We train Charniak’s parser (Charniak,
2000) on CTB5.0 for Chinese and ETB3.0 for Eng-
lish and modify it to output packed forest. GIZA++
(Och and Ney, 2003) and the heuristics
“grow-diag-final-and” are used to generate m-to-n
word alignments. For the MER training (Och, 2003),
Koehn’s MER trainer (Koehn, 2007) is modified for
our system. For significance test, we use Zhang et
al.’s implementation (Zhang et al, 2004). Our evalu-
ation metrics is case-sensitive closest BLEU-4 (Pa-
pineni et al., 2002). We use following features in our
systems: 1) bidirectional tree-to-tree sequence proba-
bility, 2) bidirectional tree-to-string probability, 3)
bidirectional lexical translation probability, 4) target
language model, 5) source tree probability 6) the av-
erage number of unmatched nodes in the target forest.
7) the length of the target translation, 8) the number
of glue rules used.
</bodyText>
<subsectionHeader confidence="0.994728">
5.2 Empirical Study on Small Data
</subsectionHeader>
<bodyText confidence="0.999149875">
We set forest pruning threshold (Mi et al., 2008) to 8
on both source and target forests for rule extraction.
For each source sub-tree, we set its height up to 3,
width up to 7 and extract up to 10-best target struc-
tures. In decoding, we set the pruning threshold to 10
for the input source forest. Table 1 compares the
performance in NIST 2003 data set of our method
and several state-of-the-art systems as our baseline.
</bodyText>
<listItem confidence="0.964976615384616">
1) MOSES: phrase-based system (Koehn et al.,
2007)
2) FT2S: forest-based tree-to-string system (Mi
and Huang, 2008; Mi et al., 2008)
3) FT2T: forest-based tree-to-tree system (Liu et
al., 2009).
4) FT2TS (1to1): our forest-based tree-to-tree
sequence system, where 1to1 means only
one-to-one frontier non-terminal node map-
ping is allowed, thus the system does not fol-
low our non-isomorphic mapping framework.
5) FT2TS (1toN): our forest-based tree-to-tree
sequence system that allows one-to-many
</listItem>
<bodyText confidence="0.959870333333333">
frontier non-terminal node mapping by fol-
lowing our non-isomorphic mapping frame-
work
In addition, our proposed parallel searching space
(PSS) technology can be applied to both tree to tree
and tree to sequence systems. Thus in table 1, for the
tree-to-tree/tree sequence systems, we report two
BLEU scores, one uses this technology (withPSS)
and one does not (noPSS).
</bodyText>
<table confidence="0.998675666666667">
Model BLEU-4
MOSES 23.39
FT2S 26.10
FT2T noPSS 23.40
withPSS 24.46
FT2TS (1to1) noPSS 25.39
withPSS 26.58
FT2TS (1toN) noPSS 26.30
withPSS 27.70
</table>
<tableCaption confidence="0.731635">
Table 1. Performance comparison of different methods
From Table 1, we can see that:
</tableCaption>
<page confidence="0.995262">
447
</page>
<listItem confidence="0.954005333333333">
1) All the syntax-based systems (except FT2T
(noPSS) (23.40)) consistently outperform the
phrase-based system MOSES significantly
</listItem>
<bodyText confidence="0.866689692307692">
(p &lt; 0.01), indicating that syntactic know-
ledge is very useful to SMT.
2) The PSS technology shows significant perfor-
mance improvement ሺp &lt; 0.01ሻ in all mod-
els, which clearly shows effectiveness of the
PSS technology in utilizing target structures
for target language generation.
3) FT2TS (1toN) significantly outperforms
(p &lt; 0.01) FT2TS (1to1) in both cases (noPSS
and withPSS). This convincingly shows the
effectiveness of our non-isomorphic mapping
framework in capturing the non-isomorphic
structure translation equivalences.
</bodyText>
<listItem confidence="0.779434333333333">
4) Both FT2TS systems significantly outperform
FT2T( p &lt; 0.01). This verifies the effective-
ness of tree sequence rules.
</listItem>
<bodyText confidence="0.982794428571429">
5) FT2TS shows different level of performance
improvements over FT2S with the best case
having 1.6 (27.70-26.10) BLEU score im-
provement over FT2S. This suggests that the
target structure information is very useful, but
we need to find a correct way to effectively
utilize it.
</bodyText>
<table confidence="0.3903785">
1to1 1toN ratio
1735871 2363771 1:1.36
</table>
<tableCaption confidence="0.593356">
Table 2. Statistics on node mapping in forest, where
“1to1” means the number of nodes in source forest
that can be translated into one node in target forest
and “1toN” means the number of nodes in source
forest that have to be translated into more than one
node in target forest, where the node refers to
non-terminal nodes only
</tableCaption>
<table confidence="0.9975285">
Model # of rules T2S covered
FT2T 295732 26.8%
FT2TS(1to1) 631487 57.1%
FT2TS (1toN) 1945168 100%
</table>
<tableCaption confidence="0.977000666666667">
Table 3. Statistics of rule coverage, where “T2S
covered” means the percentage of tree-to-string
rules that can be covered by the model
</tableCaption>
<bodyText confidence="0.989079181818182">
Table 2 studies the node isomorphism between bi-
lingual forest pair. We can see that the
non-isomorphic node translation mapping (1toN)
accounts for 57.6% (=1.36/(1+1.36)) of all the forest
non-terminal nodes with target translation. This
means that the one-to-many node mapping is a major
issue in structure transformation. It also empirically
justifies the importance of our non-isomorphic map-
ping framework.
Table 3 shows the rule coverage of different bi-
lingual structure mapping model. FT2T only covers
26.8% tree-to-string rules, so it performs worse than
FT2S as shown in Table 1. FT2TS (1to1) does not
allow one-to-many frontier node mapping, so it could
only recover the non-isomorphic node mapping in
the root level, while FT2TS (1toN) could make it at
both root and leaf levels. Therefore, it is not surpris-
ing that in Table 3, FT2TS (1toN) cover many more
rules than FT2TS (1to1) because given a source tree,
there are many leaves, if any one of them is
non-isomorphic, then it could not be covered by the
FT2TS (1to1).
</bodyText>
<figure confidence="0.7400508">
Decoding Method
Traditional:
FT2TS (1toN) (noPPS)
Ours:
FT2TS (1toN) (withPPS)
</figure>
<tableCaption confidence="0.993764">
Table 4. Performance and speed comparison
</tableCaption>
<bodyText confidence="0.984427636363637">
Table 4 clearly shows the advantage of our decod-
er over the traditional one. Ours could not only gen-
erate better translation result, but also be
152.6/5.22&gt;30 times faster. This mainly attributes to
two reasons: 1) one-to-many frontier node mapping
equipments the model with more ability to capture
more non-isomorphic structure mappings than tradi-
tional models, and 2) “parallel search space” enables
the decoder to fully utilize target syntactic informa-
tion, but keeping the size of search space the same as
that a “tree to string” model explores.
</bodyText>
<subsectionHeader confidence="0.976811">
5.3 Results on Larger Data Set
</subsectionHeader>
<bodyText confidence="0.998969">
We also carry out experiment on a larger dataset
consisting of the small dataset used in last section
and the FBIS corpus. In total, there are 280K parallel
sentence pairs with 9.3M Chinese words and 11.8M
English words. A 3-gram language model is trained
on the target side of the parallel corpus and the GI-
GA3 Xinhua portion. We compare our system
(FT2TS with 1toN and withPPS) with two
state-of-the-art baselines: the phrase-based system
MOSES and the forest-based tree-to-string system
</bodyText>
<figure confidence="0.990607166666667">
BLEU-4 Speed
(sec/sent)
26.30
27.70
152.6
5.22
</figure>
<page confidence="0.994995">
448
</page>
<bodyText confidence="0.9981634">
implemented by us. Table 5 clearly shows the effec-
tiveness of our method is consistent across small and
larger corpora, outperforming FT2S by 1.6-1.8
BLEU and the MOSES by 3.3-4.0 BLEU statistically
significantly (p&lt;0.01).
</bodyText>
<table confidence="0.9972322">
Model BLEU
NIST2003 NIST2005
MOSES 29.51 27.53
FT2S 31.21 29.72
FT2TS 32.88 31.50
</table>
<tableCaption confidence="0.999726">
Table 5. Performance on larger data set
</tableCaption>
<sectionHeader confidence="0.9954" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99998219047619">
In this paper, we propose a framework to address the
issue of bilingual non-isomorphic structure mapping
and a novel parallel searching space scheme to effec-
tively utilize target syntactic structure information in
the context of forest-based tree to tree sequence ma-
chine translation. Based on this framework, we de-
sign an efficient algorithm to extract tree-to-tree se-
quence translation rules from word aligned bilingual
forest pairs. We also elaborate the parallel searching
space-based decoding algorithm and the node label
checking scheme, which leads to very efficient de-
coding speed as fast as the forest-based tree-to-string
model does, at the same time is able to utilize infor-
mative target structure knowledge. We evaluate our
methods on both small and large training data sets
and two NIST test sets. Experimental results show
our methods statistically significantly outperform the
state-of-the-art models across different size of cor-
pora and different test sets. In the future, we are in-
terested in testing our algorithm at forest-based tree
sequence to tree sequence translation.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989131147541">
Eugene Charniak. 2000. A maximum-entropy inspired
parser. NAACL-00.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003.
Syntax-based language models for statistical machine
translation. MT Summit IX. 40–46.
David Chiang. 2007. Hierarchical phrase-based transla-
tion.Computational Linguistics, 33(2).
Steve DeNeefe, Kevin Knight. 2009. Synchronous Tree
Adjoining Machine Translation. EMNLP-2009.
727-736.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for MT. ACL-03 (companion volume).
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What’s in a translation rule?
HLT-NAACL-04. 273-280.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-Local Features. ACL-HLT-08.
586-594
Liang Huang and David Chiang. 2005. Better k-best Pars-
ing. IWPT-05. 53-64
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models.
ACL-07. 144–151
Dan Klein and Christopher D. Manning. 2001. Parsing
and Hypergraphs. IWPT-2001.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling.
ICASSP-95, 181-184
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-
lison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin
and Evan Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. ACL-07. 177-180.
(poster)
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. COLING-ACL-06. 609-616.
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007.
Forest-to-String Statistical Translation Rules. ACL-07.
704-711.
Yang Liu, Yajuan Lü, Qun Liu. 2009. Improving
Tree-to-Tree Translation with Packed Forests. ACL-09.
558-566
Haitao Mi, Liang Huang, and Qun Liu. 2008. For-
est-based translation. ACL-HLT-08. 192-199.
Haitao Mi and Liang Huang. 2008. Forest-based Transla-
tion Rule Extraction. EMNLP-08. 206-214.
Franz J. Och. 2003. Minimum error rate training in statis-
tical machine translation. ACL-03. 160-167.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics. 29(1) 19-51.
Kishore Papineni, Salim Roukos, Todd Ward and
Wei-Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. ACL-02. 311-318.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. ICSLP-02. 901-904.
Masaru Tomita. 1987. An Efficient Aug-
mented-Context-Free Parsing Algorithm. Computation-
al Linguistics 13(1-2): 31-46
</reference>
<page confidence="0.990368">
449
</page>
<reference confidence="0.995462608695652">
Xavier Carreras and Michael Collins. 2009.
Non-projective Parsing for Statistical Machine Trans-
lation. EMNLP-2009. 200-209.
K. Yamada and K. Knight. 2001. A Syntax-Based Statis-
tical Translation Model. ACL-01. 523-530.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew
Lim Tan. 2009a. Forest-based Tree Sequence to String
Translation Model. ACL-IJCNLP-09. 172-180.
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan.
2009b. Fast Translation Rule Matching for Syn-
tax-based Statistical Machine Translation. EMNLP-09.
1037-1045.
Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim
Tan and Sheng Li. 2007. A Tree-to-Tree Align-
ment-based model for statistical Machine translation.
MT-Summit-07. 535-542
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew
Lim Tan, Sheng Li. 2008. A Tree Sequence Align-
ment-based Tree-to-Tree Translation Model.
ACL-HLT-08. 559-567.
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do
we need to have a better system? LREC-04. 2051-2054.
</reference>
<page confidence="0.99777">
450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.458340">
<title confidence="0.990304">Non-isomorphic Forest Pair Translation 2, 3 Haizhou Eng Siong for Infocomm</title>
<author confidence="0.623707">Technological</author>
<affiliation confidence="0.981451">Information Science</affiliation>
<email confidence="0.748798">huizhang.fuan@gmail.com{mzhang,hli}@i2r.a-star.edu.sgaseschng@ntu.edu.sg</email>
<abstract confidence="0.999797909090909">This paper studies two issues, non-isomorphic structure translation and target syntactic structure usage, for statistical machine translation in the context of forest-based tree to tree sequence translation. For the first issue, we propose a novel non-isomorphic translation framework to capture more non-isomorphic structure mappings than traditional tree-based and tree-sequence-based translation methods. For the second issue, we propose a parallel space searching method to generate hypothesis using tree-to-string model and evaluate its syntactic goodness using tree-to-tree/tree sequence model. This not only reduces the search complexity by merging spurious-ambiguity translation paths and solves the data sparseness issue in training, but also serves as a syntax-based target language model for better grammatical generation. Experiment results on the benchmark data show our proposed two solutions are very effective, achieving significant performance improvement over baselines when applying to different translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<date>2000</date>
<note>A maximum-entropy inspired parser. NAACL-00.</note>
<contexts>
<context position="26607" citStr="Charniak, 2000" startWordPosition="4318" endWordPosition="4319">We evaluate our method on the Chinese-English translation task. We first carry out a series empirical study on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bi</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy inspired parser. NAACL-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation. MT Summit IX.</title>
<date>2003</date>
<pages>40--46</pages>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. MT Summit IX. 40–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<date>2007</date>
<booktitle>Hierarchical phrase-based translation.Computational Linguistics,</booktitle>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="23701" citStr="Chiang, 2007" startWordPosition="3823" endWordPosition="3824">tion probability and syntax goodness. Thus, the complexity of the search space is reduced into the tree-to-string model level, while keeping the target language generation syntactic aware. More importantly, we thoroughly avoid those spurious ambiguities introduced by the tree-to-tree sequence rules. 4.4 Decoding with Parallel Hypothesis Spaces Fig. 11. Derivation path and derivation forest In this subsection, we show exactly how our decoder finds the best result from the parallel spaces. We generate hypothesis by traversing the coarse forest in the parallel spaces with cube-pruning (Huang and Chiang, 2007). Given a newly generated hypothesis, it is affiliated with a derivation path (tree) in the coarse forest and a group of derivation paths (sub-forest) in the finer forest. As shown in Fig. 11, the left part is the derivation path formed by a coarse hyper-edge, consisting the newly-generated sub-tree “X =&gt; X X X” connecting with three previously-generated sub paths while the right part is the derivation forest formed by newly-generated finer hyper-edges rooted at “VP” and “S”, and previously-generated sub-forests. In this paper, we use the sum of probabilities of all the derivation paths in the</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation.Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous Tree Adjoining Machine Translation.</title>
<date>2009</date>
<volume>2009</volume>
<pages>727--736</pages>
<contexts>
<context position="2493" citStr="DeNeefe and Knight, 2009" startWordPosition="334" endWordPosition="337">tively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 &amp; 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem, currently, the combinatio</context>
<context position="6415" citStr="DeNeefe and Knight (2009)" startWordPosition="959" endWordPosition="963"> E, S &gt;, where V is non-terminal node set, E is hyper-edge set and S is leaf node set (i.e. all sentence words). Every node in V covers a consecutive sequence of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling. Liu et a</context>
</contexts>
<marker>DeNeefe, Knight, 2009</marker>
<rawString>Steve DeNeefe, Kevin Knight. 2009. Synchronous Tree Adjoining Machine Translation. EMNLP-2009. 727-736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for MT.</title>
<date>2003</date>
<note>ACL-03 (companion volume).</note>
<contexts>
<context position="2533" citStr="Eisner, 2003" startWordPosition="343" endWordPosition="344">a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 &amp; 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem, currently, the combination is driven by only the source side (bot</context>
<context position="6860" citStr="Eisner (2003)" startWordPosition="1033" endWordPosition="1034">n model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling. Liu et al. (2009) propose efficient algorithms for tree-to-tree model in the forest-based training and decoding scheme. One common limitation of the above works is they only allow the one-to-one mapping between each non-terminal frontier node, and thus they suffer from the issue of rule coverage. On the other hand, due to the data sparseness issue and model coverage issue, previous tree-to-tree (Zhang et al., 2008; Liu et al., 2009) decoder has to r</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for MT. ACL-03 (companion volume).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<pages>04--273</pages>
<contexts>
<context position="2423" citStr="Galley et al., 2004" startWordPosition="322" endWordPosition="325">es to convert the source structures into target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 &amp; 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability o</context>
<context position="6132" citStr="Galley et al. (2004)" startWordPosition="912" endWordPosition="915">e paper in section 6. 2 Related Work Much effort has been done in the syntax-based translation modeling. Yamada and Knight (2001) propose 1 A packed forest is a compact representation of a set of trees with sharing substructures; formally, it is defined as a triple a triple&lt; V, E, S &gt;, where V is non-terminal node set, E is hyper-edge set and S is leaf node set (i.e. all sentence words). Every node in V covers a consecutive sequence of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works</context>
<context position="13296" citStr="Galley et al. 2004" startWordPosition="2115" endWordPosition="2118">onding tree-to-tree sequence rule. As a result, our rule coverage is the same as tree-to-string framework while our rules contain more informative target syntax information. Later we will show that using our decoding algorithm the tree-to-tree sequence search space is exponentially reduced to the same as tree-to-string search space. That is to say, we do not need to worry about the exponential search space issue of tree-to-tree sequence model existing in previous work. 3.3 Rule Extraction in Tree Context Given a word aligned tree pair, we first extract the set of minimum tree to string rules (Galley et al. 2004), then for each tree-to-string rule, we can easily extract its corresponding tree-to-tree sequence rule by introducing the virtual span root node. After that, we generate the composite rules by iteratively combining small rules. Fig. 6. Rule combination and virtual node removing 443 Please note that in generating composite rules, if the joint node is a virtual node, we have to recover the original link and remove this virtual node to avoid unnecessary ambiguity. Fig. 6 illustrates the combination process of rule 2 and rule 3 in Fig. 5. As a result, all of our extract rules do not contain any i</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight and Daniel Marcu. 2004. What’s in a translation rule? HLT-NAACL-04. 273-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest Reranking: Discriminative Parsing with Non-Local Features.</title>
<date>2008</date>
<pages>08--586</pages>
<contexts>
<context position="4108" citStr="Huang, 2008" startWordPosition="575" endWordPosition="576">ide (string to tree model). There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target frontier non-termina</context>
<context position="16653" citStr="Huang (2008)" startWordPosition="2693" endWordPosition="2694">lease note that the starting root node must be a single node, being either a normal forest node or a virtual “span root” node. The virtual “span root” node serves as the frontier node of upper rules and root node of the currently being extracted rules. Because we extract rules in a top-to-down manner, the necessary virtual “span root” node for current sub-forest has already been added into the global forest when extracting upper level rules. Figure 7. A source sub-tree in rule 2 Fig. 8. The corresponding target sub-forest for the tree of Figure 7. 3.5 Fractional Count of Rule Following Mi and Huang (2008) and Liu et al. (2009), we assign a fractional count to a rule to measure how likely it appears given the context of the forest pair. In following equation, “S” means source sub-tree, “T” means target sub-tree, “SF” is source forest and “TF” is the target forest. P(S, T |SF,TF) - P(S|SF, TF) * P(T|SF, TF) - P(S|SF) * P(T|TF) 444 The above equation means the fractional count of a source-target tree pair is just the product of each of their fractional count in corresponding forest context in following equation. ߙߚሺ݂ݎ݁ݏݐ ݎݐሻ where ߙ and ߚ are the outside and inside probabilities. In addition, </context>
<context position="28013" citStr="Huang, 2008" startWordPosition="4548" endWordPosition="4549">ation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction. For each source sub-tree, we set its height up to 3, width up to 7 and extract up to 10-best target structures. In decoding, we set the pruning threshold to 10 for the input source forest. Table 1 compares the performance in NIST 2003 data set of our method and several state-of-the-art systems as our baseline. 1) MOSES: phrase-based system (Koehn et al., 2007) 2) FT2S: forest-based tree-to-string system (Mi and Huang, 2008; Mi et al., 2008) 3) FT2T: forest-based tree-to-tree system (Liu et al., 2009). 4) FT2TS (1to1): our forest-based tree-to-tree sequence system, where 1to1 means only one-to-one frontier non-terminal node mapping is allowed, thus the system does not follow our non-isomorphic mapping framework. 5) FT2TS (1toN): our forest-based tree-to-tree sequence system that allows one-to-many frontier non-terminal node mapping by following our non-isomorphic mapping framework In addition, our proposed parallel searching space (PSS) technology can be applied to both tree to tree and tree to sequence systems.</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest Reranking: Discriminative Parsing with Non-Local Features. ACL-HLT-08. 586-594</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<date>2005</date>
<booktitle>Better k-best Parsing. IWPT-05.</booktitle>
<pages>53--64</pages>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing. IWPT-05. 53-64</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<pages>07--144</pages>
<contexts>
<context position="23701" citStr="Huang and Chiang, 2007" startWordPosition="3821" endWordPosition="3824">ts translation probability and syntax goodness. Thus, the complexity of the search space is reduced into the tree-to-string model level, while keeping the target language generation syntactic aware. More importantly, we thoroughly avoid those spurious ambiguities introduced by the tree-to-tree sequence rules. 4.4 Decoding with Parallel Hypothesis Spaces Fig. 11. Derivation path and derivation forest In this subsection, we show exactly how our decoder finds the best result from the parallel spaces. We generate hypothesis by traversing the coarse forest in the parallel spaces with cube-pruning (Huang and Chiang, 2007). Given a newly generated hypothesis, it is affiliated with a derivation path (tree) in the coarse forest and a group of derivation paths (sub-forest) in the finer forest. As shown in Fig. 11, the left part is the derivation path formed by a coarse hyper-edge, consisting the newly-generated sub-tree “X =&gt; X X X” connecting with three previously-generated sub paths while the right part is the derivation forest formed by newly-generated finer hyper-edges rooted at “VP” and “S”, and previously-generated sub-forests. In this paper, we use the sum of probabilities of all the derivation paths in the</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. ACL-07. 144–151</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs.</title>
<date>2001</date>
<contexts>
<context position="4094" citStr="Klein and Manning, 2001" startWordPosition="570" endWordPosition="574">onal Linguistics target side (string to tree model). There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target fronti</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. IWPT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for M-gram language modeling.</title>
<date>1995</date>
<tech>ICASSP-95,</tech>
<pages>181--184</pages>
<contexts>
<context position="26562" citStr="Kneser and Ney, 1995" startWordPosition="4310" endWordPosition="4313"> bottom up. 5 Experiment 5.1 Experimental Settings We evaluate our method on the Chinese-English translation task. We first carry out a series empirical study on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bi</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. ICASSP-95, 181-184</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<volume>07</volume>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="27949" citStr="Koehn et al., 2007" startWordPosition="4537" endWordPosition="4540">unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction. For each source sub-tree, we set its height up to 3, width up to 7 and extract up to 10-best target structures. In decoding, we set the pruning threshold to 10 for the input source forest. Table 1 compares the performance in NIST 2003 data set of our method and several state-of-the-art systems as our baseline. 1) MOSES: phrase-based system (Koehn et al., 2007) 2) FT2S: forest-based tree-to-string system (Mi and Huang, 2008; Mi et al., 2008) 3) FT2T: forest-based tree-to-tree system (Liu et al., 2009). 4) FT2TS (1to1): our forest-based tree-to-tree sequence system, where 1to1 means only one-to-one frontier non-terminal node mapping is allowed, thus the system does not follow our non-isomorphic mapping framework. 5) FT2TS (1toN): our forest-based tree-to-tree sequence system that allows one-to-many frontier non-terminal node mapping by following our non-isomorphic mapping framework In addition, our proposed parallel searching space (PSS) technology c</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. ACL-07. 177-180. (poster)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-to-String Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<volume>06</volume>
<pages>609--616</pages>
<contexts>
<context position="2441" citStr="Liu et al., 2006" startWordPosition="326" endWordPosition="329">rce structures into target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 &amp; 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rule</context>
<context position="6211" citStr="Liu et al. (2006)" startWordPosition="926" endWordPosition="929"> translation modeling. Yamada and Knight (2001) propose 1 A packed forest is a compact representation of a set of trees with sharing substructures; formally, it is defined as a triple a triple&lt; V, E, S &gt;, where V is non-terminal node set, E is hyper-edge set and S is leaf node set (i.e. all sentence words). Every node in V covers a consecutive sequence of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Translation. COLING-ACL-06. 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-String Statistical Translation Rules.</title>
<date>2007</date>
<volume>07</volume>
<pages>704--711</pages>
<contexts>
<context position="6273" citStr="Liu et al. (2007)" startWordPosition="936" endWordPosition="939">cked forest is a compact representation of a set of trees with sharing substructures; formally, it is defined as a triple a triple&lt; V, E, S &gt;, where V is non-terminal node set, E is hyper-edge set and S is leaf node set (i.e. all sentence words). Every node in V covers a consecutive sequence of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the </context>
<context position="7805" citStr="Liu et al., 2007" startWordPosition="1177" endWordPosition="1180">e one-to-one mapping between each non-terminal frontier node, and thus they suffer from the issue of rule coverage. On the other hand, due to the data sparseness issue and model coverage issue, previous tree-to-tree (Zhang et al., 2008; Liu et al., 2009) decoder has to rely solely on the span information or source side information to combine the target syntactic structures, without checking the compatibility of the merging nodes, in order not to fail many translation paths. Thus, this solution fails to effectively utilize the target structure information. To address this issue, tree sequence (Liu et al., 2007; Zhang et al., 2008) and virtual node (Zhang et al., 2009a) are two concepts with promising results reported. In this paper, with the help of these two concepts, we propose a novel framework to solve the one-to-many non-isomorphic mapping issue. In addition, our proposed solution of using target syntax information enables our forest-based tree-to-tree sequence translation decoding algorithm to not only capture bilingual forest information but also have almost the same complexity as forest-based tree-to-string translation. This reduces the time/space complexity exponentially. 3 Tree to Tree Se</context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu and Shouxun Lin. 2007. Forest-to-String Statistical Translation Rules. ACL-07. 704-711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
</authors>
<title>Yajuan Lü, Qun Liu.</title>
<date>2009</date>
<pages>558--566</pages>
<marker>Liu, 2009</marker>
<rawString>Yang Liu, Yajuan Lü, Qun Liu. 2009. Improving Tree-to-Tree Translation with Packed Forests. ACL-09. 558-566</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Forest-based translation.</title>
<date>2008</date>
<pages>08--192</pages>
<contexts>
<context position="4212" citStr="Mi et al., 2008" startWordPosition="592" endWordPosition="595">nd the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target frontier non-terminal nodes. For the second issue, we propose a technology to model the combination task by considering both</context>
<context position="14059" citStr="Mi et al. (2008)" startWordPosition="2243" endWordPosition="2246">After that, we generate the composite rules by iteratively combining small rules. Fig. 6. Rule combination and virtual node removing 443 Please note that in generating composite rules, if the joint node is a virtual node, we have to recover the original link and remove this virtual node to avoid unnecessary ambiguity. Fig. 6 illustrates the combination process of rule 2 and rule 3 in Fig. 5. As a result, all of our extract rules do not contain any internal virtual nodes. 3.4 Rule Extraction in Forest Context In forest pair context, we also first generate the minimum tree-to-string rule set as Mi et al. (2008), and for each tree-to-string rule, we find its corresponding tree-to-tree sequence rules, and then do rule composition. In tree pair context, given a tree-to-string rule, there is one and only one corresponding tree-to-tree sequence rule. But in forest pair context, given one such tree-to-string rule, there are many corresponding tree-to-tree sequence rules. All these sub-trees form one or more sub-forests2 of the entire big target forest. If we can identify the sub-forests, i.e., all of the hyper-edges of the sub-forests, we can retrieve all the sub-trees from the sub-forests as the target s</context>
<context position="17774" citStr="Mi et al., 2008" startWordPosition="2891" endWordPosition="2894">equation. ߙߚሺ݂ݎ݁ݏݐ ݎݐሻ where ߙ and ߚ are the outside and inside probabilities. In addition, if a sub-tree root is a virtual node (formed by a root node sequence), then we use following equation to approximate the outside probability of the virtual node. #  ೞ  ೞ ߙሺ݊݀݁ ݏ݁ݍݑ݁݊ܿ݁ ݊ݏሻ ൌ ඨ ෑ ߙሺ݊ሻ  א ௦ 4 Decoding 4.1 Traditional Forest-based Decoding A typical translation process of a forest-based system is to first convert the source packed forest into a target translation forest, and then apply search algorithm to find the best translation result from this target translation forest (Mi et al., 2008). For the tree-to-string model, the forest conversion process is as following: given an input packed forest, we do pattern matching (Zhang et al., 2009b) with the source side structures in the rule set. For each matched rule, we establish its target side as a hyper-edge in the target forest. Fig. 9. A forest conversion step in a tree to string model Fig. 9 exemplifies a conversion step in the tree to string model. A sub-tree structure with two hyper-edge “VP[2,4] =&gt; ADVP[2,2] VP[3,4]” and “VP[3,4] =&gt; ADVP[3,4] VP[4,4]” is converted into a target hyper-edge “X-VP[2,4] =&gt; X-ADVP[3,3] X-ADVP[2,2]</context>
<context position="27526" citStr="Mi et al., 2008" startWordPosition="4461" endWordPosition="4464">ficance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the average number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction. For each source sub-tree, we set its height up to 3, width up to 7 and extract up to 10-best target structures. In decoding, we set the pruning threshold to 10 for the input source forest. Table 1 compares the performance in NIST 2003 data set of our method and several state-of-the-art systems as our baseline. 1) MOSES: phrase-based system (Koehn et al., 2007) 2) FT2S: forest-based tree-to-string system (Mi and Huang, 2008; Mi et al., 2008) 3) FT2T: forest-based tree-to-tree system (Liu et al., 2009). 4) FT2TS (1to1): our forest-base</context>
</contexts>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. ACL-HLT-08. 192-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based Translation Rule Extraction.</title>
<date>2008</date>
<volume>08</volume>
<pages>206--214</pages>
<contexts>
<context position="4181" citStr="Mi and Huang, 2008" startWordPosition="586" endWordPosition="589">both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into any number of target frontier non-terminal nodes. For the second issue, we propose a technology to model the combi</context>
<context position="16653" citStr="Mi and Huang (2008)" startWordPosition="2691" endWordPosition="2694">ings. Please note that the starting root node must be a single node, being either a normal forest node or a virtual “span root” node. The virtual “span root” node serves as the frontier node of upper rules and root node of the currently being extracted rules. Because we extract rules in a top-to-down manner, the necessary virtual “span root” node for current sub-forest has already been added into the global forest when extracting upper level rules. Figure 7. A source sub-tree in rule 2 Fig. 8. The corresponding target sub-forest for the tree of Figure 7. 3.5 Fractional Count of Rule Following Mi and Huang (2008) and Liu et al. (2009), we assign a fractional count to a rule to measure how likely it appears given the context of the forest pair. In following equation, “S” means source sub-tree, “T” means target sub-tree, “SF” is source forest and “TF” is the target forest. P(S, T |SF,TF) - P(S|SF, TF) * P(T|SF, TF) - P(S|SF) * P(T|TF) 444 The above equation means the fractional count of a source-target tree pair is just the product of each of their fractional count in corresponding forest context in following equation. ߙߚሺ݂ݎ݁ݏݐ ݎݐሻ where ߙ and ߚ are the outside and inside probabilities. In addition, </context>
<context position="28013" citStr="Mi and Huang, 2008" startWordPosition="4546" endWordPosition="4549"> translation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction. For each source sub-tree, we set its height up to 3, width up to 7 and extract up to 10-best target structures. In decoding, we set the pruning threshold to 10 for the input source forest. Table 1 compares the performance in NIST 2003 data set of our method and several state-of-the-art systems as our baseline. 1) MOSES: phrase-based system (Koehn et al., 2007) 2) FT2S: forest-based tree-to-string system (Mi and Huang, 2008; Mi et al., 2008) 3) FT2T: forest-based tree-to-tree system (Liu et al., 2009). 4) FT2TS (1to1): our forest-based tree-to-tree sequence system, where 1to1 means only one-to-one frontier non-terminal node mapping is allowed, thus the system does not follow our non-isomorphic mapping framework. 5) FT2TS (1toN): our forest-based tree-to-tree sequence system that allows one-to-many frontier non-terminal node mapping by following our non-isomorphic mapping framework In addition, our proposed parallel searching space (PSS) technology can be applied to both tree to tree and tree to sequence systems.</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. EMNLP-08. 206-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<pages>03--160</pages>
<contexts>
<context position="26837" citStr="Och, 2003" startWordPosition="4356" endWordPosition="4357"> of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the average number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. ACL-03. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics.</title>
<date>2003</date>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="26718" citStr="Och and Ney, 2003" startWordPosition="4337" endWordPosition="4340">on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the averag</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics. 29(1) 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<pages>02--311</pages>
<contexts>
<context position="27061" citStr="Papineni et al., 2002" startWordPosition="4389" endWordPosition="4393">t side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the average number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extraction. For each source sub-tree, we set its height up to 3, width up to 7 and ext</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. ACL-02. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<pages>02--901</pages>
<contexts>
<context position="26504" citStr="Stolcke, 2002" startWordPosition="4304" endWordPosition="4305"> accumulate the inside probability dynamically from bottom up. 5 Experiment 5.1 Experimental Settings We evaluate our method on the Chinese-English translation task. We first carry out a series empirical study on a set of parallel data with 30K sentence pairs, and then do experiment on a larger data set to ensure that the effectiveness of our method is consistent across data set of different size. We use the NIST 2002 test set as our dev set, and NIST 2003 and NIST 2005 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems:</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. ICSLP-02. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An Efficient Augmented-Context-Free Parsing Algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics</journal>
<volume>13</volume>
<issue>1</issue>
<pages>31--46</pages>
<contexts>
<context position="4069" citStr="Tomita, 1987" startWordPosition="568" endWordPosition="569"> for Computational Linguistics target side (string to tree model). There is no well study in considering both the source side information and the compatibility between different target syntactic structures during combination. In addition, it is well known that the traditional tree-to-tree models suffer heavily from the data sparseness issue in training and the spurious-ambiguity translation path issue (the same translation with different syntactic structures) in decoding. In addition, because of the performance limitation of automatic syntactic parser, researchers propose using packed forest (Tomita, 1987; Klein and Manning, 2001; Huang, 2008)1 instead of 1-best parse tree to carry out training (Mi and Huang, 2008) and decoding (Mi et al., 2008) in order to reduce the side effect caused by parsing errors of the one-best tree. However, when we apply the tree-to-tree model to the bilingual forest structures, both training and decoding become very complicated. In this paper, to address the first issue, we propose a framework to model the non-isomorphic translation process from source tree fragment to target tree sequence, allowing any one source frontier non-terminal node to be translated into an</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Masaru Tomita. 1987. An Efficient Augmented-Context-Free Parsing Algorithm. Computational Linguistics 13(1-2): 31-46</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Non-projective Parsing for Statistical Machine Translation.</title>
<date>2009</date>
<volume>2009</volume>
<pages>200--209</pages>
<contexts>
<context position="6447" citStr="Carreras and Collins (2009)" startWordPosition="965" endWordPosition="968">al node set, E is hyper-edge set and S is leaf node set (i.e. all sentence words). Every node in V covers a consecutive sequence of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling. Liu et al. (2009) propose efficient algo</context>
</contexts>
<marker>Carreras, Collins, 2009</marker>
<rawString>Xavier Carreras and Michael Collins. 2009. Non-projective Parsing for Statistical Machine Translation. EMNLP-2009. 200-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A Syntax-Based Statistical Translation Model.</title>
<date>2001</date>
<volume>01</volume>
<pages>523--530</pages>
<contexts>
<context position="2466" citStr="Yamada and Knight, 2001" startWordPosition="330" endWordPosition="333">o target structures iteratively and recursively while from decoding viewpoint a syntax-based system segments an input tree/forest into many sub-fragments, translates each of them separately, combines the translated sub-fragments and then finds out the best combinations. Therefore, from bilingual viewpoint, we face two fundamental problems: the mapping between bilingual structures and the way of carrying out the target structures combination. For the first issue, a number of models have been proposed to model the structure mapping between tree and string (Galley et al., 2004; Liu et al., 2006; Yamada and Knight, 2001; DeNeefe and Knight, 2009) and between tree and tree (Eisner, 2003; Zhang et al., 2007 &amp; 2008; Liu et al., 2009). However, one of the major challenges is that all the current models only allow one-to-one mapping from one source frontier non-terminal node (Galley et al., 2004) to one target frontier non-terminal node in a bilingual translation rule. Therefore, all those translation equivalents with one-to-many frontier non-terminal node mapping cannot be covered by the current state-of-the-art models. This may largely compromise the modeling ability of translation rules. For the second problem</context>
<context position="5641" citStr="Yamada and Knight (2001)" startWordPosition="817" endWordPosition="820">ish translation tasks show that our methods significantly outperform the forest-based tree to string and previous tree to tree models as well as the phrase-based model. The remaining of the paper is organized as following. Section 2 reviews the related work. In section 3 and section 4, we discuss the proposed forest-based rule extraction (non-isomorphic mapping) and decoding algorithms (target syntax information usage). Finally we report the experimental results in section 5 and conclude the paper in section 6. 2 Related Work Much effort has been done in the syntax-based translation modeling. Yamada and Knight (2001) propose 1 A packed forest is a compact representation of a set of trees with sharing substructures; formally, it is defined as a triple a triple&lt; V, E, S &gt;, where V is non-terminal node set, E is hyper-edge set and S is leaf node set (i.e. all sentence words). Every node in V covers a consecutive sequence of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string tran</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A Syntax-Based Statistical Translation Model. ACL-01. 523-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Aiti Aw and Chew Lim Tan.</title>
<date>2009</date>
<volume>09</volume>
<pages>172--180</pages>
<contexts>
<context position="6566" citStr="Zhang et al. (2009" startWordPosition="983" endWordPosition="986"> of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling. Liu et al. (2009) propose efficient algorithms for tree-to-tree model in the forest-based training and decoding scheme. One common limitation of the above work</context>
<context position="7863" citStr="Zhang et al., 2009" startWordPosition="1188" endWordPosition="1191">node, and thus they suffer from the issue of rule coverage. On the other hand, due to the data sparseness issue and model coverage issue, previous tree-to-tree (Zhang et al., 2008; Liu et al., 2009) decoder has to rely solely on the span information or source side information to combine the target syntactic structures, without checking the compatibility of the merging nodes, in order not to fail many translation paths. Thus, this solution fails to effectively utilize the target structure information. To address this issue, tree sequence (Liu et al., 2007; Zhang et al., 2008) and virtual node (Zhang et al., 2009a) are two concepts with promising results reported. In this paper, with the help of these two concepts, we propose a novel framework to solve the one-to-many non-isomorphic mapping issue. In addition, our proposed solution of using target syntax information enables our forest-based tree-to-tree sequence translation decoding algorithm to not only capture bilingual forest information but also have almost the same complexity as forest-based tree-to-string translation. This reduces the time/space complexity exponentially. 3 Tree to Tree Sequence Rules The motivation of introducing tree to tree se</context>
<context position="10235" citStr="Zhang et al. 2009" startWordPosition="1582" endWordPosition="1585">e. For example, in Fig. 3, the source tree node “ADVP” in solid eclipse is translated to “try hard to” in the target sentence, but there is no corresponding sub-tree covering and only covering it in the target side. Given the example rules in Fig. 2, what are their corresponding rules with target syntax information? The answer is that the previous tree or tree sequence-based models fail to model the Rule 1 and Rule 2 at Fig. 2, since at frontier node level they only allow one-to-one node mapping but the solution is one-to-many non-terminal frontier node mapping. The concept of “virtual node” (Zhang et al. 2009a) is a solution to this issue. To facilitate discussion, we first introduce three concepts. Fig. 3. A word-aligned bi-parsed tree Fig. 4. A restructured tree with a virtual span root • Def. 1. The “node sequence” is a sequence of nodes (either leaf or internal nodes) covering a consecutive span. For example, in Fig 3, “VBP RB TO” and “VBP ADVP TO” are two “node sequence” covering the same span “try hard to”. VP ADVP VP 442 • Def. 2. The “root node sequence” of a span is such a node sequence that any node in this sequence could not be a child of a node in other node sequence of the span. Intui</context>
<context position="12475" citStr="Zhang et al., 2009" startWordPosition="1981" endWordPosition="1984">point. After adding the “span root” as joint point, we are able to ensure that each frontier source node has only one corresponding target node, then we can use any traditional rule extraction algorithm to extract rules, including those rules with one-to-many non-terminal frontier mappings. Fig. 5. Tree-to-tree sequence rules Fig. 5 lists the corresponding rules with target structure information of the tree-to-string rules in Fig 2. All the three rules cannot be extracted by previous tree-to-tree mapping methods (Liu et al., 2009). The previous tree-sequence-based methods (Zhang et al., 2008; Zhang et al., 2009a) can extracted rule 3 since they allow one-to-many mapping in root node level. But they cannot extract rule 1 and rule 2. Therefore, for any tree-to-string rule, our method can always find the corresponding tree-to-tree sequence rule. As a result, our rule coverage is the same as tree-to-string framework while our rules contain more informative target syntax information. Later we will show that using our decoding algorithm the tree-to-tree sequence search space is exponentially reduced to the same as tree-to-string search space. That is to say, we do not need to worry about the exponential s</context>
<context position="17925" citStr="Zhang et al., 2009" startWordPosition="2915" endWordPosition="2918"> node sequence), then we use following equation to approximate the outside probability of the virtual node. #  ೞ  ೞ ߙሺ݊݀݁ ݏ݁ݍݑ݁݊ܿ݁ ݊ݏሻ ൌ ඨ ෑ ߙሺ݊ሻ  א ௦ 4 Decoding 4.1 Traditional Forest-based Decoding A typical translation process of a forest-based system is to first convert the source packed forest into a target translation forest, and then apply search algorithm to find the best translation result from this target translation forest (Mi et al., 2008). For the tree-to-string model, the forest conversion process is as following: given an input packed forest, we do pattern matching (Zhang et al., 2009b) with the source side structures in the rule set. For each matched rule, we establish its target side as a hyper-edge in the target forest. Fig. 9. A forest conversion step in a tree to string model Fig. 9 exemplifies a conversion step in the tree to string model. A sub-tree structure with two hyper-edge “VP[2,4] =&gt; ADVP[2,2] VP[3,4]” and “VP[3,4] =&gt; ADVP[3,4] VP[4,4]” is converted into a target hyper-edge “X-VP[2,4] =&gt; X-ADVP[3,3] X-ADVP[2,2] X-VP[4,4] ”. The node “X-VP[4,4]” in the target forest means that its syntactic label in target forest is “X” and it is translated from the source nod</context>
</contexts>
<marker>Zhang, Zhang, Li, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and Chew Lim Tan. 2009a. Forest-based Tree Sequence to String Translation Model. ACL-IJCNLP-09. 172-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
</authors>
<title>Fast Translation Rule Matching for Syntax-based Statistical Machine Translation.</title>
<date>2009</date>
<volume>09</volume>
<pages>1037--1045</pages>
<contexts>
<context position="6566" citStr="Zhang et al. (2009" startWordPosition="983" endWordPosition="986"> of leaf, every hyper-edge in E connect the father node to its children nodes as in a tree. Figure 8 is a packed forest contains two trees. a string to tree model. Galley et al. (2004) propose the GHKM scheme to model the string-to-tree mapping. Liu et al. (2006) propose a tree-to-string translation model. Liu et al. (2007) propose the tree sequence to string model to capture rules covered by continuous sequence of trees. Shieber (2007), DeNeefe and Knight (2009) and Carreras and Collins (2009) propose synchronous tree adjoin grammar to capture more tree-string mapping beyond the GHKM scheme. Zhang et al. (2009a) propose the concept of virtual node to reform a tree sequence as a tree, and design efficient algorithms for tree sequence model in forest context. All these works only consider either the source side or the target side syntax information. To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. Zhang et al. (2008) propose tree sequence-based tree-to-tree modeling. Liu et al. (2009) propose efficient algorithms for tree-to-tree model in the forest-based training and decoding scheme. One common limitation of the above work</context>
<context position="7863" citStr="Zhang et al., 2009" startWordPosition="1188" endWordPosition="1191">node, and thus they suffer from the issue of rule coverage. On the other hand, due to the data sparseness issue and model coverage issue, previous tree-to-tree (Zhang et al., 2008; Liu et al., 2009) decoder has to rely solely on the span information or source side information to combine the target syntactic structures, without checking the compatibility of the merging nodes, in order not to fail many translation paths. Thus, this solution fails to effectively utilize the target structure information. To address this issue, tree sequence (Liu et al., 2007; Zhang et al., 2008) and virtual node (Zhang et al., 2009a) are two concepts with promising results reported. In this paper, with the help of these two concepts, we propose a novel framework to solve the one-to-many non-isomorphic mapping issue. In addition, our proposed solution of using target syntax information enables our forest-based tree-to-tree sequence translation decoding algorithm to not only capture bilingual forest information but also have almost the same complexity as forest-based tree-to-string translation. This reduces the time/space complexity exponentially. 3 Tree to Tree Sequence Rules The motivation of introducing tree to tree se</context>
<context position="10235" citStr="Zhang et al. 2009" startWordPosition="1582" endWordPosition="1585">e. For example, in Fig. 3, the source tree node “ADVP” in solid eclipse is translated to “try hard to” in the target sentence, but there is no corresponding sub-tree covering and only covering it in the target side. Given the example rules in Fig. 2, what are their corresponding rules with target syntax information? The answer is that the previous tree or tree sequence-based models fail to model the Rule 1 and Rule 2 at Fig. 2, since at frontier node level they only allow one-to-one node mapping but the solution is one-to-many non-terminal frontier node mapping. The concept of “virtual node” (Zhang et al. 2009a) is a solution to this issue. To facilitate discussion, we first introduce three concepts. Fig. 3. A word-aligned bi-parsed tree Fig. 4. A restructured tree with a virtual span root • Def. 1. The “node sequence” is a sequence of nodes (either leaf or internal nodes) covering a consecutive span. For example, in Fig 3, “VBP RB TO” and “VBP ADVP TO” are two “node sequence” covering the same span “try hard to”. VP ADVP VP 442 • Def. 2. The “root node sequence” of a span is such a node sequence that any node in this sequence could not be a child of a node in other node sequence of the span. Intui</context>
<context position="12475" citStr="Zhang et al., 2009" startWordPosition="1981" endWordPosition="1984">point. After adding the “span root” as joint point, we are able to ensure that each frontier source node has only one corresponding target node, then we can use any traditional rule extraction algorithm to extract rules, including those rules with one-to-many non-terminal frontier mappings. Fig. 5. Tree-to-tree sequence rules Fig. 5 lists the corresponding rules with target structure information of the tree-to-string rules in Fig 2. All the three rules cannot be extracted by previous tree-to-tree mapping methods (Liu et al., 2009). The previous tree-sequence-based methods (Zhang et al., 2008; Zhang et al., 2009a) can extracted rule 3 since they allow one-to-many mapping in root node level. But they cannot extract rule 1 and rule 2. Therefore, for any tree-to-string rule, our method can always find the corresponding tree-to-tree sequence rule. As a result, our rule coverage is the same as tree-to-string framework while our rules contain more informative target syntax information. Later we will show that using our decoding algorithm the tree-to-tree sequence search space is exponentially reduced to the same as tree-to-string search space. That is to say, we do not need to worry about the exponential s</context>
<context position="17925" citStr="Zhang et al., 2009" startWordPosition="2915" endWordPosition="2918"> node sequence), then we use following equation to approximate the outside probability of the virtual node. #  ೞ  ೞ ߙሺ݊݀݁ ݏ݁ݍݑ݁݊ܿ݁ ݊ݏሻ ൌ ඨ ෑ ߙሺ݊ሻ  א ௦ 4 Decoding 4.1 Traditional Forest-based Decoding A typical translation process of a forest-based system is to first convert the source packed forest into a target translation forest, and then apply search algorithm to find the best translation result from this target translation forest (Mi et al., 2008). For the tree-to-string model, the forest conversion process is as following: given an input packed forest, we do pattern matching (Zhang et al., 2009b) with the source side structures in the rule set. For each matched rule, we establish its target side as a hyper-edge in the target forest. Fig. 9. A forest conversion step in a tree to string model Fig. 9 exemplifies a conversion step in the tree to string model. A sub-tree structure with two hyper-edge “VP[2,4] =&gt; ADVP[2,2] VP[3,4]” and “VP[3,4] =&gt; ADVP[3,4] VP[4,4]” is converted into a target hyper-edge “X-VP[2,4] =&gt; X-ADVP[3,3] X-ADVP[2,2] X-VP[4,4] ”. The node “X-VP[4,4]” in the target forest means that its syntactic label in target forest is “X” and it is translated from the source nod</context>
</contexts>
<marker>Zhang, Zhang, Li, Tan, 2009</marker>
<rawString>Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. 2009b. Fast Translation Rule Matching for Syntax-based Statistical Machine Translation. EMNLP-09. 1037-1045.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
</authors>
<title>Ai Ti Aw, Jun Sun, Chew Lim Tan and Sheng Li.</title>
<date>2007</date>
<pages>07--535</pages>
<marker>Zhang, Jiang, 2007</marker>
<rawString>Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew Lim Tan and Sheng Li. 2007. A Tree-to-Tree Alignment-based model for statistical Machine translation. MT-Summit-07. 535-542</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
</authors>
<title>Hongfei Jiang, Aiti Aw,</title>
<date>2008</date>
<volume>08</volume>
<pages>559--567</pages>
<location>Haizhou Li, Chew Lim Tan, Sheng Li.</location>
<marker>Zhang, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. ACL-HLT-08. 559-567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?</title>
<date>2004</date>
<pages>04--2051</pages>
<contexts>
<context position="26980" citStr="Zhang et al, 2004" startWordPosition="4377" endWordPosition="4380">05 test sets as our test set. A 3-gram language model is trained on the target side of the training data by the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995). We train Charniak’s parser (Charniak, 2000) on CTB5.0 for Chinese and ETB3.0 for English and modify it to output packed forest. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate m-to-n word alignments. For the MER training (Och, 2003), Koehn’s MER trainer (Koehn, 2007) is modified for our system. For significance test, we use Zhang et al.’s implementation (Zhang et al, 2004). Our evaluation metrics is case-sensitive closest BLEU-4 (Papineni et al., 2002). We use following features in our systems: 1) bidirectional tree-to-tree sequence probability, 2) bidirectional tree-to-string probability, 3) bidirectional lexical translation probability, 4) target language model, 5) source tree probability 6) the average number of unmatched nodes in the target forest. 7) the length of the target translation, 8) the number of glue rules used. 5.2 Empirical Study on Small Data We set forest pruning threshold (Mi et al., 2008) to 8 on both source and target forests for rule extra</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, Alex Waibel. 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? LREC-04. 2051-2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>