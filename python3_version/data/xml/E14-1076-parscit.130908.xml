<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000119">
<title confidence="0.994428">
Hybrid text simplification using synchronous dependency grammars with
hand-written and automatically harvested rules
</title>
<author confidence="0.946778">
Advaith Siddharthan
</author>
<affiliation confidence="0.966536">
Computing Science
University of Aberdeen
</affiliation>
<note confidence="0.456567">
UK
</note>
<email confidence="0.996017">
advaith@abdn.ac.uk
</email>
<sectionHeader confidence="0.993811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980714285714">
We present an approach to text simplifi-
cation based on synchronous dependency
grammars. The higher level of abstraction
afforded by dependency representations
allows for a linguistically sound treatment
of complex constructs requiring reorder-
ing and morphological change, such as
conversion of passive voice to active. We
present a synchronous grammar formalism
in which it is easy to write rules by hand
and also acquire them automatically from
dependency parses of aligned English and
Simple English sentences. The grammar
formalism is optimised for monolingual
translation in that it reuses ordering infor-
mation from the source sentence where ap-
propriate. We demonstrate the superiority
of our approach over a leading contempo-
rary system based on quasi-synchronous
tree substitution grammars, both in terms
of expressivity and performance.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999791625">
Text simplification is sometimes defined as the
process of reducing the grammatical and lexi-
cal complexity of a text, while still retaining the
original information content and meaning. The
main goal of simplification is to make informa-
tion more accessible to the large numbers of peo-
ple with reduced literacy. The National Lit-
eracy Trust (http://www.literacytrust.org.uk) esti-
mates that one in six adults in the UK have poor
literacy skills. The situation is often worse in de-
veloping countries. Alu´ısio et al. (2008) report
that 68% of Brazilians between 15 and 64 years
who have studied up to 4 years only reach the rudi-
mentary level of literacy, and even among those
who have studied for 8 years, only a quarter can
be considered fully literate. While there is a large
</bodyText>
<note confidence="0.9339115">
M. A. Angrosh
Computing Science
University of Aberdeen
UK
</note>
<email confidence="0.977296">
angroshmandya@abdn.ac.uk
</email>
<bodyText confidence="0.99995996">
body of evidence that manual text simplification
is an effective intervention (Anderson and Free-
body, 1981; L’Allier, 1980; Beck et al., 1991;
Anderson and Davison, 1988; Linderholm et al.,
2000; Kamalski et al., 2008), there has till recently
been little work on automatic simplification. The
pace of research has picked up in recent years
though, with many teams applying machine trans-
lation approaches to perform “monolingual trans-
lation” from English to simplified English. The
goals of this paper are to (1) identify the limita-
tions of recently published approaches to text sim-
plification with regard to their coverage of linguis-
tic constructs, (2) to describe an approach based
on synchronous grammars operating on typed de-
pendency representations that permits a more so-
phisticated handling of many linguistic constructs,
and (3) to present a hybrid system that combines a
small set of hand written grammar rules for purely
syntactic constructs with a much larger set of auto-
matically acquired rules for lexicalised constructs
in one synchronous formalism.
We summarise work on text simplification in
Section 2, before describing our method in Sec-
tion 3 and presenting our results in Section 4.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9997355">
There are two largely distinct bodies of work on
automatic text simplification – those that use hand-
crafted rules, and those that apply machine trans-
lation approaches.
</bodyText>
<subsectionHeader confidence="0.928473">
2.1 Hand-crafted text simplification systems
</subsectionHeader>
<bodyText confidence="0.999799375">
The first body of work uses hand-crafted rules
to perform syntactic simplification operations
(e.g., splitting coordinated and subordinated
clauses, and disembedding apposition and relative
clauses). Some early systems (Chandrasekar et
al., 1996; Siddharthan, 2002) used flat represen-
tations (chunked and part-of-speech tagged text).
More commonly, text simplification systems use
</bodyText>
<page confidence="0.93929">
722
</page>
<note confidence="0.9931915">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.986826875">
hand crafted rules that apply to hierarchical rep-
resentations, including constituency-based parses
(Canning, 2002; Candido Jr et al., 2009; De Belder
and Moens, 2010) and dependency parses (Bott et
al., 2012; Siddharthan, 2010; Siddharthan, 2011).
For languages without corpora of simplified texts,
hand crafted systems are typically the only avail-
able alternative.
</bodyText>
<subsectionHeader confidence="0.7312515">
2.2 Text simplification as monolingual
translation
</subsectionHeader>
<bodyText confidence="0.999424068965517">
Recent years have seen the increased application
of machine translation approaches to text simpli-
fication, often referred to as “monolingual transla-
tion”, and driven by the new availability of cor-
pora of simplified texts such as Simple English
Wikipedia (SEW).
Wubben et al. (2012) and Coster and Kauchak
(2011) apply Phrase Based Machine Translation
(PBMT) to the task of text simplification. PMBT
can only perform a small set of simplification op-
erations, such as lexical substitution, deletion and
simple paraphrase. They are not well suited for
reordering or splitting operations. Specifically,
the syntactic simplification operations that hand-
crafted systems focus on are out of scope.
Zhu et al. (2010) in contrast present an approach
based on syntax-based SMT (Yamada and Knight,
2001). Their translation model encodes proba-
bilities for four specific rewrite operations on the
parse trees of the input sentences: substitution, re-
ordering, splitting, and deletion. Splitting is en-
coded as two probabilities: A segmentation table
stores probabilities of sentence splitting at particu-
lar words (e.g., which). A completion table stores
probabilities of the splitting word to be deleted
from the translation, and for the governing phrase
to be inserted to complete the sentence. This al-
lows the translation model to handle constructs
such as relative clauses and apposition.
Dras (1999) was the first to apply synchronous
grammars to monolingual tasks. His approach is
to map between two TAG grammars using a Gen-
eralised Synchronous TAG formalism, and to use
Integer Programming to generate a text that sat-
isfies the externally imposed constraints (such as
length or readability) using minimal paraphras-
ing. Woodsend and Lapata (2011) further de-
velop this line of research. Their model is based
on quasi-synchronous grammar (Smith and Eis-
ner, 2006) and integer linear programming. Quasi-
synchronous grammars, like the Generalised Syn-
chronous TAGs of Dras (1999), aims to relax
the isomorphism constraints of synchronous gram-
mars, in this case by generating a loose alignment
between parse trees. The Woodsend and Lapata
(2011) model is trained on two different datasets:
one containing alignments between sentences in
Wikipedia and English Simple Wikipedia, and one
containing alignments between edits in the revi-
sion history of Simple Wikipedia. The latter per-
forms best in their study, and also achieves bet-
ter scores than the Zhu et al. (2010) system, both
when evaluated using BLEU, and on human eval-
uations of simplicity, grammaticality and meaning
preservation. We will directly compare our ap-
proach to Woodsend and Lapata (2011), as this is
the best performing contemporary system that has
the same linguistic scope as ours.
</bodyText>
<subsectionHeader confidence="0.993579">
2.3 Formalisms and linguistic coverage
</subsectionHeader>
<bodyText confidence="0.998926903225807">
The systems summarised above differ primarily
in the level of linguistic knowledge they encode.
PBMT systems use the least knowledge, and as
such are ill equipped to to handle simplifications
that require morphological changes, syntactic re-
ordering or sentence splitting.
Syntax based approaches use syntactic knowl-
edge. However, both Zhu et al. (2010) and Wood-
send and Lapata (2011) use the Stanford Parser
(Klein and Manning, 2003) for syntactic structure,
and this representation lacks morphological infor-
mation. This means that some simplification op-
erations such as voice conversion are not handled
well. For example, to simplify “trains are liked by
John” to “John likes trains”, besides deleting aux-
iliaries and reordering the arguments of the verb
“like”, the verb also needs to agree in number with
the new subject (“John”), and take the tense of the
auxiliary verb (“are”).
The grammar acquisition process leads to fur-
ther problems. From an aligned pair “John, who
was tired, went to sleep.” and “John was tired. He
went to sleep.”, systems would learn a simplifica-
tion rule that introduces the pronoun “He”. The
governing syntax for this rule is the verb “went”;
hence, “Susan, who was tired, went to sleep.”
might later get simplified as “Susan was tired. He
went to sleep.”.
Hand-crafted systems have an advantage here.
Such systems would typically use rules that du-
plicate the noun phrase, generating “John was
</bodyText>
<page confidence="0.997499">
723
</page>
<bodyText confidence="0.999982826086956">
tired. John went to sleep.” and “Susan was
tired. Susan went to sleep.” Systems such as Sid-
dharthan (2011) use transformation rules that en-
code morphological changes as well as deletions,
re-orderings, substitutions and sentence splitting,
and are well suited to handle the voice conversion
example above. On the other hand, hand-crafted
systems are limited in scope to syntactic simplifi-
cation. While purely syntactic rules can be written
manually, there are too many lexico-syntactic and
lexical simplifications to enumerate by hand.
In this paper, we present a hybrid text simpli-
fication system that combines manually written
synchronous grammars for common syntactic sim-
plifications with a much larger automatically ac-
quired synchronous grammar for lexicalised con-
structs. Our framework, using dependency repre-
sentations, is better suited to text simplification.
We demonstrate that the higher level of abstrac-
tion in dependency parses allows for linguistically
correct rules for complex operations such as voice
conversion, while also providing a better model of
context for lexical simplification.
</bodyText>
<sectionHeader confidence="0.982354" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.99970665">
We describe a text simplification system that uses
a synchronous grammar defined over typed depen-
dencies. We demonstrate that this has specific ad-
vantages over previous work on text simplifica-
tion: (1) it allows for better linguistic modelling
of simplification operations that require morpho-
logical changes, (2) the higher level of abstraction
makes it easy to write and read grammar rules;
thus common syntactic operations (such as con-
version of passive to active voice) can be handled
in this framework through accurate hand-written
rules, and (3) It is easier and more elegant to au-
tomatically acquire a synchronous grammar from
data, compared to synchronous grammars based
on constituency-parses. In this section we de-
scribe our framework and text simplification sys-
tem in more detail; then, in section 4, we report an
evaluation that compares our system against a hu-
man simplification and the Woodsend and Lapata
(2011) system.
</bodyText>
<subsectionHeader confidence="0.9622715">
3.1 Synchronous dependency insertion
grammars
</subsectionHeader>
<bodyText confidence="0.99801747826087">
Ding and Palmer (2005) introduce the notion of
a Synchronous Dependency Insertion Grammar
(SDIG) as a tree substitution grammar defined on
dependency trees. They define elementary trees
(ETs) to be sub-sentential dependency structures
containing one or more lexical items. The SDIG
formalism assumes that the isomorphism of the
two syntactic structures is at the ET level, thus al-
lowing for non-isomorphic tree to tree mapping
at the sentence level. We base our approach to
text simplification on SDIGs, but the formalism
is adapted for the monolingual task, and the rules
are written in a formalism that is suited to writ-
ing rules by hand as well as automatically acquir-
ing rules from aligned sentences. Our system fol-
lows the architecture proposed in Ding and Palmer
(2005), reproduced in Fig. 1. In this paper, we
will present the ET Transfer component as a set of
transformation rules. The rest of Section 3 will fo-
cus on the linguistic knowledge we need to encode
in these rules, the method for automatic acquisi-
tion of rules from a corpus of aligned sentences,
and the generation process.
</bodyText>
<figure confidence="0.9846734">
Input Sentence −+ Dependency Parse −+ Source ETs
1
ET Transfer
1
Output Sentences *--− Generation *--− Target ETs
</figure>
<figureCaption confidence="0.999973">
Figure 1: System Architecture
</figureCaption>
<subsectionHeader confidence="0.984021">
3.2 Extracting synchronous grammars from
aligned sentences
</subsectionHeader>
<bodyText confidence="0.999413166666667">
To acquire a synchronous grammar from depen-
dency parses of aligned English and simple En-
glish sentences, we just need to identify the dif-
ferences. For example, consider two aligned sen-
tences from the aligned corpus described in Wood-
send and Lapata (2011):
</bodyText>
<listItem confidence="0.998975">
1. (a) Also, lichen fungi can reproduce sexu-
ally, producing spores.
(b) Also, lichen fungi can reproduce sexu-
ally by producing spores.
</listItem>
<bodyText confidence="0.999736285714286">
An automatic comparison of the dependency
parses for the two sentences (using the Stanford
Parser, and ignoring punctuation for ease of pre-
sentation) reveals that there are two typed depen-
dencies that occur only in the parse of the first sen-
tence, and two that occur only in the parse of the
second sentence (in italics):
</bodyText>
<page confidence="0.986772">
724
</page>
<figure confidence="0.807651">
reproduce reproduce
</figure>
<figureCaption confidence="0.99905">
Figure 2: Transduction of Elementary Trees (ETs)
</figureCaption>
<equation confidence="0.9589285">
1. (a) 1. (b)
advmod(reproduce, Also) advmod(reproduce, Also)
nn(fungi, lichen) nn(fungi, lichen)
nsubj(reproduce, fungi) nsubj(reproduce, fungi)
aux(reproduce, can) aux(reproduce, can)
advmod(reproduce,sexually) advmod(reproduce,sexually)
xcomp(reproduce,producing) amod(spores,producing)
dobj(producing, spores) prep by(reproduce, spores)
</equation>
<bodyText confidence="0.9995742">
Thus, to convert the first sentence into the sec-
ond, we need to delete two dependencies and in-
troduce two others. The rule contains variables
(?Xn), which can be forced to match certain words
in square brackets:
</bodyText>
<figure confidence="0.987277857142857">
RULE: PRODUCING2BY PRODUCING
1. DELETE
(a) xcomp(?X0[reproduce], ?X1[producing])
(b) dobj(?X1[producing], ?X2[spores])
2. INSERT
(a) amod(?X2, ?X1)
(b) prep by(?X0, ?X2)
</figure>
<bodyText confidence="0.999866090909091">
By collecting such rules, we can produce
a meta-grammar that can translate dependency
parses in one language (English) into the other
(simplified English). The rule above will trans-
late “reproduce, producing spores” to “reproduce
by producing spores”. This rule is alternatively
shown as a transduction of elementary trees in Fig.
2. Such deletion and insertion operations are cen-
tral to text simplification, but a few other opera-
tions are also needed to avoid broken dependency
links in the Target ETs (cf. Fig. 1).
Consider lexical simplification; for example,
where the word “extensive” is replaced by “big”,
resulting in one amod relation being deleted and
a new one inserted. Now, a third list is automat-
ically created when a variable (?X1) is present in
the DELETE list but not the INSERT list. This
is a command to move any other relations (edges)
involving the node ?X1 to the newly created node
?X2, and ensures correct rule application in new
contexts where there might be additional relations
involving the deleted word.
</bodyText>
<figure confidence="0.976056285714286">
RULE: EXTENSIVE2BIG
1. DELETE
(a) amod(?X0[network], ?X1[extensive])
2. INSERT
(a) amod(?X0, ?X2[big])
3. NODE OPERATION
(a) MOVE: ?X1 −→ ?X2
</figure>
<bodyText confidence="0.996304">
We also apply a process of generalisation, so
that a single rule can be created from multiple
instances in the training data. For example, if
the modifier “extensive” has been simplified to
“big” in the context of a variety of words in the
?X0 position, this can be represented succinctly
as “?X0[networks, avalanches, blizzard, contro-
versy]”. Note that this list provides valid lexical
contexts for application of the rule. If the word
is seen in sufficient contexts, we make it universal
by removing the list. An example of a generalised
rule follows:
</bodyText>
<figure confidence="0.940136888888889">
RULE: *2BIG
1. DELETE
(a) amod(?X0, ?X1[extensive, large, massive, siz-
able, major, powerful, unprecedented, devel-
oped, giant])
2. INSERT
(a) amod(?X0, ?X2[big])
3. NODE OPERATION
(a) MOVE: ?X1 −→ ?X2
</figure>
<bodyText confidence="0.999634444444444">
This rule states that any of the words in “[ex-
tensive, large, massive, sizable, major, power-
ful, unprecedented, developed, giant]” can be re-
placed by “big” in any lexical context ?X0; i.e.,
these words are not ambiguous. We acquire rules
such as the above automatically, filtering out rules
that involve syntactic constructs that we require
manually-written rules for (relative clauses, appo-
sition, coordination and subordination). We have
extracted 3180 rules from SEW revision histories
and aligned SEW-EW sentence pairs. From the
same data, Woodsend and Lapata (2011) extract
1431 rules, but these include rules for deletion,
as well as for purely syntactic sentence splitting.
The 3180 rules we derive are only lexical simpli-
fications or simple paraphrases. We do not per-
form deletion operations, and use manually writ-
ten rules for sentence splitting rules
</bodyText>
<figure confidence="0.995013375">
xcomp
dobj
spores
producing
producing
prep by
spores
amod
</figure>
<page confidence="0.989665">
725
</page>
<bodyText confidence="0.999923">
Our approach allows for the encoding of local
lexico-syntactic context for lexical simplification.
Only if a simplification is seen in many contexts
do we generalise the rule by relaxing the lexi-
cal context. We consider this a better solution to
that implemented in Woodsend and Lapata (2011),
who have to discard lexical rules that are only seen
once, because they do not model lexical context.
</bodyText>
<subsectionHeader confidence="0.9310345">
3.3 Manual grammars for common syntactic
cases
</subsectionHeader>
<bodyText confidence="0.999933333333333">
In addition to the automatically acquired grammar
as described above, our system uses a small hand
crafted grammar for common syntactic simplifica-
tions. As discussed earlier, these rules are diffi-
cult to learn from corpora, as difficult morphology
and tense manipulations would have to be learnt
from specific instances seen in a corpus. In prac-
tice, it is easy enough to code these rules correctly.
We have 26 hand-crafted rules for apposition, rel-
ative clauses, and combinations of the two. A fur-
ther 85 rules handle subordination and coordina-
tion. These are greater in number because they
are lexicalised on the conjunction. 11 further rules
cover voice conversion from passive to active. Fi-
nally, we include 14 rules to standardise quota-
tions; i.e., reduce various constructs for attribution
to the form “X said: Y.” Performing this step al-
lows us to simplify constructs embedded within
quotations - another case that is not handled well
by existing systems. One of the rules for convert-
ing passive to active voice is shown below:
</bodyText>
<figure confidence="0.980547181818182">
RULE: PASSIVE2ACTIVE
1. DELETE
(a) nsubjpass(?X0, ?X1)
(b) auxpass(?X0, ?X2)
(c) agent(?X0, ?X3)
2. INSERT
(a) nsubj(?X0, ?X3)
(b) dobj(?X0, ?X1)
3. NODE OPERATIONS
(a) AGR-TENSE: ?X0 ←− ?X2
(b) AGR-NUMBER: ?X0 ←− ?X3
</figure>
<bodyText confidence="0.982299071428571">
The rule specifies that the node ?X0 should in-
herit the tense of ?X2 and agree in number with
?X3. This rule correctly captures the morpholog-
ical changes required for the verb, something not
achieved by the other systems discussed in Sec-
tion 2. The dependency representation makes such
linguistic constraints easy to write by hand. How-
ever, we are not yet in a position to learn such
constraints automatically. Our argument is that a
small number of grammar rules need to be coded
carefully by hand to allow us to express the diffi-
cult syntactic constructions, while we can harvest
large grammars for local paraphrase operations in-
cluding lexical substitution.
</bodyText>
<subsectionHeader confidence="0.974433">
3.4 Elementary tree transfer
</subsectionHeader>
<bodyText confidence="0.9947145">
In this work we apply the simplification rules ex-
haustively to the dependency parse; i.e., every rule
for which the DELETE list is matched is applied
iteratively. As an illustration, consider:
The cat was chased by a dog that was
barking.
</bodyText>
<equation confidence="0.9969535">
det(cat-2, The-1)
nsubjpass(chased-4, cat-2)
auxpass(chased-4, was-3)
det(dog-7, a-6)
agent(chased-4, dog-7)
nsubj(barking-10, dog-7)
aux(barking-10, was-9)
rcmod(dog-7, barking-10)
</equation>
<bodyText confidence="0.9681805">
Two rules match; the first simplifies relative
clauses:
</bodyText>
<figure confidence="0.976062833333333">
RULE: RELATIVECLAUSE
1. DELETE
(a) rcmod(??X0, ??X1)
(b) nsubj(??X1, ??X0)
2. INSERT
(a) nsubj(??X1, ??X0)
</figure>
<bodyText confidence="0.9996745">
This rule removes the embedding “rcmod” re-
lation, when there is a subject available for the
verb in the relative clause. Then we apply the rule
to convert passive to active voice, as described in
Section 3.3. Following these two rule applications,
we are left with the following list of dependencies:
</bodyText>
<equation confidence="0.872979166666667">
det(cat-2, The-1)
dobj(chased-4, cat-2)
det(dog-7, a-6)
nsubj(chased-4, dog-7)
aux(barking-10, was-9)
nsubj(barking-10, dog-7)
</equation>
<bodyText confidence="0.9648145">
This list now represents two trees with chased
and barking as root nodes:
</bodyText>
<page confidence="0.994475">
726
</page>
<figure confidence="0.551364">
cat dog was dog
det det det
the a a
</figure>
<subsectionHeader confidence="0.463006">
3.5 Generating from typed dependency
representations
</subsectionHeader>
<bodyText confidence="0.999845833333333">
Generating from constituency-based parse trees is
trivial, in that leaf nodes need to be output in the
order processed by a depth first LR search. The
higher level of abstraction of dependency repre-
sentations makes generation more complicated, as
the dependencies abstract away from constituent
ordering and word morphology. One option is to
use an off the shelf generator; however, this does
not work well in practice; e.g., Siddharthan (2011)
found that misanalyses by the parser can result in
unacceptable word and constituent orders in the
generated texts. In the system described here,
we follow the generation-light approach adopted
by Siddharthan (2011). We reuse the word or-
der from the input sentence as a default, and the
synchronous grammar encodes any changes in or-
dering. For example, in Rule PASSIVE2ACTIVE
above, we include a further specification:
</bodyText>
<figure confidence="0.821281">
4 Traversal Order Specifications
(a) Node ?X0: [?X3, ?X0, ?X1]
</figure>
<bodyText confidence="0.99989525">
This states that for node ?X0, the traversal order
should be subtree ?X3 followed by current node
?X0 followed by subtree ?X1. Using this specifi-
cation would allow us to traverse the tree using the
original word order for nodes with no order speci-
fication, and the specified order where a specifica-
tion exists. In the above instance, this would lead
us to simplify “The cat is chased by the dogs” to
“the dogs chase the cat”. Details of the genera-
tion process can be found elsewhere (Siddharthan,
2011, for example), but to summarise, the gen-
light approach implemented here uses four lists:
</bodyText>
<listItem confidence="0.9712736">
1. DELETE: List of relations to delete.
2. INSERT: List of relations to insert.
3. ORDERING: List of nodes with subtree order specified
4. NODE-OPERATIONS: List of morphological changes
and deletion operations on nodes.
</listItem>
<bodyText confidence="0.9998326">
At present the automatically harvested rules do
not encode morphological changes. They do how-
ever encode reordering information, which is auto-
matically detected from the relative word positions
in the original and simplified training sentences.
</bodyText>
<sectionHeader confidence="0.997655" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.975475195652174">
We performed a manual evaluation of how fluent
and simple the text produced by our simplifica-
tion system is, and the extent to which it preserves
meaning. We use the evaluation set previously
used by Woodsend and Lapata (2011), Zhu et al.
(2010) and Wubben et al. (2012). This consists
of 100 sentences from English Wikipedia, aligned
with Simple English Wikipedia (SEW) sentences.
Previous work report various automatic measures,
including BLEU and readability metrics such as
the Flesch-Kincaid Grade Level Index (FKGL).
None of these have been validated for the auto-
matic text simplification task, however, and we
prefer to conduct an evaluation with human raters.
Our system (henceforth, HYBRID) is compared
to QTSG (the system by Woodsend and Lapata
(2011) that learns a quasi-synchronous grammar
from the same data as the automated component
of HYBRID), and the manual gold standard SEW.
We selected the first 25 sentences from the evalu-
ation set for which both QTSG and HYBRID had
performed at least one simplification1. Five hu-
man raters2 were shown sets containing the origi-
nal Wikipedia sentence, followed by QTSG, HY-
BRID and SEW in a randomised order. For each
such set, they were asked to rate each simplified
version for fluency, simplicity and the extent to
which it preserved the meaning of the original, us-
ing a Likert scale of 1–5, where 1 is totally un-
usable output, and 5 is output that is perfectly
usable. The results are shown in Table 1. Our
HYBRID system outperforms QTSG on all three
metrics, and is comparable to the SEW version.
Raters R1–3 provide very similar ratings, while
R4–5 demonstrate a greater preference for the HY-
BRID system relative to the SEW. The HYBRID
system performs best on meaning preservation (in
136 sentences were considered and 11 sentences were ex-
cluded in this process. QTSG did not simplify 3 sentences
and HYBRID as many as 9, as it does not perform compres-
sion operations. One sentence was left unchanged by both
systems.
2R1–R4 are Computational Linguists, while R5 is a doc-
toral student in Public Health Communication. None of them
are connected with this research, and none of them have pre-
viously seen the output of text simplification systems.
</bodyText>
<figure confidence="0.85508075">
chased
dobj nsubj
barking
aux nsubj
</figure>
<page confidence="0.874424">
727
</page>
<table confidence="0.999204777777778">
Rater FLUENCY SEW SIMPLICITY SEW MEANING PRESERVATION
QTSG HYBRID QTSG HYBRID QTSG HYBRID SEW
R1 2.60 4.44 4.60 3.04 3.88 4.36 3.16 4.68 4.24
R2 3.08 4.24 4.52 3.20 4.08 4.48 3.28 4.76 4.36
R3 2.40 4.20 4.68 3.12 3.80 4.44 2.96 4.52 3.80
R4 2.32 3.88 3.48 2.92 3.44 3.44 2.72 4.52 3.56
R5 2.00 3.44 3.48 2.00 3.52 3.56 2.48 4.52 3.84
Mean 2.48 4.04 4.15 2.85 3.74 4.05 2.92 4.60 3.96
Median 2 4 4 3 4 4 3 5 4
</table>
<tableCaption confidence="0.999417">
Table 1: Results of human evaluation with five raters R1–R5. QTSG is the system by Woodsend and
</tableCaption>
<bodyText confidence="0.990923979166667">
Lapata (2011). HYBRID is the system described in this paper, with manual and automatically acquired
rules. SEW is the human generated simplification from Simple English Wikipedia. All differences in
means for Simplicity and Meaning Preservation are significant (p &lt; 0.001; t-test). For Fluency, HYBRID
and SEW are significantly better than QTSG (p &lt; 0.001; t-test).
large part because it is the only version that does
not delete information through sentence compres-
sion).
Table 2 shows some examples of simplifications
from the evaluation dataset, along with their av-
erage scores for fluency, simplicity and meaning
preservation. These examples have been selected
to help interpret the results in Table 1. QTSG fre-
quently generates fragments (“Komiyama is a.”,
etc.), likely through incorrect splitting rules in the
grammar; this is penalised heavily by the raters.
The HYBRID system uses manually written rules
for sentence splitting and is more robust in this re-
gard. This is confirmed by looking at standard de-
viations of ratings. For fluency, QTSG has sd =
1.41, almost twice that of HYBRID (sd = .76).
A similar trend is observed for meaning preserva-
tion, where QTSG has sd = 1.29, compared to
sd = .68 for HYBRID.
QTSG does perform very elegant compressions
in some cases; this is a strength of that system.
Our system aims to preserve meaning, which it
does rather well. However, this is is not neces-
sarily a valid objective. Perhaps future evalua-
tions should distinguish between modifying infor-
mation in misleading ways (undesirable) and re-
moving peripheral information (desirable). It is
clear that the latter, done well, is useful and will
be addressed in future work.
An error analysis shows that the main cause
of errorful output for our system is parser errors,
particularly mistakes in relative clause attachment
and clause boundary identificaton. Methods such
as those in Siddharthan (2003b) can be used to im-
prove parser performance on these tasks.
Finally, this work and the cited related work
only investigate sentence-level text simplification.
There are various discourse level effects that also
need to be considered when simplifying larger
texts, including sentence ordering (Barzilay et
al., 2002; Siddharthan, 2003a; Barzilay and La-
pata, 2008), discourse connectives (Siddharthan
and Katsos, 2010) and anaphora choice (Nenkova
et al., 2005; Siddharthan et al., 2011).
</bodyText>
<sectionHeader confidence="0.996565" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999856">
We have presented a framework for text sim-
plification based on synchronous grammars over
typed dependency representations. Our HYBRID
system, that uses hand-written rules for common
syntactic simplifications, and automatically har-
vested rules for a much larger set of lexicalised
simplifications is more robust than a similar sys-
tem based on quasi-synchronous tree substitution
grammars, outperforming it in terms of fluency,
simplicity and meaning preservation. By abstract-
ing away from constituent ordering and morpho-
logical variations, our approach allows for lin-
guistically sound rules to be written for complex
lexico-syntactic transformations, including pas-
sive to active voice. In the version of the system
described and evaluated here, changes to morphol-
ogy and constituent ordering are specified within
the rules. Alternately, an off the shelf surface re-
aliser could be used to generate from the depen-
dency representation.
</bodyText>
<sectionHeader confidence="0.9944" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.5296885">
This research is supported by an award made by
the EPSRC; award reference: EP/J018805/1.
</bodyText>
<page confidence="0.976567">
728
</page>
<bodyText confidence="0.971800901960784">
ORIGINAL QTSG HYBRID SEW
Takanobu Komiyama His father. Komiyama Takanobu Komiyama Takanobu Komiyama
(born October 3, 1984 is a. (born October 3, 1984 (born 3 October 1984)
in Chiba, Japan) is F=1, S=1.4, M=1 in Chiba, Japan) is is a Japanese football
a Japanese football a Japanese football player. He plays for
player who currently player. Takanobu Kawasaki Frontale.
plays for the J-league Komiyama at present F=4.6, S=4.4, M=4.2
team Kawasaki plays for the J-league
Frontale. team Kawasaki
Frontale.
F=4, S=3.8, M=4.8
The occupants of Swadlincote watch. The occupants of People from Swadlin-
Swadlincote often The occupants often Swadlincote often cote often shorten its
shorten its name to shorten its name to shorten its name to name to simply Swad.
simply ‘Swad’. simply ‘Swad’. just ‘Swad’. F=4.6, S=4.6, M=4.8
F=2.6, S=2.6, M=3.2 F=4.4, S=4, M=5
Today the nearest rail- Today the nearest rail- The closest railway The nearest railway
way station is Burton way station is Burton station is now Burton station is Burton upon
upon Trent, about five upon Trent, about five upon Trent, about five Trent which is five
miles away, as the rail- miles away, as the rail- miles away. This is miles away.
way between Burton way between Burton because the railway F=4, S=4, M=2.2
and Leicester lost its and Leicester lost ser- between Burton and
passenger service un- vice under the Beech- Leicester now carries
der the Beeching Plan ing Plan in the 1960s. only freight. The
in the 1960s and now It now carries freight. railway lost its passen-
carries only freight. F=4.2, S=3.6, M=4.6 ger service under the
Beeching Plan in the
1960s.
F=4.4, S=3.8, M=5
Since December 2005 Since December 2005, Since December 2005 Since December 2005
it has also been a can- it is a candidate for it has also been a it has also been a can-
didate for joining the joining the European candidate for joining didate for joining the
European Union and Union. the European Union. European Union. It
has applied for NATO F=4, S=4.2, M=3.6 And it has applied for has applied for NATO
membership. NATO membership. membership.
F=4.2, S=4, M=4.8 F=4.2, S=4, M=4.8
Although most Irish The modern Irish Supporters of Fine Most Irish politi-
political parties recog- state watch. Most Gael hold his mem- cal parties think his
nize his contribution Irish political parties ory in very esteem, contributions were
to the foundation of recognize his contri- regarding him as their important to make
the modern Irish state, bution to foundation. movement’s founding the modern Irish
supporters of Fine Supporters of Gael father, through his state. Members and
Gael hold his memory hold his memory in link to their precursor supporters of Fine
in particular esteem, particular esteem, Cumann na nGaed- Gael remember him
regarding him as their regarding him as their hael. But, all Irish in particular as one
movement’s founding movement’s founding political parties recog- of the founders of
father, through his father, through his nize his contribution their movement, or its
link to their pre- link to their pre- to the foundation of predecessor Cumann
cursor Cumann na cursor Cumann na the modern Irish state. na nGaedhael.
nGaedhael. nGaedhael. F=3.4, S=3.6, M=4.2 F=3.6, S=3.4, M=4.6
F=2.6, S=3.2, M=3.8
</bodyText>
<tableCaption confidence="0.713139">
Table 2: Examples of simplifications from the test set, along with average scores for (F)luency,
(S)implicity and (M)eaning Preservation. 729
</tableCaption>
<sectionHeader confidence="0.971469" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999581626168224">
Sandra M Alu´ısio, Lucia Specia, Thiago AS Pardo, Er-
ick G Maziero, and Renata PM Fortes. 2008. To-
wards brazilian portuguese automatic text simplifi-
cation systems. In Proceedings of the eighth ACM
symposium on Document engineering, pages 240–
248. ACM.
Richard C. Anderson and Alice Davison. 1988. Con-
ceptual and empirical bases of readibility formulas.
In Alice Davison and G. M. Green, editors, Linguis-
tic Complexity and Text Comprehension: Readabil-
ity Issues Reconsidered. Lawrence Erlbaum Asso-
ciates, Hillsdale, NJ.
Richard Anderson and Peter Freebody. 1981. Vocab-
ulary knowledge. In John Guthrie, editor, Compre-
hension and Teaching: Research Reviews, pages 77–
117. International Reading Association, Newark,
DE.
R. Barzilay and M. Lapata. 2008. Modeling Local
Coherence: An Entity-Based Approach. Computa-
tional Linguistics, 34(1):1–34.
R. Barzilay, N. Elhadad, and K. McKeown. 2002. In-
ferring Strategies for Sentence Ordering in Multi-
document News Summarization. Journal of Artifi-
cial Intelligence Research, 17(3):35–55.
Isabel L. Beck, Margaret G. McKeown, Gale M. Sina-
tra, and Jane A. Loxterman. 1991. Revising social
studies text from a text-processing perspective: Ev-
idence of improved comprehensibility. Reading Re-
search Quarterly, pages 251–276.
Stefan Bott, Horacio Saggion, and Simon Mille. 2012.
Text simplification tools for spanish. In LREC,
pages 1665–1671.
Arnaldo Candido Jr, Erick Maziero, Caroline Gasperin,
Thiago AS Pardo, Lucia Specia, and Sandra M
Aluisio. 2009. Supporting the adaptation of texts
for poor literacy readers: a text simplification ed-
itor for brazilian portuguese. In Proceedings of
the Fourth Workshop on Innovative Use of NLP
for Building EducationalApplications, pages 34–42.
Association for Computational Linguistics.
Yvonne Canning. 2002. Syntactic simplification of
Text. Ph.D. thesis, University of Sunderland, UK.
Raman Chandrasekar, Christine Doran, and Banga-
lore Srinivas. 1996. Motivations and methods for
text simplification. In Proceedings of the 16th In-
ternational Conference on Computational Linguis-
tics (COLING ’96), pages 1041–1044, Copenhagen,
Denmark.
William Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1–9. Association for Computational
Linguistics.
Jan De Belder and Marie-Francine Moens. 2010.
Text simplification for children. In Prroceedings of
the SIGIR workshop on accessible search systems,
pages 19–26.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 541–548. Association for Computa-
tional Linguistics.
Mark Dras. 1999. Tree adjoining grammar and the
reluctant paraphrasing of text. Ph.D. thesis, Mac-
quarie University NSW 2109 Australia.
J. Kamalski, T. Sanders, and L. Lentz. 2008. Coher-
ence marking, prior knowledge, and comprehension
of informative and persuasive texts: Sorting things
out. Discourse Processes, 45(4):323–345.
D. Klein and C.D. Manning. 2003. Accurate un-
lexicalized parsing. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 423–430. Association
for Computational Linguistics.
J.J. L’Allier. 1980. An evaluation study of a computer-
based lesson that adjusts reading level by monitor-
ing on task reader characteristics. Ph.D. thesis,
University of Minnesota, Minneapolis, MN.
T. Linderholm, M.G. Everson, P. van den Broek,
M. Mischinski, A. Crittenden, and J. Samuels. 2000.
Effects of Causal Text Revisions on More-and Less-
Skilled Readers’ Comprehension of Easy and Dif-
ficult Texts. Cognition and Instruction, 18(4):525–
556.
Ani Nenkova, Advaith Siddharthan, and Kathleen
McKeown. 2005. Automatically learning cog-
nitive status for multi-document summarization of
newswire. In Proceedings of HLT/EMNLP 2005,
pages 241–248, Vancouver, Canada.
Advaith Siddharthan and Napoleon Katsos. 2010.
Reformulating discourse connectives for non-expert
readers. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2010), Los Angeles, CA.
Advaith Siddharthan, Ani Nenkova, and Kathleen
McKeown. 2011. Information status distinctions
and referring expressions: An empirical study of
references to people in news summaries. Compu-
tational Linguistics, 37(4):811–842.
Advaith Siddharthan. 2002. An architecture for a text
simplification system. In Proceedings of the Lan-
guage Engineering Conference (LEC’02), pages 64–
71, Hyderabad, India.
Advaith Siddharthan. 2003a. Preserving discourse
structure when simplifying text. In Proceedings of
</reference>
<page confidence="0.964218">
730
</page>
<reference confidence="0.999245528301887">
the European Natural Language Generation Work-
shop (ENLG), 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL’03), pages 103–110, Budapest, Hun-
gary.
Advaith Siddharthan. 2003b. Resolving pronouns ro-
bustly: Plumbing the depths of shallowness. In Pro-
ceedings of the Workshop on Computational Treat-
ments of Anaphora, 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL’03), pages 7–14, Budapest, Hun-
gary.
Advaith Siddharthan. 2010. Complex lexico-syntactic
reformulation of sentences using typed dependency
representations. In Proceedings of the 6th Inter-
national Natural Language Generation Conference
(INLG 2010), Dublin Ireland.
Advaith Siddharthan. 2011. Text simplification using
typed dependencies: a comparison of the robustness
of different generation strategies. In Proceedings of
the 13th European Workshop on Natural Language
Generation, pages 2–11. Association for Computa-
tional Linguistics.
David A Smith and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft projec-
tion of syntactic dependencies. In Proceedings of
the Workshop on Statistical Machine Translation,
pages 23–30. Association for Computational Lin-
guistics.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 409–420. Association
for Computational Linguistics.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
1015–1024. Association for Computational Linguis-
tics.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics, pages 523–530. Association
for Computational Linguistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd international conference on computational lin-
guistics, pages 1353–1361. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.998002">
731
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652234">
<title confidence="0.9804355">Hybrid text simplification using synchronous dependency grammars with hand-written and automatically harvested rules</title>
<author confidence="0.683824">Advaith</author>
<affiliation confidence="0.9396985">Computing University of</affiliation>
<email confidence="0.983429">advaith@abdn.ac.uk</email>
<abstract confidence="0.999235318181818">We present an approach to text simplification based on synchronous dependency grammars. The higher level of abstraction afforded by dependency representations allows for a linguistically sound treatment of complex constructs requiring reordering and morphological change, such as conversion of passive voice to active. We present a synchronous grammar formalism in which it is easy to write rules by hand and also acquire them automatically from dependency parses of aligned English and Simple English sentences. The grammar formalism is optimised for monolingual translation in that it reuses ordering information from the source sentence where appropriate. We demonstrate the superiority of our approach over a leading contemporary system based on quasi-synchronous tree substitution grammars, both in terms of expressivity and performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sandra M Alu´ısio</author>
<author>Lucia Specia</author>
<author>Thiago AS Pardo</author>
<author>Erick G Maziero</author>
<author>Renata PM Fortes</author>
</authors>
<title>Towards brazilian portuguese automatic text simplification systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the eighth ACM symposium on Document engineering,</booktitle>
<pages>240--248</pages>
<publisher>ACM.</publisher>
<marker>Alu´ısio, Specia, Pardo, Maziero, Fortes, 2008</marker>
<rawString>Sandra M Alu´ısio, Lucia Specia, Thiago AS Pardo, Erick G Maziero, and Renata PM Fortes. 2008. Towards brazilian portuguese automatic text simplification systems. In Proceedings of the eighth ACM symposium on Document engineering, pages 240– 248. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Anderson</author>
<author>Alice Davison</author>
</authors>
<title>Conceptual and empirical bases of readibility formulas.</title>
<date>1988</date>
<booktitle>In Alice Davison</booktitle>
<editor>and G. M. Green, editors,</editor>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="2093" citStr="Anderson and Davison, 1988" startWordPosition="312" endWordPosition="315">dults in the UK have poor literacy skills. The situation is often worse in developing countries. Alu´ısio et al. (2008) report that 68% of Brazilians between 15 and 64 years who have studied up to 4 years only reach the rudimentary level of literacy, and even among those who have studied for 8 years, only a quarter can be considered fully literate. While there is a large M. A. Angrosh Computing Science University of Aberdeen UK angroshmandya@abdn.ac.uk body of evidence that manual text simplification is an effective intervention (Anderson and Freebody, 1981; L’Allier, 1980; Beck et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008), there has till recently been little work on automatic simplification. The pace of research has picked up in recent years though, with many teams applying machine translation approaches to perform “monolingual translation” from English to simplified English. The goals of this paper are to (1) identify the limitations of recently published approaches to text simplification with regard to their coverage of linguistic constructs, (2) to describe an approach based on synchronous grammars operating on typed dependency representations that permits a </context>
</contexts>
<marker>Anderson, Davison, 1988</marker>
<rawString>Richard C. Anderson and Alice Davison. 1988. Conceptual and empirical bases of readibility formulas. In Alice Davison and G. M. Green, editors, Linguistic Complexity and Text Comprehension: Readability Issues Reconsidered. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Anderson</author>
<author>Peter Freebody</author>
</authors>
<title>Vocabulary knowledge. In</title>
<date>1981</date>
<booktitle>Comprehension and Teaching: Research Reviews,</booktitle>
<pages>77--117</pages>
<editor>John Guthrie, editor,</editor>
<publisher>International Reading Association,</publisher>
<location>Newark, DE.</location>
<contexts>
<context position="2030" citStr="Anderson and Freebody, 1981" startWordPosition="301" endWordPosition="305">st (http://www.literacytrust.org.uk) estimates that one in six adults in the UK have poor literacy skills. The situation is often worse in developing countries. Alu´ısio et al. (2008) report that 68% of Brazilians between 15 and 64 years who have studied up to 4 years only reach the rudimentary level of literacy, and even among those who have studied for 8 years, only a quarter can be considered fully literate. While there is a large M. A. Angrosh Computing Science University of Aberdeen UK angroshmandya@abdn.ac.uk body of evidence that manual text simplification is an effective intervention (Anderson and Freebody, 1981; L’Allier, 1980; Beck et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008), there has till recently been little work on automatic simplification. The pace of research has picked up in recent years though, with many teams applying machine translation approaches to perform “monolingual translation” from English to simplified English. The goals of this paper are to (1) identify the limitations of recently published approaches to text simplification with regard to their coverage of linguistic constructs, (2) to describe an approach based on synchronous grammar</context>
</contexts>
<marker>Anderson, Freebody, 1981</marker>
<rawString>Richard Anderson and Peter Freebody. 1981. Vocabulary knowledge. In John Guthrie, editor, Comprehension and Teaching: Research Reviews, pages 77– 117. International Reading Association, Newark, DE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling Local Coherence: An Entity-Based Approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="26896" citStr="Barzilay and Lapata, 2008" startWordPosition="4231" endWordPosition="4235">and will be addressed in future work. An error analysis shows that the main cause of errorful output for our system is parser errors, particularly mistakes in relative clause attachment and clause boundary identificaton. Methods such as those in Siddharthan (2003b) can be used to improve parser performance on these tasks. Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). 5 Conclusions We have presented a framework for text simplification based on synchronous grammars over typed dependency representations. Our HYBRID system, that uses hand-written rules for common syntactic simplifications, and automatically harvested rules for a much larger set of lexicalised simplifications is more robust than a similar system based on quasi-synchronous tree substitution grammars, outperforming it in terms of fluency, simplicity and meaning preservation</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>R. Barzilay and M. Lapata. 2008. Modeling Local Coherence: An Entity-Based Approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
<author>K McKeown</author>
</authors>
<title>Inferring Strategies for Sentence Ordering in Multidocument News Summarization.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="26848" citStr="Barzilay et al., 2002" startWordPosition="4225" endWordPosition="4228">lear that the latter, done well, is useful and will be addressed in future work. An error analysis shows that the main cause of errorful output for our system is parser errors, particularly mistakes in relative clause attachment and clause boundary identificaton. Methods such as those in Siddharthan (2003b) can be used to improve parser performance on these tasks. Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). 5 Conclusions We have presented a framework for text simplification based on synchronous grammars over typed dependency representations. Our HYBRID system, that uses hand-written rules for common syntactic simplifications, and automatically harvested rules for a much larger set of lexicalised simplifications is more robust than a similar system based on quasi-synchronous tree substitution grammars, outperforming it in terms</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring Strategies for Sentence Ordering in Multidocument News Summarization. Journal of Artificial Intelligence Research, 17(3):35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabel L Beck</author>
<author>Margaret G McKeown</author>
<author>Gale M Sinatra</author>
<author>Jane A Loxterman</author>
</authors>
<title>Revising social studies text from a text-processing perspective: Evidence of improved comprehensibility. Reading Research Quarterly,</title>
<date>1991</date>
<pages>251--276</pages>
<contexts>
<context position="2065" citStr="Beck et al., 1991" startWordPosition="308" endWordPosition="311">s that one in six adults in the UK have poor literacy skills. The situation is often worse in developing countries. Alu´ısio et al. (2008) report that 68% of Brazilians between 15 and 64 years who have studied up to 4 years only reach the rudimentary level of literacy, and even among those who have studied for 8 years, only a quarter can be considered fully literate. While there is a large M. A. Angrosh Computing Science University of Aberdeen UK angroshmandya@abdn.ac.uk body of evidence that manual text simplification is an effective intervention (Anderson and Freebody, 1981; L’Allier, 1980; Beck et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008), there has till recently been little work on automatic simplification. The pace of research has picked up in recent years though, with many teams applying machine translation approaches to perform “monolingual translation” from English to simplified English. The goals of this paper are to (1) identify the limitations of recently published approaches to text simplification with regard to their coverage of linguistic constructs, (2) to describe an approach based on synchronous grammars operating on typed dependency rep</context>
</contexts>
<marker>Beck, McKeown, Sinatra, Loxterman, 1991</marker>
<rawString>Isabel L. Beck, Margaret G. McKeown, Gale M. Sinatra, and Jane A. Loxterman. 1991. Revising social studies text from a text-processing perspective: Evidence of improved comprehensibility. Reading Research Quarterly, pages 251–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bott</author>
<author>Horacio Saggion</author>
<author>Simon Mille</author>
</authors>
<title>Text simplification tools for spanish.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>1665--1671</pages>
<contexts>
<context position="4156" citStr="Bott et al., 2012" startWordPosition="619" endWordPosition="622">ive clauses). Some early systems (Chandrasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 Text simplification as monolingual translation Recent years have seen the increased application of machine translation approaches to text simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplificati</context>
</contexts>
<marker>Bott, Saggion, Mille, 2012</marker>
<rawString>Stefan Bott, Horacio Saggion, and Simon Mille. 2012. Text simplification tools for spanish. In LREC, pages 1665–1671.</rawString>
</citation>
<citation valid="true">
<title>Supporting the adaptation of texts for poor literacy readers: a text simplification editor for brazilian portuguese.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building EducationalApplications,</booktitle>
<pages>34--42</pages>
<editor>Arnaldo Candido Jr, Erick Maziero, Caroline Gasperin, Thiago AS Pardo, Lucia Specia, and Sandra M Aluisio.</editor>
<publisher>Association for Computational Linguistics.</publisher>
<marker>2009</marker>
<rawString>Arnaldo Candido Jr, Erick Maziero, Caroline Gasperin, Thiago AS Pardo, Lucia Specia, and Sandra M Aluisio. 2009. Supporting the adaptation of texts for poor literacy readers: a text simplification editor for brazilian portuguese. In Proceedings of the Fourth Workshop on Innovative Use of NLP for Building EducationalApplications, pages 34–42. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvonne Canning</author>
</authors>
<title>Syntactic simplification of Text.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sunderland, UK.</institution>
<contexts>
<context position="4062" citStr="Canning, 2002" startWordPosition="604" endWordPosition="605">.g., splitting coordinated and subordinated clauses, and disembedding apposition and relative clauses). Some early systems (Chandrasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 Text simplification as monolingual translation Recent years have seen the increased application of machine translation approaches to text simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and</context>
</contexts>
<marker>Canning, 2002</marker>
<rawString>Yvonne Canning. 2002. Syntactic simplification of Text. Ph.D. thesis, University of Sunderland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raman Chandrasekar</author>
<author>Christine Doran</author>
<author>Bangalore Srinivas</author>
</authors>
<title>Motivations and methods for text simplification.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING ’96),</booktitle>
<pages>1041--1044</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="3598" citStr="Chandrasekar et al., 1996" startWordPosition="543" endWordPosition="546">us formalism. We summarise work on text simplification in Section 2, before describing our method in Section 3 and presenting our results in Section 4. 2 Related work There are two largely distinct bodies of work on automatic text simplification – those that use handcrafted rules, and those that apply machine translation approaches. 2.1 Hand-crafted text simplification systems The first body of work uses hand-crafted rules to perform syntactic simplification operations (e.g., splitting coordinated and subordinated clauses, and disembedding apposition and relative clauses). Some early systems (Chandrasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). F</context>
</contexts>
<marker>Chandrasekar, Doran, Srinivas, 1996</marker>
<rawString>Raman Chandrasekar, Christine Doran, and Bangalore Srinivas. 1996. Motivations and methods for text simplification. In Proceedings of the 16th International Conference on Computational Linguistics (COLING ’96), pages 1041–1044, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Learning to simplify sentences using wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4677" citStr="Coster and Kauchak (2011)" startWordPosition="695" endWordPosition="698">ning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 Text simplification as monolingual translation Recent years have seen the increased application of machine translation approaches to text simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplification. PMBT can only perform a small set of simplification operations, such as lexical substitution, deletion and simple paraphrase. They are not well suited for reordering or splitting operations. Specifically, the syntactic simplification operations that handcrafted systems focus on are out of scope. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input se</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011. Learning to simplify sentences using wikipedia. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan De Belder</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Text simplification for children.</title>
<date>2010</date>
<booktitle>In Prroceedings of the SIGIR workshop</booktitle>
<pages>pages</pages>
<marker>De Belder, Moens, 2010</marker>
<rawString>Jan De Belder and Marie-Francine Moens. 2010. Text simplification for children. In Prroceedings of the SIGIR workshop on accessible search systems, pages 19–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>541--548</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10682" citStr="Ding and Palmer (2005)" startWordPosition="1629" endWordPosition="1632">mmar rules; thus common syntactic operations (such as conversion of passive to active voice) can be handled in this framework through accurate hand-written rules, and (3) It is easier and more elegant to automatically acquire a synchronous grammar from data, compared to synchronous grammars based on constituency-parses. In this section we describe our framework and text simplification system in more detail; then, in section 4, we report an evaluation that compares our system against a human simplification and the Woodsend and Lapata (2011) system. 3.1 Synchronous dependency insertion grammars Ding and Palmer (2005) introduce the notion of a Synchronous Dependency Insertion Grammar (SDIG) as a tree substitution grammar defined on dependency trees. They define elementary trees (ETs) to be sub-sentential dependency structures containing one or more lexical items. The SDIG formalism assumes that the isomorphism of the two syntactic structures is at the ET level, thus allowing for non-isomorphic tree to tree mapping at the sentence level. We base our approach to text simplification on SDIGs, but the formalism is adapted for the monolingual task, and the rules are written in a formalism that is suited to writ</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 541–548. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Tree adjoining grammar and the reluctant paraphrasing of text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<pages>2109</pages>
<institution>Macquarie University NSW</institution>
<contexts>
<context position="5754" citStr="Dras (1999)" startWordPosition="860" endWordPosition="861">d Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input sentences: substitution, reordering, splitting, and deletion. Splitting is encoded as two probabilities: A segmentation table stores probabilities of sentence splitting at particular words (e.g., which). A completion table stores probabilities of the splitting word to be deleted from the translation, and for the governing phrase to be inserted to complete the sentence. This allows the translation model to handle constructs such as relative clauses and apposition. Dras (1999) was the first to apply synchronous grammars to monolingual tasks. His approach is to map between two TAG grammars using a Generalised Synchronous TAG formalism, and to use Integer Programming to generate a text that satisfies the externally imposed constraints (such as length or readability) using minimal paraphrasing. Woodsend and Lapata (2011) further develop this line of research. Their model is based on quasi-synchronous grammar (Smith and Eisner, 2006) and integer linear programming. Quasisynchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomor</context>
</contexts>
<marker>Dras, 1999</marker>
<rawString>Mark Dras. 1999. Tree adjoining grammar and the reluctant paraphrasing of text. Ph.D. thesis, Macquarie University NSW 2109 Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kamalski</author>
<author>T Sanders</author>
<author>L Lentz</author>
</authors>
<title>Coherence marking, prior knowledge, and comprehension of informative and persuasive texts: Sorting things out.</title>
<date>2008</date>
<booktitle>Discourse Processes,</booktitle>
<volume>45</volume>
<issue>4</issue>
<contexts>
<context position="2142" citStr="Kamalski et al., 2008" startWordPosition="320" endWordPosition="323">ion is often worse in developing countries. Alu´ısio et al. (2008) report that 68% of Brazilians between 15 and 64 years who have studied up to 4 years only reach the rudimentary level of literacy, and even among those who have studied for 8 years, only a quarter can be considered fully literate. While there is a large M. A. Angrosh Computing Science University of Aberdeen UK angroshmandya@abdn.ac.uk body of evidence that manual text simplification is an effective intervention (Anderson and Freebody, 1981; L’Allier, 1980; Beck et al., 1991; Anderson and Davison, 1988; Linderholm et al., 2000; Kamalski et al., 2008), there has till recently been little work on automatic simplification. The pace of research has picked up in recent years though, with many teams applying machine translation approaches to perform “monolingual translation” from English to simplified English. The goals of this paper are to (1) identify the limitations of recently published approaches to text simplification with regard to their coverage of linguistic constructs, (2) to describe an approach based on synchronous grammars operating on typed dependency representations that permits a more sophisticated handling of many linguistic co</context>
</contexts>
<marker>Kamalski, Sanders, Lentz, 2008</marker>
<rawString>J. Kamalski, T. Sanders, and L. Lentz. 2008. Coherence marking, prior knowledge, and comprehension of informative and persuasive texts: Sorting things out. Discourse Processes, 45(4):323–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7571" citStr="Klein and Manning, 2003" startWordPosition="1143" endWordPosition="1146">ctly compare our approach to Woodsend and Lapata (2011), as this is the best performing contemporary system that has the same linguistic scope as ours. 2.3 Formalisms and linguistic coverage The systems summarised above differ primarily in the level of linguistic knowledge they encode. PBMT systems use the least knowledge, and as such are ill equipped to to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Syntax based approaches use syntactic knowledge. However, both Zhu et al. (2010) and Woodsend and Lapata (2011) use the Stanford Parser (Klein and Manning, 2003) for syntactic structure, and this representation lacks morphological information. This means that some simplification operations such as voice conversion are not handled well. For example, to simplify “trains are liked by John” to “John likes trains”, besides deleting auxiliaries and reordering the arguments of the verb “like”, the verb also needs to agree in number with the new subject (“John”), and take the tense of the auxiliary verb (“are”). The grammar acquisition process leads to further problems. From an aligned pair “John, who was tired, went to sleep.” and “John was tired. He went to</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J L’Allier</author>
</authors>
<title>An evaluation study of a computerbased lesson that adjusts reading level by monitoring on task reader characteristics.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Minnesota,</institution>
<location>Minneapolis, MN.</location>
<marker>L’Allier, 1980</marker>
<rawString>J.J. L’Allier. 1980. An evaluation study of a computerbased lesson that adjusts reading level by monitoring on task reader characteristics. Ph.D. thesis, University of Minnesota, Minneapolis, MN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Linderholm</author>
<author>M G Everson</author>
<author>P van den Broek</author>
<author>M Mischinski</author>
<author>A Crittenden</author>
<author>J Samuels</author>
</authors>
<date>2000</date>
<booktitle>Effects of Causal Text Revisions on More-and LessSkilled Readers’ Comprehension of Easy and Difficult Texts. Cognition and Instruction,</booktitle>
<volume>18</volume>
<issue>4</issue>
<pages>556</pages>
<marker>Linderholm, Everson, van den Broek, Mischinski, Crittenden, Samuels, 2000</marker>
<rawString>T. Linderholm, M.G. Everson, P. van den Broek, M. Mischinski, A. Crittenden, and J. Samuels. 2000. Effects of Causal Text Revisions on More-and LessSkilled Readers’ Comprehension of Easy and Difficult Texts. Cognition and Instruction, 18(4):525– 556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Advaith Siddharthan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Automatically learning cognitive status for multi-document summarization of newswire.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<pages>241--248</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="26992" citStr="Nenkova et al., 2005" startWordPosition="4245" endWordPosition="4248">or our system is parser errors, particularly mistakes in relative clause attachment and clause boundary identificaton. Methods such as those in Siddharthan (2003b) can be used to improve parser performance on these tasks. Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). 5 Conclusions We have presented a framework for text simplification based on synchronous grammars over typed dependency representations. Our HYBRID system, that uses hand-written rules for common syntactic simplifications, and automatically harvested rules for a much larger set of lexicalised simplifications is more robust than a similar system based on quasi-synchronous tree substitution grammars, outperforming it in terms of fluency, simplicity and meaning preservation. By abstracting away from constituent ordering and morphological variations, our approach allow</context>
</contexts>
<marker>Nenkova, Siddharthan, McKeown, 2005</marker>
<rawString>Ani Nenkova, Advaith Siddharthan, and Kathleen McKeown. 2005. Automatically learning cognitive status for multi-document summarization of newswire. In Proceedings of HLT/EMNLP 2005, pages 241–248, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Napoleon Katsos</author>
</authors>
<title>Reformulating discourse connectives for non-expert readers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010),</booktitle>
<location>Los Angeles, CA.</location>
<contexts>
<context position="26950" citStr="Siddharthan and Katsos, 2010" startWordPosition="4238" endWordPosition="4241">ysis shows that the main cause of errorful output for our system is parser errors, particularly mistakes in relative clause attachment and clause boundary identificaton. Methods such as those in Siddharthan (2003b) can be used to improve parser performance on these tasks. Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). 5 Conclusions We have presented a framework for text simplification based on synchronous grammars over typed dependency representations. Our HYBRID system, that uses hand-written rules for common syntactic simplifications, and automatically harvested rules for a much larger set of lexicalised simplifications is more robust than a similar system based on quasi-synchronous tree substitution grammars, outperforming it in terms of fluency, simplicity and meaning preservation. By abstracting away from constituent ordering and mo</context>
</contexts>
<marker>Siddharthan, Katsos, 2010</marker>
<rawString>Advaith Siddharthan and Napoleon Katsos. 2010. Reformulating discourse connectives for non-expert readers. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010), Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Information status distinctions and referring expressions: An empirical study of references to people in news summaries.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="27019" citStr="Siddharthan et al., 2011" startWordPosition="4249" endWordPosition="4252">r errors, particularly mistakes in relative clause attachment and clause boundary identificaton. Methods such as those in Siddharthan (2003b) can be used to improve parser performance on these tasks. Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). 5 Conclusions We have presented a framework for text simplification based on synchronous grammars over typed dependency representations. Our HYBRID system, that uses hand-written rules for common syntactic simplifications, and automatically harvested rules for a much larger set of lexicalised simplifications is more robust than a similar system based on quasi-synchronous tree substitution grammars, outperforming it in terms of fluency, simplicity and meaning preservation. By abstracting away from constituent ordering and morphological variations, our approach allows for linguistically sound </context>
</contexts>
<marker>Siddharthan, Nenkova, McKeown, 2011</marker>
<rawString>Advaith Siddharthan, Ani Nenkova, and Kathleen McKeown. 2011. Information status distinctions and referring expressions: An empirical study of references to people in news summaries. Computational Linguistics, 37(4):811–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>An architecture for a text simplification system.</title>
<date>2002</date>
<booktitle>In Proceedings of the Language Engineering Conference (LEC’02),</booktitle>
<pages>64--71</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="3618" citStr="Siddharthan, 2002" startWordPosition="547" endWordPosition="548">work on text simplification in Section 2, before describing our method in Section 3 and presenting our results in Section 4. 2 Related work There are two largely distinct bodies of work on automatic text simplification – those that use handcrafted rules, and those that apply machine translation approaches. 2.1 Hand-crafted text simplification systems The first body of work uses hand-crafted rules to perform syntactic simplification operations (e.g., splitting coordinated and subordinated clauses, and disembedding apposition and relative clauses). Some early systems (Chandrasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without</context>
</contexts>
<marker>Siddharthan, 2002</marker>
<rawString>Advaith Siddharthan. 2002. An architecture for a text simplification system. In Proceedings of the Language Engineering Conference (LEC’02), pages 64– 71, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Preserving discourse structure when simplifying text.</title>
<date>2003</date>
<booktitle>In Proceedings of the European Natural Language Generation Workshop (ENLG), 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’03),</booktitle>
<pages>103--110</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="26533" citStr="Siddharthan (2003" startWordPosition="4180" endWordPosition="4181">a strength of that system. Our system aims to preserve meaning, which it does rather well. However, this is is not necessarily a valid objective. Perhaps future evaluations should distinguish between modifying information in misleading ways (undesirable) and removing peripheral information (desirable). It is clear that the latter, done well, is useful and will be addressed in future work. An error analysis shows that the main cause of errorful output for our system is parser errors, particularly mistakes in relative clause attachment and clause boundary identificaton. Methods such as those in Siddharthan (2003b) can be used to improve parser performance on these tasks. Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). 5 Conclusions We have presented a framework for text simplification based on synchronous grammars over typed dep</context>
</contexts>
<marker>Siddharthan, 2003</marker>
<rawString>Advaith Siddharthan. 2003a. Preserving discourse structure when simplifying text. In Proceedings of the European Natural Language Generation Workshop (ENLG), 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’03), pages 103–110, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Resolving pronouns robustly: Plumbing the depths of shallowness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Computational Treatments of Anaphora, 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’03),</booktitle>
<pages>7--14</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="26533" citStr="Siddharthan (2003" startWordPosition="4180" endWordPosition="4181">a strength of that system. Our system aims to preserve meaning, which it does rather well. However, this is is not necessarily a valid objective. Perhaps future evaluations should distinguish between modifying information in misleading ways (undesirable) and removing peripheral information (desirable). It is clear that the latter, done well, is useful and will be addressed in future work. An error analysis shows that the main cause of errorful output for our system is parser errors, particularly mistakes in relative clause attachment and clause boundary identificaton. Methods such as those in Siddharthan (2003b) can be used to improve parser performance on these tasks. Finally, this work and the cited related work only investigate sentence-level text simplification. There are various discourse level effects that also need to be considered when simplifying larger texts, including sentence ordering (Barzilay et al., 2002; Siddharthan, 2003a; Barzilay and Lapata, 2008), discourse connectives (Siddharthan and Katsos, 2010) and anaphora choice (Nenkova et al., 2005; Siddharthan et al., 2011). 5 Conclusions We have presented a framework for text simplification based on synchronous grammars over typed dep</context>
</contexts>
<marker>Siddharthan, 2003</marker>
<rawString>Advaith Siddharthan. 2003b. Resolving pronouns robustly: Plumbing the depths of shallowness. In Proceedings of the Workshop on Computational Treatments of Anaphora, 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’03), pages 7–14, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Complex lexico-syntactic reformulation of sentences using typed dependency representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference (INLG 2010),</booktitle>
<location>Dublin</location>
<contexts>
<context position="4175" citStr="Siddharthan, 2010" startWordPosition="623" endWordPosition="624">early systems (Chandrasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 Text simplification as monolingual translation Recent years have seen the increased application of machine translation approaches to text simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplification. PMBT can only p</context>
</contexts>
<marker>Siddharthan, 2010</marker>
<rawString>Advaith Siddharthan. 2010. Complex lexico-syntactic reformulation of sentences using typed dependency representations. In Proceedings of the 6th International Natural Language Generation Conference (INLG 2010), Dublin Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Text simplification using typed dependencies: a comparison of the robustness of different generation strategies.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>2--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4195" citStr="Siddharthan, 2011" startWordPosition="625" endWordPosition="626">drasekar et al., 1996; Siddharthan, 2002) used flat representations (chunked and part-of-speech tagged text). More commonly, text simplification systems use 722 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 722–731, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics hand crafted rules that apply to hierarchical representations, including constituency-based parses (Canning, 2002; Candido Jr et al., 2009; De Belder and Moens, 2010) and dependency parses (Bott et al., 2012; Siddharthan, 2010; Siddharthan, 2011). For languages without corpora of simplified texts, hand crafted systems are typically the only available alternative. 2.2 Text simplification as monolingual translation Recent years have seen the increased application of machine translation approaches to text simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplification. PMBT can only perform a small set o</context>
<context position="8673" citStr="Siddharthan (2011)" startWordPosition="1327" endWordPosition="1329">eads to further problems. From an aligned pair “John, who was tired, went to sleep.” and “John was tired. He went to sleep.”, systems would learn a simplification rule that introduces the pronoun “He”. The governing syntax for this rule is the verb “went”; hence, “Susan, who was tired, went to sleep.” might later get simplified as “Susan was tired. He went to sleep.”. Hand-crafted systems have an advantage here. Such systems would typically use rules that duplicate the noun phrase, generating “John was 723 tired. John went to sleep.” and “Susan was tired. Susan went to sleep.” Systems such as Siddharthan (2011) use transformation rules that encode morphological changes as well as deletions, re-orderings, substitutions and sentence splitting, and are well suited to handle the voice conversion example above. On the other hand, hand-crafted systems are limited in scope to syntactic simplification. While purely syntactic rules can be written manually, there are too many lexico-syntactic and lexical simplifications to enumerate by hand. In this paper, we present a hybrid text simplification system that combines manually written synchronous grammars for common syntactic simplifications with a much larger </context>
<context position="20327" citStr="Siddharthan (2011)" startWordPosition="3147" endWordPosition="3148">dog-7) This list now represents two trees with chased and barking as root nodes: 726 cat dog was dog det det det the a a 3.5 Generating from typed dependency representations Generating from constituency-based parse trees is trivial, in that leaf nodes need to be output in the order processed by a depth first LR search. The higher level of abstraction of dependency representations makes generation more complicated, as the dependencies abstract away from constituent ordering and word morphology. One option is to use an off the shelf generator; however, this does not work well in practice; e.g., Siddharthan (2011) found that misanalyses by the parser can result in unacceptable word and constituent orders in the generated texts. In the system described here, we follow the generation-light approach adopted by Siddharthan (2011). We reuse the word order from the input sentence as a default, and the synchronous grammar encodes any changes in ordering. For example, in Rule PASSIVE2ACTIVE above, we include a further specification: 4 Traversal Order Specifications (a) Node ?X0: [?X3, ?X0, ?X1] This states that for node ?X0, the traversal order should be subtree ?X3 followed by current node ?X0 followed by sub</context>
</contexts>
<marker>Siddharthan, 2011</marker>
<rawString>Advaith Siddharthan. 2011. Text simplification using typed dependencies: a comparison of the robustness of different generation strategies. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 2–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>23--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6216" citStr="Smith and Eisner, 2006" startWordPosition="931" endWordPosition="935">rning phrase to be inserted to complete the sentence. This allows the translation model to handle constructs such as relative clauses and apposition. Dras (1999) was the first to apply synchronous grammars to monolingual tasks. His approach is to map between two TAG grammars using a Generalised Synchronous TAG formalism, and to use Integer Programming to generate a text that satisfies the externally imposed constraints (such as length or readability) using minimal paraphrasing. Woodsend and Lapata (2011) further develop this line of research. Their model is based on quasi-synchronous grammar (Smith and Eisner, 2006) and integer linear programming. Quasisynchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. The Woodsend and Lapata (2011) model is trained on two different datasets: one containing alignments between sentences in Wikipedia and English Simple Wikipedia, and one containing alignments between edits in the revision history of Simple Wikipedia. The latter performs best in their study, and also achieves better scores than the Zhu et al. (2010) sy</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>David A Smith and Jason Eisner. 2006. Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings of the Workshop on Statistical Machine Translation, pages 23–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>409--420</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6102" citStr="Woodsend and Lapata (2011)" startWordPosition="913" endWordPosition="916">). A completion table stores probabilities of the splitting word to be deleted from the translation, and for the governing phrase to be inserted to complete the sentence. This allows the translation model to handle constructs such as relative clauses and apposition. Dras (1999) was the first to apply synchronous grammars to monolingual tasks. His approach is to map between two TAG grammars using a Generalised Synchronous TAG formalism, and to use Integer Programming to generate a text that satisfies the externally imposed constraints (such as length or readability) using minimal paraphrasing. Woodsend and Lapata (2011) further develop this line of research. Their model is based on quasi-synchronous grammar (Smith and Eisner, 2006) and integer linear programming. Quasisynchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. The Woodsend and Lapata (2011) model is trained on two different datasets: one containing alignments between sentences in Wikipedia and English Simple Wikipedia, and one containing alignments between edits in the revision history of Simple</context>
<context position="7521" citStr="Woodsend and Lapata (2011)" startWordPosition="1134" endWordPosition="1138">rammaticality and meaning preservation. We will directly compare our approach to Woodsend and Lapata (2011), as this is the best performing contemporary system that has the same linguistic scope as ours. 2.3 Formalisms and linguistic coverage The systems summarised above differ primarily in the level of linguistic knowledge they encode. PBMT systems use the least knowledge, and as such are ill equipped to to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Syntax based approaches use syntactic knowledge. However, both Zhu et al. (2010) and Woodsend and Lapata (2011) use the Stanford Parser (Klein and Manning, 2003) for syntactic structure, and this representation lacks morphological information. This means that some simplification operations such as voice conversion are not handled well. For example, to simplify “trains are liked by John” to “John likes trains”, besides deleting auxiliaries and reordering the arguments of the verb “like”, the verb also needs to agree in number with the new subject (“John”), and take the tense of the auxiliary verb (“are”). The grammar acquisition process leads to further problems. From an aligned pair “John, who was tire</context>
<context position="10605" citStr="Woodsend and Lapata (2011)" startWordPosition="1619" endWordPosition="1622"> changes, (2) the higher level of abstraction makes it easy to write and read grammar rules; thus common syntactic operations (such as conversion of passive to active voice) can be handled in this framework through accurate hand-written rules, and (3) It is easier and more elegant to automatically acquire a synchronous grammar from data, compared to synchronous grammars based on constituency-parses. In this section we describe our framework and text simplification system in more detail; then, in section 4, we report an evaluation that compares our system against a human simplification and the Woodsend and Lapata (2011) system. 3.1 Synchronous dependency insertion grammars Ding and Palmer (2005) introduce the notion of a Synchronous Dependency Insertion Grammar (SDIG) as a tree substitution grammar defined on dependency trees. They define elementary trees (ETs) to be sub-sentential dependency structures containing one or more lexical items. The SDIG formalism assumes that the isomorphism of the two syntactic structures is at the ET level, thus allowing for non-isomorphic tree to tree mapping at the sentence level. We base our approach to text simplification on SDIGs, but the formalism is adapted for the mono</context>
<context position="12212" citStr="Woodsend and Lapata (2011)" startWordPosition="1880" endWordPosition="1884">the linguistic knowledge we need to encode in these rules, the method for automatic acquisition of rules from a corpus of aligned sentences, and the generation process. Input Sentence −+ Dependency Parse −+ Source ETs 1 ET Transfer 1 Output Sentences *--− Generation *--− Target ETs Figure 1: System Architecture 3.2 Extracting synchronous grammars from aligned sentences To acquire a synchronous grammar from dependency parses of aligned English and simple English sentences, we just need to identify the differences. For example, consider two aligned sentences from the aligned corpus described in Woodsend and Lapata (2011): 1. (a) Also, lichen fungi can reproduce sexually, producing spores. (b) Also, lichen fungi can reproduce sexually by producing spores. An automatic comparison of the dependency parses for the two sentences (using the Stanford Parser, and ignoring punctuation for ease of presentation) reveals that there are two typed dependencies that occur only in the parse of the first sentence, and two that occur only in the parse of the second sentence (in italics): 724 reproduce reproduce Figure 2: Transduction of Elementary Trees (ETs) 1. (a) 1. (b) advmod(reproduce, Also) advmod(reproduce, Also) nn(fun</context>
<context position="15965" citStr="Woodsend and Lapata (2011)" startWordPosition="2453" endWordPosition="2456">d(?X0, ?X2[big]) 3. NODE OPERATION (a) MOVE: ?X1 −→ ?X2 This rule states that any of the words in “[extensive, large, massive, sizable, major, powerful, unprecedented, developed, giant]” can be replaced by “big” in any lexical context ?X0; i.e., these words are not ambiguous. We acquire rules such as the above automatically, filtering out rules that involve syntactic constructs that we require manually-written rules for (relative clauses, apposition, coordination and subordination). We have extracted 3180 rules from SEW revision histories and aligned SEW-EW sentence pairs. From the same data, Woodsend and Lapata (2011) extract 1431 rules, but these include rules for deletion, as well as for purely syntactic sentence splitting. The 3180 rules we derive are only lexical simplifications or simple paraphrases. We do not perform deletion operations, and use manually written rules for sentence splitting rules xcomp dobj spores producing producing prep by spores amod 725 Our approach allows for the encoding of local lexico-syntactic context for lexical simplification. Only if a simplification is seen in many contexts do we generalise the rule by relaxing the lexical context. We consider this a better solution to t</context>
<context position="22105" citStr="Woodsend and Lapata (2011)" startWordPosition="3434" endWordPosition="3437">. 3. ORDERING: List of nodes with subtree order specified 4. NODE-OPERATIONS: List of morphological changes and deletion operations on nodes. At present the automatically harvested rules do not encode morphological changes. They do however encode reordering information, which is automatically detected from the relative word positions in the original and simplified training sentences. 4 Evaluation We performed a manual evaluation of how fluent and simple the text produced by our simplification system is, and the extent to which it preserves meaning. We use the evaluation set previously used by Woodsend and Lapata (2011), Zhu et al. (2010) and Wubben et al. (2012). This consists of 100 sentences from English Wikipedia, aligned with Simple English Wikipedia (SEW) sentences. Previous work report various automatic measures, including BLEU and readability metrics such as the Flesch-Kincaid Grade Level Index (FKGL). None of these have been validated for the automatic text simplification task, however, and we prefer to conduct an evaluation with human raters. Our system (henceforth, HYBRID) is compared to QTSG (the system by Woodsend and Lapata (2011) that learns a quasi-synchronous grammar from the same data as th</context>
<context position="24642" citStr="Woodsend and Lapata (2011)" startWordPosition="3874" endWordPosition="3877">hem have previously seen the output of text simplification systems. chased dobj nsubj barking aux nsubj 727 Rater FLUENCY SEW SIMPLICITY SEW MEANING PRESERVATION QTSG HYBRID QTSG HYBRID QTSG HYBRID SEW R1 2.60 4.44 4.60 3.04 3.88 4.36 3.16 4.68 4.24 R2 3.08 4.24 4.52 3.20 4.08 4.48 3.28 4.76 4.36 R3 2.40 4.20 4.68 3.12 3.80 4.44 2.96 4.52 3.80 R4 2.32 3.88 3.48 2.92 3.44 3.44 2.72 4.52 3.56 R5 2.00 3.44 3.48 2.00 3.52 3.56 2.48 4.52 3.84 Mean 2.48 4.04 4.15 2.85 3.74 4.05 2.92 4.60 3.96 Median 2 4 4 3 4 4 3 5 4 Table 1: Results of human evaluation with five raters R1–R5. QTSG is the system by Woodsend and Lapata (2011). HYBRID is the system described in this paper, with manual and automatically acquired rules. SEW is the human generated simplification from Simple English Wikipedia. All differences in means for Simplicity and Meaning Preservation are significant (p &lt; 0.001; t-test). For Fluency, HYBRID and SEW are significantly better than QTSG (p &lt; 0.001; t-test). large part because it is the only version that does not delete information through sentence compression). Table 2 shows some examples of simplifications from the evaluation dataset, along with their average scores for fluency, simplicity and meani</context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 409–420. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>1015--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Wubben, van den Bosch, Krahmer, 2012</marker>
<rawString>Sander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 1015–1024. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5158" citStr="Yamada and Knight, 2001" startWordPosition="768" endWordPosition="771"> the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplification. PMBT can only perform a small set of simplification operations, such as lexical substitution, deletion and simple paraphrase. They are not well suited for reordering or splitting operations. Specifically, the syntactic simplification operations that handcrafted systems focus on are out of scope. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input sentences: substitution, reordering, splitting, and deletion. Splitting is encoded as two probabilities: A segmentation table stores probabilities of sentence splitting at particular words (e.g., which). A completion table stores probabilities of the splitting word to be deleted from the translation, and for the governing phrase to be inserted to complete the sentence. This allows the translation model to handle constructs such as relative clauses and apposition. Dras (1999) was</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhemin Zhu</author>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>A monolingual tree-based translation model for sentence simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd international conference on computational linguistics,</booktitle>
<pages>1353--1361</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5074" citStr="Zhu et al. (2010)" startWordPosition="755" endWordPosition="758">simplification, often referred to as “monolingual translation”, and driven by the new availability of corpora of simplified texts such as Simple English Wikipedia (SEW). Wubben et al. (2012) and Coster and Kauchak (2011) apply Phrase Based Machine Translation (PBMT) to the task of text simplification. PMBT can only perform a small set of simplification operations, such as lexical substitution, deletion and simple paraphrase. They are not well suited for reordering or splitting operations. Specifically, the syntactic simplification operations that handcrafted systems focus on are out of scope. Zhu et al. (2010) in contrast present an approach based on syntax-based SMT (Yamada and Knight, 2001). Their translation model encodes probabilities for four specific rewrite operations on the parse trees of the input sentences: substitution, reordering, splitting, and deletion. Splitting is encoded as two probabilities: A segmentation table stores probabilities of sentence splitting at particular words (e.g., which). A completion table stores probabilities of the splitting word to be deleted from the translation, and for the governing phrase to be inserted to complete the sentence. This allows the translation</context>
<context position="6813" citStr="Zhu et al. (2010)" startWordPosition="1026" endWordPosition="1029">th and Eisner, 2006) and integer linear programming. Quasisynchronous grammars, like the Generalised Synchronous TAGs of Dras (1999), aims to relax the isomorphism constraints of synchronous grammars, in this case by generating a loose alignment between parse trees. The Woodsend and Lapata (2011) model is trained on two different datasets: one containing alignments between sentences in Wikipedia and English Simple Wikipedia, and one containing alignments between edits in the revision history of Simple Wikipedia. The latter performs best in their study, and also achieves better scores than the Zhu et al. (2010) system, both when evaluated using BLEU, and on human evaluations of simplicity, grammaticality and meaning preservation. We will directly compare our approach to Woodsend and Lapata (2011), as this is the best performing contemporary system that has the same linguistic scope as ours. 2.3 Formalisms and linguistic coverage The systems summarised above differ primarily in the level of linguistic knowledge they encode. PBMT systems use the least knowledge, and as such are ill equipped to to handle simplifications that require morphological changes, syntactic reordering or sentence splitting. Syn</context>
<context position="22124" citStr="Zhu et al. (2010)" startWordPosition="3438" endWordPosition="3441"> with subtree order specified 4. NODE-OPERATIONS: List of morphological changes and deletion operations on nodes. At present the automatically harvested rules do not encode morphological changes. They do however encode reordering information, which is automatically detected from the relative word positions in the original and simplified training sentences. 4 Evaluation We performed a manual evaluation of how fluent and simple the text produced by our simplification system is, and the extent to which it preserves meaning. We use the evaluation set previously used by Woodsend and Lapata (2011), Zhu et al. (2010) and Wubben et al. (2012). This consists of 100 sentences from English Wikipedia, aligned with Simple English Wikipedia (SEW) sentences. Previous work report various automatic measures, including BLEU and readability metrics such as the Flesch-Kincaid Grade Level Index (FKGL). None of these have been validated for the automatic text simplification task, however, and we prefer to conduct an evaluation with human raters. Our system (henceforth, HYBRID) is compared to QTSG (the system by Woodsend and Lapata (2011) that learns a quasi-synchronous grammar from the same data as the automated compone</context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of the 23rd international conference on computational linguistics, pages 1353–1361. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>