<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000636">
<title confidence="0.702832">
Segmented and unsegmented dialogue-act annotation with statistical
dialogue models*
</title>
<author confidence="0.628458">
Carlos D. Martinez Hinarejos, Ram´on Granell, Jos´e Miguel BenediDepartamento de Sistemas Inform´aticos y Computaci´on
Universidad Polit´ecnica de Valencia
</author>
<affiliation confidence="0.389539">
Camino de Vera, s/n, 46022, Valencia
</affiliation>
<email confidence="0.997188">
{cmartine,rgranell,jbenedi}@dsic.upv.es
</email>
<sectionHeader confidence="0.997367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843333333334">
Dialogue systems are one of the most chal-
lenging applications of Natural Language
Processing. In recent years, some statis-
tical dialogue models have been proposed
to cope with the dialogue problem. The
evaluation of these models is usually per-
formed by using them as annotation mod-
els. Many of the works on annotation
use information such as the complete se-
quence of dialogue turns or the correct
segmentation of the dialogue. This in-
formation is not usually available for dia-
logue systems. In this work, we propose a
statistical model that uses only the infor-
mation that is usually available and per-
forms the segmentation and annotation at
the same time. The results of this model
reveal the great influence that the availabil-
ity of a correct segmentation has in ob-
taining an accurate annotation of the dia-
logues.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994233607843137">
In the Natural Language Processing (NLP) field,
one of the most challenging applications is dia-
logue systems (Kuppevelt and Smith, 2003). A
dialogue system is usually defined as a com-
puter system that can interact with a human be-
ing through dialogue in order to complete a spe-
cific task (e.g., ticket reservation, timetable con-
sultation, bank operations,...) (Aust et al., 1995;
Hardy et al., 2002). Most dialogue system have a
characteristic behaviour with respect to dialogue
*Work partially supported by the Spanish project
TIC2003-08681-C02-02 and by Spanish Ministry of Culture
under FPI grants.
management, which is known as dialogue strat-
egy. It defines what the dialogue system must do
at each point of the dialogue.
Most of these strategies are rule-based, i.e., the
dialogue strategy is defined by rules that are usu-
ally defined by a human expert (Gorin et al., 1997;
Hardy et al., 2003). This approach is usually diffi-
cult to adapt or extend to new domains where the
dialogue structure could be completely different,
and it requires the definition of new rules.
Similar to other NLP problems (like speech
recognition and understanding, or statistical ma-
chine translation), an alternative data-based ap-
proach has been developed in the last decade (Stol-
cke et al., 2000; Young, 2000). This approach re-
lies on statistical models that can be automatically
estimated from annotated data, which in this case,
are dialogues from the task.
Statistical modelling learns the appropriate pa-
rameters of the models from the annotated dia-
logues. As a simplification, it could be considered
that each label is associated to a situation in the di-
alogue, and the models learn how to identify and
react to the different situations by estimating the
associations between the labels and the dialogue
events (words, the speaker, previous turns, etc.).
An appropriate annotation scheme should be de-
fined to capture the elements that are really impor-
tant for the dialogue, eliminating the information
that is irrelevant to the dialogue process. Several
annotation schemes have been proposed in the last
few years (Core and Allen, 1997; Dybkjaer and
Bernsen, 2000).
One of the most popular annotation schemes at
the dialogue level is based on Dialogue Acts (DA).
A DA is a label that defines the function of the an-
notated utterance with respect to the dialogue pro-
cess. In other words, every turn in the dialogue
</bodyText>
<page confidence="0.982174">
563
</page>
<note confidence="0.723525">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 563–570,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999816428571429">
is supposed to be composed of one or more ut-
terances. In this context, from the dialogue man-
agement viewpoint an utterance is a relevant sub-
sequence . Several DA annotation schemes have
been proposed in recent years (DAMSL (Core and
Allen, 1997), VerbMobil (Alexandersson et al.,
1998), Dihana (Alc´acer et al., 2005)).
In all these studies, it is necessary to annotate
a large amount of dialogues to estimate the pa-
rameters of the statistical models. Manual anno-
tation is the usual solution, although is very time-
consuming and there is a tendency for error (the
annotation instructions are not usually easy to in-
terpret and apply, and human annotators can com-
mit errors) (Jurafsky et al., 1997).
Therefore, the possibility of applying statistical
models to the annotation problem is really inter-
esting. Moreover, it gives the possibility of evalu-
ating the statistical models. The evaluation of the
performance of dialogue strategies models is a dif-
ficult task. Although many proposals have been
made (Walker et al., 1997; Fraser, 1997; Stolcke
et al., 2000), there is no real agreement in the NLP
community about the evaluation technique to ap-
ply.
Our main aim is the evaluation of strategy mod-
els, which provide the reaction of the system given
a user input and a dialogue history. Using these
models as annotation models gives us a possible
evaluation: the correct recognition of the labels
implies the correct recognition of the dialogue sit-
uation; consequently this information can help the
system to react appropriately. Many recent works
have attempted this approach (Stolcke et al., 2000;
Webb et al., 2005).
However, many of these works are based on the
hypothesis of the availability of the segmentation
into utterances of the turns of the dialogue. This is
an important drawback in order to evaluate these
models as strategy models, where segmentation is
usually not available. Other works rely on a de-
coupled scheme of segmentation and DA classifi-
cation (Ang et al., 2005).
In this paper, we present a new statistical model
that computes the segmentation and the annota-
tion of the turns at the same time, using a statis-
tical framework that is simpler than the models
that have been proposed to solve both problems
at the same time (Warnke et al., 1997). The results
demonstrate that segmentation accuracy is really
important in obtaining an accurate annotation of
the dialogue, and consequently in obtaining qual-
ity strategy models. Therefore, more accurate seg-
mentation models are needed to perform this pro-
cess efficiently.
This paper is organised as follows: Section 2,
presents the annotation models (for both the un-
segmented and segmented versions); Section 3,
describes the dialogue corpora used in the ex-
periments; Section 4 establishes the experimental
framework and presents a summary of the results;
Section 5, presents our conclusions and future re-
search directions.
</bodyText>
<sectionHeader confidence="0.931932" genericHeader="method">
2 Annotation models
</sectionHeader>
<bodyText confidence="0.998865837837838">
The statistical annotation model that we used ini-
tially was inspired by the one presented in (Stol-
cke et al., 2000). Under a maximum likeli-
hood framework, they developed a formulation
that assigns DAs depending on the conversation
evidence (transcribed words, recognised words
from a speech recogniser, phonetic and prosodic
features,...). Stolcke’s model uses simple and
popular statistical models: N-grams and Hidden
Markov Models. The N-grams are used to model
the probability of the DA sequence, while the
HMM are used to model the evidence likelihood
given the DA. The results presented in (Stolcke et
al., 2000) are very promising.
However, the model makes some unrealistic as-
sumptions when they are evaluated to be used as
strategy models. One of them is that there is a
complete dialogue available to perform the DA
assignation. In a real dialogue system, the only
available information is the information that is
prior to the current user input. Although this al-
ternative is proposed in (Stolcke et al., 2000), no
experimental results are given.
Another unrealistic assumption corresponds to
the availability of the segmentation of the turns
into utterances. An utterance is defined as a
dialogue-relevant subsequence of words in the cur-
rent turn (Stolcke et al., 2000). It is clear that the
only information given in a turn is the usual in-
formation: transcribed words (for text systems),
recognised words, and phonetic/prosodic features
(for speech systems). Therefore, it is necessary to
develop a model to cope with both the segmenta-
tion and the assignation problem.
Let Ud1 = U1U2 · · · Ud be the sequence of DA
assigned until the current turn, corresponding to
the first d segments of the current dialogue. Let
</bodyText>
<page confidence="0.968661">
564
</page>
<bodyText confidence="0.6452884">
W = w1w2 ... wl be the sequence of the words
of the current turn, where subsequences Wij =
wiwi+1 ... wj can be defined (1 ≤ i ≤ j ≤ l).
For the sequence of words W, a segmentation
is defined as sr1 = s0s1 ... sr, where s0 = 0 and
</bodyText>
<equation confidence="0.8761645">
W = Ws1 s0+1Ws2
s1+1 . . . Wsr
</equation>
<bodyText confidence="0.995500666666667">
sr�1+1. Therefore, the
optimal sequence of DA for the current turn will
be given by:
</bodyText>
<equation confidence="0.998750571428571">
U = argmax Pr(U|Wl1, Ud1) =
U
�
argmax Pr(Ud+r
d+1 |Wl1, Ud1)
d+r
Ud+1 (si,r)
</equation>
<bodyText confidence="0.999962666666667">
After developing this formula and making sev-
eral assumptions and simplifications, the final
model, called unsegmented model, is:
</bodyText>
<equation confidence="0.99862975">
U� = argmax
Ud+r
d+1
d+r
� Pr(Uk|Uk−1
k−n−1) Pr(Wsk�d
sk�(d+1)+1|Uk)
k=d+1
</equation>
<bodyText confidence="0.999898083333333">
This model can be easily implemented using
simple statistical models (N-grams and Hidden
Markov Models). The decoding (segmentation
and DA assignation) was implemented using the
Viterbi algorithm. A Word Insertion Penalty
(WIP) factor, similar to the one used in speech
recognition, can be incorporated into the model to
control the number of utterances and avoid exces-
sive segmentation.
When the segmentation into utterances is pro-
vided, the model can be simplified into the seg-
mented model, which is:
</bodyText>
<equation confidence="0.994121333333333">
U = argmax
Ud+r
d+1
</equation>
<bodyText confidence="0.9999012">
All the presented models only take into account
word transcriptions and dialogue acts, although
they could be extended to deal with other features
(like prosody, sintactical and semantic informa-
tion, etc.).
</bodyText>
<sectionHeader confidence="0.995142" genericHeader="method">
3 Experimental data
</sectionHeader>
<bodyText confidence="0.999972214285714">
Two corpora with very different features were
used in the experiment with the models proposed
in Section 2. The SwitchBoard corpus is com-
posed of human-human, non task-oriented dia-
logues with a large vocabulary. The Dihana corpus
is composed of human-computer, task-oriented di-
alogues with a small vocabulary.
Although two corpora are not enough to let us
draw general conclusions, they give us more reli-
able results than using only one corpus. Moreover,
the very different nature of both corpora makes
our conclusions more independent from the cor-
pus type, the annotation scheme, the vocabulary
size, etc.
</bodyText>
<subsectionHeader confidence="0.999814">
3.1 The SwitchBoard corpus
</subsectionHeader>
<bodyText confidence="0.999986">
The first corpus used in the experiments was the
well-known SwitchBoard corpus (Godfrey et al.,
1992). The SwitchBoard database consists of
human-human conversations by telephone with no
directed tasks. Both speakers discuss about gen-
eral interest topics, but without a clear task to ac-
complish.
The corpus is formed by 1,155 conversations,
which comprise 126,754 different turns of spon-
taneous and sometimes overlapped speech, using
a vocabulary of 21,797 different words. The cor-
pus was segmented into utterances, each of which
was annotated with a DA following the simpli-
fied DAMSL annotation scheme (Jurafsky et al.,
1997). The set of labels of the simplified DAMSL
scheme is composed of 42 different labels, which
define categories such as statement, backchannel,
opinion, etc. An example of annotation is pre-
sented in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999647">
3.2 The Dihana corpus
</subsectionHeader>
<bodyText confidence="0.999967647058824">
The second corpus used was a task-oriented cor-
pus called Dihana (Benediet al., 2004). It is com-
posed of computer-to-human dialogues, and the
main aim of the task is to answer telephone queries
about train timetables, fares, and services for long-
distance trains in Spanish. A total of 900 dialogues
were acquired by using the Wizard of Oz tech-
nique and semicontrolled scenarios. Therefore,
the voluntary caller was always free to express
him/herself (there were no syntactic or vocabu-
lary restrictions); however, in some dialogues, s/he
had to achieve some goals using a set of restric-
tions that had been given previously (e.g. depar-
ture/arrival times, origin/destination, travelling on
a train with some services, etc.).
These 900 dialogues comprise 6,280 user turns
and 9,133 system turns. Obviously, as a task-
</bodyText>
<figure confidence="0.8029158">
max
(sr1,r)
d+rH Pr(Uk|Uk−1
k=d+1 k−n−1) Pr(W sk�d
sk�(d+1)+1|Uk)
</figure>
<page confidence="0.910647">
565
</page>
<table confidence="0.9729425">
Utterance Label
YEAH, TO GET REFERENCES AND THAT, SO, BUT, UH, I DON’T FEEL COMFORTABLE ABOUT LEAVING MY KIDS IN A BIG
DAY CARE CENTER, SIMPLY BECAUSE THERE’S SO MANY KIDS AND SO MANY &lt;SNIFFING&gt; &lt;THROAT CLEARING&gt;
Yeah, aa
to get references and that, sd
so, but, uh, %
I don’t feel comfortable about leaving my kids in a big day care center, simply because there’s so sd
many kids and so many &lt;sniffing&gt; &lt;throat clearing&gt;
I THINK SHE HAS PROBLEMS WITH THAT, TOO.
I think she has problems with that, too. sd
</table>
<figureCaption confidence="0.99648">
Figure 1: An example of annotated turns in the SwitchBoard corpus.
</figureCaption>
<bodyText confidence="0.999862966666667">
oriented and medium size corpus, the total number
of different words in the vocabulary, 812, is not as
large as the Switchboard database.
The turns were segmented into utterances. It
was possible for more than one utterance (with
their respective labels) to appear in a turn (on av-
erage, there were 1.5 utterances per user/system
turn). A three-level annotation scheme of the ut-
terances was defined (Alc´acer et al., 2005). These
labels represent the general purpose of the utter-
ance (first level), as well as more specific semantic
information (second and third level): the second
level represents the data focus in the utterance and
the third level represents the specific data present
in the utterance. An example of three-level anno-
tated user turns is given in Figure 2. The corpus
was annotated by means of a semiautomatic pro-
cedure, and all the dialogues were manually cor-
rected by human experts using a very specific set
of defined rules.
After this process, there were 248 different la-
bels (153 for user turns, 95 for system turns) using
the three-level scheme. When the detail level was
reduced to the first and second levels, there were
72 labels (45 for user turns, 27 for system turns).
When the detail level was limited to the first level,
there were only 16 labels (7 for user turns, 9 for
system turns). The differences in the number of
labels and in the number of examples for each la-
bel with the SwitchBoard corpus are significant.
</bodyText>
<sectionHeader confidence="0.99746" genericHeader="evaluation">
4 Experiments and results
</sectionHeader>
<bodyText confidence="0.933514666666667">
The SwitchBoard database was processed to re-
move certain particularities. The main adaptations
performed were:
</bodyText>
<listItem confidence="0.977205">
• The interrupted utterances (which were la-
belled with ’+’) were joined to the correct
previous utterance, thereby avoiding inter-
ruptions (i.e., all the words of the interrupted
utterance were annotated with the same DA).
</listItem>
<tableCaption confidence="0.866084">
Table 1: SwitchBoard database statistics (mean for
the ten cross-validation partitions)
</tableCaption>
<table confidence="0.999756833333333">
Training Test
Dialogues 1,136 19
Turns 113,370 1,885
Utterances 201,474 3,718
Running words 1,837,222 33,162
Vocabulary 21,248 2,579
</table>
<listItem confidence="0.986198">
• All the words were transcribed in lowercase.
• Puntuaction marks were separated from
words.
</listItem>
<bodyText confidence="0.999965117647059">
The experiments were performed using a cross-
validation approach to avoid the statistical bias
that can be introduced by the election of fixed
training and test partitions. This cross-validation
approach has also been adopted in other recent
works on this corpus (Webb et al., 2005). In our
case, we performed 10 different experiments. In
each experiment, the training partition was com-
posed of 1,136 dialogues, and the test partition
was composed of 19 dialogues. This proportion
was adopted so that our results could be compared
with the results in (Stolcke et al., 2000), where
similar training and test sizes were used. The
mean figures for the training and test partitions are
shown in Table 1.
With respect to the Dihana database, the prepro-
cessing included the following points:
</bodyText>
<listItem confidence="0.991120625">
• A categorisation process was performed for
categories such as town names, the time,
dates, train types, etc.
• All the words were transcribed in lowercase.
• Puntuaction marks were separated from
words.
• All the words were preceded by the speaker
identification (U for user, M for system).
</listItem>
<page confidence="0.996645">
566
</page>
<table confidence="0.971094777777778">
Utterance 1st level 2nd level 3rd level
YES, TIMES AND FARES.
Yes, Acceptance Dep Hour Nil
times and fares Question Dep Hour,Fare Nil
YES, I WANT TIMES AND FARES OF TRAINS THAT ARRIVE BEFORE SEVEN.
Yes, I want times and fares of trains that arrive before seven. Question Dep Hour,Fare Arr Hour
ON THURSDAY IN THE AFTERNOON.
On thursday Answer Day Day
in the afternoon Answer Time Time
</table>
<figureCaption confidence="0.689579">
Figure 2: An example of annotated turns in the Dihana corpus. Original turns were in Spanish.
</figureCaption>
<tableCaption confidence="0.9824265">
Table 2: Dihana database statistics (mean for the
five cross-validation partitions)
</tableCaption>
<table confidence="0.999382714285714">
Training Test
Dialogues 720 180
Turns 12,330 3,083
User turns 5,024 1,256
System turns 7,206 1,827
Utterances 18,837 4,171
User utterances 7,773 1,406
System utterances 11,064 2,765
Running words 162,613 40,765
User running words 42,806 10,815
System running words 119,807 29,950
Vocabulary 832 485
User vocabulary 762 417
System vocabulary 208 174
</table>
<bodyText confidence="0.999748818181818">
A cross-validation approach was adopted in Di-
hana as well. In this case, only 5 different parti-
tions were used. Each of them had 720 dialogues
for training and 180 for testing. The statistics on
the Dihana corpus are presented in Table 2.
For both corpora, different N-gram models,
with N = 2, 3, 4, and HMM of one state were
trained from the training database. In the case of
the SwitchBoard database, all the turns in the test
set were used to compute the labelling accuracy.
However, for the Dihana database, only the user
turns were taken into account (because system
turns follow a regular, template-based scheme,
which presents artificially high labelling accura-
cies). Furthermore, in order to use a really sig-
nificant set of labels in the Dihana corpus, we
performed the experiments using only two-level
labels instead of the complete three-level labels.
This restriction allowed us to be more independent
from the understanding issues, which are strongly
related to the third level. It also allowed us to con-
centrate on the dialogue issues, which relate more
</bodyText>
<tableCaption confidence="0.971365">
Table 3: SwitchBoard results for the segmented
model
</tableCaption>
<table confidence="0.98042425">
N-gram Utt. accuracy Turn accuracy
2-gram 68.19% 59.33%
3-gram 68.50% 59.75%
4-gram 67.90% 59.14%
</table>
<bodyText confidence="0.982238">
to the first and second levels.
The results in the case of the segmented ap-
proach described in Section 2 for SwitchBoard are
presented in Table 3. Two different definitions of
accuracy were used to assess the results:
</bodyText>
<listItem confidence="0.919708">
• Utterance accuracy: computes the proportion
of well-labelled utterances.
• Turn accuracy: computes the proportion of
totally well-labelled turns (i.e.: if the la-
belling has the same labels in the same or-
der as in the reference, it is taken as a well-
labelled turn).
</listItem>
<bodyText confidence="0.999548461538461">
As expected, the utterance accuracy results are
a bit worse than those presented in (Stolcke et al.,
2000). This may be due to the use of only the
past history and possibly to the cross-validation
approach used in the experiments. The turn accu-
racy was calculated to compare the segmented and
the unsegmented models. This was necessary be-
cause the utterance accuracy does not make sense
for the unsegmented model.
The results for the unsegmented approach for
SwitchBoard are presented in Table 4. In this case,
three different definitions of accuracy were used to
assess the results:
</bodyText>
<listItem confidence="0.986713">
• Accuracy at DA level: the edit distance be-
tween the reference and the labelling of the
turn was computed; then, the number of cor-
rect substitutions (c), wrong substitutions (s),
deletions (d) and insertions (i) was com-
</listItem>
<page confidence="0.998478">
567
</page>
<tableCaption confidence="0.85647925">
Table 5: Dihana results for the segmented model
(only two-level labelling for user turns)
Table 4: SwitchBoard results for the unsegmented
model (WIP=50)
</tableCaption>
<table confidence="0.99924725">
N-gram DA acc. Turn acc. Segm. acc.
2-gram 38.19% 39.47% 38.92%
3-gram 38.58% 39.61% 39.06%
4-gram 38.49% 39.52% 38.96%
N-gram Utt. accuracy Turn accuracy
2-gram 75.70% 74.46%
3-gram 76.28% 74.93%
4-gram 76.39% 75.10%
</table>
<bodyText confidence="0.713311">
puted, and the accuracy was calculated as
</bodyText>
<equation confidence="0.735244">
100 � c
(c+9+i+d).
</equation>
<listItem confidence="0.971835">
• Accuracy at turn level: this provides the pro-
portion of well-labelled turns, without taking
into account the segmentation (i.e., if the la-
belling has the same labels in the same or-
der as in the reference, it is taken as a well-
labelled turn).
• Accuracy at segmentation level: this pro-
vides the proportion of well-labelled and seg-
mented turns (i.e., the labels are the same as
in the reference and they affect the same ut-
terances).
</listItem>
<bodyText confidence="0.999052551724138">
The WIP parameter used in Table 4 was 50,
which is the one that offered the best results. The
segmentation accuracy in Table 4 must be com-
pared with the turn accuracy in Table 3. As Table 4
shows, the accuracy of the labelling decreased dra-
matically. This reveals the strong influence of the
availability of the real segmentation of the turns.
To confirm this hypothesis, similar experiments
were performed with the Dihana database. Ta-
ble 5 presents the results with the segmented cor-
pus, and Table 6 presents the results with the un-
segmented corpus (with WIP=50, which gave the
best results). In this case, only user turns were
taken into account to compute the accuracy, al-
though the model was applied to all the turns (both
user and system turns). For the Dihana corpus,
the degradation of the results of the unsegmented
approach with respect to the segmented approach
was not as high as in the SwitchBoard corpus, due
to the smaller vocabulary and complexity of the
dialogues.
These results led us to the same conclusion,
even for such a different corpus (much more la-
bels, task-oriented, etc.). In any case, these ac-
curacy figures must be taken as a lower bound on
the model performance because sometimes an in-
correct recognition of segment boundaries or dia-
logue acts does not cause an inappropriate reaction
of the dialogue strategy.
</bodyText>
<tableCaption confidence="0.647473333333333">
Table 6: Dihana results for the unsegmented
model (WIP=50, only two-level labelling for user
turns)
</tableCaption>
<table confidence="0.96722075">
N-gram DA acc. Turn acc. Segm. acc.
2-gram 60.36% 62.86% 58.15%
3-gram 60.05% 62.49% 57.87%
4-gram 59.81% 62.44% 57.88%
</table>
<bodyText confidence="0.999894636363636">
An illustrative example of annotation errors in
the SwitchBoard database, is presented in Figure 3
for the same turns as in Figure 1. An error anal-
ysis of the segmented model was performed. The
results reveals that, in the case of most of the er-
rors were produced by the confusion of the ’sv’
and ’sd’ classes (about 50% of the times ’sv’ was
badly labelled, the wrong label was ’sd’) The sec-
ond turn in Figure 3 is an example of this type of
error. The confusions between the ’aa’ and ’b’
classes were also significant (about 27% of the
times ’aa’ was badly labelled, the wrong label was
’b’). This was reasonable due to the similar defini-
tions of these classes (which makes the annotation
difficult, even for human experts). These errors
were similar for all the N-grams used. In the case
of the unsegmented model, most of the errors were
produced by deletions of the ’sd’ and ’sv’ classes,
as in the first turn in Figure 3 (about 50% of the
errors). This can be explained by the presence of
very short and very long utterances in both classes
(i.e., utterances for ’sd’ and ’sv’ did not present a
regular length).
Some examples of errors in the Dihana corpus
are shown in Figure 4 (in this case, for the same
turns as those presented in Figure 2). In the seg-
mented model, most of the errors were substitu-
tions between labels with the same first level (es-
pecially questions and answers) where the second
level was difficult to recognise. The first and third
turn in Figure 4 are examples of this type of er-
ror. This was because sometimes the expressions
only differed with each other by one word, or
</bodyText>
<page confidence="0.992938">
568
</page>
<table confidence="0.998581428571429">
Utt Label
1 % Yeah, to get references and that, so, but, uh, I don’t
2 sd
feel comfortable about leaving my kids in a big day care center, simply because
there’s so many kids and so many &lt;sniffing&gt; &lt;throat clearing&gt;
Utt Label
1 sv I think she has problems with that, too.
</table>
<figureCaption confidence="0.985836">
Figure 3: An example of errors produced by the model in the SwitchBoard corpus
</figureCaption>
<bodyText confidence="0.999979">
the previous segment influence (i.e., the language
model weight) was not enough to get the appro-
priate label. This was true for all the N-grams
tested. In the case of the unsegmented model, most
of the errors were caused by similar misrecogni-
tions in the second level (which are more frequent
due to the absence of utterance boundaries); how-
ever, deletion and insertion errors were also sig-
nificant. The deletion errors corresponded to ac-
ceptance utterances, which were too short (most
of them were “Yes”). The insertion errors corre-
sponded to “Yes” words that were placed after a
new-consult system utterance, which is the case
of the second turn presented in Figure 4. These
words should not have been labelled as a separate
utterance. In both cases, these errors were very
dependant on the WIP factor, and we had to get
an adequate WIP value which did not increase the
insertions and did not cause too many deletions.
</bodyText>
<sectionHeader confidence="0.997167" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9999935">
In this work, we proposed a method for simultane-
ous segmentation and annotation of dialogue ut-
terances. In contrast to previous models for this
task, our model does not assume manual utterance
segmentation. Instead of treating utterance seg-
mentation as a separate task, the proposed method
selects utterance boundaries to optimize the accu-
racy of the generated labels. We performed ex-
periments to determine the effect of the availabil-
ity of the correct segmentation of dialogue turns
in utterances in the statistical DA labelling frame-
work. Our results reveal that, as shown in previ-
ous work (Warnke et al., 1999), having the correct
segmentation is very important in obtaining accu-
rate results in the labelling task. This conclusion
is supported by the results obtained in very differ-
ent dialogue corpora: different amounts of training
and test data, different natures (general and task-
oriented), different sets of labels, etc.
Future work on this task will be carried out
in several directions. As segmentation appears
to be an important step in these tasks, it would
be interesting to obtain an automatic and accu-
rate segmentation model that can be easily inte-
grated in our statistical model. The application of
our statistical models to other tasks (like VerbMo-
bil (Alexandersson et al., 1998)) would allow us to
confirm our conclusions and compare results with
other works.
The error analysis we performed shows the need
for incorporating new and more reliable informa-
tion resources to the presented model. Therefore,
the use of alternative models in both corpora, such
as the N-gram-based model presented in (Webb et
al., 2005) or an evolution of the presented statis-
tical model with other information sources would
be useful. The combination of these two models
might be a good way to improve results.
Finally, it must be pointed out that the main task
of the dialogue models is to allow the most correct
reaction of a dialogue system given the user in-
put. Therefore, the correct evaluation technique
must be based on the system behaviour as well
as on the accurate assignation of DA to the user
input. Therefore, future evaluation results should
take this fact into account.
</bodyText>
<sectionHeader confidence="0.99649" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999937666666667">
The authors wish to thank Nick Webb, Mark Hep-
ple and Yorick Wilks for their comments and
suggestions and for providing the preprocessed
SwitchBoard corpus. We also want to thank the
anonymous reviewers for their criticism and sug-
gestions.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992833428571428">
N. Alc´acer, J. M. Benedi, F. Blat, R. Granell, C. D.
Martinez, and F. Torres. 2005. Acquisition and
labelling of a spontaneous speech dialogue corpus.
In Proceedings of SPECOM, pages 583–586, Patras,
Greece.
Jan Alexandersson, Bianka Buschbeck-Wolf, Tsu-
tomu Fujinami, Michael Kipp, Stephan Koch, Elis-
</reference>
<page confidence="0.99555">
569
</page>
<table confidence="0.827432">
Utterance 1st level 2nd level
Yes, times Acceptance Dep Hour,Fare
and fares Question Dep Hour,Fare
Yes, I want Acceptance Dep Hour,Fare
times and fares of trains that arrive before seven. Question Dep Hour,Fare
On thursday in the afternoon Answer Time
</table>
<figureCaption confidence="0.969227">
Figure 4: An example of errors produced by the model in the Dihana corpus
</figureCaption>
<reference confidence="0.999898022727273">
abeth Maier, Norbert Reithinger, Birte Schmitz,
and Melanie Siegel. 1998. Dialogue acts in
VERBMOBIL-2 (second edition). Technical Report
226, DFKI GmbH, Saarbr¨ucken, Germany, July.
J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dia-
log act segmentation and classification in multiparty
meetings. In Proceedings of the International Con-
ference of Acoustics, Speech, and Signal Process-
ings, volume 1, pages 1061–1064, Philadelphia.
H. Aust, M. Oerder, F. Seide, and V. Steinbiss. 1995.
The philips automatic train timetable information
system. Speech Communication, 17:249–263.
J. M. Benedi, A. Varona, and E. Lleida. 2004. Dihana:
Dialogue system for information access using spon-
taneous speech in several environments tic2002-
04103-c03. In Reports for Jornadas de Seguimiento
- Programa Nacional de Tecnologias Inform´aticas,
M´alaga, Spain.
Mark G. Core and James F. Allen. 1997. Coding di-
alogs with the damsl annotation scheme. In Work-
ing Notes of AAAI Fall Symposium on Communica-
tive Action in Humans and Machines, Boston, MA,
November.
Layla Dybkjaer and Niels Ole Bernsen. 2000. The
mate workbench.
N. Fraser, 1997. Assessment of interactive systems,
pages 564–614. Mouton de Gruyter.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. In Proc. ICASSP-92, pages 517–
520.
A. Gorin, G. Riccardi, and J. Wright. 1997. How may
i help you? Speech Communication, 23:113–127.
Hilda Hardy, Kirk Baker, Laurence Devillers, Lori
Lamel, Sophie Rosset, Tomek Strzalkowski, Cris-
tian Ursu, and Nick Webb. 2002. Multi-layer di-
alogue annotation for automated multilingual cus-
tomer service. In Proceedings of the ISLE Workshop
on Dialogue Tagging for Multi-Modal Human Com-
puter Interaction, Edinburgh, Scotland, December.
Hilda Hardy, Tomek Strzalkowski, and Min Wu. 2003.
Dialogue management for an automated multilin-
gual call center. In Proceedings of HLT-NAACL
2003 Workshop: Research Directions in Dialogue
Processing, pages 10–12, Edmonton, Canada, June.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board swbd-damsl shallow- discourse-function an-
notation coders manual - draft 13. Technical Report
97-01, University of Colorado Institute of Cognitive
Science.
J. Van Kuppevelt and R. W. Smith. 2003. Current
and New Directions in Discourse and Dialogue, vol-
ume 22 of Text, Speech and Language Technology.
Springer.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, and M. Meteer. 2000. Dialogue act modelling
for automatic tagging and recognition of conversa-
tional speech. Computational Linguistics, 26(3):1–
34.
Marilyn A. Walker, Diane Litman J., Candace A.
Kamm, and Alicia Abella. 1997. PARADISE: A
framework for evaluating spoken dialogue agents.
In Philip R. Cohen and Wolfgang Wahlster, edi-
tors, Proceedings of the Thirty-Fifth Annual Meet-
ing of the Association for Computational Linguis-
tics and Eighth Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 271–280, Somerset, New Jersey. Association
for Computational Linguistics.
V. Warnke, R. Kompe, H. Niemann, and E. N¨oth. 1997.
Integrated Dialog Act Segmentation and Classifica-
tion using Prosodic Features and Language Models.
In Proc. European Conf. on Speech Communication
and Technology, volume 1, pages 207–210, Rhodes.
V. Warnke, S. Harbeck, E. N¨oth, H. Niemann, and
M. Levit. 1999. Discriminative Estimation of Inter-
polation Parameters for Language Model Classifiers.
In Proceedings of the IEEE Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
525–528, Phoenix, AZ, March.
N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue
act classification using intra-utterance features. In
Proceedings of the AAAI Workshop on Spoken Lan-
guage Understanding, Pittsburgh.
S. Young. 2000. Probabilistic methods in spoken di-
alogue systems. Philosophical Trans Royal Society
(Series A), 358(1769):1389–1402.
</reference>
<page confidence="0.996951">
570
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369030">
<title confidence="0.982424">Segmented and unsegmented dialogue-act annotation with statistical</title>
<author confidence="0.452194">D Martinez Hinarejos</author>
<author confidence="0.452194">Ram´on Granell</author>
<author confidence="0.452194">Jos´e Miguel de_Sistemas Inform´aticos y Computaci´on</author>
<affiliation confidence="0.981842">Universidad Polit´ecnica de Valencia</affiliation>
<address confidence="0.993368">Camino de Vera, s/n, 46022, Valencia</address>
<abstract confidence="0.991467545454546">Dialogue systems are one of the most challenging applications of Natural Language Processing. In recent years, some statistical dialogue models have been proposed to cope with the dialogue problem. The evaluation of these models is usually performed by using them as annotation models. Many of the works on annotation use information such as the complete sequence of dialogue turns or the correct segmentation of the dialogue. This information is not usually available for dialogue systems. In this work, we propose a statistical model that uses only the information that is usually available and performs the segmentation and annotation at the same time. The results of this model reveal the great influence that the availability of a correct segmentation has in obtaining an accurate annotation of the dialogues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Alc´acer</author>
<author>J M Benedi</author>
<author>F Blat</author>
<author>R Granell</author>
<author>C D Martinez</author>
<author>F Torres</author>
</authors>
<title>Acquisition and labelling of a spontaneous speech dialogue corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of SPECOM,</booktitle>
<pages>583--586</pages>
<location>Patras, Greece.</location>
<marker>Alc´acer, Benedi, Blat, Granell, Martinez, Torres, 2005</marker>
<rawString>N. Alc´acer, J. M. Benedi, F. Blat, R. Granell, C. D. Martinez, and F. Torres. 2005. Acquisition and labelling of a spontaneous speech dialogue corpus. In Proceedings of SPECOM, pages 583–586, Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Alexandersson</author>
<author>Bianka Buschbeck-Wolf</author>
<author>Tsutomu Fujinami</author>
<author>Michael Kipp</author>
<author>Stephan Koch</author>
<author>Elisabeth Maier</author>
<author>Norbert Reithinger</author>
<author>Birte Schmitz</author>
<author>Melanie Siegel</author>
</authors>
<date>1998</date>
<tech>Technical Report 226,</tech>
<institution>DFKI GmbH,</institution>
<location>Saarbr¨ucken, Germany,</location>
<note>Dialogue acts in VERBMOBIL-2 (second edition).</note>
<contexts>
<context position="3996" citStr="Alexandersson et al., 1998" startWordPosition="633" endWordPosition="636">evel is based on Dialogue Acts (DA). A DA is a label that defines the function of the annotated utterance with respect to the dialogue process. In other words, every turn in the dialogue 563 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 563–570, Sydney, July 2006. c�2006 Association for Computational Linguistics is supposed to be composed of one or more utterances. In this context, from the dialogue management viewpoint an utterance is a relevant subsequence . Several DA annotation schemes have been proposed in recent years (DAMSL (Core and Allen, 1997), VerbMobil (Alexandersson et al., 1998), Dihana (Alc´acer et al., 2005)). In all these studies, it is necessary to annotate a large amount of dialogues to estimate the parameters of the statistical models. Manual annotation is the usual solution, although is very timeconsuming and there is a tendency for error (the annotation instructions are not usually easy to interpret and apply, and human annotators can commit errors) (Jurafsky et al., 1997). Therefore, the possibility of applying statistical models to the annotation problem is really interesting. Moreover, it gives the possibility of evaluating the statistical models. The eval</context>
<context position="25933" citStr="Alexandersson et al., 1998" startWordPosition="4298" endWordPosition="4301">taining accurate results in the labelling task. This conclusion is supported by the results obtained in very different dialogue corpora: different amounts of training and test data, different natures (general and taskoriented), different sets of labels, etc. Future work on this task will be carried out in several directions. As segmentation appears to be an important step in these tasks, it would be interesting to obtain an automatic and accurate segmentation model that can be easily integrated in our statistical model. The application of our statistical models to other tasks (like VerbMobil (Alexandersson et al., 1998)) would allow us to confirm our conclusions and compare results with other works. The error analysis we performed shows the need for incorporating new and more reliable information resources to the presented model. Therefore, the use of alternative models in both corpora, such as the N-gram-based model presented in (Webb et al., 2005) or an evolution of the presented statistical model with other information sources would be useful. The combination of these two models might be a good way to improve results. Finally, it must be pointed out that the main task of the dialogue models is to allow th</context>
</contexts>
<marker>Alexandersson, Buschbeck-Wolf, Fujinami, Kipp, Koch, Maier, Reithinger, Schmitz, Siegel, 1998</marker>
<rawString>Jan Alexandersson, Bianka Buschbeck-Wolf, Tsutomu Fujinami, Michael Kipp, Stephan Koch, Elisabeth Maier, Norbert Reithinger, Birte Schmitz, and Melanie Siegel. 1998. Dialogue acts in VERBMOBIL-2 (second edition). Technical Report 226, DFKI GmbH, Saarbr¨ucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>Y Liu</author>
<author>E Shriberg</author>
</authors>
<title>Automatic dialog act segmentation and classification in multiparty meetings.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference of Acoustics, Speech, and Signal Processings,</booktitle>
<volume>1</volume>
<pages>1061--1064</pages>
<location>Philadelphia.</location>
<contexts>
<context position="5695" citStr="Ang et al., 2005" startWordPosition="913" endWordPosition="916"> recognition of the labels implies the correct recognition of the dialogue situation; consequently this information can help the system to react appropriately. Many recent works have attempted this approach (Stolcke et al., 2000; Webb et al., 2005). However, many of these works are based on the hypothesis of the availability of the segmentation into utterances of the turns of the dialogue. This is an important drawback in order to evaluate these models as strategy models, where segmentation is usually not available. Other works rely on a decoupled scheme of segmentation and DA classification (Ang et al., 2005). In this paper, we present a new statistical model that computes the segmentation and the annotation of the turns at the same time, using a statistical framework that is simpler than the models that have been proposed to solve both problems at the same time (Warnke et al., 1997). The results demonstrate that segmentation accuracy is really important in obtaining an accurate annotation of the dialogue, and consequently in obtaining quality strategy models. Therefore, more accurate segmentation models are needed to perform this process efficiently. This paper is organised as follows: Section 2,</context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dialog act segmentation and classification in multiparty meetings. In Proceedings of the International Conference of Acoustics, Speech, and Signal Processings, volume 1, pages 1061–1064, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Aust</author>
<author>M Oerder</author>
<author>F Seide</author>
<author>V Steinbiss</author>
</authors>
<title>The philips automatic train timetable information system.</title>
<date>1995</date>
<journal>Speech Communication,</journal>
<pages>17--249</pages>
<contexts>
<context position="1533" citStr="Aust et al., 1995" startWordPosition="234" endWordPosition="237">le and performs the segmentation and annotation at the same time. The results of this model reveal the great influence that the availability of a correct segmentation has in obtaining an accurate annotation of the dialogues. 1 Introduction In the Natural Language Processing (NLP) field, one of the most challenging applications is dialogue systems (Kuppevelt and Smith, 2003). A dialogue system is usually defined as a computer system that can interact with a human being through dialogue in order to complete a specific task (e.g., ticket reservation, timetable consultation, bank operations,...) (Aust et al., 1995; Hardy et al., 2002). Most dialogue system have a characteristic behaviour with respect to dialogue *Work partially supported by the Spanish project TIC2003-08681-C02-02 and by Spanish Ministry of Culture under FPI grants. management, which is known as dialogue strategy. It defines what the dialogue system must do at each point of the dialogue. Most of these strategies are rule-based, i.e., the dialogue strategy is defined by rules that are usually defined by a human expert (Gorin et al., 1997; Hardy et al., 2003). This approach is usually difficult to adapt or extend to new domains where the</context>
</contexts>
<marker>Aust, Oerder, Seide, Steinbiss, 1995</marker>
<rawString>H. Aust, M. Oerder, F. Seide, and V. Steinbiss. 1995. The philips automatic train timetable information system. Speech Communication, 17:249–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Benedi</author>
<author>A Varona</author>
<author>E Lleida</author>
</authors>
<title>Dihana: Dialogue system for information access using spontaneous speech in several environments tic2002-04103-c03.</title>
<date>2004</date>
<booktitle>In Reports for Jornadas de Seguimiento - Programa Nacional de Tecnologias Inform´aticas,</booktitle>
<location>M´alaga,</location>
<marker>Benedi, Varona, Lleida, 2004</marker>
<rawString>J. M. Benedi, A. Varona, and E. Lleida. 2004. Dihana: Dialogue system for information access using spontaneous speech in several environments tic2002-04103-c03. In Reports for Jornadas de Seguimiento - Programa Nacional de Tecnologias Inform´aticas, M´alaga, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogs with the damsl annotation scheme.</title>
<date>1997</date>
<booktitle>In Working Notes of AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="3278" citStr="Core and Allen, 1997" startWordPosition="515" endWordPosition="518">the models from the annotated dialogues. As a simplification, it could be considered that each label is associated to a situation in the dialogue, and the models learn how to identify and react to the different situations by estimating the associations between the labels and the dialogue events (words, the speaker, previous turns, etc.). An appropriate annotation scheme should be defined to capture the elements that are really important for the dialogue, eliminating the information that is irrelevant to the dialogue process. Several annotation schemes have been proposed in the last few years (Core and Allen, 1997; Dybkjaer and Bernsen, 2000). One of the most popular annotation schemes at the dialogue level is based on Dialogue Acts (DA). A DA is a label that defines the function of the annotated utterance with respect to the dialogue process. In other words, every turn in the dialogue 563 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 563–570, Sydney, July 2006. c�2006 Association for Computational Linguistics is supposed to be composed of one or more utterances. In this context, from the dialogue management viewpoint an utterance is a relevant subsequence . Several DA annot</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G. Core and James F. Allen. 1997. Coding dialogs with the damsl annotation scheme. In Working Notes of AAAI Fall Symposium on Communicative Action in Humans and Machines, Boston, MA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Layla Dybkjaer</author>
<author>Niels Ole Bernsen</author>
</authors>
<date>2000</date>
<note>The mate workbench.</note>
<contexts>
<context position="3307" citStr="Dybkjaer and Bernsen, 2000" startWordPosition="519" endWordPosition="522">notated dialogues. As a simplification, it could be considered that each label is associated to a situation in the dialogue, and the models learn how to identify and react to the different situations by estimating the associations between the labels and the dialogue events (words, the speaker, previous turns, etc.). An appropriate annotation scheme should be defined to capture the elements that are really important for the dialogue, eliminating the information that is irrelevant to the dialogue process. Several annotation schemes have been proposed in the last few years (Core and Allen, 1997; Dybkjaer and Bernsen, 2000). One of the most popular annotation schemes at the dialogue level is based on Dialogue Acts (DA). A DA is a label that defines the function of the annotated utterance with respect to the dialogue process. In other words, every turn in the dialogue 563 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 563–570, Sydney, July 2006. c�2006 Association for Computational Linguistics is supposed to be composed of one or more utterances. In this context, from the dialogue management viewpoint an utterance is a relevant subsequence . Several DA annotation schemes have been propo</context>
</contexts>
<marker>Dybkjaer, Bernsen, 2000</marker>
<rawString>Layla Dybkjaer and Niels Ole Bernsen. 2000. The mate workbench.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fraser</author>
</authors>
<title>Assessment of interactive systems,</title>
<date>1997</date>
<pages>564--614</pages>
<note>Mouton de Gruyter.</note>
<contexts>
<context position="4746" citStr="Fraser, 1997" startWordPosition="757" endWordPosition="758">s of the statistical models. Manual annotation is the usual solution, although is very timeconsuming and there is a tendency for error (the annotation instructions are not usually easy to interpret and apply, and human annotators can commit errors) (Jurafsky et al., 1997). Therefore, the possibility of applying statistical models to the annotation problem is really interesting. Moreover, it gives the possibility of evaluating the statistical models. The evaluation of the performance of dialogue strategies models is a difficult task. Although many proposals have been made (Walker et al., 1997; Fraser, 1997; Stolcke et al., 2000), there is no real agreement in the NLP community about the evaluation technique to apply. Our main aim is the evaluation of strategy models, which provide the reaction of the system given a user input and a dialogue history. Using these models as annotation models gives us a possible evaluation: the correct recognition of the labels implies the correct recognition of the dialogue situation; consequently this information can help the system to react appropriately. Many recent works have attempted this approach (Stolcke et al., 2000; Webb et al., 2005). However, many of t</context>
</contexts>
<marker>Fraser, 1997</marker>
<rawString>N. Fraser, 1997. Assessment of interactive systems, pages 564–614. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP-92,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="10464" citStr="Godfrey et al., 1992" startWordPosition="1699" endWordPosition="1702">itchBoard corpus is composed of human-human, non task-oriented dialogues with a large vocabulary. The Dihana corpus is composed of human-computer, task-oriented dialogues with a small vocabulary. Although two corpora are not enough to let us draw general conclusions, they give us more reliable results than using only one corpus. Moreover, the very different nature of both corpora makes our conclusions more independent from the corpus type, the annotation scheme, the vocabulary size, etc. 3.1 The SwitchBoard corpus The first corpus used in the experiments was the well-known SwitchBoard corpus (Godfrey et al., 1992). The SwitchBoard database consists of human-human conversations by telephone with no directed tasks. Both speakers discuss about general interest topics, but without a clear task to accomplish. The corpus is formed by 1,155 conversations, which comprise 126,754 different turns of spontaneous and sometimes overlapped speech, using a vocabulary of 21,797 different words. The corpus was segmented into utterances, each of which was annotated with a DA following the simplified DAMSL annotation scheme (Jurafsky et al., 1997). The set of labels of the simplified DAMSL scheme is composed of 42 differ</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey, E. Holliman, and J. McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proc. ICASSP-92, pages 517– 520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gorin</author>
<author>G Riccardi</author>
<author>J Wright</author>
</authors>
<title>How may i help you? Speech Communication,</title>
<date>1997</date>
<pages>23--113</pages>
<contexts>
<context position="2032" citStr="Gorin et al., 1997" startWordPosition="315" endWordPosition="318"> to complete a specific task (e.g., ticket reservation, timetable consultation, bank operations,...) (Aust et al., 1995; Hardy et al., 2002). Most dialogue system have a characteristic behaviour with respect to dialogue *Work partially supported by the Spanish project TIC2003-08681-C02-02 and by Spanish Ministry of Culture under FPI grants. management, which is known as dialogue strategy. It defines what the dialogue system must do at each point of the dialogue. Most of these strategies are rule-based, i.e., the dialogue strategy is defined by rules that are usually defined by a human expert (Gorin et al., 1997; Hardy et al., 2003). This approach is usually difficult to adapt or extend to new domains where the dialogue structure could be completely different, and it requires the definition of new rules. Similar to other NLP problems (like speech recognition and understanding, or statistical machine translation), an alternative data-based approach has been developed in the last decade (Stolcke et al., 2000; Young, 2000). This approach relies on statistical models that can be automatically estimated from annotated data, which in this case, are dialogues from the task. Statistical modelling learns the </context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. Gorin, G. Riccardi, and J. Wright. 1997. How may i help you? Speech Communication, 23:113–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilda Hardy</author>
<author>Kirk Baker</author>
<author>Laurence Devillers</author>
<author>Lori Lamel</author>
<author>Sophie Rosset</author>
<author>Tomek Strzalkowski</author>
<author>Cristian Ursu</author>
<author>Nick Webb</author>
</authors>
<title>Multi-layer dialogue annotation for automated multilingual customer service.</title>
<date>2002</date>
<booktitle>In Proceedings of the ISLE Workshop on Dialogue Tagging for Multi-Modal Human Computer Interaction,</booktitle>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1554" citStr="Hardy et al., 2002" startWordPosition="238" endWordPosition="241"> segmentation and annotation at the same time. The results of this model reveal the great influence that the availability of a correct segmentation has in obtaining an accurate annotation of the dialogues. 1 Introduction In the Natural Language Processing (NLP) field, one of the most challenging applications is dialogue systems (Kuppevelt and Smith, 2003). A dialogue system is usually defined as a computer system that can interact with a human being through dialogue in order to complete a specific task (e.g., ticket reservation, timetable consultation, bank operations,...) (Aust et al., 1995; Hardy et al., 2002). Most dialogue system have a characteristic behaviour with respect to dialogue *Work partially supported by the Spanish project TIC2003-08681-C02-02 and by Spanish Ministry of Culture under FPI grants. management, which is known as dialogue strategy. It defines what the dialogue system must do at each point of the dialogue. Most of these strategies are rule-based, i.e., the dialogue strategy is defined by rules that are usually defined by a human expert (Gorin et al., 1997; Hardy et al., 2003). This approach is usually difficult to adapt or extend to new domains where the dialogue structure c</context>
</contexts>
<marker>Hardy, Baker, Devillers, Lamel, Rosset, Strzalkowski, Ursu, Webb, 2002</marker>
<rawString>Hilda Hardy, Kirk Baker, Laurence Devillers, Lori Lamel, Sophie Rosset, Tomek Strzalkowski, Cristian Ursu, and Nick Webb. 2002. Multi-layer dialogue annotation for automated multilingual customer service. In Proceedings of the ISLE Workshop on Dialogue Tagging for Multi-Modal Human Computer Interaction, Edinburgh, Scotland, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilda Hardy</author>
<author>Tomek Strzalkowski</author>
<author>Min Wu</author>
</authors>
<title>Dialogue management for an automated multilingual call center.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003 Workshop: Research Directions in Dialogue Processing,</booktitle>
<pages>10--12</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="2053" citStr="Hardy et al., 2003" startWordPosition="319" endWordPosition="322">fic task (e.g., ticket reservation, timetable consultation, bank operations,...) (Aust et al., 1995; Hardy et al., 2002). Most dialogue system have a characteristic behaviour with respect to dialogue *Work partially supported by the Spanish project TIC2003-08681-C02-02 and by Spanish Ministry of Culture under FPI grants. management, which is known as dialogue strategy. It defines what the dialogue system must do at each point of the dialogue. Most of these strategies are rule-based, i.e., the dialogue strategy is defined by rules that are usually defined by a human expert (Gorin et al., 1997; Hardy et al., 2003). This approach is usually difficult to adapt or extend to new domains where the dialogue structure could be completely different, and it requires the definition of new rules. Similar to other NLP problems (like speech recognition and understanding, or statistical machine translation), an alternative data-based approach has been developed in the last decade (Stolcke et al., 2000; Young, 2000). This approach relies on statistical models that can be automatically estimated from annotated data, which in this case, are dialogues from the task. Statistical modelling learns the appropriate parameter</context>
</contexts>
<marker>Hardy, Strzalkowski, Wu, 2003</marker>
<rawString>Hilda Hardy, Tomek Strzalkowski, and Min Wu. 2003. Dialogue management for an automated multilingual call center. In Proceedings of HLT-NAACL 2003 Workshop: Research Directions in Dialogue Processing, pages 10–12, Edmonton, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>E Shriberg</author>
<author>D Biasca</author>
</authors>
<title>Switchboard swbd-damsl shallow- discourse-function annotation coders manual - draft 13.</title>
<date>1997</date>
<tech>Technical Report 97-01,</tech>
<institution>University of Colorado Institute of Cognitive Science.</institution>
<contexts>
<context position="4406" citStr="Jurafsky et al., 1997" startWordPosition="703" endWordPosition="706">om the dialogue management viewpoint an utterance is a relevant subsequence . Several DA annotation schemes have been proposed in recent years (DAMSL (Core and Allen, 1997), VerbMobil (Alexandersson et al., 1998), Dihana (Alc´acer et al., 2005)). In all these studies, it is necessary to annotate a large amount of dialogues to estimate the parameters of the statistical models. Manual annotation is the usual solution, although is very timeconsuming and there is a tendency for error (the annotation instructions are not usually easy to interpret and apply, and human annotators can commit errors) (Jurafsky et al., 1997). Therefore, the possibility of applying statistical models to the annotation problem is really interesting. Moreover, it gives the possibility of evaluating the statistical models. The evaluation of the performance of dialogue strategies models is a difficult task. Although many proposals have been made (Walker et al., 1997; Fraser, 1997; Stolcke et al., 2000), there is no real agreement in the NLP community about the evaluation technique to apply. Our main aim is the evaluation of strategy models, which provide the reaction of the system given a user input and a dialogue history. Using these</context>
<context position="10989" citStr="Jurafsky et al., 1997" startWordPosition="1780" endWordPosition="1783">first corpus used in the experiments was the well-known SwitchBoard corpus (Godfrey et al., 1992). The SwitchBoard database consists of human-human conversations by telephone with no directed tasks. Both speakers discuss about general interest topics, but without a clear task to accomplish. The corpus is formed by 1,155 conversations, which comprise 126,754 different turns of spontaneous and sometimes overlapped speech, using a vocabulary of 21,797 different words. The corpus was segmented into utterances, each of which was annotated with a DA following the simplified DAMSL annotation scheme (Jurafsky et al., 1997). The set of labels of the simplified DAMSL scheme is composed of 42 different labels, which define categories such as statement, backchannel, opinion, etc. An example of annotation is presented in Figure 1. 3.2 The Dihana corpus The second corpus used was a task-oriented corpus called Dihana (Benediet al., 2004). It is composed of computer-to-human dialogues, and the main aim of the task is to answer telephone queries about train timetables, fares, and services for longdistance trains in Spanish. A total of 900 dialogues were acquired by using the Wizard of Oz technique and semicontrolled sce</context>
</contexts>
<marker>Jurafsky, Shriberg, Biasca, 1997</marker>
<rawString>D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switchboard swbd-damsl shallow- discourse-function annotation coders manual - draft 13. Technical Report 97-01, University of Colorado Institute of Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Van Kuppevelt</author>
<author>R W Smith</author>
</authors>
<title>Text, Speech and Language Technology.</title>
<date>2003</date>
<booktitle>Current and New Directions in Discourse and Dialogue,</booktitle>
<volume>22</volume>
<publisher>Springer.</publisher>
<marker>Van Kuppevelt, Smith, 2003</marker>
<rawString>J. Van Kuppevelt and R. W. Smith. 2003. Current and New Directions in Discourse and Dialogue, volume 22 of Text, Speech and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>N Coccaro</author>
<author>R Bates</author>
<author>P Taylor</author>
<author>C van EssDykema</author>
<author>K Ries</author>
<author>E Shriberg</author>
<author>D Jurafsky</author>
<author>R Martin</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modelling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>34</pages>
<marker>Stolcke, Coccaro, Bates, Taylor, van EssDykema, Ries, Shriberg, Jurafsky, Martin, Meteer, 2000</marker>
<rawString>A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. van EssDykema, K. Ries, E. Shriberg, D. Jurafsky, R. Martin, and M. Meteer. 2000. Dialogue act modelling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):1– 34.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marilyn A Walker</author>
<author>Diane Litman J</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>271--280</pages>
<editor>In Philip R. Cohen and Wolfgang Wahlster, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="4732" citStr="Walker et al., 1997" startWordPosition="753" endWordPosition="756">stimate the parameters of the statistical models. Manual annotation is the usual solution, although is very timeconsuming and there is a tendency for error (the annotation instructions are not usually easy to interpret and apply, and human annotators can commit errors) (Jurafsky et al., 1997). Therefore, the possibility of applying statistical models to the annotation problem is really interesting. Moreover, it gives the possibility of evaluating the statistical models. The evaluation of the performance of dialogue strategies models is a difficult task. Although many proposals have been made (Walker et al., 1997; Fraser, 1997; Stolcke et al., 2000), there is no real agreement in the NLP community about the evaluation technique to apply. Our main aim is the evaluation of strategy models, which provide the reaction of the system given a user input and a dialogue history. Using these models as annotation models gives us a possible evaluation: the correct recognition of the labels implies the correct recognition of the dialogue situation; consequently this information can help the system to react appropriately. Many recent works have attempted this approach (Stolcke et al., 2000; Webb et al., 2005). Howe</context>
</contexts>
<marker>Walker, J, Kamm, Abella, 1997</marker>
<rawString>Marilyn A. Walker, Diane Litman J., Candace A. Kamm, and Alicia Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. In Philip R. Cohen and Wolfgang Wahlster, editors, Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 271–280, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Warnke</author>
<author>R Kompe</author>
<author>H Niemann</author>
<author>E N¨oth</author>
</authors>
<title>Integrated Dialog Act Segmentation and Classification using Prosodic Features and Language Models. In</title>
<date>1997</date>
<booktitle>Proc. European Conf. on Speech Communication and Technology,</booktitle>
<volume>1</volume>
<pages>207--210</pages>
<location>Rhodes.</location>
<marker>Warnke, Kompe, Niemann, N¨oth, 1997</marker>
<rawString>V. Warnke, R. Kompe, H. Niemann, and E. N¨oth. 1997. Integrated Dialog Act Segmentation and Classification using Prosodic Features and Language Models. In Proc. European Conf. on Speech Communication and Technology, volume 1, pages 207–210, Rhodes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Warnke</author>
<author>S Harbeck</author>
<author>E N¨oth</author>
<author>H Niemann</author>
<author>M Levit</author>
</authors>
<title>Discriminative Estimation of Interpolation Parameters for Language Model Classifiers.</title>
<date>1999</date>
<booktitle>In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>525--528</pages>
<location>Phoenix, AZ,</location>
<marker>Warnke, Harbeck, N¨oth, Niemann, Levit, 1999</marker>
<rawString>V. Warnke, S. Harbeck, E. N¨oth, H. Niemann, and M. Levit. 1999. Discriminative Estimation of Interpolation Parameters for Language Model Classifiers. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 525–528, Phoenix, AZ, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Webb</author>
<author>M Hepple</author>
<author>Y Wilks</author>
</authors>
<title>Dialogue act classification using intra-utterance features.</title>
<date>2005</date>
<booktitle>In Proceedings of the AAAI Workshop on Spoken Language Understanding,</booktitle>
<location>Pittsburgh.</location>
<contexts>
<context position="5326" citStr="Webb et al., 2005" startWordPosition="851" endWordPosition="854">ade (Walker et al., 1997; Fraser, 1997; Stolcke et al., 2000), there is no real agreement in the NLP community about the evaluation technique to apply. Our main aim is the evaluation of strategy models, which provide the reaction of the system given a user input and a dialogue history. Using these models as annotation models gives us a possible evaluation: the correct recognition of the labels implies the correct recognition of the dialogue situation; consequently this information can help the system to react appropriately. Many recent works have attempted this approach (Stolcke et al., 2000; Webb et al., 2005). However, many of these works are based on the hypothesis of the availability of the segmentation into utterances of the turns of the dialogue. This is an important drawback in order to evaluate these models as strategy models, where segmentation is usually not available. Other works rely on a decoupled scheme of segmentation and DA classification (Ang et al., 2005). In this paper, we present a new statistical model that computes the segmentation and the annotation of the turns at the same time, using a statistical framework that is simpler than the models that have been proposed to solve bot</context>
<context position="15077" citStr="Webb et al., 2005" startWordPosition="2454" endWordPosition="2457">ed with the same DA). Table 1: SwitchBoard database statistics (mean for the ten cross-validation partitions) Training Test Dialogues 1,136 19 Turns 113,370 1,885 Utterances 201,474 3,718 Running words 1,837,222 33,162 Vocabulary 21,248 2,579 • All the words were transcribed in lowercase. • Puntuaction marks were separated from words. The experiments were performed using a crossvalidation approach to avoid the statistical bias that can be introduced by the election of fixed training and test partitions. This cross-validation approach has also been adopted in other recent works on this corpus (Webb et al., 2005). In our case, we performed 10 different experiments. In each experiment, the training partition was composed of 1,136 dialogues, and the test partition was composed of 19 dialogues. This proportion was adopted so that our results could be compared with the results in (Stolcke et al., 2000), where similar training and test sizes were used. The mean figures for the training and test partitions are shown in Table 1. With respect to the Dihana database, the preprocessing included the following points: • A categorisation process was performed for categories such as town names, the time, dates, tra</context>
<context position="26269" citStr="Webb et al., 2005" startWordPosition="4352" endWordPosition="4355">pears to be an important step in these tasks, it would be interesting to obtain an automatic and accurate segmentation model that can be easily integrated in our statistical model. The application of our statistical models to other tasks (like VerbMobil (Alexandersson et al., 1998)) would allow us to confirm our conclusions and compare results with other works. The error analysis we performed shows the need for incorporating new and more reliable information resources to the presented model. Therefore, the use of alternative models in both corpora, such as the N-gram-based model presented in (Webb et al., 2005) or an evolution of the presented statistical model with other information sources would be useful. The combination of these two models might be a good way to improve results. Finally, it must be pointed out that the main task of the dialogue models is to allow the most correct reaction of a dialogue system given the user input. Therefore, the correct evaluation technique must be based on the system behaviour as well as on the accurate assignation of DA to the user input. Therefore, future evaluation results should take this fact into account. Acknowledgements The authors wish to thank Nick We</context>
</contexts>
<marker>Webb, Hepple, Wilks, 2005</marker>
<rawString>N. Webb, M. Hepple, and Y. Wilks. 2005. Dialogue act classification using intra-utterance features. In Proceedings of the AAAI Workshop on Spoken Language Understanding, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
</authors>
<title>Probabilistic methods in spoken dialogue systems.</title>
<date>2000</date>
<journal>Philosophical Trans Royal Society (Series A),</journal>
<volume>358</volume>
<issue>1769</issue>
<contexts>
<context position="2448" citStr="Young, 2000" startWordPosition="384" endWordPosition="385">ystem must do at each point of the dialogue. Most of these strategies are rule-based, i.e., the dialogue strategy is defined by rules that are usually defined by a human expert (Gorin et al., 1997; Hardy et al., 2003). This approach is usually difficult to adapt or extend to new domains where the dialogue structure could be completely different, and it requires the definition of new rules. Similar to other NLP problems (like speech recognition and understanding, or statistical machine translation), an alternative data-based approach has been developed in the last decade (Stolcke et al., 2000; Young, 2000). This approach relies on statistical models that can be automatically estimated from annotated data, which in this case, are dialogues from the task. Statistical modelling learns the appropriate parameters of the models from the annotated dialogues. As a simplification, it could be considered that each label is associated to a situation in the dialogue, and the models learn how to identify and react to the different situations by estimating the associations between the labels and the dialogue events (words, the speaker, previous turns, etc.). An appropriate annotation scheme should be defined</context>
</contexts>
<marker>Young, 2000</marker>
<rawString>S. Young. 2000. Probabilistic methods in spoken dialogue systems. Philosophical Trans Royal Society (Series A), 358(1769):1389–1402.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>