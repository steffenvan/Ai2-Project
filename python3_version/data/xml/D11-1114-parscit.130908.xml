<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9808795">
Exact Inference for Generative Probabilistic
Non-Projective Dependency Parsing
</title>
<author confidence="0.996739">
Shay B. Cohen Carlos G´omez-Rodriguez
</author>
<affiliation confidence="0.998323">
School of Computer Science Departamento de Computaci´on
Carnegie Mellon University, USA Universidade da Coru˜na, Spain
</affiliation>
<email confidence="0.985833">
scohen@cs.cmu.edu cgomezr@udc.es
</email>
<author confidence="0.997518">
Giorgio Satta
</author>
<affiliation confidence="0.999835">
Dept. of Information Engineering
University of Padua, Italy
</affiliation>
<email confidence="0.984879">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.994568" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998665">
We describe a generative model for non-
projective dependency parsing based on a sim-
plified version of a transition system that has
recently appeared in the literature. We then
develop a dynamic programming parsing al-
gorithm for our model, and derive an inside-
outside algorithm that can be used for unsu-
pervised learning of non-projective depend-
ency trees.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962389830509">
Dependency grammars have received considerable
attention in the statistical parsing community in
recent years. These grammatical formalisms of-
fer a good balance between structural expressiv-
ity and processing efficiency. Most notably, when
non-projectivity is supported, these formalisms can
model crossing syntactic relations that are typical in
languages with relatively free word order.
Recent work has reduced non-projective parsing
to the identification of a maximum spanning tree in a
graph (McDonald et al., 2005; Koo et al., 2007; Mc-
Donald and Satta, 2007; Smith and Smith, 2007).
An alternative to this approach is to use transition-
based parsing (Yamada and Matsumoto, 2003; Nivre
and Nilsson, 2005; Attardi, 2006; Nivre, 2009;
G´omez-Rodr´ıguez and Nivre, 2010), where there is
an incremental processing of a string with a model
that scores transitions between parser states, condi-
tioned on the parse history. This paper focuses on
the latter approach.
The above work on transition-based parsing has
focused on greedy algorithms set in a statistical
framework (Nivre, 2008). More recently, dynamic
programming has been successfully used for pro-
jective parsing (Huang and Sagae, 2010; Kuhlmann
et al., 2011). Dynamic programming algorithms for
parsing (also known as chart-based algorithms) al-
low polynomial space representations of all parse
trees for a given input string, even in cases where
the size of this set is exponential in the length of
the string itself. In combination with appropriate
semirings, these packed representations can be ex-
ploited to compute many values of interest for ma-
chine learning, such as best parses and feature ex-
pectations (Goodman, 1999; Li and Eisner, 2009).
In this paper we move one step forward with re-
spect to Huang and Sagae (2010) and Kuhlmann et
al. (2011) and present a polynomial dynamic pro-
gramming algorithm for non-projective transition-
based parsing. Our algorithm is coupled with a
simplified version of the transition system from At-
tardi (2006), which has high coverage for the type
of non-projective structures that appear in various
treebanks. Instead of an additional transition oper-
ation which permits swapping of two elements in
the stack (Titov et al., 2009; Nivre, 2009), Attardi’s
system allows reduction of elements at non-adjacent
positions in the stack. We also present a generat-
ive probabilistic model for transition-based parsing.
The implication for this, for example, is that one can
now approach the problem of unsupervised learning
of non-projective dependency structures within the
transition-based framework.
Dynamic programming algorithms for non-
projective parsing have been proposed by Kahane et
al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhl-
mann and Satta (2009), but they all run in exponen-
tial time in the ‘gap degree’ of the parsed structures.
To the best of our knowledge, this paper is the first to
</bodyText>
<page confidence="0.94508">
1234
</page>
<note confidence="0.9577065">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999847727272727">
introduce a dynamic programming algorithm for in-
ference with non-projective structures of unbounded
gap degree.
The rest of this paper is organized as follows. In
§2 and §3 we outline the transition-based model we
use, together with a probabilistic generative inter-
pretation. In §4 we give the tabular algorithm for
parsing, and in §5 we discuss statistical inference
using expectation maximization. We then discuss
some other aspects of the work in §6 and conclude
in §7.
</bodyText>
<sectionHeader confidence="0.947927" genericHeader="method">
2 Transition-based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999872166666667">
In this section we briefly introduce the basic defini-
tions for transition-based dependency parsing. For a
more detailed presentation of this subject, we refer
the reader to Nivre (2008). We then define a spe-
cific transition-based model for non-projective de-
pendency parsing that we investigate in this paper.
</bodyText>
<subsectionHeader confidence="0.935671">
2.1 General Transition Systems
</subsectionHeader>
<bodyText confidence="0.999470702702703">
Assume an input alphabet Σ with a special symbol
$ ∈ Σ, which we use as the root of our parse struc-
tures. Throughout this paper we denote the input
string as w = a0 ···an_1, n ≥ 1, where a0 = $ and
ai ∈ Σ \ {$} for each i with 1 ≤ i ≤ n − 1.
A dependency tree for w is a directed tree Gw =
(Vw, Aw), where Vw = {0, ... , n − 1} is the set of
nodes, and Aw ⊆ Vw × Vw is the set of arcs. The
root of Gw is the node 0. The intended meaning
is that each node in Vw encodes the position of a
token in w. Furthermore, each arc in Aw encodes a
dependency relation between two tokens. We write
i → j to denote a directed arc (i, j) ∈ Aw, where
node i is the head and node j is the dependent.
A transition system (for dependency parsing) is a
tuple 5 = (C, T, I, Ct), where C is a set of configur-
ations, defined below, T is a finite set of transitions,
which are partial functions t: C — C, I is a total
initialization function mapping each input string to
a unique initial configuration, and Ct ⊆ C is a set of
terminal configurations.
A configuration is defined relative to some input
string w, and is a triple (Q, 0, A), where Q and 0 are
disjoint lists called stack and buffer, respectively,
and A ⊆ Vw × Vw is a set of arcs. Elements of
Q and 0 are nodes from Vw and, in the case of the
stack, a special symbol ¢ that we will use as initial
stack symbol. If t is a transition and c1, c2 are con-
figurations such that t(c1) = c2, we write c1 `t c2,
or simply c1 ` c2 if t is understood from the context.
Given an input string w, a parser based on 5 in-
crementally processes w from left to right, starting
in the initial configuration I(w). At each step, the
parser nondeterministically applies one transition, or
else it stops if it has reached some terminal config-
uration. The dependency graph defined by the arc
set associated with a terminal configuration is then
returned as one possible analysis for w.
Formally, a computation of 5 is a sequence γ =
c0, ... , cm, m ≥ 1, of configurations such that, for
every i with 1 ≤ i ≤ m, ci_1 `ti ci for some ti ∈ T.
In other words, each configuration in a computa-
tion is obtained as the value of the preceding con-
figuration under some transition. A computation is
called complete whenever c0 = I(w) for some in-
put string w, and cm ∈ Ct.
We can view a transition-based dependency
parser as a device mapping strings into graphs (de-
pendency trees). Without any restriction on trans-
ition functions in T, these functions might have an
infinite domain, and could thus encode even non-
recursively enumerable languages. However, in
standard practice for natural language parsing, trans-
itions are always specified by some finite mean. In
particular, the definition of each transition depends
on some finite window at the top of the stack and
some finite window at the beginning of the buffer
in each configuration. In this case, we can view a
transition-based dependency parser as a notational
variant of a push-down transducer (Hopcroft et al.,
2000), whose computations output sequences that
directly encode dependency trees. These transducers
are nondeterministic, meaning that several trans-
itions can be applied to some configurations. The
transition systems we investigate in this paper fol-
low these principles.
We close this subsection with some additional
notation. We denote the stack with its topmost ele-
ment to the right and the buffer with its first ele-
ment to the left. We indicate concatenation in the
stack and buffer by a vertical bar. For example, for
k ∈ Vw, Q|k denotes some stack with topmost ele-
ment k and k|0 denotes some buffer with first ele-
ment k. For 0 ≤ i ≤ n − 1, 0i denotes the buffer
</bodyText>
<page confidence="0.825146">
1235
</page>
<bodyText confidence="0.380313">
[i, i + 1, ... , n − 1]; for i ≥ n, βz denotes [] (the
empty buffer).
</bodyText>
<subsectionHeader confidence="0.997723">
2.2 A Non-projective Transition System
</subsectionHeader>
<bodyText confidence="0.999989380952381">
We now turn to give a description of our trans-
ition system for non-projective parsing. While a
projective dependency tree satisfies the requirement
that, for every arc in the tree, there is a direc-
ted path between its headword and each of the
words between the two endpoints of the arc, a non-
projective dependency tree may violate this condi-
tion. Even though some natural languages exhibit
syntactic phenomena which require non-projective
expressive power, most often such a resource is used
in a limited way.
This idea is demonstrated by Attardi (2006), who
proposes a transition system whose individual trans-
itions can deal with non-projective dependencies
only to a limited extent, depending on the distance
in the stack of the nodes involved in the newly con-
structed dependency. The author defines this dis-
tance as the degree of the transition, with transitions
of degree one being able to handle only projective
dependencies. This formulation permits parsing a
subset of the non-projective trees, where this subset
depends on the degree of the transitions. The repor-
ted coverage in Attardi (2006) is already very high
when the system is restricted to transitions of degree
two or three. For instance, on training data for Czech
containing 28,934 non-projective relations, 27,181
can be handled by degree two transitions, and 1,668
additional dependencies can be handled by degree
three transitions. Table 1 gives additional statistics
for treebanks from the CoNLL-X shared task (Buch-
holz and Marsi, 2006).
We now turn to describe our variant of the trans-
ition system of Attardi (2006), which is equivalent to
the original system restricted to transitions of degree
two. Our results are based on such a restriction. It is
not difficult to extend our algorithms (§4) to higher
degree transitions, but this comes at the expense of
higher complexity. See §6 for more discussion on
this issue.
Let w = a0 · · · an−1 be an input string over Σ
defined as in §2.1, with a0 = $. Our transition sys-
tem for non-projective dependency parsing is
</bodyText>
<equation confidence="0.994685">
S(np) = (C, T(np), I(np), C(np)
t ),
</equation>
<table confidence="0.999070615384615">
Language Deg. 2 Deg. 3 Deg. 4
Arabic 180 21 7
Bulgarian 961 41 10
Czech 27181 1668 85
Danish 876 136 53
Dutch 9072 2119 171
German 15827 2274 466
Japanese 1484 143 9
Portuguese 3104 424 37
Slovene 601 48 13
Spanish 66 7 0
Swedish 1566 226 79
Turkish 579 185 8
</table>
<tableCaption confidence="0.681508">
Table 1: The number of non-projective relations of vari-
ous degrees for several treebanks (training sets), as repor-
ted by the parser of Attardi (2006). Deg. stands for ‘de-
gree.’ The parser did not detect non-projective relations
of degree higher than 4.
</tableCaption>
<bodyText confidence="0.99952">
where C is the same set of configurations defined
in §2.1. The initialization function I(np) maps each
string w to the initial configuration ([¢],β0, ∅). The
set of terminal configurations C(np)
</bodyText>
<equation confidence="0.751119">
t contains all con-
</equation>
<bodyText confidence="0.925830857142857">
figurations of the form ([¢, 0], [], A), for any set of
arcs A.
The set of transition functions is defined as
T(np) = {shb  |b ∈ Σ} ∪ {la1, ra1,la2, ra2},
where each transition is specified below. We let vari-
ables i, j, k, l range over Vw, and variable σ is a list
of stack elements from Vw ∪ {¢}:
</bodyText>
<equation confidence="0.996902">
shb : (σ, k|β, A) ` (σ|k, β, A) if ak = b;
la1 : (σ|i|j,β,A) ` (σ|j,β,A ∪ {j → i});
ra1 : (σ|i|j,β,A) ` (σ|i,β,A ∪ {i → j});
la2 : (σ|i|j|k,β,A) ` (σ|j|k,β,A ∪ {k → i});
ra2 : (σ|i|j|k,β,A) ` (σ|i|j,β,A ∪ {i → k}).
</equation>
<bodyText confidence="0.99997375">
Each of the above transitions is undefined on config-
urations that do not match the forms specified above.
As an example, transition la2 is not defined for a
configuration (σ, β, A) with |σ |≤ 2, and transition
shb is not defined for a configuration (σ, k|β, A)
with b =6 ak, or for a configuration (σ, [], A).
Transition shb removes the first node from the buf-
fer, in case this node represents symbol b ∈ Σ,
</bodyText>
<page confidence="0.948139">
1236
</page>
<bodyText confidence="0.995100152173913">
and pushes it into the stack. These transitions are
called shift transitions. The remaining four trans-
itions are called reduce transitions, i.e., transitions
that consume nodes from the stack. Notice that in
the transition system at hand all the reduce trans-
itions decrease the size of the stack by one ele-
ment. Transition la1 creates a new arc with the top-
most node on the stack as the head and the second-
topmost node as the dependent, and removes the
latter from the stack. Transition ra1 is symmetric
with respect to la1. Transitions la1 and ra1 have
degree one, as already explained. When restricted
to these three transitions, the system is equivalent
to the so-called stack-based arc-standard model of
Nivre (2004). Transition la2 and transition ra2 are
very similar to la1 and ra1, respectively, but with
the difference that they create a new arc between
the topmost node in the stack and a node which is
two positions below the topmost node. Hence, these
transitions have degree two, and are the key com-
ponents in parsing of non-projective dependencies.
We turn next to describe the equivalence between
our system and the system in Attardi (2006). The
transition-based parser presented by Attardi pushes
back into the buffer elements that are in the top pos-
ition of the stack. However, a careful analysis shows
that only the first position in the buffer can be af-
fected by this operation, in the sense that elements
that are pushed back from the stack are never found
in buffer positions other than the first. This means
that we can consider the first element of the buffer
as an additional stack element, always sitting on the
top of the top-most stack symbol.
More formally, we can define a function m, :
C -+ C that maps configurations in the original al-
gorithm to those in our variant as follows:
m,((a, k|β, A)) = (a|k,β, A)
By applying this mapping to the source and target
configuration of each transition in the original sys-
tem, it is easy to check that c1 �_ c2 in that parser if
and only if m,(c1) �- m,(c2) in our variant. We ex-
tend this and define an isomorphism between com-
putations in both systems, such that a computation
c0, ... , cm in the original parser is mapped to a com-
putation m,(c0), ... , m,(cm) in the variant, with
both generating the same dependency graph A. This
</bodyText>
<figureCaption confidence="0.9931535">
Figure 1: A dependency structure of arbitrary gap degree
that can be parsed with Attardi’s parser.
</figureCaption>
<bodyText confidence="0.999540954545455">
proves that our notational variant is in fact equival-
ent to Attardi’s parser.
A relevant property of the set of dependency
structures that can be processed by Attardi’s parser,
even when restricted to transitions of degree two, is
that the number of discontinuities present in each of
their subtrees, defined as the gap degree by Bod-
irsky et al. (2005), is not bounded. For example, the
dependency graph in Figure 1 has gap degree n − 1,
and it can be parsed by the algorithm for any arbit-
rary n &gt; 1 by applying 2n shb transitions to push
all the nodes into the stack, followed by (2n − 2)
ra2 transitions to create the crossing arcs, and finally
one ra1 transition to create the dependency 1 -+ 2.
As mentioned in §1, the computational complex-
ity of the dynamic programming algorithm that will
be described in later sections does not depend on the
gap degree, contrary to the non-projective depend-
ency chart parsers presented by G´omez-Rodr´ıguez et
al. (2009) and by Kuhlmann and Satta (2009), whose
running time is exponential in the maximum gap de-
gree allowed by the grammar.
</bodyText>
<sectionHeader confidence="0.95722" genericHeader="method">
3 A Generative Probabilistic Model
</sectionHeader>
<bodyText confidence="0.999857">
In this section we introduce a generative probabil-
istic model based on the transition system of §2.2.
In formal language theory, there is a standard way
of giving a probabilistic interpretation to a non-
deterministic parser whose computations are based
on sequences of elementary operations such as trans-
itions. The idea is to define conditional probability
distributions over instances of the transition func-
tions, and to ‘combine’ these probabilities to assign
probabilities to computations and strings.
One difficulty we have to face with when dealing
with transition systems is that the notion of compu-
tation, defined in §2.1, depends on the input string,
because of the buffer component appearing in each
configuration. This is a pitfall to generative model-
</bodyText>
<equation confidence="0.6904695">
1 2 3  2n − 2 2n − 1 2n

</equation>
<page confidence="0.918506">
1237
</page>
<bodyText confidence="0.999760277777778">
ing, where we are interested in a system whose com-
putations lead to the generation of any string. To
overcome this problem, we observe that each com-
putation, defined as a sequence of stacks and buffers
(the configurations) can equivalently be expressed as
a sequence of stacks and transitions.
More precisely, consider a computation γ =
c0, ... , cm, m &gt; 1. Let σi, be the stack associated
with ci, for each i with 0 &lt; i &lt; m. Let also Cσ be
the set of all stacks associated with configurations in
C. We can make explicit the transitions that have
been used in the computation by rewriting γ in the
form σ0 fit1 σ1 ··· σm−1 fitm σm. In this way, γ
generates a string that is composed by all symbols
that are pushed into the stack by transitions shb, in
the left to right order.
We can now associate a probability to (our repres-
entation of) sequence γ by setting
</bodyText>
<equation confidence="0.999194">
p(γ) = �m p(ti  |σi−1). (1)
i=1
</equation>
<bodyText confidence="0.998588166666667">
To assign probabilities to complete computations we
should further multiply p(γ) by factors ps(σ0) and
pe(σm), where ps and pe are start and end probabil-
ity distributions, respectively, both defined over Cσ.
Note however that, as defined in §2.2, all initial con-
figurations are associated with stack [¢] and all final
configurations are associated with stack [¢, 0], thus
ps and pe are deterministic. Note that the Markov
chain represented in Eq. 1 is homogeneous, i.e., the
probabilities of the transition operations do not de-
pend on the time step.
As a second step we observe that, according to the
definition of transition system, each t E T has an in-
finite domain. A commonly adopted solution is to
introduce a special function, called history function
and denoted by H, defined over the set Cσ and tak-
ing values over some finite set. For each t E T and
σ, σ&apos; E Cσ, we then impose the condition
</bodyText>
<equation confidence="0.939567">
p(t  |σ) = p(t  |σ&apos;)
</equation>
<bodyText confidence="0.999630434782609">
whenever H(σ) = H(σ&apos;). Since H is finitely val-
ued, and since T is a finite set, the above condition
guarantees that there will only be a finite number of
parameters p(t  |σ) in our model.
So far we have presented a general discussion of
how to turn a transition-based parser into a gener-
ative probabilistic model, and have avoided further
specification of the history function. We now turn
our attention to the non-projective transition system
of §2.2. To actually transform that system into a
parametrized probabilistic model, and to develop an
associated efficient inference procedure as well, we
need to balance between the amount of information
we put into the history function and the computa-
tional complexity which is required for inference.
We start the discussion with a naive model using a
history function defined by a fixed size window over
the topmost portion of the stack. More precisely,
each transition is conditioned on the lexical form of
the three symbols at the top of the stack σ, indic-
ated as b3, b2, b1 E E below, with b1 referring to the
topmost symbol. The parameters of the model are
defined as follows.
</bodyText>
<equation confidence="0.999338222222222">
p(shb  |b3, b2, b1) = θshb
b3,b2,b1 , bb E E ,
p(la1  |b3,b2, b1) = θla1
b3,b2,b1 ,
p(ra1  |b3, b2, b1) = θra1 b1 ,
b3 b2
p(la2  |b3, b2, b1) =
ra2
p(ra2  |b3, b2, b1) = θb3,b2,b1 .
</equation>
<bodyText confidence="0.829295333333333">
The parameters above are subject to the follow-
ing normalization conditions, for every choice of
b3, b2, b1 E E:
</bodyText>
<equation confidence="0.980664888888889">
θla
1
+ θra1
+ θla2 +
b3,b2,b1 b3,b2,b1 b3,b2,b1
�
θra2 +
b3,b2,b1
bEΣ
</equation>
<bodyText confidence="0.999915285714286">
This naive model presents two practical problems.
The first problem relates to the efficiency of an in-
ference algorithm, which has a quite high computa-
tional complexity, as it will be discussed in §5. A
second problem arises in the probabilistic setting.
Using this model would require estimating many
parameters which are based on trigrams. This leads
to higher sample complexity to avoid sparse counts:
we would need more samples to accurately estimate
the model.
We therefore consider a more elaborated model,
which tackles both of the above problems. Again,
let b3, b2, b1 E E indicate the lexical form of the
three symbols at the top of the stack. We define the
</bodyText>
<equation confidence="0.986260666666667">
θla2
b3,b2,b1 ,
θshb
b3,b2,b1 = 1 .
1238
distributions p(t  |σ) as follows:
p(ra2  |b3, b2, b1) = θrd b1&apos; θrd2
b2,b1 �θra2
b3,b2,b1 .
</equation>
<bodyText confidence="0.8784725">
The parameters above are subject to the following
normalization conditions, for every b3, b2, b1 E Σ:
</bodyText>
<equation confidence="0.99753325">
θshb + θrd = 1 , (2)
b1 b1
θla1 + θra1 + θrd2 = 1 (3)
b2,b1 b2,b1 b2,b1
</equation>
<bodyText confidence="0.9917945">
Intuitively, parameter θrdb denotes the probability
that we perform a reduce transition instead of a shift
transition, given that we have seen lexical form b at
the top of the stack. Similarly, parameter θrd2
</bodyText>
<equation confidence="0.611286">
b2,b1 de-
</equation>
<bodyText confidence="0.927828555555556">
notes the probability that we perform a reduce trans-
ition of degree 2 (see §2.2) instead of a reduce trans-
ition of degree 1, given that we have seen lexical
forms b1 and b2 at the top of the stack.
We observe that the above model has a num-
ber of parameters |Σ |+ 4 - |Σ|2 + 2 - |Σ|3 (not
all independent). This should be contrasted with
the naive model, that has a number of parameters
4 • |Σ|3 + |Σ|4.
</bodyText>
<sectionHeader confidence="0.917312" genericHeader="method">
4 Tabular parsing
</sectionHeader>
<bodyText confidence="0.999986066666667">
We present here a dynamic programming algorithm
for simulating the computations of the system from
§2–3. Given an input string w, our algorithm pro-
duces a compact representation of the set Γ(w),
defined as the set of all possible computations of
the model when processing w. In combination with
the appropriate semirings, this method can provide
for instance the highest probability computation in
Γ(w), or else the probability of w, defined as the
sum of all probabilities of computations in Γ(w).
We follow a standard approach in the literature
on dynamic programming simulation of stack-based
automata (Lang, 1974; Tomita, 1986; Billot and
Lang, 1989). More recently, this approach has also
been applied by Huang and Sagae (2010) and by
</bodyText>
<figureCaption confidence="0.94973">
Figure 2: Schematic representation of the computations
</figureCaption>
<bodyText confidence="0.936957357142857">
γ associated with item [h1, i, h2h3, j].
Kuhlmann et al. (2011) to the simulation of pro-
jective transition-based parsers. The basic idea in
this approach is to decompose computations of the
parser into smaller parts, group them into equival-
ence classes and recombine to obtain larger parts of
computations.
Let w = a0 • • • an−1, Vw and S(np) be defined as
in §2. We use a structure called item, defined as
[h1, i, h2h3, j],
where 0 &lt; i &lt; j &lt; n and h1, h2, h3 E Vw must
satisfy h1 &lt; i and i &lt; h2 &lt; h3 &lt; j. The intended
interpretation of an item can be stated as follows; see
also Figure 2.
</bodyText>
<listItem confidence="0.984143714285714">
• There exists a computation γ of S(np) on w hav-
ing the form c0, ... , cm, m &gt; 1, with c0 =
(σ|h1, βi, A) and cm = (σ|h2|h3, βj, A&apos;) for
some stack σ and some arc sets A and A&apos;;
• For each i with 1 &lt; i &lt; m, the stack σi associated
with configuration ci has the list σ at the bottom
and satisfies |σi |&gt; |σ |+ 2.
</listItem>
<bodyText confidence="0.9997196">
Some comments on the above conditions are in
order here. Let t1, • • • , tm be the sequence of trans-
itions in T (np) associated with computation γ. Then
we have t1 = shai, since |σ1 |&gt; |σ |+ 2. Thus we
conclude that |σ1 |= |σ |+ 2.
The most important consequence of the definition
of item is that each transition ti with 2 &lt; i &lt; m
does not depend on the content of the σ portion of
the stack σi. To see this, consider transition ci−1 rti
ci. If ti = shai, the content of σ is irrelevant at
</bodyText>
<equation confidence="0.829102826086957">
p(shb  |b1) = θshb
b1 , bb E Σ ,
rd la1
p(la1  |b2, b1) = θb1 θb2,b1 ,
Ord θra1
p(ra1  |b2, b1) = Bb1 &apos; b2,b1 ,
p(la2  |b3, b2, b1) =
θrd.θrd2 θla2
b1b2,b1 b3,b2,b1 ,
θla2 θra2
b b
3,b2,b1 + 3,b2,b1 = 1 . (4)
h3
h2
j
σ
σ
i + 1
i
hl
i
hl
i + 1
</equation>
<figure confidence="0.962389142857143">
C1
σ
minimum
stack
length in
c1, . . . , cm
buffer size
stack size
���
stack
���
buffer
E
bEΣ
</figure>
<page confidence="0.991997">
1239
</page>
<bodyText confidence="0.9886555">
this step, since in our model shaz is conditioned only
on the topmost stack symbol of Qi−1, and we have
</bodyText>
<equation confidence="0.552633">
|Qi−1 |&gt;_ |Q |+ 2.
</equation>
<bodyText confidence="0.999830222222222">
Consider now the case of ti = la2. From |Qi |&gt;_
|Q |+ 2 we have that |Qi−1 |&gt;_ |Q |+ 3. Again, the
content of Q is irrelevant at this step, since in our
model la2 is conditioned only on the three topmost
stack symbols of Qi−1. A similar argument applies
to the cases of ti E {ra2, la1, ra1}.
From the above, we conclude that if we apply the
transitions t1, ... , tm to stacks of the form Q|h1, the
resulting computations have all identical probabilit-
ies, independently of the choice of Q.
Each computation satisfying the two conditions
above will be called an I-computation associ-
ated with item [h1, i, h2h3, j]. Notice that an I-
computation has the overall effect of replacing node
h1 sitting above a stack Q with nodes h2 and h3.
This is the key property in the development of our
algorithm below.
We specify our dynamic programming algorithm
as a deduction system (Shieber et al., 1995). The
deduction system starts with axiom [¢, 0, ¢0,1], cor-
responding to an initial stack [¢] and to the shift of
a0 = $ from the buffer into the stack. The set F(w)
is non-empty if and only if item [¢, 0, ¢0, n] can be
derived using the inference rules specified below.
Each inference rule is annotated with the type of
transition it simulates, along with the arc constructed
by the transition itself, if any.
</bodyText>
<equation confidence="0.999251846153846">
[h1,i,h2h3,j] (shad)
[h3, j, h3j, j + 1]
[h1, i, h2h3, k] [h3, k, h4h5, j]
(la1i h5 -+ h4)
[h1, i, h2h5, j]
[h1, i, h2h3, k] [h3, k, h4h5, j]
(ra1i h4 -+ h5)
[h1, i, h2h4, j]
[h1, i, h2h3, k] [h3, k, h4h5, j]
[h1, i, h4h5, j]
[h1, i, h2h3, k] [h3, k, h4h5, j]
(ra2i h2 -+ h5)
[h1, i, h2h4, j]
</equation>
<bodyText confidence="0.99982375">
The above deduction system infers items in a
bottom-up fashion. This means that longer compu-
tations over substrings of w are built by combining
shorter ones. In particular, the inference rule shad
asserts the existence of I-computations consisting of
a single shad transition. Such computations are rep-
resented by the consequent item [h3, j, h3j, j + 1],
indicating that the index of the shifted word aj is
added to the stack by pushing it on top of h3.
The remaining four rules implement the reduce
transitions of the model. We have already ob-
served in §2.2 that all available reduce transitions
shorten the size of the stack by one unit. This al-
lows us to combine pairs of I-computations with
a reduce transition, resulting in a computation that
is again an I-computation. More precisely, if we
concatenate an I-computation asserted by an item
[h1, i, h2h3, k] with an I-computation asserted by an
item [h3, k, h4h5, j], we obtain a computation that
has the overall effect of increasing the size of the
stack by 2, replacing the topmost stack element h1
with stack elements h2, h4 and h5. If we now apply
any of the reduce transitions from the inventory of
the model, we will remove one of these three nodes
from the stack, and the overall result will be again
an I-computation, which can then be asserted by a
certain item. For example, if we apply the reduce
transition la1, the consequent item is [h1, i, h2h5, j],
since an la1 transition removes the second topmost
element from the stack (h4). The other reduce trans-
itions remove a different element, and thus their
rules produce different consequent items.
The above argument shows the soundness of the
deduction system, i.e., an item I = [h1, i, h2h3, j]
is only generated if there exists an I-computation
-y = c0, ... , cm with c0 = (Q|h1, βi, A) and cm =
(Q|h2|h3,βj, A&apos;). To prove completeness, we must
show the converse result, i.e., that the existence of
an I-computation -y implies that item I is inferred.
We first do this under the assumption that the infer-
ence rule for the shift transitions do not have an ante-
cedent, i.e., items [h1, j, h1j, j + 1] are considered
as axioms. We proceed by using strong induction on
the length m of the computation -y.
For m = 1, -y consists of a single transition shad,
and the corresponding item I = [h1, j, h1j, j + 1]
is constructed as an axiom. For m &gt; 1, let -y be
as specified above. The transition that produced
</bodyText>
<equation confidence="0.9725">
(la2i h5 -+ h2)
</equation>
<page confidence="0.842057">
1240
</page>
<bodyText confidence="0.999406">
cm must have been a reduce transition, otherwise (2001) or Nederhof (2003). Such a parse forest en-
γ would not be an I-computation. Let ck be the codes all valid computations in Γ(w), as desired.
rightmost configuration in c0,... , cm−1 whose stack The algorithm runs in O(n8) time. Using meth-
size is |σ |+ 2. Then it can be shown that the com- ods similar to those specified in Eisner and Satta
putations γ1 = c0, ... , ck and γ2 = ck, ... , cm−1 (1999), we can reduce the running time to O(n7).
are again I-computations. Since γ1 and γ2 have However, we do not further pursue this idea here,
strictly fewer transitions than γ, by the induction hy- and proceed with the discussion of exact inference,
pothesis, the system constructs items [h1, i, h2h3, k] found in the next section.
and [h3, k, h4h5, j], where h2 and h3 are the stack 5 Inference
elements at the top of ck. Applying to these items We turn next to specify exact inference with our
the inference rule corresponding to the reduce trans- model, for computing feature expectations. Such
ition at hand, we can construct item I. inference enables, for example, the derivation of
When the inference rule for the shift transition has an expectation-maximization algorithm for unsuper-
an antecedent [h1, i, h2h3, j], as indicated above, we vised parsing.
have the overall effect that I-computations consist- Here, a feature is a function over computations,
ing of a single transition shifting aj on the top of h3 providing the count of a pattern related to a para-
are simulated only in case there exists a computation meter. We denote by fb3,b2,b1 (γ), for instance,
starting with configuration ([¢],β0) and reaching a the number of occurrences of transition lag within
configuration of the form (σ|h2|h3, βj). This acts as γ with topmost stack symbols having word forms
a filter on the search space of the algorithm, but does b3, b2, b1 E Σ, with b1 associated with the topmost
not invalidate the completeness property. However, stack symbol.
in this case the proof is considerably more involved, Feature expectations are computed by using an
and we do not report it here. inside-outside algorithm for the items in the tabu-
An important property of the deduction system lar algorithm. More specifically, given a string w,
above, which will be used in the next section, is we associate each item [h1, i, h2h3, j] defined as in
that the system is unambiguous, that is, each I- §4 with two quantities:
computation is constructed by the system in a I([h1,i,h2h3, j]) = � p(γ) ; (5)
unique way. This can be seen by observing that, in γ=([h1],βz),...,([h2,h3],βj)
the sketch of the completeness proof reported above, O([h1, i, h2h3, j]) = � p(γ) · p(γ0) . (6)
there always is an unique choice of ck that decom- σ,γ=([¢],β0),...,(σ|h1,βz)
poses I-computation γ into I-computations γ1 and γ&apos;=(σ|h2|h3,βj),...,([¢,0],βn)
γ2. In fact, if we choose a configuration ck&apos; other I([h1, i, h2h3, j]) and O([h1, i, h2h3, j]) are called
than ck with stack size |σ |+ 2, the computation the inside and the outside probabilities, respect-
γ0 2 = ck&apos;,. . . ,cm−1 will contain ck as an interme- ively, of item [h1, i, h2h3, j]. The tabular algorithm
diate configuration, which violates the definition of of §4 can be used to compute the inside probabilit-
I-computation because of an intervening stack hav- ies. Using the gradient transformation (Eisner et al.,
ing size not larger than the size of the stack associ- 2005), a technique for deriving outside probabilities
ated with the initial configuration. from a set of inference rules, we can also compute
As a final remark, we observe that we can keep O([h1, i, h2h3, j]). The use of the gradient trans-
track of all inference rules that have been applied formation is valid in our case because the tabular al-
in the computation of each item by the above al- gorithm is unambiguous (see §4).
gorithm, by encoding each application of a rule as Using the inside and outside probabilities, we can
a reference to the pair of items that were taken as now efficiently compute feature expectations for our
antecedent in the inference. In this way, we ob-
tain a parse forest structure that can be viewed as a
hypergraph or as a non-recursive context-free gram-
mar, similar to the case of parsing based on context-
free grammars. See for instance Klein and Manning
1241
</bodyText>
<equation confidence="0.999223457142857">
X
Ep(γ|w)[fla2
b3,b2,b1(γ)] =
γ∈Γ(w)
X
p(γ  |w) · fla2
b3,b2,b1(γ) = 1
p(w) ·
p(γ) · fla2
b3,b2,b1(γ)
γ∈Γ(w)
1 X
p(w) ·
σ,i,k,j,
h1,h2,h3,h4,h5,
s.t. ah2=b3,
ah4=b2, ah5=b1
X p(γ0) · p(γ1) · p(γ2) · p(la2  |b3, b2, b1) · p(γ3)
γ0=([¢],β0),...,(σ|h1,βi),
γ1=(σ|h1,βi),...,(σ|h2|h3,βk),
γ2=(σ|h2|h3,βk),...,(σ|h2|h4|h5,βj),
γ3=(σ|h2|h5,βj),...,([¢,0],βn)
X·
σ,i,j,
h1,h2,h5, s.t.
ah2=b3, ah5=b1
X p(γ0) · p(γ3) ·
γ0=([¢],β0),...,(σ|h1,βi),
γ3=(σ|h2|h5,βj),...,([¢,0],βn)
θrd θrd2 θla2
b1 b2,b1 b3,b2,b1
p(w)
X· X p(γ1) · X p(γ2)
k,h3,h4, γ1=(σ|h1,βi),...,(σ|h2|h3,βk) γ2=(σ|h2|h3,βk),...,(σ|h2|h4|h5,βj)
s.t. ah4=b2
</equation>
<figureCaption confidence="0.999053">
Figure 3: Decomposition of the feature expectation Ep(γ|w)[fla2
</figureCaption>
<bodyText confidence="0.862116833333333">
b3,b2,b1(γ)] into a finite summation. Quantity p(w) above
is the sum over all probabilities of computations in Γ(w).
model. Figure 3 shows how to express the expect-
ation of feature fla2
b3,b2,b1(γ) by means of a finite
summation. Using Eq. 5 and 6 and the relation
</bodyText>
<equation confidence="0.975245909090909">
p(w) = I([¢, 0, ¢0, n]) we can then write:
θrd· θrd2 θla2
Ep(
la2 b1 b2,b1 b3,b2,b1
γ |w) [fb3,b2,b1(γ)] =I([¢, 0, ¢0, n]) ·
X· O([h1, i, h4h5,j]) ·
i,j,h1,h4,h5,
s.t. ah4=b2, ah5=b1
X· I([h1,i,h2h3,k]) · I([h3,k,h4h5,j]) .
k,h2,h3,
s.t. ah2=b3
</equation>
<bodyText confidence="0.928336">
Very similar expressions can be derived for the ex-
pectations for features fra2
</bodyText>
<figure confidence="0.73146725">
b3,b2,b1(γ), fla1
b2,b1(γ), and
fra1(&apos;Y) As for feature fbhb (ry), b E E, the above
b ib
approach leads to
Ep(γ|w)[fshb
b1 (γ)] =
θb1b
</figure>
<bodyText confidence="0.887893928571429">
I ([¢, 0, ¢0, n] )
σ,i,h, s.t.
ah=b1, ai=b
As mentioned above, these expectations can be
used, for example, to derive an EM algorithm for our
model. The EM algorithm in our case is not com-
pletely straightforward because of the way we para-
metrize the model. We give now the re-estimation
steps for such an EM algorithm. We assume that all
expectations below are taken with respect to a set of
parameters θ from iteration s − 1 of the algorithm,
and we are required to update these θ. To simplify
notation, let us assume that there is only one string w
in the training corpus. For each b1 ∈ Σ, we define:
</bodyText>
<equation confidence="0.966625666666667">
Ep(γ |w) hi
fb2,b1 la1 ra1 (γ) + fb2,b1 (γ)
h i
fla2
Ep(γ|w) b3,b2,b1(γ) + fra2
b3,b2,b1(γ) ;
XZb2,b1 = Ep(γ |w) hi
b3∈E fb3,b2,b1 la2 (1) + ra2
fb3,b2,b1 (γ) .
</equation>
<bodyText confidence="0.93322">
We then have, for every b ∈ Σ:
</bodyText>
<equation confidence="0.926311333333333">
θshb
b1 (s) ← .
Zb1 + Pb&apos;∈E Ep(γ|w)[fshb&apos;
b1 (γ)]
O([h, i, hi, i + 1]) .
XZb1 =
b2∈E
X
+
b3,b2∈E
Ep(γ|w)[fshb
b1 (γ)]
</equation>
<page confidence="0.884551">
1242
</page>
<bodyText confidence="0.869682">
Furthermore, we have:
</bodyText>
<equation confidence="0.952716">
la1 (γ)]
la1 Ep(γ|w) [fb2,b1
N,b1 (s) +__
</equation>
<bodyText confidence="0.312848">
and:
</bodyText>
<table confidence="0.964093142857143">
l .
a2 b (γ)]
la2 Ep(γ|w) [fb3,b2, i
θ
b3,b2,b1 (s) +__a2 ra2
Ep(γ |w) [43,b2,b1γ
() + fb3,b2,b1 (γ)
</table>
<bodyText confidence="0.983065666666667">
The rest of the parameter updates can easily be de-
rived using the above updates because of the sum-
to-1 constraints in Eq. 2–4.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999900285714286">
We note that our model inherits spurious ambigu-
ity from Attardi’s model. More specifically, we can
have different derivations, corresponding to differ-
ent system computations, that result in identical de-
pendency graphs and strings. While running our
tabular algorithm with the Viterbi semiring effi-
ciently computes the highest probability computa-
tion in Γ(w), spurious ambiguity means that find-
ing the highest probability dependency tree is NP-
hard. This latter result can be shown using proof
techniques similar to those developed by Sima’an
(1996). We leave it for future work how to eliminate
spurious ambiguity from the model.
While in the previous sections we have described
a tabular method for the transition system of Attardi
(2006) restricted to transitions of degree up to two, it
is possible to generalize the model to include higher-
degree transitions. In the general formulation of At-
tardi parser, transitions of degree d create links in-
volving nodes located d positions beneath the top-
most position in the stack:
</bodyText>
<equation confidence="0.9989875">
lad : (σ|i1|i2 |... |id+1,β, A) �
(σ|i2 |... |id+1,β, A U {id+1 —+ i1});
rad : (σ|i1|i2 |... |id+1,β,A) �
(σ|i1|i2 |... |id,β, A U {i1 —+ id+1}).
</equation>
<bodyText confidence="0.998995777777778">
To define a transition system that supports trans-
itions up to degree D, we use a set of
items of the form [s1 ... sD−1, i, e1 ... eD, j], cor-
responding (in the sense of §4) to compu-
tations of the form c0, ... , cm, m &gt; 1,
with c0 = (σ|s1 |... |sD−1,βz, A) and cm =
(σ|e1 |... |eD,βj, A0). The deduction steps corres-
ponding to reduce transitions in this general system
have the general form
</bodyText>
<equation confidence="0.98878025">
[s1 ... sD−1, i, e1m1 ... mD−1, j]
[m1 ... mD−1, j, e2 ... eD+1, w]
(ep —+ ec)
[s1 ... sD−1, i, e1 ... ec−1ec+1 ... eD+1, w]
</equation>
<bodyText confidence="0.99995856">
where the values of p and c differ for each transition:
to obtain the inference rule corresponding to a lad
transition, we make p = D + 1 and c = D + 1 − d;
and to obtain the rule for a rad transition, we make
p = D + 1 − d and c = D + 1. Note that the parser
runs in time O(n3D+2), where D stands for the max-
imum transition degree, so each unit increase in the
transition degree adds a cubic factor to the parser’s
polynomial time complexity. This is in contrast to a
previous tabular formulation of the Attardi parser by
G´omez-Rodr´ıguez et al. (2011), which ran in expo-
nential time.
The model for the transition system we give in this
paper is generative. It is not hard to naturally extend
this model to the discriminative setting. In this case,
we would condition the model on the input string to
get a conditional distribution over derivations. It is
perhaps more natural in this setting to use arbitrary
weights for the parameter values, since the compu-
tation of a normalization constant (the probability of
a string) is required in any case. Arbitrary weights
in the generative setting could be more problematic,
because it would require computing a normalization
constant corresponding to a sum over all strings and
derivations.
</bodyText>
<sectionHeader confidence="0.99819" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999365">
We presented in this paper a generative probabilistic
model for non-projective parsing, together with the
description of an efficient tabular algorithm for pars-
ing and doing statistical inference with the model.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999729333333333">
The authors thank Marco Kuhlmann for helpful
comments on an early draft of the paper. The authors
also thank Giuseppe Attardi for the help received to
extract the parsing statistics. The second author has
been partially supported by Ministerio de Ciencia e
Innovaci´on and FEDER (TIN2010-18552-C03-02).
</bodyText>
<equation confidence="0.87455825">
[42,b1
a1 ra1
Zb2,b1 + Ep(γ |w) (γ) + fb2,b1(γ)
� ,
</equation>
<page confidence="0.974064">
1243
</page>
<sectionHeader confidence="0.982331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999216571428571">
Giuseppe Attardi. 2006. Experiments with a multil-
anguage non-projective dependency parser. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 166–170,
New York, USA.
Sylvie Billot and Bernard Lang. 1989. The structure
of shared forests in ambiguous parsing. In Proceed-
ings of the 27th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 143–151,
Vancouver, Canada.
Manuel Bodirsky, Marco Kuhlmann, and Mathias M¨ohl.
2005. Well-nested drawings as models of syntactic
structure. In Tenth Conference on Formal Gram-
mar and Ninth Meeting on Mathematics of Language,
pages 195–203, Edinburgh, UK.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL), pages
149–164, New York, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Auto-
maton Grammars. In Proceedings of the 37th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 457–464, College Park, MD,
USA.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005.
Compiling Comp Ling: Practical weighted dynamic
programming and the Dyna language. In Proceedings
of HLT-EMNLP, pages 281–290.
Carlos G´omez-Rodriguez and Joakim Nivre. 2010. A
transition-based parser for 2-planar dependency struc-
tures. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1492–1501, Uppsala, Sweden.
Carlos G´omez-Rodr´ıguez, David J. Weir, and John Car-
roll. 2009. Parsing mildly non-projective dependency
structures. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 291–299, Athens, Greece.
Carlos G´omez-Rodr´ıguez, John Carroll, and David Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics (in press), 37(3).
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573–605.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2000. Introduction to Automata Theory.
Addison-Wesley, 2nd edition.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1077–1086,
Uppsala, Sweden.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 18th International Conference on Compu-
tational Linguistics (COLING-ACL), pages 646–652,
Montr´eal, Canada.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of the IWPT, pages
123–134.
Terry Koo, Amir Globerson, Xavier Carreras, and Mi-
chael Collins. 2007. Structured prediction models
via the matrix-tree theorem. In Proceedings of the
EMNLP-CoNLL, pages 141–150.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective depend-
ency parsing. In Twelfth Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 478–486, Athens, Greece.
Marco Kuhlmann, Carlos G´omez-Rodriguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics (ACL), Portland, Oregon,
USA.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In Jacques Loecx,
editor, Automata, Languages and Programming, 2nd
Colloquium, University of Saarbr¨ucken, July 29–
August 2, 1974, number 14 in Lecture Notes in Com-
puter Science, pages 255–269. Springer.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 40–51, Singa-
pore.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Tenth International Conference on Parsing
Technologies (IWPT), pages 121–132, Prague, Czech
Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Human Language
Technology Conference (HLT) and Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 523–530, Vancouver, Canada.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and knuth’s algorithm. Computational Linguistics,
29(1):135–143.
</reference>
<page confidence="0.840545">
1244
</page>
<reference confidence="0.999550384615385">
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In 43rd Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 99–106, Ann Arbor, USA.
Joakim Nivre. 2004. Incrementality in deterministic de-
pendency parsing. In Workshop on Incremental Pars-
ing: Bringing Engineering and Cognition Together,
pages 50–57, Barcelona, Spain.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguist-
ics, 34(4):513–553.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the 47th An-
nual Meeting of the ACL and the Fourth International
Joint Conference on Natural Language Processing of
the AFNLP, pages 351–359, Singapore.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive
parsing. Journal of Logic Programming, 24(1–2):3–
36.
Khalil Sima’an. 1996. Computational complexity
of probabilistic disambiguation by means of tree-
grammars. In Proceedings of COLING, pages 1175–
1180.
David A. Smith and Noah A. Smith. 2007. Probab-
ilistic models of nonprojective dependency trees. In
Proceedings of the EMNLP-CoNLL, pages 132–140.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of IJCAI, pages 281–290.
Masaru Tomita. 1986. Efficient Parsing for Natural
Language: A Fast Algorithm for Practical Systems.
Springer.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies (IWPT), pages 195–206, Nancy,
France.
</reference>
<page confidence="0.990897">
1245
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594276">
<title confidence="0.9993335">Exact Inference for Generative Non-Projective Dependency Parsing</title>
<author confidence="0.999986">Shay B Cohen Carlos G´omez-Rodriguez</author>
<affiliation confidence="0.994672">School of Computer Science Departamento de Computaci´on Mellon University, USA Universidade da Spain</affiliation>
<email confidence="0.630663">scohen@cs.cmu.educgomezr@udc.es</email>
<author confidence="0.999512">Giorgio Satta</author>
<affiliation confidence="0.999756">Dept. of Information University of Padua,</affiliation>
<email confidence="0.995017">satta@dei.unipd.it</email>
<abstract confidence="0.9957988">We describe a generative model for nonprojective dependency parsing based on a simplified version of a transition system that has recently appeared in the literature. We then develop a dynamic programming parsing algorithm for our model, and derive an insideoutside algorithm that can be used for unsupervised learning of non-projective dependency trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giuseppe Attardi</author>
</authors>
<title>Experiments with a multilanguage non-projective dependency parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>166--170</pages>
<location>New York, USA.</location>
<contexts>
<context position="1462" citStr="Attardi, 2006" startWordPosition="207" endWordPosition="208">s. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space</context>
<context position="2744" citStr="Attardi (2006)" startWordPosition="409" endWordPosition="411">n cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programm</context>
<context position="8994" citStr="Attardi (2006)" startWordPosition="1524" endWordPosition="1525">] (the empty buffer). 2.2 A Non-projective Transition System We now turn to give a description of our transition system for non-projective parsing. While a projective dependency tree satisfies the requirement that, for every arc in the tree, there is a directed path between its headword and each of the words between the two endpoints of the arc, a nonprojective dependency tree may violate this condition. Even though some natural languages exhibit syntactic phenomena which require non-projective expressive power, most often such a resource is used in a limited way. This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency. The author defines this distance as the degree of the transition, with transitions of degree one being able to handle only projective dependencies. This formulation permits parsing a subset of the non-projective trees, where this subset depends on the degree of the transitions. The reported coverage in Attardi (2006) is already very high when the system is restricted </context>
<context position="10925" citStr="Attardi (2006)" startWordPosition="1859" endWordPosition="1860">scussion on this issue. Let w = a0 · · · an−1 be an input string over Σ defined as in §2.1, with a0 = $. Our transition system for non-projective dependency parsing is S(np) = (C, T(np), I(np), C(np) t ), Language Deg. 2 Deg. 3 Deg. 4 Arabic 180 21 7 Bulgarian 961 41 10 Czech 27181 1668 85 Danish 876 136 53 Dutch 9072 2119 171 German 15827 2274 466 Japanese 1484 143 9 Portuguese 3104 424 37 Slovene 601 48 13 Spanish 66 7 0 Swedish 1566 226 79 Turkish 579 185 8 Table 1: The number of non-projective relations of various degrees for several treebanks (training sets), as reported by the parser of Attardi (2006). Deg. stands for ‘degree.’ The parser did not detect non-projective relations of degree higher than 4. where C is the same set of configurations defined in §2.1. The initialization function I(np) maps each string w to the initial configuration ([¢],β0, ∅). The set of terminal configurations C(np) t contains all configurations of the form ([¢, 0], [], A), for any set of arcs A. The set of transition functions is defined as T(np) = {shb |b ∈ Σ} ∪ {la1, ra1,la2, ra2}, where each transition is specified below. We let variables i, j, k, l range over Vw, and variable σ is a list of stack elements f</context>
<context position="13321" citStr="Attardi (2006)" startWordPosition="2296" endWordPosition="2297"> and ra1 have degree one, as already explained. When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004). Transition la2 and transition ra2 are very similar to la1 and ra1, respectively, but with the difference that they create a new arc between the topmost node in the stack and a node which is two positions below the topmost node. Hence, these transitions have degree two, and are the key components in parsing of non-projective dependencies. We turn next to describe the equivalence between our system and the system in Attardi (2006). The transition-based parser presented by Attardi pushes back into the buffer elements that are in the top position of the stack. However, a careful analysis shows that only the first position in the buffer can be affected by this operation, in the sense that elements that are pushed back from the stack are never found in buffer positions other than the first. This means that we can consider the first element of the buffer as an additional stack element, always sitting on the top of the top-most stack symbol. More formally, we can define a function m, : C -+ C that maps configurations in the </context>
<context position="35771" citStr="Attardi (2006)" startWordPosition="6368" endWordPosition="6369">s, corresponding to different system computations, that result in identical dependency graphs and strings. While running our tabular algorithm with the Viterbi semiring efficiently computes the highest probability computation in Γ(w), spurious ambiguity means that finding the highest probability dependency tree is NPhard. This latter result can be shown using proof techniques similar to those developed by Sima’an (1996). We leave it for future work how to eliminate spurious ambiguity from the model. While in the previous sections we have described a tabular method for the transition system of Attardi (2006) restricted to transitions of degree up to two, it is possible to generalize the model to include higherdegree transitions. In the general formulation of Attardi parser, transitions of degree d create links involving nodes located d positions beneath the topmost position in the stack: lad : (σ|i1|i2 |... |id+1,β, A) � (σ|i2 |... |id+1,β, A U {id+1 —+ i1}); rad : (σ|i1|i2 |... |id+1,β,A) � (σ|i1|i2 |... |id,β, A U {i1 —+ id+1}). To define a transition system that supports transitions up to degree D, we use a set of items of the form [s1 ... sD−1, i, e1 ... eD, j], corresponding (in the sense of</context>
</contexts>
<marker>Attardi, 2006</marker>
<rawString>Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 166–170, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>143--151</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="22041" citStr="Billot and Lang, 1989" startWordPosition="3856" endWordPosition="3859">mming algorithm for simulating the computations of the system from §2–3. Given an input string w, our algorithm produces a compact representation of the set Γ(w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in Γ(w), or else the probability of w, defined as the sum of all probabilities of computations in Γ(w). We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989). More recently, this approach has also been applied by Huang and Sagae (2010) and by Figure 2: Schematic representation of the computations γ associated with item [h1, i, h2h3, j]. Kuhlmann et al. (2011) to the simulation of projective transition-based parsers. The basic idea in this approach is to decompose computations of the parser into smaller parts, group them into equivalence classes and recombine to obtain larger parts of computations. Let w = a0 • • • an−1, Vw and S(np) be defined as in §2. We use a structure called item, defined as [h1, i, h2h3, j], where 0 &lt; i &lt; j &lt; n and h1, h2, h3</context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL), pages 143–151, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Bodirsky</author>
<author>Marco Kuhlmann</author>
<author>Mathias M¨ohl</author>
</authors>
<title>Well-nested drawings as models of syntactic structure.</title>
<date>2005</date>
<booktitle>In Tenth Conference on Formal Grammar and Ninth Meeting on Mathematics of Language,</booktitle>
<pages>195--203</pages>
<location>Edinburgh, UK.</location>
<marker>Bodirsky, Kuhlmann, M¨ohl, 2005</marker>
<rawString>Manuel Bodirsky, Marco Kuhlmann, and Mathias M¨ohl. 2005. Well-nested drawings as models of syntactic structure. In Tenth Conference on Formal Grammar and Ninth Meeting on Mathematics of Language, pages 195–203, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLLX shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>149--164</pages>
<location>New York, USA.</location>
<contexts>
<context position="9950" citStr="Buchholz and Marsi, 2006" startWordPosition="1671" endWordPosition="1675">g able to handle only projective dependencies. This formulation permits parsing a subset of the non-projective trees, where this subset depends on the degree of the transitions. The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. For instance, on training data for Czech containing 28,934 non-projective relations, 27,181 can be handled by degree two transitions, and 1,668 additional dependencies can be handled by degree three transitions. Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. Our results are based on such a restriction. It is not difficult to extend our algorithms (§4) to higher degree transitions, but this comes at the expense of higher complexity. See §6 for more discussion on this issue. Let w = a0 · · · an−1 be an input string over Σ defined as in §2.1, with a0 = $. Our transition system for non-projective dependency parsing is S(np) = (C, T(np), I(np), C(np) t ), Language Deg. 2 Deg. 3 Deg. 4 Arab</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLLX shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 149–164, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and Head Automaton Grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>457--464</pages>
<location>College Park, MD, USA.</location>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and Head Automaton Grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 457–464, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Eric Goldlust</author>
<author>Noah A Smith</author>
</authors>
<title>Compiling Comp Ling: Practical weighted dynamic programming and the Dyna language.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP,</booktitle>
<pages>281--290</pages>
<marker>Eisner, Goldlust, Smith, 2005</marker>
<rawString>Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005. Compiling Comp Ling: Practical weighted dynamic programming and the Dyna language. In Proceedings of HLT-EMNLP, pages 281–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>Joakim Nivre</author>
</authors>
<title>A transition-based parser for 2-planar dependency structures.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1492--1501</pages>
<location>Uppsala,</location>
<marker>G´omez-Rodriguez, Nivre, 2010</marker>
<rawString>Carlos G´omez-Rodriguez and Joakim Nivre. 2010. A transition-based parser for 2-planar dependency structures. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1492–1501, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>David J Weir</author>
<author>John Carroll</author>
</authors>
<title>Parsing mildly non-projective dependency structures.</title>
<date>2009</date>
<booktitle>In Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>291--299</pages>
<location>Athens, Greece.</location>
<marker>G´omez-Rodr´ıguez, Weir, Carroll, 2009</marker>
<rawString>Carlos G´omez-Rodr´ıguez, David J. Weir, and John Carroll. 2009. Parsing mildly non-projective dependency structures. In Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 291–299, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>John Carroll</author>
<author>David Weir</author>
</authors>
<title>Dependency parsing schemata and mildly nonprojective dependency parsing. Computational Linguistics</title>
<date>2011</date>
<note>(in press), 37(3).</note>
<marker>G´omez-Rodr´ıguez, Carroll, Weir, 2011</marker>
<rawString>Carlos G´omez-Rodr´ıguez, John Carroll, and David Weir. 2011. Dependency parsing schemata and mildly nonprojective dependency parsing. Computational Linguistics (in press), 37(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="2421" citStr="Goodman, 1999" startWordPosition="356" endWordPosition="357">k (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of eleme</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Joshua Goodman. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Rajeev Motwani</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to Automata Theory. Addison-Wesley, 2nd edition.</title>
<date>2000</date>
<contexts>
<context position="7665" citStr="Hopcroft et al., 2000" startWordPosition="1289" endWordPosition="1292">graphs (dependency trees). Without any restriction on transition functions in T, these functions might have an infinite domain, and could thus encode even nonrecursively enumerable languages. However, in standard practice for natural language parsing, transitions are always specified by some finite mean. In particular, the definition of each transition depends on some finite window at the top of the stack and some finite window at the beginning of the buffer in each configuration. In this case, we can view a transition-based dependency parser as a notational variant of a push-down transducer (Hopcroft et al., 2000), whose computations output sequences that directly encode dependency trees. These transducers are nondeterministic, meaning that several transitions can be applied to some configurations. The transition systems we investigate in this paper follow these principles. We close this subsection with some additional notation. We denote the stack with its topmost element to the right and the buffer with its first element to the left. We indicate concatenation in the stack and buffer by a vertical bar. For example, for k ∈ Vw, Q|k denotes some stack with topmost element k and k|0 denotes some buffer w</context>
</contexts>
<marker>Hopcroft, Motwani, Ullman, 2000</marker>
<rawString>John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2000. Introduction to Automata Theory. Addison-Wesley, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1077--1086</pages>
<location>Uppsala,</location>
<contexts>
<context position="1932" citStr="Huang and Sagae, 2010" startWordPosition="276" endWordPosition="279">and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlm</context>
<context position="22119" citStr="Huang and Sagae (2010)" startWordPosition="3869" endWordPosition="3872"> an input string w, our algorithm produces a compact representation of the set Γ(w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in Γ(w), or else the probability of w, defined as the sum of all probabilities of computations in Γ(w). We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989). More recently, this approach has also been applied by Huang and Sagae (2010) and by Figure 2: Schematic representation of the computations γ associated with item [h1, i, h2h3, j]. Kuhlmann et al. (2011) to the simulation of projective transition-based parsers. The basic idea in this approach is to decompose computations of the parser into smaller parts, group them into equivalence classes and recombine to obtain larger parts of computations. Let w = a0 • • • an−1, Vw and S(np) be defined as in §2. We use a structure called item, defined as [h1, i, h2h3, j], where 0 &lt; i &lt; j &lt; n and h1, h2, h3 E Vw must satisfy h1 &lt; i and i &lt; h2 &lt; h3 &lt; j. The intended interpretation of </context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1077–1086, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 18th International Conference on Computational Linguistics (COLING-ACL),</booktitle>
<pages>646--652</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="3427" citStr="Kahane et al. (1998)" startWordPosition="508" endWordPosition="511">ures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to 1234 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the trans</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998. Pseudo-projectivity: A polynomially parsable nonprojective dependency grammar. In 36th Annual Meeting of the Association for Computational Linguistics and 18th International Conference on Computational Linguistics (COLING-ACL), pages 646–652, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the IWPT,</booktitle>
<pages>123--134</pages>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and hypergraphs. In Proceedings of the IWPT, pages 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Amir Globerson</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Structured prediction models via the matrix-tree theorem.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="1277" citStr="Koo et al., 2007" startWordPosition="175" endWordPosition="178">ed for unsupervised learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successf</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins. 2007. Structured prediction models via the matrix-tree theorem. In Proceedings of the EMNLP-CoNLL, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Treebank grammar techniques for non-projective dependency parsing.</title>
<date>2009</date>
<booktitle>In Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<pages>478--486</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="3490" citStr="Kuhlmann and Satta (2009)" startWordPosition="517" endWordPosition="521">ional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To the best of our knowledge, this paper is the first to 1234 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1234–1245, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics introduce a dynamic programming algorithm for inference with non-projective structures of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition-based model we use, together with a probabilistic generat</context>
<context position="15560" citStr="Kuhlmann and Satta (2009)" startWordPosition="2698" endWordPosition="2701"> For example, the dependency graph in Figure 1 has gap degree n − 1, and it can be parsed by the algorithm for any arbitrary n &gt; 1 by applying 2n shb transitions to push all the nodes into the stack, followed by (2n − 2) ra2 transitions to create the crossing arcs, and finally one ra1 transition to create the dependency 1 -+ 2. As mentioned in §1, the computational complexity of the dynamic programming algorithm that will be described in later sections does not depend on the gap degree, contrary to the non-projective dependency chart parsers presented by G´omez-Rodr´ıguez et al. (2009) and by Kuhlmann and Satta (2009), whose running time is exponential in the maximum gap degree allowed by the grammar. 3 A Generative Probabilistic Model In this section we introduce a generative probabilistic model based on the transition system of §2.2. In formal language theory, there is a standard way of giving a probabilistic interpretation to a nondeterministic parser whose computations are based on sequences of elementary operations such as transitions. The idea is to define conditional probability distributions over instances of the transition functions, and to ‘combine’ these probabilities to assign probabilities to </context>
</contexts>
<marker>Kuhlmann, Satta, 2009</marker>
<rawString>Marco Kuhlmann and Giorgio Satta. 2009. Treebank grammar techniques for non-projective dependency parsing. In Twelfth Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 478–486, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Carlos G´omez-Rodriguez</author>
<author>Giorgio Satta</author>
</authors>
<title>Dynamic programming algorithms for transition-based dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Portland, Oregon, USA.</location>
<marker>Kuhlmann, G´omez-Rodriguez, Satta, 2011</marker>
<rawString>Marco Kuhlmann, Carlos G´omez-Rodriguez, and Giorgio Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>Automata, Languages and Programming, 2nd Colloquium, University of Saarbr¨ucken, July 29– August 2, 1974, number 14 in Lecture Notes in Computer Science,</booktitle>
<pages>255--269</pages>
<editor>In Jacques Loecx, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="22003" citStr="Lang, 1974" startWordPosition="3852" endWordPosition="3853">sent here a dynamic programming algorithm for simulating the computations of the system from §2–3. Given an input string w, our algorithm produces a compact representation of the set Γ(w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in Γ(w), or else the probability of w, defined as the sum of all probabilities of computations in Γ(w). We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989). More recently, this approach has also been applied by Huang and Sagae (2010) and by Figure 2: Schematic representation of the computations γ associated with item [h1, i, h2h3, j]. Kuhlmann et al. (2011) to the simulation of projective transition-based parsers. The basic idea in this approach is to decompose computations of the parser into smaller parts, group them into equivalence classes and recombine to obtain larger parts of computations. Let w = a0 • • • an−1, Vw and S(np) be defined as in §2. We use a structure called item, defined as [h1, i, h2h3, </context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>Bernard Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In Jacques Loecx, editor, Automata, Languages and Programming, 2nd Colloquium, University of Saarbr¨ucken, July 29– August 2, 1974, number 14 in Lecture Notes in Computer Science, pages 255–269. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Jason Eisner</author>
</authors>
<title>First- and second-order expectation semirings with applications to minimumrisk training on translation forests.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>40--51</pages>
<contexts>
<context position="2443" citStr="Li and Eisner, 2009" startWordPosition="358" endWordPosition="361">. More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent po</context>
</contexts>
<marker>Li, Eisner, 2009</marker>
<rawString>Zhifei Li and Jason Eisner. 2009. First- and second-order expectation semirings with applications to minimumrisk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 40–51, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Tenth International Conference on Parsing Technologies (IWPT),</booktitle>
<pages>121--132</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1303" citStr="McDonald and Satta, 2007" startWordPosition="179" endWordPosition="183">d learning of non-projective dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective p</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>Ryan McDonald and Giorgio Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Tenth International Conference on Parsing Technologies (IWPT), pages 121–132, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–530, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted deductive parsing and knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="28142" citStr="Nederhof (2003)" startWordPosition="5056" endWordPosition="5057">the existence of an I-computation -y implies that item I is inferred. We first do this under the assumption that the inference rule for the shift transitions do not have an antecedent, i.e., items [h1, j, h1j, j + 1] are considered as axioms. We proceed by using strong induction on the length m of the computation -y. For m = 1, -y consists of a single transition shad, and the corresponding item I = [h1, j, h1j, j + 1] is constructed as an axiom. For m &gt; 1, let -y be as specified above. The transition that produced (la2i h5 -+ h2) 1240 cm must have been a reduce transition, otherwise (2001) or Nederhof (2003). Such a parse forest enγ would not be an I-computation. Let ck be the codes all valid computations in Γ(w), as desired. rightmost configuration in c0,... , cm−1 whose stack The algorithm runs in O(n8) time. Using methsize is |σ |+ 2. Then it can be shown that the com- ods similar to those specified in Eisner and Satta putations γ1 = c0, ... , ck and γ2 = ck, ... , cm−1 (1999), we can reduce the running time to O(n7). are again I-computations. Since γ1 and γ2 have However, we do not further pursue this idea here, strictly fewer transitions than γ, by the induction hy- and proceed with the disc</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted deductive parsing and knuth’s algorithm. Computational Linguistics, 29(1):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>99--106</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1447" citStr="Nivre and Nilsson, 2005" startWordPosition="203" endWordPosition="206"> community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow p</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective dependency parsing. In 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 99–106, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<location>Barcelona,</location>
<contexts>
<context position="12887" citStr="Nivre (2004)" startWordPosition="2222" endWordPosition="2223"> are called reduce transitions, i.e., transitions that consume nodes from the stack. Notice that in the transition system at hand all the reduce transitions decrease the size of the stack by one element. Transition la1 creates a new arc with the topmost node on the stack as the head and the secondtopmost node as the dependent, and removes the latter from the stack. Transition ra1 is symmetric with respect to la1. Transitions la1 and ra1 have degree one, as already explained. When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004). Transition la2 and transition ra2 are very similar to la1 and ra1, respectively, but with the difference that they create a new arc between the topmost node in the stack and a node which is two positions below the topmost node. Hence, these transitions have degree two, and are the key components in parsing of non-projective dependencies. We turn next to describe the equivalence between our system and the system in Attardi (2006). The transition-based parser presented by Attardi pushes back into the buffer elements that are in the top position of the stack. However, a careful analysis shows t</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Algorithms for deterministic incremental dependency parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="1823" citStr="Nivre, 2008" startWordPosition="262" endWordPosition="263"> spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representations of all parse trees for a given input string, even in cases where the size of this set is exponential in the length of the string itself. In combination with appropriate semirings, these packed representations can be exploited to compute many values of interest for machine learning, such as best parses and feature expectations (Goodman, 1999; </context>
<context position="4530" citStr="Nivre (2008)" startWordPosition="683" endWordPosition="684">es of unbounded gap degree. The rest of this paper is organized as follows. In §2 and §3 we outline the transition-based model we use, together with a probabilistic generative interpretation. In §4 we give the tabular algorithm for parsing, and in §5 we discuss statistical inference using expectation maximization. We then discuss some other aspects of the work in §6 and conclude in §7. 2 Transition-based Dependency Parsing In this section we briefly introduce the basic definitions for transition-based dependency parsing. For a more detailed presentation of this subject, we refer the reader to Nivre (2008). We then define a specific transition-based model for non-projective dependency parsing that we investigate in this paper. 2.1 General Transition Systems Assume an input alphabet Σ with a special symbol $ ∈ Σ, which we use as the root of our parse structures. Throughout this paper we denote the input string as w = a0 ···an_1, n ≥ 1, where a0 = $ and ai ∈ Σ \ {$} for each i with 1 ≤ i ≤ n − 1. A dependency tree for w is a directed tree Gw = (Vw, Aw), where Vw = {0, ... , n − 1} is the set of nodes, and Aw ⊆ Vw × Vw is the set of arcs. The root of Gw is the node 0. The intended meaning is that </context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>Joakim Nivre. 2008. Algorithms for deterministic incremental dependency parsing. Computational Linguistics, 34(4):513–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the Fourth International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>351--359</pages>
<contexts>
<context position="1475" citStr="Nivre, 2009" startWordPosition="209" endWordPosition="210">tical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-based algorithms) allow polynomial space representati</context>
<context position="2977" citStr="Nivre, 2009" startWordPosition="447" endWordPosition="448">h as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed structures. To</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the 47th Annual Meeting of the ACL and the Fourth International Joint Conference on Natural Language Processing of the AFNLP, pages 351–359, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>36</pages>
<contexts>
<context position="24923" citStr="Shieber et al., 1995" startWordPosition="4455" endWordPosition="4458">of ti E {ra2, la1, ra1}. From the above, we conclude that if we apply the transitions t1, ... , tm to stacks of the form Q|h1, the resulting computations have all identical probabilities, independently of the choice of Q. Each computation satisfying the two conditions above will be called an I-computation associated with item [h1, i, h2h3, j]. Notice that an Icomputation has the overall effect of replacing node h1 sitting above a stack Q with nodes h2 and h3. This is the key property in the development of our algorithm below. We specify our dynamic programming algorithm as a deduction system (Shieber et al., 1995). The deduction system starts with axiom [¢, 0, ¢0,1], corresponding to an initial stack [¢] and to the shift of a0 = $ from the buffer into the stack. The set F(w) is non-empty if and only if item [¢, 0, ¢0, n] can be derived using the inference rules specified below. Each inference rule is annotated with the type of transition it simulates, along with the arc constructed by the transition itself, if any. [h1,i,h2h3,j] (shad) [h3, j, h3j, j + 1] [h1, i, h2h3, k] [h3, k, h4h5, j] (la1i h5 -+ h4) [h1, i, h2h5, j] [h1, i, h2h3, k] [h3, k, h4h5, j] (ra1i h4 -+ h5) [h1, i, h2h4, j] [h1, i, h2h3, k</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation by means of treegrammars.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1175--1180</pages>
<marker>Sima’an, 1996</marker>
<rawString>Khalil Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of treegrammars. In Proceedings of COLING, pages 1175– 1180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic models of nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP-CoNLL,</booktitle>
<pages>132--140</pages>
<contexts>
<context position="1327" citStr="Smith and Smith, 2007" startWordPosition="184" endWordPosition="187">ve dependency trees. 1 Introduction Dependency grammars have received considerable attention in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae,</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>David A. Smith and Noah A. Smith. 2007. Probabilistic models of nonprojective dependency trees. In Proceedings of the EMNLP-CoNLL, pages 132–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>281--290</pages>
<contexts>
<context position="2963" citStr="Titov et al., 2009" startWordPosition="443" endWordPosition="446">achine learning, such as best parses and feature expectations (Goodman, 1999; Li and Eisner, 2009). In this paper we move one step forward with respect to Huang and Sagae (2010) and Kuhlmann et al. (2011) and present a polynomial dynamic programming algorithm for non-projective transitionbased parsing. Our algorithm is coupled with a simplified version of the transition system from Attardi (2006), which has high coverage for the type of non-projective structures that appear in various treebanks. Instead of an additional transition operation which permits swapping of two elements in the stack (Titov et al., 2009; Nivre, 2009), Attardi’s system allows reduction of elements at non-adjacent positions in the stack. We also present a generative probabilistic model for transition-based parsing. The implication for this, for example, is that one can now approach the problem of unsupervised learning of non-projective dependency structures within the transition-based framework. Dynamic programming algorithms for nonprojective parsing have been proposed by Kahane et al. (1998), G´omez-Rodr´ıguez et al. (2009) and Kuhlmann and Satta (2009), but they all run in exponential time in the ‘gap degree’ of the parsed </context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of IJCAI, pages 281–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems.</title>
<date>1986</date>
<publisher>Springer.</publisher>
<contexts>
<context position="22017" citStr="Tomita, 1986" startWordPosition="3854" endWordPosition="3855">dynamic programming algorithm for simulating the computations of the system from §2–3. Given an input string w, our algorithm produces a compact representation of the set Γ(w), defined as the set of all possible computations of the model when processing w. In combination with the appropriate semirings, this method can provide for instance the highest probability computation in Γ(w), or else the probability of w, defined as the sum of all probabilities of computations in Γ(w). We follow a standard approach in the literature on dynamic programming simulation of stack-based automata (Lang, 1974; Tomita, 1986; Billot and Lang, 1989). More recently, this approach has also been applied by Huang and Sagae (2010) and by Figure 2: Schematic representation of the computations γ associated with item [h1, i, h2h3, j]. Kuhlmann et al. (2011) to the simulation of projective transition-based parsers. The basic idea in this approach is to decompose computations of the parser into smaller parts, group them into equivalence classes and recombine to obtain larger parts of computations. Let w = a0 • • • an−1, Vw and S(np) be defined as in §2. We use a structure called item, defined as [h1, i, h2h3, j], where 0 &lt; </context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Masaru Tomita. 1986. Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<location>Nancy, France.</location>
<contexts>
<context position="1422" citStr="Yamada and Matsumoto, 2003" startWordPosition="199" endWordPosition="202">n in the statistical parsing community in recent years. These grammatical formalisms offer a good balance between structural expressivity and processing efficiency. Most notably, when non-projectivity is supported, these formalisms can model crossing syntactic relations that are typical in languages with relatively free word order. Recent work has reduced non-projective parsing to the identification of a maximum spanning tree in a graph (McDonald et al., 2005; Koo et al., 2007; McDonald and Satta, 2007; Smith and Smith, 2007). An alternative to this approach is to use transitionbased parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. This paper focuses on the latter approach. The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). More recently, dynamic programming has been successfully used for projective parsing (Huang and Sagae, 2010; Kuhlmann et al., 2011). Dynamic programming algorithms for parsing (also known as chart-</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the Eighth International Workshop on Parsing Technologies (IWPT), pages 195–206, Nancy, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>