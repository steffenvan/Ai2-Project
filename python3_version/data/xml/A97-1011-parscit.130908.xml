<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.996259">
A non-projective dependency parser
</title>
<author confidence="0.97717">
Pasi Tapanainen and Timo Jarvinen
</author>
<affiliation confidence="0.779587">
University of Helsinki, Department of General Linguistics
Research Unit for Multilingual Language Technology
</affiliation>
<address confidence="0.789412">
P.O. Box 4, FIN-00014 University of Helsinki, Finland
</address>
<email confidence="0.817695">
{Pasi.Tapanainen,Timo.Jarvinen}oning.Helsinki.fi
</email>
<sectionHeader confidence="0.979941" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999891111111111">
We describe a practical parser for unre-
stricted dependencies. The parser creates
links between words and names the links
according to their syntactic functions. We
first describe the older Constraint Gram-
mar parser where many of the ideas come
from. Then we proceed to describe the cen-
tral ideas of our new parser. Finally, the
parser is evaluated.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998239">
We are concerned with surface-syntactic parsing of
running text. Our main goal is to describe syntac-
tic analyses of sentences using dependency links that
show the head-modifier relations between words. In
addition, these links have labels that refer to the
syntactic function of the modifying word. A simpli-
fied example is in Figure 1, where the link between
I and see denotes that I is the modifier of see and
its syntactic function is that of subject. Similarly, a
modifies bird, and it is a determiner.
</bodyText>
<figure confidence="0.895946666666667">
see
bird
a
</figure>
<figureCaption confidence="0.998756">
Figure 1: Dependencies for sentence I see a bird.
</figureCaption>
<bodyText confidence="0.999866375">
First, in this paper, we explain some central con-
cepts of the Constraint Grammar framework from
which many of the ideas are derived. Then, we give
some linguistic background to the notations we are
using, with a brief comparison to other current de-
pendency formalisms and systems. New formalism
is described briefly, and it is utilised in a small toy
grammar to illustrate how the formalism works. Fi-
nally, the real parsing system, with a grammar of
some 2 500 rules, is evaluated.
The parser corresponds to over three man-years of
work, which does not include the lexical analyser and
the morphological disambiguator, both parts of the
existing English Constraint Grammar parser (Karls-
son et al., 1995). The parsers can be tested via
WWW&apos;.
</bodyText>
<sectionHeader confidence="0.987655" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999933909090909">
Our work is partly based on the work done with
the Constraint Grammar framework that was orig-
inally proposed by Fred Karlsson (1990). A de-
tailed description of the English Constraint Gram-
mar (ENGCG) is in Karlsson et al. (1995). The basic
rule types of the Constraint Grammar (Tapanainen,
1996)2 are REMOVE and SELECT for discarding and se-
lecting an alternative reading of a word. Rules also
have contextual tests that describe the condition ac-
cording to which they may be applied. For example,
the rule
</bodyText>
<equation confidence="0.667136">
REMOVE (V) IF (-1C DET);
</equation>
<bodyText confidence="0.923212625">
discards a verb (V) reading if the preceding word
(-1) is unambiguously (C) a determiner (DET). More
than one such test can be appended to a rule.
The rule above represents a local rule: the test
checks only neighbouring words in a foreknown po-
sition before or after the target word. The test may
also refer to the positions somewhere in the sentence
without specifying the exact location. For instance,
</bodyText>
<sectionHeader confidence="0.87763" genericHeader="method">
SELECT (IMP) IF (NOT *-1 NOM-HEAD);
</sectionHeader>
<bodyText confidence="0.928875">
means that a nominal head (NOM-HEAD is a set that
contains part-of-speech tags that may represent a
nominal head) may not appear anywhere to the left
(NOT *-1).
</bodyText>
<footnote confidence="0.852447">
1 at http://www.ling.helsinki.fi/-,tapanain/dg/
&apos;The CG-2 notation here (Tapanainen, 1996) is dif-
ferent from the former (Karlsson et al., 1995). A con-
cise introduction to the formalism is also to be found in
Samuelsson et al. (1996) and Hurslcainen (1996).
</footnote>
<page confidence="0.999081">
64
</page>
<bodyText confidence="0.988949">
This &amp;quot;anywhere&amp;quot; to the left or right may be re-
stricted by BARRIERs, which restrict the area of the
test. Basically, the barrier can be used to limit
the test only to the current clause (by using clause
boundary markers and &amp;quot;stop words&amp;quot;) or to a con-
stituent (by using &amp;quot;stop categories&amp;quot;) instead of the
whole sentence. In addition, another test may be
added relative to the unrestricted context position
using keyword LINK. For example, the following rule
discards the syntactic function&apos; CI-OBJ (indirect ob-
ject):
</bodyText>
<equation confidence="0.822674666666667">
REMOVE (@I-OBJ)
IF (*-1C VFIN BARRIER SVO0
LINK NOT 0 SV00);
</equation>
<bodyText confidence="0.961244102040816">
The rule holds if the closest finite verb to the left is
unambiguously (C) a finite verb (VFIN), and there is
no ditransitive verb or participle (subcategorisation
SV00) between the verb and the indirect object. If,
in addition, the verb does not take indirect objects,
i.e. there is no SVO0 in the same verb (LINK NOT 0
SV00), the CI-OBJ reading will be discarded.
In essence, the same formalism is used in the syn-
tactic analysis in Jarvinen (1994) and Anttila (1995).
After the morphological disambiguation, all legiti-
mate surface-syntactic labels are added to the set of
morphological readings. Then, the syntactic rules
discard contextually illegitimate alternatives or se-
lect legitimate ones.
The syntactic tagset of the Constraint Grammar
provides an underspecific dependency description.
For example, labels for functional heads (such as
CSUBJ, COBJ, CI-OBJ) mark the word which is a head
of a noun phrase having that function in the clause,
but the parent is not indicated. In addition, the rep-
resentation is shallow, which means that, e.g., ob-
jects of infinitives and participles receive the same
type of label as objects of finite verbs. On the other
hand, the non-finite verb forms functioning as ob-
jects receive only verbal labels.
When using the grammar formalism described
above, a considerable amount of syntactic ambigu-
ity can not be resolved reliably and is therefore left
pending in the parse. As a consequence, the output
is not optimal in many applications. For example, it
is not possible to reliably pick head-modifier pairs
from the parser output or collect arguments of verbs,
which was one of the tasks we originally were inter-
ested in.
To solve the problems, we developed a more pow-
erful rule formalism which utilises an explicit depen-
dency representation. The basic Constraint Gram-
&apos;The convention in the Constraint Grammar is that
the tags for syntactic functions begin with the 0-sign.
mar idea of introducing the information in a piece-
meal fashion is retained, but the integration of dif-
ferent pieces of information is more efficient in the
new system.
3 Dependency grammars in a
nutshell
Our notation follows the classical model of depen-
dency theory (Heringer, 1993) introduced by Lucien
Tesniere (1959) and later advocated by Igor Mel&apos;auk
(1987).
</bodyText>
<subsectionHeader confidence="0.999921">
3.1 Uniqueness and projectivity
</subsectionHeader>
<bodyText confidence="0.999837548387097">
In Tesniere&apos;s and Mel&apos;auk&apos;s dependency notation ev-
ery element of the dependency tree has a unique
head. The verb serves as the head of a clause and
the top element of the sentence is thus the main
verb of the main clause. In some other theories, e.g.
Hudson (1991), several heads are allowed.
Projectivity (or adjacency4) was not an issue for
Tesniere (1959, ch. 10), because he thought that the
linear order of the words does not belong to the syn-
tactic level of representation which comprises the
structural order only.
Some early formalisations, c.f. (Hays, 1964), have
brought the strict projectivity (context-free) require-
ment into the dependency framework. This kind
of restriction is present in many dependency-based
parsing systems (McCord, 1990; Sleator and Tern-
perley, 1991; Eisner, 1996).
But obviously any recognition grammar should
deal with non-projective phenomena to the extent
they occur in natural languages as, for example, in
the analysis shown in Figure 2. Our system has no
in-built restrictions concerning projectivity, though
the formalism allows us to state when crossing links
are not permitted.
We maintain that one is generally also interested
in the linear order of elements, and therefore it is
presented in the tree diagrams. But, for some pur-
poses, presenting all arguments in a canonical order
might be more adequate. This, however, is a matter
of output formatting, for which the system makes
several options available.
</bodyText>
<subsectionHeader confidence="0.997197">
3.2 Valency and categories
</subsectionHeader>
<bodyText confidence="0.948017">
The verbs (as well as other elements) have a valency
that describes the number and type of the modifiers
they may have. In valency theory, usually, comple-
ments (obligatory) and adjuncts (optional) are dis-
tinguished.
</bodyText>
<footnote confidence="0.8332065">
4D is adjacent to H provided that every word between
D and H is a subordinate of H (Hudson, 1991).
</footnote>
<page confidence="0.993426">
65
</page>
<figure confidence="0.997478571428571">
main: &lt;ROOT&gt;
&lt;SAID&gt; obj:
VFIN
&lt;JOAN&gt; &lt;SUITS&gt;
NSG subj: SGS VFIN
&lt;LIKES&gt; &lt;HER&gt;
SGS VFIN°b- &lt;DECIDE&gt; PRON ACC SGS
subj obj:
&lt;WHATEVER&gt;&lt;JOHN&gt;
PRON WH NSG
INF
nfmark:
&lt;TO&gt;
INFMARK&gt;
</figure>
<figureCaption confidence="0.999825">
Figure 2: A dependency structure for the sentence: Joan said whatever John likes to decide suits her.
</figureCaption>
<bodyText confidence="0.999996384615385">
Our notation makes a difference between valency
(rule-based) and subcategorisation (lexical): the va-
lency tells which arguments are expected; the sub-
categorisation tells which combinations are legiti-
mate. The valency merely provides a possibility to
have an argument. Thus, a verb having three va-
lency slots may have e.g. subcategorisation SVOO or
SVOC. The former denotes: Subject, Verb, indirect
Object and Object, and the latter: Subject, Verb,
Object and Object Complement. The default is a
nominal type of complement, but there might also
be additional information concerning the range of
possible complements, e.g., the verb say may have
an object (SVO), which may also be realised as a
to-infinitive clause, WH-clause, that-clause or quote
structure.
The adjuncts are not usually marked in the verbs
because most of the verbs may have e.g. spatio-
temporal arguments. Instead, adverbial comple-
ments and adjuncts that are typical of particular
verbs are indicated. For instance, the verb decide
has the tag &lt;P/on&gt; which means that the preposi-
tional phrase on is typically attached to it.
The distinction between the complements and the
adjuncts is vague in the implementation; neither the
complements nor the adjuncts are obligatory.
</bodyText>
<sectionHeader confidence="0.994872" genericHeader="method">
4 Introducing the dependencies
</sectionHeader>
<bodyText confidence="0.999707117647059">
Usually, both the dependent element and its head
are implicitly (and ambiguously) present in the Con-
straint Grammar type of rule. Here, we make this
dependency relation explicit. This is done by declar-
ing the heads and the dependents (complement or
modifier) in the context tests.
For example, the subject label (OSOBJ) is chosen
and marked as a dependent of the immediately fol-
lowing auxiliary (AUXMOD) in the following rule:
SELECT (@SUBJ) IF (IC AUXMOD HEAD);
To get the full benefit of the parser, it is also use-
ful to name the valency slot in the rule. This has
two effects: (1) the valency slot is unique, i.e. no
more than one subject is linked to a finite verb&apos;,
and (2) we can explicitly state in rules which kind
of valency slots we expect to be filled. The rule thus
is of the form:
</bodyText>
<sectionHeader confidence="0.36928" genericHeader="method">
SELECT (@SUBJ)
</sectionHeader>
<bodyText confidence="0.8657495">
IF (IC AUXMOD HEAD = subject);
The rule above works well in an unambiguous con-
text but there is still need to specify more tolerant
rules for ambiguous contexts. The rule
</bodyText>
<equation confidence="0.413224">
INDEX (@SUBJ)
IF (1C AUXMOD HEAD = subject);
</equation>
<bodyText confidence="0.999911">
differs from the previous rule in that it leaves the
other readings of the noun intact and only adds a
(possible) subject dependency, while both the previ-
ous rules disambiguated the noun reading also.
But especially in the rule above, the contextual
test is far from being sufficient to select the subject
reading reliably. Instead, it leaves open a possibil-
ity to attach a dependency from another syntactic
function, i.e. the dependency relations remain am-
biguous. The grammar tries to be careful not to
introduce false dependencies but for an obvious rea-
son this is not always possible. If several syntac-
tic functions of a word have dependency relations,
they form a dependency forest. Therefore, when the
syntactic function is not rashly disambiguated, the
correct reading may survive even after illegitimate
</bodyText>
<footnote confidence="0.6642255">
5 Coordination is handled via the coordinator that col-
lects coordinated subjects in one slot.
</footnote>
<page confidence="0.977954">
66
</page>
<bodyText confidence="0.999251533333333">
linking, as the global pruning (Section 5) later ex-
tracts dependency links that form consistent trees.
Links formed between syntactic labels constitute
partial trees, usually around verbal nuclei. But a
new mechanism is needed to make full use of the
structural information provided by multiple rules.
Once a link is formed between labels, it can be used
by the other rules. For example, when a head of an
object phrase (OM) is found and indexed to a verb,
the noun phrase to the right (if any) is probably an
object complement (CPCOMPL-0). It should have the
same head as the existing object if the verb has the
proper subcategorisation tag (SVOC). The following
rule establishes a dependency relation of a verb and
its object complement, if the object already exists.
</bodyText>
<equation confidence="0.831976">
INDEX (@PCOMPL-0)
IF (*-1 ©OBJ BARRIER @NPHEAD
LINK 0 UP object SVOC HEAD=o-compl);
</equation>
<bodyText confidence="0.999787928571429">
The rule says that a dependency relation (o-compl)
should be added but the syntactic functions should
not be disambiguated (INDEX). The object comple-
ment OPCOMPL-0 is linked to the verb readings hav-
ing the subcategorisation SVOC. The relation of the
object complement and its head is such that the
noun phrase to the left of the object complement is
an object (CM) that has established a dependency
relation (object) to the verb.
Naturally, the dependency relations may also be
followed downwards (DOWN). But it is also possible to
declare the last item in a chain of the links (e.g. the
verb chain would have been wanted) using the key-
words TOP and BOTTOM.
</bodyText>
<sectionHeader confidence="0.983957" genericHeader="method">
5 Ambiguity and pruning
</sectionHeader>
<bodyText confidence="0.9972925">
We pursue the following strategy for linking and dis-
ambiguation.
</bodyText>
<listItem confidence="0.98885725">
• In the best case, we are sure that some reading
is correct in the current context. In this case,
both disambiguation and linking can be done
at the same time (with command SELECT and
keyword HEAD).
• The most typical case is that the context gives
some evidence about the correct reading, but we
know that there are some rare instances when
that reading is not correct. In such a case, we
only add a link.
• Sometimes the context gives strong hints as to
what the correct reading can not be. In such
a case we can remove some readings even if
we do not know what the correct alternative
is. This is a fairly typical case in the Con-
straint Grammar framework, but relatively rare
</listItem>
<bodyText confidence="0.999588837837838">
in the new dependency grammar. In practice,
these rules are most likely to cause errors, apart
from their linguistic interpretation often being
rather obscure. Moreover, there is no longer
any need to remove these readings explicitly by
rules, because the global pruning removes read-
ings which have not obtained any &amp;quot;extra evi-
dence&amp;quot;.
Roughly, one could say that the REMOVE rules of
the Constraint Grammar are replaced by the INDEX
rules. The overall result is that the rules in the
new framework are much more careful than those
of ENGCG.
As already noted, the dependency grammar has
a big advantage over ENGCG in dealing with am-
biguity. Because the dependencies are supposed to
form a tree, we can heuristically prune readings that
are not likely to appear in such a tree. We have the
following hypotheses: (1) the dependency forest is
quite sparse and a whole parse tree can not always
be found; (2) pruning should favour large (sub)trees;
(3) unlinked readings of a word can be removed when
there is a linked reading present among the alterna-
tives; (4) unambiguous subtrees are more likely to be
correct than ambiguous ones; and (5) pruning need
not force the words to be unambiguous. Instead,
we can apply the rules iteratively, and usually some
of the rules apply when the ambiguity is reduced.
Pruning is then applied again, and so on. Further-
more, the pruning mechanism does not contain any
language specific statistics, but works on a topolog-
ical basis only.
Some of the most heuristic rules may be applied
only after pruning. This has two advantages: very
heuristic links would confuse the pruning mecha-
nism, and words that would not otherwise have a
head, may still get one.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="method">
6 Toy-grammar example
</sectionHeader>
<bodyText confidence="0.998532785714286">
In this section, we present a set of rules, and show
how those rules can parse the sentence &amp;quot;Joan said
whatever John likes to decide suits her&amp;quot;. The toy
grammar containing 8 rules is presented in Figure 3.
The rules are extracted from the real grammar, and
they are then simplified; some tests are omitted and
some tests are made simpler. The grammar is ap-
plied to the input sentence in Figure 4, where the
tags are almost equivalent to those used by the
English Constraint Grammar, and the final result
equals Figure 2, where only the dependencies be-
tween the words and certain tags are printed.
Some comments concerning the rules in the toy
grammar (Figure 3) are in order:
</bodyText>
<page confidence="0.994282">
67
</page>
<figure confidence="0.983549">
INDEX (@SUBJ) IF (1 @+F HEAD = subj:);
INDEX (INF @-FMAINV) IF (-1 INFMARK) (-2 PTC1-COMPL-V + SVO HEAD = obj:);
INDEX (@INFMARK&gt;) IF (1 (INF @-FMAINV) HEAD = infmark:);
SELECT (PRON ACC @OBJ) IF (1C CLB) (-1 @MAINV HEAD = obj:);
INDEX (PRON WH @OBJ)
IF (*1 @SUBJ BARRIER @NPHEAD-MAIN
LINK 0 UP subj: @+F
LINK 0 TOP v-ch: @MAINV
LINK 0 BOTTOM obj: SVO + @-FMAINV HEAD = obj:);
INDEX @MAINV
IF (*-1 WH BARRIER @MV-CLB/CC LINK -1 @MV-CLB/CC)
(*1C @+F BARRIER @SUBJ OR CLB HEAD = subj:);
PRUNING
INDEX @MAINV
IF (NOT *1 @+F BARRIER SUBJ-BARRIER)
(*-1 (PRON WH) BARRIER CLB LINK -1 VCOG + SVO + @MAINV HEAD = obj:);
INDEX @+FMAINV
IF (NOT 0 @+FAUXV) (NOT *1 @+F BARRIER CLB)
(0 DOWN subj: @SUBJ LINK NOT *-1 @CS) (@0 (&lt;s&gt;) HEAD = main:);
</figure>
<figureCaption confidence="0.994629">
Figure 3: A toy grammar of 8 rules
</figureCaption>
<listItem confidence="0.96809335">
1. A simple rule shows how the subject (0SOS1) is
indexed to a finite verb by a link named subj.
2. The infinitives preceded by the infinitive marker
to can be reliably linked to the verbs with the
proper subcategorisation, i.e. the verb belongs
to both categories PTC1-COMPL-V and SVO.
3. The infinitive marker is indexed to the infinitive
by the link named inf mark.
4. Personal pronouns have morphological ambigu-
ity between nominative (NOM) and accusative
(ACC) readings. Here, the accusative reading
is selected and linked to the main verb imme-
diately to the left, if there is an unambiguous
clause boundary immediately to the right.
5. The WH-pronoun is a clause boundary marker,
but the only reliable means to find its head is
to follow the links. Therefore, the WH-pronoun
is not indexed before the appropriate subject is
linked to the verb chain which also has a verbal
object.
</listItem>
<bodyText confidence="0.995375666666667">
The rule states: the first noun phrase head la-
bel to the right is a subject (OSOBJ), link subj
exists and is followed up to the finite verb (0+F)
in a verb chain (v-ch), which is then followed
up to the main verb. Then object or comple-
ment links are followed downwards (BOTTOM),
to the last verbal reading (here decide). If then
a verb with subcategorisation for objects is en-
countered, an object link from the WH-pronoun
is formed.
This kind of rule that starts from word A, fol-
lows links up to word B and then down to word
C, introduces a non-projective dependency link
if word B is between words A and C.
Note that the conditions TOP and BOTTOM follow
the chain of named link, if any, to the upper or
lower end of a chain of a multiple (zero or more)
links with the same name. Therefore TOP v-ch:
@MA INV always ends with the main verb in the
verb chain, whether this be a single finite verb
like likes or a chain like would have been liked.
6. The WH-clause itself may function as a subject,
object, etc. Therefore, there is a set of rules
for each function. The &amp;quot;WH-clause as subject&amp;quot;
rule looks for a finite verb to the right. No in-
tervening subject labels and clause boundaries
are allowed.
* Rules 1-5 are applied in the first round. After
that, the pruning operation disambiguates finite
verbs, and rule 6 will apply.
Pruning will be applied once again. The sen-
tence is thus disambiguated both morphologi-
cally and morphosyntactically, and a syntactic
</bodyText>
<page confidence="0.995324">
68
</page>
<figure confidence="0.997603724137931">
&amp;quot;&lt;Joan&gt;&amp;quot;
&amp;quot;joan&amp;quot; N NOM SG ONH OSUBJ ©OBJ @I-OBJ OPCOMPL-S OPCOMPL-0 ©APP @A&gt; @&lt;P @O-ADVL
&amp;quot;&lt;said&gt;&amp;quot;
&amp;quot;say&amp;quot; PCP2 O&lt;P-FMAINV @-FMAINV
&amp;quot;say&amp;quot; V PAST VFIN 0-I-FMAINV
&amp;quot;say&amp;quot; A ABS @PCOMPL-S @PCOMPL-0 @A&gt; @APP @SUBJ @OBJ @I-OBJ O&lt;P @&lt;NOM
&amp;quot;&lt;whatever&gt;&amp;quot;
&amp;quot;whatever&amp;quot; ADV @ADVL ©AD-A&gt;
&amp;quot;whatever&amp;quot; DET CENTRAL WH SG/PL @DN&gt;
&amp;quot;whatever&amp;quot; &lt;CLB&gt; PRON WH SG/PL @SUBJ ©OBJ 0I-OBJ @PCOMPL-S OPCOMPL-0 d&lt;P O&lt;NOM
&amp;quot;&lt;John&gt;&amp;quot;
&amp;quot;john&amp;quot; N NOM SG ©NH OSUBJ ©OBJ 0I-OBJ OPCOMPL-S OPCOMPL-0 ©APP @A&gt; @GP 00-ADVL
&amp;quot;&lt;likes&gt;&amp;quot;
&amp;quot;like&amp;quot; N NOM PL ©NH OSUBJ ©OBJ 0I-OBJ @PCOMPL-S OPCOMPL-0 ©APP @A&gt; O&lt;P 00-ADVL
&amp;quot;like&amp;quot; V PRES SG3 VFIN 0-FFMAINV
&amp;quot;&lt;to&gt;&amp;quot;
&amp;quot;to&amp;quot; PREP @&lt;NOM @ADVL
&amp;quot;to&amp;quot; INFMARK&gt; @INFMARK&gt;
&amp;quot;&lt;decide&gt;&amp;quot;
&amp;quot;decide&amp;quot; V SUBJUNCTIVE VFIN 0-FFMAINV
&amp;quot;decide&amp;quot; V IMP VFIN @-FFMAINV
&amp;quot;decide&amp;quot; V INF ©-FMAINV @&lt;P-FMAINV
&amp;quot;decide&amp;quot; V PRES -SG3 VFIN @+FMAINV
&amp;quot;&lt;suits&gt;&amp;quot;
&amp;quot;suit&amp;quot; V PRES SG3 VFIN @-FFMAINV
&amp;quot;suit&amp;quot; N NOM PL ©NH @SUBJ ©OBJ 0I-OBJ OPCOMPL-S @PCOMPL-0 @APP ©A&gt; @&lt;P 00-ADVL
&amp;quot;&lt;her&gt;&amp;quot;
&amp;quot;she&amp;quot; PRON PERS FEM GEN SG3 OGN&gt;
&amp;quot;she&amp;quot; PRON PERS FEM ACC SG3 ©OBJ
</figure>
<figureCaption confidence="0.999985">
Figure 4: A sentence after morphological analysis. Each line presents a morphological and @-signs mor-
</figureCaption>
<bodyText confidence="0.8274811">
phosyntactic alternatives, e.g. whatever is ambiguous in 10 ways. The subcategorisation/valency information
is not printed here.
reading from each word belongs to a subtree of
which the root is said or suits.
7. The syntactic relationship between the verbs is
established by a rule stating that the rightmost
main verb is the (clause) object of a main verb
to the left, which allows such objects.
8. Finally, there is a single main verb, which is
indexed to the root (&lt;s&gt;) (in position 00).
</bodyText>
<sectionHeader confidence="0.988978" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.963303">
7.1 Efficiency
</subsectionHeader>
<bodyText confidence="0.9797458">
The evaluation was done using small excerpts of
data, not used in the development of the system.
All text samples were excerpted from three different
genres in the Bank of English (Jarvinen, 1994) data:
American National Public Radio (broadcast), British
Books data (literature), and The Independent (news-
paper). Figure 5 lists the samples, their sizes, and
the average and maximum sentence lengths. The
measure is in words excluding punctuation.
size avg. max. total
</bodyText>
<figure confidence="0.55565775">
(w) length length time
2281 19 44 12 sec.
1920 15 51 8.5 sec.
1427 19 47 8.5 sec.
</figure>
<figureCaption confidence="0.995337">
Figure 5: Benchmark used in the evaluation
</figureCaption>
<bodyText confidence="0.999755125">
In addition, Figure 5 shows the total processing
time required for the syntactic analysis of the sam-
ples. The syntactic analysis has been done in a nor-
mal PC with the Linux operating system. The PC
has a Pentium 90 MHz processor and 16 MB of mem-
ory. The speed roughly corresponds to 200 words in
second. The time does not include morphological
analysis and disambiguation6.
</bodyText>
<footnote confidence="0.983771333333333">
6The CG-2 program (Tapanainen, 1996) runs a mod-
ified disambiguation grammar of Voutilainen (1995)
about 1000 words in second.
</footnote>
<bodyText confidence="0.796606666666667">
broadcast
literature
newspaper
</bodyText>
<page confidence="0.994858">
69
</page>
<table confidence="0.9737506">
DG ENGCG
succ. amb. succ. amb.
broadcast 97.0 % 3.2 % 96.8 % 12.7 %
literature 97.3 % 3.3 % 95.9% 11.3%
newspaper 96.4 % 3.3 % 94.2 % 13.7 %
</table>
<figureCaption confidence="0.9519545">
Figure 6: ENGCG syntax and morphosyntactic level
of the dependency grammar
</figureCaption>
<subsectionHeader confidence="0.99918">
7.2 Comparison to ENGCG syntax
</subsectionHeader>
<bodyText confidence="0.957512625">
One obvious point of reference is the ENGCG syn-
tax, which shares a level of similar representation
with an almost identical tagset to the new system.
In addition, both systems use the front parts of the
ENGCG system for processing the input. These in-
clude the tokeniser, lexical analyser and morpholog-
ical disambiguator.
Figure 6 shows the results of the comparison of the
ENGCG syntax and the morphosyntactic level of the
dependency grammar. Because both systems leave
some amount of the ambiguity pending, two figures
are given: the success rate, which is the percent-
age of correct morphosyntactic labels present in the
output, and the ambiguity rate, which is the percent-
age of words containing more than one label. The
ENGCG results compare to those reported elsewhere
(Jirvinen, 1994; Tapanainen and Jarvinen, 1994).
The DG success rate is similar or maybe even
slightly better than in ENGCG. More importantly,
the ambiguity rate is only about a quarter of that
in the ENGCG output. The overall result should be
considered good in the sense that the output con-
tains information about the syntactic functions (see
Figure 4) not only part-of-speech tags.
</bodyText>
<subsectionHeader confidence="0.966219">
7.3 Dependencies
</subsectionHeader>
<bodyText confidence="0.999986">
The major improvement over ENGCG is the level
of explicit dependency representation, which makes
it possible to excerpt modifiers of certain elements,
such as arguments of verbs. This section evaluates
the success of the level of dependencies.
</bodyText>
<subsectionHeader confidence="0.502344">
7.3.1 Unnamed dependencies
</subsectionHeader>
<bodyText confidence="0.994267307692308">
One of the crude measures to evaluate depen-
dencies is to count how many times the correct
head is found. The results are listed in Fig-
( received. correct links,
)
ure 7. Precision is and re-
received links
call ( ). received correct links, The difference between
‘ desired lnks
precision and recalli is due to the fact that the parser
does not force a head on every word. Trying out
some very heuristic methods to assign heads would
raise recall but lower precision. A similar measure
</bodyText>
<table confidence="0.98882975">
precision recall
broadcast 93.4 % 88.0 %
literature 96.0 % 88.6 %
newspaper 95.3 % 87.9 %
</table>
<figureCaption confidence="0.574479">
Figure 7: Percentages of heads correctly attached
</figureCaption>
<table confidence="0.998732166666667">
broadcast precision recall N
subjects 95 % 89 % 244
objects 89 % 83 % 140
predicatives 96 % 86 % 57
literature precision recall N
subjects 98 % 92 % 195
objects 94 % 91 % 118
predicatives 97 % 93 % 72
newspaper precision recall N
subjects 95 % 83 % 136
objects 94 % 88 % 103
predicatives 92 % 96 % 23
</table>
<figureCaption confidence="0.987222">
Figure 8: Rates for main functional dependencies
</figureCaption>
<bodyText confidence="0.988741">
is used in (Eisner, 1996) except that every word has
a head, i.e. the precision equals recall, reported as
79.2%.
</bodyText>
<subsubsectionHeader confidence="0.594942">
7.3.2 Named dependencies
</subsubsectionHeader>
<bodyText confidence="0.999973411764706">
We evaluated our parser against the selected de-
pendencies in the test samples. The samples be-
ing rather small, only the most common dependen-
cies are evaluated: subject, object and predicative.
These dependencies are usually resolved more re-
liably than, say, appositions, prepositional attach-
ments etc. The results of the test samples are listed
in Figure 8. It seems the parser leaves some amount
of the words unlinked (e.g. 10-15% of subjects) but
what it has recognised is generally correct (precision
95-98% for subjects).
Dekang Lin (1996) has earlier used this kind of
evaluation, where precision and recall were for sub-
jects 87% and 78%, and for complements (includ-
ing objects) 84% and 72 %, respectively. The results
are not strictly comparable because the syntactic de-
scription is somewhat different.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9998984">
In this paper, we have presented some main features
of our new framework for dependency syntax. The
most important result is that the new framework al-
lows us to describe non-projective dependency gram-
mars and apply them efficiently. This is a property
</bodyText>
<page confidence="0.992795">
70
</page>
<bodyText confidence="0.999577730769231">
that will be crucial when we will apply this frame-
work to a language having free word-order.
Basically, the parsing framework combines the
Constraint Grammar framework (removing ambigu-
ous readings) with a mechanism that adds depen-
dencies between readings or tags. This means that
while the parser disambiguates it also builds up a
dependency forest that, in turn, is reduced by other
disambiguation rules and a global pruning mecha-
nism.
This setup makes it possible to operate on several
layers of information, and use and combine struc-
tural information more efficiently than in the orig-
inal Constraint Grammar framework, without any
further disadvantage in dealing with ambiguity.
First preliminary evaluations are presented. Com-
pared to the ENGCG syntactic analyser, the output
not only contains more information but it is also
more accurate and explicit. The ambiguity rate is
reduced to a quarter without any compromise in cor-
rectness. We did not have access to other systems,
and care must be taken when interpreting the re-
sults which are not strictly comparable. However,
the comparison to other current systems suggests
that our dependency parser is very promising both
theoretically and practically.
</bodyText>
<sectionHeader confidence="0.912545" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99669725">
We are using Atro Voutilainen&apos;s (1995) improved
part-of-speech disambiguation grammar which runs in
the CG-2 parser. Voutilainen and Juha Heikkild created
the original ENGCG lexicon.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999538328571428">
Arto Anttila. 1995. How to recognise subjects in
English. In Karlsson et al., chapt. 9, pp. 315-358.
Dekang Lin. 1996. Evaluation of Principar with the
Susanne corpus. In John Carroll, editor, Work-
shop on Robust Parsing, pages 54-69, Prague.
Jason M. Eisner. 1996. Three new probabilistic
models for dependency parsing: An exploration.
In The 16th International Conference on Compu-
tational Linguistics, pages 340-345. Copenhagen.
David G. Hays. 1964. Dependency theory: A
formalism and some observations. Language,
40(4):511-525.
Hans Jürgen Heringer. 1993. Dependency syntax -
basic ideas and the classical model. In Joachim
Jacobs, Arnim von Stechow, Wolfgang Sternefeld,
and Theo Venneman, editors, Syntax - An In-
ternational Handbook of Contemporary Research,
volume 1, chapter 12, pages 298-316. Walter de
Gruyter, Berlin - New York.
Richard Hudson. 1991. English Word Grammar.
Basil Blackwell, Cambridge, MA.
Arvi Hurskainen. 1996. Disambiguation of morpho-
logical analysis in Bantu languages. In The 16th
International Conference on Computational Lin-
guistics, pages 568-573. Copenhagen.
Timo Jarvinen. 1994. Annotating 200 million
words: the Bank of English project. In The 15th
International Conference on Computational Lin-
guistics Proceedings, pages 565-568. Kyoto.
Fred Karlsson, Atro Voutilainen, Juha Heikkila, and
Arto Anttila, editors. 1995. Constraint Gram-
mar: a language-independent system for parsing
unrestricted text, volume 4 of Natural Language
Processing. Mouton de Gruyter, Berlin and N.Y.
Fred Karlsson. 1990. Constraint grammar as a
framework for parsing running text. In Hans Karl-
gren, editor, Papers presented to the 13th Interna-
tional Conference on Computational Linguistics,
volume 3, pages 168-173, Helsinki, Finland.
Michael McCord. 1990. Slot grammar: A system for
simpler construction of practical natural language
grammars. In R Studer, editor, Natural Language
and Logic: International Scientific Symposium,
Lecture Notes in Computer Science, pages 118-
145. Springer, Berlin.
Igor A. Merthik. 1987. Dependency Syntax: Theory
and Practice. State University of New York Press,
Albany.
Christer Samuelsson, Pasi Tapanainen, and Atro
Voutilainen. 1996. Inducing constraint gram-
mars. In Laurent Miclet and Colin de la Higuera,
editors, Grammatical Inference: Learning Syntax
from Sentences, volume 1147 of Lecture Notes in
Artificial Intelligence, pages 146-155, Springer.
Daniel Sleator and Davy Temperley. 1991. Parsing
English with a link grammar. Technical Report
CMU-CS-91-196, Carnegie Mellon University.
Pasi Tapanainen and Timo Jarvinen. 1994. Syn-
tactic analysis of natural language using linguis-
tic rules and corpus-based patterns. In The 15th
International Conference on Computational Lin-
guistics Proceedings, pages 629-634. Kyoto.
Pasi Tapanainen. 1996. The Constraint Grammar
Parser CG-2. Number 27 in Publications of the
Department of General Linguistics, University of
Helsinki.
Lucien Tesniere. 1959. Elements de syntaxe struc-
turale. Editions Klincksieck, Paris.
Atro Voutilainen. 1995. Morphological disambigua-
tion. In Karlsson et al., chapter 6, pages 165-284.
</reference>
<page confidence="0.999142">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.719382">
<title confidence="0.998743">A non-projective dependency parser</title>
<author confidence="0.999965">Tapanainen Jarvinen</author>
<affiliation confidence="0.919124">University of Helsinki, Department of General Linguistics Research Unit for Multilingual Language Technology</affiliation>
<address confidence="0.999744">P.O. Box 4, FIN-00014 University of Helsinki, Finland</address>
<email confidence="0.911085">Pasi.Tapanainenoning.Helsinki.fi</email>
<email confidence="0.911085">Timo.Jarvinenoning.Helsinki.fi</email>
<abstract confidence="0.9914512">We describe a practical parser for unrestricted dependencies. The parser creates links between words and names the links according to their syntactic functions. We first describe the older Constraint Grammar parser where many of the ideas come from. Then we proceed to describe the central ideas of our new parser. Finally, the parser is evaluated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arto Anttila</author>
</authors>
<title>How to recognise subjects in English.</title>
<date>1995</date>
<journal>In Karlsson et al., chapt. 9,</journal>
<pages>315--358</pages>
<contexts>
<context position="4407" citStr="Anttila (1995)" startWordPosition="728" endWordPosition="729">ample, the following rule discards the syntactic function&apos; CI-OBJ (indirect object): REMOVE (@I-OBJ) IF (*-1C VFIN BARRIER SVO0 LINK NOT 0 SV00); The rule holds if the closest finite verb to the left is unambiguously (C) a finite verb (VFIN), and there is no ditransitive verb or participle (subcategorisation SV00) between the verb and the indirect object. If, in addition, the verb does not take indirect objects, i.e. there is no SVO0 in the same verb (LINK NOT 0 SV00), the CI-OBJ reading will be discarded. In essence, the same formalism is used in the syntactic analysis in Jarvinen (1994) and Anttila (1995). After the morphological disambiguation, all legitimate surface-syntactic labels are added to the set of morphological readings. Then, the syntactic rules discard contextually illegitimate alternatives or select legitimate ones. The syntactic tagset of the Constraint Grammar provides an underspecific dependency description. For example, labels for functional heads (such as CSUBJ, COBJ, CI-OBJ) mark the word which is a head of a noun phrase having that function in the clause, but the parent is not indicated. In addition, the representation is shallow, which means that, e.g., objects of infinit</context>
</contexts>
<marker>Anttila, 1995</marker>
<rawString>Arto Anttila. 1995. How to recognise subjects in English. In Karlsson et al., chapt. 9, pp. 315-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Evaluation of Principar with the Susanne corpus. In</title>
<date>1996</date>
<booktitle>Workshop on Robust Parsing,</booktitle>
<pages>54--69</pages>
<editor>John Carroll, editor,</editor>
<location>Prague.</location>
<contexts>
<context position="25401" citStr="Lin (1996)" startWordPosition="4290" endWordPosition="4291">ecision equals recall, reported as 79.2%. 7.3.2 Named dependencies We evaluated our parser against the selected dependencies in the test samples. The samples being rather small, only the most common dependencies are evaluated: subject, object and predicative. These dependencies are usually resolved more reliably than, say, appositions, prepositional attachments etc. The results of the test samples are listed in Figure 8. It seems the parser leaves some amount of the words unlinked (e.g. 10-15% of subjects) but what it has recognised is generally correct (precision 95-98% for subjects). Dekang Lin (1996) has earlier used this kind of evaluation, where precision and recall were for subjects 87% and 78%, and for complements (including objects) 84% and 72 %, respectively. The results are not strictly comparable because the syntactic description is somewhat different. 8 Conclusion In this paper, we have presented some main features of our new framework for dependency syntax. The most important result is that the new framework allows us to describe non-projective dependency grammars and apply them efficiently. This is a property 70 that will be crucial when we will apply this framework to a langua</context>
</contexts>
<marker>Lin, 1996</marker>
<rawString>Dekang Lin. 1996. Evaluation of Principar with the Susanne corpus. In John Carroll, editor, Workshop on Robust Parsing, pages 54-69, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In The 16th International Conference on Computational Linguistics,</booktitle>
<pages>340--345</pages>
<location>Copenhagen.</location>
<contexts>
<context position="7047" citStr="Eisner, 1996" startWordPosition="1150" endWordPosition="1151">us the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Ternperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also interested in the linear order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. This, however, is a matter of output formatting</context>
<context position="24744" citStr="Eisner, 1996" startWordPosition="4185" endWordPosition="4186"> very heuristic methods to assign heads would raise recall but lower precision. A similar measure precision recall broadcast 93.4 % 88.0 % literature 96.0 % 88.6 % newspaper 95.3 % 87.9 % Figure 7: Percentages of heads correctly attached broadcast precision recall N subjects 95 % 89 % 244 objects 89 % 83 % 140 predicatives 96 % 86 % 57 literature precision recall N subjects 98 % 92 % 195 objects 94 % 91 % 118 predicatives 97 % 93 % 72 newspaper precision recall N subjects 95 % 83 % 136 objects 94 % 88 % 103 predicatives 92 % 96 % 23 Figure 8: Rates for main functional dependencies is used in (Eisner, 1996) except that every word has a head, i.e. the precision equals recall, reported as 79.2%. 7.3.2 Named dependencies We evaluated our parser against the selected dependencies in the test samples. The samples being rather small, only the most common dependencies are evaluated: subject, object and predicative. These dependencies are usually resolved more reliably than, say, appositions, prepositional attachments etc. The results of the test samples are listed in Figure 8. It seems the parser leaves some amount of the words unlinked (e.g. 10-15% of subjects) but what it has recognised is generally c</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In The 16th International Conference on Computational Linguistics, pages 340-345. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--4</pages>
<contexts>
<context position="6815" citStr="Hays, 1964" startWordPosition="1118" endWordPosition="1119">&apos;auk (1987). 3.1 Uniqueness and projectivity In Tesniere&apos;s and Mel&apos;auk&apos;s dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Ternperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also inter</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>David G. Hays. 1964. Dependency theory: A formalism and some observations. Language, 40(4):511-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Jürgen Heringer</author>
</authors>
<title>Dependency syntax -basic ideas and the classical model.</title>
<date>1993</date>
<booktitle>Syntax - An International Handbook of Contemporary Research,</booktitle>
<volume>1</volume>
<pages>298--316</pages>
<editor>In Joachim Jacobs, Arnim von Stechow, Wolfgang Sternefeld, and Theo Venneman, editors,</editor>
<location>Berlin - New York.</location>
<contexts>
<context position="6135" citStr="Heringer, 1993" startWordPosition="1005" endWordPosition="1006">arguments of verbs, which was one of the tasks we originally were interested in. To solve the problems, we developed a more powerful rule formalism which utilises an explicit dependency representation. The basic Constraint Gram&apos;The convention in the Constraint Grammar is that the tags for syntactic functions begin with the 0-sign. mar idea of introducing the information in a piecemeal fashion is retained, but the integration of different pieces of information is more efficient in the new system. 3 Dependency grammars in a nutshell Our notation follows the classical model of dependency theory (Heringer, 1993) introduced by Lucien Tesniere (1959) and later advocated by Igor Mel&apos;auk (1987). 3.1 Uniqueness and projectivity In Tesniere&apos;s and Mel&apos;auk&apos;s dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which c</context>
</contexts>
<marker>Heringer, 1993</marker>
<rawString>Hans Jürgen Heringer. 1993. Dependency syntax -basic ideas and the classical model. In Joachim Jacobs, Arnim von Stechow, Wolfgang Sternefeld, and Theo Venneman, editors, Syntax - An International Handbook of Contemporary Research, volume 1, chapter 12, pages 298-316. Walter de Gruyter, Berlin - New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1991</date>
<publisher>Basil Blackwell,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6513" citStr="Hudson (1991)" startWordPosition="1070" endWordPosition="1071">ecemeal fashion is retained, but the integration of different pieces of information is more efficient in the new system. 3 Dependency grammars in a nutshell Our notation follows the classical model of dependency theory (Heringer, 1993) introduced by Lucien Tesniere (1959) and later advocated by Igor Mel&apos;auk (1987). 3.1 Uniqueness and projectivity In Tesniere&apos;s and Mel&apos;auk&apos;s dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Ternperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-proje</context>
<context position="8043" citStr="Hudson, 1991" startWordPosition="1313" endWordPosition="1314">near order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. This, however, is a matter of output formatting, for which the system makes several options available. 3.2 Valency and categories The verbs (as well as other elements) have a valency that describes the number and type of the modifiers they may have. In valency theory, usually, complements (obligatory) and adjuncts (optional) are distinguished. 4D is adjacent to H provided that every word between D and H is a subordinate of H (Hudson, 1991). 65 main: &lt;ROOT&gt; &lt;SAID&gt; obj: VFIN &lt;JOAN&gt; &lt;SUITS&gt; NSG subj: SGS VFIN &lt;LIKES&gt; &lt;HER&gt; SGS VFIN°b- &lt;DECIDE&gt; PRON ACC SGS subj obj: &lt;WHATEVER&gt;&lt;JOHN&gt; PRON WH NSG INF nfmark: &lt;TO&gt; INFMARK&gt; Figure 2: A dependency structure for the sentence: Joan said whatever John likes to decide suits her. Our notation makes a difference between valency (rule-based) and subcategorisation (lexical): the valency tells which arguments are expected; the subcategorisation tells which combinations are legitimate. The valency merely provides a possibility to have an argument. Thus, a verb having three valency slots may have</context>
</contexts>
<marker>Hudson, 1991</marker>
<rawString>Richard Hudson. 1991. English Word Grammar. Basil Blackwell, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvi Hurskainen</author>
</authors>
<title>Disambiguation of morphological analysis in Bantu languages.</title>
<date>1996</date>
<booktitle>In The 16th International Conference on Computational Linguistics,</booktitle>
<pages>568--573</pages>
<location>Copenhagen.</location>
<marker>Hurskainen, 1996</marker>
<rawString>Arvi Hurskainen. 1996. Disambiguation of morphological analysis in Bantu languages. In The 16th International Conference on Computational Linguistics, pages 568-573. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Jarvinen</author>
</authors>
<title>Annotating 200 million words: the Bank of English project.</title>
<date>1994</date>
<booktitle>In The 15th International Conference on Computational Linguistics Proceedings,</booktitle>
<pages>565--568</pages>
<publisher>Kyoto.</publisher>
<contexts>
<context position="4388" citStr="Jarvinen (1994)" startWordPosition="725" endWordPosition="726">keyword LINK. For example, the following rule discards the syntactic function&apos; CI-OBJ (indirect object): REMOVE (@I-OBJ) IF (*-1C VFIN BARRIER SVO0 LINK NOT 0 SV00); The rule holds if the closest finite verb to the left is unambiguously (C) a finite verb (VFIN), and there is no ditransitive verb or participle (subcategorisation SV00) between the verb and the indirect object. If, in addition, the verb does not take indirect objects, i.e. there is no SVO0 in the same verb (LINK NOT 0 SV00), the CI-OBJ reading will be discarded. In essence, the same formalism is used in the syntactic analysis in Jarvinen (1994) and Anttila (1995). After the morphological disambiguation, all legitimate surface-syntactic labels are added to the set of morphological readings. Then, the syntactic rules discard contextually illegitimate alternatives or select legitimate ones. The syntactic tagset of the Constraint Grammar provides an underspecific dependency description. For example, labels for functional heads (such as CSUBJ, COBJ, CI-OBJ) mark the word which is a head of a noun phrase having that function in the clause, but the parent is not indicated. In addition, the representation is shallow, which means that, e.g.,</context>
<context position="21122" citStr="Jarvinen, 1994" startWordPosition="3561" endWordPosition="3562">rmation is not printed here. reading from each word belongs to a subtree of which the root is said or suits. 7. The syntactic relationship between the verbs is established by a rule stating that the rightmost main verb is the (clause) object of a main verb to the left, which allows such objects. 8. Finally, there is a single main verb, which is indexed to the root (&lt;s&gt;) (in position 00). 7 Evaluation 7.1 Efficiency The evaluation was done using small excerpts of data, not used in the development of the system. All text samples were excerpted from three different genres in the Bank of English (Jarvinen, 1994) data: American National Public Radio (broadcast), British Books data (literature), and The Independent (newspaper). Figure 5 lists the samples, their sizes, and the average and maximum sentence lengths. The measure is in words excluding punctuation. size avg. max. total (w) length length time 2281 19 44 12 sec. 1920 15 51 8.5 sec. 1427 19 47 8.5 sec. Figure 5: Benchmark used in the evaluation In addition, Figure 5 shows the total processing time required for the syntactic analysis of the samples. The syntactic analysis has been done in a normal PC with the Linux operating system. The PC has a</context>
<context position="23120" citStr="Jarvinen, 1994" startWordPosition="3896" endWordPosition="3897">GCG system for processing the input. These include the tokeniser, lexical analyser and morphological disambiguator. Figure 6 shows the results of the comparison of the ENGCG syntax and the morphosyntactic level of the dependency grammar. Because both systems leave some amount of the ambiguity pending, two figures are given: the success rate, which is the percentage of correct morphosyntactic labels present in the output, and the ambiguity rate, which is the percentage of words containing more than one label. The ENGCG results compare to those reported elsewhere (Jirvinen, 1994; Tapanainen and Jarvinen, 1994). The DG success rate is similar or maybe even slightly better than in ENGCG. More importantly, the ambiguity rate is only about a quarter of that in the ENGCG output. The overall result should be considered good in the sense that the output contains information about the syntactic functions (see Figure 4) not only part-of-speech tags. 7.3 Dependencies The major improvement over ENGCG is the level of explicit dependency representation, which makes it possible to excerpt modifiers of certain elements, such as arguments of verbs. This section evaluates the success of the level of dependencies. 7</context>
</contexts>
<marker>Jarvinen, 1994</marker>
<rawString>Timo Jarvinen. 1994. Annotating 200 million words: the Bank of English project. In The 15th International Conference on Computational Linguistics Proceedings, pages 565-568. Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Atro Voutilainen, Juha Heikkila, and Arto Anttila,</title>
<date>1995</date>
<booktitle>of Natural Language Processing. Mouton de Gruyter,</booktitle>
<volume>4</volume>
<location>editors.</location>
<marker>Karlsson, 1995</marker>
<rawString>Fred Karlsson, Atro Voutilainen, Juha Heikkila, and Arto Anttila, editors. 1995. Constraint Grammar: a language-independent system for parsing unrestricted text, volume 4 of Natural Language Processing. Mouton de Gruyter, Berlin and N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Constraint grammar as a framework for parsing running text.</title>
<date>1990</date>
<booktitle>Papers presented to the 13th International Conference on Computational Linguistics,</booktitle>
<volume>3</volume>
<pages>168--173</pages>
<editor>In Hans Karlgren, editor,</editor>
<location>Helsinki, Finland.</location>
<contexts>
<context position="2107" citStr="Karlsson (1990)" startWordPosition="337" endWordPosition="338"> systems. New formalism is described briefly, and it is utilised in a small toy grammar to illustrate how the formalism works. Finally, the real parsing system, with a grammar of some 2 500 rules, is evaluated. The parser corresponds to over three man-years of work, which does not include the lexical analyser and the morphological disambiguator, both parts of the existing English Constraint Grammar parser (Karlsson et al., 1995). The parsers can be tested via WWW&apos;. 2 Background Our work is partly based on the work done with the Constraint Grammar framework that was originally proposed by Fred Karlsson (1990). A detailed description of the English Constraint Grammar (ENGCG) is in Karlsson et al. (1995). The basic rule types of the Constraint Grammar (Tapanainen, 1996)2 are REMOVE and SELECT for discarding and selecting an alternative reading of a word. Rules also have contextual tests that describe the condition according to which they may be applied. For example, the rule REMOVE (V) IF (-1C DET); discards a verb (V) reading if the preceding word (-1) is unambiguously (C) a determiner (DET). More than one such test can be appended to a rule. The rule above represents a local rule: the test checks </context>
</contexts>
<marker>Karlsson, 1990</marker>
<rawString>Fred Karlsson. 1990. Constraint grammar as a framework for parsing running text. In Hans Karlgren, editor, Papers presented to the 13th International Conference on Computational Linguistics, volume 3, pages 168-173, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael McCord</author>
</authors>
<title>Slot grammar: A system for simpler construction of practical natural language grammars. In</title>
<date>1990</date>
<booktitle>Natural Language and Logic: International Scientific Symposium, Lecture Notes in Computer Science,</booktitle>
<pages>118--145</pages>
<editor>R Studer, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="7002" citStr="McCord, 1990" startWordPosition="1143" endWordPosition="1144">se and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. Some early formalisations, c.f. (Hays, 1964), have brought the strict projectivity (context-free) requirement into the dependency framework. This kind of restriction is present in many dependency-based parsing systems (McCord, 1990; Sleator and Ternperley, 1991; Eisner, 1996). But obviously any recognition grammar should deal with non-projective phenomena to the extent they occur in natural languages as, for example, in the analysis shown in Figure 2. Our system has no in-built restrictions concerning projectivity, though the formalism allows us to state when crossing links are not permitted. We maintain that one is generally also interested in the linear order of elements, and therefore it is presented in the tree diagrams. But, for some purposes, presenting all arguments in a canonical order might be more adequate. Th</context>
</contexts>
<marker>McCord, 1990</marker>
<rawString>Michael McCord. 1990. Slot grammar: A system for simpler construction of practical natural language grammars. In R Studer, editor, Natural Language and Logic: International Scientific Symposium, Lecture Notes in Computer Science, pages 118-145. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Merthik</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1987</date>
<publisher>State University of New York Press,</publisher>
<location>Albany.</location>
<marker>Merthik, 1987</marker>
<rawString>Igor A. Merthik. 1987. Dependency Syntax: Theory and Practice. State University of New York Press, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Pasi Tapanainen, and Atro Voutilainen.</title>
<date>1996</date>
<booktitle>In Laurent Miclet and Colin de la Higuera, editors, Grammatical Inference: Learning Syntax from Sentences,</booktitle>
<volume>1147</volume>
<pages>146--155</pages>
<publisher>Springer.</publisher>
<marker>Samuelsson, 1996</marker>
<rawString>Christer Samuelsson, Pasi Tapanainen, and Atro Voutilainen. 1996. Inducing constraint grammars. In Laurent Miclet and Colin de la Higuera, editors, Grammatical Inference: Learning Syntax from Sentences, volume 1147 of Lecture Notes in Artificial Intelligence, pages 146-155, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel Sleator and Davy Temperley. 1991. Parsing English with a link grammar. Technical Report CMU-CS-91-196, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Jarvinen</author>
</authors>
<title>Syntactic analysis of natural language using linguistic rules and corpus-based patterns.</title>
<date>1994</date>
<booktitle>In The 15th International Conference on Computational Linguistics Proceedings,</booktitle>
<pages>629--634</pages>
<publisher>Kyoto.</publisher>
<contexts>
<context position="23120" citStr="Tapanainen and Jarvinen, 1994" startWordPosition="3894" endWordPosition="3897">parts of the ENGCG system for processing the input. These include the tokeniser, lexical analyser and morphological disambiguator. Figure 6 shows the results of the comparison of the ENGCG syntax and the morphosyntactic level of the dependency grammar. Because both systems leave some amount of the ambiguity pending, two figures are given: the success rate, which is the percentage of correct morphosyntactic labels present in the output, and the ambiguity rate, which is the percentage of words containing more than one label. The ENGCG results compare to those reported elsewhere (Jirvinen, 1994; Tapanainen and Jarvinen, 1994). The DG success rate is similar or maybe even slightly better than in ENGCG. More importantly, the ambiguity rate is only about a quarter of that in the ENGCG output. The overall result should be considered good in the sense that the output contains information about the syntactic functions (see Figure 4) not only part-of-speech tags. 7.3 Dependencies The major improvement over ENGCG is the level of explicit dependency representation, which makes it possible to excerpt modifiers of certain elements, such as arguments of verbs. This section evaluates the success of the level of dependencies. 7</context>
</contexts>
<marker>Tapanainen, Jarvinen, 1994</marker>
<rawString>Pasi Tapanainen and Timo Jarvinen. 1994. Syntactic analysis of natural language using linguistic rules and corpus-based patterns. In The 15th International Conference on Computational Linguistics Proceedings, pages 629-634. Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
</authors>
<title>The Constraint Grammar Parser CG-2.</title>
<date>1996</date>
<journal>Number</journal>
<volume>27</volume>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<note>in Publications of the</note>
<contexts>
<context position="2269" citStr="Tapanainen, 1996" startWordPosition="364" endWordPosition="365"> with a grammar of some 2 500 rules, is evaluated. The parser corresponds to over three man-years of work, which does not include the lexical analyser and the morphological disambiguator, both parts of the existing English Constraint Grammar parser (Karlsson et al., 1995). The parsers can be tested via WWW&apos;. 2 Background Our work is partly based on the work done with the Constraint Grammar framework that was originally proposed by Fred Karlsson (1990). A detailed description of the English Constraint Grammar (ENGCG) is in Karlsson et al. (1995). The basic rule types of the Constraint Grammar (Tapanainen, 1996)2 are REMOVE and SELECT for discarding and selecting an alternative reading of a word. Rules also have contextual tests that describe the condition according to which they may be applied. For example, the rule REMOVE (V) IF (-1C DET); discards a verb (V) reading if the preceding word (-1) is unambiguously (C) a determiner (DET). More than one such test can be appended to a rule. The rule above represents a local rule: the test checks only neighbouring words in a foreknown position before or after the target word. The test may also refer to the positions somewhere in the sentence without specif</context>
<context position="21929" citStr="Tapanainen, 1996" startWordPosition="3698" endWordPosition="3699">ence lengths. The measure is in words excluding punctuation. size avg. max. total (w) length length time 2281 19 44 12 sec. 1920 15 51 8.5 sec. 1427 19 47 8.5 sec. Figure 5: Benchmark used in the evaluation In addition, Figure 5 shows the total processing time required for the syntactic analysis of the samples. The syntactic analysis has been done in a normal PC with the Linux operating system. The PC has a Pentium 90 MHz processor and 16 MB of memory. The speed roughly corresponds to 200 words in second. The time does not include morphological analysis and disambiguation6. 6The CG-2 program (Tapanainen, 1996) runs a modified disambiguation grammar of Voutilainen (1995) about 1000 words in second. broadcast literature newspaper 69 DG ENGCG succ. amb. succ. amb. broadcast 97.0 % 3.2 % 96.8 % 12.7 % literature 97.3 % 3.3 % 95.9% 11.3% newspaper 96.4 % 3.3 % 94.2 % 13.7 % Figure 6: ENGCG syntax and morphosyntactic level of the dependency grammar 7.2 Comparison to ENGCG syntax One obvious point of reference is the ENGCG syntax, which shares a level of similar representation with an almost identical tagset to the new system. In addition, both systems use the front parts of the ENGCG system for processin</context>
</contexts>
<marker>Tapanainen, 1996</marker>
<rawString>Pasi Tapanainen. 1996. The Constraint Grammar Parser CG-2. Number 27 in Publications of the Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesniere</author>
</authors>
<title>Elements de syntaxe structurale. Editions Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<contexts>
<context position="6172" citStr="Tesniere (1959)" startWordPosition="1010" endWordPosition="1011">the tasks we originally were interested in. To solve the problems, we developed a more powerful rule formalism which utilises an explicit dependency representation. The basic Constraint Gram&apos;The convention in the Constraint Grammar is that the tags for syntactic functions begin with the 0-sign. mar idea of introducing the information in a piecemeal fashion is retained, but the integration of different pieces of information is more efficient in the new system. 3 Dependency grammars in a nutshell Our notation follows the classical model of dependency theory (Heringer, 1993) introduced by Lucien Tesniere (1959) and later advocated by Igor Mel&apos;auk (1987). 3.1 Uniqueness and projectivity In Tesniere&apos;s and Mel&apos;auk&apos;s dependency notation every element of the dependency tree has a unique head. The verb serves as the head of a clause and the top element of the sentence is thus the main verb of the main clause. In some other theories, e.g. Hudson (1991), several heads are allowed. Projectivity (or adjacency4) was not an issue for Tesniere (1959, ch. 10), because he thought that the linear order of the words does not belong to the syntactic level of representation which comprises the structural order only. S</context>
</contexts>
<marker>Tesniere, 1959</marker>
<rawString>Lucien Tesniere. 1959. Elements de syntaxe structurale. Editions Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>Morphological disambiguation.</title>
<date>1995</date>
<booktitle>In Karlsson et al., chapter 6,</booktitle>
<pages>165--284</pages>
<contexts>
<context position="21990" citStr="Voutilainen (1995)" startWordPosition="3707" endWordPosition="3708"> size avg. max. total (w) length length time 2281 19 44 12 sec. 1920 15 51 8.5 sec. 1427 19 47 8.5 sec. Figure 5: Benchmark used in the evaluation In addition, Figure 5 shows the total processing time required for the syntactic analysis of the samples. The syntactic analysis has been done in a normal PC with the Linux operating system. The PC has a Pentium 90 MHz processor and 16 MB of memory. The speed roughly corresponds to 200 words in second. The time does not include morphological analysis and disambiguation6. 6The CG-2 program (Tapanainen, 1996) runs a modified disambiguation grammar of Voutilainen (1995) about 1000 words in second. broadcast literature newspaper 69 DG ENGCG succ. amb. succ. amb. broadcast 97.0 % 3.2 % 96.8 % 12.7 % literature 97.3 % 3.3 % 95.9% 11.3% newspaper 96.4 % 3.3 % 94.2 % 13.7 % Figure 6: ENGCG syntax and morphosyntactic level of the dependency grammar 7.2 Comparison to ENGCG syntax One obvious point of reference is the ENGCG syntax, which shares a level of similar representation with an almost identical tagset to the new system. In addition, both systems use the front parts of the ENGCG system for processing the input. These include the tokeniser, lexical analyser an</context>
</contexts>
<marker>Voutilainen, 1995</marker>
<rawString>Atro Voutilainen. 1995. Morphological disambiguation. In Karlsson et al., chapter 6, pages 165-284.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>