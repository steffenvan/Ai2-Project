<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.572840571428571">
LR RECURSIVE TRANSITION NETWORKS
FOR EARLEY AND TOMITA PARSING
Mark Perlin
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
Internet: perlin@cs.cmu.edu
</note>
<sectionHeader confidence="0.867171" genericHeader="abstract">
ABSTRACT*
</sectionHeader>
<bodyText confidence="0.965994416666667">
Efficient syntactic and semantic parsing for
ambiguous context-free languages are generally
characterized as complex, specialized, highly formal
algorithms. In fact, they are readily constructed from
straightforward recursive transition networks (RTNs).
In this paper, we introduce LR-RTNs, and then
computationally motivate a uniform progression from
basic LR parsing, to Earley&apos;s (chart) parsing,
concluding with Tomita&apos;s parser. These apparently
disparate algorithms are unified into a single
implementation, which was used to automatically
generate all the figures in this paper.
</bodyText>
<sectionHeader confidence="0.99906" genericHeader="keywords">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.99896352631579">
Ambiguous context-free grammars (CFGs) are
currently used in the syntactic and semantic
processing of natural language. For efficient parsing,
two major computational methods are used. The rust
is Earley&apos;s algorithm (Earley, 1970), which merges
parse trees to reduce the computational dependence
on input sentence length from exponential to cubic
cost. Numerous variations on Earley&apos;s dynamic
programming method have developed into a family of
chart parsing (Winograd, 1983) algorithms. The
second is Tomita&apos;s algorithm (Tomita, 1986), which
generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s
(DeRemer, 1971) computer language LR parsing
techniques. Tomita&apos;s algorithm augments the LR
parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s
ideas.
What is not currently appreciated is the continuity
between these apparently distinct computational
methods.
</bodyText>
<listItem confidence="0.992828222222222">
• Tomita has proposed (Tomita, 1985) constructing
his algorithm from Earley&apos;s parser, instead of
DeRemer&apos;s LR parser. In fact, as we shall show,
Earley&apos;s algorithm may be viewed as one form
of LR parsing.
• Incremental constructions of Tomita&apos;s algorithm
(Heering, Klint, and Rekers, 1990) may
similarly be viewed as just one point along a
continuum of methods.
</listItem>
<bodyText confidence="0.879923470588235">
* This work was supported in part by grant R29
LM 04707 from the National Library of Medicine,
and by the Pittsburgh NMR Institute.
The apparent distinctions between these related
methods follows from the distinct complex formal
and mathematical apparati (Lang, 1974; Lang, 1991)
currently employed to construct these CF parsing
algorithms.
To effect a uniform synthesis of these methods, in
this paper we introduce LR Recursive Transition
Networks (LR-RTNs) as a simpler framework on
which to build CF parsing algorithms. While RTNs
(Woods, 1970) have been widely used in Artificial
Intelligence (AI) for natural language parsing, their
representational advantages have not been fully
exploited for efficiency. The LR-RTNs, however, are
efficient, and shall be used to construct:
</bodyText>
<listItem confidence="0.899510619047619">
(1) a nondeterministic parser,
(2) a basic LR(0) parser,
(3) Earley&apos;s algorithm (and the chart parsers), and
(4) incremental and compiled versions of Tomita&apos;s
algorithm.
Our uniform construction has advantages over the
current highly formal, non-RTN-based, nonuniform
approaches to CF parsing:
• Clarity of algorithm construction, permitting LR,
Earley, and Tomita parsers to be understood as
a family of related parsing algorithm.
• Computational motivation and justification for
each algorithm in this family.
• Uniform extensibility of these syntactic methods
to semantic parsing.
• Shared graphical representations, useful in
building interactive programming environments
for computational linguists.
• Parallelization of these parsing algorithms.
• All of the known advantages of RTNs, together
with efficiencies of LR parsing.
</listItem>
<bodyText confidence="0.8244965">
All of these improvements will be discussed in the
paper.
</bodyText>
<sectionHeader confidence="0.997814" genericHeader="introduction">
2. LR RECURSIVE TRANSITION
NETWORKS
</sectionHeader>
<bodyText confidence="0.990313285714286">
A transition network is a directed graph, used as a
finite state machine (Hoperoft and Ullman, 1979).
The network&apos;s nodes or edges are labelled; in this
paper, we shall label the nodes. When an input
sentence is read, state moves from node to node. A
sentence is accepted if reading the entire sentence
directs the network traversal so as to arrive at an
</bodyText>
<page confidence="0.997933">
98
</page>
<figure confidence="0.990967428571429">
Nonterminal
symbol S
410 Rule #1
Start symbol S
Symbols NP d VP
M
(A) (B) (C) (D) (E)
</figure>
<figureCaption confidence="0.998668">
Figure 1. Expanding Rule#1: S—&gt;NP VP. A. Expanding the nonterminal symbol S. B. Expanding the rule node for
Rule #1. C. Expanding the symbol node VP. D. Expanding the symbol node NP. E. Expanding the start node S.
</figureCaption>
<bodyText confidence="0.98017142">
accepting node. To increase computational power
from regular languages to context-free languages,
recursive transition networks (RTNs) are introduced.
An RTN is a forest of disconnected transition
networks, each identified by a nonterminal label. All
other labels are terminal labels. When, in traversing
a transition network, a nonterminal label is
encountered, control recursively passes to the
beginning of the correspondingly labelled transition
network. Should this labelled network be
successfully traversed, on exit, control returns back to
the labelled calling node.
The linear text of a context-free grammar can be
cast into an RTN structure (Perlin, 1989). This is
done by expanding each grammar rule into a linear
chain. The top-down expansion amounts to a partial
evaluation (Futamura, 1971) of the rule into a
computational expectation: an eventual bottom-up
data-directed instantiation that will complete the
expansion.
Figure 1, for example, shows the expansion of the
grammar rule #1 S--*NP VP. First, the nonterminal
S, which labels this connected component, is
expanded as a nonterminal node. One method for
realizing this nonterminal node, is via Rule#1; its rule
node is therefore expanded. Rule#1 sets up the
expectation for the VP symbol node, which in turn
sets up the expectation for the NP symbol node. NP,
the first symbol node in the chain, creates the start
node S. In subsequent processing, posting an
instance of this start symbol would indicate an
expectation to instantiate the entire chain of Rule#1,
thereby detecting a nonterminal symbol S. Partial
instantiation of the Rule&apos;s chain indicates the partial
progress in sequencing the Rule&apos;s right-hand-side
symbols.
The expansion in Figure 1 constructs an LR-RTN.
That is, it sets up a Left-to-right parse of a Rightmost
derivation. Such derivations are developed in the
next Section. As used in AI natural language parsing,
RTNs have more typically been LL-RTNs, for
effecting parses of leftmost derivations (Woods,
1970), as shown in Figure 2A. (Other, more efficient,
control structures have also been used (Kaplan,
1973).) Our shift from LL to LR, shown in Figure
2B, uses the chain expansion to set up a subsequent
data-driven completion, thereby permitting greater
parsing efficiency.
In Figure 3, we show the RTN expansion of the
simple grammar used in our first set of examples:
</bodyText>
<equation confidence="0.984757333333333">
S --&gt; NP VP
NP --&gt; NI DN
VP ---&gt; V NP .
</equation>
<bodyText confidence="0.999209461538462">
Chains that share identical prefixes are merged
(Perlin, 1989) into a directed acyclic graph (DAG)
(Aho, Hoperoft, and Ullman, 1983). This makes our
RTN a forest of DAGs, rather than trees. For
example, the shared NP start node initiates the chains
for Rules #2 and #3 in the NP component.
In augmented recursive transition networks
(ATNs) (Woods, 1970), semantic constraints may be
expressed. These constraints can employ case
grammars, functional grammars, unification, and so
on (Winograd, 1983). In our RTN formulation,
semantic testing occurs when instantiating rule nodes:
failing a constraint removes a parse from further
</bodyText>
<figure confidence="0.877931">
(A) (B)
</figure>
<figureCaption confidence="0.999553">
Figure 2. A. An LL-RTN for S---&gt;NP VP. This expansion does not set up an expectation for a data-driven leftward
parse. B. The corresponding LR-RTN. The rightmost expansion sets up subsequent data-driven leftward parses.
</figureCaption>
<page confidence="0.983674">
99
</page>
<bodyText confidence="0.881352666666667">
processing. This approach applies to every parsing
algorithm in this paper, and will not be discussed
further.
</bodyText>
<figureCaption confidence="0.6375075">
Figure 3. The RTN of an entire grammar. The three
connected components correspond to the three
</figureCaption>
<bodyText confidence="0.938046">
nonterminals in the grammar. Each symbol node in
the RTN denotes a subsequence originating from its
leftmost start symbol.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="method">
3. NONDETERMINISTIC DERIVATIONS
</sectionHeader>
<bodyText confidence="0.9973242">
A grammar&apos;s RTN can be used as a template for
parsing. A sentence (the data) directs the
instantiation of individual rule chains into a parse
tree. The RTN instances exactly correspond to parse-
tree nodes. This is most easily seen with
nondeterministic rightmost derivations.
Given an input sentence of n words, we may
derive a sentence in the language with the
nondeterministic algorithm (Perlin, 1990):
Put an instance of nonterminal
node S into the last column.
From right to left, for every
column:
From top to bottom, within the
column:
</bodyText>
<listItem confidence="0.665134333333333">
(1) Recursively expand the
column top-down by
nondeterministic selection of
rule instances.
(2) Install the next (leftward)
symbol instance.
</listItem>
<bodyText confidence="0.914202782608696">
In substep (1), following selection, a rule node and its
immediately downward symbol node are instantiated.
The instantiation process creates a new object that
inherits from the template RTN node, adding
information about column position and local link
connections.
For example, to derive &amp;quot;I Saw A Man&amp;quot; we would
nondeterministically select and instantiate the correct
rule choices #1, #4, #2, and #3, as in Figure 4.
Following the algorithm, the derivation is (two
dimensionally) top-down: top-to-bottom and right-to-
left. To actually use this nondeterministic derivation
algorithm to obtain all parses, one might enumerate
and test all possible sequences of rules. This,
however, has exponential cost in n, the input size. A
more efficient approach is to reverse the top-down
derivation, and recursively generate the parse(s)
bottom-up from the input data.
Figure 4. The completed top-down derivation (parse-
tree) of &amp;quot;I Saw A Man&amp;quot;. Each parse-tree symbol
node denotes a subsequence of a recognized RTN
chain. Rule #0 connects a word to its terminal
symbol(s).
</bodyText>
<sectionHeader confidence="0.922838" genericHeader="method">
4. BASIC LR(0) PARSING
</sectionHeader>
<bodyText confidence="0.999525333333333">
To construct a parser, we reverse the above top-
down nondeterministic derivation technique into a
bottom-up deterministic algorithm. We first build an
inefficient LR-parser, illustrating the reversal. For
efficiency, we then introduce the Follow-Set, and
modify our parser accordingly.
</bodyText>
<subsectionHeader confidence="0.974589">
4.1 AN INEFFICIENT BLR(0) PARSER
</subsectionHeader>
<bodyText confidence="0.944148125">
A simple, inefficient parsing algorithm for
computing all possible parse-trees is:
Put an instance of start node S
into the 0 column.
From left to right, for every
column:
From bottom to top, within the
column:
</bodyText>
<listItem confidence="0.9926018">
(1) Initialize the column with
the input word.
(2) Recursively complete the
column bottom-up using the
INSERT method.
</listItem>
<bodyText confidence="0.95016875">
This reverses the derivation algorithm into bottom-up
generation: bottom-to-top, and left-to-right. In the
inner loop, the Step (1) initialization is
straightforward; we elaborate Step (2).
</bodyText>
<page confidence="0.954234">
100
</page>
<bodyText confidence="0.72576">
Step (2) uses the following method (Perlin, 1991)
to insert instances of RTN nodes:
INSERT (instance)
ASK instance
</bodyText>
<listItem confidence="0.9933672">
(1) Link up with predecessor
instances.
(2) Install self.
(3) ENQUEUE successor instances
for insertion. }
</listItem>
<bodyText confidence="0.984694947368421">
In (1), links are constructed between the instance and
its predecessor instances. In (2), the instance
becomes available for cartesian product formation.
In (3), the computationally nontrivial step, the
instance enqueues any successor instances within its
own column. Most of the INSERT action is done by
instances of symbol and rule RTN nodes.
Using our INSERT method, a new symbol
instance in the parse-tree links with predecessor
instances, and installs itself. If the symbol&apos;s RTN
node leads upwards to a rule node, one new rule
instance successor is enqueued; otherwise, not.
Rule instances enqueue their successors in a more
complicated way, and may require cartesian product
formation. A rule instance must instantiate and
enqueue all RTN symbol nodes from which they
could possibly be derived. At most, this is the set
SAME-LABEL(rule) =
{ N RTN I N is a symbol node, and
the label of N is identical to the label of the
rule&apos;s nonterminal successor node 1.
For every symbol node in SAME-LABEL(rule),
instances may be enqueued. If X E SAME-
LABEL(rule) immediately follows a start node, i.e., it
begins a chain, then a single instance of it is
enqueued.
If Y E SAME-LABEL(rule) does not immediately
follow a start node, then more effort is required. Let
X be the unique RTN node to the left of Y. Every
instantiated node in the parse tree is the root of some
subtree that spans an interval of the input sentence.
Let the left border j be the position just to left of this
interval, and k be the rightmost position, i.e., the
current column.
Then, as shown in Figure 5, for every instance x
of X currently in position j, an instance y (of Y) is a
valid extension of subsequence x that has support
from the input sentence data. The cartesian product
</bodyText>
<equation confidence="0.865068">
( x I x an instance of X in column j 1
</equation>
<bodyText confidence="0.989747">
x rule instance)
forms the set of all valid predecessor pairs for new
instances of Y. Each such new instance y of Y is
enqueued, with some x and the rule instance as its
two predecessors. Each y is a parse-tree node
representing further progress in parsing a
subsequence.
</bodyText>
<figure confidence="0.942410375">
RTN chain - X - -
x
x
XI
Ys
&amp;quot;
position
i i i&amp;quot; j
</figure>
<figureCaption confidence="0.9772535">
Figure 5. The symbol node Y has a left neighbor
symbol node X in the WIN. The instance y of Y is
</figureCaption>
<bodyText confidence="0.86437225">
the root of a parse-subtree that spans (j+1,k).
Therefore, the rule instance r enqueues (at least) all
instances of y, indexed by the predecessor product:
{x in column j} x {r}.
</bodyText>
<sectionHeader confidence="0.511879" genericHeader="method">
4.2. USING THE FOLLOW-SET
</sectionHeader>
<bodyText confidence="0.9866965625">
Although a rule parse-node is restricted to
enqueue successor instances of WIN nodes in SAME-
LABEL(rule), it can be constrained further.
Specifically, if the sentence data gives no evidence
for a parse-subtree, the associated symbol node
instance need never be generated. This restriction
can be determined column-by-column as the parsing
progresses.
We therefore extend our bottom-up parsing
algorithm to:
Put an instance of start node S
into the 0 column.
From left to right, for every
column:
From bottom to top, within the
column:
</bodyText>
<listItem confidence="0.999838857142857">
(1) Initialize the column with
the input word.
(2) Recursively complete the
column bottom-up using the
INSERT method.
(3) Compute the column&apos;s
(rightward) Follow-Set.
</listItem>
<bodyText confidence="0.864717">
With the addition of Step (3), this defmes our Basic
LR(0), or BLR(0), parser. We now describe the
Follow-Set.
Once an RTN node X has been instantiated in
some column, it sets up an expectation for
</bodyText>
<listItem confidence="0.84806175">
• The RTN node(s) Yg that immediately follow it;
• For each immediate follower Yg, all those RTN
symbol nodes Wg,h that initiate chains that
could recursively lead up to Yg.
</listItem>
<bodyText confidence="0.978661">
This is the Follow-Set (Aho, Sethi, and Ullman,
1986). The Follow-Set(X) is computed directly from
the RTN by the recursion:
</bodyText>
<page confidence="0.991255">
101
</page>
<figure confidence="0.989167818181818">
Follow-Set(X)
LET Result f-
For every unvisited RTN node Y
following X:
Result ( Y }
IF Y&apos;s label is a terminal
symbol,
THEN 0;
ELSE Follow-Set of the
start symbol of Y&apos;s label
Return Result
</figure>
<figureCaption confidence="0.981548083333333">
As is clear from the recursive defmition,
Follow-Set (ug Xg }) = ug Follow-Set (Xg).
Therefore, the Follow-Set of a column&apos;s symbol
nodes can be deferred to Step (3) of the BLR(0)
parsing algorithm, after the determination of all the
nodes has completed. By only recursing on unvisited
nodes, this traversal of the grammar RTN has time
cost 0(IGI) (Aho, Sethi, and Ullman, 1986), where IGI
!RINI is the size of the grammar (or its RTN
graph). A Follow-Set computation is illustrated in
Figure 6.
Figure 6. The Follow-Set (highlighted in the
</figureCaption>
<bodyText confidence="0.989174444444445">
display) of RTN node V consists of the immediately
following nonterminal node NP, and the two nodes
immediately following the start NP node, D and N.
Since D and N are terminal symbols, the traversal
halts.
The set of symbol RTN nodes that a rule instance
r spanning (j+1,k) can enqueue is therefore not
SAME-LABEL(rule),
but the possibly smaller set of RTN nodes
SAME-LABEL(rule) r Follow-Set(j).
To enqueue es successors in INSERT,
LET Nodes = SAME-LABEL(rule) n
Follow-Set(j).
For every RTN node Y in Nodes,
create and enqueue all instances
y in Y:
Let X be the leftward RTN symbol
node neighbor of Y.
Let PROD =
(x I x an instance of X in
column j) x Cr), if X exists;
{r}, otherwise.
Enqueue all members of PROD as
instances of y.
The cartesian product PROD is nonempty, since an
instantiated rule anticipates those elements of PROD
mandated by Follow-Sets of preceding columns. The
pruning of Nodes by the Follow-Set eliminates all
bottom-up parsing that cannot lead to a parse-subtree
at column k.
In the example in Figure 7, Rule instance r is in
position 4, with j=3 and k=4. We have:
SAME-LABEL(r) = IN2, N31,
i.e, the two symbol nodes labelled N in the
sequences of Rules #2 and #3, shown in the
LR-RTN of Figure 6.
</bodyText>
<equation confidence="0.979360666666667">
Follow-Set(3) = Follow-Set({ D2 } )
= {N2}.
Therefore, SAME-LABEL(r)nFollow-Set(3) = (N2).
</equation>
<figureCaption confidence="0.955398333333333">
Figure 7. The rule instance r can only instantiate the
single successor instance N2. r uses the RTN to find
the left RTN neighbor D of N2. r then computes the
</figureCaption>
<bodyText confidence="0.9165705">
cartesian product of instance d with r as {d}x{r} ,
generating the successor instance of N2 shown.
</bodyText>
<sectionHeader confidence="0.998469" genericHeader="method">
5. EARLEY&apos;S PARSING ALGORITHM
</sectionHeader>
<bodyText confidence="0.9918648">
Natural languages such as English are ambiguous.
A single sentence may have multiple syntactic
structures. For example, extending our simple
grammar with rules accounting for Prepositions and
Prepositional-Phrases (Tomita, 1986)
</bodyText>
<equation confidence="0.995423666666667">
S -+ S PP
NP -+ NP PP
PP P NP,
</equation>
<bodyText confidence="0.998580833333333">
the sentence &amp;quot;I saw a man on the hill with a telescope
through the window&amp;quot; has 14 valid derivations. In
parsing, separate reconstructions of these different
parses can lead to exponential cost.
For parsing efficiency, partially constructed
instance-trees can be merged (Earley, 1970). As
before, parse-node x denotes a point along a parse-
sequence, say, v-w-x. The left-border i of this parse-
sequence is the left-border of the leftmost parse-node
in the sequence. All parse-sequences of RTN symbol
node X that cover columns i+1 through k may be
collected into a single equivalence class X(i,k). For
</bodyText>
<page confidence="0.997529">
102
</page>
<bodyText confidence="0.997957">
the purposes of (1) continuing with the parse and (2)
disambiguating parse-trees, members of X(i,k) are
indistinguishable. Over an input sentence of length n,
there are therefore no more than 0(n2) equivalence
classes of X.
of algorithms that couple efficient parse-tree merging
with various control organizations (Winograd, 1983).
</bodyText>
<sectionHeader confidence="0.877925" genericHeader="method">
6. TOMITA&apos;S PARSING ALGORITHM
</sectionHeader>
<bodyText confidence="0.996544888888889">
Suppose X precedes Y in the RTN. When an
instance y of Y is added to position k, k5n, and the
cartesian product is formed, there are only 0(k2)
possible equivalence classes of X for y to combine
with. Summing over all n positions, there are no
more than 0(n3) possible product formations with Y
in parsing an entire sentence.
Merging is effected by adding a MERGE step to
INSERT:
</bodyText>
<equation confidence="0.432812333333333">
INSERT (instance)
instance ÷- MERGE(instance)
ASK instance
</equation>
<listItem confidence="0.9929926">
(1) Link up with predecessor
instances.
(2) Install self.
(3) ENQUEUE successor instances
for insertion. }
</listItem>
<bodyText confidence="0.989124">
The parsing merge predicate considers two
instantiated sequences equivalent when:
</bodyText>
<listItem confidence="0.999352666666667">
(1) Their RTN symbol nodes X are the same.
(2) They are in the same column k.
(3) They have identical left borders i.
</listItem>
<bodyText confidence="0.99956762962963">
The total number of links formed by INSERT during
an entire parse, accounting for every grammar RTN
node, is 0(n3)x0(1G1). The chart parsers are a family
In our BLR(0) parsing algorithm, even with
merging, the Follow-Set is computed at every
column. While this computation is just 0(101), it can
become a bottleneck with the very large grammars
used in machine translation. By caching the requisite
Follow-Set computations into a graph, subsequent
Follow-Set computation is reduced. This incremental
construction is similar to (Heering, Klint, and Rekers,
1990)&apos;s, asymptotically constructing Tomita&apos;s all-
paths LR parsing algorithm (Tomita, 1986).
The Follow-Set cache (or LR-table) can be
dynamically constructed by Call-Graph Caching
(Perlin, 1989) during the parsing. Every time a
Follow-Set computation is required, it is looked up in
the cache. When not present, the Follow-Set is
computed and cached as a graph.
Following DeRemer (DeRemer, 1971), each
cached Follow-Set node is finely partitioned, as
needed, into disjoint subsets indexed by the RTN
label name, as shown in the graphs of Figure 8. The
partitioning reduces the cache size: instead of
allowing all possible subsets of the RTN, the cache
graph nodes contain smaller subsets of identically
labelled symbol nodes.
</bodyText>
<figure confidence="0.903640833333333">
When a Follow-Set node has the same subset of
—6
P— 5
3 4 N
S — 6
P— 2 VIP P-- 5
</figure>
<figureCaption confidence="0.7843865">
Figure 8. (A) A parse of &amp;quot;I Saw A Man&amp;quot; using the grammar in (Tomita, 1986). (B) The Follow-Set cache
dynamically constructed during parsing. Each cache node represents a subset of RTN symbol nodes. The numbers
indicate order of appearance; the lettered nodes partition their preceding node by symbol name. Since the cache was
created on an as-needed basis, its shape parallels the shape of the parse-tree. (C) Compressing the shape of (B).
</figureCaption>
<page confidence="0.964801">
103
</page>
<figure confidence="0.618234833333333">
P-1
211 P(.7.2.41 P-5 —8 4r471—N1
iS —6 41t--7
P— 2 V P
, P
1 N
</figure>
<figureCaption confidence="0.802936">
Figure 9. The LR table cache graph when parsing &amp;quot;I Saw A Man On The Hill With A Telescope Through The
Window&amp;quot; (A) without cache node merging, and (B) with merging.
</figureCaption>
<bodyText confidence="0.9999477">
grammar symbol nodes as an already existing
Follow-Set node, it is merged into the older node&apos;s
equivalence class. This avoids redundant expansions,
without which the cache would be an infmite tree of
parse paths, rather than a graph. A comparison is
shown in Figure 9. If the entire LR-table cache is
needed, an ambiguous sentence containing all
possible lexical categories at each position can be
presented; convergence follows from the finiteness of
the subset construction.
</bodyText>
<sectionHeader confidence="0.9999205" genericHeader="method">
7. IMPLEMENTATION AND
CURRENT WORK
</sectionHeader>
<bodyText confidence="0.999878590909091">
We have developed an interactive graphical
programming environment for constructing LR-
parsers. It uses the color MAC/II computer in the
Object LISP extension of Common LISP. The
system is built on CACHE I&apos;m (Perlin, © 1990), a
general Call-Graph Caching system for animating Al
algorithms.
The RTNs are built from grammars. A variety of
LR-RTN-based parsers, including BLR(0), with or
without merging, and with or without Follow-Set
caching have been constructed. Every algorithm
described in this paper is implemented. Visualization
is heavily exploited. For example, selecting an LR-
table cache node will select all its members in the
RT&apos;N display. The graphical animation component
automatically drew all the RTNs and parse-trees in
the Figures, and has generated color slides useful in
teaching.
Fine-grained parallel implementations of BLR(0)
on the Connection Machine are underway to reduce
the costly cartesian product step to constant time. We
are also adding semantic constraints.
</bodyText>
<sectionHeader confidence="0.926601" genericHeader="conclusions">
8. CONCLUSION
</sectionHeader>
<bodyText confidence="0.99903684">
We have introduced BLR(0), a simple bottom-up
LR RTN-based CF parsing algorithm. We explicitly
expand grammars to RTNs, and only then construct
our parsing algorithm. This intermediate step
eliminates the complex algebra usually associated
with parsing, and renders more transparent the close
relations between different parsers.
Earley&apos;s algorithm is seen to be fundamentally an
LR parser. Earley&apos;s propose expansion step is a
recursion analogous to our Follow-Set traversal of the
RTN. By explicating the LR-RTN graph in the
computation, no other complex data structures are
required. The efficient merging is accomplished by
using an option available to BLR(0): merging parse
nodes into equivalence classes.
Tomita&apos;s algorithm uses the cached LR Follow-Set
option, in addition to merging. Again, by using the
RTN as a concrete data structure, the technical feats
associated with Tomita&apos;s parser disappear. His shared
packed forest follows immediately from our merge
option. His graph stack and his parse forest are, for
us, the same entity: the shared parse tree. Even the
LR table is seen to derive from this parsing activity,
particularly with incremental construction from the
RTN.
</bodyText>
<page confidence="0.996519">
104
</page>
<bodyText confidence="0.9985605">
Bringing the RTN into parsing as an explicit
realization of the original grammar appears to be a
conceptual and implementational improvement over
less uniform treatments.
</bodyText>
<sectionHeader confidence="0.998047" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.99602925">
Numerous conversations with Jaime Carbonell
were helpful in developing these ideas. I thank the
students at CMU and in the Tools for Al tutorial
whose many questions helped clarify this approach.
</bodyText>
<sectionHeader confidence="0.999716" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.988727721311475">
Aho, A.V., Hoperoft, J.E., and Ullman, J.D.
1983. Data Structures and Algorithms. Reading, MA:
Addison-Wesley.
Aho, A.V., Sethi, R., and Ullman, J.D. 1986.
Compilers: Principles, Techniques and Tools.
Reading, MA: Addison-Wesley.
DeRemer, F. 1971. Simple LR(k) grammars.
Communications of the ACM, 14(7): 453-460.
Earley, J. 1970. An Efficient Context-Free
Parsing Algorithm. Communications of the ACM,
13(2): 94-102.
Futamura, Y. 1971. Partial evaluation of
computation process - an approach to a compiler-
compiler. Comp. Sys. Cont., 2(5): 45-50.
Heering, J., Klint, P., and Rekers, J. 1990.
Incremental Generation of Parsers. IEEE Trans.
Software Engineering, 16(12): 1344-1351.
Hoperoft, J.E., and Ullman, J.D. 1979.
Introduction to Automata Theory, Languages, and
Computation. Reading, Mass.: Addison-Wesley.
Kaplan, R.M. 1973. A General Syntactic
Processor. In Natural Language Processing, Rustin,
R., ed., 193-241. New York, NY: Algorithmics Press.
Knuth, D.E. 1965. On the Translation of
Languages from Left to Right. Information and
Control, 8(6): 607-639.
Lang, B. 1974. Deterministic techniques for
efficient non-deterministic parsers. In Proc. Second
Colloquium Automata, Languages and Programming,
255-269. Loeckx, J., ed., (Lecture Notes in Computer
Science, vol. 14), New York: Springer-Verlag.
Lang, B. 1991. Towards a Uniform Formal
Framework for Parsing. In Current Issues in Parsing
Technology, Tomita, M., ed., 153-172. Boston:
Kluwer Academic Publishers.
Perlin, M.W. 1989. Call-Graph Caching:
Transforming Programs into Networks. In Proc. of
the Eleventh Int. Joint Conf. on Artificial Intelligence,
122-128. Detroit, Michigan, Morgan Kaufmann.
Perlin, M.W. 1990. Progress in Call-Graph
Caching, Tech Report, CMU-CS-90-132, Carnegie-
Mellon University.
Perlin, M.W. 1991. RETE and Chart Parsing
from Bottom-Up Call-Graph Caching, submitted to
conference, Carnegie Mellon University.
Perlin, M.W. 1990. CACHE: a Color
Animated Call-grapH Environment, ver. 1.3,
Common LISP MACINTOSH Program, Pittsburgh,
PA.
Tomita, M. 1985. An Efficient Context-Free
Parsing Algorithm for Natural Languages. In
Proceedings of the Ninth IJCAI, 756-764. Los
Angeles, CA,.
Tomita, M. 1986. Efficient Parsing for Natural
Language. Kluwar Publishing.
Winograd, T. 1983. Language as a Cognitive
Process, Volume I: Syntax. Reading, MA: Addison-
Wesley.
Woods, W.A. 1970. Transition network
grammars for natural language analysis. Comm ACM,
13(10): 591-606.
</reference>
<page confidence="0.998971">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936789">
<title confidence="0.998494">LR RECURSIVE TRANSITION NETWORKS FOR EARLEY AND TOMITA PARSING</title>
<author confidence="0.999996">Mark Perlin</author>
<affiliation confidence="0.999934">School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.999959">Pittsburgh, PA 15213</address>
<email confidence="0.980151">Internet:perlin@cs.cmu.edu</email>
<abstract confidence="0.996400166666667">Efficient syntactic and semantic parsing for ambiguous context-free languages are generally characterized as complex, specialized, highly formal algorithms. In fact, they are readily constructed from straightforward recursive transition networks (RTNs). In this paper, we introduce LR-RTNs, and then computationally motivate a uniform progression from basic LR parsing, to Earley&apos;s (chart) parsing, concluding with Tomita&apos;s parser. These apparently disparate algorithms are unified into a single implementation, which was used to automatically generate all the figures in this paper.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>Data Structures and Algorithms.</title>
<date>1983</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="6905" citStr="Aho, Hoperoft, and Ullman, 1983" startWordPosition="1042" endWordPosition="1046">Ns have more typically been LL-RTNs, for effecting parses of leftmost derivations (Woods, 1970), as shown in Figure 2A. (Other, more efficient, control structures have also been used (Kaplan, 1973).) Our shift from LL to LR, shown in Figure 2B, uses the chain expansion to set up a subsequent data-driven completion, thereby permitting greater parsing efficiency. In Figure 3, we show the RTN expansion of the simple grammar used in our first set of examples: S --&gt; NP VP NP --&gt; NI DN VP ---&gt; V NP . Chains that share identical prefixes are merged (Perlin, 1989) into a directed acyclic graph (DAG) (Aho, Hoperoft, and Ullman, 1983). This makes our RTN a forest of DAGs, rather than trees. For example, the shared NP start node initiates the chains for Rules #2 and #3 in the NP component. In augmented recursive transition networks (ATNs) (Woods, 1970), semantic constraints may be expressed. These constraints can employ case grammars, functional grammars, unification, and so on (Winograd, 1983). In our RTN formulation, semantic testing occurs when instantiating rule nodes: failing a constraint removes a parse from further (A) (B) Figure 2. A. An LL-RTN for S---&gt;NP VP. This expansion does not set up an expectation for a dat</context>
</contexts>
<marker>Aho, Hoperoft, Ullman, 1983</marker>
<rawString>Aho, A.V., Hoperoft, J.E., and Ullman, J.D. 1983. Data Structures and Algorithms. Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>R Sethi</author>
<author>J D Ullman</author>
</authors>
<date>1986</date>
<booktitle>Compilers: Principles, Techniques and Tools.</booktitle>
<publisher>Addison-Wesley.</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="14360" citStr="Aho, Sethi, and Ullman, 1986" startWordPosition="2267" endWordPosition="2271">om bottom to top, within the column: (1) Initialize the column with the input word. (2) Recursively complete the column bottom-up using the INSERT method. (3) Compute the column&apos;s (rightward) Follow-Set. With the addition of Step (3), this defmes our Basic LR(0), or BLR(0), parser. We now describe the Follow-Set. Once an RTN node X has been instantiated in some column, it sets up an expectation for • The RTN node(s) Yg that immediately follow it; • For each immediate follower Yg, all those RTN symbol nodes Wg,h that initiate chains that could recursively lead up to Yg. This is the Follow-Set (Aho, Sethi, and Ullman, 1986). The Follow-Set(X) is computed directly from the RTN by the recursion: 101 Follow-Set(X) LET Result fFor every unvisited RTN node Y following X: Result ( Y } IF Y&apos;s label is a terminal symbol, THEN 0; ELSE Follow-Set of the start symbol of Y&apos;s label Return Result As is clear from the recursive defmition, Follow-Set (ug Xg }) = ug Follow-Set (Xg). Therefore, the Follow-Set of a column&apos;s symbol nodes can be deferred to Step (3) of the BLR(0) parsing algorithm, after the determination of all the nodes has completed. By only recursing on unvisited nodes, this traversal of the grammar RTN has tim</context>
</contexts>
<marker>Aho, Sethi, Ullman, 1986</marker>
<rawString>Aho, A.V., Sethi, R., and Ullman, J.D. 1986. Compilers: Principles, Techniques and Tools. Reading, MA: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F DeRemer</author>
</authors>
<title>Simple LR(k) grammars.</title>
<date>1971</date>
<journal>Communications of the ACM,</journal>
<volume>14</volume>
<issue>7</issue>
<pages>453--460</pages>
<contexts>
<context position="1393" citStr="DeRemer, 1971" startWordPosition="185" endWordPosition="186">ODUCTION Ambiguous context-free grammars (CFGs) are currently used in the syntactic and semantic processing of natural language. For efficient parsing, two major computational methods are used. The rust is Earley&apos;s algorithm (Earley, 1970), which merges parse trees to reduce the computational dependence on input sentence length from exponential to cubic cost. Numerous variations on Earley&apos;s dynamic programming method have developed into a family of chart parsing (Winograd, 1983) algorithms. The second is Tomita&apos;s algorithm (Tomita, 1986), which generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s (DeRemer, 1971) computer language LR parsing techniques. Tomita&apos;s algorithm augments the LR parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s ideas. What is not currently appreciated is the continuity between these apparently distinct computational methods. • Tomita has proposed (Tomita, 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constructions of Tomita&apos;s algorithm (Heering, Klint, and Rekers, 1990) may similarly be viewed as just one point along a continuum of meth</context>
<context position="19694" citStr="DeRemer, 1971" startWordPosition="3153" endWordPosition="3154">rammars used in machine translation. By caching the requisite Follow-Set computations into a graph, subsequent Follow-Set computation is reduced. This incremental construction is similar to (Heering, Klint, and Rekers, 1990)&apos;s, asymptotically constructing Tomita&apos;s allpaths LR parsing algorithm (Tomita, 1986). The Follow-Set cache (or LR-table) can be dynamically constructed by Call-Graph Caching (Perlin, 1989) during the parsing. Every time a Follow-Set computation is required, it is looked up in the cache. When not present, the Follow-Set is computed and cached as a graph. Following DeRemer (DeRemer, 1971), each cached Follow-Set node is finely partitioned, as needed, into disjoint subsets indexed by the RTN label name, as shown in the graphs of Figure 8. The partitioning reduces the cache size: instead of allowing all possible subsets of the RTN, the cache graph nodes contain smaller subsets of identically labelled symbol nodes. When a Follow-Set node has the same subset of —6 P— 5 3 4 N S — 6 P— 2 VIP P-- 5 Figure 8. (A) A parse of &amp;quot;I Saw A Man&amp;quot; using the grammar in (Tomita, 1986). (B) The Follow-Set cache dynamically constructed during parsing. Each cache node represents a subset of RTN symb</context>
</contexts>
<marker>DeRemer, 1971</marker>
<rawString>DeRemer, F. 1971. Simple LR(k) grammars. Communications of the ACM, 14(7): 453-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>94--102</pages>
<contexts>
<context position="1018" citStr="Earley, 1970" startWordPosition="133" endWordPosition="134">ecursive transition networks (RTNs). In this paper, we introduce LR-RTNs, and then computationally motivate a uniform progression from basic LR parsing, to Earley&apos;s (chart) parsing, concluding with Tomita&apos;s parser. These apparently disparate algorithms are unified into a single implementation, which was used to automatically generate all the figures in this paper. 1. INTRODUCTION Ambiguous context-free grammars (CFGs) are currently used in the syntactic and semantic processing of natural language. For efficient parsing, two major computational methods are used. The rust is Earley&apos;s algorithm (Earley, 1970), which merges parse trees to reduce the computational dependence on input sentence length from exponential to cubic cost. Numerous variations on Earley&apos;s dynamic programming method have developed into a family of chart parsing (Winograd, 1983) algorithms. The second is Tomita&apos;s algorithm (Tomita, 1986), which generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s (DeRemer, 1971) computer language LR parsing techniques. Tomita&apos;s algorithm augments the LR parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s ideas. What is not currently appreciated is the continuity between these apparently distinct compu</context>
<context position="17316" citStr="Earley, 1970" startWordPosition="2777" endWordPosition="2778">ing the successor instance of N2 shown. 5. EARLEY&apos;S PARSING ALGORITHM Natural languages such as English are ambiguous. A single sentence may have multiple syntactic structures. For example, extending our simple grammar with rules accounting for Prepositions and Prepositional-Phrases (Tomita, 1986) S -+ S PP NP -+ NP PP PP P NP, the sentence &amp;quot;I saw a man on the hill with a telescope through the window&amp;quot; has 14 valid derivations. In parsing, separate reconstructions of these different parses can lead to exponential cost. For parsing efficiency, partially constructed instance-trees can be merged (Earley, 1970). As before, parse-node x denotes a point along a parsesequence, say, v-w-x. The left-border i of this parsesequence is the left-border of the leftmost parse-node in the sequence. All parse-sequences of RTN symbol node X that cover columns i+1 through k may be collected into a single equivalence class X(i,k). For 102 the purposes of (1) continuing with the parse and (2) disambiguating parse-trees, members of X(i,k) are indistinguishable. Over an input sentence of length n, there are therefore no more than 0(n2) equivalence classes of X. of algorithms that couple efficient parse-tree merging wi</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. 1970. An Efficient Context-Free Parsing Algorithm. Communications of the ACM, 13(2): 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Futamura</author>
</authors>
<title>Partial evaluation of computation process - an approach to a compilercompiler.</title>
<date>1971</date>
<journal>Comp. Sys. Cont.,</journal>
<volume>2</volume>
<issue>5</issue>
<pages>45--50</pages>
<contexts>
<context position="5167" citStr="Futamura, 1971" startWordPosition="762" endWordPosition="763">ansition networks, each identified by a nonterminal label. All other labels are terminal labels. When, in traversing a transition network, a nonterminal label is encountered, control recursively passes to the beginning of the correspondingly labelled transition network. Should this labelled network be successfully traversed, on exit, control returns back to the labelled calling node. The linear text of a context-free grammar can be cast into an RTN structure (Perlin, 1989). This is done by expanding each grammar rule into a linear chain. The top-down expansion amounts to a partial evaluation (Futamura, 1971) of the rule into a computational expectation: an eventual bottom-up data-directed instantiation that will complete the expansion. Figure 1, for example, shows the expansion of the grammar rule #1 S--*NP VP. First, the nonterminal S, which labels this connected component, is expanded as a nonterminal node. One method for realizing this nonterminal node, is via Rule#1; its rule node is therefore expanded. Rule#1 sets up the expectation for the VP symbol node, which in turn sets up the expectation for the NP symbol node. NP, the first symbol node in the chain, creates the start node S. In subseq</context>
</contexts>
<marker>Futamura, 1971</marker>
<rawString>Futamura, Y. 1971. Partial evaluation of computation process - an approach to a compilercompiler. Comp. Sys. Cont., 2(5): 45-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Heering</author>
<author>P Klint</author>
<author>J Rekers</author>
</authors>
<title>Incremental Generation of Parsers.</title>
<date>1990</date>
<journal>IEEE Trans. Software Engineering,</journal>
<volume>16</volume>
<issue>12</issue>
<pages>1344--1351</pages>
<contexts>
<context position="1924" citStr="Heering, Klint, and Rekers, 1990" startWordPosition="259" endWordPosition="263">ita&apos;s algorithm (Tomita, 1986), which generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s (DeRemer, 1971) computer language LR parsing techniques. Tomita&apos;s algorithm augments the LR parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s ideas. What is not currently appreciated is the continuity between these apparently distinct computational methods. • Tomita has proposed (Tomita, 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constructions of Tomita&apos;s algorithm (Heering, Klint, and Rekers, 1990) may similarly be viewed as just one point along a continuum of methods. * This work was supported in part by grant R29 LM 04707 from the National Library of Medicine, and by the Pittsburgh NMR Institute. The apparent distinctions between these related methods follows from the distinct complex formal and mathematical apparati (Lang, 1974; Lang, 1991) currently employed to construct these CF parsing algorithms. To effect a uniform synthesis of these methods, in this paper we introduce LR Recursive Transition Networks (LR-RTNs) as a simpler framework on which to build CF parsing algorithms. Whi</context>
<context position="19303" citStr="Heering, Klint, and Rekers, 1990" startWordPosition="3093" endWordPosition="3097"> They are in the same column k. (3) They have identical left borders i. The total number of links formed by INSERT during an entire parse, accounting for every grammar RTN node, is 0(n3)x0(1G1). The chart parsers are a family In our BLR(0) parsing algorithm, even with merging, the Follow-Set is computed at every column. While this computation is just 0(101), it can become a bottleneck with the very large grammars used in machine translation. By caching the requisite Follow-Set computations into a graph, subsequent Follow-Set computation is reduced. This incremental construction is similar to (Heering, Klint, and Rekers, 1990)&apos;s, asymptotically constructing Tomita&apos;s allpaths LR parsing algorithm (Tomita, 1986). The Follow-Set cache (or LR-table) can be dynamically constructed by Call-Graph Caching (Perlin, 1989) during the parsing. Every time a Follow-Set computation is required, it is looked up in the cache. When not present, the Follow-Set is computed and cached as a graph. Following DeRemer (DeRemer, 1971), each cached Follow-Set node is finely partitioned, as needed, into disjoint subsets indexed by the RTN label name, as shown in the graphs of Figure 8. The partitioning reduces the cache size: instead of allo</context>
</contexts>
<marker>Heering, Klint, Rekers, 1990</marker>
<rawString>Heering, J., Klint, P., and Rekers, J. 1990. Incremental Generation of Parsers. IEEE Trans. Software Engineering, 16(12): 1344-1351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<location>Reading, Mass.:</location>
<contexts>
<context position="3806" citStr="Hoperoft and Ullman, 1979" startWordPosition="539" endWordPosition="542"> of related parsing algorithm. • Computational motivation and justification for each algorithm in this family. • Uniform extensibility of these syntactic methods to semantic parsing. • Shared graphical representations, useful in building interactive programming environments for computational linguists. • Parallelization of these parsing algorithms. • All of the known advantages of RTNs, together with efficiencies of LR parsing. All of these improvements will be discussed in the paper. 2. LR RECURSIVE TRANSITION NETWORKS A transition network is a directed graph, used as a finite state machine (Hoperoft and Ullman, 1979). The network&apos;s nodes or edges are labelled; in this paper, we shall label the nodes. When an input sentence is read, state moves from node to node. A sentence is accepted if reading the entire sentence directs the network traversal so as to arrive at an 98 Nonterminal symbol S 410 Rule #1 Start symbol S Symbols NP d VP M (A) (B) (C) (D) (E) Figure 1. Expanding Rule#1: S—&gt;NP VP. A. Expanding the nonterminal symbol S. B. Expanding the rule node for Rule #1. C. Expanding the symbol node VP. D. Expanding the symbol node NP. E. Expanding the start node S. accepting node. To increase computational </context>
</contexts>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>Hoperoft, J.E., and Ullman, J.D. 1979. Introduction to Automata Theory, Languages, and Computation. Reading, Mass.: Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
</authors>
<title>A General Syntactic Processor.</title>
<date>1973</date>
<booktitle>In Natural Language Processing,</booktitle>
<pages>193--241</pages>
<editor>Rustin, R., ed.,</editor>
<publisher>Algorithmics Press.</publisher>
<location>New York, NY:</location>
<contexts>
<context position="6471" citStr="Kaplan, 1973" startWordPosition="967" endWordPosition="968">instantiate the entire chain of Rule#1, thereby detecting a nonterminal symbol S. Partial instantiation of the Rule&apos;s chain indicates the partial progress in sequencing the Rule&apos;s right-hand-side symbols. The expansion in Figure 1 constructs an LR-RTN. That is, it sets up a Left-to-right parse of a Rightmost derivation. Such derivations are developed in the next Section. As used in AI natural language parsing, RTNs have more typically been LL-RTNs, for effecting parses of leftmost derivations (Woods, 1970), as shown in Figure 2A. (Other, more efficient, control structures have also been used (Kaplan, 1973).) Our shift from LL to LR, shown in Figure 2B, uses the chain expansion to set up a subsequent data-driven completion, thereby permitting greater parsing efficiency. In Figure 3, we show the RTN expansion of the simple grammar used in our first set of examples: S --&gt; NP VP NP --&gt; NI DN VP ---&gt; V NP . Chains that share identical prefixes are merged (Perlin, 1989) into a directed acyclic graph (DAG) (Aho, Hoperoft, and Ullman, 1983). This makes our RTN a forest of DAGs, rather than trees. For example, the shared NP start node initiates the chains for Rules #2 and #3 in the NP component. In augm</context>
</contexts>
<marker>Kaplan, 1973</marker>
<rawString>Kaplan, R.M. 1973. A General Syntactic Processor. In Natural Language Processing, Rustin, R., ed., 193-241. New York, NY: Algorithmics Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>On the Translation of Languages from Left to Right.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<issue>6</issue>
<pages>607--639</pages>
<contexts>
<context position="1363" citStr="Knuth, 1965" startWordPosition="181" endWordPosition="182">gures in this paper. 1. INTRODUCTION Ambiguous context-free grammars (CFGs) are currently used in the syntactic and semantic processing of natural language. For efficient parsing, two major computational methods are used. The rust is Earley&apos;s algorithm (Earley, 1970), which merges parse trees to reduce the computational dependence on input sentence length from exponential to cubic cost. Numerous variations on Earley&apos;s dynamic programming method have developed into a family of chart parsing (Winograd, 1983) algorithms. The second is Tomita&apos;s algorithm (Tomita, 1986), which generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s (DeRemer, 1971) computer language LR parsing techniques. Tomita&apos;s algorithm augments the LR parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s ideas. What is not currently appreciated is the continuity between these apparently distinct computational methods. • Tomita has proposed (Tomita, 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constructions of Tomita&apos;s algorithm (Heering, Klint, and Rekers, 1990) may similarly be viewed as just one p</context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>Knuth, D.E. 1965. On the Translation of Languages from Left to Right. Information and Control, 8(6): 607-639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<journal>Lecture Notes in Computer Science,</journal>
<booktitle>In Proc. Second Colloquium Automata, Languages and Programming,</booktitle>
<volume>14</volume>
<pages>255--269</pages>
<editor>Loeckx, J., ed.,</editor>
<publisher>Springer-Verlag.</publisher>
<location>New York:</location>
<contexts>
<context position="2264" citStr="Lang, 1974" startWordPosition="317" endWordPosition="318">sed (Tomita, 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constructions of Tomita&apos;s algorithm (Heering, Klint, and Rekers, 1990) may similarly be viewed as just one point along a continuum of methods. * This work was supported in part by grant R29 LM 04707 from the National Library of Medicine, and by the Pittsburgh NMR Institute. The apparent distinctions between these related methods follows from the distinct complex formal and mathematical apparati (Lang, 1974; Lang, 1991) currently employed to construct these CF parsing algorithms. To effect a uniform synthesis of these methods, in this paper we introduce LR Recursive Transition Networks (LR-RTNs) as a simpler framework on which to build CF parsing algorithms. While RTNs (Woods, 1970) have been widely used in Artificial Intelligence (AI) for natural language parsing, their representational advantages have not been fully exploited for efficiency. The LR-RTNs, however, are efficient, and shall be used to construct: (1) a nondeterministic parser, (2) a basic LR(0) parser, (3) Earley&apos;s algorithm (and </context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>Lang, B. 1974. Deterministic techniques for efficient non-deterministic parsers. In Proc. Second Colloquium Automata, Languages and Programming, 255-269. Loeckx, J., ed., (Lecture Notes in Computer Science, vol. 14), New York: Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Towards a Uniform Formal Framework for Parsing. In Current Issues</title>
<date>1991</date>
<pages>153--172</pages>
<editor>in Parsing Technology, Tomita, M., ed.,</editor>
<location>Boston:</location>
<contexts>
<context position="2277" citStr="Lang, 1991" startWordPosition="319" endWordPosition="320"> 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constructions of Tomita&apos;s algorithm (Heering, Klint, and Rekers, 1990) may similarly be viewed as just one point along a continuum of methods. * This work was supported in part by grant R29 LM 04707 from the National Library of Medicine, and by the Pittsburgh NMR Institute. The apparent distinctions between these related methods follows from the distinct complex formal and mathematical apparati (Lang, 1974; Lang, 1991) currently employed to construct these CF parsing algorithms. To effect a uniform synthesis of these methods, in this paper we introduce LR Recursive Transition Networks (LR-RTNs) as a simpler framework on which to build CF parsing algorithms. While RTNs (Woods, 1970) have been widely used in Artificial Intelligence (AI) for natural language parsing, their representational advantages have not been fully exploited for efficiency. The LR-RTNs, however, are efficient, and shall be used to construct: (1) a nondeterministic parser, (2) a basic LR(0) parser, (3) Earley&apos;s algorithm (and the chart par</context>
</contexts>
<marker>Lang, 1991</marker>
<rawString>Lang, B. 1991. Towards a Uniform Formal Framework for Parsing. In Current Issues in Parsing Technology, Tomita, M., ed., 153-172. Boston:</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Perlin</author>
</authors>
<title>Call-Graph Caching: Transforming Programs into Networks.</title>
<date>1989</date>
<booktitle>In Proc. of the Eleventh Int. Joint Conf. on Artificial Intelligence,</booktitle>
<pages>122--128</pages>
<publisher>Kluwer Academic</publisher>
<location>Detroit, Michigan,</location>
<contexts>
<context position="5029" citStr="Perlin, 1989" startWordPosition="740" endWordPosition="741"> regular languages to context-free languages, recursive transition networks (RTNs) are introduced. An RTN is a forest of disconnected transition networks, each identified by a nonterminal label. All other labels are terminal labels. When, in traversing a transition network, a nonterminal label is encountered, control recursively passes to the beginning of the correspondingly labelled transition network. Should this labelled network be successfully traversed, on exit, control returns back to the labelled calling node. The linear text of a context-free grammar can be cast into an RTN structure (Perlin, 1989). This is done by expanding each grammar rule into a linear chain. The top-down expansion amounts to a partial evaluation (Futamura, 1971) of the rule into a computational expectation: an eventual bottom-up data-directed instantiation that will complete the expansion. Figure 1, for example, shows the expansion of the grammar rule #1 S--*NP VP. First, the nonterminal S, which labels this connected component, is expanded as a nonterminal node. One method for realizing this nonterminal node, is via Rule#1; its rule node is therefore expanded. Rule#1 sets up the expectation for the VP symbol node,</context>
<context position="6836" citStr="Perlin, 1989" startWordPosition="1034" endWordPosition="1035">Section. As used in AI natural language parsing, RTNs have more typically been LL-RTNs, for effecting parses of leftmost derivations (Woods, 1970), as shown in Figure 2A. (Other, more efficient, control structures have also been used (Kaplan, 1973).) Our shift from LL to LR, shown in Figure 2B, uses the chain expansion to set up a subsequent data-driven completion, thereby permitting greater parsing efficiency. In Figure 3, we show the RTN expansion of the simple grammar used in our first set of examples: S --&gt; NP VP NP --&gt; NI DN VP ---&gt; V NP . Chains that share identical prefixes are merged (Perlin, 1989) into a directed acyclic graph (DAG) (Aho, Hoperoft, and Ullman, 1983). This makes our RTN a forest of DAGs, rather than trees. For example, the shared NP start node initiates the chains for Rules #2 and #3 in the NP component. In augmented recursive transition networks (ATNs) (Woods, 1970), semantic constraints may be expressed. These constraints can employ case grammars, functional grammars, unification, and so on (Winograd, 1983). In our RTN formulation, semantic testing occurs when instantiating rule nodes: failing a constraint removes a parse from further (A) (B) Figure 2. A. An LL-RTN fo</context>
<context position="19493" citStr="Perlin, 1989" startWordPosition="3120" endWordPosition="3121">ers are a family In our BLR(0) parsing algorithm, even with merging, the Follow-Set is computed at every column. While this computation is just 0(101), it can become a bottleneck with the very large grammars used in machine translation. By caching the requisite Follow-Set computations into a graph, subsequent Follow-Set computation is reduced. This incremental construction is similar to (Heering, Klint, and Rekers, 1990)&apos;s, asymptotically constructing Tomita&apos;s allpaths LR parsing algorithm (Tomita, 1986). The Follow-Set cache (or LR-table) can be dynamically constructed by Call-Graph Caching (Perlin, 1989) during the parsing. Every time a Follow-Set computation is required, it is looked up in the cache. When not present, the Follow-Set is computed and cached as a graph. Following DeRemer (DeRemer, 1971), each cached Follow-Set node is finely partitioned, as needed, into disjoint subsets indexed by the RTN label name, as shown in the graphs of Figure 8. The partitioning reduces the cache size: instead of allowing all possible subsets of the RTN, the cache graph nodes contain smaller subsets of identically labelled symbol nodes. When a Follow-Set node has the same subset of —6 P— 5 3 4 N S — 6 P—</context>
</contexts>
<marker>Perlin, 1989</marker>
<rawString>Kluwer Academic Publishers. Perlin, M.W. 1989. Call-Graph Caching: Transforming Programs into Networks. In Proc. of the Eleventh Int. Joint Conf. on Artificial Intelligence, 122-128. Detroit, Michigan, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Perlin</author>
</authors>
<title>Progress in Call-Graph Caching,</title>
<date>1990</date>
<tech>Tech Report, CMU-CS-90-132,</tech>
<institution>CarnegieMellon University.</institution>
<contexts>
<context position="8397" citStr="Perlin, 1990" startWordPosition="1278" endWordPosition="1279"> The three connected components correspond to the three nonterminals in the grammar. Each symbol node in the RTN denotes a subsequence originating from its leftmost start symbol. 3. NONDETERMINISTIC DERIVATIONS A grammar&apos;s RTN can be used as a template for parsing. A sentence (the data) directs the instantiation of individual rule chains into a parse tree. The RTN instances exactly correspond to parsetree nodes. This is most easily seen with nondeterministic rightmost derivations. Given an input sentence of n words, we may derive a sentence in the language with the nondeterministic algorithm (Perlin, 1990): Put an instance of nonterminal node S into the last column. From right to left, for every column: From top to bottom, within the column: (1) Recursively expand the column top-down by nondeterministic selection of rule instances. (2) Install the next (leftward) symbol instance. In substep (1), following selection, a rule node and its immediately downward symbol node are instantiated. The instantiation process creates a new object that inherits from the template RTN node, adding information about column position and local link connections. For example, to derive &amp;quot;I Saw A Man&amp;quot; we would nondeter</context>
</contexts>
<marker>Perlin, 1990</marker>
<rawString>Perlin, M.W. 1990. Progress in Call-Graph Caching, Tech Report, CMU-CS-90-132, CarnegieMellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Perlin</author>
</authors>
<title>RETE and Chart Parsing from Bottom-Up Call-Graph Caching, submitted to conference,</title>
<date>1991</date>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="10649" citStr="Perlin, 1991" startWordPosition="1622" endWordPosition="1623">dingly. 4.1 AN INEFFICIENT BLR(0) PARSER A simple, inefficient parsing algorithm for computing all possible parse-trees is: Put an instance of start node S into the 0 column. From left to right, for every column: From bottom to top, within the column: (1) Initialize the column with the input word. (2) Recursively complete the column bottom-up using the INSERT method. This reverses the derivation algorithm into bottom-up generation: bottom-to-top, and left-to-right. In the inner loop, the Step (1) initialization is straightforward; we elaborate Step (2). 100 Step (2) uses the following method (Perlin, 1991) to insert instances of RTN nodes: INSERT (instance) ASK instance (1) Link up with predecessor instances. (2) Install self. (3) ENQUEUE successor instances for insertion. } In (1), links are constructed between the instance and its predecessor instances. In (2), the instance becomes available for cartesian product formation. In (3), the computationally nontrivial step, the instance enqueues any successor instances within its own column. Most of the INSERT action is done by instances of symbol and rule RTN nodes. Using our INSERT method, a new symbol instance in the parse-tree links with predec</context>
</contexts>
<marker>Perlin, 1991</marker>
<rawString>Perlin, M.W. 1991. RETE and Chart Parsing from Bottom-Up Call-Graph Caching, submitted to conference, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Perlin</author>
</authors>
<title>CACHE: a Color Animated Call-grapH Environment,</title>
<date>1990</date>
<booktitle>ver. 1.3, Common LISP MACINTOSH Program,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="8397" citStr="Perlin, 1990" startWordPosition="1278" endWordPosition="1279"> The three connected components correspond to the three nonterminals in the grammar. Each symbol node in the RTN denotes a subsequence originating from its leftmost start symbol. 3. NONDETERMINISTIC DERIVATIONS A grammar&apos;s RTN can be used as a template for parsing. A sentence (the data) directs the instantiation of individual rule chains into a parse tree. The RTN instances exactly correspond to parsetree nodes. This is most easily seen with nondeterministic rightmost derivations. Given an input sentence of n words, we may derive a sentence in the language with the nondeterministic algorithm (Perlin, 1990): Put an instance of nonterminal node S into the last column. From right to left, for every column: From top to bottom, within the column: (1) Recursively expand the column top-down by nondeterministic selection of rule instances. (2) Install the next (leftward) symbol instance. In substep (1), following selection, a rule node and its immediately downward symbol node are instantiated. The instantiation process creates a new object that inherits from the template RTN node, adding information about column position and local link connections. For example, to derive &amp;quot;I Saw A Man&amp;quot; we would nondeter</context>
</contexts>
<marker>Perlin, 1990</marker>
<rawString>Perlin, M.W. 1990. CACHE: a Color Animated Call-grapH Environment, ver. 1.3, Common LISP MACINTOSH Program, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm for Natural Languages.</title>
<date>1985</date>
<booktitle>In Proceedings of the Ninth IJCAI,</booktitle>
<pages>756--764</pages>
<location>Los Angeles, CA,.</location>
<contexts>
<context position="1672" citStr="Tomita, 1985" startWordPosition="223" endWordPosition="224">computational dependence on input sentence length from exponential to cubic cost. Numerous variations on Earley&apos;s dynamic programming method have developed into a family of chart parsing (Winograd, 1983) algorithms. The second is Tomita&apos;s algorithm (Tomita, 1986), which generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s (DeRemer, 1971) computer language LR parsing techniques. Tomita&apos;s algorithm augments the LR parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s ideas. What is not currently appreciated is the continuity between these apparently distinct computational methods. • Tomita has proposed (Tomita, 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constructions of Tomita&apos;s algorithm (Heering, Klint, and Rekers, 1990) may similarly be viewed as just one point along a continuum of methods. * This work was supported in part by grant R29 LM 04707 from the National Library of Medicine, and by the Pittsburgh NMR Institute. The apparent distinctions between these related methods follows from the distinct complex formal and mathematical apparati (Lang, 1974; Lang, </context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Tomita, M. 1985. An Efficient Context-Free Parsing Algorithm for Natural Languages. In Proceedings of the Ninth IJCAI, 756-764. Los Angeles, CA,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<publisher>Kluwar Publishing.</publisher>
<contexts>
<context position="1322" citStr="Tomita, 1986" startWordPosition="176" endWordPosition="177"> used to automatically generate all the figures in this paper. 1. INTRODUCTION Ambiguous context-free grammars (CFGs) are currently used in the syntactic and semantic processing of natural language. For efficient parsing, two major computational methods are used. The rust is Earley&apos;s algorithm (Earley, 1970), which merges parse trees to reduce the computational dependence on input sentence length from exponential to cubic cost. Numerous variations on Earley&apos;s dynamic programming method have developed into a family of chart parsing (Winograd, 1983) algorithms. The second is Tomita&apos;s algorithm (Tomita, 1986), which generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s (DeRemer, 1971) computer language LR parsing techniques. Tomita&apos;s algorithm augments the LR parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s ideas. What is not currently appreciated is the continuity between these apparently distinct computational methods. • Tomita has proposed (Tomita, 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constructions of Tomita&apos;s algorithm (Heering, Klint, and Rekers, 19</context>
<context position="17001" citStr="Tomita, 1986" startWordPosition="2723" endWordPosition="2724">re 6. Follow-Set(3) = Follow-Set({ D2 } ) = {N2}. Therefore, SAME-LABEL(r)nFollow-Set(3) = (N2). Figure 7. The rule instance r can only instantiate the single successor instance N2. r uses the RTN to find the left RTN neighbor D of N2. r then computes the cartesian product of instance d with r as {d}x{r} , generating the successor instance of N2 shown. 5. EARLEY&apos;S PARSING ALGORITHM Natural languages such as English are ambiguous. A single sentence may have multiple syntactic structures. For example, extending our simple grammar with rules accounting for Prepositions and Prepositional-Phrases (Tomita, 1986) S -+ S PP NP -+ NP PP PP P NP, the sentence &amp;quot;I saw a man on the hill with a telescope through the window&amp;quot; has 14 valid derivations. In parsing, separate reconstructions of these different parses can lead to exponential cost. For parsing efficiency, partially constructed instance-trees can be merged (Earley, 1970). As before, parse-node x denotes a point along a parsesequence, say, v-w-x. The left-border i of this parsesequence is the left-border of the leftmost parse-node in the sequence. All parse-sequences of RTN symbol node X that cover columns i+1 through k may be collected into a single </context>
<context position="19389" citStr="Tomita, 1986" startWordPosition="3106" endWordPosition="3107">by INSERT during an entire parse, accounting for every grammar RTN node, is 0(n3)x0(1G1). The chart parsers are a family In our BLR(0) parsing algorithm, even with merging, the Follow-Set is computed at every column. While this computation is just 0(101), it can become a bottleneck with the very large grammars used in machine translation. By caching the requisite Follow-Set computations into a graph, subsequent Follow-Set computation is reduced. This incremental construction is similar to (Heering, Klint, and Rekers, 1990)&apos;s, asymptotically constructing Tomita&apos;s allpaths LR parsing algorithm (Tomita, 1986). The Follow-Set cache (or LR-table) can be dynamically constructed by Call-Graph Caching (Perlin, 1989) during the parsing. Every time a Follow-Set computation is required, it is looked up in the cache. When not present, the Follow-Set is computed and cached as a graph. Following DeRemer (DeRemer, 1971), each cached Follow-Set node is finely partitioned, as needed, into disjoint subsets indexed by the RTN label name, as shown in the graphs of Figure 8. The partitioning reduces the cache size: instead of allowing all possible subsets of the RTN, the cache graph nodes contain smaller subsets of</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Tomita, M. 1986. Efficient Parsing for Natural Language. Kluwar Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Language as a Cognitive Process, Volume I: Syntax.</title>
<date>1983</date>
<publisher>AddisonWesley.</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="1262" citStr="Winograd, 1983" startWordPosition="168" endWordPosition="169">algorithms are unified into a single implementation, which was used to automatically generate all the figures in this paper. 1. INTRODUCTION Ambiguous context-free grammars (CFGs) are currently used in the syntactic and semantic processing of natural language. For efficient parsing, two major computational methods are used. The rust is Earley&apos;s algorithm (Earley, 1970), which merges parse trees to reduce the computational dependence on input sentence length from exponential to cubic cost. Numerous variations on Earley&apos;s dynamic programming method have developed into a family of chart parsing (Winograd, 1983) algorithms. The second is Tomita&apos;s algorithm (Tomita, 1986), which generalizes Knuth&apos;s (Knuth, 1965) and DeRemer&apos;s (DeRemer, 1971) computer language LR parsing techniques. Tomita&apos;s algorithm augments the LR parsing &amp;quot;set of items&amp;quot; construction with Earley&apos;s ideas. What is not currently appreciated is the continuity between these apparently distinct computational methods. • Tomita has proposed (Tomita, 1985) constructing his algorithm from Earley&apos;s parser, instead of DeRemer&apos;s LR parser. In fact, as we shall show, Earley&apos;s algorithm may be viewed as one form of LR parsing. • Incremental constru</context>
<context position="7272" citStr="Winograd, 1983" startWordPosition="1102" endWordPosition="1103">w the RTN expansion of the simple grammar used in our first set of examples: S --&gt; NP VP NP --&gt; NI DN VP ---&gt; V NP . Chains that share identical prefixes are merged (Perlin, 1989) into a directed acyclic graph (DAG) (Aho, Hoperoft, and Ullman, 1983). This makes our RTN a forest of DAGs, rather than trees. For example, the shared NP start node initiates the chains for Rules #2 and #3 in the NP component. In augmented recursive transition networks (ATNs) (Woods, 1970), semantic constraints may be expressed. These constraints can employ case grammars, functional grammars, unification, and so on (Winograd, 1983). In our RTN formulation, semantic testing occurs when instantiating rule nodes: failing a constraint removes a parse from further (A) (B) Figure 2. A. An LL-RTN for S---&gt;NP VP. This expansion does not set up an expectation for a data-driven leftward parse. B. The corresponding LR-RTN. The rightmost expansion sets up subsequent data-driven leftward parses. 99 processing. This approach applies to every parsing algorithm in this paper, and will not be discussed further. Figure 3. The RTN of an entire grammar. The three connected components correspond to the three nonterminals in the grammar. Eac</context>
<context position="17965" citStr="Winograd, 1983" startWordPosition="2879" endWordPosition="2880">a point along a parsesequence, say, v-w-x. The left-border i of this parsesequence is the left-border of the leftmost parse-node in the sequence. All parse-sequences of RTN symbol node X that cover columns i+1 through k may be collected into a single equivalence class X(i,k). For 102 the purposes of (1) continuing with the parse and (2) disambiguating parse-trees, members of X(i,k) are indistinguishable. Over an input sentence of length n, there are therefore no more than 0(n2) equivalence classes of X. of algorithms that couple efficient parse-tree merging with various control organizations (Winograd, 1983). 6. TOMITA&apos;S PARSING ALGORITHM Suppose X precedes Y in the RTN. When an instance y of Y is added to position k, k5n, and the cartesian product is formed, there are only 0(k2) possible equivalence classes of X for y to combine with. Summing over all n positions, there are no more than 0(n3) possible product formations with Y in parsing an entire sentence. Merging is effected by adding a MERGE step to INSERT: INSERT (instance) instance ÷- MERGE(instance) ASK instance (1) Link up with predecessor instances. (2) Install self. (3) ENQUEUE successor instances for insertion. } The parsing merge pred</context>
</contexts>
<marker>Winograd, 1983</marker>
<rawString>Winograd, T. 1983. Language as a Cognitive Process, Volume I: Syntax. Reading, MA: AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Transition network grammars for natural language analysis.</title>
<date>1970</date>
<journal>Comm ACM,</journal>
<volume>13</volume>
<issue>10</issue>
<pages>591--606</pages>
<contexts>
<context position="2545" citStr="Woods, 1970" startWordPosition="360" endWordPosition="361">ilarly be viewed as just one point along a continuum of methods. * This work was supported in part by grant R29 LM 04707 from the National Library of Medicine, and by the Pittsburgh NMR Institute. The apparent distinctions between these related methods follows from the distinct complex formal and mathematical apparati (Lang, 1974; Lang, 1991) currently employed to construct these CF parsing algorithms. To effect a uniform synthesis of these methods, in this paper we introduce LR Recursive Transition Networks (LR-RTNs) as a simpler framework on which to build CF parsing algorithms. While RTNs (Woods, 1970) have been widely used in Artificial Intelligence (AI) for natural language parsing, their representational advantages have not been fully exploited for efficiency. The LR-RTNs, however, are efficient, and shall be used to construct: (1) a nondeterministic parser, (2) a basic LR(0) parser, (3) Earley&apos;s algorithm (and the chart parsers), and (4) incremental and compiled versions of Tomita&apos;s algorithm. Our uniform construction has advantages over the current highly formal, non-RTN-based, nonuniform approaches to CF parsing: • Clarity of algorithm construction, permitting LR, Earley, and Tomita p</context>
<context position="6369" citStr="Woods, 1970" startWordPosition="951" endWordPosition="952"> In subsequent processing, posting an instance of this start symbol would indicate an expectation to instantiate the entire chain of Rule#1, thereby detecting a nonterminal symbol S. Partial instantiation of the Rule&apos;s chain indicates the partial progress in sequencing the Rule&apos;s right-hand-side symbols. The expansion in Figure 1 constructs an LR-RTN. That is, it sets up a Left-to-right parse of a Rightmost derivation. Such derivations are developed in the next Section. As used in AI natural language parsing, RTNs have more typically been LL-RTNs, for effecting parses of leftmost derivations (Woods, 1970), as shown in Figure 2A. (Other, more efficient, control structures have also been used (Kaplan, 1973).) Our shift from LL to LR, shown in Figure 2B, uses the chain expansion to set up a subsequent data-driven completion, thereby permitting greater parsing efficiency. In Figure 3, we show the RTN expansion of the simple grammar used in our first set of examples: S --&gt; NP VP NP --&gt; NI DN VP ---&gt; V NP . Chains that share identical prefixes are merged (Perlin, 1989) into a directed acyclic graph (DAG) (Aho, Hoperoft, and Ullman, 1983). This makes our RTN a forest of DAGs, rather than trees. For e</context>
</contexts>
<marker>Woods, 1970</marker>
<rawString>Woods, W.A. 1970. Transition network grammars for natural language analysis. Comm ACM, 13(10): 591-606.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>