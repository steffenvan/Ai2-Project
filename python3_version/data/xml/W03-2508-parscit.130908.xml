<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008472">
<title confidence="0.999508">
Language Technology in a Predictive, Restricted On-screen Keyboard
with Dynamic Layout for Severely Disabled People
</title>
<author confidence="0.967702333333333">
Anders Sewerin Johansen,
John Paulin Hansen, Dan
Witzner Hansen
</author>
<affiliation confidence="0.882642">
The IT University of Copen-
hagen
</affiliation>
<address confidence="0.914222333333333">
Glentevej 67,
2400 Copenhagen, Denmark
{dduck, paulin,
</address>
<email confidence="0.812315">
witzner}@it—c.dk
</email>
<author confidence="0.935939">
Kenji Itoh, Satoru Mashino
</author>
<affiliation confidence="0.953553">
Tokyo Inst. of Technology,
</affiliation>
<address confidence="0.905951">
2-12-1 Oh-okayama Meguro-
ku,
Tokyo 152-8552 Japan,
</address>
<email confidence="0.628444">
ken@ie.me.titech.ac.
iP
</email>
<sectionHeader confidence="0.999153" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983714285714">
We are currently developing an eye-typing com-
munication tool — GazeTalk — for people who suf-
fer from amyotrophic lateral sclerosis (ALS), also
known as Lou Gehrig&apos;s disease, who have lost
their voice and mobility and may only be able to
move their eyes. To make the tool widely avail-
able, we use standard PCs and low-resolution, off-
the-shelf digital cameras to track eye positions. As
the precision of eye-tracking is very dependent on
the quality of the cameras used, we decided on a
basic layout, which consisted of few, large on-
screen buttons.
In many cases eye-tracking systems have to deal
with the &amp;quot;Midas Touch&amp;quot; problem: Everywhere you
look, a command may be activated without your
intention (Jacob, 1991). A dwell time activation
principle has been the preferred solution to this
problem since the very first eye-tracking based
communication systems (Hutchinson, 1989; Ma-
jaranta and Raiha, 2002). The dwell time period is
typically between 500 and 1500 milliseconds.
</bodyText>
<subsectionHeader confidence="0.999707">
1.1 GazeTalk —goals and design
</subsectionHeader>
<bodyText confidence="0.999731555555555">
The goal of the GazeTalk project is to develop an
eye-tracking based Augmented and Alternative
Communication (AAC) system that supports sev-
eral languages, facilitates fast text entry and is both
sufficiently feature-complete to be deployed as the
primary AAC tool for users, yet sufficiently flexi-
ble and technically advanced to be used for re-
search purposes. The system is designed for
several target languages, initially Danish, English
</bodyText>
<sectionHeader confidence="0.666644" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.99970525">
This paper describes the GazeTalk
augmentative and alternative
communications (AAC) system, and
presents results from two user studies of
initial typing rates among novice users.
GazeTalk can be operated using eye
tracking, mouse or other pointing devices.
The system presents the user with a user
interface that is based on 12 large on-
screen buttons.
GazeTalk supports a wide range of con-
figurations, including several variants of
probabilistic or ambiguous/clustered key-
boards. The language model used in Ga-
zeTalk is based on a corpus constructed on
the basis of text extracted from Usenet dis-
cussion groups. The results from the user
studies indicated that the prediction-based
input system was less efficient than a static
layout. However, user comments suggest
that this was mainly caused by design re-
lated factors, which were not directly re-
lated to the basic design principles. In the
next design iteration, we aim to improve
the design by eliminating the problems and
to increase the quality of the language
model by using a significantly larger train-
ing corpus.
</bodyText>
<page confidence="0.997913">
59
</page>
<bodyText confidence="0.9992054">
and Japanese. The system has a built-in voice out-
put subsystem, that supports the use of either ex-
ternal (usually synthetic) voice, using the SAPI
API or the Windows clipboard, or an internally
developed digitized voice system. This digitized
voice is currently only available in Danish, and has
a vocabulary of approximately 33.000 words.
The motivation for the project was to explore
and eventually deploy eye-tracking as an input
modality for ALS patients, and to explore issues in
text input in constrained user interfaces (UIs), such
as mobile devices. We explore the relationship be-
tween text input in AAC systems and mobile de-
vices extensively in (Johansen and Hansen, 2002).
The GazeTalk system consists of three elements:
</bodyText>
<listItem confidence="0.9570112">
• The actual GazeTalk program, which imple-
ments the basic UI elements and functionality
(email, voice output, saving and loading
documents etc.).
• A layout editor — StateEditor — which is used
for designing and building the layouts used by
GazeTalk.
• A library of layouts built with StateEditor,
which are designed for various situations (de-
ployment, research etc.).
</listItem>
<bodyText confidence="0.968212142857143">
All layouts in the library use the four by three ma-
trix format shown in Fig. 1, which allows for a
maximum of 12 active on-screen buttons.
GazeTalk and StateEditor support various text-
input modes, which include direct selection of let-
ters, letter- and word prediction/completion and
several tvues of ambiguous/clustered keyboards.
</bodyText>
<figureCaption confidence="0.8121087">
This is the text f A to Z Backspace
[8 most A I 0
likely
words]
Space R L U
Figure 1: Layout of the on-screen keyboard. The subject
is typing &amp;quot;This is the text field&amp;quot;. Letter and word predic-
tions are refined continuously as the user types. The
progress bar indicates the remaining time before the &amp;quot;1&amp;quot;
button is activated by the dwell time selection system.
</figureCaption>
<bodyText confidence="0.99991675">
GazeTalk contains a wide variety of reporting
functions which allow an experimenter to measure
variables such as average selection speed; word per
minute (WPM) rate; pointer trails in terms of entry,
exit and time spent per button; the type and spe-
cific content of buttons inspected and selected: as
well as keeping track of the user&apos;s navigation
within the system.
</bodyText>
<sectionHeader confidence="0.568098" genericHeader="introduction">
2 Language technology in GazeTalk
</sectionHeader>
<bodyText confidence="0.999973636363636">
The decision to use affordable off the shelf cam-
eras for the eye tracking subsystem rather than ex-
pensive, custom equipment imposed severe
restriction on the number of UI elements available
in the basic layout. Since we could not expect to
attain a level of performance from the eye-tracking
system that would allow us to use a full-size on-
screen keyboard, we decided to investigate alterna-
tive approaches. Language technology emerged as
a potential means to provide fast text input, despite
the small number of on-screen buttons.
</bodyText>
<subsectionHeader confidence="0.996529">
2.1 Language models
</subsectionHeader>
<bodyText confidence="0.999991346153846">
Measurements of the information content of writ-
ten language indicate that, given a sufficiently effi-
cient language model, only approximately one bit
of information is needed per character (Shannon,
1951). Language models can be based on many
different natural language processing (NLP) algo-
rithms with varying results in terms of perform-
ance, capabilities and resource consumption. A
simple language model (frequency count) achieves
an entropy of 4.03 bits per character (Shannon,
1951), and an advanced state-of-the-art model
(maximum entropy) achieves 1.2 bits per character
(Rosenfeld, 1996). The traditional n-gram ap-
proach is relatively high performing at 1.5 bits per
character (Tilbourg, 1998). David J. Ward tabu-
lates the performance of most current approaches
and discusses the construction of language models
extensively in (Ward, 2002).
Most language models need to be primed in or-
der to perform optimally. In the case of a trigram
model, which predicts a word on the basis of the
two preceding words, it needs to be primed with
two words in order to supply any predictions at all.
In other words, most (possibly all) language mod-
els will supply unreliable predictions for the first
few words or characters of input. As a conse-
</bodyText>
<page confidence="0.991162">
60
</page>
<bodyText confidence="0.999985166666667">
quence, when designing text input systems based
on language models, one must allow for the possi-
bility that the predictions are wrong.
One of the most common problems when using
word-level language models is the dictionary prob-
lem, also known as the out of vocabulary (00V)
problem: What happens when the user wants to
enter a word which is not in the dictionary? It is
possible to estimate the probability of an unknown
word, but it is obviously impossible to predict the
actual (unknown) word. It is not just impractical to
add all known words to the dictionary (Websters
Ninth New Collegiate Dictionary boasts almost
160.000 entries!) — it is virtually impossible as new
words are constantly added to the vocabulary of all
living languages, and as the number of proper
nouns (names of people, places, products, pets...) is
virtually unlimited. This is a problem of great con-
cern with regards to GazeTalk, as users of an
AAC-system are very likely to develop slang and
abbreviations that are well understood by caretak-
ers in a daily communication context but incom-
prehensible to outsiders and not represented in any
dictionary.
</bodyText>
<subsectionHeader confidence="0.999205">
2.2 The GazeTalk language model
</subsectionHeader>
<bodyText confidence="0.999985941176471">
GazeTalk currently only supports the use of letter-
and word prediction/completion when using Euro-
pean languages. Faced with the choice between a
two-stage hierarchic and a probabilistic input con-
figuration in the European versions, we decided on
a six-key probabilistic on-screen keyboard as the
basic input strategy. Given that the cost of a
prediction hit is one selection, and the cost of a
prediction miss is three selections — one to access
the hierarchical input system, and two to select the
desired letter — the probabilistic keyboard will be
superior to the hierarchical, if the prediction algo-
rithm can supply the desired letter as one of the top
six predictions more than 75% of the time. Obvi-
ously this simple calculation ignores the additional
cognitive load placed on the user by the need to
search for the desired letter or word. However, as
the advantage of the probabilistic keyboard is po-
tentially very big (e.g., 1.2 selections per symbol
compared to two selections on a two-level static,
hierarchical system, given that the prediction algo-
rithm includes the desired letter among the top six
candidates 90% of the time), we decided to base
our prototype on a probabilistic keyboard.
Both the word- and letter-level language models
in the current version use a Katz-style back-off
Markov model as described in (Katz, 1987). The
actual letter predictions are generated by a back-off
algorithm, which consults both the word- and let-
ter-level models. As the word-level predictions are
more likely to be correct, since they are based on a
higher-level context than the letter based model,
the letter prediction algorithm gives higher priority
to the predictions from the word level model.
</bodyText>
<subsectionHeader confidence="0.999772">
2.3 Corpus construction
</subsectionHeader>
<bodyText confidence="0.999978315789473">
As we did not at the time have access to either dic-
tionaries or corpora, we decided to construct our
own. The current Danish language model is based
on a corpus collected from the Danish Usenet dis-
cussion groups, specifically the groups dk.snak
(general conversation) and dk.helbred.handicap
(special interest group for disabilities). The deci-
sion to extract a corpus from these sources was
based on both the availability of the data, and the
fact that the expected language exhibited by the
users would be conversation-level Danish, which is
the primary form of language used in the Usenet
groups. The use of Usenet posting as a basis for a
corpus is also attractive from a legal standpoint. It
is generally agreed that by posting on Usenet, the
author de facto grants everyone the right to store
and otherwise process the material produced.
We collected an initial corpus of approximately
1.5 million words, and extracted and proofread a
vocabulary of approximately 33.000 words from
this, based on their frequency in the corpus. Unfor-
tunately the initial corpus contained a lot of unde-
sirable features, including (but not limited to) use
of other languages than Danish (primarily English,
Swedish and Norwegian), bad spelling, excessive
or missing punctuation, slang and imitation of
speech-level language (e.g., thought-sounds
equivalent to &amp;quot;oh&amp;quot; and &amp;quot;uhm&amp;quot;, as seen in transcrip-
tions of conversations). We therefore applied sev-
eral heuristics to the corpus, using the proofread
vocabulary, in order to filter out the unwanted fea-
tures. These heuristics primarily consisted of dis-
carding sentences containing &amp;quot;stop words&amp;quot;, mainly
frequent non-Danish words; discarding sentences
that were extremely short or extremely long, on the
assumption that they were artifacts of missing or
excessive punctuation; and discarding sentences
that contained more than one or two out-of-
</bodyText>
<page confidence="0.996644">
61
</page>
<bodyText confidence="0.999306666666667">
vocabulary words. This resulting corpus consists of
approximately 467.000 words, a reduction of ap-
proximately 2/3 from the initial corpus.
</bodyText>
<subsectionHeader confidence="0.997123">
2.4 Adaptive vocabulary
</subsectionHeader>
<bodyText confidence="0.999982464285714">
As a means to increase the performance of the lan-
guage model in face of a small vocabulary and
training corpus, we decided to implement auto-
matic collection and integration of vocabulary and
n-grams during daily use. This feature has been
reported to increase user acceptance and text pro-
duction rate significantly in (Carlberger, 1997) and
(Darragh et al., 1990). Additionally, measurements
of cross training performance indicate that an in-
domain model performs significantly better than an
out-of-domain model, even for very large training
sets and very similar test sets (Rosenfeld, 1996).
We confirmed this by performing perfect-user
simulations, i.e., simulations where the number of
selections needed to input a test corpus is com-
puted on the assumption that the user will make no
selection errors, and always use the most efficient
input strategy in terms of the number of selections
used.
The user n-grams are integrated in the combined
word level model by giving priority to n-grams on
the basis of length and source. Thus, e.g., the com-
bined model will consider predictions based on
user generated trigrams more precise than those
based on trigrams from the base model, but predic-
tions based on user generated bigrams are consid-
ered less precise than predictions based on trigrams
from the base model.
</bodyText>
<subsectionHeader confidence="0.8518715">
2.5 Clustered/ambigous keyboards vs.
dynamic/predictive keyboards
</subsectionHeader>
<bodyText confidence="0.999975822222222">
When faced with the task of designing a text input
system on a reduced keyboard, there are three ob-
vious alternatives: A traditional hierarchic system,
as seen in the &amp;quot;Multi Tap&amp;quot; input method; a clus-
tered or ambiguous system, such as the Tegic T9
system or Kuhn and Joni (2001); and a dynamic
keyboard that reassigns the content of the buttons
based on current likelihood. Each has advantages
and disadvantages.
The hierarchical system is inherently inefficient,
but the static nature of the system allows for eyes-
free operation, rote learning and — perhaps more
importantly — it allows for unambiguous letter in-
put, which is the input mode that users expect from
any other way of entering text (typewriters, long-
hand, word processing etc.).
An ambiguous input system has the potential to
perform significantly better than a static, hierarchi-
cal system, and allows for eyes-free operation on a
per-word basis. Given a sufficiently powerful lan-
guage model (or a small dictionary, that the user is
able to memorize) it does support full, or almost
full eyes-free operation. It is however suffering
from the 00V problem: Unless augmented by
morphological algorithms as seen in Rau and
Skiena (1995), it is necessary to allow for an alter-
native input method, for the user to be able to enter
words not found in the vocabulary.
A dynamic keyboard also has a potential for
high performance, but does not allow for eyes-free
operation or rote learning. However, it does con-
form to the user&apos;s expectations that letter entry is
unambiguous and final.
In conclusion, both ambiguous and dynamic
keyboards are potential high-performing alterna-
tives to the static, hierarchic layout, but they both
require the user to accept and adapt to an input sys-
tem that does not conform to their expectations.
Whether the users are more likely to accept am-
biguous input or dynamic letter placement is an
open research issue, which we hope to explore fur-
ther using the GazeTalk system. As a consequence,
the GazeTalk system supports both static, hierar-
chic keyborad layouts, as well as dynamic and am-
biguous keyboards.
</bodyText>
<subsectionHeader confidence="0.996914">
2.6 Layout considerations
</subsectionHeader>
<bodyText confidence="0.9999850625">
In this, the first series of user experiments, we de-
cided to focus on the intial performance among
users presented with a dynamic keyboard layout.
The built-in context-sensitive letter prediction al-
gorithm was used to supply the six most likely let-
ters for the dynamic keyboard. They were then
placed according to the workings of the parafoveal
vision, i.e. with the most probable suggestion in
the center position (&apos;I&apos; in Fig 1), and the other sug-
gestions placed according to probability in a clock-
wise fashion around the center position (&apos;O&apos;, `U&apos;,
`R&apos; and &apos;A&apos; in Fig. 1). This was done on the
assumption that users would quickly learn to an-
ticipate the placement of the desired letter and then
— in case the letter prediction did not supply this as
the primary candidate — be able to evaluate the
</bodyText>
<page confidence="0.99822">
62
</page>
<bodyText confidence="0.999917057142857">
other candidates with a minimum of eye move-
ment.
Furthermore this mode featured buttons for
backspace and space, as well as buttons for access
to word prediction/completion mode and an alpha-
betical letter entry mode (&amp;quot;A to Z&amp;quot;). The word pre-
diction/completion mode presented the current
eight most likely words (the actual words are
shown on the &amp;quot;eight most likely words&amp;quot; button) in
a four by two matrix and featured buttons for ac-
cess to alphabetical letter entry mode and the pri-
mary letter entry mode. The system remains in
word completion mode, until the user explicitly
exits, in order to encourage the user to use the
word predictions. This may not be the optimum
strategy for a dictation task, but the goal of the Ga-
zeTalk project is to accelerate a text composition
task. When engaged in a composition task, it&apos;s a
reasonable assumption that the user will often ac-
cept a synonym for the desired word, rather than
spending time explicitly requesting the desired
word. This feature may lead to so-called &amp;quot;parrot
speech&amp;quot;, i.e., that the text composed with the sys-
tem lacks the individual character that text pro-
duced by other means would exhibit, but as the
primary concern is to aid a disabled user who
struggles to keep up with normal spoken conversa-
tion (which is often conducted at rates in excess of
150 WPM) we consider this an acceptable design
trade-off.
The alphabetical letter entry mode enabled the
user to select the desired letter in a two-stage proc-
ess, by first selecting a group of letters (e.g.
&amp;quot;ABCDEFG&amp;quot;) containing the desired letter, and
then the letter.
</bodyText>
<sectionHeader confidence="0.99389" genericHeader="method">
3 Prototype tests of GazeTalk
</sectionHeader>
<bodyText confidence="0.997936846153846">
During the design process, we conducted experi-
ments and user evaluations to determine the initial
effectiveness of various designs. Our focus on ini-
tial performance is motivated by our concern that
early impressions of system effectiveness may
have a major impact on the users&apos; determination to
use a new system. User motivation is of highest
importance to AAC systems as they are often in-
troduced in periods of life crisis, e.g., during re-
covery from an accident or during a serious
progressive disease such as ALS.
In our first experiment we performed a be-
tween-subjects comparison of differences in per-
formance between mouse-click typing and mouse-
dwell typing on a Danish on-screen keyboard with
letter- and word prediction among novice users
(initial performance) (Hansen et al., 2003). Users
of dwell-time activation showed longer selection
times and larger overproduction rates than users of
mouse click activation. The average productivity in
terms of WPM was 5.51 and the overproduction
rate (unnecessary clicks and error corrections) was
16.9% for click selections, while the group of users
with dwell selections produced 4.79 WPM on av-
erage and had an overproduction rate of 26.2%
(Hansen et al., 2003).
</bodyText>
<figureCaption confidence="0.998718">
Figure 2: The Danish version of GazeTalk.
</figureCaption>
<bodyText confidence="0.999950095238095">
Our second experiment was a within-subjects
study of an input system for Hiragana and Kata-
kana (&amp;quot;Kana&amp;quot;) characters, which used the same
basic layout and UI-elements as the Danish system
(cf. Fig. 2 and 3). Each subject made dwell-time
selections using either a mouse or an eye-tracker.
The Japanese system had no prediction functions.
Instead, it displayed the characters in a conven-
tional, static, two-level hierarchical manner. The
purpose of the second experiment was to compare
mouse dwell selections with eye dwell selections at
an initial performance level. Selection by dwell-
time activation using mouse and eye-tracking in-
teraction was found to be almost equally fast, but
using the mouse as pointing device was far more
precise than eye-tracking. Consequently, the pro-
ductivity in terms of characters per minute (CPM)
was 33% higher when using the mouse, (22.1 CPM
versus 16.6 CPM) (Hansen et al., 2003).
The two experiments suggests that users in gen-
eral can be productive from the very first encounter
</bodyText>
<figure confidence="0.991810615384615">
Tale
ABCD...
Jog
Rblerne
Kari
Har
Det
Behandl
11.n
Redigeringl
Hoed menu
Mellentrum &amp;quot;
nOlin0 a
</figure>
<page confidence="0.997538">
63
</page>
<bodyText confidence="0.999964333333333">
with a dwell-time based system, but the productiv-
ity depends on the familiarity with the input dis-
play structure and the input mode (i.e., hand or
eye).
Based on a comparison of 20 similar sentences
in Danish and Japanese, we estimated that it takes
two Japanese characters to produce the equivalent
of one Danish word. Using this ratio, WPM can be
compared to CPM. There was a large difference
between the Danish average production of 4.78
WPM and a Japanese average production of 22.5
CPM, which equals 11.3 WPM.
</bodyText>
<figureCaption confidence="0.997944">
Figure 3: The Japanese version of GazeTalk.
</figureCaption>
<bodyText confidence="0.999474941176471">
The problems experienced by initial users of
highly ambiguous displays, as indicated by the
markedly lower productivity of Danish users who
used the predictive layout, when compared to the
Japanese users, who used a static layout, are most
likely primarily caused by confusion with regard to
the position of the desired letter. We assumed that
the users would appreciate that the letters were
positioned according to their current probability,
but that turned out not to be the case. Apparently
this placement strategy was counterintuitive, and
prevented them from developing an efficient strat-
egy for selecting letters, as they found themselves
constantly scanning the on-screen keyboard in or-
der to locate the desired letter, as evidenced by this
post-experiment comment from one of the sub-
jects:
</bodyText>
<construct confidence="0.91217225">
&amp;quot;It&apos;s a bit confusing that the letters change posi-
tion after each selection. You spot the &apos;T&apos; in one
position, and then you have to find it all over again
after the next selection.&amp;quot;
</construct>
<sectionHeader confidence="0.99548" genericHeader="method">
4 Future improvements
</sectionHeader>
<bodyText confidence="0.999981212121212">
Obviously, scanning for a target letter or word
among predicted candidates consumes time, and
may give rise to frustration. The design implica-
tions are that reduced keyboards should strive to
introduce regularities and invariants which con-
form to the expectations of the users in the dynam-
ics whenever possible. As a consequence, for the
next prototype we have decided to assign &amp;quot;home
positions&amp;quot; for all letters. Thus, a &amp;quot;T&amp;quot; will occur on
the same button whenever the prediction algorithm
determines it&apos;s a likely candidate for the next let-
ter, unless another letter that shares the same home
position has a higher probability. The placement
algorithm selects the distribution of positions that
minimizes the number of home position conflicts.
The home positions are assigned by sorting the
letters according to their static probability, and
then assigning them positions in a round-robin
fashion. Thus, given that the most common seven
letters according to frequency counts in the Danish
corpus are &amp;quot;ERTNDAI&amp;quot;, &apos;E&apos; and &apos;I&apos; share the same
home position. The purpose of this home position
assignment strategy is to minimize the number of
placement conflicts.
The present WPM rates achieved by novice
users of GazeTalk are compatible with the produc-
tivity found in e.g. SMS text-production (James
and Reischel, 2001) and compares well to the typ-
ing speed of other advanced AAC-systems with a
limited number of (large) buttons, e.g. (Kuhn and
Jam, 2001). However, we believe that a consider-
able increase in text production rates may be
achieved in two ways:
</bodyText>
<listItem confidence="0.9190555">
• By including a language model specific to the
individual user.
• By increasing the size of the corpus in the gen-
eral language model.
</listItem>
<bodyText confidence="0.999977666666667">
We have therefore developed a companion pro-
gram for GazeTalk, which allows the user to train
the user generated language model on any text ma-
terial which can be transferred to the Windows
clipboard or exported to ASCII or RTF format. As
most computer users maintain a personal archive in
the form of sent email and other documents, it is
feasible to use this as a personal corpus, with the
aid of out language model training tool.
Moreover, we will continue our work on ex-
panding the corpus and vocabulary. We expect to
use a much larger open-source vocabulary (Den
</bodyText>
<figure confidence="0.99712">
St1.1
(11VelEl)
tz
aSi-1 + —
61-1.
,•=r
*Fr, &apos;e,fiir
+4.
</figure>
<page confidence="0.993886">
64
</page>
<bodyText confidence="0.999885214285714">
Store Danske Ordliste - http://da.speling.org/),
which contains approximately 370.000 wordforms.
Also, we have expanded the base corpus by includ-
ing a much wider range of Usenet groups. We ex-
pect the resulting corpus to consist of
approximately 40 million words after application
of the previously mentioned heuristics. Expanding
the vocabulary and corpus by nearly two orders of
magnitude will also necessitate major revisions of
the implementation of the language model.
Conversion from Kana to Kanji (Chinese) ideo-
grams is a most immediate concern for the Japa-
nese users and will thus be implemented in the
next Japanese version of GazeTalk.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999950833333333">
Performance among first time users of the predic-
tion-based input system was not as high as we had
hoped, but the experimental measurements and
user comments have indicated several ways to im-
prove the system. Although we did not succeed in
realizing the inherent performance advantage of a
prediction-based input system, it is encouraging
that the users appeared to accept the dynamic input
configuration without any major reservations.
As an initial study of alternative input methods,
we consider the experiment a success, as it vali-
dated the usefulness of GazeTalk as a test bed for
alternative input configurations, and identified sev-
eral factors that impact novice user performance. It
is specifically worth noticing that novice user per-
formance appeared to be dominated by design re-
lated factors, rather than language technology
related issues.
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99692425">
The GazeTalk project is supported by The Danish
Ministry of Science,
The Nordic Academy for Advanced Study
(NorFA) provides grants for the first author.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867125">
Carlberger, J (1997) ; Design and Implementation of a
Probabilistic Word Prediction Program ; NADA re-
port TRITA-NA-E9751, Swedish Royal Institute of
Technology, dept. of Numerical Analysis and Com-
puter Science (NADA), 1997.
Darragh, John J., Witten, Ian H., James, Mark J. (1990)
&amp;quot;The Reactive Keyboard: A Predictive Typing Aid&amp;quot;,
IEEE Computer, 23(11):41-49, November 1990
Hansen, John Paulin ; Johansen, Anders Sewerin; Han-
sen, Dan Witzner; Itoh, Kenji ; Mashino, Saturo
(2003): Command Without a Click: User Studies of
Dwell Time Typing by Mouse and Eye-Gaze Selec-
tions. Paper available at: http://www.it-
c.dleresearch/EyeGazeInteraction/Papers/Hansen_et_
al_2003.pdf
Hutchinson, T. E., White, K. P., Martin, W. N., Rei-
chert, K. C. &amp; Frey, L. A. (1989) &amp;quot;Human-Computer
Interaction Using Eye-Gaze Input&amp;quot; IEEE Transac-
tions on Systems, Man and Cybernetics, Vol. 19, No.
6, November/December 1989, Pages 1527 — 1534.
Jacob, J. K. (1991) The Uses of Eye Movements in
Human-Computer Interaction Techniques: What You
Look At is What You Get, ACM Transactions on In-
formation Systems, Vol. 9, No. 3, April 1991, Pages
152— 169.
James, Christina L. ; Reischel, Kelly M. (2001) &amp;quot;Text
Input for Mobile Devices: Comparing Model Predic-
tion to Actual Performance&amp;quot;, SIGCHI&apos;01 March 31-
April 4, 2001, pp. 365-371, Seattle, WA, USA
Johansen, Anders S. &amp; Hansen, John P. (2002): Aug-
mentative and alternative communication: The future
of text on the move. Proceedings of 7th ERCIM
Workshop &amp;quot;User Interfaces for all&amp;quot;, 23 - 25 October
2002, Paris (Chantilly), France. Pages 367 - 386
Katz, A (1987) Estimation of probabilities from Sparse
Data for the Language Model Component of a
Speech Recognizer&amp;quot; IEEE Transactions on Acous-
tics, Speech and Signal Processing, VOL ASSP-35,
no. 3, March 1987
Kiihn, Michael ; Garbe, Jorn (2001) Predictive and
Highly Ambiguous Typing for a Severely Speech and
Motion Impaired User, Universal Access in Human-
Computer Interaction.Proc. of UAHCI 2001, New
Orleans, August, 5-10. Mahwah (NJ): Lawrence Erl-
baum Associates
Majaranta, P. &amp; Raba, K-J (2002) Twenty Years of Eye
Typing: Systems and Design Issues, Proceedings of
the Symposium on ETRA 2002: Eye Tracking Re-
search &amp; Applications Symposium 2002, New Or-
leans, Louisiana. Pages 15 — 22
Rau, H ; Skiena, S. S. (1996) Dialing for Documents:
An Experiment in Information Theory, Journal of
Visual Languages and Computing 7(1), pp. 79-95
Rosenfeld, R (1996) A Maximum Entropy Approach to
Adaptive Statistical Language Modelling, Computer,
Speech and Language, 10, 1996, pp. 187-288
</reference>
<page confidence="0.990372">
65
</page>
<reference confidence="0.9991246">
Shannon, C. E (1951) Prediction and Entropy of Printed
English, Bell Systems Technical Journal 30, January
1951, pp. 50-64
Tilbourg, H (1998) An Introduction to Cryptology,
1988, Kluwer Academic Publishers
Ward, David J. (2002) Adaptive Computer Interfaces,
Ph.D.-thesis, Inference Group, Cavendish Labora-
tory, University of Cambridge, November 2001,
http://www.inference.phy.cam.ac.uledjw30/papers/th
esis.html, verified January 13 2003
</reference>
<page confidence="0.98854">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.057766">
<title confidence="0.9963145">Language Technology in a Predictive, Restricted On-screen Keyboard with Dynamic Layout for Severely Disabled People</title>
<author confidence="0.9911965">Anders Sewerin John Paulin Hansen</author>
<email confidence="0.425835">Witzner</email>
<affiliation confidence="0.790423">IT University of Glentevej</affiliation>
<address confidence="0.997931">2400 Copenhagen,</address>
<email confidence="0.959291">dduck@it—c.dk</email>
<email confidence="0.959291">witzner@it—c.dk</email>
<author confidence="0.539138">Kenji Itoh</author>
<author confidence="0.539138">Satoru</author>
<affiliation confidence="0.724483">Tokyo Inst. of Oh-okayama</affiliation>
<address confidence="0.873953">Tokyo 152-8552</address>
<intro confidence="0.641092">iP</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Carlberger</author>
</authors>
<title>Design and Implementation of a Probabilistic Word Prediction Program ;</title>
<date>1997</date>
<tech>NADA report TRITA-NA-E9751,</tech>
<institution>Swedish Royal Institute of Technology, dept. of Numerical Analysis and Computer Science (NADA),</institution>
<contexts>
<context position="12196" citStr="Carlberger, 1997" startWordPosition="1954" endWordPosition="1955"> artifacts of missing or excessive punctuation; and discarding sentences that contained more than one or two out-of61 vocabulary words. This resulting corpus consists of approximately 467.000 words, a reduction of approximately 2/3 from the initial corpus. 2.4 Adaptive vocabulary As a means to increase the performance of the language model in face of a small vocabulary and training corpus, we decided to implement automatic collection and integration of vocabulary and n-grams during daily use. This feature has been reported to increase user acceptance and text production rate significantly in (Carlberger, 1997) and (Darragh et al., 1990). Additionally, measurements of cross training performance indicate that an indomain model performs significantly better than an out-of-domain model, even for very large training sets and very similar test sets (Rosenfeld, 1996). We confirmed this by performing perfect-user simulations, i.e., simulations where the number of selections needed to input a test corpus is computed on the assumption that the user will make no selection errors, and always use the most efficient input strategy in terms of the number of selections used. The user n-grams are integrated in the </context>
</contexts>
<marker>Carlberger, 1997</marker>
<rawString>Carlberger, J (1997) ; Design and Implementation of a Probabilistic Word Prediction Program ; NADA report TRITA-NA-E9751, Swedish Royal Institute of Technology, dept. of Numerical Analysis and Computer Science (NADA), 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Darragh</author>
<author>Ian H Witten</author>
<author>Mark J James</author>
</authors>
<title>The Reactive Keyboard: A Predictive Typing Aid&amp;quot;,</title>
<date>1990</date>
<journal>IEEE Computer,</journal>
<pages>23--11</pages>
<contexts>
<context position="12223" citStr="Darragh et al., 1990" startWordPosition="1957" endWordPosition="1960">r excessive punctuation; and discarding sentences that contained more than one or two out-of61 vocabulary words. This resulting corpus consists of approximately 467.000 words, a reduction of approximately 2/3 from the initial corpus. 2.4 Adaptive vocabulary As a means to increase the performance of the language model in face of a small vocabulary and training corpus, we decided to implement automatic collection and integration of vocabulary and n-grams during daily use. This feature has been reported to increase user acceptance and text production rate significantly in (Carlberger, 1997) and (Darragh et al., 1990). Additionally, measurements of cross training performance indicate that an indomain model performs significantly better than an out-of-domain model, even for very large training sets and very similar test sets (Rosenfeld, 1996). We confirmed this by performing perfect-user simulations, i.e., simulations where the number of selections needed to input a test corpus is computed on the assumption that the user will make no selection errors, and always use the most efficient input strategy in terms of the number of selections used. The user n-grams are integrated in the combined word level model b</context>
</contexts>
<marker>Darragh, Witten, James, 1990</marker>
<rawString>Darragh, John J., Witten, Ian H., James, Mark J. (1990) &amp;quot;The Reactive Keyboard: A Predictive Typing Aid&amp;quot;, IEEE Computer, 23(11):41-49, November 1990</rawString>
</citation>
<citation valid="false">
<authors>
<author>John Paulin Hansen</author>
</authors>
<title>Anders Sewerin; Hansen, Dan Witzner; Itoh, Kenji ; Mashino, Saturo (2003): Command Without a Click: User Studies of Dwell Time Typing by Mouse and Eye-Gaze Selections. Paper available at:</title>
<note>http://www.itc.dleresearch/EyeGazeInteraction/Papers/Hansen_et_ al_2003.pdf</note>
<marker>Hansen, </marker>
<rawString>Hansen, John Paulin ; Johansen, Anders Sewerin; Hansen, Dan Witzner; Itoh, Kenji ; Mashino, Saturo (2003): Command Without a Click: User Studies of Dwell Time Typing by Mouse and Eye-Gaze Selections. Paper available at: http://www.itc.dleresearch/EyeGazeInteraction/Papers/Hansen_et_ al_2003.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>T E Hutchinson</author>
<author>K P White</author>
<author>W N Martin</author>
<author>K C Reichert</author>
<author>L A Frey</author>
</authors>
<title>Human-Computer Interaction Using Eye-Gaze Input&amp;quot;</title>
<date>1989</date>
<journal>IEEE Transactions on Systems, Man and Cybernetics,</journal>
<volume>19</volume>
<pages>1527--1534</pages>
<marker>Hutchinson, White, Martin, Reichert, Frey, 1989</marker>
<rawString>Hutchinson, T. E., White, K. P., Martin, W. N., Reichert, K. C. &amp; Frey, L. A. (1989) &amp;quot;Human-Computer Interaction Using Eye-Gaze Input&amp;quot; IEEE Transactions on Systems, Man and Cybernetics, Vol. 19, No. 6, November/December 1989, Pages 1527 — 1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Jacob</author>
</authors>
<title>The Uses of Eye Movements in Human-Computer Interaction Techniques: What You Look At is What You Get,</title>
<date>1991</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>9</volume>
<pages>152--169</pages>
<contexts>
<context position="1138" citStr="Jacob, 1991" startWordPosition="175" endWordPosition="176">m amyotrophic lateral sclerosis (ALS), also known as Lou Gehrig&apos;s disease, who have lost their voice and mobility and may only be able to move their eyes. To make the tool widely available, we use standard PCs and low-resolution, offthe-shelf digital cameras to track eye positions. As the precision of eye-tracking is very dependent on the quality of the cameras used, we decided on a basic layout, which consisted of few, large onscreen buttons. In many cases eye-tracking systems have to deal with the &amp;quot;Midas Touch&amp;quot; problem: Everywhere you look, a command may be activated without your intention (Jacob, 1991). A dwell time activation principle has been the preferred solution to this problem since the very first eye-tracking based communication systems (Hutchinson, 1989; Majaranta and Raiha, 2002). The dwell time period is typically between 500 and 1500 milliseconds. 1.1 GazeTalk —goals and design The goal of the GazeTalk project is to develop an eye-tracking based Augmented and Alternative Communication (AAC) system that supports several languages, facilitates fast text entry and is both sufficiently feature-complete to be deployed as the primary AAC tool for users, yet sufficiently flexible and t</context>
</contexts>
<marker>Jacob, 1991</marker>
<rawString>Jacob, J. K. (1991) The Uses of Eye Movements in Human-Computer Interaction Techniques: What You Look At is What You Get, ACM Transactions on Information Systems, Vol. 9, No. 3, April 1991, Pages 152— 169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina L James</author>
</authors>
<title>Text Input for Mobile Devices: Comparing Model Prediction to Actual Performance&amp;quot;,</title>
<date>2001</date>
<volume>31</volume>
<pages>365--371</pages>
<location>Seattle, WA, USA</location>
<marker>James, 2001</marker>
<rawString>James, Christina L. ; Reischel, Kelly M. (2001) &amp;quot;Text Input for Mobile Devices: Comparing Model Prediction to Actual Performance&amp;quot;, SIGCHI&apos;01 March 31-April 4, 2001, pp. 365-371, Seattle, WA, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders S Johansen</author>
<author>John P Hansen</author>
</authors>
<title>Augmentative and alternative communication: The future of text on the move.</title>
<date>2002</date>
<booktitle>Proceedings of 7th ERCIM Workshop &amp;quot;User Interfaces for</booktitle>
<pages>23--25</pages>
<location>Paris (Chantilly), France.</location>
<contexts>
<context position="3635" citStr="Johansen and Hansen, 2002" startWordPosition="568" endWordPosition="571">bsystem, that supports the use of either external (usually synthetic) voice, using the SAPI API or the Windows clipboard, or an internally developed digitized voice system. This digitized voice is currently only available in Danish, and has a vocabulary of approximately 33.000 words. The motivation for the project was to explore and eventually deploy eye-tracking as an input modality for ALS patients, and to explore issues in text input in constrained user interfaces (UIs), such as mobile devices. We explore the relationship between text input in AAC systems and mobile devices extensively in (Johansen and Hansen, 2002). The GazeTalk system consists of three elements: • The actual GazeTalk program, which implements the basic UI elements and functionality (email, voice output, saving and loading documents etc.). • A layout editor — StateEditor — which is used for designing and building the layouts used by GazeTalk. • A library of layouts built with StateEditor, which are designed for various situations (deployment, research etc.). All layouts in the library use the four by three matrix format shown in Fig. 1, which allows for a maximum of 12 active on-screen buttons. GazeTalk and StateEditor support various t</context>
</contexts>
<marker>Johansen, Hansen, 2002</marker>
<rawString>Johansen, Anders S. &amp; Hansen, John P. (2002): Augmentative and alternative communication: The future of text on the move. Proceedings of 7th ERCIM Workshop &amp;quot;User Interfaces for all&amp;quot;, 23 - 25 October 2002, Paris (Chantilly), France. Pages 367 - 386</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Katz</author>
</authors>
<title>Estimation of probabilities from Sparse Data for the Language Model Component of a Speech Recognizer&amp;quot;</title>
<date>1987</date>
<booktitle>IEEE Transactions on Acoustics, Speech and Signal Processing, VOL ASSP-35,</booktitle>
<volume>3</volume>
<contexts>
<context position="9435" citStr="Katz, 1987" startWordPosition="1522" endWordPosition="1523">tion ignores the additional cognitive load placed on the user by the need to search for the desired letter or word. However, as the advantage of the probabilistic keyboard is potentially very big (e.g., 1.2 selections per symbol compared to two selections on a two-level static, hierarchical system, given that the prediction algorithm includes the desired letter among the top six candidates 90% of the time), we decided to base our prototype on a probabilistic keyboard. Both the word- and letter-level language models in the current version use a Katz-style back-off Markov model as described in (Katz, 1987). The actual letter predictions are generated by a back-off algorithm, which consults both the word- and letter-level models. As the word-level predictions are more likely to be correct, since they are based on a higher-level context than the letter based model, the letter prediction algorithm gives higher priority to the predictions from the word level model. 2.3 Corpus construction As we did not at the time have access to either dictionaries or corpora, we decided to construct our own. The current Danish language model is based on a corpus collected from the Danish Usenet discussion groups, </context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, A (1987) Estimation of probabilities from Sparse Data for the Language Model Component of a Speech Recognizer&amp;quot; IEEE Transactions on Acoustics, Speech and Signal Processing, VOL ASSP-35, no. 3, March 1987</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kiihn</author>
</authors>
<title>Predictive and Highly Ambiguous Typing for a Severely Speech and Motion Impaired User,</title>
<date>2001</date>
<booktitle>Universal Access in HumanComputer Interaction.Proc. of UAHCI 2001,</booktitle>
<location>New Orleans,</location>
<marker>Kiihn, 2001</marker>
<rawString>Kiihn, Michael ; Garbe, Jorn (2001) Predictive and Highly Ambiguous Typing for a Severely Speech and Motion Impaired User, Universal Access in HumanComputer Interaction.Proc. of UAHCI 2001, New Orleans, August, 5-10. Mahwah (NJ): Lawrence Erlbaum Associates</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Majaranta</author>
<author>K-J Raba</author>
</authors>
<title>Twenty Years of Eye Typing: Systems and Design Issues,</title>
<date>2002</date>
<journal>Pages 15 —</journal>
<booktitle>Proceedings of the Symposium on ETRA 2002: Eye Tracking Research &amp; Applications Symposium</booktitle>
<volume>22</volume>
<location>New Orleans, Louisiana.</location>
<marker>Majaranta, Raba, 2002</marker>
<rawString>Majaranta, P. &amp; Raba, K-J (2002) Twenty Years of Eye Typing: Systems and Design Issues, Proceedings of the Symposium on ETRA 2002: Eye Tracking Research &amp; Applications Symposium 2002, New Orleans, Louisiana. Pages 15 — 22</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Skiena</author>
</authors>
<title>Dialing for Documents: An Experiment in Information Theory,</title>
<date>1996</date>
<journal>Journal of Visual Languages and Computing</journal>
<volume>7</volume>
<issue>1</issue>
<pages>79--95</pages>
<marker>Skiena, 1996</marker>
<rawString>Rau, H ; Skiena, S. S. (1996) Dialing for Documents: An Experiment in Information Theory, Journal of Visual Languages and Computing 7(1), pp. 79-95</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A Maximum Entropy Approach to Adaptive Statistical Language Modelling,</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<volume>10</volume>
<pages>187--288</pages>
<contexts>
<context position="6287" citStr="Rosenfeld, 1996" startWordPosition="996" endWordPosition="997">. 2.1 Language models Measurements of the information content of written language indicate that, given a sufficiently efficient language model, only approximately one bit of information is needed per character (Shannon, 1951). Language models can be based on many different natural language processing (NLP) algorithms with varying results in terms of performance, capabilities and resource consumption. A simple language model (frequency count) achieves an entropy of 4.03 bits per character (Shannon, 1951), and an advanced state-of-the-art model (maximum entropy) achieves 1.2 bits per character (Rosenfeld, 1996). The traditional n-gram approach is relatively high performing at 1.5 bits per character (Tilbourg, 1998). David J. Ward tabulates the performance of most current approaches and discusses the construction of language models extensively in (Ward, 2002). Most language models need to be primed in order to perform optimally. In the case of a trigram model, which predicts a word on the basis of the two preceding words, it needs to be primed with two words in order to supply any predictions at all. In other words, most (possibly all) language models will supply unreliable predictions for the first </context>
<context position="12451" citStr="Rosenfeld, 1996" startWordPosition="1991" endWordPosition="1992"> 2.4 Adaptive vocabulary As a means to increase the performance of the language model in face of a small vocabulary and training corpus, we decided to implement automatic collection and integration of vocabulary and n-grams during daily use. This feature has been reported to increase user acceptance and text production rate significantly in (Carlberger, 1997) and (Darragh et al., 1990). Additionally, measurements of cross training performance indicate that an indomain model performs significantly better than an out-of-domain model, even for very large training sets and very similar test sets (Rosenfeld, 1996). We confirmed this by performing perfect-user simulations, i.e., simulations where the number of selections needed to input a test corpus is computed on the assumption that the user will make no selection errors, and always use the most efficient input strategy in terms of the number of selections used. The user n-grams are integrated in the combined word level model by giving priority to n-grams on the basis of length and source. Thus, e.g., the combined model will consider predictions based on user generated trigrams more precise than those based on trigrams from the base model, but predict</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>Rosenfeld, R (1996) A Maximum Entropy Approach to Adaptive Statistical Language Modelling, Computer, Speech and Language, 10, 1996, pp. 187-288</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>Prediction and Entropy of Printed English,</title>
<date>1951</date>
<journal>Bell Systems Technical Journal</journal>
<volume>30</volume>
<pages>50--64</pages>
<contexts>
<context position="5896" citStr="Shannon, 1951" startWordPosition="940" endWordPosition="941">estriction on the number of UI elements available in the basic layout. Since we could not expect to attain a level of performance from the eye-tracking system that would allow us to use a full-size onscreen keyboard, we decided to investigate alternative approaches. Language technology emerged as a potential means to provide fast text input, despite the small number of on-screen buttons. 2.1 Language models Measurements of the information content of written language indicate that, given a sufficiently efficient language model, only approximately one bit of information is needed per character (Shannon, 1951). Language models can be based on many different natural language processing (NLP) algorithms with varying results in terms of performance, capabilities and resource consumption. A simple language model (frequency count) achieves an entropy of 4.03 bits per character (Shannon, 1951), and an advanced state-of-the-art model (maximum entropy) achieves 1.2 bits per character (Rosenfeld, 1996). The traditional n-gram approach is relatively high performing at 1.5 bits per character (Tilbourg, 1998). David J. Ward tabulates the performance of most current approaches and discusses the construction of </context>
</contexts>
<marker>Shannon, 1951</marker>
<rawString>Shannon, C. E (1951) Prediction and Entropy of Printed English, Bell Systems Technical Journal 30, January 1951, pp. 50-64</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tilbourg</author>
</authors>
<title>An Introduction to Cryptology,</title>
<date>1998</date>
<publisher>Kluwer Academic Publishers</publisher>
<contexts>
<context position="6393" citStr="Tilbourg, 1998" startWordPosition="1012" endWordPosition="1013">fficiently efficient language model, only approximately one bit of information is needed per character (Shannon, 1951). Language models can be based on many different natural language processing (NLP) algorithms with varying results in terms of performance, capabilities and resource consumption. A simple language model (frequency count) achieves an entropy of 4.03 bits per character (Shannon, 1951), and an advanced state-of-the-art model (maximum entropy) achieves 1.2 bits per character (Rosenfeld, 1996). The traditional n-gram approach is relatively high performing at 1.5 bits per character (Tilbourg, 1998). David J. Ward tabulates the performance of most current approaches and discusses the construction of language models extensively in (Ward, 2002). Most language models need to be primed in order to perform optimally. In the case of a trigram model, which predicts a word on the basis of the two preceding words, it needs to be primed with two words in order to supply any predictions at all. In other words, most (possibly all) language models will supply unreliable predictions for the first few words or characters of input. As a conse60 quence, when designing text input systems based on language</context>
</contexts>
<marker>Tilbourg, 1998</marker>
<rawString>Tilbourg, H (1998) An Introduction to Cryptology, 1988, Kluwer Academic Publishers</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Ward</author>
</authors>
<title>Adaptive Computer Interfaces, Ph.D.-thesis,</title>
<date>2002</date>
<institution>Inference Group, Cavendish Laboratory, University of Cambridge,</institution>
<note>http://www.inference.phy.cam.ac.uledjw30/papers/th esis.html, verified</note>
<contexts>
<context position="6539" citStr="Ward, 2002" startWordPosition="1034" endWordPosition="1035">n many different natural language processing (NLP) algorithms with varying results in terms of performance, capabilities and resource consumption. A simple language model (frequency count) achieves an entropy of 4.03 bits per character (Shannon, 1951), and an advanced state-of-the-art model (maximum entropy) achieves 1.2 bits per character (Rosenfeld, 1996). The traditional n-gram approach is relatively high performing at 1.5 bits per character (Tilbourg, 1998). David J. Ward tabulates the performance of most current approaches and discusses the construction of language models extensively in (Ward, 2002). Most language models need to be primed in order to perform optimally. In the case of a trigram model, which predicts a word on the basis of the two preceding words, it needs to be primed with two words in order to supply any predictions at all. In other words, most (possibly all) language models will supply unreliable predictions for the first few words or characters of input. As a conse60 quence, when designing text input systems based on language models, one must allow for the possibility that the predictions are wrong. One of the most common problems when using word-level language models </context>
</contexts>
<marker>Ward, 2002</marker>
<rawString>Ward, David J. (2002) Adaptive Computer Interfaces, Ph.D.-thesis, Inference Group, Cavendish Laboratory, University of Cambridge, November 2001, http://www.inference.phy.cam.ac.uledjw30/papers/th esis.html, verified January 13 2003</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>