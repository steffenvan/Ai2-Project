<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000447">
<title confidence="0.998302">
Using Word Dependent Transition Models in HMM based Word
Alignment for Statistical Machine Translation
</title>
<author confidence="0.994463">
Xiaodong He
</author>
<affiliation confidence="0.968421">
Microsoft Research
</affiliation>
<address confidence="0.956247">
One Microsoft Way
Redmond, WA 98052 USA
</address>
<email confidence="0.999351">
xiaohe@microsoft.com
</email>
<sectionHeader confidence="0.996667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871388888889">
In this paper, we present a Bayesian Learn-
ing based method to train word dependent
transition models for HMM based word
alignment. We present word alignment re-
sults on the Canadian Hansards corpus as
compared to the conventional HMM and
IBM model 4. We show that this method
gives consistent and significant alignment
error rate (AER) reduction. We also con-
ducted machine translation (MT) experi-
ments on the Europarl corpus. MT results
show that word alignment based on this
method can be used in a phrase-based ma-
chine translation system to yield up to 1%
absolute improvement in BLEU score,
compared to a conventional HMM, and
0.8% compared to a IBM model 4 based
word alignment.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999609">
Word alignment is an important step of most
modern approaches to statistical machine
translation (Koehn et al., 2003). The classical
approaches to word alignment are based on IBM
models 1-5 (Brown et al., 1994) and the HMM
based alignment model (Vogel et al., 1996) (Och
and Ney, 2000a, 2000b), while recently
discriminative approaches (Moore, 2006) and
syntax based approaches (Zhang and Gildea, 2005)
for word alignment are also studied. In this paper,
we present improvements to the HMM based
alignment model originally proposed by (Vogel et
al., 1996, Och and Ney, 2000a).
Although HMM based word alignment ap-
proaches give good performance, one weakness of
it is the coarse transition models. In the HMM
based alignment model (Vogel et al., 1996), it is
assumed that the HMM transition probabilities de-
pend only on the jump width from the last state to
the next state. Therefore, the knowledge of transi-
tion probabilities given a particular source word e
is not sufficiently modeled.
In order to improve transition models in the
HMM based alignment, Och and Ney (2000a) ex-
tended the transition models to be word-class de-
pendent. In that approach, words of the source lan-
guage are first clustered into a number of word
classes, and then a set of transition parameters is
estimated for each word class. In (2002), Toutano-
va et al. modeled self-transition (i.e., jump width is
zero) probability separately from other transition
probabilities. A word dependent self-transition
model P(stay|e) is introduced to decide whether to
stay at the current source word e at the next step, or
jump to a different word. It was also shown that
with the assumption that a source word with fertili-
ty greater than one generates consecutive words in
the target language, this probability approximates
fertility modeling. Deng and Byrne in (2005) im-
proved this idea. They proposed a word-to-phrase
HMM in which a source word dependent phrase
length model is used to model the approximate
fertility, i.e., the length of consecutive target words
generated by the source word. It provides more
powerful modeling of approximate fertility than
the single P(stay|e) parameter.
However, these methods only model the proba-
bility of state occupancy rather than a full set of
transition probabilities. Important knowledge of
jumping from e to another position, e.g., jumping
</bodyText>
<page confidence="0.973813">
80
</page>
<note confidence="0.894214">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 80–87,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.998084904761905">
forward (monotonic alignment) or jumping back-
ward (non-monotonic alignment), is not modeled.
In this paper, we present a method to further im-
prove the transition models for HMM alignment
model. For each source word e, we not only model
its self-transition probability, but also the probabil-
ity of jumping from word e to a different word. For
this purpose, we estimate a full transition model
for each source word.
A key problem for detailed word-dependent
transition modeling is data sparsity. In (Toutanova
et al., 2002), the word dependent self-transition
probability P(stay|e) is interpolated with the global
HMM self-transition probability to alleviate the
data sparsity problem, where an interpolation
weight is used for all words and that weight is
tuned on a hold-out set. In the proposed word de-
pendent transition model, because there are a large
number of parameters to estimate, the data sparsity
problem is even more severe. Moreover, since the
sparsity of different words are very different, it is
difficult to find a one-size-fits-all interpolation
weight, and therefore simple linear interpolation is
not optimal. In order to address this problem, we
use Bayesian learning so that the transition model
parameters are estimated by maximum a posteriori
(MAP) training. With the help of the prior distribu-
tion of the model, the training is regularized and
results in robust models.
In the next section we briefly review modeling
of transition probabilities in a conventional HMM
alignment model (Vogel et al., 1996, Och and Ney,
2000a). Then we describe the equations of MAP
training for word dependent transition models. In
section 5, we present word alignment results that
show significant alignment error rate reductions
compared to the baseline HMM and IBM model 4.
We also conducted phrase-based machine transla-
tion experiments on the Europarl corpus, English –
French track, and shown that the proposed method
can lead to significant BLEU score improvement
compared to the HMM and IBM model 4.
</bodyText>
<sectionHeader confidence="0.995035" genericHeader="method">
2 Baseline HMM alignment model
</sectionHeader>
<bodyText confidence="0.999621">
We briefly review the HMM based word alignment
models (Vogel, 1996, Och and Ney, 2000a). Let’s
</bodyText>
<equation confidence="0.922682142857143">
denote by 1 (1 ,..., )
f = f fJ as the French sentence,
Je = e eI as the English sentence, and
1 ( 1 ,..., )
Ia = a aJ as the alignment that specifies the
1 ( 1,..., )
J
</equation>
<bodyText confidence="0.986419333333333">
position of the English word aligned to each
French word. In the HMM based word alignment,
a HMM is built at English side, i.e., each (position,
word) pair, ( , )
aj eaj , is a HMM state, which emits
the French word fj. In order to mitigate the sparse
data problem, it is assumed that the emission prob-
ability only depends on the English word, i.e.,
p(fj  |aj,eaj) = p(fj  |eaj ) , and the transition prob-
ability only depends on the position of the last
state and the length of the English sentence, i.e.,
p aj aj eaj I p aj aj I
</bodyText>
<equation confidence="0.937318785714286">
(  |, , ) (  |1 , )
= . Then, Vogel et
−1 −
−1
al. (1996) give
J
p(fJ  |e;)=1:∏[p(aj  |aj−,I)p(fj  |eaj)
a j
J =1
1
In the HMM of (Vogel et al., 1996), it is further
assumed these transition probabilities
p(aj = i  |aj−1 = i′, I) depend only on the jump
width (i - i&apos;), i.e.,
</equation>
<bodyText confidence="0.9517755">
Therefore, the transition probability
p(aj  |aj−1,I) depends on aj-1 but only through the
distortion set {c(i - i&apos;)}.
In (Och and Ney, 2000a), the word null is intro-
duced to generate the French words that don&apos;t align
to any English words. If we denote by j_ the posi-
tion of the last French word before j that aligns to a
non-null English word, the transition probabilities
p(aj = i  |aj−1 = i′,I) in (1) is computed as
p(aj =i|aj_=i′,I)=p%(i|i′,I), where
</bodyText>
<equation confidence="0.9985396">
I p if 0
i =
0
p i i I
(  |, )
</equation>
<bodyText confidence="0.980181">
′ =i(1−p0)⋅p(i|i′j) otherwise
state i=0 denotes the state of a null word at the
English side, and p0 is the probability of jumping
to state 0, which is estimated from hold-out data.
For convenience, we denote by
Λ ={p(i  |i′,I),p(fj  |ei)} the HMM parameter set.
</bodyText>
<equation confidence="0.9967321">
p
c(i −i)
(2)
(i  |i′, I )
c(l − i)
1
I
1:
l
I (1)
</equation>
<page confidence="0.984967">
81
</page>
<bodyText confidence="0.963329666666667">
In the training stage, A are usually estimated
through maximum likelihood (ML) training, i.e.,
argmax ( 1  |1 , )
</bodyText>
<equation confidence="0.9520556">
p f e
J I
Λ = Λ (3)
ML
Λ
</equation>
<bodyText confidence="0.9990717">
and the efficient Expectation-Maximization al-
gorithm can be used to optimize A iteratively until
convergence (Rabiner 1989).
For the interest of this paper, we elaborate tran-
sition parameter estimation with more details.
These transition probabilities {p(i  |i ′,I)} is a mul-
tinomial distribution estimated according to (2),
where at each iteration the distortion set {c(i - i&apos;)}
is the fractional count of transitions with jump
width d = i - i&apos;, i.e.,
</bodyText>
<equation confidence="0.9981748">
J I
−1
c(d) =EEPr(aj =i,aj+1 = i + d  |fJ,e1,Λ) (4)
j=
1 i=1
</equation>
<bodyText confidence="0.999933125">
where A&apos; is the model obtained from the immediate
previous iteration and these terms in (4) can be
efficiently computed by using the Forward-
Backward algorithm (Rabiner 1989). In practice,
we can bucket the distortion parameters {c(d)} into
a few buckets as implemented in (Liang et al.,
2006). In our implementation, 15 buckets are used
for c(≤-7), c(-6), ... c(0), ..., c(≥7). The probability
mass for transitions with jump width larger than 6
is uniformly divided. As suggested in (Liang et al.,
2006), we also use two separate sets of distortion
parameters for transitioning into the first state, and
for transitioning out of the last state, respectively.
Finally, we further smooth transition probabilities
with a uniform distribution as described in (Och
and Ney, 2000a),
</bodyText>
<equation confidence="0.989261428571428">
1
p a a I
′( j |j , ) =α ⋅ + −α ⋅
(1 ) (  |, )
p aj aj I .
_ _
I
</equation>
<bodyText confidence="0.925389666666667">
After training, Viterbi decoding is used to find
the best alignment sequence 1ˆJ
a . i.e.,
</bodyText>
<sectionHeader confidence="0.985251" genericHeader="method">
3 Word-dependent transition models in
HMM based alignment model
</sectionHeader>
<bodyText confidence="0.999416846153846">
As discussed in the previous sections, conventional
transition models that only depend on source word
positions are not accurate enough. There are only
limited distortion parameters to model the transi-
tion between HMM states for all English words,
and the knowledge of transition probabilities given
a particular source word is not represented. In or-
der to improve the transition model in HMM, we
extend the transition probabilities to be word de-
pendent so that the probability of jumping from
state aj_to aj not only depends on aj_, but also de-
pends on the English word at position aj_. This
gives
</bodyText>
<equation confidence="0.92046225">
J
p(.fJ  |e1) =E∏[p(aj  |aj_,eaj ,I)p(fj  |ea)
a j
J =1
1
Compared to (1), we need to estimate the transition
parameter (  |_ , _ , )
p aj aj eaj I which is
</equation>
<bodyText confidence="0.996433">
dent. Correspondingly, the HMM parameters we
need to estimate are Λ = { p(i  |i′, ei′ , I), p(fj  |ei )} ,
which provides a much richer set of free parame-
ters to model transition probabilities.
</bodyText>
<sectionHeader confidence="0.9732565" genericHeader="method">
4 Bayesian Learning for word-dependent
transition models
</sectionHeader>
<subsectionHeader confidence="0.99975">
4.1 Maximum a posteriori training
</subsectionHeader>
<bodyText confidence="0.9993245">
Using ML training, we can obtain the estimation
formula for word dependent transition probabilities
</bodyText>
<equation confidence="0.989009">
{p(i  |i′,e,I)} similar as (2), i.e.,
pML(i|i′,e,I)= Ic(i−i;e) (5)
E c(l − i′; e)
l=1
</equation>
<bodyText confidence="0.887727666666667">
where at each training iteration the word dependent
distortion set {c(i - i&apos;;e)} is computed by
c (d ; e)
</bodyText>
<equation confidence="0.997780916666667">
J I
−1
EE δ e e a i a i d f e
J I
Λ ′
= = = +  |, , )
( )Pr( ,
j
a j j+ 1 1 1
j=
1 i=1
(6)
</equation>
<bodyText confidence="0.99993">
where d = i - i&apos; is the jump width, and ( )
δ eaj = e is
the Kronecker delta function that equals one if
eaj = e, and zero otherwise.
However, for many non-frequent words, the
data samples for c(d;e) is very limited and there-
fore may lead to a biased model that severely over-
fits to the sparse data. In order to address this issue,
maximum a posteriori (MAP) framework is ap-
plied (Gauvain and Lee, 1994). In MAP training,
an appropriate prior distribution is used to incorpo-
</bodyText>
<figure confidence="0.9952179">
J
aˆi =argmax∏[p(aj  |aj _,I)p(fj  |eaj)
aJ1
=1
j
.
I
.
I
eaj _ depen-
</figure>
<page confidence="0.981476">
82
</page>
<bodyText confidence="0.990032">
rate prior knowledge into the model parameter es-
timation,
</bodyText>
<equation confidence="0.990921">
argmax ( 1  |1 , ) (  |1 )
J I Λ Λ (7)
I
Λ = p f e g e
MAPΛ
</equation>
<bodyText confidence="0.953781">
where the prior distribution (  |1 )
</bodyText>
<equation confidence="0.9092715">
g Λ e characterizes
I
</equation>
<bodyText confidence="0.9980934">
the distribution of the model parameter set Λ giv-
en the English sentence. The relation between ML
and MAP estimation is through the Bayes&apos; theorem
where the posterior distribution
p Λ f e ∝ p f e Λ g Λ e
</bodyText>
<equation confidence="0.9729622">
(  |1 , 1 ) ( 1  |1 , ) (  |1 )
J I J I I , and
p f e Λ is the likelihood function.
( 1  |1 , )
J I
</equation>
<bodyText confidence="0.99967075">
In transition model estimation, the transition
model { p(i  |i′, ei′ , I )} is a multinomial distribution.
Its conjugate prior distribution is a Dirichlet distri-
bution taking the following form (Bishop 2006),
</bodyText>
<equation confidence="0.998181222222222">
I
g p i i e I e
( (  |, , )  |) ∝∏ ′
′ i ′ 1 i ′
I 1
p i i e I ′ −
(  |, , ) i i
v , (8)
i 1
</equation>
<bodyText confidence="0.998059571428571">
where{vi′ ,i } is the set of hyper-parameters of the
prior distribution. Note that for mathematic tracta-
bility, vi′,i needs to be greater than 1, which is
usually the case in practice.
Substitute (8) into (7) and using EM algorithm,
we can obtain the iterative MAP training formula
for transition models (Gauvain and Lee, 1994)
</bodyText>
<equation confidence="0.964925">
E c(l −i;e)+ E vi′,l − I
l =1 l=1
</equation>
<subsectionHeader confidence="0.6150615">
4.2 Setting hyper-parameters for the prior
distribution
</subsectionHeader>
<bodyText confidence="0.9997334">
In Bayesian learning, the hyper-parameter set
{vi′,i} of the prior distribution is assumed known
based on a subjective knowledge about the model.
In our method, we set the prior with word-
independent transition probabilities.
</bodyText>
<equation confidence="0.589868">
vi′,i =τ ⋅ p(i  |i′,I)+1 (10)
</equation>
<bodyText confidence="0.9999028">
where r is a positive parameter that needs to tune
on a hold-out data set. We will investigate the ef-
fect of r with experimental results in later sections.
Substituting (10) into (9), the MAP based transi-
tion model training formula becomes
</bodyText>
<equation confidence="0.997957">
pMAP(i  |i′,e,I)= c(i−Ii′;e)+τ⋅p(i  |i′, I) (11)
E c(l − i′; e) +τ
l=1
</equation>
<bodyText confidence="0.944892176470588">
Note that for frequent words that have a large
amount of data samples for c(d;e), the sum of
El= Ic l − i′ e is large, so that pMAP (i  |i′, e, I) is
1,..., ( ; )
dominated by the data distribution. For rare words
that have low counts of c(d;e), pMAP (i  |i′, e, I) will
approach to the word independent model. On the
other hand, for the same word, when a small r is
used, a weak prior is applied, and the transition
probability is more dependent on the training data
of that word. When r becomes larger and larger, a
stronger prior knowledge is applied, and the word
dependent transition model will approach to the
word-independent transition model. Therefore, we
can vary the parameter r to control the contribution
of prior distribution in model training and tune the
word alignment performance.
</bodyText>
<sectionHeader confidence="0.999484" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.9610265">
5.1 Word alignment on the Canadian Han-
sards English-French corpus
</subsectionHeader>
<bodyText confidence="0.9998545">
We evaluated our word dependent transition mod-
els for HMM based word alignment on the Eng-
lish-French Hansards corpus. Only a subset of
500K sentence pairs was used in our experiments
including 447 test sentence-pairs. Tests sentence-
pairs were manually aligned and were marked with
both sure and possible alignments (Och and Ney
2000a). Using this annotation, we report the word
alignment performance in terms of alignment error
rate (AER) as defined by Och and Ney (2000a):
</bodyText>
<equation confidence="0.631517">
AER=1− |A∩S|+|A∩P |(12)
|A|+|S|
</equation>
<bodyText confidence="0.9999704">
where S denotes the set of sure gold alignments, P
denotes the set of possible gold alignments, A de-
notes the set of alignments generated by the word
alignment method under test.
We first trained the IBM model 1 and then a
baseline HMM model as described in section 2 on
the Hansards corpus. As the common practice, we
initialized the translation probabilities of model 1
with uniform distribution over word pairs occur
together in a same sentence pair. HMM was initia-
</bodyText>
<equation confidence="0.996849857142857">
pMAP(i  |i′,e,I) = I I
c i i e v
( ; )
− ′ + −1
i i
′ ,
(9)
</equation>
<page confidence="0.983792">
83
</page>
<bodyText confidence="0.999895433962264">
lized with uniform transition probabilities and
model 1 translation probabilities. Both model 1 and
HMM were trained with 5 iterations. For the pro-
posed word dependent transition model based
HMM (WDHMM), we used the same settings as
the HMM baseline except that the transition prob-
ability is computed according to (11). We also
trained IBM model 4 using GIZA++ provided by
Och and Ney (2000c), where 5 iterations of model
4 training was performed after 5 iterations of mod-
el 1 plus 5 iterations of HMM.
The effect of hyper-parameters in the prior dis-
tribution for WDHMM is shown in Figure 1. The
horizontal dot line represents the AER given by the
baseline HMM. The dash-line curve represents the
AERs of WDHMM given different r’s. We vary
the value of r in the range from 0 to 1E5 and
present that range in a log-scale in the figure. Since
r = 0 is not a valid value in the log domain, we ac-
tually use the left-most point in the figure to
represent the case of r = 0. From Fig. 1 it is shown
that when r is zero, we actually use the ML trained
word-dependent transition model. Due to the
sparse data problem, the model is poorly estimated
and lead to a high AER. When increase r to a larg-
er value, a stronger prior is applied to give a more
robust model. Then in a large range
of τ ∈ [100,2000] , WDHMM outperforms baseline
HMM significantly. When r gets even larger, MAP
model training becomes being over-dominated by
the prior distribution, and that eventually results in
a performance approaching to that of the baseline
HMM. Fig. 1 only presents AER results that are
calculated after combination of word alignments of
both E→F and F→E directions based on a set of
heuristics proposed by Och and Ney (2000b). We
have observed the similar trend of AER change for
the E→F and F→E alignment directions, respec-
tively. However, due to the limit of the space, we
didn’t include them in this paper.
In table 1-3, we give a detailed comparison be-
tween baseline HMM, WDHMM (with r = 1000),
and IBM model 4. Compared to the baseline
HMM, the proposed WDHMM can reduce AER by
more than 13%. It even outperforms IBM model 4
after two direction word alignment combination.
Meanwhile we noticed that although IBM model 4
gives superior performance over the baseline
HMM on both of the two alignment directions, its
AER after combination is almost the same as that
of the baseline HMM. We hypothesize that it may
due to the modeling mechanism difference be-
tween HMM and model 4.
</bodyText>
<equation confidence="0.804787">
log10(tau)
</equation>
<figureCaption confidence="0.989556166666667">
Figure 1: The AER of HMM baseline and the AER
of WDHMM as the prior parameter r is varied from 0 to
1E5. Note that the x axis is in log scale and we use the
left-most point in the figure to represent the case of r =
0. These results are calculated after combination of
word alignments of both E→F and F→E directions.
</figureCaption>
<table confidence="0.990637333333333">
model E → F F → E combined
baseline HMM 12.7 13.7 9.8
WDHMM 11.6 12.7 8.5
(τ = 1000)
IBM model 4 11.3 12.1 9.7
(GIZA++)
</table>
<tableCaption confidence="0.992340666666667">
Table 1: Comparison of test set AER between vari-
ous models trained on 500K sentence pairs. All numbers
are in percentage.
</tableCaption>
<table confidence="0.999443666666667">
model E → F F → E combined
baseline HMM 85.2 83.1 91.7
WDHMM 86.1 83.8 93.3
(τ = 1000)
IBM model 4 87.2 86.2 91.6
(GIZA++)
</table>
<tableCaption confidence="0.972163666666667">
Table 2: Comparison of test set Precision between
various models trained on 500K sentence pairs. All
numbers are in percentage.
</tableCaption>
<table confidence="0.998961833333333">
model E → F F → E combined
baseline HMM 90.6 91.4 88.3
WDHMM 91.9 92.6 89.1
(τ = 1000)
IBM model 4 91.1 90.8 88.4
(GIZA++)
</table>
<tableCaption confidence="0.997704666666667">
Table 3: Comparison of test set Recall between vari-
ous models trained on 500K sentence pairs. All numbers
are in percentage.
</tableCaption>
<figure confidence="0.9922917">
WDHMM
HMM baseline
10
9.5
9
8.5
80 1 2 3 4 5
11
AER %
10.5
</figure>
<page confidence="0.980951">
84
</page>
<subsectionHeader confidence="0.89125">
5.2 Machine translation on Europarl corpus
</subsectionHeader>
<bodyText confidence="0.999980591836735">
We further tested our WDHMM on a phrase-based
machine translation system to see whether our im-
provement on word alignment can also improve
MT accuracy measured by BLEU score (Papineni
et al., 2002). The machine translation experiment
was conducted on the English-to-French track of
NAACL 2006 Europarl evaluation workshop. The
supplied training corpus contains 688K sentence
pairs. Text data are already tokenized. In our expe-
riment, we first lower-cased all text, then word
clustering was performed to cluster words of Eng-
lish and French into 32 word classes respectively
using the tool provided by (J. Goodman). Then
word alignment was performed. Both baseline
HMM and IBM model 4 use word-class based
transition models, and in WDHMM the word-class
based transition model was used for prior distribu-
tion. The IBM model 4 is trained by GIZA++ with
a regimen of 5 iterations of Model 1, 5 iterations of
HMM, and 5 iterations of Model 4. Alignments of
both directions are generated and then are com-
bined by heuristic rules described in (Och and Ney
2000b). Then phrase table was extracted from the
word aligned bilingual texts. The maximum phrase
length was set to 7. In the phrase-based MT system,
there are four channel models. They are direct
maximum likelihood estimate of the probability of
target phrase given source phrase, and the same
estimate of source given target; we also compute
the lexicon weighting features for source given
target and target given source, respectively. Other
models include word count and phrase count, and a
3-gram language model provided by the workshop.
These models are combined in a log-linear frame-
work with different weights (Och and Ney, 2002).
The model weight vector is trained on a dev set
with 2000 English sentences, each of which has
one French translation reference. In the experiment,
only the first 500 sentences were used to train the
log-linear model weight vector, where minimum
error rate (MER) training was used (Och, 2003).
After MER training, the weight vector that gives
the best accuracy on the development set was se-
lected. We then applied it to tests. There are 2000
sentences in the development-test set devtest, 2000
sentences in a test set test, and 1064 out-of-domain
sentences called nc-test. The Pharaoh phrase-based
decoder (Koehn 2004b) was used for decoding.
The maximum re-ordering limit for decoding was
set to 7. We used default settings for all other pa-
rameters.
We present BLEU scores of MT systems using
different word alignments on all three test sets,
where Fig 2 shows BLEU scores of the two in-
domain tests, and Fig 3 shows MT results on the
out-of-domain test set. In testing, the prior parame-
ter τ of WDHMM was varied in the range of [20,
5000].
In Fig. 2, the horizontal dash line and the hori-
zontal dot line represent BLEU scores of the base-
line HMM on devtest set and test set, respectively.
The dash-line curve and dot-line curve represent
the BLEU scores of WDHMM on these two tests.
It is shown in the figure that WDHMM can
achieve the best BLEU scores on both devtest and
test when the prior parameter τ is set to 100. Fur-
thermore, WDHMM also gives considerable im-
provement on BLEU score over the baseline HMM
in a broad range of τ from 50 to 1000, which indi-
cates that WDHMM works pretty stable within a
reasonable range of prior distributions.
In Fig. 3, the horizontal dash line represents the
BLEU score of baseline HMM on nc-test set and
the dash-line curve represents BLEU scores of
WDHMM on the out-of-domain test. The best
BLEU is obtained at τ = 500. It is interesting to see
that the best τ for the out-of-domain test is larger
than that of an in-domain test. One possible expla-
nation is that for out-of-domain data, we need
more robust modeling for outliers other than more
accurate (in-domain) modeling. However, since the
difference between τ = 500 and τ = 100 are very
small, further experiments are needed before we
can draw a conclusion.
We gives a detailed BLEU-wise comparison be-
tween baseline HMM and WDHMM in Table 4,
where for WDHMM, τ =100 is used since it gives
the best performance on the development-test set
devtest. In the same table, we also provide BLEU
results of using IBM model 4. Compared to base-
line HMM alignment model, WDHMM can im-
prove the BLEU score nearly 1% on in-domain test
sets, and the improvement reduces to about 0.5%
on the out-of-domain test. When compared to IBM
model 4, WDHMM still gives higher BLEU
scores, and outperform model 4 by about 0.8% on
the test set. However the gain is reduced to 0.3%
on devtest and 0.5% on the out-of-domain nc-test.
</bodyText>
<page confidence="0.994468">
85
</page>
<figure confidence="0.871356">
log10(tau)
</figure>
<figureCaption confidence="0.9837744">
Figure 2: Machine translation results on Europarl,
English to French track, devtest and test sets. The
BLEU score of HMM baseline and the BLEU score of
WDHMM as the prior parameter τ is varied from 20 to
5000. Note that the x axis is in log scale.
</figureCaption>
<figure confidence="0.768738">
log10(tau)
</figure>
<figureCaption confidence="0.9750904">
Figure 3: Machine translation results on Europarl,
English to French track, out-of-domain test sets. The
BLEU score of HMM baseline and the BLEU score of
WDHMM as the prior parameter τ is varied from 20 to
5000. Note that the x axis is in log scale.
</figureCaption>
<table confidence="0.99899325">
model devtest test nc-test
baseline HMM 29.69 29.65 20.51
WDHMM (i = 100) 30.59 30.65 20.96
IBM model 4 30.29 29.86 20.51
</table>
<tableCaption confidence="0.974649333333333">
Table 4: Comparison of BLEU scores on devtest, test,
and nc-test set between various word alignment models.
All numbers are in percentage.
</tableCaption>
<bodyText confidence="0.999689555555556">
In order to verify whether these gains from
WDHMM are statistically significant, we imple-
mented paired bootstrap resampling method pro-
posed by Koehn (2004b) to compute statistical sig-
nificance of the above test results. In table 5, it is
shown that BLEU gains of WDHMM over HMM
and IBM-4 on different test sets, except the gain
over IBM model 4 on the devtest set, are statistical-
ly significant with a significance level &gt; 95%.
</bodyText>
<table confidence="0.9989594">
significance level devtest test nc-test
WDHMM (i=100) 99.9% 99.9% 99.5%
vs. HMM
WDHMM (i=100) 93.7% 99.9% 99.3%
vs. IBM model 4
</table>
<tableCaption confidence="0.989131">
Table 5: Statistical significance test of the BLEU im-
provement of WDHMM (i = 100) vs. HMM baseline,
and WDHMM (i = 100) vs. IBM model 4 on devtest,
test, and nc-test sets.
</tableCaption>
<subsectionHeader confidence="0.991204">
5.3 Runtime performance of WDHMM
</subsectionHeader>
<bodyText confidence="0.9960004375">
WDHMM runs as fast as a normal HMM, and
the extra memory needed for the word dependent
transition model is proportional to the vocabulary
size of the source language given that the distortion
sets of {c(d;e)} are bucketed. Runtime speed of
WDHMM and IBM-model 4 using GIZA++ is ta-
bulated in table 6. The results are based on Euro-
parl English to French alignment and these tests
were conducted on a fast PC with 3.0GHz CPU
and 16GB memory. In Table 6, WDHMM includes
5 iterations of model 1 training followed by 5 itera-
tions of WDHMM, while &amp;quot;IBM model 4&amp;quot; includes
5 iterations for model 1, 5 iterations for HMM, and
5 iterations for model 4. It is shown in Table 6 that
WDHMM is more than four times faster to pro-
duce the end-to-end word alignment.
</bodyText>
<table confidence="0.9138295">
model runtime
(min)
WDHMM 121
IBM model 4 537
</table>
<tableCaption confidence="0.956439">
Table 6: comparison of runtime performance bew-
teen WDHMM training and IBM model 4 training using
GIZA++.
</tableCaption>
<sectionHeader confidence="0.999414" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999490111111111">
Other works have been done to improve transition
models in HMM based word alignment. Och and
Ney (2000a) have suggested estimating word-class
based transition models so as to provide more de-
tailed transition probabilities. However, due to the
sparse data problem, only a small number of word
classes are usually used and the many words in the
same class still have to share the same transition
model. Toutanova et al. (2002) has proposed to
</bodyText>
<figure confidence="0.9951322">
BLEU %
21.5
20.5
22
21
201 1.5 2 2.5 3 3.5 4
nc-test
BLEU %
29.5
30.5
291 1.5 2 2.5 3 3.5 4
31
30
devtest
test
</figure>
<page confidence="0.982327">
86
</page>
<bodyText confidence="0.999901868421053">
estimate a word-dependent self-transition model
P(stay|e) so that each word can have its own prob-
ability to decide whether to stay or jump to a dif-
ferent word. Later Deng and Byrne (2005) pro-
posed a word dependent phrase length model to
better model state occupancy. However, these
model can only model the probability of self-
jumping. Important knowledge of jumping from e
to a different position should also be word depen-
dent but is not modeled.
Another interesting comparison is between
WDHMM and the fertility-based models, e.g.,
IBM model 3-5. Compared to these models, a ma-
jor disadvantage of HMM is the absence of a mod-
el of source word fertility. However, as discussed
in (Toutanova et al. 2002),the word dependent self-
transition model can be viewed as an approxima-
tion of fertility model. i.e., it models the number of
consecutive target words generated by the source
word with a geometric distribution. Therefore, with
a well estimated word dependent transition model,
this weakness of HMM is alleviated.
In this work, we proposed estimating a full
word-dependent transition models in HMM based
word alignment, and with Bayesian learning we
can achieve robust model estimation under the
sparse data condition. We have conducted a series
of experiments to evaluate this method on word
alignment and machine translation tests, and show
significant improvement over baseline HMM in
terms of AER and BLEU. It also performs better
than the much more complicated IBM model 4
based word alignment model on various word
alignment and machine translation tasks.
Acknowledgments The author is grateful to Chris
Quirk and Arul Menezes for assistance with the
MT system and for the valuable suggestions and
discussions.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888392857143">
C. M. Bishop, 2006. Pattern Recognition and Machine
Learning. Springer.
P. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1994. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19:263–311.
Y. Deng and W. Byrne, 2005, HMM Word and Phrase
Alignment For Statistical Machine Translation, in
Proceedings of HLT/EMNLP.
J. Gauvain and C.-H. Lee, 1994, Maximum a Posteriori
Estimation For Multivariate Gaussian Mixture Ob-
servations Of Markov Chains, IEEE Trans on Speech
and Audio Processing.
J. Goodman, http://research.microsoft.com/~joshuago/
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of HLT-
NAACL.
P. Koehn, 2004a, Statistical Significance Tests for Ma-
chine Translation Evaluation, in Proceedings of
EMNLP.
P. Koehn. 2004b. Pharaoh: A Beam Search Decoder For
Phrase Based Statistical Machine Translation Mod-
els. In Proceedings of AMTA.
P. Liang, B. Taskar, and D. Klein, 2006, Alignment by
Agreement, in Proceedings of NAACL.
R. Moore, W. Yih and A. Bode, 2006, Improved Dis-
criminative Bilingual Word Alignment, In Proceed-
ings of COLING/ACL.
F. J. Och and H. Ney. 2000a. A comparison of Align-
ment Models for Statistical Machine Translation. In
Proceedings of COLING.
F. J. Och and H. Ney. 2000b. Improved Statistical
Alignment Models. In Proceedings of ACL.
F. J. Och and H. Ney. 2000c. Giza++: Training of statis-
tical translation models. http://www-i6.informatik.
rwthaachen.de/och/software/GIZA++.html.
F. J. Och and H. Ney. 2002. Discriminative training and
Maximum Entropy Models for Statistical Machine
Translation, In Proceedings of ACL.
F. J. Och, 2003, Minimum Error Rate Training in Statis-
tical Machine Translation. In Proceedings of ACL.
K. A. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002. Bleu: A Method For Automatic Evaluation Of
Machine Translation. in Proceedings of ACL.
L. R. Rabiner, 1989 A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE.
K. Toutanova, H. T. Ilhan, and C. D. Manning. 2002.
Extensions to HMM-based Statistical Word Align-
ment Models. In Proceedings of EMNLP.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
Word Alignment In Statistical Translation. In Pro-
ceedings of COLING.
H. Zhang and D. Gildea, 2005, Stochastic Lexicalized
Inversion Transduction Grammar for Alignment, In
Proceedings of ACL.
</reference>
<page confidence="0.999475">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.615025">
<title confidence="0.999636">Using Word Dependent Transition Models in HMM based Alignment for Statistical Machine Translation</title>
<author confidence="0.946115">Xiaodong He</author>
<affiliation confidence="0.958316">Microsoft</affiliation>
<address confidence="0.9942325">One Microsoft Way Redmond, WA 98052 USA</address>
<email confidence="0.999851">xiaohe@microsoft.com</email>
<abstract confidence="0.963292894736842">In this paper, we present a Bayesian Learning based method to train word dependent transition models for HMM based word alignment. We present word alignment results on the Canadian Hansards corpus as compared to the conventional HMM and IBM model 4. We show that this method gives consistent and significant alignment error rate (AER) reduction. We also conducted machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="11592" citStr="Bishop 2006" startWordPosition="2076" endWordPosition="2077"> |1 , ) ( |1 ) J I Λ Λ (7) I Λ = p f e g e MAPΛ where the prior distribution ( |1 ) g Λ e characterizes I the distribution of the model parameter set Λ given the English sentence. The relation between ML and MAP estimation is through the Bayes&apos; theorem where the posterior distribution p Λ f e ∝ p f e Λ g Λ e ( |1 , 1 ) ( 1 |1 , ) ( |1 ) J I J I I , and p f e Λ is the likelihood function. ( 1 |1 , ) J I In transition model estimation, the transition model { p(i |i′, ei′ , I )} is a multinomial distribution. Its conjugate prior distribution is a Dirichlet distribution taking the following form (Bishop 2006), I g p i i e I e ( ( |, , ) |) ∝∏ ′ ′ i ′ 1 i ′ I 1 p i i e I ′ − ( |, , ) i i v , (8) i 1 where{vi′ ,i } is the set of hyper-parameters of the prior distribution. Note that for mathematic tractability, vi′,i needs to be greater than 1, which is usually the case in practice. Substitute (8) into (7) and using EM algorithm, we can obtain the iterative MAP training formula for transition models (Gauvain and Lee, 1994) E c(l −i;e)+ E vi′,l − I l =1 l=1 4.2 Setting hyper-parameters for the prior distribution In Bayesian learning, the hyper-parameter set {vi′,i} of the prior distribution is assumed</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>C. M. Bishop, 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<date>1994</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>19--263</pages>
<contexts>
<context position="1111" citStr="Brown et al., 1994" startWordPosition="175" endWordPosition="178">ent and significant alignment error rate (AER) reduction. We also conducted machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models. In the HMM based alignment model (Vogel et al., 1996), it is assumed that the HMM transition probabilities depend</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1994</marker>
<rawString>P. Brown, S. D. Pietra, V. J. D. Pietra, and R. L. Mercer. 1994. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Deng</author>
<author>W Byrne</author>
</authors>
<title>HMM Word and Phrase Alignment For Statistical Machine Translation,</title>
<date>2005</date>
<booktitle>in Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="26015" citStr="Deng and Byrne (2005)" startWordPosition="4670" endWordPosition="4673">stimating word-class based transition models so as to provide more detailed transition probabilities. However, due to the sparse data problem, only a small number of word classes are usually used and the many words in the same class still have to share the same transition model. Toutanova et al. (2002) has proposed to BLEU % 21.5 20.5 22 21 201 1.5 2 2.5 3 3.5 4 nc-test BLEU % 29.5 30.5 291 1.5 2 2.5 3 3.5 4 31 30 devtest test 86 estimate a word-dependent self-transition model P(stay|e) so that each word can have its own probability to decide whether to stay or jump to a different word. Later Deng and Byrne (2005) proposed a word dependent phrase length model to better model state occupancy. However, these model can only model the probability of selfjumping. Important knowledge of jumping from e to a different position should also be word dependent but is not modeled. Another interesting comparison is between WDHMM and the fertility-based models, e.g., IBM model 3-5. Compared to these models, a major disadvantage of HMM is the absence of a model of source word fertility. However, as discussed in (Toutanova et al. 2002),the word dependent selftransition model can be viewed as an approximation of fertili</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Y. Deng and W. Byrne, 2005, HMM Word and Phrase Alignment For Statistical Machine Translation, in Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gauvain</author>
<author>C-H Lee</author>
</authors>
<title>Maximum a Posteriori Estimation For Multivariate Gaussian Mixture Observations Of Markov Chains,</title>
<date>1994</date>
<booktitle>IEEE Trans on Speech and Audio Processing.</booktitle>
<contexts>
<context position="10771" citStr="Gauvain and Lee, 1994" startWordPosition="1893" endWordPosition="1896">(5) E c(l − i′; e) l=1 where at each training iteration the word dependent distortion set {c(i - i&apos;;e)} is computed by c (d ; e) J I −1 EE δ e e a i a i d f e J I Λ ′ = = = + |, , ) ( )Pr( , j a j j+ 1 1 1 j= 1 i=1 (6) where d = i - i&apos; is the jump width, and ( ) δ eaj = e is the Kronecker delta function that equals one if eaj = e, and zero otherwise. However, for many non-frequent words, the data samples for c(d;e) is very limited and therefore may lead to a biased model that severely overfits to the sparse data. In order to address this issue, maximum a posteriori (MAP) framework is applied (Gauvain and Lee, 1994). In MAP training, an appropriate prior distribution is used to incorpoJ aˆi =argmax∏[p(aj |aj _,I)p(fj |eaj) aJ1 =1 j . I . I eaj _ depen82 rate prior knowledge into the model parameter estimation, argmax ( 1 |1 , ) ( |1 ) J I Λ Λ (7) I Λ = p f e g e MAPΛ where the prior distribution ( |1 ) g Λ e characterizes I the distribution of the model parameter set Λ given the English sentence. The relation between ML and MAP estimation is through the Bayes&apos; theorem where the posterior distribution p Λ f e ∝ p f e Λ g Λ e ( |1 , 1 ) ( 1 |1 , ) ( |1 ) J I J I I , and p f e Λ is the likelihood function. </context>
<context position="12011" citStr="Gauvain and Lee, 1994" startWordPosition="2171" endWordPosition="2174">transition model estimation, the transition model { p(i |i′, ei′ , I )} is a multinomial distribution. Its conjugate prior distribution is a Dirichlet distribution taking the following form (Bishop 2006), I g p i i e I e ( ( |, , ) |) ∝∏ ′ ′ i ′ 1 i ′ I 1 p i i e I ′ − ( |, , ) i i v , (8) i 1 where{vi′ ,i } is the set of hyper-parameters of the prior distribution. Note that for mathematic tractability, vi′,i needs to be greater than 1, which is usually the case in practice. Substitute (8) into (7) and using EM algorithm, we can obtain the iterative MAP training formula for transition models (Gauvain and Lee, 1994) E c(l −i;e)+ E vi′,l − I l =1 l=1 4.2 Setting hyper-parameters for the prior distribution In Bayesian learning, the hyper-parameter set {vi′,i} of the prior distribution is assumed known based on a subjective knowledge about the model. In our method, we set the prior with wordindependent transition probabilities. vi′,i =τ ⋅ p(i |i′,I)+1 (10) where r is a positive parameter that needs to tune on a hold-out data set. We will investigate the effect of r with experimental results in later sections. Substituting (10) into (9), the MAP based transition model training formula becomes pMAP(i |i′,e,I)</context>
</contexts>
<marker>Gauvain, Lee, 1994</marker>
<rawString>J. Gauvain and C.-H. Lee, 1994, Maximum a Posteriori Estimation For Multivariate Gaussian Mixture Observations Of Markov Chains, IEEE Trans on Speech and Audio Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Goodman</author>
</authors>
<location>http://research.microsoft.com/~joshuago/</location>
<marker>Goodman, </marker>
<rawString>J. Goodman, http://research.microsoft.com/~joshuago/</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="1018" citStr="Koehn et al., 2003" startWordPosition="159" endWordPosition="162">s as compared to the conventional HMM and IBM model 4. We show that this method gives consistent and significant alignment error rate (AER) reduction. We also conducted machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models. In the HMM based ali</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
</authors>
<title>2004a, Statistical Significance Tests for Machine Translation Evaluation,</title>
<booktitle>in Proceedings of EMNLP.</booktitle>
<marker>Koehn, </marker>
<rawString>P. Koehn, 2004a, Statistical Significance Tests for Machine Translation Evaluation, in Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Pharaoh: A Beam Search Decoder For Phrase Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="20532" citStr="Koehn 2004" startWordPosition="3682" endWordPosition="3683">el weight vector is trained on a dev set with 2000 English sentences, each of which has one French translation reference. In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (MER) training was used (Och, 2003). After MER training, the weight vector that gives the best accuracy on the development set was selected. We then applied it to tests. There are 2000 sentences in the development-test set devtest, 2000 sentences in a test set test, and 1064 out-of-domain sentences called nc-test. The Pharaoh phrase-based decoder (Koehn 2004b) was used for decoding. The maximum re-ordering limit for decoding was set to 7. We used default settings for all other parameters. We present BLEU scores of MT systems using different word alignments on all three test sets, where Fig 2 shows BLEU scores of the two indomain tests, and Fig 3 shows MT results on the out-of-domain test set. In testing, the prior parameter τ of WDHMM was varied in the range of [20, 5000]. In Fig. 2, the horizontal dash line and the horizontal dot line represent BLEU scores of the baseline HMM on devtest set and test set, respectively. The dash-line curve and dot</context>
<context position="23754" citStr="Koehn (2004" startWordPosition="4260" endWordPosition="4261">lish to French track, out-of-domain test sets. The BLEU score of HMM baseline and the BLEU score of WDHMM as the prior parameter τ is varied from 20 to 5000. Note that the x axis is in log scale. model devtest test nc-test baseline HMM 29.69 29.65 20.51 WDHMM (i = 100) 30.59 30.65 20.96 IBM model 4 30.29 29.86 20.51 Table 4: Comparison of BLEU scores on devtest, test, and nc-test set between various word alignment models. All numbers are in percentage. In order to verify whether these gains from WDHMM are statistically significant, we implemented paired bootstrap resampling method proposed by Koehn (2004b) to compute statistical significance of the above test results. In table 5, it is shown that BLEU gains of WDHMM over HMM and IBM-4 on different test sets, except the gain over IBM model 4 on the devtest set, are statistically significant with a significance level &gt; 95%. significance level devtest test nc-test WDHMM (i=100) 99.9% 99.9% 99.5% vs. HMM WDHMM (i=100) 93.7% 99.9% 99.3% vs. IBM model 4 Table 5: Statistical significance test of the BLEU improvement of WDHMM (i = 100) vs. HMM baseline, and WDHMM (i = 100) vs. IBM model 4 on devtest, test, and nc-test sets. 5.3 Runtime performance of</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004b. Pharaoh: A Beam Search Decoder For Phrase Based Statistical Machine Translation Models. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by Agreement,</title>
<date>2006</date>
<booktitle>in Proceedings of NAACL.</booktitle>
<contexts>
<context position="8248" citStr="Liang et al., 2006" startWordPosition="1413" endWordPosition="1416">meter estimation with more details. These transition probabilities {p(i |i ′,I)} is a multinomial distribution estimated according to (2), where at each iteration the distortion set {c(i - i&apos;)} is the fractional count of transitions with jump width d = i - i&apos;, i.e., J I −1 c(d) =EEPr(aj =i,aj+1 = i + d |fJ,e1,Λ) (4) j= 1 i=1 where A&apos; is the model obtained from the immediate previous iteration and these terms in (4) can be efficiently computed by using the ForwardBackward algorithm (Rabiner 1989). In practice, we can bucket the distortion parameters {c(d)} into a few buckets as implemented in (Liang et al., 2006). In our implementation, 15 buckets are used for c(≤-7), c(-6), ... c(0), ..., c(≥7). The probability mass for transitions with jump width larger than 6 is uniformly divided. As suggested in (Liang et al., 2006), we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively. Finally, we further smooth transition probabilities with a uniform distribution as described in (Och and Ney, 2000a), 1 p a a I ′( j |j , ) =α ⋅ + −α ⋅ (1 ) ( |, ) p aj aj I . _ _ I After training, Viterbi decoding is used to find th</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein, 2006, Alignment by Agreement, in Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>W Yih</author>
<author>A Bode</author>
</authors>
<title>Improved Discriminative Bilingual Word Alignment,</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<marker>Moore, Yih, Bode, 2006</marker>
<rawString>R. Moore, W. Yih and A. Bode, 2006, Improved Discriminative Bilingual Word Alignment, In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of Alignment Models for Statistical Machine Translation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1185" citStr="Och and Ney, 2000" startWordPosition="189" endWordPosition="192">machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models. In the HMM based alignment model (Vogel et al., 1996), it is assumed that the HMM transition probabilities depend only on the jump width from the last state to the next state. Therefore, </context>
<context position="4948" citStr="Och and Ney, 2000" startWordPosition="786" endWordPosition="789"> severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal. In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training. With the help of the prior distribution of the model, the training is regularized and results in robust models. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a). Then we describe the equations of MAP training for word dependent transition models. In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4. We also conducted phrase-based machine translation experiments on the Europarl corpus, English – French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4. 2 Baseline HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a). Let’s denote </context>
<context position="6647" citStr="Och and Ney, 2000" startWordPosition="1113" endWordPosition="1116">nds on the English word, i.e., p(fj |aj,eaj) = p(fj |eaj ) , and the transition probability only depends on the position of the last state and the length of the English sentence, i.e., p aj aj eaj I p aj aj I ( |, , ) ( |1 , ) = . Then, Vogel et −1 − −1 al. (1996) give J p(fJ |e;)=1:∏[p(aj |aj−,I)p(fj |eaj) a j J =1 1 In the HMM of (Vogel et al., 1996), it is further assumed these transition probabilities p(aj = i |aj−1 = i′, I) depend only on the jump width (i - i&apos;), i.e., Therefore, the transition probability p(aj |aj−1,I) depends on aj-1 but only through the distortion set {c(i - i&apos;)}. In (Och and Ney, 2000a), the word null is introduced to generate the French words that don&apos;t align to any English words. If we denote by j_ the position of the last French word before j that aligns to a non-null English word, the transition probabilities p(aj = i |aj−1 = i′,I) in (1) is computed as p(aj =i|aj_=i′,I)=p%(i|i′,I), where I p if 0 i = 0 p i i I ( |, ) ′ =i(1−p0)⋅p(i|i′j) otherwise state i=0 denotes the state of a null word at the English side, and p0 is the probability of jumping to state 0, which is estimated from hold-out data. For convenience, we denote by Λ ={p(i |i′,I),p(fj |ei)} the HMM parameter</context>
<context position="8729" citStr="Och and Ney, 2000" startWordPosition="1489" endWordPosition="1492">hm (Rabiner 1989). In practice, we can bucket the distortion parameters {c(d)} into a few buckets as implemented in (Liang et al., 2006). In our implementation, 15 buckets are used for c(≤-7), c(-6), ... c(0), ..., c(≥7). The probability mass for transitions with jump width larger than 6 is uniformly divided. As suggested in (Liang et al., 2006), we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively. Finally, we further smooth transition probabilities with a uniform distribution as described in (Och and Ney, 2000a), 1 p a a I ′( j |j , ) =α ⋅ + −α ⋅ (1 ) ( |, ) p aj aj I . _ _ I After training, Viterbi decoding is used to find the best alignment sequence 1ˆJ a . i.e., 3 Word-dependent transition models in HMM based alignment model As discussed in the previous sections, conventional transition models that only depend on source word positions are not accurate enough. There are only limited distortion parameters to model the transition between HMM states for all English words, and the knowledge of transition probabilities given a particular source word is not represented. In order to improve the transiti</context>
<context position="13883" citStr="Och and Ney 2000" startWordPosition="2495" endWordPosition="2498">oach to the word-independent transition model. Therefore, we can vary the parameter r to control the contribution of prior distribution in model training and tune the word alignment performance. 5 Experimental Results 5.1 Word alignment on the Canadian Hansards English-French corpus We evaluated our word dependent transition models for HMM based word alignment on the English-French Hansards corpus. Only a subset of 500K sentence pairs was used in our experiments including 447 test sentence-pairs. Tests sentencepairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a). Using this annotation, we report the word alignment performance in terms of alignment error rate (AER) as defined by Och and Ney (2000a): AER=1− |A∩S|+|A∩P |(12) |A|+|S| where S denotes the set of sure gold alignments, P denotes the set of possible gold alignments, A denotes the set of alignments generated by the word alignment method under test. We first trained the IBM model 1 and then a baseline HMM model as described in section 2 on the Hansards corpus. As the common practice, we initialized the translation probabilities of model 1 with uniform distribution over word pairs occur togeth</context>
<context position="16296" citStr="Och and Ney (2000" startWordPosition="2938" endWordPosition="2941">rse data problem, the model is poorly estimated and lead to a high AER. When increase r to a larger value, a stronger prior is applied to give a more robust model. Then in a large range of τ ∈ [100,2000] , WDHMM outperforms baseline HMM significantly. When r gets even larger, MAP model training becomes being over-dominated by the prior distribution, and that eventually results in a performance approaching to that of the baseline HMM. Fig. 1 only presents AER results that are calculated after combination of word alignments of both E→F and F→E directions based on a set of heuristics proposed by Och and Ney (2000b). We have observed the similar trend of AER change for the E→F and F→E alignment directions, respectively. However, due to the limit of the space, we didn’t include them in this paper. In table 1-3, we give a detailed comparison between baseline HMM, WDHMM (with r = 1000), and IBM model 4. Compared to the baseline HMM, the proposed WDHMM can reduce AER by more than 13%. It even outperforms IBM model 4 after two direction word alignment combination. Meanwhile we noticed that although IBM model 4 gives superior performance over the baseline HMM on both of the two alignment directions, its AER </context>
<context position="19281" citStr="Och and Ney 2000" startWordPosition="3478" endWordPosition="3481"> first lower-cased all text, then word clustering was performed to cluster words of English and French into 32 word classes respectively using the tool provided by (J. Goodman). Then word alignment was performed. Both baseline HMM and IBM model 4 use word-class based transition models, and in WDHMM the word-class based transition model was used for prior distribution. The IBM model 4 is trained by GIZA++ with a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. Alignments of both directions are generated and then are combined by heuristic rules described in (Och and Ney 2000b). Then phrase table was extracted from the word aligned bilingual texts. The maximum phrase length was set to 7. In the phrase-based MT system, there are four channel models. They are direct maximum likelihood estimate of the probability of target phrase given source phrase, and the same estimate of source given target; we also compute the lexicon weighting features for source given target and target given source, respectively. Other models include word count and phrase count, and a 3-gram language model provided by the workshop. These models are combined in a log-linear framework with diffe</context>
<context position="25375" citStr="Och and Ney (2000" startWordPosition="4551" endWordPosition="4554"> fast PC with 3.0GHz CPU and 16GB memory. In Table 6, WDHMM includes 5 iterations of model 1 training followed by 5 iterations of WDHMM, while &amp;quot;IBM model 4&amp;quot; includes 5 iterations for model 1, 5 iterations for HMM, and 5 iterations for model 4. It is shown in Table 6 that WDHMM is more than four times faster to produce the end-to-end word alignment. model runtime (min) WDHMM 121 IBM model 4 537 Table 6: comparison of runtime performance bewteen WDHMM training and IBM model 4 training using GIZA++. 6 Discussion Other works have been done to improve transition models in HMM based word alignment. Och and Ney (2000a) have suggested estimating word-class based transition models so as to provide more detailed transition probabilities. However, due to the sparse data problem, only a small number of word classes are usually used and the many words in the same class still have to share the same transition model. Toutanova et al. (2002) has proposed to BLEU % 21.5 20.5 22 21 201 1.5 2 2.5 3 3.5 4 nc-test BLEU % 29.5 30.5 291 1.5 2 2.5 3 3.5 4 31 30 devtest test 86 estimate a word-dependent self-transition model P(stay|e) so that each word can have its own probability to decide whether to stay or jump to a dif</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000a. A comparison of Alignment Models for Statistical Machine Translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1185" citStr="Och and Ney, 2000" startWordPosition="189" endWordPosition="192">machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models. In the HMM based alignment model (Vogel et al., 1996), it is assumed that the HMM transition probabilities depend only on the jump width from the last state to the next state. Therefore, </context>
<context position="4948" citStr="Och and Ney, 2000" startWordPosition="786" endWordPosition="789"> severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal. In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training. With the help of the prior distribution of the model, the training is regularized and results in robust models. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a). Then we describe the equations of MAP training for word dependent transition models. In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4. We also conducted phrase-based machine translation experiments on the Europarl corpus, English – French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4. 2 Baseline HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a). Let’s denote </context>
<context position="6647" citStr="Och and Ney, 2000" startWordPosition="1113" endWordPosition="1116">nds on the English word, i.e., p(fj |aj,eaj) = p(fj |eaj ) , and the transition probability only depends on the position of the last state and the length of the English sentence, i.e., p aj aj eaj I p aj aj I ( |, , ) ( |1 , ) = . Then, Vogel et −1 − −1 al. (1996) give J p(fJ |e;)=1:∏[p(aj |aj−,I)p(fj |eaj) a j J =1 1 In the HMM of (Vogel et al., 1996), it is further assumed these transition probabilities p(aj = i |aj−1 = i′, I) depend only on the jump width (i - i&apos;), i.e., Therefore, the transition probability p(aj |aj−1,I) depends on aj-1 but only through the distortion set {c(i - i&apos;)}. In (Och and Ney, 2000a), the word null is introduced to generate the French words that don&apos;t align to any English words. If we denote by j_ the position of the last French word before j that aligns to a non-null English word, the transition probabilities p(aj = i |aj−1 = i′,I) in (1) is computed as p(aj =i|aj_=i′,I)=p%(i|i′,I), where I p if 0 i = 0 p i i I ( |, ) ′ =i(1−p0)⋅p(i|i′j) otherwise state i=0 denotes the state of a null word at the English side, and p0 is the probability of jumping to state 0, which is estimated from hold-out data. For convenience, we denote by Λ ={p(i |i′,I),p(fj |ei)} the HMM parameter</context>
<context position="8729" citStr="Och and Ney, 2000" startWordPosition="1489" endWordPosition="1492">hm (Rabiner 1989). In practice, we can bucket the distortion parameters {c(d)} into a few buckets as implemented in (Liang et al., 2006). In our implementation, 15 buckets are used for c(≤-7), c(-6), ... c(0), ..., c(≥7). The probability mass for transitions with jump width larger than 6 is uniformly divided. As suggested in (Liang et al., 2006), we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively. Finally, we further smooth transition probabilities with a uniform distribution as described in (Och and Ney, 2000a), 1 p a a I ′( j |j , ) =α ⋅ + −α ⋅ (1 ) ( |, ) p aj aj I . _ _ I After training, Viterbi decoding is used to find the best alignment sequence 1ˆJ a . i.e., 3 Word-dependent transition models in HMM based alignment model As discussed in the previous sections, conventional transition models that only depend on source word positions are not accurate enough. There are only limited distortion parameters to model the transition between HMM states for all English words, and the knowledge of transition probabilities given a particular source word is not represented. In order to improve the transiti</context>
<context position="13883" citStr="Och and Ney 2000" startWordPosition="2495" endWordPosition="2498">oach to the word-independent transition model. Therefore, we can vary the parameter r to control the contribution of prior distribution in model training and tune the word alignment performance. 5 Experimental Results 5.1 Word alignment on the Canadian Hansards English-French corpus We evaluated our word dependent transition models for HMM based word alignment on the English-French Hansards corpus. Only a subset of 500K sentence pairs was used in our experiments including 447 test sentence-pairs. Tests sentencepairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a). Using this annotation, we report the word alignment performance in terms of alignment error rate (AER) as defined by Och and Ney (2000a): AER=1− |A∩S|+|A∩P |(12) |A|+|S| where S denotes the set of sure gold alignments, P denotes the set of possible gold alignments, A denotes the set of alignments generated by the word alignment method under test. We first trained the IBM model 1 and then a baseline HMM model as described in section 2 on the Hansards corpus. As the common practice, we initialized the translation probabilities of model 1 with uniform distribution over word pairs occur togeth</context>
<context position="16296" citStr="Och and Ney (2000" startWordPosition="2938" endWordPosition="2941">rse data problem, the model is poorly estimated and lead to a high AER. When increase r to a larger value, a stronger prior is applied to give a more robust model. Then in a large range of τ ∈ [100,2000] , WDHMM outperforms baseline HMM significantly. When r gets even larger, MAP model training becomes being over-dominated by the prior distribution, and that eventually results in a performance approaching to that of the baseline HMM. Fig. 1 only presents AER results that are calculated after combination of word alignments of both E→F and F→E directions based on a set of heuristics proposed by Och and Ney (2000b). We have observed the similar trend of AER change for the E→F and F→E alignment directions, respectively. However, due to the limit of the space, we didn’t include them in this paper. In table 1-3, we give a detailed comparison between baseline HMM, WDHMM (with r = 1000), and IBM model 4. Compared to the baseline HMM, the proposed WDHMM can reduce AER by more than 13%. It even outperforms IBM model 4 after two direction word alignment combination. Meanwhile we noticed that although IBM model 4 gives superior performance over the baseline HMM on both of the two alignment directions, its AER </context>
<context position="19281" citStr="Och and Ney 2000" startWordPosition="3478" endWordPosition="3481"> first lower-cased all text, then word clustering was performed to cluster words of English and French into 32 word classes respectively using the tool provided by (J. Goodman). Then word alignment was performed. Both baseline HMM and IBM model 4 use word-class based transition models, and in WDHMM the word-class based transition model was used for prior distribution. The IBM model 4 is trained by GIZA++ with a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. Alignments of both directions are generated and then are combined by heuristic rules described in (Och and Ney 2000b). Then phrase table was extracted from the word aligned bilingual texts. The maximum phrase length was set to 7. In the phrase-based MT system, there are four channel models. They are direct maximum likelihood estimate of the probability of target phrase given source phrase, and the same estimate of source given target; we also compute the lexicon weighting features for source given target and target given source, respectively. Other models include word count and phrase count, and a 3-gram language model provided by the workshop. These models are combined in a log-linear framework with diffe</context>
<context position="25375" citStr="Och and Ney (2000" startWordPosition="4551" endWordPosition="4554"> fast PC with 3.0GHz CPU and 16GB memory. In Table 6, WDHMM includes 5 iterations of model 1 training followed by 5 iterations of WDHMM, while &amp;quot;IBM model 4&amp;quot; includes 5 iterations for model 1, 5 iterations for HMM, and 5 iterations for model 4. It is shown in Table 6 that WDHMM is more than four times faster to produce the end-to-end word alignment. model runtime (min) WDHMM 121 IBM model 4 537 Table 6: comparison of runtime performance bewteen WDHMM training and IBM model 4 training using GIZA++. 6 Discussion Other works have been done to improve transition models in HMM based word alignment. Och and Ney (2000a) have suggested estimating word-class based transition models so as to provide more detailed transition probabilities. However, due to the sparse data problem, only a small number of word classes are usually used and the many words in the same class still have to share the same transition model. Toutanova et al. (2002) has proposed to BLEU % 21.5 20.5 22 21 201 1.5 2 2.5 3 3.5 4 nc-test BLEU % 29.5 30.5 291 1.5 2 2.5 3 3.5 4 31 30 devtest test 86 estimate a word-dependent self-transition model P(stay|e) so that each word can have its own probability to decide whether to stay or jump to a dif</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000b. Improved Statistical Alignment Models. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<note>http://www-i6.informatik. rwthaachen.de/och/software/GIZA++.html.</note>
<contexts>
<context position="1185" citStr="Och and Ney, 2000" startWordPosition="189" endWordPosition="192">machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models. In the HMM based alignment model (Vogel et al., 1996), it is assumed that the HMM transition probabilities depend only on the jump width from the last state to the next state. Therefore, </context>
<context position="4948" citStr="Och and Ney, 2000" startWordPosition="786" endWordPosition="789"> severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal. In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training. With the help of the prior distribution of the model, the training is regularized and results in robust models. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a). Then we describe the equations of MAP training for word dependent transition models. In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4. We also conducted phrase-based machine translation experiments on the Europarl corpus, English – French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4. 2 Baseline HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a). Let’s denote </context>
<context position="6647" citStr="Och and Ney, 2000" startWordPosition="1113" endWordPosition="1116">nds on the English word, i.e., p(fj |aj,eaj) = p(fj |eaj ) , and the transition probability only depends on the position of the last state and the length of the English sentence, i.e., p aj aj eaj I p aj aj I ( |, , ) ( |1 , ) = . Then, Vogel et −1 − −1 al. (1996) give J p(fJ |e;)=1:∏[p(aj |aj−,I)p(fj |eaj) a j J =1 1 In the HMM of (Vogel et al., 1996), it is further assumed these transition probabilities p(aj = i |aj−1 = i′, I) depend only on the jump width (i - i&apos;), i.e., Therefore, the transition probability p(aj |aj−1,I) depends on aj-1 but only through the distortion set {c(i - i&apos;)}. In (Och and Ney, 2000a), the word null is introduced to generate the French words that don&apos;t align to any English words. If we denote by j_ the position of the last French word before j that aligns to a non-null English word, the transition probabilities p(aj = i |aj−1 = i′,I) in (1) is computed as p(aj =i|aj_=i′,I)=p%(i|i′,I), where I p if 0 i = 0 p i i I ( |, ) ′ =i(1−p0)⋅p(i|i′j) otherwise state i=0 denotes the state of a null word at the English side, and p0 is the probability of jumping to state 0, which is estimated from hold-out data. For convenience, we denote by Λ ={p(i |i′,I),p(fj |ei)} the HMM parameter</context>
<context position="8729" citStr="Och and Ney, 2000" startWordPosition="1489" endWordPosition="1492">hm (Rabiner 1989). In practice, we can bucket the distortion parameters {c(d)} into a few buckets as implemented in (Liang et al., 2006). In our implementation, 15 buckets are used for c(≤-7), c(-6), ... c(0), ..., c(≥7). The probability mass for transitions with jump width larger than 6 is uniformly divided. As suggested in (Liang et al., 2006), we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively. Finally, we further smooth transition probabilities with a uniform distribution as described in (Och and Ney, 2000a), 1 p a a I ′( j |j , ) =α ⋅ + −α ⋅ (1 ) ( |, ) p aj aj I . _ _ I After training, Viterbi decoding is used to find the best alignment sequence 1ˆJ a . i.e., 3 Word-dependent transition models in HMM based alignment model As discussed in the previous sections, conventional transition models that only depend on source word positions are not accurate enough. There are only limited distortion parameters to model the transition between HMM states for all English words, and the knowledge of transition probabilities given a particular source word is not represented. In order to improve the transiti</context>
<context position="13883" citStr="Och and Ney 2000" startWordPosition="2495" endWordPosition="2498">oach to the word-independent transition model. Therefore, we can vary the parameter r to control the contribution of prior distribution in model training and tune the word alignment performance. 5 Experimental Results 5.1 Word alignment on the Canadian Hansards English-French corpus We evaluated our word dependent transition models for HMM based word alignment on the English-French Hansards corpus. Only a subset of 500K sentence pairs was used in our experiments including 447 test sentence-pairs. Tests sentencepairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a). Using this annotation, we report the word alignment performance in terms of alignment error rate (AER) as defined by Och and Ney (2000a): AER=1− |A∩S|+|A∩P |(12) |A|+|S| where S denotes the set of sure gold alignments, P denotes the set of possible gold alignments, A denotes the set of alignments generated by the word alignment method under test. We first trained the IBM model 1 and then a baseline HMM model as described in section 2 on the Hansards corpus. As the common practice, we initialized the translation probabilities of model 1 with uniform distribution over word pairs occur togeth</context>
<context position="16296" citStr="Och and Ney (2000" startWordPosition="2938" endWordPosition="2941">rse data problem, the model is poorly estimated and lead to a high AER. When increase r to a larger value, a stronger prior is applied to give a more robust model. Then in a large range of τ ∈ [100,2000] , WDHMM outperforms baseline HMM significantly. When r gets even larger, MAP model training becomes being over-dominated by the prior distribution, and that eventually results in a performance approaching to that of the baseline HMM. Fig. 1 only presents AER results that are calculated after combination of word alignments of both E→F and F→E directions based on a set of heuristics proposed by Och and Ney (2000b). We have observed the similar trend of AER change for the E→F and F→E alignment directions, respectively. However, due to the limit of the space, we didn’t include them in this paper. In table 1-3, we give a detailed comparison between baseline HMM, WDHMM (with r = 1000), and IBM model 4. Compared to the baseline HMM, the proposed WDHMM can reduce AER by more than 13%. It even outperforms IBM model 4 after two direction word alignment combination. Meanwhile we noticed that although IBM model 4 gives superior performance over the baseline HMM on both of the two alignment directions, its AER </context>
<context position="19281" citStr="Och and Ney 2000" startWordPosition="3478" endWordPosition="3481"> first lower-cased all text, then word clustering was performed to cluster words of English and French into 32 word classes respectively using the tool provided by (J. Goodman). Then word alignment was performed. Both baseline HMM and IBM model 4 use word-class based transition models, and in WDHMM the word-class based transition model was used for prior distribution. The IBM model 4 is trained by GIZA++ with a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4. Alignments of both directions are generated and then are combined by heuristic rules described in (Och and Ney 2000b). Then phrase table was extracted from the word aligned bilingual texts. The maximum phrase length was set to 7. In the phrase-based MT system, there are four channel models. They are direct maximum likelihood estimate of the probability of target phrase given source phrase, and the same estimate of source given target; we also compute the lexicon weighting features for source given target and target given source, respectively. Other models include word count and phrase count, and a 3-gram language model provided by the workshop. These models are combined in a log-linear framework with diffe</context>
<context position="25375" citStr="Och and Ney (2000" startWordPosition="4551" endWordPosition="4554"> fast PC with 3.0GHz CPU and 16GB memory. In Table 6, WDHMM includes 5 iterations of model 1 training followed by 5 iterations of WDHMM, while &amp;quot;IBM model 4&amp;quot; includes 5 iterations for model 1, 5 iterations for HMM, and 5 iterations for model 4. It is shown in Table 6 that WDHMM is more than four times faster to produce the end-to-end word alignment. model runtime (min) WDHMM 121 IBM model 4 537 Table 6: comparison of runtime performance bewteen WDHMM training and IBM model 4 training using GIZA++. 6 Discussion Other works have been done to improve transition models in HMM based word alignment. Och and Ney (2000a) have suggested estimating word-class based transition models so as to provide more detailed transition probabilities. However, due to the sparse data problem, only a small number of word classes are usually used and the many words in the same class still have to share the same transition model. Toutanova et al. (2002) has proposed to BLEU % 21.5 20.5 22 21 201 1.5 2 2.5 3 3.5 4 nc-test BLEU % 29.5 30.5 291 1.5 2 2.5 3 3.5 4 31 30 devtest test 86 estimate a word-dependent self-transition model P(stay|e) so that each word can have its own probability to decide whether to stay or jump to a dif</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000c. Giza++: Training of statistical translation models. http://www-i6.informatik. rwthaachen.de/och/software/GIZA++.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and Maximum Entropy Models for Statistical Machine Translation,</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="19913" citStr="Och and Ney, 2002" startWordPosition="3579" endWordPosition="3582">e table was extracted from the word aligned bilingual texts. The maximum phrase length was set to 7. In the phrase-based MT system, there are four channel models. They are direct maximum likelihood estimate of the probability of target phrase given source phrase, and the same estimate of source given target; we also compute the lexicon weighting features for source given target and target given source, respectively. Other models include word count and phrase count, and a 3-gram language model provided by the workshop. These models are combined in a log-linear framework with different weights (Och and Ney, 2002). The model weight vector is trained on a dev set with 2000 English sentences, each of which has one French translation reference. In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (MER) training was used (Och, 2003). After MER training, the weight vector that gives the best accuracy on the development set was selected. We then applied it to tests. There are 2000 sentences in the development-test set devtest, 2000 sentences in a test set test, and 1064 out-of-domain sentences called nc-test. The Pharaoh phrase-based </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F. J. Och and H. Ney. 2002. Discriminative training and Maximum Entropy Models for Statistical Machine Translation, In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="20207" citStr="Och, 2003" startWordPosition="3630" endWordPosition="3631">target; we also compute the lexicon weighting features for source given target and target given source, respectively. Other models include word count and phrase count, and a 3-gram language model provided by the workshop. These models are combined in a log-linear framework with different weights (Och and Ney, 2002). The model weight vector is trained on a dev set with 2000 English sentences, each of which has one French translation reference. In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (MER) training was used (Och, 2003). After MER training, the weight vector that gives the best accuracy on the development set was selected. We then applied it to tests. There are 2000 sentences in the development-test set devtest, 2000 sentences in a test set test, and 1064 out-of-domain sentences called nc-test. The Pharaoh phrase-based decoder (Koehn 2004b) was used for decoding. The maximum re-ordering limit for decoding was set to 7. We used default settings for all other parameters. We present BLEU scores of MT systems using different word alignments on all three test sets, where Fig 2 shows BLEU scores of the two indomai</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och, 2003, Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: A Method For Automatic Evaluation Of Machine Translation.</title>
<date>2002</date>
<booktitle>in Proceedings of ACL.</booktitle>
<contexts>
<context position="18426" citStr="Papineni et al., 2002" startWordPosition="3337" endWordPosition="3340">s models trained on 500K sentence pairs. All numbers are in percentage. model E → F F → E combined baseline HMM 90.6 91.4 88.3 WDHMM 91.9 92.6 89.1 (τ = 1000) IBM model 4 91.1 90.8 88.4 (GIZA++) Table 3: Comparison of test set Recall between various models trained on 500K sentence pairs. All numbers are in percentage. WDHMM HMM baseline 10 9.5 9 8.5 80 1 2 3 4 5 11 AER % 10.5 84 5.2 Machine translation on Europarl corpus We further tested our WDHMM on a phrase-based machine translation system to see whether our improvement on word alignment can also improve MT accuracy measured by BLEU score (Papineni et al., 2002). The machine translation experiment was conducted on the English-to-French track of NAACL 2006 Europarl evaluation workshop. The supplied training corpus contains 688K sentence pairs. Text data are already tokenized. In our experiment, we first lower-cased all text, then word clustering was performed to cluster words of English and French into 32 word classes respectively using the tool provided by (J. Goodman). Then word alignment was performed. Both baseline HMM and IBM model 4 use word-class based transition models, and in WDHMM the word-class based transition model was used for prior dist</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. A. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: A Method For Automatic Evaluation Of Machine Translation. in Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE.</booktitle>
<contexts>
<context position="7567" citStr="Rabiner 1989" startWordPosition="1297" endWordPosition="1298">, where I p if 0 i = 0 p i i I ( |, ) ′ =i(1−p0)⋅p(i|i′j) otherwise state i=0 denotes the state of a null word at the English side, and p0 is the probability of jumping to state 0, which is estimated from hold-out data. For convenience, we denote by Λ ={p(i |i′,I),p(fj |ei)} the HMM parameter set. p c(i −i) (2) (i |i′, I ) c(l − i) 1 I 1: l I (1) 81 In the training stage, A are usually estimated through maximum likelihood (ML) training, i.e., argmax ( 1 |1 , ) p f e J I Λ = Λ (3) ML Λ and the efficient Expectation-Maximization algorithm can be used to optimize A iteratively until convergence (Rabiner 1989). For the interest of this paper, we elaborate transition parameter estimation with more details. These transition probabilities {p(i |i ′,I)} is a multinomial distribution estimated according to (2), where at each iteration the distortion set {c(i - i&apos;)} is the fractional count of transitions with jump width d = i - i&apos;, i.e., J I −1 c(d) =EEPr(aj =i,aj+1 = i + d |fJ,e1,Λ) (4) j= 1 i=1 where A&apos; is the model obtained from the immediate previous iteration and these terms in (4) can be efficiently computed by using the ForwardBackward algorithm (Rabiner 1989). In practice, we can bucket the disto</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner, 1989 A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>H T Ilhan</author>
<author>C D Manning</author>
</authors>
<title>Extensions to HMM-based Statistical Word Alignment Models.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3925" citStr="Toutanova et al., 2002" startWordPosition="626" endWordPosition="629">Machine Translation, pages 80–87, Prague, June 2007. c�2007 Association for Computational Linguistics forward (monotonic alignment) or jumping backward (non-monotonic alignment), is not modeled. In this paper, we present a method to further improve the transition models for HMM alignment model. For each source word e, we not only model its self-transition probability, but also the probability of jumping from word e to a different word. For this purpose, we estimate a full transition model for each source word. A key problem for detailed word-dependent transition modeling is data sparsity. In (Toutanova et al., 2002), the word dependent self-transition probability P(stay|e) is interpolated with the global HMM self-transition probability to alleviate the data sparsity problem, where an interpolation weight is used for all words and that weight is tuned on a hold-out set. In the proposed word dependent transition model, because there are a large number of parameters to estimate, the data sparsity problem is even more severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not op</context>
<context position="25697" citStr="Toutanova et al. (2002)" startWordPosition="4604" endWordPosition="4607">uce the end-to-end word alignment. model runtime (min) WDHMM 121 IBM model 4 537 Table 6: comparison of runtime performance bewteen WDHMM training and IBM model 4 training using GIZA++. 6 Discussion Other works have been done to improve transition models in HMM based word alignment. Och and Ney (2000a) have suggested estimating word-class based transition models so as to provide more detailed transition probabilities. However, due to the sparse data problem, only a small number of word classes are usually used and the many words in the same class still have to share the same transition model. Toutanova et al. (2002) has proposed to BLEU % 21.5 20.5 22 21 201 1.5 2 2.5 3 3.5 4 nc-test BLEU % 29.5 30.5 291 1.5 2 2.5 3 3.5 4 31 30 devtest test 86 estimate a word-dependent self-transition model P(stay|e) so that each word can have its own probability to decide whether to stay or jump to a different word. Later Deng and Byrne (2005) proposed a word dependent phrase length model to better model state occupancy. However, these model can only model the probability of selfjumping. Important knowledge of jumping from e to a different position should also be word dependent but is not modeled. Another interesting co</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>K. Toutanova, H. T. Ilhan, and C. D. Manning. 2002. Extensions to HMM-based Statistical Word Alignment Models. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based Word Alignment In Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1166" citStr="Vogel et al., 1996" startWordPosition="185" endWordPosition="188">n. We also conducted machine translation (MT) experiments on the Europarl corpus. MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models. In the HMM based alignment model (Vogel et al., 1996), it is assumed that the HMM transition probabilities depend only on the jump width from the last state to the next</context>
<context position="4929" citStr="Vogel et al., 1996" startWordPosition="782" endWordPosition="785">problem is even more severe. Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal. In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training. With the help of the prior distribution of the model, the training is regularized and results in robust models. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a). Then we describe the equations of MAP training for word dependent transition models. In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4. We also conducted phrase-based machine translation experiments on the Europarl corpus, English – French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4. 2 Baseline HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 20</context>
<context position="6384" citStr="Vogel et al., 1996" startWordPosition="1066" endWordPosition="1069">rd. In the HMM based word alignment, a HMM is built at English side, i.e., each (position, word) pair, ( , ) aj eaj , is a HMM state, which emits the French word fj. In order to mitigate the sparse data problem, it is assumed that the emission probability only depends on the English word, i.e., p(fj |aj,eaj) = p(fj |eaj ) , and the transition probability only depends on the position of the last state and the length of the English sentence, i.e., p aj aj eaj I p aj aj I ( |, , ) ( |1 , ) = . Then, Vogel et −1 − −1 al. (1996) give J p(fJ |e;)=1:∏[p(aj |aj−,I)p(fj |eaj) a j J =1 1 In the HMM of (Vogel et al., 1996), it is further assumed these transition probabilities p(aj = i |aj−1 = i′, I) depend only on the jump width (i - i&apos;), i.e., Therefore, the transition probability p(aj |aj−1,I) depends on aj-1 but only through the distortion set {c(i - i&apos;)}. In (Och and Ney, 2000a), the word null is introduced to generate the French words that don&apos;t align to any English words. If we denote by j_ the position of the last French word before j that aligns to a non-null English word, the transition probabilities p(aj = i |aj−1 = i′,I) in (1) is computed as p(aj =i|aj_=i′,I)=p%(i|i′,I), where I p if 0 i = 0 p i i I</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based Word Alignment In Statistical Translation. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Stochastic Lexicalized Inversion Transduction Grammar for Alignment,</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1303" citStr="Zhang and Gildea, 2005" startWordPosition="204" endWordPosition="207">ethod can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment. 1 Introduction Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models. In the HMM based alignment model (Vogel et al., 1996), it is assumed that the HMM transition probabilities depend only on the jump width from the last state to the next state. Therefore, the knowledge of transition probabilities given a particular source word e is not sufficiently modeled. In order to im</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>H. Zhang and D. Gildea, 2005, Stochastic Lexicalized Inversion Transduction Grammar for Alignment, In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>