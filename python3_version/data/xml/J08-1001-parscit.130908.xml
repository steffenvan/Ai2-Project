<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997571">
Modeling Local Coherence:
An Entity-Based Approach
</title>
<author confidence="0.998403">
Regina Barzilay*
</author>
<affiliation confidence="0.995708">
Massachusetts Institute of Technology
</affiliation>
<author confidence="0.973174">
Mirella Lapata**
</author>
<affiliation confidence="0.987276">
University of Edinburgh
</affiliation>
<bodyText confidence="0.98890075">
This article proposes a novel framework for representing and measuring local coherence. Central
to this approach is the entity-grid representation of discourse, which captures patterns of entity
distribution in a text. The algorithm introduced in the article automatically abstracts a text
into a set of entity transition sequences and records distributional, syntactic, and referential
information about discourse entities. We re-conceptualize coherence assessment as a learning
task and show that our entity-based representation is well-suited for ranking-based generation
and text classification tasks. Using the proposed representation, we achieve good performance on
text ordering, summary coherence evaluation, and readability assessment.
</bodyText>
<sectionHeader confidence="0.995167" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999854125">
A key requirement for any system that produces text is the coherence of its output.
Not surprisingly, a variety of coherence theories have been developed over the years
(e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found
application in many symbolic text generation systems (e.g., Scott and de Souza 1990;
Kibble and Power 2004). The ability of these systems to generate high quality text,
almost indistinguishable from human writing, makes the incorporation of coherence
theories in robust large-scale systems particularly appealing. The task is, however,
challenging considering that most previous efforts have relied on handcrafted rules,
valid only for limited domains, with no guarantee of scalability or portability (Reiter
and Dale 2000). Furthermore, coherence constraints are often embedded in complex
representations (e.g., Asher and Lascarides 2003) which are hard to implement in a
robust application.
This article focuses on local coherence, which captures text relatedness at the level
of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global
coherence and has received considerable attention in computational linguistics (Foltz,
Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller
</bodyText>
<note confidence="0.935831375">
* Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar
Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu.
** School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk.
Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for
publication: 5 May 2007.
© 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
2004; Karamanis et al. 2004). It is also supported by much psycholinguistic evidence. For
</note>
<bodyText confidence="0.993725846153846">
instance, McKoon and Ratcliff (1992) argue that local coherence is the primary source of
inference-making during reading.
The key premise of our work is that the distribution of entities in locally coher-
ent texts exhibits certain regularities. This assumption is not arbitrary—some of these
regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein
1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The
algorithm introduced in the article automatically abstracts a text into a set of entity tran-
sition sequences, a representation that reflects distributional, syntactic, and referential
information about discourse entities.
We argue that the proposed entity-based representation of discourse allows us
to learn the properties of coherent texts from a corpus, without recourse to manual
annotation or a predefined knowledge base. We demonstrate the usefulness of this rep-
resentation by testing its predictive power in three applications: text ordering, automatic
evaluation of summary coherence, and readability assessment.
We formulate the first two problems—text ordering and summary evaluation—as
ranking problems, and present an efficiently learnable model that ranks alternative ren-
derings of the same information based on their degree of local coherence. Such a mecha-
nism is particularly appropriate for generation and summarization systems as they can
produce multiple text realizations of the same underlying content, either by varying pa-
rameter values, or by relaxing constraints that control the generation process. A system
equipped with a ranking mechanism could compare the quality of the candidate
outputs, in much the same way speech recognizers employ language models at the
sentence level.
In the text-ordering task our algorithm has to select a maximally coherent sen-
tence order from a set of candidate permutations. In the summary evaluation task,
we compare the rankings produced by the model against human coherence judgments
elicited for automatically generated summaries. In both experiments, our method yields
improvements over state-of-the-art models. We also show the benefits of the entity-
based representation in a readability assessment task, where the goal is to predict the
comprehension difficulty of a given text. In contrast to existing systems which focus on
intra-sentential features, we explore the contribution of discourse-level features to this
task. By incorporating coherence features stemming from the proposed entity-based
representation, we improve the performance of a state-of-the-art readability assessment
system (Schwarm and Ostendorf 2005).
In the following section, we provide an overview of entity-based theories of lo-
cal coherence and outline previous work on its computational treatment. Then, we
introduce our entity-based representation, and define its linguistic properties. In the
subsequent sections, we present our three evaluation tasks, and report the results of our
experiments. Discussion of the results concludes the article.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="related work">
2. Related Work
</sectionHeader>
<bodyText confidence="0.999300166666667">
Our approach is inspired by entity-based theories of local coherence, and is well-suited
for developing a coherence metric in the context of a ranking-based text generation
system. We first summarize entity-based theories of discourse, and overview previous
attempts for translating their underlying principles into computational coherence mod-
els. Next, we describe ranking approaches to natural language generation and focus on
coherence metrics used in current text planners.
</bodyText>
<page confidence="0.985969">
2
</page>
<note confidence="0.72236">
Barzilay and Lapata Modeling Local Coherence
</note>
<subsectionHeader confidence="0.529876">
2.1 Entity-Based Approaches to Local Coherence
</subsectionHeader>
<bodyText confidence="0.99868785106383">
Linguistic Modeling. Entity-based accounts of local coherence have a long tradition
within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday
and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi,
and Weinstein 1995). A unifying assumption underlying different approaches is that
discourse coherence is achieved in view of the way discourse entities are introduced
and discussed. This observation is commonly formalized by devising constraints on the
linguistic realization and distribution of discourse entities in coherent texts.
At any point in the discourse, some entities are considered more salient than
others, and consequently are expected to exhibit different properties. In Centering
Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and
Hahn 1999; Poesio et al. 2004), salience concerns how entities are realized in an utterance
(e.g., whether they are they pronominalized or not). In other theories, salience is defined
in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and
Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More
refined accounts expand the notion of salience from a binary distinction to a scalar one;
examples include Prince’s (1981) familiarity scale, and Givon’s (1987) and Ariel’s (1988)
givenness-continuum.
The salience status of an entity is often reflected in its grammatical function and
the linguistic form of its subsequent mentions. Salient entities are more likely to ap-
pear in prominent syntactic positions (such as subject or object), and to be introduced
in a main clause. The linguistic realization of subsequent mentions—in particular,
pronominalization—is so tightly linked to salience that in some theories (e.g., Givon
1987) it provides the sole basis for defining a salience hierarchy. The hypothesis is that
the degree of underspecification in a referring expression indicates the topical status of
its antecedent (e.g., pronouns refer to very salient entities, whereas full NPs refer to less
salient ones). In Centering Theory, this phenomenon is captured in the Pronoun Rule,
and Givon’s Scale of Topicality and Ariel’s Accessibility Marking Scale propose a graded
hierarchy of underspecification that ranges from zero anaphora to full noun phrases,
and includes stressed and unstressed pronouns, demonstratives with modifiers, and
definite descriptions.
Entity-based theories capture coherence by characterizing the distribution of en-
tities across discourse utterances, distinguishing between salient entities and the rest.
The intuition here is that texts about the same discourse entity are perceived to be
more coherent than texts fraught with abrupt switches from one topic to the next. The
patterned distribution of discourse entities is a natural consequence of topic continuity
observed in a coherent text. Centering Theory formalizes fluctuations in topic continuity
in terms of transitions between adjacent utterances. The transitions are ranked, that
is, texts demonstrating certain types of transitions are deemed more coherent than texts
where such transitions are absent or infrequent. For example, CONTINUE transitions
require that two utterances have at least one entity in common and are preferred
over transitions that repeatedly SHIFT from one entity to the other. Givon’s (1987) and
Hoey’s (1991) accounts of discourse continuity complement local measurements by
considering global characteristics of entity distribution, such as the lifetime of an entity
in discourse and the referential distance between subsequent mentions.
Computational Modeling. An important practical question is how to translate principles
of these linguistic theories into a robust coherence metric. A great deal of research
has been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich
</bodyText>
<page confidence="0.997162">
3
</page>
<note confidence="0.802786">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.998336">
2000; Hasler 2004; Karamanis et al. 2004). Such translation is challenging in several
respects: one has to determine ways of combining the effects of various constraints and
to instantiate parameters of the theory that are often left underspecified. Poesio et al.
(2004) note that even for fundamental concepts of Centering Theory such as “utterance,”
“realization,” and “ranking,” multiple—and often contradictory—interpretations have
been developed over the years, because in the original theory these concepts are not
explicitly fleshed out. For instance, in some Centering papers, entities are ranked with
respect to their grammatical function (Brennan, Friedman, and Pollard 1987; Walker,
Iida, and Cote 1994; Grosz, Joshi, and Weinstein 1995), and in others with respect to their
position in Prince’s (1981) givenness hierarchy (Strube and Hahn 1999) or their thematic
role (Sidner 1979). As a result, two “instantiations” of the same theory make different
predictions for the same input. Poesio et al. (2004) explore alternative specifications
proposed in the literature, and demonstrate that the predictive power of the theory is
highly sensitive to its parameter definitions.
A common methodology for translating entity-based theories into computational
models is to evaluate alternative specifications on manually annotated corpora. Some
studies aim to find an instantiation of parameters that is most consistent with observable
data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies
adopt a specific instantiation with the goal of improving the performance of a metric on
a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays
with entity transition information, and show that the distribution of transitions corre-
lates with human grades. Analogously, Hasler (2004) investigates whether Centering
Theory can be used in evaluating the readability of automatic summaries by annotating
human and machine generated extracts with entity transition information.
The present work differs from these approaches in goal and methodology. Although
our work builds upon existing linguistic theories, we do not aim to directly implement
or refine any of them in particular. We provide our model with sources of knowledge
identified as essential by these theories, and leave it to the inference procedure to
determine the parameter values and an optimal way to combine them. From a design
viewpoint, we emphasize automatic computation for both the underlying discourse
representation and the inference procedure. Thus, our work is complementary to com-
putational models developed on manually annotated data (Miltsakaki and Kukich 2000;
Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us
to perform a large scale evaluation of differently instantiated coherence models across
genres and applications.
</bodyText>
<subsectionHeader confidence="0.99968">
2.2 Ranking Approaches in Natural Language Generation
</subsectionHeader>
<bodyText confidence="0.999886">
Ranking approaches have enjoyed an increasing popularity at all stages in the
generation pipeline, ranging from text planning to surface realization (Knight and
Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow,
and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an
underlying system produces a potentially large set of candidate outputs, with respect
to various text generation rules encoded as hard constraints. Not all of the resulting
alternatives will correspond to well-formed texts, and of those which may be judged ac-
ceptable, some will be preferable to others. The candidate generation phase is followed
by an assessment phase in which the candidates are ranked based on a set of desirable
properties encoded in a ranking function. The top-ranked candidate is selected for
presentation. A two-stage generate-and-rank architecture circumvents the complexity
</bodyText>
<page confidence="0.992535">
4
</page>
<note confidence="0.588324">
Barzilay and Lapata Modeling Local Coherence
</note>
<bodyText confidence="0.999782756097561">
of traditional generation systems, where numerous, often conflicting constraints, have
to be encoded during development in order to produce a single high-quality output.
Because the focus of our work is on text coherence, we discuss here rank-
ing approaches applied to text planning (see Walker et al. [2001] and Knight and
Hatzivassiloglou [1995] for ranking approaches to sentence planning and surface re-
alization, respectively). The goal of text planning is to determine the content of a text
by selecting a set of information-bearing units and arranging them into a structure that
yields well-formed output. Depending on the system, text plans are represented as dis-
course trees (Mellish et al. 1998) or linear sequences of propositions (Karamanis 2003).
Candidate text structures may differ in terms of the selected propositions, the sequence
in which facts are presented, the topology of the tree, or the order in which entities are
introduced. A set of plausible candidates can be created via stochastic search (Mellish
et al. 1998) or by a symbolic text planner following different text-formation rules (Kibble
and Power 2004). The best candidate is chosen using an evaluation or ranking function
often encoding coherence constraints. Although the type and complexity of constraints
vary greatly across systems, they are commonly inspired by Rhetorical Structure Theory
or entity-based constraints similar to the ones captured by our method. For instance,
the ranking function used by Mellish et al. gives preference to plans where consecutive
facts mention the same entities and is sensitive to the syntactic environment in which
the entity is first introduced (e.g., in a subject or object position). Karamanis finds
that a ranking function based solely on the principle of continuity achieves competi-
tive performance against more sophisticated alternatives when applied to ordering
short descriptions of museum artifacts.1 In other applications, the ranking function is
more complex, integrating rules from Centering Theory along with stylistic constraints
(Kibble and Power 2004).
A common feature of current implementations is that the specification of the rank-
ing function—feature selection and weighting—is performed manually based on the
intuition of the system developer. However, even in a limited domain this task has
proven difficult. Mellish et al. (1998; page 100) note: “The problem is far too complex
and our knowledge of the issues involved so meager that only a token gesture can be
made at this point.” Moreover, these ranking functions operate over semantically rich
input representations that cannot be created automatically without extensive knowl-
edge engineering. The need for manual coding impairs the portability of existing meth-
ods for coherence ranking to new applications, most notably to text-to-text generation
applications, such as summarization.
In the next section, we present a method for coherence assessment that overcomes
these limitations: We introduce an entity-based representation of discourse that is auto-
matically computed from raw text; we argue that the proposed representation reveals
entity transition patterns characteristic of coherent texts. The latter can be easily trans-
lated into a large feature space which lends itself naturally to the effective learning of a
ranking function, without explicit manual involvement.
</bodyText>
<sectionHeader confidence="0.996417" genericHeader="method">
3. The Coherence Model
</sectionHeader>
<bodyText confidence="0.8922025">
In this section we describe our entity-based representation of discourse. We explain how
it is computed and how entity transition patterns are extracted. We also discuss how
</bodyText>
<footnote confidence="0.910677">
1 Each utterance in the discourse refers to at least one entity in the utterance that precedes it.
</footnote>
<page confidence="0.966085">
5
</page>
<note confidence="0.287329">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.972082">
these patterns can be encoded as feature vectors appropriate for performing coherence-
related ranking and classification tasks.
</bodyText>
<subsectionHeader confidence="0.999738">
3.1 The Entity-Grid Discourse Representation
</subsectionHeader>
<bodyText confidence="0.999982259259259">
Each text is represented by an entity grid, a two-dimensional array that captures
the distribution of discourse entities across text sentences. We follow Miltsakaki and
Kukich (2000) in assuming that our unit of analysis is the traditional sentence (i.e., a
main clause with accompanying subordinate and adjunct clauses). The rows of the
grid correspond to sentences, and the columns correspond to discourse entities. By
discourse entity we mean a class of coreferent noun phrases (we explain in Section 3.3
how coreferent entities are identified). For each occurrence of a discourse entity in the
text, the corresponding grid cell contains information about its presence or absence
in a sequence of sentences. In addition, for entities present in a given sentence, grid
cells contain information about their syntactic role. Such information can be expressed
in many ways (e.g., using constituent labels or thematic role information). Because
grammatical relations figure prominently in entity-based theories of local coherence (see
Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to
a string from a set of categories reflecting whether the entity in question is a subject (S),
object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–).
Grammatical role information can be extracted from the output of a broad-coverage
dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical
parser (Collins 1997; Charniak 2000). We discuss how this information was computed
for our experiments in Section 3.3.
Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2.
Because the text contains six sentences, the grid columns are of length six. Consider
for instance the grid column for the entity trial, [O – – – – X]. It records that trial is
present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the
sentences. Also note that the grid in Table 1 takes coreference resolution into account.
Even though the same entity appears in different linguistic forms, for example, Microsoft
Corp., Microsoft, and the company, it is mapped to a single entry in the grid (see the
column introduced by Microsoft in Table 1).
</bodyText>
<tableCaption confidence="0.799975">
Table 1
</tableCaption>
<bodyText confidence="0.8189585">
A fragment of the entity grid. Noun phrases are represented by their head nouns. Grid cells
correspond to grammatical roles: subjects (S), objects (O), or neither (X).
</bodyText>
<page confidence="0.883008818181818">
1 S O S X 1
O
2 – – O – – X S 2
O
3 – – S O – – – – S O O – – – – 3
4 – – S – – – 4
S
5 S O – 5
6 – X O 6
S
6
</page>
<figure confidence="0.9962605625">
Department
Trial
Microsoft
Evidence
Competitors
Markets
Products
Brands
Case
Netscape
Software
Tactics
Government
Suit
Earnings
Barzilay and Lapata Modeling Local Coherence
</figure>
<tableCaption confidence="0.831039">
Table 2
</tableCaption>
<footnote confidence="0.749893818181818">
Summary augmented with syntactic annotations for grid computation.
1 [The Justice Department]S is conducting an [anti-trust trial]O against [Microsoft Corp.]X
with [evidence]X that [the company]S is increasingly attempting to crush [competitors]O.
2 [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its own
products]S are not competitive enough to unseat [established brands]O.
3 [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring
[Netscape]O into merging [browser software]O.
4 [Microsoft]S claims [its tactics]S are commonplace and good economically.
5 [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb [competition]O
through [collusion]X is [a violation of the Sherman Act]O.
6 [Microsoft]S continues to show [increased earnings]O despite [the trial]X.
</footnote>
<bodyText confidence="0.999577333333333">
When a noun is attested more than once with a different grammatical role in the
same sentence, we default to the role with the highest grammatical ranking: subjects are
ranked higher than objects, which in turn are ranked higher than the rest. For example,
the entity Microsoft is mentioned twice in Sentence 1 with the grammatical roles x (for
Microsoft Corp.) and s (for the company), but is represented only by s in the grid (see
Tables 1 and 2).
</bodyText>
<subsectionHeader confidence="0.999105">
3.2 Entity Grids as Feature Vectors
</subsectionHeader>
<bodyText confidence="0.996217555555555">
A fundamental assumption underlying our approach is that the distribution of entities
in coherent texts exhibits certain regularities reflected in grid topology. Some of these
regularities are formalized in Centering Theory as constraints on transitions of the
local focus in adjacent sentences. Grids of coherent texts are likely to have some dense
columns (i.e., columns with just a few gaps, such as Microsoft in Table 1) and many
sparse columns which will consist mostly of gaps (see markets and earnings in Table 1).
One would further expect that entities corresponding to dense columns are more often
subjects or objects. These characteristics will be less pronounced in low-coherence texts.
Inspired by Centering Theory, our analysis revolves around patterns of local entity
transitions. A local entity transition is a sequence {S, O, X, –}n that represents entity
occurrences and their syntactic roles in n adjacent sentences. Local transitions can be
easily obtained from a grid as continuous subsequences of each column. Each transition
will have a certain probability in a given grid. For instance, the probability of the
transition [S –] in the grid from Table 1 is 0.08 (computed as a ratio of its frequency
[i.e., six] divided by the total number of transitions of length two [i.e., 75]). Each text
can thus be viewed as a distribution defined over transition types.
We can now go one step further and represent each text by a fixed set of transition
sequences using a standard feature vector notation. Each grid rendering j of a document
di corresponds to a feature vector Φ(xij) = (p1(xij), p2(xij), ... , pm(xij)), where m is the
number of all predefined entity transitions, and pt(xij) the probability of transition t
in grid xij. This feature vector representation is usefully amenable to machine learning
algorithms (see our experiments in Sections 4–6). Furthermore, it allows the consid-
eration of large numbers of transitions which could potentially uncover novel entity
distribution patterns relevant for coherence assessment or other coherence-related tasks.
Note that considerable latitude is available when specifying the transition types to
be included in a feature vector. These can be all transitions of a given length (e.g., two
or three) or the most frequent transitions within a document collection. An example of
</bodyText>
<page confidence="0.997007">
7
</page>
<figure confidence="0.759584666666667">
Computational Linguistics Volume 34, Number 1
a feature space with transitions of length two is illustrated in Table 3. The second row
(introduced by d1) is the feature vector representation of the grid in Table 1.
</figure>
<subsectionHeader confidence="0.998274">
3.3 Grid Construction: Linguistic Dimensions
</subsectionHeader>
<bodyText confidence="0.999891">
One of the central research issues in developing entity-based models of coherence is
determining what sources of linguistic knowledge are essential for accurate prediction,
and how to encode them succinctly in a discourse representation. Previous approaches
tend to agree on the features of entity distribution related to local coherence—the
disagreement lies in the way these features are modeled.
Our study of alternative encodings is not a mere duplication of previous ef-
forts (Poesio et al. 2004) that focus on linguistic aspects of parameterization. Because we
are interested in an automatically constructed model, we have to take into account com-
putational and learning issues when considering alternative representations. Therefore,
our exploration of the parameter space is guided by three considerations: the linguistic
importance of a parameter, the accuracy of its automatic computation, and the size of the
resulting feature space. From the linguistic side, we focus on properties of entity distri-
bution that are tightly linked to local coherence, and at the same time allow for multiple
interpretations during the encoding process. Computational considerations prevent us
from considering discourse representations that cannot be computed reliably by exist-
ing tools. For instance, we could not experiment with the granularity of an utterance—
sentence versus clause—because available clause separators introduce substantial noise
into a grid construction. Finally, we exclude representations that will explode the size of
the feature space, thereby increasing the amount of data required for training the model.
Entity Extraction. The accurate computation of entity classes is key to computing mean-
ingful entity grids. In previous implementations of entity-based models, classes of coref-
erent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis
et al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvious
solution for identifying entity classes is to employ an automatic coreference resolution
tool that determines which noun phrases refer to the same entity in a document.
Current approaches recast coreference resolution as a classification task. A pair
of NPs is classified as coreferring or not based on constraints that are learned from
an annotated corpus. A separate clustering mechanism then coordinates the possibly
contradictory pairwise classifications and constructs a partition on the set of NPs. In
our experiments, we employ Ng and Cardie’s (2002) coreference resolution system.
The system decides whether two NPs are coreferent by exploiting a wealth of lexical,
grammatical, semantic, and positional features. It is trained on the MUC (6–7) data sets
and yields state-of-the-art performance (70.4 F-measure on MUC-6 and 63.4 on MUC-7).
</bodyText>
<tableCaption confidence="0.746578">
Table 3
</tableCaption>
<bodyText confidence="0.644496">
Example of a feature-vector document representation using all transitions of length two given
syntactic categories S, O, X, and –.
</bodyText>
<equation confidence="0.77925675">
S S S O S X S – O S O O O X O – X S X O X X X – – S – O – X – –
d1 .01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59
d2 .02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36
d3 .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39
</equation>
<page confidence="0.975377">
8
</page>
<bodyText confidence="0.990332575">
Barzilay and Lapata Modeling Local Coherence
Although machine learning approaches to coreference resolution have been rea-
sonably successful—state-of-the-art coreference tools today reach an F-measure2 of
70% when trained on newspaper texts—it is unrealistic to assume that such tools will
be readily available for different domains and languages. We therefore consider an
additional approach to entity extraction where entity classes are constructed simply by
clustering nouns on the basis of their identity. In other words, each noun in a text cor-
responds to a different entity in a grid, and two nouns are considered coreferent only if
they are identical. Under this view Microsoft Corp. from Table 2 (Sentence 1) corresponds
to two entities, Microsoft and Corp., which are in turn distinct from the company. This
approach is only a rough approximation to fully fledged coreference resolution, but it
is simple from an implementational perspective and produces consistent results across
domains and languages.
Grammatical Function. Several entity-based approaches assert that grammatical function
is indicative of an entity’s prominence in discourse (Hudson, Tanenhaus, and Dell 1986;
Kameyama 1986; Brennan, Friedman, and Pollard 1987; Grosz, Joshi, and Weinstein
1995). Most theories discriminate between subject, object, and the remaining grammati-
cal roles: subjects are ranked higher than objects, and these are ranked higher than other
grammatical functions.
In our framework, we can easily assess the impact of syntactic knowledge by
modifying how transitions are represented in the entity grid. In syntactically aware
grids, transitions are expressed by four categories: s, o, x and –, whereas in simplified
grids, we only record whether an entity is present (x) or absent (–) in a sentence.
We employ a robust statistical parser (Collins 1997) to determine the constituent
structure for each sentence, from which subjects (s), objects (o), and relations other than
subject or object (x) are identified. The phrase-structure output of Collins’s parser is
transformed into a dependency tree from which grammatical relations are extracted.
Passive verbs are recognized using a small set of patterns, and the underlying deep
grammatical role for arguments involved in the passive construction is entered in the
grid (see the grid cell o for Microsoft, Sentence 2, Table 2). For more details on the gram-
matical relations extraction component we refer the interested reader to Barzilay (2003).
Salience. Centering and other discourse theories conjecture that the way an entity is
introduced and mentioned depends on its global role in a given discourse. We evaluate
the impact of salience information by considering two types of models: The first model
treats all entities uniformly, whereas the second one discriminates between transitions
of salient entities and the rest. We identify salient entities based on their frequency,3 fol-
lowing the widely accepted view that frequency of occurrence correlates with discourse
prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991).
To implement a salience-based model, we modify our feature generation proce-
dure by computing transition probabilities for each salience group separately, and then
</bodyText>
<footnote confidence="0.812637333333333">
2 When evaluating the output of coreference algorithms, performance is typically measured using a
model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the
recall error by taking each equivalence class S in the gold standard and determining the number of
coreference links m that would have to be added to the system’s output to place all entities in S into
the same equivalence class produced by the system. Recall error then is the sum of ms divided by the
number of links in the gold standard. Precision error is computed by reversing the roles of the gold
standard and system output.
3 The frequency threshold is empirically determined on the development set. See Section 4.2 for further
discussion.
</footnote>
<page confidence="0.981454">
9
</page>
<note confidence="0.29423">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.998734769230769">
combining them into a single feature vector. For n transitions with k salience classes,
the feature space will be of size n × k. While we can easily build a model with multiple
salience classes, we opt for a binary distinction (i.e., k = 2). This is more in line with
theoretical accounts of salience (Chafe 1976; Grosz, Joshi, and Weinstein 1995) and
results in a moderate feature space for which reliable parameter estimation is possible.
Considering a large number of salience classes would unavoidably increase the number
of features. Parameter estimation in such a space requires a large sample of training
examples that is unavailable for most domains and applications.
Different classes of models can be defined along the linguistic dimensions just dis-
cussed. Our experiments will consider several models with varying degrees of linguistic
complexity, while attempting to strike a balance between expressivity of representation
and ease of computation. In the following sections we evaluate their performance on
three tasks: sentence ordering, summary coherence rating, and readability assessment.
</bodyText>
<subsectionHeader confidence="0.980986">
3.4 Learning
</subsectionHeader>
<bodyText confidence="0.999992925925926">
Equipped with the feature vector representation introduced herein, we can view co-
herence assessment as a machine learning problem. When considering text generation
applications, it is desirable to rank rather than classify instances: There is often no single
coherent rendering of a given text but many different possibilities that can be partially
ordered. It is therefore not surprising that systems often employ scoring functions to
select the most coherent output among alternative renderings (see the discussion in
Section 2.2). In this article we argue that encoding texts as entity transition sequences
constitutes an appropriate feature set for learning (rather than manually specifying)
such a ranking function (see Section 4 for details). We present two task-based exper-
iments that put this hypothesis to the test: information ordering (Experiment 1) and
summary coherence rating (Experiment 2). Both tasks can be naturally formulated as
ranking problems; the learner takes as input a set of alternative renderings of the
same document and ranks them based on their degree of local coherence. Examples
of such renderings are a set of different sentence orderings of the same text and a set
of summaries produced by different systems for the same document. Note that in both
ranking experiments we assume that the algorithm is provided with a limited number
of alternatives. In practice, the space of candidates can be vast, and finding the optimal
candidate may require pairing our ranking algorithm with a decoder similar to the ones
used in machine translation (Germann et al. 2004).
Although the majority of our experiments fall within the generate-and-rank frame-
work previously sketched, nothing prevents the use of our feature vector representation
for conventional classification tasks. We offer an illustration in Experiment 3, where
features extracted from entity grids are used to enhance the performance of a readability
assessment system. Here, the learner takes as input a set of documents labeled with
discrete classes (e.g., denoting whether a text is difficult or easy to read) and learns to
make predictions for unseen instances (see Section 6 for details on the machine learning
paradigm we employ).
</bodyText>
<sectionHeader confidence="0.830106" genericHeader="method">
4. Experiment 1: Sentence Ordering
</sectionHeader>
<bodyText confidence="0.997380333333333">
Text structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al. 2004)
are commonly evaluated by their performance at information-ordering. The task con-
cerns determining a sequence in which to present a pre-selected set of information-
</bodyText>
<page confidence="0.957445">
10
</page>
<bodyText confidence="0.990090136363636">
Barzilay and Lapata Modeling Local Coherence
bearing items; this is an essential step in concept-to-text generation, multi-document
summarization, and other text-synthesis problems. The information bearing items can
be database entries (Karamanis et al. 2004), propositions (Mellish et al. 1998) or sen-
tences (Lapata 2003; Barzilay and Lee 2004). In sentence ordering, a document is viewed
as a bag of sentences and the algorithm’s task is to try to find the ordering which
maximizes coherence according to some criterion (e.g., the probability of an order).
As explained previously, we use our coherence model to rank alternative sentence
orderings instead of trying to find an optimal ordering. We do not assume that local
coherence is sufficient to uniquely determine a maximally coherent ordering—other
constraints clearly play a role here. It is nevertheless a key property of well-formed
text (documents lacking local coherence are naturally globally incoherent), and a model
which takes it into account should be able to discriminate coherent from incoher-
ent texts. In our sentence-ordering task we generate random permutations of a test
document and measure how often a permutation is ranked higher than the original
document. A non-deficient model should prefer the original text more frequently than
its permutations (see Section 4.2 for details).
We begin by explaining how a ranking function can be learned for the sentence
ordering task. Next, we give details regarding the corpus used for our experiments,
describe the methods used for comparison with our approach, and note the evaluation
metric employed for assessing model performance. Our results are presented in Sec-
tion 4.3.
</bodyText>
<subsectionHeader confidence="0.997653">
4.1 Modeling
</subsectionHeader>
<bodyText confidence="0.9997965">
Our training set consists of ordered pairs of alternative renderings (xij, xik) of the same
document di, where xij exhibits a higher degree of coherence than xik (we describe in
Section 4.2 how such training instances are obtained). Without loss of generality, we
assume j &gt; k. The goal of the training procedure is to find a parameter vector w that
yields a “ranking score” function which minimizes the number of violations of pairwise
rankings provided in the training set
</bodyText>
<equation confidence="0.939399">
V(xij,xik) E r* : w · (D(xij) &gt; w · (D(xik)
</equation>
<bodyText confidence="0.9999334">
where (xij,xik) E r* if xij is ranked higher than xik for the optimal ranking r* (in the
training data), and (D(xij) and (D(xik) are a mapping onto features representing the
coherence properties of renderings xij and xik. In our case the features correspond to
the entity transition probabilities introduced in Section 3.2. Thus, the ideal ranking
function, represented by the weight vector w would satisfy the condition
</bodyText>
<equation confidence="0.631619">
w · ((D(xij) − (D(xik)) &gt; 0 Vj, i,k such that j &gt; k
</equation>
<bodyText confidence="0.9972535">
The problem is typically treated as a Support Vector Machine constraint optimization
problem, and can be solved using the search technique described in Joachims (2002).
This approach has been shown to be highly effective in various tasks ranging from
collaborative filtering (Joachims 2002) to parsing (Toutanova, Markova, and Manning
2004). Other discriminative formulations of the ranking problem are possible (Collins
2002; Freund et al. 2003); however, we leave this to future work.
</bodyText>
<page confidence="0.995566">
11
</page>
<table confidence="0.437384">
Computational Linguistics Volume 34, Number 1
</table>
<tableCaption confidence="0.992551">
Table 4
</tableCaption>
<bodyText confidence="0.9885565">
The size of the training and test instances for the Earthquakes and Accidents corpora (measured
by the number of pairs that contain the original order and a random permutation of this order).
</bodyText>
<subsectionHeader confidence="0.743736">
Training Testing
</subsectionHeader>
<bodyText confidence="0.7592548">
Earthquakes 1,896 2,056
Accidents 2,095 2,087
Once the ranking function is learned, unseen renderings (xij,xik) of document di
can be ranked simply by computing the values w*Φ(xij) and w*Φ(xik) and sorting them
accordingly. Here, w* is the optimized parameter vector resulting from training.
</bodyText>
<subsectionHeader confidence="0.993621">
4.2 Method
</subsectionHeader>
<bodyText confidence="0.99983737037037">
Data. To acquire a large collection for training and testing, we create synthetic data,
wherein the candidate set consists of a source document and permutations of its sen-
tences. This framework for data acquisition enables large-scale automatic evaluation
and is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003;
Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004). The underlying assump-
tion is that the original sentence order in the source document must be coherent, and
so we should prefer models that rank it higher than other permutations. Because we
do not know the relative quality of different permutations, our corpus includes only
pairwise rankings that comprise the original document and one of its permutations.
Given k original documents, each with n randomly generated permutations, we obtain
k · n (trivially) annotated pairwise rankings for training and testing.
Using the technique described herein, we collected data4 in two different genres:
newspaper articles and accident reports written by government officials. The first col-
lection consists of Associated Press articles from the North American News Corpus
on the topic of earthquakes (Earthquakes). The second includes narratives from the
National Transportation Safety Board’s aviation accident database (Accidents). Both
corpora have documents of comparable length—the average number of sentences is 10.4
and 11.5, respectively. For each set, we used 100 source articles with up to 20 randomly
generated permutations for training.5 A similar method was used to obtain the test data.
Table 4 shows the size of the training and test corpora used in our experiments. We held
out 10 documents (i.e., 200 pairwise rankings) from the training data for development
purposes.
Features and Parameter Settings. In order to investigate the contribution of linguistic
knowledge on model performance we experimented with a variety of grid representa-
tions resulting in different parameterizations of the feature space from which our model
is learned. We focused on three sources of linguistic knowledge—syntax, coreference
resolution, and salience—which play a prominent role in entity-based analyses of dis-
</bodyText>
<footnote confidence="0.305898">
4 The collections are available from http://people.csail.mit.edu/regina/coherence/.
5 Short texts may have less than 20 permutations. The corpus described in the original ACL publication
(Barzilay and Lapata 2005) contained a number of duplicate permutations. These were removed from
the current version of the corpus.
</footnote>
<page confidence="0.970827">
12
</page>
<bodyText confidence="0.995197543478261">
Barzilay and Lapata Modeling Local Coherence
course coherence (see Section 3.3 for details). An additional motivation for our study
was to explore the trade-off between robustness and richness of linguistic annotations.
NLP tools are typically trained on human-authored texts, and may deteriorate in per-
formance when applied to automatically generated texts with coherence violations.
We thus compared a linguistically rich model against models that use more im-
poverished representations. More concretely, our full model (Coreference+Syntax+
Salience+) uses coreference resolution, denotes entity transition sequences via gram-
matical roles, and differentiates between salient and non-salient entities. Our less-
expressive models (seven in total) use only a subset of these linguistic features
during the grid construction process. We evaluated the effect of syntactic knowl-
edge by eliminating the identification of grammatical relations and recording solely
whether an entity is present or absent in a sentence. This process created a class
of four models of the form Coreference[+/−]Syntax−Salience[+/−]. The effect of
fully fledged coreference resolution was assessed by creating models where entity
classes were constructed simply by clustering nouns on the basis of their identity
(Coreference−Syntax[+/−]Salience[+/−]). Finally, the contribution of salience was
measured by comparing the full model which accounts separately for patterns of salient
and non-salient entities against models that do not attempt to discriminate between
them (Coreference[+/−]Syntax[+/−]Salience−).
We would like to note that in this experiment we apply a coreference resolution tool
to the original text and then generate permutations for the pairwise ranking task. An
alternative design is to apply coreference resolution to permuted texts. Because existing
methods for coreference resolution take into consideration the order of noun phrases in
a text, the accuracy of these tools on permuted sentence sequences is close to random.
Therefore, we opt to resolve coreference within the original text. Although this design
has an oracle feel to it, it is not uncommon in practical applications. For instance, in text
generation systems, content planners often operate over fully specified semantic rep-
resentations, and can thus take advantage of coreference information during sentence
ordering.
Besides variations in the underlying linguistic representation, our model is also
specified by two free parameters: the frequency threshold used to identify salient en-
tities and the length of the transition sequence. These parameters were tuned separately
for each data set on the corresponding held-out development set. Optimal salience-
based models were obtained for entities with frequency &gt;2. The optimal transition
length was &lt;3.6
In our ordering experiments, we used Joachims’s (2002) SVMlight package for train-
ing and testing with all parameters set to their default values.
Comparison with State-of-the-Art Methods. We compared the performance of our algo-
rithm against two state-of-the-art models proposed by Foltz, Kintsch, and Landauer
(1998) and Barzilay and Lee (2004). These models rely largely on lexical information
for assessing document coherence, contrary to our models which are in essence un-
lexicalized. Recall from Section 3 that our approach captures local coherence by mod-
eling patterns of entity distribution in discourse, without taking note of their lexical
instantiations. In the following we briefly describe the lexicalized models we employed
in our comparative study and motivate their selection.
</bodyText>
<footnote confidence="0.998838">
6 The models we used in our experiments are available from http://people.csail.mit.edu/
regina/coherence/ and http://homepages.inf.ed.ac.uk/mlap/coherence/.
</footnote>
<page confidence="0.996806">
13
</page>
<note confidence="0.287987">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.999876533333333">
Foltz, Kintsch, and Landauer (1998) model measures coherence as a function of
semantic relatedness between adjacent sentences. The underlying intuition here is that
coherent texts will contain a high number of semantically related words. Semantic
relatedness is computed automatically using Latent Semantic Analysis (LSA; Landauer
and Dumais 1997) from raw text without employing syntactic or other annotations. In
this framework, a word’s meaning is captured in a multi-dimensional space by a vector
representing its co-occurrence with neighboring words. Co-occurrence information is
collected in a frequency matrix, where each row corresponds to a unique word, and each
column represents a given linguistic context (e.g., sentence, document, or paragraph).
Foltz, Kintsch, and Landauer’s model use singular value decomposition (SVD; Berry,
Dumais, and O’Brien 1994) to reduce the dimensionality of the space. The transforma-
tion renders sparse matrices more informative and can be thought of as a means of
uncovering latent structure in distributional data. The meaning of a sentence is next
represented as a vector by taking the mean of the vectors of its words. The similarity
between two sentences is determined by measuring the cosine of their means:
</bodyText>
<equation confidence="0.9997446">
sim(S1,S2) = cos(µ(51),µ(�S2))
µj(S1)µj(SD
~ ~n � ~n
(µj( �S1))2 (µj( �S2))2
j=1 j=1
</equation>
<bodyText confidence="0.999424">
where µ(Si) = |Si |Eu∈Si u, and u is the vector for word u. An overall text coherence
measure can be easily obtained by averaging the cosines for all pairs of adjacent sen-
tences Si and Si+1:
</bodyText>
<equation confidence="0.9960795">
n−1~ cos(Si, Si+1)
coherence(T) = i=1 n − 1 (2)
</equation>
<bodyText confidence="0.965667823529412">
This model is a good point of comparison for several reasons: (a) it is fully automatic
and has relatively few parameters (i.e., the dimensionality of the space and the choice of
similarity function), (b) it correlates reliably with human judgments and has been used
to analyze discourse structure, and (c) it models an aspect of local coherence which is
orthogonal to ours. The LSA model is lexicalized: coherence amounts to quantifying the
degree of semantic similarity between sentences. In contrast, our model does not incor-
porate any notion of similarity: coherence is encoded in terms of transition sequences
that are document-specific rather than sentence-specific.
Our implementation of the LSA model followed closely Foltz, Kintsch, and
Landauer (1998). We constructed vector-based representations for individual words
from a lemmatized version of the North American News Corpus7 (350 million words)
using a term–document matrix. We used SVD to reduce the semantic space to 100
dimensions obtaining thus a space similar to LSA. We estimated the coherence of a doc-
ument using Equations (1) and (2). A ranking can be trivially inferred by comparing the
7 Our selection of this corpus was motivated by two factors: (a) the corpus is large enough to yield a
reliable semantic space, and (b) it consists of news stories and is therefore similar in style, vocabulary,
and content to most of the corpora employed in our coherence experiments.
</bodyText>
<equation confidence="0.8830075">
=
~n
j=1
(1)
</equation>
<page confidence="0.941805">
14
</page>
<bodyText confidence="0.994236710526316">
Barzilay and Lapata Modeling Local Coherence
coherence score assigned to the original document against each of its permutations. Ties
are resolved randomly.
Both LSA and our entity-grid model are local—they model sentence-to-sentence
transitions without being aware of global document structure. In contrast, the content
models developed by Barzilay and Lee (2004) learn to represent more global text prop-
erties by capturing topics and the order in which these topics appear in texts from the
same domain. For instance, a typical earthquake newspaper report contains information
about the quake’s epicenter, how much it measured, the time it was felt, and whether
there were any victims or damage. By encoding constraints on the ordering of these
topics, content models have a pronounced advantage in modeling document structure
because they can learn to represent how documents begin and end, but also how the
discourse shifts from one topic to the next. Like LSA, the content models are lexicalized;
however, unlike LSA, they are domain-specific, and would expectedly yield inferior
performance on out-of-domain texts.
Barzilay and Lee (2004) implemented content models using an HMM wherein states
correspond to distinct topics (for instance, the epicenter of an earthquake or the number
of victims), and state transitions represent the probability of changing from one topic
to another, thereby capturing possible topic-presentation orderings within a domain.
Topics refer to text spans of varying granularity and length. Barzilay and Lee used
sentences in their experiments, but clauses or paragraphs would also be possible.
Barzilay and Lee (2004) employed their content models to find a high-probability
ordering for a document whose sentences had been randomly shuffled. Here, we use
content models for the simpler coherence ranking task. Given two text permutations,
we estimate their likelihood according to their HMM model and select the text with the
highest probability. Because the two candidates contain the same set of sentences, the
assumption is that a more probable text corresponds to an ordering that is more typical
for the domain of interest.
In our experiments, we built two content models, one for the Accidents corpus and
one for the Earthquake corpus. Although these models are trained in an unsupervised
fashion, a number of parameters related to the model topology (i.e., number of states
and smoothing parameters) affect their performance. These parameters were tuned on
the development set and chosen so as to optimize the models’ performance on the
pairwise ranking task.
Evaluation Metric. Given a set of pairwise rankings (an original document and one of
its permutations), we measure accuracy as the ratio of correct predictions made by the
model over the size of the test set. In this setup, random prediction results in an accuracy
of 50%.
</bodyText>
<subsectionHeader confidence="0.686641">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.97770825">
Impact of Linguistic Representation. We first investigate how different types of linguistic
knowledge influence our model’s performance. Table 5 shows the accuracy on the or-
dering task when the model is trained on different grid representations. As can be seen,
in both domains, the full model Coreference+Syntax+Salience+ significantly outper-
forms a linguistically naive model which simply records the presence (and absence)
of entities in discourse (Coreference−Syntax−Salience−). Moreover, we observe that
linguistically impoverished models consistently perform worse than their linguisti-
cally elaborate counterparts. We assess whether differences in accuracy are statistically
</bodyText>
<page confidence="0.965168">
15
</page>
<table confidence="0.578772">
Computational Linguistics Volume 34, Number 1
</table>
<tableCaption confidence="0.935168857142857">
Table 5
Accuracy measured as a fraction of correct pairwise rankings in the test set. Coreference[+/−]
indicates whether coreference information has been used in the construction of the entity grid.
Similarly, Syntax[+/−] and Salience[+/−] reflect the use of syntactic and salience information.
Diacritics ** (p &lt; .01) and * (p &lt; .05) indicate whether differences in accuracy between the full
model (Coreference+Syntax+Salience+) and all other models are significant (using a Fisher
Sign test).
</tableCaption>
<table confidence="0.999593090909091">
Model Earthquakes Accidents
Coreference+Syntax+Salience+ 87.2 90.4
Coreference+Syntax+Salience− 88.3 90.1
Coreference+Syntax−Salience+ 86.6 88.4**
Coreference−Syntax+Salience+ 83.0** 89.9
Coreference+Syntax−Salience− 86.1 89.2
Coreference−Syntax+Salience− 82.3** 88.6*
Coreference−Syntax−Salience+ 83.0** 86.5**
Coreference−Syntax−Salience− 81.4** 86.0**
HMM-based Content Models 88.0 75.8**
Latent Semantic Analysis 81.0** 87.3**
</table>
<bodyText confidence="0.999924666666667">
significant using a Fisher Sign Test. Specifically, we compare the full model against each
of the less expressive models (see Table 5).
Let us first discuss in more detail how the contribution of different knowl-
edge sources varies across domains. On the Earthquakes corpus every model that
does not use coreference information (Coreference−Syntax[+/−]Salience[+/−]) per-
forms significantly worse than models augmented with coreference (Coreference+
Syntax[+/−]Salience[+/−]). This effect is less pronounced on the Accidents corpus,
especially for model Coreference−Syntax+Salience+ whose accuracy drops only
by 0.5% (the difference between Coreference−Syntax+Salience+ and Coreference+
Syntax+Salience+ is not statistically significant). The same model’s performance de-
creases by 4.2% on the Earthquakes corpus. This variation can be explained by differ-
ences in entity realization between the two domains. In particular, the two corpora vary
in the amount of coreference they employ; texts from the Earthquakes corpus contain
many examples of referring expressions that our simple identity-based approach cannot
possibly resolve. Consider for instance the text in Table 6. Here, the expressions the
same area, the remote region, and site all refer to Menglian county. In comparison, the text
from the Accidents corpus contains fewer referring expressions, in fact entities are often
repeated verbatim across several sentences, and therefore could be straightforwardly
resolved with a shallow approach (see the pilot, the pilot, the pilot in Table 6).
The omission of syntactic information causes a drop in accuracy for models applied
to the Accidents corpus. This effect is less noticeable on the Earthquakes corpus (com-
pare the performance of model Coreference+Syntax−Salience+ on the two corpora).
We explain this variation by the substantial difference in the type/token ratio between
the two domains—12.1 for Earthquakes versus 5.0 for Accidents. The low type/token
ratio for Accidents means that most sentences in a text have some words in common.
For example, the entities pilot, airplane, and airport appear in multiple sentences in the
text from Table 6. Because there is so much repetition in this domain, the syntax-free
grids will be relatively similar for both coherent (original) and incoherent texts (permu-
tations). In fact, inspection of the grids from the Accidents corpus reveals that they have
many sequences of the form [X X X], [X − − X], [X X − −], and [− − X X] in common,
</bodyText>
<page confidence="0.986212">
16
</page>
<note confidence="0.601798">
Barzilay and Lapata Modeling Local Coherence
</note>
<tableCaption confidence="0.98114">
Table 6
</tableCaption>
<bodyText confidence="0.8201145">
Two texts from the Earthquakes and Accidents corpus. One entity class for each document is
shown to demonstrate the difference in referring expressions used in the two corpora.
</bodyText>
<table confidence="0.5776655">
Example Text from Earthquakes
A strong earthquake hit the China-Burma border early Wednesday morning, but there
were no reports of deaths, according to China’s Central Seismology Bureau. The 7.3 quake
✄ �
✂The same area was struck by a 6.2 temblor early Monday
✁
</table>
<tableCaption confidence="0.5475115">
morning, the bureau said. The county is on the China-Burma border, and is a sparsely populated,
mountainous region. The bureau’s Xu Wei said some buildings sustained damage and there were
</tableCaption>
<figure confidence="0.9635694375">
✞ ☎
some injuries, but he had no further details. Communication with the remote region is difficult,
✝ ✆
and satellite phones sent from the neighboring province of Sichuan have not yet reached
However, he said the likelihood of deaths was low because residents should have been evacuated
�
the area following Monday’s quake.
✁
Example Text from Accidents
✞ ☎
hit Menglian county at 5:46 am.
✝ ✆
✄
the site.
✂
�
✁
✄
from
✂
✞ ☎
When the pilot failed to arrive for his brother’s college graduation, concerned family members
✝ ✆
reported that he and his airplane were missing. A search was initiated, and the Civil Air Patrol
✞ ☎
located the airplane on top of Pine Mountain. According to the pilot ’s flight log, the intended
✝ ✆
destination was Pensacola, FL, with intermediate stops for fuel at Thomson, GA, and Greenville,
AL. Airport personal at Thomson confirmed that the airplane landed about 1630 on 11/6/97.
✞ ☎
They reported that the pilot purchased 26.5 gallons of 100LL fuel and departed about 1700.
✝ ✆
</figure>
<figureCaption confidence="0.788753">
Witnesses at the Thomson Airport stated that when he took off, the weather was marginal VFR
</figureCaption>
<bodyText confidence="0.998593454545454">
and deteriorating rapidly. Witnesses near Pine Mountain stated that the visibility at the time of
the accident was about 1/4 mile in haze/fog.
whereas such sequences are more common in coherent Earthquakes documents and
more sparse in their permutations. This indicates that syntax-free analysis can suffi-
ciently discriminate coherent from incoherent texts in the Earthquakes domain, while
a more refined representation of entity transition types is required for the Accidents
domain.
The contribution of salience is less pronounced in both domains—the differ-
ence in performance between the full model (Coreference+Syntax+Salience+) and
its salience-agnostic counterpart (Coreference+Syntax+Salience+) is not statisti-
cally significant. Salience-based models do deliver some benefits for linguistically
impoverished models—for instance, Coreference−Syntax−Salience+ improves over
Coreference−Syntax−Salience− (p &lt; 0.06) on the Earthquakes corpus. We hypothesize
that the small contribution of salience is related to the way it is currently represented.
Addition of this knowledge source to our grid representation, doubles the number
of features that serve as input to the learning algorithm. In other words, salience-
aware models need to learn twice as many parameters as salience-free models, while
having access to the same amount of training data. Achieving any improvement in these
conditions is challenging.
Comparison with State-of-the-Art Methods. We next discuss the performance of the HMM-
based content models (Barzilay and Lee 2004) and LSA (Foltz, Kintsch, and Landauer
1998) in comparison to our model (Coreference+Syntax+Salience+).
</bodyText>
<page confidence="0.989564">
17
</page>
<note confidence="0.48332">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.999836959183674">
First, note that the entity-grid model significantly outperforms LSA on both do-
mains (p &lt; .01 using a Sign test, see Table 5). In contrast to our model, LSA is nei-
ther entity-based nor unlexicalized: It measures the degree of semantic overlap across
successive sentences, without handling discourse entities in a special way (all content
words in a sentence contribute towards its meaning). We attribute our model’s superior
performance, despite the lack of lexicalization, to three factors: (a) the use of more
elaborate linguistic knowledge (coreference and grammatical role information); (b) a
more holistic representation of coherence (recall that our entity grids operate over texts
rather than individual sentences; furthermore, entity transitions can span more than
two consecutive sentences, something which is not possible with the LSA model); and
(c) exposure to domain relevant texts (the LSA model used in our experiments was not
particularly tuned to the Earthquakes or Accidents corpus). Our semantic space was
created from a large news corpus (see Section 4.2) covering a wide variety of topics
and writing styles. This is necessary for constructing robust vector representations that
are not extremely sparse. We thus expect the grid models to be more sensitive to the
discourse conventions of the training/test data.
The accuracy of the HMM-based content modes is comparable to the grid model on
the Earthquakes corpus (the difference is not statistically significant) but is significantly
lower on the Accidents texts (see Table 5). Although the grid model yields similar
performance on the two domains, content models exhibit high variability. These results
are not surprising. The analysis presented in Barzilay and Lee (2004) shows that the
Earthquakes texts are quite formulaic in their structure, following the editorial style of
the Associated Press. In contrast, the Accidents texts are more challenging for content
models—reports in this set do not undergo centralized editing and therefore exhibit
more variability in lexical choice and style. The LSA model also significantly outper-
forms the content model on the Earthquakes domain (p &lt; .01 using a Sign test). Being a
local model, LSA is less sensitive to the way documents are structured and is therefore
more likely to deliver consistent performance across domains.
The comparison in Table 5 covers a broad spectrum of coherence models. At one
end of the spectrum is LSA, a lexicalized model of local discourse coherence which is
fairly robust and domain independent. In the middle of the spectrum lies our entity-
grid model, which is unlexicalized but linguistically informed and goes beyond sim-
ple sentence-to-sentence transitions without, however, fully modeling global discourse
structure. At the other end of the spectrum are the HMM-based content models, which
are both global and lexicalized. Our results indicate that these models are complemen-
tary and that their combination could yield improved results. For example, we could
lexicalize our entity grids or supply the content models with local information either in
the style of LSA or as entity transitions. However, we leave this to future work.
Training Requirements. We now examine in more detail the training requirements for the
entity-grid models. Although for our ordering experiments we obtained training data
cheaply, this will not generally be the case and some effort will have to be invested
in collecting appropriate data with coherence ratings. We thus address two questions:
(1) How much training data is required for achieving satisfactory performance? (2) How
domain sensitive are the entity-grid models? In other words, does their performance
degrade gracefully when applied to out-of-domain texts?
Figure 1 shows learning curves for the best performing model (Coreference+
Syntax+Salience+) on the Earthquakes and Accidents corpora. We observe that the
amount of data required depends on the domain at hand. The Accidents texts are more
repetitive and therefore less training data is required to achieve good performance. The
</bodyText>
<page confidence="0.988471">
18
</page>
<note confidence="0.50655">
Barzilay and Lapata Modeling Local Coherence
</note>
<figureCaption confidence="0.98175">
Figure 1
</figureCaption>
<bodyText confidence="0.853924307692308">
Learning curves for the entity-based model Coreference+Syntax+Salience+ on the
Earthquakes and Accidents corpora.
learning curve is steeper for the Earthquakes documents. Irrespective of the domain
differences, the model reaches good accuracies when half of the data set is used (1,000
pairwise rankings). This is encouraging, because for some applications (e.g., summa-
rization) large amounts of training data may be not readily available.
Table 7 illustrates the accuracy of the best performing model Coreference+
Syntax+Salience+ when trained on the Earthquakes corpus and tested on Accidents
texts and reversely when trained on the Accident corpus and tested on Earthquakes
documents. We also illustrate how this model performs when trained and tested on
a data set that contains texts from both domains. For the latter experiment the train-
ing data set was created by randomly sampling 50 Earthquakes and 50 Accidents
documents.
</bodyText>
<tableCaption confidence="0.7018316">
Table 7
Accuracy of entity-based model (Coreference+Syntax+Salience+) and HHM-based content
model on out-of-domain texts. Diacritics ** (p &lt; .01) and * (p &lt; .05) indicate whether
performances on in-domain and out-of-domain data are significantly different using a Fisher
Sign Test.
</tableCaption>
<table confidence="0.984348909090909">
Coreference+Syntax+Salience
����� Test Earthquakes Accidents
Train
Earthquakes 87.3 67.0**
Accidents 69.7** 90.4
EarthAccid 86.7 88.5*
HMM-Based Content Models
����� Test Earthquakes Accidents
Train
Earthquakes 88.0 31.7**
Accidents 60.3** 75.8
</table>
<page confidence="0.835925">
19
</page>
<note confidence="0.40836">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.998785411764706">
As can be seen from Table 7, the model’s performance degrades considerably
(approximately by 20%) when tested on out-of-domain texts. On the positive side,
the model’s out-of-domain performance is better than chance (i.e., 50%). Furthermore,
once the model is trained on data representative of both domains, it performs almost
as well as a model which has been trained exclusively on in-domain texts (see the
row EarthAccid in Table 7). To put these results into context, we also considered the
cross-domain performance of the content models. As Table 7 shows, the decrease in
performance is more dramatic for the content models. In fact, the model trained on
the Earthquakes domain plummets below the random baseline when applied to the
Accidents domain. These results are expected for content models—the two domains
have little overlap in topics and do not share structural constraints. Note that the LSA
model is not sensitive to cross-domain issues. The semantic space is constructed over
many different domains without taking into account style or writing conventions.
The cross-training performance of the entity-based models is somewhat puzzling:
these models are not lexicalized, and one would expect that valid entity transitions
are preserved across domains. Although transition types are not domain-specific, their
distribution could vary from one domain to another. To give a simple example, some
domains will have more entities than others (e.g., descriptive texts). In other words,
entity transitions capture not only text coherence properties, but also reflect stylistic
and genre-specific discourse properties. This hypothesis is indirectly confirmed by the
observed differences in the contribution of various linguistic features across the two
domains discussed above. Cross-domain differences in the distribution and occurrence
of entities have been also observed in other empirical studies of local coherence. For
instance, Poesio et al. (2004) show differences in transition types between instructional
texts and descriptions of museum texts. In Section 6, we show that features derived
from the entity grid help determine the readability level for a given text, thereby
verifying more directly the hypothesis that the grid representation captures stylistic
discourse factors.
The results presented so far suggest that adapting the proposed model to a new
domain would involve some effort in collecting representative texts with associated
coherence ratings. Thankfully, the entity grids are constructed in a fully automatic
fashion, without requiring manual annotation. This contrasts with traditional imple-
mentations of Centering Theory that operate over linguistically richer representations
that are typically hand-coded.
</bodyText>
<sectionHeader confidence="0.937364" genericHeader="method">
5. Experiment 2: Summary Coherence Rating
</sectionHeader>
<bodyText confidence="0.999813076923077">
We further test the ability of our method to assess coherence by comparing model
induced rankings against rankings elicited by human judges. Admittedly, the synthetic
data used in the ordering task only partially approximates coherence violations that
human readers encounter in machine generated texts. A representative example of
such texts are automatically generated summaries which often contain sentences taken
out of context and thus display problems with respect to local coherence (e.g., dan-
gling anaphors, thematically unrelated sentences). A model that exhibits high agree-
ment with human judges not only accurately captures the coherence properties of
the summaries in question, but ultimately holds promise for the automatic evaluation
of machine-generated texts. Existing automatic evaluation measures such as BLEU
(Papineni et al. 2002) and ROUGE (Lin and Hovy 2003) are not designed for the
coherence assessment task, because they focus on content similarity between system
output and reference texts.
</bodyText>
<page confidence="0.941845">
20
</page>
<bodyText confidence="0.447525">
Barzilay and Lapata Modeling Local Coherence
</bodyText>
<subsectionHeader confidence="0.989874">
5.1 Modeling
</subsectionHeader>
<bodyText confidence="0.9999962">
Summary coherence rating can be also formulated as a ranking learning task. We are
assuming that the learner has access to several summaries corresponding to the same
document or document cluster. Such summaries can be produced by several systems
that operate over identical inputs or by a single system (e.g., by varying the compression
length or by switching on or off individual system modules, for example a sentence
compression or anaphora resolution module). Similarly to the sentence ordering task,
our training data includes pairs of summaries (xij,xik) of the same document(s) di,
where xij is more coherent than xik. An optimal learner should return a ranking r∗ that
orders the summaries according to their coherence. As in Experiment 1 we adopt an
optimization approach and follow the training regime put forward by Joachims (2002).
</bodyText>
<subsectionHeader confidence="0.997509">
5.2 Method
</subsectionHeader>
<bodyText confidence="0.999899333333333">
Data. Our evaluation was based on materials from the Document Understanding Con-
ference (DUC 2003), which include multi-document summaries produced by human
writers and by automatic summarization systems. In order to learn a ranking, we
require a set of summaries, each of which has been rated in terms of coherence. One
stumbling block to performing this kind of evaluation is the coherence ratings them-
selves, which are not routinely provided by DUC summary evaluators. In DUC 2003, the
quality of automatically generated summaries was assessed along several dimensions
ranging from grammatically, to content selection, fluency, and readability. Coherence
was indirectly evaluated by noting the number of sentences indicating an awkward
time sequence, suggesting a wrong cause–effect relationship, or being semantically
incongruent with their neighboring sentences.8 Unfortunately, the observed coherence
violations were not fine-grained enough to be of use in our rating experiments. In
the majority of cases DUC evaluators noted either 0 or 1 violations; however, without
judging the coherence of the summary as a whole, we cannot know whether a single
violation disrupts coherence severely or not.
We therefore obtained judgments for automatically generated summaries from hu-
man subjects.9 We randomly selected 16 input document clusters and five systems that
had produced summaries for these sets, along with reference summaries composed by
humans. Coherence ratings were collected during an elicitation study by 177 unpaid
volunteers, all native speakers of English. The study was conducted remotely over the
Internet. Participants first saw a set of instructions that explained the task, and defined
the notion of coherence using multiple examples. The summaries were randomized in
lists following a Latin square design ensuring that no two summaries in a given list
were generated from the same document cluster. Participants were asked to use a seven-
point-scale to rate how coherent the summaries were without having seen the source
texts. The ratings (approximately 23 per summary) given by our subjects were averaged
to provide a rating between 1 and 7 for each summary.
The reliability of the collected judgments is crucial for our analysis; we therefore
performed several tests to validate the quality of the annotations. First, we measured
how well humans agree in their coherence assessment. We employed leave-one-out
</bodyText>
<footnote confidence="0.975775">
8 See question 12 in http://duc.nist.gov/duc2003/quality.html.
9 The ratings are available from http://homepages.inf.ed.ac.uk/mlap/coherence/.
</footnote>
<page confidence="0.998337">
21
</page>
<note confidence="0.497004">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.999315545454546">
resampling10 (Weiss and Kulikowski 1991), by correlating the data obtained from each
participant with the mean coherence ratings obtained from all other participants. The
inter-subject agreement was r = .768 (p &lt; .01.) Second, we examined the effect of differ-
ent types of summaries (human- vs. machine-generated.) An ANOVA revealed a reliable
effect of summary type: F(1;15) = 20.38, p &lt; .01 indicating that human summaries are
perceived as significantly more coherent than system-generated ones. Finally, we also
compared the elicited ratings against the DUC evaluations using correlation analysis.
The human judgments were discretized to two classes (i.e., 0 or 1) using entropy-based
discretization (Witten and Frank 2000). We found a moderate correlation between the
human ratings and DUC coherence violations (r = .41, p &lt; .01). This is expected given
that DUC evaluators were using a different scale and and were not explicitly assessing
summary coherence.
The summaries used in our rating elicitation study form the basis of a corpus
used for the development of our entity-based coherence models. To increase the size
of our training and test sets, we augmented the materials used in the elicitation study
with additional DUC summaries generated by humans for the same input sets. We
assumed that these summaries were maximally coherent. As mentioned previously, our
participants tend to rate human-authored summaries higher than machine-generated
ones. To ensure that we do not tune a model to a particular system, we used the output
summaries of distinct systems for training and testing. Our set of training materials
contained 6 x 16 summaries (average length 4.8), yielding (2) x 16 = 240 pairwise rank-
ings. Because human summaries often have identical (high) scores, we eliminated pairs
of such summaries from the training set. Consequently, the resulting training corpus
consisted of 144 summaries. In a similar fashion, we obtained 80 pairwise rankings for
the test set. Six documents from the training data were used as a development set.
Features, Parameter Settings, and Training Requirements. We examine the influence of lin-
guistic knowledge on model performance by comparing models with varying degrees
of linguistic complexity. To be able to assess the performance of our models across tasks
(e.g., sentence ordering vs. summarization), we experimented with the same model
types introduced in the previous experiment (see Section 4.3). We also investigate the
training requirements for these models on the summary coherence task.
Experiment 1 differs from the present study in the way coreference information
was obtained. In Experiment 1, a coreference resolution tool was applied to human-
written texts, which are grammatical and coherent. Here, we apply a coreference tool
to automatically generated summaries. Because many summaries in our corpus are
fraught with coherence violations, the performance of a coreference resolution tool
is likely to drop. Unfortunately, resolving coreference in the input documents would
require a multi-document coreference tool, which is currently unavailable to us.
As in Experiment 1, the frequency threshold and the length of the transition se-
quence were optimized on the development set. Optimal salience-based models were
obtained for entities with frequency ≥2. The optimal transition length was ≤2. All
models were trained and tested using SVMlight (Joachims 2002).
Comparison with State-of-the-Art Methods. Our results were compared to the LSA model
introduced in Experiment 1 (see Section 4.2 for details). Unfortunately, we could not
</bodyText>
<footnote confidence="0.5287825">
10 We cannot apply the commonly used Kappa statistic for measuring agreement because it is appropriate
for nominal scales, whereas our summaries are rated on an ordinal scale.
</footnote>
<page confidence="0.973503">
22
</page>
<bodyText confidence="0.9261864">
Barzilay and Lapata Modeling Local Coherence
employ Barzilay and Lee’s (2004) content models for the summary ranking task. Being
domain-dependent, these models require access to domain representative texts for train-
ing. Our summary corpus, however, contains texts from multiple domains and does not
provide an appropriate sample for reliably training content models.
</bodyText>
<subsectionHeader confidence="0.654227">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999553347826087">
Impact of Linguistic Representation. Our results are summarized in Table 8. Similarly
to the sentence ordering task, we observe that the linguistically impoverished model
Coreference−Syntax−Salience− exhibits decreased accuracy when compared against
models that operate over more sophisticated representations. However, the contribution
of individual knowledge sources differs in this task. For instance, coreference resolu-
tion improved model performance in ordering, but it causes a decrease in accuracy
in summary evaluation (compare the models Coreference+Syntax+Salience+ and
Coreference−Syntax+Salience+ in Tables 5 and 8). This drop in performance can be
attributed to two factors both related to the fact that our summary corpus contains
many machine-generated texts. First, an automatic coreference resolution tool will be
expected to be less accurate on our corpus, because it was trained on well-formed
human-authored texts. Second, automatic summarization systems do not use anaphoric
expressions as often as humans do. Therefore, a simple entity clustering method is more
suitable for automatic summaries.
Both salience and syntactic information contribute to the accuracy of the ranking
model. The impact of each of these knowledge sources in isolation is not dramatic—
dropping either of them yields some decrease in accuracy, but the difference is not sta-
tistically significant. However, eliminating both salience and syntactic information sig-
nificantly decreases performance (compare Coreference−Syntax+Salience+ against
Coreference+Syntax−Salience− and Coreference−Syntax−Salience− in Table 8).
Figure 2 shows the learning curve for our best model Coreference−Syntax+
Salience+. Although the model performs poorly when trained on a small fraction of the
data, it stabilizes relatively fast (with 80 pairwise rankings), and does not improve after
</bodyText>
<tableCaption confidence="0.715157666666667">
Table 8
Summary ranking accuracy measured as fraction of correct pairwise rankings in the test set.
Coreference[+/−] indicates whether anaphoric information has been used when constructing
the entity grid. Similarly, Syntax[+/−] and Salience[+/−] reflect the use of syntactic and
salience information. Diacritics ** (p &lt; .01) and * (p &lt; .05) indicate whether Coreference−
Syntax+Salience+ is significantly different from all other models (using a Fisher Sign Test).
</tableCaption>
<table confidence="0.977297">
Model Accuracy
Coreference+Syntax+Salience+ 80.0
Coreference+Syntax+Salience− 75.0
Coreference+Syntax−Salience+ 78.8
Coreference−Syntax+Salience+ 83.8
Coreference+Syntax−Salience− 71.3∗
Coreference−Syntax+Salience− 78.8
Coreference−Syntax−Salience+ 77.5
Coreference−Syntax−Salience− 73.8∗
Latent Semantic Analysis 52.5∗∗
</table>
<page confidence="0.957878">
23
</page>
<figure confidence="0.815102">
Computational Linguistics Volume 34, Number 1
</figure>
<figureCaption confidence="0.97584">
Figure 2
</figureCaption>
<bodyText confidence="0.995699346153846">
Learning curve for the entity-based model Coreference−Syntax+Salience+ applied to the
summary ranking task.
a certain point. These results suggest that further improvements to summary ranking
are unlikely to come from adding more annotated data.
Comparison with the State-of-the-Art. As in Experiment 1, we compared the best per-
forming grid model (Coreference−Syntax+Salience+) against LSA (see Table 8). The
former model significantly outperforms the latter (p &lt; .01) by a wide margin. LSA is per-
haps at a disadvantage here because it has been exposed only to human-authored texts.
Machine-generated summaries are markedly distinct from human texts even when
these are incoherent (as in the case of our ordering experiment). For example, manual
inspection of our summary corpus revealed that low-quality summaries often contain
repetitive information. In such cases, simply knowing about high cross-sentential over-
lap is not sufficient to distinguish a repetitive summary from a well-formed one.
Furthermore, note that in contrast to the documents in Experiment 1, the summaries
being ranked here differ in lexical choice. Some are written by humans (and are thus
abstracts), whereas others have been produced by systems following different summa-
rization paradigms (some systems perform rewriting whereas others extract sentences
verbatim from the source documents). This means that LSA may consider a summary
coherent simply because its vocabulary is familiar (i.e., it contains words for which
reliable vectors have been obtained). Analogously, a summary with a large number
of out-of-vocabulary lexical items will be given low similarity scores, irrespective of
whether it is coherent or not. This is not uncommon in summaries with many proper
names. These often do not overlap with the proper names found in the North American
News Corpus used for training the LSA model. Lexical differences exert much less
influence on the entity-grid model which abstracts away from alternative verbalizations
of the same content and captures coherence solely on the basis of grid topology.
</bodyText>
<sectionHeader confidence="0.969164" genericHeader="evaluation">
6. Experiment 3: Readability Assessment
</sectionHeader>
<bodyText confidence="0.9962455">
So far, our experiments have explored the potential of the proposed discourse repre-
sentation for coherence modeling. We have presented several classes of grid models
</bodyText>
<page confidence="0.981833">
24
</page>
<bodyText confidence="0.98838165625">
Barzilay and Lapata Modeling Local Coherence
achieving good performance in discerning coherent from incoherent texts. Our experi-
ments also reveal a surprising property of grid models: Even though these models are
not lexicalized, they are domain- and style-dependent. In this section, we investigate in
detail this feature of grid models. Here, we move away from the coherence rating task
and put the entity-grid representation further to the test by examining whether it can
be usefully employed in style classification. Specifically, we embed our entity grids into
a system that assesses document readability. The term describes the ease with which
a document can be read and understood. The quantitative measurement of readability
has attracted considerable interest and debate over the last 70 years (see Mitchell [1985]
and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP
technology (Schwarm and Ostendorf 2005).
A number of readability formulas have been developed with the primary aim of
assessing whether texts or books are suitable for students at particular grade levels
or ages. Many readability methods focus on simple approximations of semantic factors
concerning the words used and syntactic factors concerning the length or structure of
sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz
and Bauer 2001). Despite their widespread applicability in education and technical
writing (Kincaid et al. 1981), readability formulas are often criticized for being too
simplistic; they systematically ignore many important factors that affect readability such
as discourse coherence and cohesion, layout and formatting, use of illustrations, the
nature of the topic, the characteristics of the readers, and so forth.
Schwarm and Ostendorf (2005) developed a method for assessing readability which
addresses some of the shortcomings of previous approaches. By recasting readability
assessment as a classification task, they are able to combine several knowledge sources
ranging from traditional reading level measures, to statistical language models, and
syntactic analysis. Evaluation results show that their system outperforms two com-
monly used reading level measures (the Flesch-Kincaid Grade Level index and Lexile).
In the following we build on their approach and examine whether the entity-grid rep-
resentation introduced in this article contributes to the readability assessment task. The
incorporation of coherence-based information in the measurement of text readability is,
to our knowledge, novel.
</bodyText>
<subsectionHeader confidence="0.99939">
6.1 Modeling
</subsectionHeader>
<bodyText confidence="0.980749">
We follow Schwarm and Ostendorf (2005) in treating readability assessment as a classifi-
cation task. The unit of classification is a single article and the learner’s task is to predict
whether it is easy or difficult to read. A variety of machine learning techniques are
amenable to this problem. Because our goal was to replicate Schwarm and Ostendorf’s
system as closely as possible, we followed their choice of support vector machines
(SVMs) (Joachims 1998b) for our classification experiments. Our training sample there-
fore consisted of n documents such that
(�x1,y1),---, (4,yn) xi E R&apos;,yi E {−1,+1}
where xi is a feature vector for the ith document in the training sample and yi its
(positive or negative) class label. In the basic SVM framework, we try to separate the
positive and negative instances by a hyperplane. This means that there is a weight
</bodyText>
<page confidence="0.996151">
25
</page>
<note confidence="0.448425">
Computational Linguistics Volume 34, Number 1
</note>
<tableCaption confidence="0.993893">
Table 9
</tableCaption>
<note confidence="0.4025345">
Excerpts from the Britannica readability corpus
The Lemma Valletta in Britannica
</note>
<bodyText confidence="0.97987175">
Also spelled Valletta, seaport and capital of Malta, on the northeast coast of the island. The
nucleus of the city is built on the promontory of Mount Sceberras that runs like a tongue into
the middle of a bay, which it thus divides into two harbours, Grand Harbour to the east and
Marsamxett (Marsamuscetto) Harbour to the west. Built after the Great Siege of Malta in 1565,
which checked the advance of Ottoman power in southern Europe, it was named after Jean Parisot
de la Valette, grand master of the order of Hospitallers (Knights of St. John of Jerusalem), and
became the Maltese capital in 1570. The Hospitallers were driven out by the French in 1798, and
a Maltese revolt against the French garrison led to Valletta’s seizure by the British in 1800.
</bodyText>
<subsectionHeader confidence="0.50057">
The Lemma Valletta in Britannica Elementary
</subsectionHeader>
<bodyText confidence="0.9998992">
A port city, Valletta is the capital of the island country of Malta in the Mediterranean Sea. Valletta
is located on the eastern coast of the largest island, which is also named Malta. Valletta lies on a
peninsula—a land mass surrounded by water on three sides. It borders Marsamxett Harbor to the
north and Grand Harbor to the south. The eastern end of the city juts out into the Mediterranean.
Valletta was planned in the 16th century by the Italian architect Francesco Laparelli. To make
traveling through Valletta easier, Laparelli designed the city in a grid pattern with straight streets
that crossed each other and ran the entire width and length of the town. Valletta was one of the
first towns to be laid out in this way.
vector w and a threshold b, so that all positive training examples are on one side of the
hyperplane, while all negative ones lie on the other side. This is equivalent to requiring
</bodyText>
<equation confidence="0.727578">
yi[(w ·�xi) + b] &gt; 0
</equation>
<bodyText confidence="0.9997536">
Finding the optimal hyperplane is an optimization problem which can be solved
efficiently using the procedure described in Vapnik (1998). SVMs have been widely
used for many NLP tasks ranging from text classification (Joachims 1998b), to syntactic
chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al.
2005).
</bodyText>
<subsectionHeader confidence="0.999814">
6.2 Method
</subsectionHeader>
<bodyText confidence="0.999974285714286">
Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003)
from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version
targeted at children. The corpus contains 107 articles from the full version of the encyclo-
pedia and their corresponding simplified articles from Britannica Elementary (214 articles
in total). Although these texts are not explicitly annotated with grade levels, they still
represent two broad readability categories, namely, easy and difficult.11 Examples of
these two categories are given in Table 9.
</bodyText>
<footnote confidence="0.698425666666667">
11 The Britannica corpus was also used by Schwarm and Ostendorf (2005); in addition they make use of a
corpus compiled from the Weekly Reader, an educational newspaper with documents targeted at grade
levels 2–5. Unfortunately, this corpus is not publicly available.
</footnote>
<page confidence="0.978031">
26
</page>
<bodyText confidence="0.982197962962963">
Barzilay and Lapata Modeling Local Coherence
Features and Parameter Settings. We created two system versions: the first one used solely
Schwarm and Ostendorf (2005) features;12 the second one employed a richer feature
space—we added the entity-based representation proposed here to their original feature
set. We will briefly describe the readability-related features used in our systems and
direct the interested reader to Schwarm and Ostendorf for a more detailed discussion.
Schwarm and Ostendorf (2005) use three broad classes of features: syntactic, se-
mantic, and their combination. Their syntactic features are average sentence length and
features extracted from parse trees computed using Charniak’s (2000) parser. The latter
include average parse tree height, average number of NPs, average number of VPs, and
average number of subordinate clauses (SBARs). We computed average sentence length
by measuring the number of tokens per sentence.
Their semantic features include the average number of syllables per word, and
language model perplexity scores. A unigram, bigram, and trigram model was esti-
mated for each class, and perplexity scores were used to assess their performance on
test data. Following Schwarm and Ostendorf (2005) we used information gain to select
words that were good class discriminants. All remaining words were replaced by their
parts of speech. The vocabulary thus consisted of 300 words with high information
gain and 36 Penn Treebank part-of-speech tags. The language models were estimated
using maximum likelihood estimation and smoothed with Witten-Bell discounting.
The language models described in this article were all built using the CMU statistical
language modeling toolkit (Clarkson and Rosenfeld 1997). Our perplexity scores were
six in total (2 classes x 3 language models).
Finally, the Flesch-Kincaid Grade Level score was included as a feature that cap-
tures both syntactic and semantic text properties. The Flesch-Kincaid formula estimates
readability as a combination of the the average number of syllables per word and the
average number of words per sentence:
</bodyText>
<equation confidence="0.4847995">
0.39 ( total words I + 11.8 ( total syllables 1 − 15.59 (3)
\ total sentences) ` total words J
</equation>
<bodyText confidence="0.999965">
We also enriched Schwarm and Ostendorf’s (2005) feature space with coherence-
based features. Each document was represented as a feature vector using the entity tran-
sition notation introduced in Section 3. We experimented with two models that yielded
good performances in our previous experiments: Coreference+Syntax+Salience+ (see
Experiment 1) and Coreference−Syntax+Salience+ (see Experiment 2). The transition
length was ≤2 and entities were considered salient if they occurred ≥2 times. As in our
previous experiments, we compared the entity-based representation against LSA. The
latter is a measure of the semantic relatedness across pairs of sentences. We could not
apply the HMM-based content models (Barzilay and Lee 2004) to the readability data
set. The encyclopedia lemmas are written by different authors and consequently vary
considerably in structure and vocabulary choice. Recall that these models are suitable
for more restricted domains and texts that are more formulaic in nature.
</bodyText>
<footnote confidence="0.957822666666667">
12 Schwarm and Ostendorf (2005) define out-of-vocabulary (OOV) scores relative to the most common
words in grade 2, the lowest grade level in their corpus; it was not possible to estimate OOV scores,
because we did not have access to grade 2 texts.
</footnote>
<page confidence="0.993752">
27
</page>
<note confidence="0.449309">
Computational Linguistics Volume 34, Number 1
</note>
<tableCaption confidence="0.994298">
Table 10
</tableCaption>
<note confidence="0.871593333333333">
The contribution of coherence-based features to the automatic readability assessment task.
Diacritics ** (p &lt; .01) and * (p &lt; .05) indicate whether differences in accuracy between
Schwarm and Ostendorf and all other models are significant (using a Fisher Sign test).
</note>
<table confidence="0.997139">
Model Accuracy
Schwarm &amp; Ostendorf 78.56
Schwarm &amp; Ostendorf, Coreference+Syntax+Salience+ 88.79*
Schwarm &amp; Ostendorf, Coreference−Syntax+Salience+ 79.49
Schwarm &amp; Ostendorf, Latent Semantic Analysis 78.56
Coreference+Syntax+Salience+ 50.90**
Coreference−Syntax+Salience+ 49.55**
Latent Semantic Analysis 48.58**
</table>
<bodyText confidence="0.999672111111111">
The different systems were trained and tested on the Britannica corpus using five-
fold cross-validation.13 The language models were created anew for every fold using the
documents in the training data. We use Joachims’ (1998a) SVMlight package for training
and testing with all parameters set to their default values.
Evaluation Metric. We measure classification accuracy (i.e., the number of classes as-
signed correctly by the SVM over the size of the test set). We report accuracy averaged
over folds. A chance baseline (selecting one class at random) yields an accuracy of 50%.
Our training and test sets have the same number of documents for the two readability
categories.
</bodyText>
<subsectionHeader confidence="0.876234">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999839133333334">
Table 10 summarizes our results on the readability assessment task. We first com-
pared Schwarm and Ostendorf’s (2005) system against a system that incorporates
entity-based coherence features (see rows 3–4 in Table 10). As can be seen, the sys-
tem’s accuracy significantly increases by 10% when the full feature set is included
(Coreference+Syntax+Salience+). Entity-grid features that do not incorporate corefer-
ence information (Coreference−Syntax+Salience+) perform numerically better (com-
pare row 1 and 3 in Table 10); however, the difference is not statistically significant.
The superior performance of the Coreference+Syntax+Salience+ feature set is not
entirely unexpected. Inspection of our corpus revealed that easy and difficult texts differ
in their distribution of pronouns and coreference chains in general. Easy texts tend to
employ less coreference and the use of personal pronouns is relatively sparse. To give
a concrete example, the pronoun they is attested 173 times in the difficult corpus and
only 73 in the easy corpus. This observation suggests that coreference information is a
good indicator of the level of reading difficulty and explains why its omission from the
entity-based feature space yields inferior performance.
</bodyText>
<footnote confidence="0.5228795">
13 The data for the experiments reported here can be found at http://homepages.inf.ed.ac.uk/
mlap/coherence/.
</footnote>
<page confidence="0.991807">
28
</page>
<bodyText confidence="0.9850776875">
Barzilay and Lapata Modeling Local Coherence
Furthermore, note that discourse-level information is absent from Schwarm and
Ostendorf’s (2005) original model. The latter employs a large number of lexical and
syntactic features which capture sentential differences among documents. Our entity-
based representation supplements their feature space with information spanning two or
more successive sentences. We thus are able to model stylistic differences in readability
that go beyond syntax and lexical choice. Besides coreference, our feature representa-
tion captures important information about the presence and distribution of entities in
discourse. For example, difficult texts tend to have twice as many entities as easy ones.
Consequently, easy and difficult texts are represented by entity transition sequences
with different probabilities (e.g., the sequences [S S] and [S O] are more probable in
difficult texts). Interestingly, when coherence is quantified using LSA, we observe no
improvement to the classification task. The LSA scores capture lexical or semantic text
properties similar to those expressed by the Flesch Kincaid index and the perplexity
scores (e.g., word repetition). It is therefore not surprising that their inclusion in the
feature set does not increase performance.
We also evaluated the training requirements for the readability system described
herein. Figure 3 shows the learning curve for Schwarm and Ostendorf’s (2005) model
enhanced with the Coreference+Syntax+Salience+ feature space and on its own. As
can be seen, both models perform relatively well when trained on small data sets
(e.g., 20–40 documents) and reach peak accuracy with half of the training data. The
inclusion of discourse-based features consistently increases accuracy irrespective of the
amount of training data available. Figure 3 thus suggests that better feature engineering
is likely to bring further performance improvements on the readability task.
Our results indicate that the entity-based text representation introduced here cap-
tures aspects of text readability and can be successfully incorporated into a practical
system. Coherence is by no means the sole predictor of readability. In fact, on its own,
it performs poorly on this task as demonstrated when using either LSA or the entity-
based feature space without Schwarm and Ostendorf’s (2005) features (see rows 5–7 in
Table 10). Rather, we claim that coherence is one among many factors contributing to
text readability and that our entity-grid representation is well-suited for text classifica-
tion tasks such as reading level assessment.
</bodyText>
<footnote confidence="0.574927">
Figure 3
Learning curve for Schwarm and Ostendorf’s (2005) model on its own and enhanced with the
Coreference+Syntax+Salience+ feature space.
</footnote>
<page confidence="0.994572">
29
</page>
<note confidence="0.53891">
Computational Linguistics Volume 34, Number 1
7. Discussion and Conclusions
</note>
<bodyText confidence="0.999758854166667">
In this article we proposed a novel framework for representing and measuring text co-
herence. Central to this framework is the entity-grid representation of discourse, which
we argue captures important patterns of sentence transitions. We re-conceptualize co-
herence assessment as a learning task and show that our entity-based representation
is well-suited for ranking-based generation and text classification tasks. Using the
proposed representation, we achieve good performance on text ordering, summary
coherence evaluation, and readability assessment.
The entity grid is a flexible, yet computationally tractable, representation. We
investigated three important parameters for grid construction: the computation of
coreferring entity classes, the inclusion of syntactic knowledge, and the influence of
salience. All these knowledge sources figure prominently in theories of discourse
(see Section 2) and are considered important in determining coherence. Our results
empirically validate the importance of salience and syntactic information (expressed
by S, O, X, and –) for coherence-based models. The combination of both knowledge
sources (Syntax+Salience) yields models with consistently good performance for all
our tasks.
The benefits of full coreference resolution are less uniform. This is partly due to
mismatches between training and testing conditions. The system we employ (Ng and
Cardie 2002) was trained on human-authored newspaper texts. The corpora we used
in our sentence ordering and readability assessment experiments are somewhat similar
(i.e., human-authored narratives), whereas our summary coherence rating experiment
employed machine generated texts. It is therefore not surprising that coreference reso-
lution delivers performance gains on the first two tasks but not on the latter (see Table 5
in Section 4 and Table 10 in Section 6.3). Our results further show that in lieu of an
automatic coreference resolution system, entity classes can be approximated simply
by string matching. The latter is a good indicator of nominal coreference; it is often
included as a feature in machine learning approaches to coreference resolution (Soon,
Ng, and Lim 2001; Ng and Cardie 2002) and is relatively robust (i.e., likely to deliver
consistent results in the face of different domains and genres).
It is important to note that, although inspired by entity-based theories of discourse
coherence, our approach is not a direct implementation of any theory in particular.
Rather, we sacrifice linguistic faithfulness for automatic computation and breadth of
coverage. Despite approximations and unavoidable errors (e.g., in the parser’s output),
our results indicate that entity grids are a useful representational framework across
tasks and text genres. In agreement with Poesio et al. (2004) we find that pronomi-
nalization is a good indicator of document coherence. We also find that coherent texts
are characterized by transitions with particular properties which do not hold for all
discourses. Contrary to Centering Theory, we remain agnostic to the type of transi-
tions that our models capture (e.g., CONTINUE, SHIFT). We simply record whether an
entity is mentioned in the discourse and in what grammatical role. Our experiments
quantitatively measured the predictive power of various linguistic features for several
coherence-related tasks. Crucially, we find that our models are sensitive to the domain at
hand and the type of texts under consideration (human-authored vs. machine generated
texts). This is an unavoidable consequence of the grid representation, which is entity-
specific. Differences in entity distribution indicate not only differences in coherence, but
also in writing conventions and style. Similar observations have been made in other
work which is closer in spirit to Centering’s claims (Hasler 2004; Karamanis et al. 2004;
Poesio et al. 2004).
</bodyText>
<page confidence="0.968815">
30
</page>
<bodyText confidence="0.980386272727273">
Barzilay and Lapata Modeling Local Coherence
An important future direction lies in augmenting our entity-based representation
with more fine-grained lexico-semantic knowledge. One way to achieve this goal is to
cluster entities based on their semantic relatedness, thereby creating a grid represen-
tation over lexical chains (Morris and Hirst 1991). An entirely different approach is to
develop fully lexicalized models, akin to traditional language models. Cache language
models (Kuhn and De Mori 1990) seem particularly promising in this context. The
granularity of syntactic information is another topic that warrants further investigation.
So far we have only considered the contribution of “core” grammatical relations to
the grid construction. Expanding our grammatical categories to modifiers and adjuncts
may provide additional information, in particular when considering machine generated
texts. We also plan to investigate whether the proposed discourse representation and
modeling approaches generalize across different languages. For instance the identifi-
cation and extraction of entities poses additional challenges in grid construction for
Chinese where word boundaries are not denoted orthographically (by space). Similar
challenges arise in German, a language with a large number of inflected forms and
productive derivational processes (e.g., compounding) not indicated by orthography.
In the discourse literature, entity-based theories are primarily applied at the level
of local coherence, while relational models, such as Rhetorical Structure Theory (Mann
and Thomson 1988; Marcu 2000), are used to model the global structure of discourse.
We plan to investigate how to combine the two for improved prediction on both local
and global levels, with the ultimate goal of handling longer texts.
</bodyText>
<sectionHeader confidence="0.997813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.671157875">
The authors acknowledge the support of
the National Science Foundation (Barzilay;
CAREER grant IIS-0448168 and grant
IIS-0415865) and EPSRC (Lapata; grant
GR/T04540/01). We are grateful to Claire
Cardie and Vincent Ng for providing us
the results of their coreference system
on our data. Thanks to Eli Barzilay, Eugene
</bodyText>
<reference confidence="0.7929268">
Charniak, Michael Elhadad, Noemie
Elhadad, Nikiforos Karamanis, Frank Keller,
Alex Lascarides, Igor Malioutov, Smaranda
Muresan, Martin Rinard, Kevin Simler,
Caroline Sporleder, Chao Wang, Bonnie
</reference>
<bodyText confidence="0.996181142857143">
Webber, and three anonymous reviewers
for helpful comments and suggestions.
Any opinions, findings, and conclusions or
recommendations expressed herein are those
of the authors and do not necessarily reflect
the views of the National Science Foundation
or EPSRC.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.97428843902439">
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of the
42nd Annual Meeting of the Association for
Computational Linguistics, pages 399–406,
Barcelona, Spain.
Ariel, Mira. 1988. Referring and accessibility.
Journal of Linguistics, 24:65–87.
Asher, Nicholas and Alex Lascarides. 2003.
Logics of Conversation. Cambridge
University Press, Cambridge, England.
Barzilay, Regina. 2003. Information Fusion
for Multi-Document Summarization:
Praphrasing and Generation. Ph.D. thesis,
Columbia University, New York.
Barzilay, Regina and Noemie Elhadad. 2003.
Sentence alignment for monolingual
comparable corpora. In Proceedings of the
8th Conference on Empirical Methods in
Natural Language Processing, pages 25–32,
Sapporo, Japan.
Barzilay, Regina and Mirella Lapata. 2005.
Modeling local coherence: An entity-based
approach. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 141–148, Ann Arbor, MI.
Barzilay, Regina and Lillian Lee. 2004.
Catching the drift: Probabilistic content
models, with applications to generationm
and sumarization. In Proceedings of the
2nd Human Language Technology Conference
and Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 113–120, Boston, MA.
Berry, Michael W., Susan T. Dumais, and
Gavin W. O’Brien. 1994. Using linear
algebra for intelligent information
retrieval. SIAM Review, 37(4):573–595.
Brennan, Susan E., Marilyn W. Friedman,
and Charles J. Pollard. 1987. A centering
approach to pronouns. In Proceedings of the
</reference>
<page confidence="0.999277">
31
</page>
<note confidence="0.520633">
Computational Linguistics Volume 34, Number 1
</note>
<reference confidence="0.997940762711865">
25th Annual Meeting of the Association for
Computational Linguistics, pages 155–162,
Palo Alto, CA.
Briscoe, Ted and John Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 1499–1504, Las Palmas,
Canary Islands.
Chafe, Wallace L. 1976. Givenness,
contrastiveness, definiteness, subjects,
topics, and point of view. In Charles N. Li,
editor, Subject and Topic. Academic Press,
New York, pages 25–55.
Chall, Jeanne S. 1958. Readability: An
Appraisal of Research and Application.
Number 34 in Bureau of Educational
Research Monographs. Ohio State
University Press, Columbus.
Chall, Jeanne S. and Edgar Dale. 1995.
Readability Revisited: The New Dale-Chall
Readability Formula. Brookline Books,
Cambridge, MA.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 132–139,
Seattle, WA.
Clark, Herbert H. and Susan E. Haviland.
1977. Comprehension and the given-new
contract. In Roy O. Freedle, editor,
Discourse Production and Comprehension.
Ablex, Norwood, NJ, pages 1–39.
Clarkson, Philip and Ronald Rosenfeld.
1997. Statistical language modeling.
In Proceedings of ESCA EuroSpeech’97,
pages 2707–2710, Rhodes, Greece.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the Association for Computational
Linguistics and 8th Conference of the
European Chapter of the Association for
Computational Linguistics, pages 16–23,
Madrid, Spain.
Collins, Michael. 2002. Discriminative
reranking for natural language parsing.
In Proceedings of the 17th International
Conference on Machine Learning,
pages 175–182, Palo Alto, CA.
Foltz, Peter W., Walter Kintsch, and
Thomas K. Landauer. 1998. Textual
coherence using latent semantic analysis.
Discourse Processes, 25(2&amp;3):285–307.
Freund, Yovav, Raj Iyer, Robert E. Schapire,
and Yoram Singer. 2003. An efficient
boosting algorithm for combining
preferencs. Machine Learning, 4:933–969.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2004. Fast and optimal decoding for
machine translation. Artificial Intelligence,
154(1–2):127–143.
Givon, Talmy. 1987. Beyond foreground and
background. In Russell S. Tomlin, editor,
Coherence and Grounding in Discourse.
Benjamins, Amsterdam/Philadelphia,
pages 175–188.
Grosz, Barbara, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203–225.
Gundel, Jaenette K., Nancy Hedberg, and
Ron Zacharski.1993. Cognitive status
and the form of referring expressions
in discourse. Language, 69(2):274–307.
Gunning, Robert. 1952. The Technique of Clear
Writing. McGraw Hill, New York.
Halliday, M. A. K. and Ruqaiya Hasan. 1976.
Cohesion in English. Longman, London.
Hasler, Laura. 2004. An investigation into
the use of centering transitions for
summarisation. In Proceedings of the
7th Annual CLUK Research Colloquium,
pages 100–107, Birmingham, UK.
Hoey, Michael. 1991. Patterns of Lexis in Text.
Oxford University Press, Oxford, England.
Hudson, S. B., M. K. Tanenhaus, and G. S.
Dell. 1986. The effect of the discourse
center on the local coherence of a
discourse. In Proceedings of the 8th Annual
Meeting of the Cognitive Science Society,
pages 96–101, Amherst, MA.
Joachims, Thorsten.1998a. Making
large-scale support vector machine
learning practical. In Bernard Sch¨olkopf,
Christopher Burges, and Alexander Smola,
editors, Advances in Kernel Methods:
Support Vector Machines. MIT Press,
Cambridge, MA.
Joachims, Thorsten.1998b. Text
categorization with support vector
machines: Learning with many relevant
features. In Proceedings of the European
Conference on Machine Learning,
pages 137–142, Berlin, Springer.
Joachims, Thorsten. 2002. Optimizing search
engines using clickthrough data. In
Proceedings of ACM Conference on Knowledge
Discovery and Data Mining, pages 133–142,
Chicago, IL.
Kameyama, Megumi.1986. A
property-sharing constraint in centering.
In Proceedings of the 24th Annual Meeting of
the Association for Computational Linguistics,
pages 200–206, New York.
</reference>
<page confidence="0.978491">
32
</page>
<note confidence="0.349358">
Barzilay and Lapata Modeling Local Coherence
</note>
<reference confidence="0.99976686440678">
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, University of Edinburgh,
Edinburgh, Scotland.
Karamanis, Nikiforos, Massimo Poesio,
Chris Mellish, and Jon Oberlander.
2004. Evaluating centering-based
metrics of coherence for text structuring
using a reliably annotated corpus. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 391–398, Barcelona, Spain.
Karttunen, Lauri. 1976. Discourse referents.
In James D. McCawley, editor, Syntax
and Semantics: Notes from the Linguistic
Underground, volume 7. Academic Press,
New York, pages 363–386.
Katz, Irvin R. and Malcolm I. Bauer. 2001.
Sourcefinder: Course preparation via
linguistically targeted web search. Journal
of Educational Technology and Society,
4(3):45–49.
Kibble, Rodger and Richard Power. 2004.
Optimising referential coherence in text
generation. Computational Linguistics,
30(4):401–416.
Kincaid, J. Peter, James Aagard, John O’Hara,
and Larry Cottrell. 1981. Computer
readability editing system. IEEE
Transactions on Professional Communication,
1(24):34–81.
Kincaid, Peter J., Robert P. Fishburne,
Richard L. Rodgers, and Brad S. Chissom.
1975. Derivation of new readability
formulas for Navy enlisted personnel.
Research Branch Report 8-75, U.S.
Naval Air Station, Memphis, TN.
Knight, Kevin and Vasileios Hatzivassiloglou.
1995. Two-level, many-path generation. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
pages 252–260, Cambridge, MA.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Thorsten Joachims, editor, Proceedings
of the 2nd Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, pages 192–199,
Pittsburgh, PA.
Kuhn, R. and R. De Mori.1990. A
cache-based natural language model for
speech recognition. IEEE Transactions on
PAMI, 12(6):570–583.
Kuno, Susumu. 1972. Functional sentence
perspective. Linguistic Inquiry, 3:269–320.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato’s problem:
The latent semantic analysis theory of
acquisition, induction and representation
of knowledge. Psychological Review,
104(2):211–240.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings
of the 17th International Conference on
Computational Linguistics and 36th
Annual Meeting of the Association for
Computational Linguistics, pages 704–710,
Montr´eal, Canada.
Lapata, Mirella. 2003. Probabilistic text
structuring: Experiments with sentence
ordering. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics, pages 545–552, Sapporo, Japan.
Lin, Chin-Yew and Eduard H. Hovy. 2003.
Automatic evaluation of summaries using
n-gram co-occurrence statistics. In
Proceedings of the 2nd Human Language
Technology Conference and Annual Meeting of
the North American Chapter of the Association
for Computational Linguistics, pages 71–78,
Boston, MA.
Lin, Dekang. 2001. LaTaT: Language and
text analysis tools. In Proceedings of the 1st
International Conference on Human Language
Technology Research, pages 222–227,
San Francisco, CA.
Mann, William C. and Sandra A. Thomson.
1988. Rhetorical structure theory. Text,
8(3):243–281.
Marcu, Daniel. 2000. The Theory and Practice
of Discourse Parsing and Summarization.
MIT Press, Cambridge, MA.
McKoon, Gail and Roger Ratcliff. 1992.
Inference during reading. Psychological
Review, 99(3):440–446.
Mellish, Chris, Mick O’Donnell, Jon
Oberlander, and Alistair Knott. 1998.
Experiments using stochastic search
for text planning. In Proceedings of the
9th International Workshop on Natural
Language Generation, pages 98–107,
New Brunswick, NJ.
Miltsakaki, Eleni and Karen Kukich. 2000.
The role of centering theory’s rough-shift
in the teaching and evaluation of writing
skills. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 408–415, Hong Kong.
Mitchell, James V. 1985. The Ninth Mental
Measurements Yearbook. University of
Nebraska Press, Lincoln.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 1(17):21–43.
Ng, Vincent and Claire Cardie. 2002.
Improving machine learning approaches
</reference>
<page confidence="0.96503">
33
</page>
<reference confidence="0.994098628571429">
Computational Linguistics Volume 34, Number 1
to coreference resolution. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104–111,
Philadelphia, PA.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of
machine translation. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 311–318,
Philadelphia, PA.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: A parametric theory and
its instantiations. Computational Linguistics,
30(3):309–363.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James H. Martin,
and Dan Jurafsky. 2005. Support vector
learning for semantic argument
classification. Machine Learning,
60(1):11–39.
Prince, Ellen. 1978. A comparison of wh-clefts
and it-clefts in discourse. Language,
54:883–906.
Prince, Ellen. 1981. Toward a taxonomy of
given-new information. In Peter Cole,
editor, Radical Pragmatics. Academic Press,
New York, pages 223–255.
Reiter, Ehud and Robert Dale. 2000.
Building Natural-Language Generation
Systems. Cambridge University Press,
Cambridge, England.
Schwarm, Sarah E. and Mari Ostendorf. 2005.
Reading level assessment using support
vector machines and statistical language
models. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 523–530, Ann Arbor, MI.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press,
New York, pages 47–73.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English Discourse. Ph.D.
thesis, MIT.
Soon, W. M., Hwee Tou Ng, and D. C. Y. Lim.
2001. A machine learning approach to
coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
Stenner, A. Jackson. 1996. Measuring
reading comprehension with the
lexile framework. Presented at the
California Comparability Symposium,
Burlingame, CA.
Strube, Michael and Udo Hahn. 1999.
Functional centering—Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309–344.
Toutanova, Kristina, Penka Markova, and
Christopher D. Manning. 2004. The leaf
projection path view of parse trees:
Exploring string kernels for HPSG
parse selection. In Proceedings of the
Conference on Empirical Methods
in Natural Language Processing,
pages 166–173, Barcelona, Spain.
Vapnik, Vladimir. 1998. Statistical Learning
Theory. Wiley, Chichester, UK.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of the 6th
Message Understanding Conference (MUC-6),
pages 45–52, San Francisco, CA.
Walker, Marilyn, Masayo Iida, and Sharon
Cote. 1994. Japanese discourse and the
process of centering. Computational
Linguistics, 20(2):193–232.
Walker, Marilyn, Aravind Joshi, and
Ellen Prince, editors. 1998. Centering
Theory in Discourse. Clarendon Press,
Oxford, UK.
Walker, Marilyn A., Owen Rambow, and
Monica Rogati. 2001. Spot: A trainable
sentence planner. In Proceedings of the
2nd Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 17–24, Pittsburgh, PA.
Weiss, Sholom M. and Casimir A.
Kulikowski. 1991. Computer Systems that
Learn: Classification and Prediction Methods
from, Statistics, Neural Nets, Machine
Learning, and Expert Systems. Morgan
Kaufmann, San Mateo, CA.
Witten, Ian H. and Eibe Frank. 2000. Data
Mining: Practical Machine Learning Tools
and Techniques with Java Implementations.
Morgan Kaufman, San Mateo, CA.
</reference>
<page confidence="0.999317">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.588547">
<title confidence="0.9718555">Modeling Local Coherence: An Entity-Based Approach</title>
<affiliation confidence="0.995096">Massachusetts Institute of Technology University of Edinburgh</affiliation>
<abstract confidence="0.950176625">This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Michael Charniak</author>
</authors>
<title>Elhadad, Noemie Elhadad, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Igor Malioutov,</title>
<location>Smaranda Muresan, Martin Rinard, Kevin Simler, Caroline Sporleder, Chao Wang, Bonnie</location>
<marker>Charniak, </marker>
<rawString>Charniak, Michael Elhadad, Noemie Elhadad, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Igor Malioutov, Smaranda Muresan, Martin Rinard, Kevin Simler, Caroline Sporleder, Chao Wang, Bonnie</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ernst Althaus</author>
<author>Nikiforos Karamanis</author>
<author>Alexander Koller</author>
</authors>
<title>Computing locally coherent discourses.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>399--406</pages>
<location>Barcelona,</location>
<marker>Althaus, Karamanis, Koller, 2004</marker>
<rawString>Althaus, Ernst, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 399–406, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mira Ariel</author>
</authors>
<title>Referring and accessibility.</title>
<date>1988</date>
<journal>Journal of Linguistics,</journal>
<pages>24--65</pages>
<contexts>
<context position="31107" citStr="Ariel 1988" startWordPosition="4703" endWordPosition="4704"> refer the interested reader to Barzilay (2003). Salience. Centering and other discourse theories conjecture that the way an entity is introduced and mentioned depends on its global role in a given discourse. We evaluate the impact of salience information by considering two types of models: The first model treats all entities uniformly, whereas the second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). To implement a salience-based model, we modify our feature generation procedure by computing transition probabilities for each salience group separately, and then 2 When evaluating the output of coreference algorithms, performance is typically measured using a model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the recall error by taking each equivalence class S in the gold standard and determining the number of coreference links m that would have to be added to the system’s output to place all entities in S into </context>
</contexts>
<marker>Ariel, 1988</marker>
<rawString>Ariel, Mira. 1988. Referring and accessibility. Journal of Linguistics, 24:65–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="1797" citStr="Asher and Lascarides 2003" startWordPosition="244" endWordPosition="247">n many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article focuses on local coherence, which captures text relatedness at the level of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global coherence and has received considerable attention in computational linguistics (Foltz, Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller * Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu. ** School of Informatics,</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Asher, Nicholas and Alex Lascarides. 2003. Logics of Conversation. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
</authors>
<title>Information Fusion for Multi-Document Summarization: Praphrasing and Generation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University,</institution>
<location>New York.</location>
<contexts>
<context position="30544" citStr="Barzilay (2003)" startWordPosition="4619" endWordPosition="4620">t structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified. The phrase-structure output of Collins’s parser is transformed into a dependency tree from which grammatical relations are extracted. Passive verbs are recognized using a small set of patterns, and the underlying deep grammatical role for arguments involved in the passive construction is entered in the grid (see the grid cell o for Microsoft, Sentence 2, Table 2). For more details on the grammatical relations extraction component we refer the interested reader to Barzilay (2003). Salience. Centering and other discourse theories conjecture that the way an entity is introduced and mentioned depends on its global role in a given discourse. We evaluate the impact of salience information by considering two types of models: The first model treats all entities uniformly, whereas the second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). </context>
</contexts>
<marker>Barzilay, 2003</marker>
<rawString>Barzilay, Regina. 2003. Information Fusion for Multi-Document Summarization: Praphrasing and Generation. Ph.D. thesis, Columbia University, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="87299" citStr="Barzilay and Elhadad (2003)" startWordPosition="13169" endWordPosition="13172">r w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi[(w ·�xi) + b] &gt; 0 Finding the optimal hyperplane is an optimization problem which can be solved efficiently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classification (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simplified articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difficult.11 Examples of these two categories are given in Table 9. 11 The Britannica corpus was also used by Schwarm and Ostendorf (2005); in addition they make use of a corpus</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Barzilay, Regina and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 8th Conference on Empirical Methods in Natural Language Processing, pages 25–32, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>141--148</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="41905" citStr="Barzilay and Lapata 2005" startWordPosition="6371" endWordPosition="6374">Parameter Settings. In order to investigate the contribution of linguistic knowledge on model performance we experimented with a variety of grid representations resulting in different parameterizations of the feature space from which our model is learned. We focused on three sources of linguistic knowledge—syntax, coreference resolution, and salience—which play a prominent role in entity-based analyses of dis4 The collections are available from http://people.csail.mit.edu/regina/coherence/. 5 Short texts may have less than 20 permutations. The corpus described in the original ACL publication (Barzilay and Lapata 2005) contained a number of duplicate permutations. These were removed from the current version of the corpus. 12 Barzilay and Lapata Modeling Local Coherence course coherence (see Section 3.3 for details). An additional motivation for our study was to explore the trade-off between robustness and richness of linguistic annotations. NLP tools are typically trained on human-authored texts, and may deteriorate in performance when applied to automatically generated texts with coherence violations. We thus compared a linguistically rich model against models that use more impoverished representations. Mo</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Barzilay, Regina and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 141–148, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generationm and sumarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2nd Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<location>Boston, MA.</location>
<contexts>
<context position="35544" citStr="Barzilay and Lee 2004" startWordPosition="5391" endWordPosition="5394">thing prevents the use of our feature vector representation for conventional classification tasks. We offer an illustration in Experiment 3, where features extracted from entity grids are used to enhance the performance of a readability assessment system. Here, the learner takes as input a set of documents labeled with discrete classes (e.g., denoting whether a text is difficult or easy to read) and learns to make predictions for unseen instances (see Section 6 for details on the machine learning paradigm we employ). 4. Experiment 1: Sentence Ordering Text structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al. 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information10 Barzilay and Lapata Modeling Local Coherence bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. The information bearing items can be database entries (Karamanis et al. 2004), propositions (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). In sentence ordering, a document is viewed as a bag of sentenc</context>
<context position="39888" citStr="Barzilay and Lee 2004" startWordPosition="6073" endWordPosition="6076">rned, unseen renderings (xij,xik) of document di can be ranked simply by computing the values w*Φ(xij) and w*Φ(xik) and sorting them accordingly. Here, w* is the optimized parameter vector resulting from training. 4.2 Method Data. To acquire a large collection for training and testing, we create synthetic data, wherein the candidate set consists of a source document and permutations of its sentences. This framework for data acquisition enables large-scale automatic evaluation and is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003; Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004). The underlying assumption is that the original sentence order in the source document must be coherent, and so we should prefer models that rank it higher than other permutations. Because we do not know the relative quality of different permutations, our corpus includes only pairwise rankings that comprise the original document and one of its permutations. Given k original documents, each with n randomly generated permutations, we obtain k · n (trivially) annotated pairwise rankings for training and testing. Using the technique described herein, we collected data4 in two different genres: new</context>
<context position="45172" citStr="Barzilay and Lee (2004)" startWordPosition="6840" endWordPosition="6843"> salient entities and the length of the transition sequence. These parameters were tuned separately for each data set on the corresponding held-out development set. Optimal saliencebased models were obtained for entities with frequency &gt;2. The optimal transition length was &lt;3.6 In our ordering experiments, we used Joachims’s (2002) SVMlight package for training and testing with all parameters set to their default values. Comparison with State-of-the-Art Methods. We compared the performance of our algorithm against two state-of-the-art models proposed by Foltz, Kintsch, and Landauer (1998) and Barzilay and Lee (2004). These models rely largely on lexical information for assessing document coherence, contrary to our models which are in essence unlexicalized. Recall from Section 3 that our approach captures local coherence by modeling patterns of entity distribution in discourse, without taking note of their lexical instantiations. In the following we briefly describe the lexicalized models we employed in our comparative study and motivate their selection. 6 The models we used in our experiments are available from http://people.csail.mit.edu/ regina/coherence/ and http://homepages.inf.ed.ac.uk/mlap/coherenc</context>
<context position="49230" citStr="Barzilay and Lee (2004)" startWordPosition="7462" endWordPosition="7465">ed by two factors: (a) the corpus is large enough to yield a reliable semantic space, and (b) it consists of news stories and is therefore similar in style, vocabulary, and content to most of the corpora employed in our coherence experiments. = ~n j=1 (1) 14 Barzilay and Lapata Modeling Local Coherence coherence score assigned to the original document against each of its permutations. Ties are resolved randomly. Both LSA and our entity-grid model are local—they model sentence-to-sentence transitions without being aware of global document structure. In contrast, the content models developed by Barzilay and Lee (2004) learn to represent more global text properties by capturing topics and the order in which these topics appear in texts from the same domain. For instance, a typical earthquake newspaper report contains information about the quake’s epicenter, how much it measured, the time it was felt, and whether there were any victims or damage. By encoding constraints on the ordering of these topics, content models have a pronounced advantage in modeling document structure because they can learn to represent how documents begin and end, but also how the discourse shifts from one topic to the next. Like LSA</context>
<context position="50518" citStr="Barzilay and Lee (2004)" startWordPosition="7660" endWordPosition="7663">re domain-specific, and would expectedly yield inferior performance on out-of-domain texts. Barzilay and Lee (2004) implemented content models using an HMM wherein states correspond to distinct topics (for instance, the epicenter of an earthquake or the number of victims), and state transitions represent the probability of changing from one topic to another, thereby capturing possible topic-presentation orderings within a domain. Topics refer to text spans of varying granularity and length. Barzilay and Lee used sentences in their experiments, but clauses or paragraphs would also be possible. Barzilay and Lee (2004) employed their content models to find a high-probability ordering for a document whose sentences had been randomly shuffled. Here, we use content models for the simpler coherence ranking task. Given two text permutations, we estimate their likelihood according to their HMM model and select the text with the highest probability. Because the two candidates contain the same set of sentences, the assumption is that a more probable text corresponds to an ordering that is more typical for the domain of interest. In our experiments, we built two content models, one for the Accidents corpus and one f</context>
<context position="59232" citStr="Barzilay and Lee 2004" startWordPosition="8964" endWordPosition="8967"> on the Earthquakes corpus. We hypothesize that the small contribution of salience is related to the way it is currently represented. Addition of this knowledge source to our grid representation, doubles the number of features that serve as input to the learning algorithm. In other words, salienceaware models need to learn twice as many parameters as salience-free models, while having access to the same amount of training data. Achieving any improvement in these conditions is challenging. Comparison with State-of-the-Art Methods. We next discuss the performance of the HMMbased content models (Barzilay and Lee 2004) and LSA (Foltz, Kintsch, and Landauer 1998) in comparison to our model (Coreference+Syntax+Salience+). 17 Computational Linguistics Volume 34, Number 1 First, note that the entity-grid model significantly outperforms LSA on both domains (p &lt; .01 using a Sign test, see Table 5). In contrast to our model, LSA is neither entity-based nor unlexicalized: It measures the degree of semantic overlap across successive sentences, without handling discourse entities in a special way (all content words in a sentence contribute towards its meaning). We attribute our model’s superior performance, despite t</context>
<context position="61131" citStr="Barzilay and Lee (2004)" startWordPosition="9252" endWordPosition="9255">tyles. This is necessary for constructing robust vector representations that are not extremely sparse. We thus expect the grid models to be more sensitive to the discourse conventions of the training/test data. The accuracy of the HMM-based content modes is comparable to the grid model on the Earthquakes corpus (the difference is not statistically significant) but is significantly lower on the Accidents texts (see Table 5). Although the grid model yields similar performance on the two domains, content models exhibit high variability. These results are not surprising. The analysis presented in Barzilay and Lee (2004) shows that the Earthquakes texts are quite formulaic in their structure, following the editorial style of the Associated Press. In contrast, the Accidents texts are more challenging for content models—reports in this set do not undergo centralized editing and therefore exhibit more variability in lexical choice and style. The LSA model also significantly outperforms the content model on the Earthquakes domain (p &lt; .01 using a Sign test). Being a local model, LSA is less sensitive to the way documents are structured and is therefore more likely to deliver consistent performance across domains.</context>
<context position="90996" citStr="Barzilay and Lee 2004" startWordPosition="13725" endWordPosition="13728">as represented as a feature vector using the entity transition notation introduced in Section 3. We experimented with two models that yielded good performances in our previous experiments: Coreference+Syntax+Salience+ (see Experiment 1) and Coreference−Syntax+Salience+ (see Experiment 2). The transition length was ≤2 and entities were considered salient if they occurred ≥2 times. As in our previous experiments, we compared the entity-based representation against LSA. The latter is a measure of the semantic relatedness across pairs of sentences. We could not apply the HMM-based content models (Barzilay and Lee 2004) to the readability data set. The encyclopedia lemmas are written by different authors and consequently vary considerably in structure and vocabulary choice. Recall that these models are suitable for more restricted domains and texts that are more formulaic in nature. 12 Schwarm and Ostendorf (2005) define out-of-vocabulary (OOV) scores relative to the most common words in grade 2, the lowest grade level in their corpus; it was not possible to estimate OOV scores, because we did not have access to grade 2 texts. 27 Computational Linguistics Volume 34, Number 1 Table 10 The contribution of cohe</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Barzilay, Regina and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generationm and sumarization. In Proceedings of the 2nd Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 113–120, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Susan T Dumais</author>
<author>Gavin W O’Brien</author>
</authors>
<title>Using linear algebra for intelligent information retrieval.</title>
<date>1994</date>
<journal>SIAM Review,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Berry, Dumais, O’Brien, 1994</marker>
<rawString>Berry, Michael W., Susan T. Dumais, and Gavin W. O’Brien. 1994. Using linear algebra for intelligent information retrieval. SIAM Review, 37(4):573–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Marilyn W Friedman</author>
<author>Charles J Pollard</author>
</authors>
<title>A centering approach to pronouns.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>155--162</pages>
<location>Palo Alto, CA.</location>
<marker>Brennan, Friedman, Pollard, 1987</marker>
<rawString>Brennan, Susan E., Marilyn W. Friedman, and Charles J. Pollard. 1987. A centering approach to pronouns. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 155–162, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1499--1504</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="19637" citStr="Briscoe and Carroll 2002" startWordPosition="2855" endWordPosition="2858">tic role. Such information can be expressed in many ways (e.g., using constituent labels or thematic role information). Because grammatical relations figure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reflecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [O – – – – X]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. E</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Briscoe, Ted and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 1499–1504, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wallace L Chafe</author>
</authors>
<title>Givenness, contrastiveness, definiteness, subjects, topics, and point of view.</title>
<date>1976</date>
<pages>25--55</pages>
<editor>In Charles N. Li, editor, Subject and Topic.</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="6578" citStr="Chafe 1976" startWordPosition="920" endWordPosition="921">in the context of a ranking-based text generation system. We first summarize entity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models. Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners. 2 Barzilay and Lapata Modeling Local Coherence 2.1 Entity-Based Approaches to Local Coherence Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (</context>
<context position="32413" citStr="Chafe 1976" startWordPosition="4919" endWordPosition="4920"> the number of links in the gold standard. Precision error is computed by reversing the roles of the gold standard and system output. 3 The frequency threshold is empirically determined on the development set. See Section 4.2 for further discussion. 9 Computational Linguistics Volume 34, Number 1 combining them into a single feature vector. For n transitions with k salience classes, the feature space will be of size n × k. While we can easily build a model with multiple salience classes, we opt for a binary distinction (i.e., k = 2). This is more in line with theoretical accounts of salience (Chafe 1976; Grosz, Joshi, and Weinstein 1995) and results in a moderate feature space for which reliable parameter estimation is possible. Considering a large number of salience classes would unavoidably increase the number of features. Parameter estimation in such a space requires a large sample of training examples that is unavailable for most domains and applications. Different classes of models can be defined along the linguistic dimensions just discussed. Our experiments will consider several models with varying degrees of linguistic complexity, while attempting to strike a balance between expressi</context>
</contexts>
<marker>Chafe, 1976</marker>
<rawString>Chafe, Wallace L. 1976. Givenness, contrastiveness, definiteness, subjects, topics, and point of view. In Charles N. Li, editor, Subject and Topic. Academic Press, New York, pages 25–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeanne S Chall</author>
</authors>
<title>Readability: An Appraisal of Research and Application.</title>
<date>1958</date>
<journal>Number</journal>
<volume>34</volume>
<publisher>University Press,</publisher>
<institution>Bureau of Educational Research Monographs. Ohio State</institution>
<location>Columbus.</location>
<marker>Chall, 1958</marker>
<rawString>Chall, Jeanne S. 1958. Readability: An Appraisal of Research and Application. Number 34 in Bureau of Educational Research Monographs. Ohio State University Press, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeanne S Chall</author>
<author>Edgar Dale</author>
</authors>
<title>Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books,</title>
<date>1995</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="82895" citStr="Chall and Dale 1995" startWordPosition="12461" endWordPosition="12464">has attracted considerable interest and debate over the last 70 years (see Mitchell [1985] and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP technology (Schwarm and Ostendorf 2005). A number of readability formulas have been developed with the primary aim of assessing whether texts or books are suitable for students at particular grade levels or ages. Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz and Bauer 2001). Despite their widespread applicability in education and technical writing (Kincaid et al. 1981), readability formulas are often criticized for being too simplistic; they systematically ignore many important factors that affect readability such as discourse coherence and cohesion, layout and formatting, use of illustrations, the nature of the topic, the characteristics of the readers, and so forth. Schwarm and Ostendorf (2005) developed a method for assessing readability which addresses some of the shortcomings of previous approaches. By recasting readabili</context>
</contexts>
<marker>Chall, Dale, 1995</marker>
<rawString>Chall, Jeanne S. and Edgar Dale. 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="19710" citStr="Charniak 2000" startWordPosition="2867" endWordPosition="2868">bels or thematic role information). Because grammatical relations figure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reflecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [O – – – – X]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. Even though the same entity appears in different linguistic forms, for exa</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Susan E Haviland</author>
</authors>
<title>Comprehension and the given-new contract. In</title>
<date>1977</date>
<booktitle>Discourse Production and Comprehension. Ablex,</booktitle>
<pages>1--39</pages>
<editor>Roy O. Freedle, editor,</editor>
<location>Norwood, NJ,</location>
<contexts>
<context position="6644" citStr="Clark and Haviland 1977" startWordPosition="928" endWordPosition="931">tem. We first summarize entity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models. Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners. 2 Barzilay and Lapata Modeling Local Coherence 2.1 Entity-Based Approaches to Local Coherence Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; </context>
</contexts>
<marker>Clark, Haviland, 1977</marker>
<rawString>Clark, Herbert H. and Susan E. Haviland. 1977. Comprehension and the given-new contract. In Roy O. Freedle, editor, Discourse Production and Comprehension. Ablex, Norwood, NJ, pages 1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling.</title>
<date>1997</date>
<booktitle>In Proceedings of ESCA EuroSpeech’97,</booktitle>
<pages>2707--2710</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="89806" citStr="Clarkson and Rosenfeld 1997" startWordPosition="13541" endWordPosition="13544">ch class, and perplexity scores were used to assess their performance on test data. Following Schwarm and Ostendorf (2005) we used information gain to select words that were good class discriminants. All remaining words were replaced by their parts of speech. The vocabulary thus consisted of 300 words with high information gain and 36 Penn Treebank part-of-speech tags. The language models were estimated using maximum likelihood estimation and smoothed with Witten-Bell discounting. The language models described in this article were all built using the CMU statistical language modeling toolkit (Clarkson and Rosenfeld 1997). Our perplexity scores were six in total (2 classes x 3 language models). Finally, the Flesch-Kincaid Grade Level score was included as a feature that captures both syntactic and semantic text properties. The Flesch-Kincaid formula estimates readability as a combination of the the average number of syllables per word and the average number of words per sentence: 0.39 ( total words I + 11.8 ( total syllables 1 − 15.59 (3) \ total sentences) ` total words J We also enriched Schwarm and Ostendorf’s (2005) feature space with coherencebased features. Each document was represented as a feature vect</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical language modeling. In Proceedings of ESCA EuroSpeech’97, pages 2707–2710, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="19694" citStr="Collins 1997" startWordPosition="2865" endWordPosition="2866">constituent labels or thematic role information). Because grammatical relations figure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reflecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [O – – – – X]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. Even though the same entity appears in different linguisti</context>
<context position="29901" citStr="Collins 1997" startWordPosition="4520" endWordPosition="4521">rosz, Joshi, and Weinstein 1995). Most theories discriminate between subject, object, and the remaining grammatical roles: subjects are ranked higher than objects, and these are ranked higher than other grammatical functions. In our framework, we can easily assess the impact of syntactic knowledge by modifying how transitions are represented in the entity grid. In syntactically aware grids, transitions are expressed by four categories: s, o, x and –, whereas in simplified grids, we only record whether an entity is present (x) or absent (–) in a sentence. We employ a robust statistical parser (Collins 1997) to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified. The phrase-structure output of Collins’s parser is transformed into a dependency tree from which grammatical relations are extracted. Passive verbs are recognized using a small set of patterns, and the underlying deep grammatical role for arguments involved in the passive construction is entered in the grid (see the grid cell o for Microsoft, Sentence 2, Table 2). For more details on the grammatical relations extraction component we ref</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>175--182</pages>
<location>Palo Alto, CA.</location>
<contexts>
<context position="38860" citStr="Collins 2002" startWordPosition="5917" endWordPosition="5918">babilities introduced in Section 3.2. Thus, the ideal ranking function, represented by the weight vector w would satisfy the condition w · ((D(xij) − (D(xik)) &gt; 0 Vj, i,k such that j &gt; k The problem is typically treated as a Support Vector Machine constraint optimization problem, and can be solved using the search technique described in Joachims (2002). This approach has been shown to be highly effective in various tasks ranging from collaborative filtering (Joachims 2002) to parsing (Toutanova, Markova, and Manning 2004). Other discriminative formulations of the ranking problem are possible (Collins 2002; Freund et al. 2003); however, we leave this to future work. 11 Computational Linguistics Volume 34, Number 1 Table 4 The size of the training and test instances for the Earthquakes and Accidents corpora (measured by the number of pairs that contain the original order and a random permutation of this order). Training Testing Earthquakes 1,896 2,056 Accidents 2,095 2,087 Once the ranking function is learned, unseen renderings (xij,xik) of document di can be ranked simply by computing the values w*Φ(xij) and w*Φ(xik) and sorting them accordingly. Here, w* is the optimized parameter vector resul</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002. Discriminative reranking for natural language parsing. In Proceedings of the 17th International Conference on Machine Learning, pages 175–182, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas K Landauer</author>
</authors>
<title>Textual coherence using latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Foltz, Peter W., Walter Kintsch, and Thomas K. Landauer. 1998. Textual coherence using latent semantic analysis. Discourse Processes, 25(2&amp;3):285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yovav Freund</author>
<author>Raj Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferencs.</title>
<date>2003</date>
<booktitle>Machine Learning,</booktitle>
<pages>4--933</pages>
<contexts>
<context position="38881" citStr="Freund et al. 2003" startWordPosition="5919" endWordPosition="5922">roduced in Section 3.2. Thus, the ideal ranking function, represented by the weight vector w would satisfy the condition w · ((D(xij) − (D(xik)) &gt; 0 Vj, i,k such that j &gt; k The problem is typically treated as a Support Vector Machine constraint optimization problem, and can be solved using the search technique described in Joachims (2002). This approach has been shown to be highly effective in various tasks ranging from collaborative filtering (Joachims 2002) to parsing (Toutanova, Markova, and Manning 2004). Other discriminative formulations of the ranking problem are possible (Collins 2002; Freund et al. 2003); however, we leave this to future work. 11 Computational Linguistics Volume 34, Number 1 Table 4 The size of the training and test instances for the Earthquakes and Accidents corpora (measured by the number of pairs that contain the original order and a random permutation of this order). Training Testing Earthquakes 1,896 2,056 Accidents 2,095 2,087 Once the ranking function is learned, unseen renderings (xij,xik) of document di can be ranked simply by computing the values w*Φ(xij) and w*Φ(xik) and sorting them accordingly. Here, w* is the optimized parameter vector resulting from training. 4</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Freund, Yovav, Raj Iyer, Robert E. Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferencs. Machine Learning, 4:933–969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast and optimal decoding for machine translation.</title>
<date>2004</date>
<journal>Artificial Intelligence,</journal>
<pages>154--1</pages>
<contexts>
<context position="34813" citStr="Germann et al. 2004" startWordPosition="5281" endWordPosition="5284">ner takes as input a set of alternative renderings of the same document and ranks them based on their degree of local coherence. Examples of such renderings are a set of different sentence orderings of the same text and a set of summaries produced by different systems for the same document. Note that in both ranking experiments we assume that the algorithm is provided with a limited number of alternatives. In practice, the space of candidates can be vast, and finding the optimal candidate may require pairing our ranking algorithm with a decoder similar to the ones used in machine translation (Germann et al. 2004). Although the majority of our experiments fall within the generate-and-rank framework previously sketched, nothing prevents the use of our feature vector representation for conventional classification tasks. We offer an illustration in Experiment 3, where features extracted from entity grids are used to enhance the performance of a readability assessment system. Here, the learner takes as input a set of documents labeled with discrete classes (e.g., denoting whether a text is difficult or easy to read) and learns to make predictions for unseen instances (see Section 6 for details on the machi</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2004</marker>
<rawString>Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2004. Fast and optimal decoding for machine translation. Artificial Intelligence, 154(1–2):127–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Talmy Givon</author>
</authors>
<title>Beyond foreground and background.</title>
<date>1987</date>
<booktitle>Coherence and Grounding in Discourse. Benjamins, Amsterdam/Philadelphia,</booktitle>
<pages>175--188</pages>
<editor>In Russell S. Tomlin, editor,</editor>
<contexts>
<context position="3221" citStr="Givon 1987" startWordPosition="441" endWordPosition="442">n for Computational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psycholinguistic evidence. For instance, McKoon and Ratcliff (1992) argue that local coherence is the primary source of inference-making during reading. The key premise of our work is that the distribution of entities in locally coherent texts exhibits certain regularities. This assumption is not arbitrary—some of these regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein 1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences, a representation that reflects distributional, syntactic, and referential information about discourse entities. We argue that the proposed entity-based representation of discourse allows us to learn the properties of coherent texts from a corpus, without recourse to manual annotation or a predefined knowledge base. We demonstrate the usefulness of this representation by testing its predictive power in three applications: text ordering, automatic evaluation of summar</context>
<context position="8247" citStr="Givon 1987" startWordPosition="1164" endWordPosition="1165">d the notion of salience from a binary distinction to a scalar one; examples include Prince’s (1981) familiarity scale, and Givon’s (1987) and Ariel’s (1988) givenness-continuum. The salience status of an entity is often reflected in its grammatical function and the linguistic form of its subsequent mentions. Salient entities are more likely to appear in prominent syntactic positions (such as subject or object), and to be introduced in a main clause. The linguistic realization of subsequent mentions—in particular, pronominalization—is so tightly linked to salience that in some theories (e.g., Givon 1987) it provides the sole basis for defining a salience hierarchy. The hypothesis is that the degree of underspecification in a referring expression indicates the topical status of its antecedent (e.g., pronouns refer to very salient entities, whereas full NPs refer to less salient ones). In Centering Theory, this phenomenon is captured in the Pronoun Rule, and Givon’s Scale of Topicality and Ariel’s Accessibility Marking Scale propose a graded hierarchy of underspecification that ranges from zero anaphora to full noun phrases, and includes stressed and unstressed pronouns, demonstratives with mod</context>
<context position="31095" citStr="Givon 1987" startWordPosition="4701" endWordPosition="4702">component we refer the interested reader to Barzilay (2003). Salience. Centering and other discourse theories conjecture that the way an entity is introduced and mentioned depends on its global role in a given discourse. We evaluate the impact of salience information by considering two types of models: The first model treats all entities uniformly, whereas the second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). To implement a salience-based model, we modify our feature generation procedure by computing transition probabilities for each salience group separately, and then 2 When evaluating the output of coreference algorithms, performance is typically measured using a model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the recall error by taking each equivalence class S in the gold standard and determining the number of coreference links m that would have to be added to the system’s output to place all entitie</context>
</contexts>
<marker>Givon, 1987</marker>
<rawString>Givon, Talmy. 1987. Beyond foreground and background. In Russell S. Tomlin, editor, Coherence and Grounding in Discourse. Benjamins, Amsterdam/Philadelphia, pages 175–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1125" citStr="Grosz et al. 1995" startWordPosition="150" endWordPosition="153">syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are oft</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Grosz, Barbara, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jaenette K Gundel</author>
<author>Nancy Hedberg</author>
<author>Ron Zacharski 1993</author>
</authors>
<title>Cognitive status and the form of referring expressions in discourse.</title>
<journal>Language,</journal>
<volume>69</volume>
<issue>2</issue>
<marker>Gundel, Hedberg, 1993, </marker>
<rawString>Gundel, Jaenette K., Nancy Hedberg, and Ron Zacharski.1993. Cognitive status and the form of referring expressions in discourse. Language, 69(2):274–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gunning</author>
</authors>
<title>The Technique of Clear Writing.</title>
<date>1952</date>
<publisher>McGraw Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="82853" citStr="Gunning 1952" startWordPosition="12455" endWordPosition="12456">itative measurement of readability has attracted considerable interest and debate over the last 70 years (see Mitchell [1985] and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP technology (Schwarm and Ostendorf 2005). A number of readability formulas have been developed with the primary aim of assessing whether texts or books are suitable for students at particular grade levels or ages. Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz and Bauer 2001). Despite their widespread applicability in education and technical writing (Kincaid et al. 1981), readability formulas are often criticized for being too simplistic; they systematically ignore many important factors that affect readability such as discourse coherence and cohesion, layout and formatting, use of illustrations, the nature of the topic, the characteristics of the readers, and so forth. Schwarm and Ostendorf (2005) developed a method for assessing readability which addresses some of the shortcomings of p</context>
</contexts>
<marker>Gunning, 1952</marker>
<rawString>Gunning, Robert. 1952. The Technique of Clear Writing. McGraw Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="6603" citStr="Halliday and Hasan 1976" startWordPosition="922" endWordPosition="925">xt of a ranking-based text generation system. We first summarize entity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models. Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners. 2 Barzilay and Lapata Modeling Local Coherence 2.1 Entity-Based Approaches to Local Coherence Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinste</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, M. A. K. and Ruqaiya Hasan. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Hasler</author>
</authors>
<title>An investigation into the use of centering transitions for summarisation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 7th Annual CLUK Research Colloquium,</booktitle>
<pages>100--107</pages>
<location>Birmingham, UK.</location>
<contexts>
<context position="10397" citStr="Hasler 2004" startWordPosition="1475" endWordPosition="1476">one entity to the other. Givon’s (1987) and Hoey’s (1991) accounts of discourse continuity complement local measurements by considering global characteristics of entity distribution, such as the lifetime of an entity in discourse and the referential distance between subsequent mentions. Computational Modeling. An important practical question is how to translate principles of these linguistic theories into a robust coherence metric. A great deal of research has been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich 3 Computational Linguistics Volume 34, Number 1 2000; Hasler 2004; Karamanis et al. 2004). Such translation is challenging in several respects: one has to determine ways of combining the effects of various constraints and to instantiate parameters of the theory that are often left underspecified. Poesio et al. (2004) note that even for fundamental concepts of Centering Theory such as “utterance,” “realization,” and “ranking,” multiple—and often contradictory—interpretations have been developed over the years, because in the original theory these concepts are not explicitly fleshed out. For instance, in some Centering papers, entities are ranked with respect</context>
<context position="12219" citStr="Hasler (2004)" startWordPosition="1743" endWordPosition="1744">ty-based theories into computational models is to evaluate alternative specifications on manually annotated corpora. Some studies aim to find an instantiation of parameters that is most consistent with observable data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies adopt a specific instantiation with the goal of improving the performance of a metric on a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with entity transition information, and show that the distribution of transitions correlates with human grades. Analogously, Hasler (2004) investigates whether Centering Theory can be used in evaluating the readability of automatic summaries by annotating human and machine generated extracts with entity transition information. The present work differs from these approaches in goal and methodology. Although our work builds upon existing linguistic theories, we do not aim to directly implement or refine any of them in particular. We provide our model with sources of knowledge identified as essential by these theories, and leave it to the inference procedure to determine the parameter values and an optimal way to combine them. From</context>
<context position="100870" citStr="Hasler 2004" startWordPosition="15176" endWordPosition="15177">e. Our experiments quantitatively measured the predictive power of various linguistic features for several coherence-related tasks. Crucially, we find that our models are sensitive to the domain at hand and the type of texts under consideration (human-authored vs. machine generated texts). This is an unavoidable consequence of the grid representation, which is entityspecific. Differences in entity distribution indicate not only differences in coherence, but also in writing conventions and style. Similar observations have been made in other work which is closer in spirit to Centering’s claims (Hasler 2004; Karamanis et al. 2004; Poesio et al. 2004). 30 Barzilay and Lapata Modeling Local Coherence An important future direction lies in augmenting our entity-based representation with more fine-grained lexico-semantic knowledge. One way to achieve this goal is to cluster entities based on their semantic relatedness, thereby creating a grid representation over lexical chains (Morris and Hirst 1991). An entirely different approach is to develop fully lexicalized models, akin to traditional language models. Cache language models (Kuhn and De Mori 1990) seem particularly promising in this context. The</context>
</contexts>
<marker>Hasler, 2004</marker>
<rawString>Hasler, Laura. 2004. An investigation into the use of centering transitions for summarisation. In Proceedings of the 7th Annual CLUK Research Colloquium, pages 100–107, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Hoey</author>
</authors>
<title>Patterns of Lexis in Text.</title>
<date>1991</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, England.</location>
<contexts>
<context position="31118" citStr="Hoey 1991" startWordPosition="4705" endWordPosition="4706">nterested reader to Barzilay (2003). Salience. Centering and other discourse theories conjecture that the way an entity is introduced and mentioned depends on its global role in a given discourse. We evaluate the impact of salience information by considering two types of models: The first model treats all entities uniformly, whereas the second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). To implement a salience-based model, we modify our feature generation procedure by computing transition probabilities for each salience group separately, and then 2 When evaluating the output of coreference algorithms, performance is typically measured using a model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the recall error by taking each equivalence class S in the gold standard and determining the number of coreference links m that would have to be added to the system’s output to place all entities in S into the same eq</context>
</contexts>
<marker>Hoey, 1991</marker>
<rawString>Hoey, Michael. 1991. Patterns of Lexis in Text. Oxford University Press, Oxford, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Hudson</author>
<author>M K Tanenhaus</author>
<author>G S Dell</author>
</authors>
<title>The effect of the discourse center on the local coherence of a discourse.</title>
<date>1986</date>
<booktitle>In Proceedings of the 8th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>96--101</pages>
<location>Amherst, MA.</location>
<marker>Hudson, Tanenhaus, Dell, 1986</marker>
<rawString>Hudson, S. B., M. K. Tanenhaus, and G. S. Dell. 1986. The effect of the discourse center on the local coherence of a discourse. In Proceedings of the 8th Annual Meeting of the Cognitive Science Society, pages 96–101, Amherst, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thorsten 1998a Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<booktitle>Advances in Kernel Methods: Support Vector Machines.</booktitle>
<editor>In Bernard Sch¨olkopf, Christopher Burges, and Alexander Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Joachims, </marker>
<rawString>Joachims, Thorsten.1998a. Making large-scale support vector machine learning practical. In Bernard Sch¨olkopf, Christopher Burges, and Alexander Smola, editors, Advances in Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thorsten 1998b Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<booktitle>In Proceedings of the European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<publisher>Springer.</publisher>
<location>Berlin,</location>
<marker>Joachims, </marker>
<rawString>Joachims, Thorsten.1998b. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning, pages 137–142, Berlin, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>133--142</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="38602" citStr="Joachims (2002)" startWordPosition="5881" endWordPosition="5882">ij is ranked higher than xik for the optimal ranking r* (in the training data), and (D(xij) and (D(xik) are a mapping onto features representing the coherence properties of renderings xij and xik. In our case the features correspond to the entity transition probabilities introduced in Section 3.2. Thus, the ideal ranking function, represented by the weight vector w would satisfy the condition w · ((D(xij) − (D(xik)) &gt; 0 Vj, i,k such that j &gt; k The problem is typically treated as a Support Vector Machine constraint optimization problem, and can be solved using the search technique described in Joachims (2002). This approach has been shown to be highly effective in various tasks ranging from collaborative filtering (Joachims 2002) to parsing (Toutanova, Markova, and Manning 2004). Other discriminative formulations of the ranking problem are possible (Collins 2002; Freund et al. 2003); however, we leave this to future work. 11 Computational Linguistics Volume 34, Number 1 Table 4 The size of the training and test instances for the Earthquakes and Accidents corpora (measured by the number of pairs that contain the original order and a random permutation of this order). Training Testing Earthquakes 1,</context>
<context position="69745" citStr="Joachims (2002)" startWordPosition="10538" endWordPosition="10539">by several systems that operate over identical inputs or by a single system (e.g., by varying the compression length or by switching on or off individual system modules, for example a sentence compression or anaphora resolution module). Similarly to the sentence ordering task, our training data includes pairs of summaries (xij,xik) of the same document(s) di, where xij is more coherent than xik. An optimal learner should return a ranking r∗ that orders the summaries according to their coherence. As in Experiment 1 we adopt an optimization approach and follow the training regime put forward by Joachims (2002). 5.2 Method Data. Our evaluation was based on materials from the Document Understanding Conference (DUC 2003), which include multi-document summaries produced by human writers and by automatic summarization systems. In order to learn a ranking, we require a set of summaries, each of which has been rated in terms of coherence. One stumbling block to performing this kind of evaluation is the coherence ratings themselves, which are not routinely provided by DUC summary evaluators. In DUC 2003, the quality of automatically generated summaries was assessed along several dimensions ranging from gra</context>
<context position="75812" citStr="Joachims 2002" startWordPosition="11451" endWordPosition="11452">nerated summaries. Because many summaries in our corpus are fraught with coherence violations, the performance of a coreference resolution tool is likely to drop. Unfortunately, resolving coreference in the input documents would require a multi-document coreference tool, which is currently unavailable to us. As in Experiment 1, the frequency threshold and the length of the transition sequence were optimized on the development set. Optimal salience-based models were obtained for entities with frequency ≥2. The optimal transition length was ≤2. All models were trained and tested using SVMlight (Joachims 2002). Comparison with State-of-the-Art Methods. Our results were compared to the LSA model introduced in Experiment 1 (see Section 4.2 for details). Unfortunately, we could not 10 We cannot apply the commonly used Kappa statistic for measuring agreement because it is appropriate for nominal scales, whereas our summaries are rated on an ordinal scale. 22 Barzilay and Lapata Modeling Local Coherence employ Barzilay and Lee’s (2004) content models for the summary ranking task. Being domain-dependent, these models require access to domain representative texts for training. Our summary corpus, however,</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims, Thorsten. 2002. Optimizing search engines using clickthrough data. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining, pages 133–142, Chicago, IL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Megumi 1986 Kameyama</author>
</authors>
<title>A property-sharing constraint in centering.</title>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>200--206</pages>
<location>New York.</location>
<marker>Kameyama, </marker>
<rawString>Kameyama, Megumi.1986. A property-sharing constraint in centering. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 200–206, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
</authors>
<title>Entity Coherence for Descriptive Text Structuring.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="13609" citStr="Karamanis 2003" startWordPosition="1945" endWordPosition="1946">tional models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function. The top-ranked candidate is selected for presentation. A two-stage generate-a</context>
<context position="15060" citStr="Karamanis 2003" startWordPosition="2166" endWordPosition="2167"> single high-quality output. Because the focus of our work is on text coherence, we discuss here ranking approaches applied to text planning (see Walker et al. [2001] and Knight and Hatzivassiloglou [1995] for ranking approaches to sentence planning and surface realization, respectively). The goal of text planning is to determine the content of a text by selecting a set of information-bearing units and arranging them into a structure that yields well-formed output. Depending on the system, text plans are represented as discourse trees (Mellish et al. 1998) or linear sequences of propositions (Karamanis 2003). Candidate text structures may differ in terms of the selected propositions, the sequence in which facts are presented, the topology of the tree, or the order in which entities are introduced. A set of plausible candidates can be created via stochastic search (Mellish et al. 1998) or by a symbolic text planner following different text-formation rules (Kibble and Power 2004). The best candidate is chosen using an evaluation or ranking function often encoding coherence constraints. Although the type and complexity of constraints vary greatly across systems, they are commonly inspired by Rhetori</context>
<context position="39814" citStr="Karamanis 2003" startWordPosition="6064" endWordPosition="6065">1,896 2,056 Accidents 2,095 2,087 Once the ranking function is learned, unseen renderings (xij,xik) of document di can be ranked simply by computing the values w*Φ(xij) and w*Φ(xik) and sorting them accordingly. Here, w* is the optimized parameter vector resulting from training. 4.2 Method Data. To acquire a large collection for training and testing, we create synthetic data, wherein the candidate set consists of a source document and permutations of its sentences. This framework for data acquisition enables large-scale automatic evaluation and is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003; Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004). The underlying assumption is that the original sentence order in the source document must be coherent, and so we should prefer models that rank it higher than other permutations. Because we do not know the relative quality of different permutations, our corpus includes only pairwise rankings that comprise the original document and one of its permutations. Given k original documents, each with n randomly generated permutations, we obtain k · n (trivially) annotated pairwise rankings for training and testing. Using the t</context>
</contexts>
<marker>Karamanis, 2003</marker>
<rawString>Karamanis, Nikiforos. 2003. Entity Coherence for Descriptive Text Structuring. Ph.D. thesis, University of Edinburgh, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikiforos Karamanis</author>
<author>Massimo Poesio</author>
<author>Chris Mellish</author>
<author>Jon Oberlander</author>
</authors>
<title>Evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>391--398</pages>
<location>Barcelona,</location>
<contexts>
<context position="2717" citStr="Karamanis et al. 2004" startWordPosition="365" endWordPosition="368">nguistics (Foltz, Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller * Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu. ** School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psycholinguistic evidence. For instance, McKoon and Ratcliff (1992) argue that local coherence is the primary source of inference-making during reading. The key premise of our work is that the distribution of entities in locally coherent texts exhibits certain regularities. This assumption is not arbitrary—some of these regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein 1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The algorithm introduced in the article automatically abstracts a text into a se</context>
<context position="10421" citStr="Karamanis et al. 2004" startWordPosition="1477" endWordPosition="1480"> the other. Givon’s (1987) and Hoey’s (1991) accounts of discourse continuity complement local measurements by considering global characteristics of entity distribution, such as the lifetime of an entity in discourse and the referential distance between subsequent mentions. Computational Modeling. An important practical question is how to translate principles of these linguistic theories into a robust coherence metric. A great deal of research has been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich 3 Computational Linguistics Volume 34, Number 1 2000; Hasler 2004; Karamanis et al. 2004). Such translation is challenging in several respects: one has to determine ways of combining the effects of various constraints and to instantiate parameters of the theory that are often left underspecified. Poesio et al. (2004) note that even for fundamental concepts of Centering Theory such as “utterance,” “realization,” and “ranking,” multiple—and often contradictory—interpretations have been developed over the years, because in the original theory these concepts are not explicitly fleshed out. For instance, in some Centering papers, entities are ranked with respect to their grammatical fu</context>
<context position="11868" citStr="Karamanis et al. 2004" startWordPosition="1687" endWordPosition="1690">hematic role (Sidner 1979). As a result, two “instantiations” of the same theory make different predictions for the same input. Poesio et al. (2004) explore alternative specifications proposed in the literature, and demonstrate that the predictive power of the theory is highly sensitive to its parameter definitions. A common methodology for translating entity-based theories into computational models is to evaluate alternative specifications on manually annotated corpora. Some studies aim to find an instantiation of parameters that is most consistent with observable data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies adopt a specific instantiation with the goal of improving the performance of a metric on a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with entity transition information, and show that the distribution of transitions correlates with human grades. Analogously, Hasler (2004) investigates whether Centering Theory can be used in evaluating the readability of automatic summaries by annotating human and machine generated extracts with entity transition information. The present work differs from these approaches in goal and</context>
<context position="26747" citStr="Karamanis et al. 2004" startWordPosition="3993" endWordPosition="3996">ng tools. For instance, we could not experiment with the granularity of an utterance— sentence versus clause—because available clause separators introduce substantial noise into a grid construction. Finally, we exclude representations that will explode the size of the feature space, thereby increasing the amount of data required for training the model. Entity Extraction. The accurate computation of entity classes is key to computing meaningful entity grids. In previous implementations of entity-based models, classes of coreferent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis et al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvious solution for identifying entity classes is to employ an automatic coreference resolution tool that determines which noun phrases refer to the same entity in a document. Current approaches recast coreference resolution as a classification task. A pair of NPs is classified as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NPs. In our experiments, </context>
<context position="35568" citStr="Karamanis et al. 2004" startWordPosition="5395" endWordPosition="5398">of our feature vector representation for conventional classification tasks. We offer an illustration in Experiment 3, where features extracted from entity grids are used to enhance the performance of a readability assessment system. Here, the learner takes as input a set of documents labeled with discrete classes (e.g., denoting whether a text is difficult or easy to read) and learns to make predictions for unseen instances (see Section 6 for details on the machine learning paradigm we employ). 4. Experiment 1: Sentence Ordering Text structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al. 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information10 Barzilay and Lapata Modeling Local Coherence bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. The information bearing items can be database entries (Karamanis et al. 2004), propositions (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). In sentence ordering, a document is viewed as a bag of sentences and the algorithm’s t</context>
<context position="100893" citStr="Karamanis et al. 2004" startWordPosition="15178" endWordPosition="15181">ments quantitatively measured the predictive power of various linguistic features for several coherence-related tasks. Crucially, we find that our models are sensitive to the domain at hand and the type of texts under consideration (human-authored vs. machine generated texts). This is an unavoidable consequence of the grid representation, which is entityspecific. Differences in entity distribution indicate not only differences in coherence, but also in writing conventions and style. Similar observations have been made in other work which is closer in spirit to Centering’s claims (Hasler 2004; Karamanis et al. 2004; Poesio et al. 2004). 30 Barzilay and Lapata Modeling Local Coherence An important future direction lies in augmenting our entity-based representation with more fine-grained lexico-semantic knowledge. One way to achieve this goal is to cluster entities based on their semantic relatedness, thereby creating a grid representation over lexical chains (Morris and Hirst 1991). An entirely different approach is to develop fully lexicalized models, akin to traditional language models. Cache language models (Kuhn and De Mori 1990) seem particularly promising in this context. The granularity of syntact</context>
</contexts>
<marker>Karamanis, Poesio, Mellish, Oberlander, 2004</marker>
<rawString>Karamanis, Nikiforos, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 391–398, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Discourse referents.</title>
<date>1976</date>
<booktitle>Syntax and Semantics: Notes from the Linguistic Underground,</booktitle>
<volume>7</volume>
<pages>363--386</pages>
<editor>In James D. McCawley, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="6619" citStr="Karttunen 1976" startWordPosition="926" endWordPosition="927">t generation system. We first summarize entity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models. Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners. 2 Barzilay and Lapata Modeling Local Coherence 2.1 Entity-Based Approaches to Local Coherence Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker,</context>
</contexts>
<marker>Karttunen, 1976</marker>
<rawString>Karttunen, Lauri. 1976. Discourse referents. In James D. McCawley, editor, Syntax and Semantics: Notes from the Linguistic Underground, volume 7. Academic Press, New York, pages 363–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irvin R Katz</author>
<author>Malcolm I Bauer</author>
</authors>
<title>Sourcefinder: Course preparation via linguistically targeted web search.</title>
<date>2001</date>
<journal>Journal of Educational Technology and Society,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="82931" citStr="Katz and Bauer 2001" startWordPosition="12467" endWordPosition="12470"> and debate over the last 70 years (see Mitchell [1985] and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP technology (Schwarm and Ostendorf 2005). A number of readability formulas have been developed with the primary aim of assessing whether texts or books are suitable for students at particular grade levels or ages. Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz and Bauer 2001). Despite their widespread applicability in education and technical writing (Kincaid et al. 1981), readability formulas are often criticized for being too simplistic; they systematically ignore many important factors that affect readability such as discourse coherence and cohesion, layout and formatting, use of illustrations, the nature of the topic, the characteristics of the readers, and so forth. Schwarm and Ostendorf (2005) developed a method for assessing readability which addresses some of the shortcomings of previous approaches. By recasting readability assessment as a classification ta</context>
</contexts>
<marker>Katz, Bauer, 2001</marker>
<rawString>Katz, Irvin R. and Malcolm I. Bauer. 2001. Sourcefinder: Course preparation via linguistically targeted web search. Journal of Educational Technology and Society, 4(3):45–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodger Kibble</author>
<author>Richard Power</author>
</authors>
<title>Optimising referential coherence in text generation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1265" citStr="Kibble and Power 2004" startWordPosition="172" endWordPosition="175">at our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article f</context>
<context position="13633" citStr="Kibble and Power 2004" startWordPosition="1947" endWordPosition="1950">veloped on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function. The top-ranked candidate is selected for presentation. A two-stage generate-and-rank architecture cir</context>
<context position="15437" citStr="Kibble and Power 2004" startWordPosition="2224" endWordPosition="2227">et of information-bearing units and arranging them into a structure that yields well-formed output. Depending on the system, text plans are represented as discourse trees (Mellish et al. 1998) or linear sequences of propositions (Karamanis 2003). Candidate text structures may differ in terms of the selected propositions, the sequence in which facts are presented, the topology of the tree, or the order in which entities are introduced. A set of plausible candidates can be created via stochastic search (Mellish et al. 1998) or by a symbolic text planner following different text-formation rules (Kibble and Power 2004). The best candidate is chosen using an evaluation or ranking function often encoding coherence constraints. Although the type and complexity of constraints vary greatly across systems, they are commonly inspired by Rhetorical Structure Theory or entity-based constraints similar to the ones captured by our method. For instance, the ranking function used by Mellish et al. gives preference to plans where consecutive facts mention the same entities and is sensitive to the syntactic environment in which the entity is first introduced (e.g., in a subject or object position). Karamanis finds that a </context>
</contexts>
<marker>Kibble, Power, 2004</marker>
<rawString>Kibble, Rodger and Richard Power. 2004. Optimising referential coherence in text generation. Computational Linguistics, 30(4):401–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Peter Kincaid</author>
<author>James Aagard</author>
<author>John O’Hara</author>
<author>Larry Cottrell</author>
</authors>
<title>Computer readability editing system.</title>
<date>1981</date>
<journal>IEEE Transactions on Professional Communication,</journal>
<volume>1</volume>
<issue>24</issue>
<marker>Kincaid, Aagard, O’Hara, Cottrell, 1981</marker>
<rawString>Kincaid, J. Peter, James Aagard, John O’Hara, and Larry Cottrell. 1981. Computer readability editing system. IEEE Transactions on Professional Communication, 1(24):34–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Kincaid</author>
<author>Robert P Fishburne</author>
<author>Richard L Rodgers</author>
<author>Brad S Chissom</author>
</authors>
<title>Derivation of new readability formulas for Navy enlisted personnel.</title>
<date>1975</date>
<journal>Research Branch</journal>
<tech>Report 8-75,</tech>
<institution>U.S. Naval Air Station,</institution>
<location>Memphis, TN.</location>
<contexts>
<context position="82874" citStr="Kincaid et al. 1975" startWordPosition="12457" endWordPosition="12460">ement of readability has attracted considerable interest and debate over the last 70 years (see Mitchell [1985] and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP technology (Schwarm and Ostendorf 2005). A number of readability formulas have been developed with the primary aim of assessing whether texts or books are suitable for students at particular grade levels or ages. Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz and Bauer 2001). Despite their widespread applicability in education and technical writing (Kincaid et al. 1981), readability formulas are often criticized for being too simplistic; they systematically ignore many important factors that affect readability such as discourse coherence and cohesion, layout and formatting, use of illustrations, the nature of the topic, the characteristics of the readers, and so forth. Schwarm and Ostendorf (2005) developed a method for assessing readability which addresses some of the shortcomings of previous approaches. B</context>
</contexts>
<marker>Kincaid, Fishburne, Rodgers, Chissom, 1975</marker>
<rawString>Kincaid, Peter J., Robert P. Fishburne, Richard L. Rodgers, and Brad S. Chissom. 1975. Derivation of new readability formulas for Navy enlisted personnel. Research Branch Report 8-75, U.S. Naval Air Station, Memphis, TN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Two-level, many-path generation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>252--260</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="13512" citStr="Knight and Hatzivassiloglou 1995" startWordPosition="1928" endWordPosition="1931">oth the underlying discourse representation and the inference procedure. Thus, our work is complementary to computational models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in </context>
</contexts>
<marker>Knight, Hatzivassiloglou, 1995</marker>
<rawString>Knight, Kevin and Vasileios Hatzivassiloglou. 1995. Two-level, many-path generation. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 252–260, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>192--199</pages>
<editor>In Thorsten Joachims, editor,</editor>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="87151" citStr="Kudo and Matsumoto 2001" startWordPosition="13145" endWordPosition="13148">ts that crossed each other and ran the entire width and length of the town. Valletta was one of the first towns to be laid out in this way. vector w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi[(w ·�xi) + b] &gt; 0 Finding the optimal hyperplane is an optimization problem which can be solved efficiently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classification (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simplified articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difficult.11 Examples of thes</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Kudo, Taku and Yuji Matsumoto. 2001. Chunking with support vector machines. In Thorsten Joachims, editor, Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 192–199, Pittsburgh, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Kuhn</author>
<author>R De</author>
</authors>
<title>Mori.1990. A cache-based natural language model for speech recognition.</title>
<journal>IEEE Transactions on PAMI,</journal>
<volume>12</volume>
<issue>6</issue>
<marker>Kuhn, De, </marker>
<rawString>Kuhn, R. and R. De Mori.1990. A cache-based natural language model for speech recognition. IEEE Transactions on PAMI, 12(6):570–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Kuno</author>
</authors>
<title>Functional sentence perspective. Linguistic Inquiry,</title>
<date>1972</date>
<pages>3--269</pages>
<contexts>
<context position="6566" citStr="Kuno 1972" startWordPosition="918" endWordPosition="919">nce metric in the context of a ranking-based text generation system. We first summarize entity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models. Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners. 2 Barzilay and Lapata Modeling Local Coherence 2.1 Entity-Based Approaches to Local Coherence Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Center</context>
</contexts>
<marker>Kuno, 1972</marker>
<rawString>Kuno, Susumu. 1972. Functional sentence perspective. Linguistic Inquiry, 3:269–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="46172" citStr="Landauer and Dumais 1997" startWordPosition="6976" endWordPosition="6979">dels we employed in our comparative study and motivate their selection. 6 The models we used in our experiments are available from http://people.csail.mit.edu/ regina/coherence/ and http://homepages.inf.ed.ac.uk/mlap/coherence/. 13 Computational Linguistics Volume 34, Number 1 Foltz, Kintsch, and Landauer (1998) model measures coherence as a function of semantic relatedness between adjacent sentences. The underlying intuition here is that coherent texts will contain a high number of semantically related words. Semantic relatedness is computed automatically using Latent Semantic Analysis (LSA; Landauer and Dumais 1997) from raw text without employing syntactic or other annotations. In this framework, a word’s meaning is captured in a multi-dimensional space by a vector representing its co-occurrence with neighboring words. Co-occurrence information is collected in a frequency matrix, where each row corresponds to a unique word, and each column represents a given linguistic context (e.g., sentence, document, or paragraph). Foltz, Kintsch, and Landauer’s model use singular value decomposition (SVD; Berry, Dumais, and O’Brien 1994) to reduce the dimensionality of the space. The transformation renders sparse ma</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas K. and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics and 36th</booktitle>
<contexts>
<context position="13539" citStr="Langkilde and Knight 1998" startWordPosition="1932" endWordPosition="1935">sentation and the inference procedure. Thus, our work is complementary to computational models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function. The top</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde, Irene and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the 17th International Conference on Computational Linguistics and 36th</rawString>
</citation>
<citation valid="false">
<title>Annual Meeting of the Association for Computational Linguistics,</title>
<pages>704--710</pages>
<location>Montr´eal, Canada.</location>
<marker></marker>
<rawString>Annual Meeting of the Association for Computational Linguistics, pages 704–710, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>545--552</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2164" citStr="Lapata 2003" startWordPosition="296" endWordPosition="297">ied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article focuses on local coherence, which captures text relatedness at the level of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global coherence and has received considerable attention in computational linguistics (Foltz, Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller * Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu. ** School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psycholinguistic</context>
<context position="35521" citStr="Lapata 2003" startWordPosition="5389" endWordPosition="5390"> sketched, nothing prevents the use of our feature vector representation for conventional classification tasks. We offer an illustration in Experiment 3, where features extracted from entity grids are used to enhance the performance of a readability assessment system. Here, the learner takes as input a set of documents labeled with discrete classes (e.g., denoting whether a text is difficult or easy to read) and learns to make predictions for unseen instances (see Section 6 for details on the machine learning paradigm we employ). 4. Experiment 1: Sentence Ordering Text structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al. 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information10 Barzilay and Lapata Modeling Local Coherence bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. The information bearing items can be database entries (Karamanis et al. 2004), propositions (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). In sentence ordering, a document is vie</context>
<context position="39827" citStr="Lapata 2003" startWordPosition="6066" endWordPosition="6067">dents 2,095 2,087 Once the ranking function is learned, unseen renderings (xij,xik) of document di can be ranked simply by computing the values w*Φ(xij) and w*Φ(xik) and sorting them accordingly. Here, w* is the optimized parameter vector resulting from training. 4.2 Method Data. To acquire a large collection for training and testing, we create synthetic data, wherein the candidate set consists of a source document and permutations of its sentences. This framework for data acquisition enables large-scale automatic evaluation and is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003; Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004). The underlying assumption is that the original sentence order in the source document must be coherent, and so we should prefer models that rank it higher than other permutations. Because we do not know the relative quality of different permutations, our corpus includes only pairwise rankings that comprise the original document and one of its permutations. Given k original documents, each with n randomly generated permutations, we obtain k · n (trivially) annotated pairwise rankings for training and testing. Using the technique desc</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Lapata, Mirella. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 545–552, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2nd Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>71--78</pages>
<location>Boston, MA.</location>
<contexts>
<context position="68704" citStr="Lin and Hovy 2003" startWordPosition="10373" endWordPosition="10376">ers encounter in machine generated texts. A representative example of such texts are automatically generated summaries which often contain sentences taken out of context and thus display problems with respect to local coherence (e.g., dangling anaphors, thematically unrelated sentences). A model that exhibits high agreement with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts. Existing automatic evaluation measures such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003) are not designed for the coherence assessment task, because they focus on content similarity between system output and reference texts. 20 Barzilay and Lapata Modeling Local Coherence 5.1 Modeling Summary coherence rating can be also formulated as a ranking learning task. We are assuming that the learner has access to several summaries corresponding to the same document or document cluster. Such summaries can be produced by several systems that operate over identical inputs or by a single system (e.g., by varying the compression length or by switching on or off individual system modules, for </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, Chin-Yew and Eduard H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2nd Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 71–78, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>LaTaT: Language and text analysis tools.</title>
<date>2001</date>
<booktitle>In Proceedings of the 1st International Conference on Human Language Technology Research,</booktitle>
<pages>222--227</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="19610" citStr="Lin 2001" startWordPosition="2853" endWordPosition="2854">eir syntactic role. Such information can be expressed in many ways (e.g., using constituent labels or thematic role information). Because grammatical relations figure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reflecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [O – – – – X]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference</context>
</contexts>
<marker>Lin, 2001</marker>
<rawString>Lin, Dekang. 2001. LaTaT: Language and text analysis tools. In Proceedings of the 1st International Conference on Human Language Technology Research, pages 222–227, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thomson</author>
</authors>
<title>Rhetorical structure theory.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1105" citStr="Mann and Thomson 1988" startWordPosition="146" endWordPosition="149">ecords distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence</context>
</contexts>
<marker>Mann, Thomson, 1988</marker>
<rawString>Mann, William C. and Sandra A. Thomson. 1988. Rhetorical structure theory. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2151" citStr="Marcu 2000" startWordPosition="294" endWordPosition="295">rts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article focuses on local coherence, which captures text relatedness at the level of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global coherence and has received considerable attention in computational linguistics (Foltz, Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller * Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu. ** School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psy</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Marcu, Daniel. 2000. The Theory and Practice of Discourse Parsing and Summarization. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gail McKoon</author>
<author>Roger Ratcliff</author>
</authors>
<title>Inference during reading.</title>
<date>1992</date>
<journal>Psychological Review,</journal>
<volume>99</volume>
<issue>3</issue>
<contexts>
<context position="2815" citStr="McKoon and Ratcliff (1992)" startWordPosition="379" endWordPosition="382"> Koller * Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu. ** School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psycholinguistic evidence. For instance, McKoon and Ratcliff (1992) argue that local coherence is the primary source of inference-making during reading. The key premise of our work is that the distribution of entities in locally coherent texts exhibits certain regularities. This assumption is not arbitrary—some of these regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein 1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences, a representation that reflects distributional, syntactic, and re</context>
</contexts>
<marker>McKoon, Ratcliff, 1992</marker>
<rawString>McKoon, Gail and Roger Ratcliff. 1992. Inference during reading. Psychological Review, 99(3):440–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Mick O’Donnell</author>
<author>Jon Oberlander</author>
<author>Alistair Knott</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation,</booktitle>
<pages>98--107</pages>
<location>New Brunswick, NJ.</location>
<marker>Mellish, O’Donnell, Oberlander, Knott, 1998</marker>
<rawString>Mellish, Chris, Mick O’Donnell, Jon Oberlander, and Alistair Knott. 1998. Experiments using stochastic search for text planning. In Proceedings of the 9th International Workshop on Natural Language Generation, pages 98–107, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleni Miltsakaki</author>
<author>Karen Kukich</author>
</authors>
<title>The role of centering theory’s rough-shift in the teaching and evaluation of writing skills.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>408--415</pages>
<location>Hong Kong.</location>
<contexts>
<context position="12044" citStr="Miltsakaki and Kukich (2000)" startWordPosition="1716" endWordPosition="1719">specifications proposed in the literature, and demonstrate that the predictive power of the theory is highly sensitive to its parameter definitions. A common methodology for translating entity-based theories into computational models is to evaluate alternative specifications on manually annotated corpora. Some studies aim to find an instantiation of parameters that is most consistent with observable data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies adopt a specific instantiation with the goal of improving the performance of a metric on a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with entity transition information, and show that the distribution of transitions correlates with human grades. Analogously, Hasler (2004) investigates whether Centering Theory can be used in evaluating the readability of automatic summaries by annotating human and machine generated extracts with entity transition information. The present work differs from these approaches in goal and methodology. Although our work builds upon existing linguistic theories, we do not aim to directly implement or refine any of them in particular. We provide our model with sou</context>
<context position="18379" citStr="Miltsakaki and Kukich (2000)" startWordPosition="2659" endWordPosition="2662">presentation of discourse. We explain how it is computed and how entity transition patterns are extracted. We also discuss how 1 Each utterance in the discourse refers to at least one entity in the utterance that precedes it. 5 Computational Linguistics Volume 34, Number 1 these patterns can be encoded as feature vectors appropriate for performing coherencerelated ranking and classification tasks. 3.1 The Entity-Grid Discourse Representation Each text is represented by an entity grid, a two-dimensional array that captures the distribution of discourse entities across text sentences. We follow Miltsakaki and Kukich (2000) in assuming that our unit of analysis is the traditional sentence (i.e., a main clause with accompanying subordinate and adjunct clauses). The rows of the grid correspond to sentences, and the columns correspond to discourse entities. By discourse entity we mean a class of coreferent noun phrases (we explain in Section 3.3 how coreferent entities are identified). For each occurrence of a discourse entity in the text, the corresponding grid cell contains information about its presence or absence in a sequence of sentences. In addition, for entities present in a given sentence, grid cells conta</context>
<context position="26724" citStr="Miltsakaki and Kukich 2000" startWordPosition="3989" endWordPosition="3992"> computed reliably by existing tools. For instance, we could not experiment with the granularity of an utterance— sentence versus clause—because available clause separators introduce substantial noise into a grid construction. Finally, we exclude representations that will explode the size of the feature space, thereby increasing the amount of data required for training the model. Entity Extraction. The accurate computation of entity classes is key to computing meaningful entity grids. In previous implementations of entity-based models, classes of coreferent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamanis et al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvious solution for identifying entity classes is to employ an automatic coreference resolution tool that determines which noun phrases refer to the same entity in a document. Current approaches recast coreference resolution as a classification task. A pair of NPs is classified as coreferring or not based on constraints that are learned from an annotated corpus. A separate clustering mechanism then coordinates the possibly contradictory pairwise classifications and constructs a partition on the set of NP</context>
</contexts>
<marker>Miltsakaki, Kukich, 2000</marker>
<rawString>Miltsakaki, Eleni and Karen Kukich. 2000. The role of centering theory’s rough-shift in the teaching and evaluation of writing skills. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 408–415, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James V Mitchell</author>
</authors>
<title>The Ninth Mental Measurements Yearbook.</title>
<date>1985</date>
<publisher>University of Nebraska Press,</publisher>
<location>Lincoln.</location>
<marker>Mitchell, 1985</marker>
<rawString>Mitchell, James V. 1985. The Ninth Mental Measurements Yearbook. University of Nebraska Press, Lincoln.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>1</volume>
<issue>17</issue>
<contexts>
<context position="31142" citStr="Morris and Hirst 1991" startWordPosition="4707" endWordPosition="4710">eader to Barzilay (2003). Salience. Centering and other discourse theories conjecture that the way an entity is introduced and mentioned depends on its global role in a given discourse. We evaluate the impact of salience information by considering two types of models: The first model treats all entities uniformly, whereas the second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). To implement a salience-based model, we modify our feature generation procedure by computing transition probabilities for each salience group separately, and then 2 When evaluating the output of coreference algorithms, performance is typically measured using a model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the recall error by taking each equivalence class S in the gold standard and determining the number of coreference links m that would have to be added to the system’s output to place all entities in S into the same equivalence class produced</context>
<context position="101266" citStr="Morris and Hirst 1991" startWordPosition="15232" endWordPosition="15235">ences in entity distribution indicate not only differences in coherence, but also in writing conventions and style. Similar observations have been made in other work which is closer in spirit to Centering’s claims (Hasler 2004; Karamanis et al. 2004; Poesio et al. 2004). 30 Barzilay and Lapata Modeling Local Coherence An important future direction lies in augmenting our entity-based representation with more fine-grained lexico-semantic knowledge. One way to achieve this goal is to cluster entities based on their semantic relatedness, thereby creating a grid representation over lexical chains (Morris and Hirst 1991). An entirely different approach is to develop fully lexicalized models, akin to traditional language models. Cache language models (Kuhn and De Mori 1990) seem particularly promising in this context. The granularity of syntactic information is another topic that warrants further investigation. So far we have only considered the contribution of “core” grammatical relations to the grid construction. Expanding our grammatical categories to modifiers and adjuncts may provide additional information, in particular when considering machine generated texts. We also plan to investigate whether the pro</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 1(17):21–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches Computational Linguistics Volume 34, Number 1 to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="98435" citStr="Ng and Cardie 2002" startWordPosition="14807" endWordPosition="14810">the influence of salience. All these knowledge sources figure prominently in theories of discourse (see Section 2) and are considered important in determining coherence. Our results empirically validate the importance of salience and syntactic information (expressed by S, O, X, and –) for coherence-based models. The combination of both knowledge sources (Syntax+Salience) yields models with consistently good performance for all our tasks. The benefits of full coreference resolution are less uniform. This is partly due to mismatches between training and testing conditions. The system we employ (Ng and Cardie 2002) was trained on human-authored newspaper texts. The corpora we used in our sentence ordering and readability assessment experiments are somewhat similar (i.e., human-authored narratives), whereas our summary coherence rating experiment employed machine generated texts. It is therefore not surprising that coreference resolution delivers performance gains on the first two tasks but not on the latter (see Table 5 in Section 4 and Table 10 in Section 6.3). Our results further show that in lieu of an automatic coreference resolution system, entity classes can be approximated simply by string matchi</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Ng, Vincent and Claire Cardie. 2002. Improving machine learning approaches Computational Linguistics Volume 34, Number 1 to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 104–111, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="68674" citStr="Papineni et al. 2002" startWordPosition="10367" endWordPosition="10370">erence violations that human readers encounter in machine generated texts. A representative example of such texts are automatically generated summaries which often contain sentences taken out of context and thus display problems with respect to local coherence (e.g., dangling anaphors, thematically unrelated sentences). A model that exhibits high agreement with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts. Existing automatic evaluation measures such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003) are not designed for the coherence assessment task, because they focus on content similarity between system output and reference texts. 20 Barzilay and Lapata Modeling Local Coherence 5.1 Modeling Summary coherence rating can be also formulated as a ranking learning task. We are assuming that the learner has access to several summaries corresponding to the same document or document cluster. Such summaries can be produced by several systems that operate over identical inputs or by a single system (e.g., by varying the compression length or by switching on or off i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Rosemary Stevenson</author>
<author>Barbara Di Eugenio</author>
<author>Janet Hitzeman</author>
</authors>
<title>Centering: A parametric theory and its instantiations.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<marker>Poesio, Stevenson, Di Eugenio, Hitzeman, 2004</marker>
<rawString>Poesio, Massimo, Rosemary Stevenson, Barbara Di Eugenio, and Janet Hitzeman. 2004. Centering: A parametric theory and its instantiations. Computational Linguistics, 30(3):309–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<issue>1</issue>
<contexts>
<context position="87203" citStr="Pradhan et al. 2005" startWordPosition="13153" endWordPosition="13156">length of the town. Valletta was one of the first towns to be laid out in this way. vector w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi[(w ·�xi) + b] &gt; 0 Finding the optimal hyperplane is an optimization problem which can be solved efficiently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classification (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simplified articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difficult.11 Examples of these two categories are given in Table 9. 11 The Britan</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Dan Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning, 60(1):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Prince</author>
</authors>
<title>A comparison of wh-clefts and it-clefts in discourse.</title>
<date>1978</date>
<journal>Language,</journal>
<pages>54--883</pages>
<contexts>
<context position="7487" citStr="Prince 1978" startWordPosition="1054" endWordPosition="1055"> commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and Hahn 1999; Poesio et al. 2004), salience concerns how entities are realized in an utterance (e.g., whether they are they pronominalized or not). In other theories, salience is defined in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More refined accounts expand the notion of salience from a binary distinction to a scalar one; examples include Prince’s (1981) familiarity scale, and Givon’s (1987) and Ariel’s (1988) givenness-continuum. The salience status of an entity is often reflected in its grammatical function and the linguistic form of its subsequent mentions. Salient entities are more likely to appear in prominent syntactic positions (such as subject or object), and to be introduced in a main clau</context>
</contexts>
<marker>Prince, 1978</marker>
<rawString>Prince, Ellen. 1978. A comparison of wh-clefts and it-clefts in discourse. Language, 54:883–906.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Prince</author>
</authors>
<title>Toward a taxonomy of given-new information.</title>
<date>1981</date>
<pages>223--255</pages>
<editor>In Peter Cole, editor, Radical Pragmatics.</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="3235" citStr="Prince 1981" startWordPosition="443" endWordPosition="444">ational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psycholinguistic evidence. For instance, McKoon and Ratcliff (1992) argue that local coherence is the primary source of inference-making during reading. The key premise of our work is that the distribution of entities in locally coherent texts exhibits certain regularities. This assumption is not arbitrary—some of these regularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein 1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981). The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences, a representation that reflects distributional, syntactic, and referential information about discourse entities. We argue that the proposed entity-based representation of discourse allows us to learn the properties of coherent texts from a corpus, without recourse to manual annotation or a predefined knowledge base. We demonstrate the usefulness of this representation by testing its predictive power in three applications: text ordering, automatic evaluation of summary coherence, a</context>
<context position="6657" citStr="Prince 1981" startWordPosition="932" endWordPosition="933">ntity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models. Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners. 2 Barzilay and Lapata Modeling Local Coherence 2.1 Entity-Based Approaches to Local Coherence Linguistic Modeling. Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and Ha</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Prince, Ellen. 1981. Toward a taxonomy of given-new information. In Peter Cole, editor, Radical Pragmatics. Academic Press, New York, pages 223–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural-Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="1681" citStr="Reiter and Dale 2000" startWordPosition="230" endWordPosition="233">d over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article focuses on local coherence, which captures text relatedness at the level of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global coherence and has received considerable attention in computational linguistics (Foltz, Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller * Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute o</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Reiter, Ehud and Robert Dale. 2000. Building Natural-Language Generation Systems. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah E Schwarm</author>
<author>Mari Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="5427" citStr="Schwarm and Ostendorf 2005" startWordPosition="755" endWordPosition="758">tomatically generated summaries. In both experiments, our method yields improvements over state-of-the-art models. We also show the benefits of the entitybased representation in a readability assessment task, where the goal is to predict the comprehension difficulty of a given text. In contrast to existing systems which focus on intra-sentential features, we explore the contribution of discourse-level features to this task. By incorporating coherence features stemming from the proposed entity-based representation, we improve the performance of a state-of-the-art readability assessment system (Schwarm and Ostendorf 2005). In the following section, we provide an overview of entity-based theories of local coherence and outline previous work on its computational treatment. Then, we introduce our entity-based representation, and define its linguistic properties. In the subsequent sections, we present our three evaluation tasks, and report the results of our experiments. Discussion of the results concludes the article. 2. Related Work Our approach is inspired by entity-based theories of local coherence, and is well-suited for developing a coherence metric in the context of a ranking-based text generation system. W</context>
<context position="82494" citStr="Schwarm and Ostendorf 2005" startWordPosition="12399" endWordPosition="12402"> grid models. Here, we move away from the coherence rating task and put the entity-grid representation further to the test by examining whether it can be usefully employed in style classification. Specifically, we embed our entity grids into a system that assesses document readability. The term describes the ease with which a document can be read and understood. The quantitative measurement of readability has attracted considerable interest and debate over the last 70 years (see Mitchell [1985] and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP technology (Schwarm and Ostendorf 2005). A number of readability formulas have been developed with the primary aim of assessing whether texts or books are suitable for students at particular grade levels or ages. Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz and Bauer 2001). Despite their widespread applicability in education and technical writing (Kincaid et al. 1981), readability formulas are often criticized for being too simplist</context>
<context position="84175" citStr="Schwarm and Ostendorf (2005)" startWordPosition="12641" endWordPosition="12644">e to combine several knowledge sources ranging from traditional reading level measures, to statistical language models, and syntactic analysis. Evaluation results show that their system outperforms two commonly used reading level measures (the Flesch-Kincaid Grade Level index and Lexile). In the following we build on their approach and examine whether the entity-grid representation introduced in this article contributes to the readability assessment task. The incorporation of coherence-based information in the measurement of text readability is, to our knowledge, novel. 6.1 Modeling We follow Schwarm and Ostendorf (2005) in treating readability assessment as a classification task. The unit of classification is a single article and the learner’s task is to predict whether it is easy or difficult to read. A variety of machine learning techniques are amenable to this problem. Because our goal was to replicate Schwarm and Ostendorf’s system as closely as possible, we followed their choice of support vector machines (SVMs) (Joachims 1998b) for our classification experiments. Our training sample therefore consisted of n documents such that (�x1,y1),---, (4,yn) xi E R&apos;,yi E {−1,+1} where xi is a feature vector for t</context>
<context position="87860" citStr="Schwarm and Ostendorf (2005)" startWordPosition="13253" endWordPosition="13256">periments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simplified articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difficult.11 Examples of these two categories are given in Table 9. 11 The Britannica corpus was also used by Schwarm and Ostendorf (2005); in addition they make use of a corpus compiled from the Weekly Reader, an educational newspaper with documents targeted at grade levels 2–5. Unfortunately, this corpus is not publicly available. 26 Barzilay and Lapata Modeling Local Coherence Features and Parameter Settings. We created two system versions: the first one used solely Schwarm and Ostendorf (2005) features;12 the second one employed a richer feature space—we added the entity-based representation proposed here to their original feature set. We will briefly describe the readability-related features used in our systems and direct t</context>
<context position="89300" citStr="Schwarm and Ostendorf (2005)" startWordPosition="13467" endWordPosition="13470"> are average sentence length and features extracted from parse trees computed using Charniak’s (2000) parser. The latter include average parse tree height, average number of NPs, average number of VPs, and average number of subordinate clauses (SBARs). We computed average sentence length by measuring the number of tokens per sentence. Their semantic features include the average number of syllables per word, and language model perplexity scores. A unigram, bigram, and trigram model was estimated for each class, and perplexity scores were used to assess their performance on test data. Following Schwarm and Ostendorf (2005) we used information gain to select words that were good class discriminants. All remaining words were replaced by their parts of speech. The vocabulary thus consisted of 300 words with high information gain and 36 Penn Treebank part-of-speech tags. The language models were estimated using maximum likelihood estimation and smoothed with Witten-Bell discounting. The language models described in this article were all built using the CMU statistical language modeling toolkit (Clarkson and Rosenfeld 1997). Our perplexity scores were six in total (2 classes x 3 language models). Finally, the Flesch</context>
<context position="91296" citStr="Schwarm and Ostendorf (2005)" startWordPosition="13770" endWordPosition="13773">transition length was ≤2 and entities were considered salient if they occurred ≥2 times. As in our previous experiments, we compared the entity-based representation against LSA. The latter is a measure of the semantic relatedness across pairs of sentences. We could not apply the HMM-based content models (Barzilay and Lee 2004) to the readability data set. The encyclopedia lemmas are written by different authors and consequently vary considerably in structure and vocabulary choice. Recall that these models are suitable for more restricted domains and texts that are more formulaic in nature. 12 Schwarm and Ostendorf (2005) define out-of-vocabulary (OOV) scores relative to the most common words in grade 2, the lowest grade level in their corpus; it was not possible to estimate OOV scores, because we did not have access to grade 2 texts. 27 Computational Linguistics Volume 34, Number 1 Table 10 The contribution of coherence-based features to the automatic readability assessment task. Diacritics ** (p &lt; .01) and * (p &lt; .05) indicate whether differences in accuracy between Schwarm and Ostendorf and all other models are significant (using a Fisher Sign test). Model Accuracy Schwarm &amp; Ostendorf 78.56 Schwarm &amp; Ostend</context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>Schwarm, Sarah E. and Mari Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 523–530, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donia Scott</author>
<author>Clarisse Sieckenius de Souza</author>
</authors>
<title>Getting the message across in RST-based text generation.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation.</booktitle>
<pages>47--73</pages>
<editor>In Robert Dale, Chris Mellish, and Michael Zock, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<marker>Scott, de Souza, 1990</marker>
<rawString>Scott, Donia and Clarisse Sieckenius de Souza. 1990. Getting the message across in RST-based text generation. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation. Academic Press, New York, pages 47–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>Towards a Computational Theory of Definite Anaphora Comprehension in English Discourse.</title>
<date>1979</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<contexts>
<context position="11273" citStr="Sidner 1979" startWordPosition="1603" endWordPosition="1604">even for fundamental concepts of Centering Theory such as “utterance,” “realization,” and “ranking,” multiple—and often contradictory—interpretations have been developed over the years, because in the original theory these concepts are not explicitly fleshed out. For instance, in some Centering papers, entities are ranked with respect to their grammatical function (Brennan, Friedman, and Pollard 1987; Walker, Iida, and Cote 1994; Grosz, Joshi, and Weinstein 1995), and in others with respect to their position in Prince’s (1981) givenness hierarchy (Strube and Hahn 1999) or their thematic role (Sidner 1979). As a result, two “instantiations” of the same theory make different predictions for the same input. Poesio et al. (2004) explore alternative specifications proposed in the literature, and demonstrate that the predictive power of the theory is highly sensitive to its parameter definitions. A common methodology for translating entity-based theories into computational models is to evaluate alternative specifications on manually annotated corpora. Some studies aim to find an instantiation of parameters that is most consistent with observable data (Strube and Hahn 1999; Karamanis et al. 2004; Poe</context>
</contexts>
<marker>Sidner, 1979</marker>
<rawString>Sidner, Candace L. 1979. Towards a Computational Theory of Definite Anaphora Comprehension in English Discourse. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>Hwee Tou Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Soon, W. M., Hwee Tou Ng, and D. C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jackson Stenner</author>
</authors>
<title>Measuring reading comprehension with the lexile framework. Presented at the California Comparability Symposium,</title>
<date>1996</date>
<location>Burlingame, CA.</location>
<contexts>
<context position="82909" citStr="Stenner 1996" startWordPosition="12465" endWordPosition="12466">rable interest and debate over the last 70 years (see Mitchell [1985] and Chall [1958] for detailed overviews) and has recently benefited from the use of NLP technology (Schwarm and Ostendorf 2005). A number of readability formulas have been developed with the primary aim of assessing whether texts or books are suitable for students at particular grade levels or ages. Many readability methods focus on simple approximations of semantic factors concerning the words used and syntactic factors concerning the length or structure of sentences (Gunning 1952; Kincaid et al. 1975; Chall and Dale 1995; Stenner 1996; Katz and Bauer 2001). Despite their widespread applicability in education and technical writing (Kincaid et al. 1981), readability formulas are often criticized for being too simplistic; they systematically ignore many important factors that affect readability such as discourse coherence and cohesion, layout and formatting, use of illustrations, the nature of the topic, the characteristics of the readers, and so forth. Schwarm and Ostendorf (2005) developed a method for assessing readability which addresses some of the shortcomings of previous approaches. By recasting readability assessment </context>
</contexts>
<marker>Stenner, 1996</marker>
<rawString>Stenner, A. Jackson. 1996. Measuring reading comprehension with the lexile framework. Presented at the California Comparability Symposium, Burlingame, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Udo Hahn</author>
</authors>
<title>Functional centering—Grounding referential coherence in information structure.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="7264" citStr="Strube and Hahn 1999" startWordPosition="1017" endWordPosition="1020">; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and Hahn 1999; Poesio et al. 2004), salience concerns how entities are realized in an utterance (e.g., whether they are they pronominalized or not). In other theories, salience is defined in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More refined accounts expand the notion of salience from a binary distinction to a scalar one; examples include Prince’s (1981) familiarity scale, and Givon’s (1987) and Ariel’s (1988) givenness-continuum. The salience status of an entity is often reflect</context>
<context position="11236" citStr="Strube and Hahn 1999" startWordPosition="1595" endWordPosition="1598">nderspecified. Poesio et al. (2004) note that even for fundamental concepts of Centering Theory such as “utterance,” “realization,” and “ranking,” multiple—and often contradictory—interpretations have been developed over the years, because in the original theory these concepts are not explicitly fleshed out. For instance, in some Centering papers, entities are ranked with respect to their grammatical function (Brennan, Friedman, and Pollard 1987; Walker, Iida, and Cote 1994; Grosz, Joshi, and Weinstein 1995), and in others with respect to their position in Prince’s (1981) givenness hierarchy (Strube and Hahn 1999) or their thematic role (Sidner 1979). As a result, two “instantiations” of the same theory make different predictions for the same input. Poesio et al. (2004) explore alternative specifications proposed in the literature, and demonstrate that the predictive power of the theory is highly sensitive to its parameter definitions. A common methodology for translating entity-based theories into computational models is to evaluate alternative specifications on manually annotated corpora. Some studies aim to find an instantiation of parameters that is most consistent with observable data (Strube and </context>
</contexts>
<marker>Strube, Hahn, 1999</marker>
<rawString>Strube, Michael and Udo Hahn. 1999. Functional centering—Grounding referential coherence in information structure. Computational Linguistics, 25(3):309–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher D Manning</author>
</authors>
<title>The leaf projection path view of parse trees: Exploring string kernels for HPSG parse selection.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>166--173</pages>
<location>Barcelona,</location>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Toutanova, Kristina, Penka Markova, and Christopher D. Manning. 2004. The leaf projection path view of parse trees: Exploring string kernels for HPSG parse selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 166–173, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley, Chichester, UK.</publisher>
<contexts>
<context position="87005" citStr="Vapnik (1998)" startWordPosition="13125" endWordPosition="13126">itect Francesco Laparelli. To make traveling through Valletta easier, Laparelli designed the city in a grid pattern with straight streets that crossed each other and ran the entire width and length of the town. Valletta was one of the first towns to be laid out in this way. vector w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi[(w ·�xi) + b] &gt; 0 Finding the optimal hyperplane is an optimization problem which can be solved efficiently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classification (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simplified articles from Britannica Elementary (214 articles in total). Although these texts are</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vapnik, Vladimir. 1998. Statistical Learning Theory. Wiley, Chichester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="31468" citStr="Vilain et al. (1995)" startWordPosition="4753" endWordPosition="4756">second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). To implement a salience-based model, we modify our feature generation procedure by computing transition probabilities for each salience group separately, and then 2 When evaluating the output of coreference algorithms, performance is typically measured using a model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the recall error by taking each equivalence class S in the gold standard and determining the number of coreference links m that would have to be added to the system’s output to place all entities in S into the same equivalence class produced by the system. Recall error then is the sum of ms divided by the number of links in the gold standard. Precision error is computed by reversing the roles of the gold standard and system output. 3 The frequency threshold is empirically determined on the development set. See Section 4.2 for further discussion. 9 Computational</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pages 45–52, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Masayo Iida</author>
<author>Sharon Cote</author>
</authors>
<title>Japanese discourse and the process of centering.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<marker>Walker, Iida, Cote, 1994</marker>
<rawString>Walker, Marilyn, Masayo Iida, and Sharon Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193–232.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Centering Theory in Discourse.</booktitle>
<editor>Walker, Marilyn, Aravind Joshi, and Ellen Prince, editors.</editor>
<publisher>Clarendon Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="45144" citStr="(1998)" startWordPosition="6838" endWordPosition="6838">to identify salient entities and the length of the transition sequence. These parameters were tuned separately for each data set on the corresponding held-out development set. Optimal saliencebased models were obtained for entities with frequency &gt;2. The optimal transition length was &lt;3.6 In our ordering experiments, we used Joachims’s (2002) SVMlight package for training and testing with all parameters set to their default values. Comparison with State-of-the-Art Methods. We compared the performance of our algorithm against two state-of-the-art models proposed by Foltz, Kintsch, and Landauer (1998) and Barzilay and Lee (2004). These models rely largely on lexical information for assessing document coherence, contrary to our models which are in essence unlexicalized. Recall from Section 3 that our approach captures local coherence by modeling patterns of entity distribution in discourse, without taking note of their lexical instantiations. In the following we briefly describe the lexicalized models we employed in our comparative study and motivate their selection. 6 The models we used in our experiments are available from http://people.csail.mit.edu/ regina/coherence/ and http://homepage</context>
<context position="48166" citStr="(1998)" startWordPosition="7296" endWordPosition="7296">space and the choice of similarity function), (b) it correlates reliably with human judgments and has been used to analyze discourse structure, and (c) it models an aspect of local coherence which is orthogonal to ours. The LSA model is lexicalized: coherence amounts to quantifying the degree of semantic similarity between sentences. In contrast, our model does not incorporate any notion of similarity: coherence is encoded in terms of transition sequences that are document-specific rather than sentence-specific. Our implementation of the LSA model followed closely Foltz, Kintsch, and Landauer (1998). We constructed vector-based representations for individual words from a lemmatized version of the North American News Corpus7 (350 million words) using a term–document matrix. We used SVD to reduce the semantic space to 100 dimensions obtaining thus a space similar to LSA. We estimated the coherence of a document using Equations (1) and (2). A ranking can be trivially inferred by comparing the 7 Our selection of this corpus was motivated by two factors: (a) the corpus is large enough to yield a reliable semantic space, and (b) it consists of news stories and is therefore similar in style, vo</context>
<context position="87005" citStr="(1998)" startWordPosition="13126" endWordPosition="13126">rancesco Laparelli. To make traveling through Valletta easier, Laparelli designed the city in a grid pattern with straight streets that crossed each other and ran the entire width and length of the town. Valletta was one of the first towns to be laid out in this way. vector w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi[(w ·�xi) + b] &gt; 0 Finding the optimal hyperplane is an optimization problem which can be solved efficiently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classification (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simplified articles from Britannica Elementary (214 articles in total). Although these texts are</context>
</contexts>
<marker>1998</marker>
<rawString>Walker, Marilyn, Aravind Joshi, and Ellen Prince, editors. 1998. Centering Theory in Discourse. Clarendon Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Owen Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>Spot: A trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>17--24</pages>
<location>Pittsburgh, PA.</location>
<marker>Walker, Rambow, Rogati, 2001</marker>
<rawString>Walker, Marilyn A., Owen Rambow, and Monica Rogati. 2001. Spot: A trainable sentence planner. In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 17–24, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sholom M Weiss</author>
<author>Casimir A Kulikowski</author>
</authors>
<date>1991</date>
<booktitle>Computer Systems that Learn: Classification and Prediction Methods from, Statistics, Neural Nets, Machine Learning, and Expert Systems.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="72423" citStr="Weiss and Kulikowski 1991" startWordPosition="10930" endWordPosition="10933">ce texts. The ratings (approximately 23 per summary) given by our subjects were averaged to provide a rating between 1 and 7 for each summary. The reliability of the collected judgments is crucial for our analysis; we therefore performed several tests to validate the quality of the annotations. First, we measured how well humans agree in their coherence assessment. We employed leave-one-out 8 See question 12 in http://duc.nist.gov/duc2003/quality.html. 9 The ratings are available from http://homepages.inf.ed.ac.uk/mlap/coherence/. 21 Computational Linguistics Volume 34, Number 1 resampling10 (Weiss and Kulikowski 1991), by correlating the data obtained from each participant with the mean coherence ratings obtained from all other participants. The inter-subject agreement was r = .768 (p &lt; .01.) Second, we examined the effect of different types of summaries (human- vs. machine-generated.) An ANOVA revealed a reliable effect of summary type: F(1;15) = 20.38, p &lt; .01 indicating that human summaries are perceived as significantly more coherent than system-generated ones. Finally, we also compared the elicited ratings against the DUC evaluations using correlation analysis. The human judgments were discretized to </context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>Weiss, Sholom M. and Casimir A. Kulikowski. 1991. Computer Systems that Learn: Classification and Prediction Methods from, Statistics, Neural Nets, Machine Learning, and Expert Systems. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2000</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan</publisher>
<location>Kaufman, San Mateo, CA.</location>
<contexts>
<context position="73108" citStr="Witten and Frank 2000" startWordPosition="11033" endWordPosition="11036">he mean coherence ratings obtained from all other participants. The inter-subject agreement was r = .768 (p &lt; .01.) Second, we examined the effect of different types of summaries (human- vs. machine-generated.) An ANOVA revealed a reliable effect of summary type: F(1;15) = 20.38, p &lt; .01 indicating that human summaries are perceived as significantly more coherent than system-generated ones. Finally, we also compared the elicited ratings against the DUC evaluations using correlation analysis. The human judgments were discretized to two classes (i.e., 0 or 1) using entropy-based discretization (Witten and Frank 2000). We found a moderate correlation between the human ratings and DUC coherence violations (r = .41, p &lt; .01). This is expected given that DUC evaluators were using a different scale and and were not explicitly assessing summary coherence. The summaries used in our rating elicitation study form the basis of a corpus used for the development of our entity-based coherence models. To increase the size of our training and test sets, we augmented the materials used in the elicitation study with additional DUC summaries generated by humans for the same input sets. We assumed that these summaries were </context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Witten, Ian H. and Eibe Frank. 2000. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufman, San Mateo, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>