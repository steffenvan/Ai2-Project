<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.068867">
<title confidence="0.942278">
The CALO Meeting Assistant
</title>
<author confidence="0.903473">
L. Lynn Voss Patrick Ehlen
</author>
<affiliation confidence="0.8846915">
Engineering and Systems Division CSLI
SRI International Stanford University
</affiliation>
<address confidence="0.878887">
Menlo Park, CA 94025 Stanford, CA 94305
</address>
<email confidence="0.992818">
loren.voss@sri.com ehlen@stanford.edu
</email>
<bodyText confidence="0.695389">
and
The DARPA† CALO Meeting Assistant Project Team*
</bodyText>
<sectionHeader confidence="0.948041" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999199857142857">
The CALO Meeting Assistant is an inte-
grated, multimodal meeting assistant tech-
nology that captures speech, gestures, and
multimodal data from multiparty interactions
during meetings, and uses machine learning
and robust discourse processing to provide a
rich, browsable record of a meeting.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9985039375">
Technologies that assist in making meetings more
productive have a long history. The latest chapter in
that history involves projects that integrate recent
advances in speech, natural language understanding,
vision, and multimodal interaction technologies in
an effort to produce tools that can perceive what
happens at a meeting, extract salient events and in-
teractions, and produce a record of the meeting that
people can later consult or analyze.
Research projects such as the ICSI Meeting Pro-
ject (Janin et al. 2004) have sought to produce auto-
mated and segmented transcripts from natural, multi-
party speech as it occurs in meetings. Others, like
the ISL Smart Meeting Room Task (Waibel et al.
2003), and the M4 and AMI projects (Nijholt, op
den Akker, &amp; Heylen 2005), employ instrumented
</bodyText>
<footnote confidence="0.877819777777778">
† This material is based upon work supported by the Defense
Advanced Research Projects Agency (DARPA) under Contract
No. NBCHD030010. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of DARPA or
the Department of Interior-National Business Center.
* The DARPA CALO MA Project is a collaborative effort
among researchers at Adapx, CMU, Georgia Tech, MIT, SRI,
Stanford University, UC Berkeley, and UC Santa Cruz.
</footnote>
<bodyText confidence="0.999888710526316">
meeting rooms to collect multiple streams of behav-
ior data and analyze the interactions of meeting par-
ticipants to produce a rich and flexible record of
their meeting activities, while also providing a sup-
portive environment for collaboration.
The CALO Meeting Assistant is similar to the lat-
ter in that it collects multiple streams of information
about the behaviors of people in meetings, and as-
similates speech, movement, and note-taking behav-
ior to create a rich representation of the meeting that
can be analyzed and reviewed at many levels. How-
ever, a primary aim of the CALO Meeting Assistant
is to integrate its observations with those of a larger
system of agents, which can assess the meeting data
it collects in the context of the ongoing projects and
workflow in the work lives of each of the meeting
participants. Thus, our meeting assistant aims to
reach beyond an intelligent room that understands
only the activities of people in meetings, and at-
tempts to understand their overarching concerns and
interpret their behaviors from the perspective of
what their meetings mean to them.
That overarching system of agents is being devel-
oped under the DARPA CALO (Cognitive Assistant
that Learns and Organizes) Program, which seeks to
produce machine learning technology in the form of
personalized agents that support high-level knowl-
edge workers in carrying out their professional ac-
tivities. The CALO system handles a broad range of
interrelated decision-making tasks that are tradition-
ally resistant to automation; doing so partly by inter-
acting with, being advised by, and learning from its
users. The CALO system can take initiative on com-
pleting routine tasks, and on assisting when the un-
expected happens.
CALO is designed from the ground up as a cogni-
tive system. Whereas conventional, hand-coded soft-
ware excels at a narrow set of capabilities in a
</bodyText>
<page confidence="0.994504">
17
</page>
<author confidence="0.2365">
NAACL HLT Demonstration Program, pages 17–18,
</author>
<affiliation confidence="0.731459">
Rochester, New York, USA, April 2007. c�2007 Association for Computational Linguistics
</affiliation>
<bodyText confidence="0.999749241379311">
particular domain, cognitive systems maintain ex-
plicit, declarative models of their capabilities, ongo-
ing activities, and operating environments. These
models enable CALO to extend and improve its ca-
pabilities through learning and adaptation. Cognitive
systems are better equipped to cope with unexpected
developments, learn to improve over time, and adapt
to the contexts and requirements of different situa-
tions. CALO also uses natural interfaces that enable
simple, effective interactions with humans and other
cognitive systems.
The CALO Meeting Assistance Project is devel-
oping capabilities to enable CALO to participate in
group discussions and meetings. Unlike instru-
mented “intelligent room” meeting projects, this sys-
tem is designed for users in an office environment
with access to the Internet, a laptop, and some small,
off-the-shelf peripheral devices (such as headsets,
webcams, and digital writing devices) to capture
speech, gestures, and handwriting. It aims to be un-
obtrusive by leveraging cross-training, unsupervised
learning, and lightweight supervision captured from
normal user interaction (e.g., users reviewing and
editing notes, or adding detected action items to a to-
do list).
These data are transparently processed at a central
server location and redistributed, so the meeting as-
sistant interacts seamlessly with other CALO desk-
top functionalities, using a common ontology.
</bodyText>
<sectionHeader confidence="0.840933" genericHeader="method">
2 What it does
</sectionHeader>
<bodyText confidence="0.99958334375">
The CALO Meeting Assistant helps its owners by
capturing and interpreting meeting conversations
and activities and, as appropriate, retrieving relevant
information. Information gleaned from a meeting
can be incorporated in the respective owner’s CALO
knowledge stores to, for example, track commit-
ments and remember references to projects, people,
places, and dates. An archive of each meeting pro-
vides a searchable record for users, as well as a his-
tory of training data for CALO’s learning
components. Learning areas include the following:
Speech processing—Automatic transcriptions are pro-
duced from conversational speech among multiple
speakers while adapting to speaker and background
noise; recognizing prosodic cues; learning new vo-
cabulary; and constructing person, role, and topic-
specific language models.
Visual recognition—Faces, gaze direction, gestures,
and activities are detected, and detection is improved
through lightly-supervised learning and unsuper-
vised cross-training.
Discourse understanding—Dialog moves are recog-
nized, topics are segmented and grouped through
supervised and unsupervised generative models, ac-
tion items are detected, and discussions can be
threaded across documents and email.
Multimodal reinforcement—Pen, speech, and text in-
puts combine to offer natural communications.
Meeting activity—Speech and note-taking activities
combine to provide cross-training for recognizing
meeting phases, and for tracking agendas and docu-
ment usage.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="method">
3 Demo
</sectionHeader>
<bodyText confidence="0.999967625">
We demonstrate how the CALO Meeting Assistant
captures speech, pen, and other meeting data using
an ordinary laptop; produces an automated tran-
script; segments by topic; and performs shallow dis-
course understanding to produce a list of probable
action items arising from a single, pre-recorded
meeting. We then demonstrate a Meeting Rapporteur
that provides a meeting summary and allows partici-
pants to review and organize the meeting transcript,
audio, notes, action items, and topics—all while
providing actions in a feedback loop that supports
the meeting assistant’s semi-supervised learning
process. Finally, we discuss the potential and current
development of real-time capabilities that allow us-
ers to interact with the meeting assistant during an
ongoing meeting.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997445625">
Janin, A., Ang, J., Bhagat, S., Dhillon, R., Edwards, J.,
Marcias-Guarasa, J., Morgan, N., Peskin, B., Shriberg,
E., Stolcke, A., Wooters, C., and Wrede, B. 2004. The
ICSI meeting project: Resources and research. In Pro-
ceedings of the 2004 IEEE International Conference
on Acoustics, Speech, and Signal Processing (ICASSP
‘04) Meeting Recognition Workshop (NIST RT-04).
Nijholt, A., op den Akker, R., and Heylen, D. 2005. Meet-
ings and meeting modeling in smart environments. AI
&amp; Society, 20(2):202-220.
Waibel, A., Schultz, T., Bett, M., Denecke, M., Malkin,
R., Rogina, I., Stiefelhagen, R., and Yang, J. 2003.
SMaRT: The smart meeting room task at ISL. In Pro-
ceedings of the 2003 IEEE International Conference
on Acoustics, Speech, and Signal Processing (ICASSP
&apos;03), pp 752-755.
</reference>
<page confidence="0.99929">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.259706">
<title confidence="0.999775">The CALO Meeting Assistant</title>
<author confidence="0.998816">L Lynn Voss Patrick Ehlen</author>
<affiliation confidence="0.9250995">Engineering and Systems Division CSLI SRI International Stanford University</affiliation>
<address confidence="0.999945">Menlo Park, CA 94025 Stanford, CA 94305</address>
<email confidence="0.989437">loren.voss@sri.comehlen@stanford.edu</email>
<author confidence="0.485919">CALO Meeting Assistant Project Team</author>
<abstract confidence="0.999057875">The CALO Meeting Assistant is an integrated, multimodal meeting assistant technology that captures speech, gestures, and multimodal data from multiparty interactions during meetings, and uses machine learning and robust discourse processing to provide a rich, browsable record of a meeting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>J Ang</author>
<author>S Bhagat</author>
<author>R Dhillon</author>
<author>J Edwards</author>
<author>J Marcias-Guarasa</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
<author>B Wrede</author>
</authors>
<title>The ICSI meeting project: Resources and research.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ‘04) Meeting Recognition Workshop (NIST RT-04).</booktitle>
<contexts>
<context position="1095" citStr="Janin et al. 2004" startWordPosition="157" endWordPosition="160"> and robust discourse processing to provide a rich, browsable record of a meeting. 1 Introduction Technologies that assist in making meetings more productive have a long history. The latest chapter in that history involves projects that integrate recent advances in speech, natural language understanding, vision, and multimodal interaction technologies in an effort to produce tools that can perceive what happens at a meeting, extract salient events and interactions, and produce a record of the meeting that people can later consult or analyze. Research projects such as the ICSI Meeting Project (Janin et al. 2004) have sought to produce automated and segmented transcripts from natural, multiparty speech as it occurs in meetings. Others, like the ISL Smart Meeting Room Task (Waibel et al. 2003), and the M4 and AMI projects (Nijholt, op den Akker, &amp; Heylen 2005), employ instrumented † This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-Na</context>
</contexts>
<marker>Janin, Ang, Bhagat, Dhillon, Edwards, Marcias-Guarasa, Morgan, Peskin, Shriberg, Stolcke, Wooters, Wrede, 2004</marker>
<rawString>Janin, A., Ang, J., Bhagat, S., Dhillon, R., Edwards, J., Marcias-Guarasa, J., Morgan, N., Peskin, B., Shriberg, E., Stolcke, A., Wooters, C., and Wrede, B. 2004. The ICSI meeting project: Resources and research. In Proceedings of the 2004 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ‘04) Meeting Recognition Workshop (NIST RT-04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nijholt</author>
<author>op den Akker</author>
<author>R</author>
<author>D Heylen</author>
</authors>
<title>Meetings and meeting modeling in smart environments.</title>
<date>2005</date>
<pages>20--2</pages>
<publisher>AI &amp; Society,</publisher>
<marker>Nijholt, den Akker, R, Heylen, 2005</marker>
<rawString>Nijholt, A., op den Akker, R., and Heylen, D. 2005. Meetings and meeting modeling in smart environments. AI &amp; Society, 20(2):202-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>T Schultz</author>
<author>M Bett</author>
<author>M Denecke</author>
<author>R Malkin</author>
<author>I Rogina</author>
<author>R Stiefelhagen</author>
<author>J Yang</author>
</authors>
<title>SMaRT: The smart meeting room task at ISL.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP &apos;03),</booktitle>
<pages>752--755</pages>
<contexts>
<context position="1278" citStr="Waibel et al. 2003" startWordPosition="188" endWordPosition="191"> latest chapter in that history involves projects that integrate recent advances in speech, natural language understanding, vision, and multimodal interaction technologies in an effort to produce tools that can perceive what happens at a meeting, extract salient events and interactions, and produce a record of the meeting that people can later consult or analyze. Research projects such as the ICSI Meeting Project (Janin et al. 2004) have sought to produce automated and segmented transcripts from natural, multiparty speech as it occurs in meetings. Others, like the ISL Smart Meeting Room Task (Waibel et al. 2003), and the M4 and AMI projects (Nijholt, op den Akker, &amp; Heylen 2005), employ instrumented † This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-National Business Center. * The DARPA CALO MA Project is a collaborative effort among researchers at Adapx, CMU, Georgia Tech, MIT, SRI, Stanford University, UC Berkeley, and UC Santa C</context>
</contexts>
<marker>Waibel, Schultz, Bett, Denecke, Malkin, Rogina, Stiefelhagen, Yang, 2003</marker>
<rawString>Waibel, A., Schultz, T., Bett, M., Denecke, M., Malkin, R., Rogina, I., Stiefelhagen, R., and Yang, J. 2003. SMaRT: The smart meeting room task at ISL. In Proceedings of the 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP &apos;03), pp 752-755.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>