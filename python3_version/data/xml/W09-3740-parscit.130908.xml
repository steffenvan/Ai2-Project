<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.130230">
<title confidence="0.958307">
A novel approach to mapping
FrameNet lexical units to WordNet synsets
</title>
<author confidence="0.978283">
Sara Tonelli, Emanuele Pianta
</author>
<sectionHeader confidence="0.739477" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.975506333333333">
In this paper we present a novel approach to mapping FrameNet
lexical units to WordNet synsets in order to automatically enrich the
lexical unit set of a given frame. While the mapping approaches pro-
posed in the past mainly rely on the semantic similarity between lexical
units in a frame and lemmas in a synset, we exploit the definition of
the lexical entries in FrameNet and the WordNet glosses to find the
best candidate synset(s) for the mapping. Evaluation results are also
reported and discussed.
1 FrameNet and the existing mapping approaches
The FrameNet database [1] is a lexical resource of English describing some
prototypical situations, the frames, and the frame-evoking words or expres-
sions associated with them, the lexical units (LU). Every frame corresponds
to a scenario involving a set of participants, the frame elements, that are typ-
ically the syntactic dependents of the lexical units. The FrameNet resource
is corpus-based, i.e. every lexical unit should be instantiated by at least
one example sentence, even if at the moment the definition and annotation
step is still incomplete for several LUs. FrameNet has proved to be useful
in a number of NLP tasks, from textual entailment to question answering,
but its coverage is still a major problem. In order to expand the resource,
it would be a good solution to acquire lexical knowledge encoded in other
existing resources and import it into the FrameNet database. WordNet [4],
for instance, covers the majority of nouns, verbs, adjectives and adverbs in
the English language, organized in synonym sets called synsets. Mapping
FrameNet LUs to WordNet synsets would automatically increase the num-
ber of LUs per frame by importing all synonyms from the mapped synset,
and would allow to exploit the semantic and lexical relations in WordNet to
enrich the information encoded in FrameNet.
</bodyText>
<page confidence="0.977744">
342
</page>
<bodyText confidence="0.981446941176471">
Proceedings of the 8th International Conference on Computational Semantics, pages 342–345,
Tilburg, January 2009. c�2009 International Conference on Computational Semantics
Several experiments have been carried out in this direction. Johansson and
Nugues [5] created a feature representation for every WordNet lemma and
used it to train an SVM classifier for each frame that tells whether a lemma
belongs to the frame or not. Crespo and Buitelaar [3] carried out an au-
tomatic mapping of medical-oriented frames to WordNet synsets, trying to
select synsets attached to a LU that were statistically significant in a given
reference corpus. De Cao et al. [2] proposed a method to detect the set
of suitable WordNet senses able to evoke the same frame by exploiting the
hypernym hierarchies that capture the largest number of LUs in the frame.
For all above mentioned approaches, a real evaluation on randomly selected
frames is missing, and accuracy was mainly computed over the new lexical
units obtained for a frame, not on a gold standard where one or more synsets
are assigned to every lexical unit in a frame. Besides, it seems that the most
common approach to carry out the mapping relies on some similarity mea-
sures that perform better on richer sets of lexical units.
</bodyText>
<sectionHeader confidence="0.683311" genericHeader="method">
2 The mapping algorithm
</sectionHeader>
<subsectionHeader confidence="0.966307">
2.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999976611111111">
We propose a mapping algorithm that is independent of the number of LUs
in a frame and from the example sentences available. In fact, we believe
that under real-usage conditions, the automatic expansion of LUs is typi-
cally required for frames with a smaller LU set, especially for those with
only one element. In the FrameNet database (v. 1.3), 33 frames out of 720
are described only by one lexical unit, and 63 are described by two. Further-
more, almost 3000 lexical units are characterized only by the lexicographic
definition and are not provided with example sentences. For this reason, we
suggest an alternative approach that makes use of usually unexploited infor-
mation collected in the FrameNet database, namely the definition associated
with every lexical unit.
Since both WordNet glosses and FrameNet definitions are manually writ-
ten by lexicographers, they usually show a high degree of similarity, and
sometimes are even identical. For example, the definition of thicken in the
Change of consistency frame is “become thick or thicker”, which is identical
to the WordNet gloss of synset n. v#00300319. The thicken lemma occurs
in three WordNet synsets, and in each of them it is the only lemma available,
so no synonymy information could be exploited for the sense disambiguation.
</bodyText>
<page confidence="0.998065">
343
</page>
<subsectionHeader confidence="0.985679">
2.2 The algorithm
</subsectionHeader>
<bodyText confidence="0.999884526315789">
We tried to devise a simple method to map a FrameNet Lexical Unit (LU)
into one or more WordNet synsets. Given a LU L from a frame F, we first
find the set of all synsets containing L (L candidate set, LCandSet). If LCan-
dSet contains only one synset, this is assigned to L. Otherwise, we look for
the synsets in LCandSet whose WN gloss has the highest similarity with the
FrameNet definition of L. We tried two baseline similarity algorithms based
respectively on stem overlap and on a modified version of the Levenshtein
algorithm taking stems as comparison unit instead of characters. Stem over-
lap turned out to perform definitely better than Levehnstein. Then we tried
to improve on simple stem overlap baseline by considering also the other
LUs in F. To this extent, we calculate the set of all synsets linked to any
LU in F (FCandSet). This is exploited in two ways. First, we boost the
similarity score of the synsets in LCandSet with the largest number of links
to other LUs in F (according to FCandSet). Secondly we assign to F the
most common WordNet Domain in FCandSet, and then boost the similarity
score of LCandSet synsets belonging to the most frequent WordNet-Domain
in F. We discard any candidate synset with a similarity score below a MIN
threshold; on the other side, we accept more than one candidate synset if
they have a similarity score higher than a MAX threshold.
</bodyText>
<sectionHeader confidence="0.994399" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999525142857143">
We created a gold standard by manually mapping 380 LUs belonging to
as many frames to the corresponding WordNet synsets. Then, we divided
our dataset into a development set of 100 LUs and a testset of 280 LUs.
We tested the Levenshtein algorithm and the Stem Overlap algorithm (SO),
then we evaluated the improvement in performance of the latter taking into
account information about the most frequent domain (D) and the most
frequent synsets (Syn). Results are reported in Table 1.
</bodyText>
<tableCaption confidence="0.999258">
Table 1: Mapping evaluation
</tableCaption>
<table confidence="0.9999366">
Precision Recall F-measure
Levenshtein 0.50 0.49 0.49
Stem Overlap (SO) 0.66 0.56 0.61
SO+Domain (D) 0.66 0.57 0.61
SO+D+Syn 0.71 0.62 0.66
</table>
<page confidence="0.998078">
344
</page>
<bodyText confidence="0.9999375">
We carried out several tests to set the MIN and MAX threshold in order
to get the best F-measure, reported in Table 1. As for precision, the best
performance obtained with SO+D+Syn and a stricter MIN/MAX threshold
scored 0.78 (recall 0.36, f-measure 0.49).
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999986">
We proposed a new method to map FrameNet LUs to WordNet synsets
by computing a similarity measure between LU definitions and WordNet
glosses. To our knowledge, this is the only approach to the task based on
this kind of similarity. The only comparable evaluation available is reported
in [5], and shows that our results are promising. De Cao at al. [2] reported a
better performance, particularly for recall, but evaluation of their mapping
algorithm relied on a gold standard of 4 selected frames having at least 10
LUs and a given number of corpus instantiations.
In the future, we plan to improve the algorithm by shallow parsing the
LU definitions and the WordNet glosses. Besides, we will exploit informa-
tion extracted from the WordNet hierarchy. We also want to evaluate the
effectiveness of the approach focusing on the new LUs to be included in the
existing frames.
</bodyText>
<sectionHeader confidence="0.99912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999884307692308">
[1] Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The Berkeley
FrameNet Project. In Proceedings of the 36th ACL Meeting and 17th
ICCL Conference. Morgan Kaufmann, 1998.
[2] Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili.
Combining Word Sense and Usage for modeling Frame Semantics. In
Proceedings of STEP 2008, Venice, Italy, 2008.
[3] Mario Crespo and Paul Buitelaar. Domain-specific English-to-Spanish
Translation of FrameNet. In Proc. of LREC 2008, Marrakech, 2008.
[4] Christiane Fellbaum, editor. WordNet: An Electronic Lexical Database.
MIT Press, 1998.
[5] R. Johansson and P. Nugues. Using WordNet to extend FrameNet cov-
erage. In Proc. of the Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, at NODALIDA, Tartu, 2007.
</reference>
<page confidence="0.999139">
345
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.210247">
<title confidence="0.9908475">A novel approach to FrameNet lexical units to WordNet synsets</title>
<author confidence="0.997985">Sara Tonelli</author>
<author confidence="0.997985">Emanuele Pianta</author>
<abstract confidence="0.990622191666667">In this paper we present a novel approach to mapping FrameNet lexical units to WordNet synsets in order to automatically enrich the lexical unit set of a given frame. While the mapping approaches proposed in the past mainly rely on the semantic similarity between lexical units in a frame and lemmas in a synset, we exploit the definition of the lexical entries in FrameNet and the WordNet glosses to find the best candidate synset(s) for the mapping. Evaluation results are also reported and discussed. 1 FrameNet and the existing mapping approaches The FrameNet database [1] is a lexical resource of English describing some situations, the and the frame-evoking words or expresassociated with them, the units Every frame corresponds a scenario involving a set of participants, the that are typically the syntactic dependents of the lexical units. The FrameNet resource is corpus-based, i.e. every lexical unit should be instantiated by at least one example sentence, even if at the moment the definition and annotation step is still incomplete for several LUs. FrameNet has proved to be useful in a number of NLP tasks, from textual entailment to question answering, but its coverage is still a major problem. In order to expand the resource, it would be a good solution to acquire lexical knowledge encoded in other existing resources and import it into the FrameNet database. WordNet [4], for instance, covers the majority of nouns, verbs, adjectives and adverbs in English language, organized in synonym sets called Mapping FrameNet LUs to WordNet synsets would automatically increase the number of LUs per frame by importing all synonyms from the mapped synset, and would allow to exploit the semantic and lexical relations in WordNet to enrich the information encoded in FrameNet. 342 of the 8th International Conference on Computational pages 342–345, January 2009. International Conference on Computational Semantics Several experiments have been carried out in this direction. Johansson and Nugues [5] created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. Crespo and Buitelaar [3] carried out an automatic mapping of medical-oriented frames to WordNet synsets, trying to select synsets attached to a LU that were statistically significant in a given reference corpus. De Cao et al. [2] proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame. For all above mentioned approaches, a real evaluation on randomly selected frames is missing, and accuracy was mainly computed over the new lexical units obtained for a frame, not on a gold standard where one or more synsets are assigned to every lexical unit in a frame. Besides, it seems that the most common approach to carry out the mapping relies on some similarity measures that perform better on richer sets of lexical units. 2 The mapping algorithm 2.1 Motivation We propose a mapping algorithm that is independent of the number of LUs in a frame and from the example sentences available. In fact, we believe that under real-usage conditions, the automatic expansion of LUs is typically required for frames with a smaller LU set, especially for those with only one element. In the FrameNet database (v. 1.3), 33 frames out of 720 are described only by one lexical unit, and 63 are described by two. Furthermore, almost 3000 lexical units are characterized only by the lexicographic definition and are not provided with example sentences. For this reason, we suggest an alternative approach that makes use of usually unexploited inforcollected in the FrameNet database, namely the with every lexical unit. Since both WordNet glosses and FrameNet definitions are manually written by lexicographers, they usually show a high degree of similarity, and are even identical. For example, the definition of the of consistency is thick or which is identical the WordNet gloss of synset n. v#00300319. The occurs in three WordNet synsets, and in each of them it is the only lemma available, so no synonymy information could be exploited for the sense disambiguation. 343 2.2 The algorithm We tried to devise a simple method to map a FrameNet Lexical Unit (LU) one or more WordNet synsets. Given a LU a frame we first the set of all synsets containing candidate set, If LCanonly one synset, this is assigned to Otherwise, we look for synsets in WN gloss has the highest similarity with the definition of We tried two baseline similarity algorithms based respectively on stem overlap and on a modified version of the Levenshtein algorithm taking stems as comparison unit instead of characters. Stem overlap turned out to perform definitely better than Levehnstein. Then we tried to improve on simple stem overlap baseline by considering also the other in To this extent, we calculate the set of all synsets linked to any in This is exploited in two ways. First, we boost the score of the synsets in the largest number of links other LUs in to Secondly we assign to common Domain and then boost the similarity of belonging to the most frequent We discard any candidate synset with a similarity score below a MIN threshold; on the other side, we accept more than one candidate synset if they have a similarity score higher than a MAX threshold. 3 Evaluation We created a gold standard by manually mapping 380 LUs belonging to as many frames to the corresponding WordNet synsets. Then, we divided our dataset into a development set of 100 LUs and a testset of 280 LUs. We tested the Levenshtein algorithm and the Stem Overlap algorithm (SO), then we evaluated the improvement in performance of the latter taking into account information about the most frequent domain (D) and the most frequent synsets (Syn). Results are reported in Table 1. Table 1: Mapping evaluation Precision Recall F-measure Levenshtein 0.50 0.49 0.49 Stem Overlap (SO) 0.66 0.56 0.61 SO+Domain (D) 0.66 0.57 0.61 SO+D+Syn 0.71 0.62 0.66 344 We carried out several tests to set the MIN and MAX threshold in order to get the best F-measure, reported in Table 1. As for precision, the best performance obtained with SO+D+Syn and a stricter MIN/MAX threshold scored 0.78 (recall 0.36, f-measure 0.49). 4 Conclusions We proposed a new method to map FrameNet LUs to WordNet synsets by computing a similarity measure between LU definitions and WordNet glosses. To our knowledge, this is the only approach to the task based on this kind of similarity. The only comparable evaluation available is reported in [5], and shows that our results are promising. De Cao at al. [2] reported a better performance, particularly for recall, but evaluation of their mapping algorithm relied on a gold standard of 4 selected frames having at least 10 LUs and a given number of corpus instantiations. In the future, we plan to improve the algorithm by shallow parsing the LU definitions and the WordNet glosses. Besides, we will exploit information extracted from the WordNet hierarchy. We also want to evaluate the effectiveness of the approach focusing on the new LUs to be included in the existing frames.</abstract>
<note confidence="0.9458936">References [1] Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The Berkeley Project. In of the 36th ACL Meeting and 17th Morgan Kaufmann, 1998. [2] Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili. Combining Word Sense and Usage for modeling Frame Semantics. In of STEP Venice, Italy, 2008. [3] Mario Crespo and Paul Buitelaar. Domain-specific English-to-Spanish of FrameNet. In of LREC Marrakech, 2008. Christiane Fellbaum, editor. An Electronic Lexical MIT Press, 1998. [5] R. Johansson and P. Nugues. Using WordNet to extend FrameNet cov- In of the Workshop on Building Frame-semantic Resources Scandinavian and Baltic Languages, at Tartu, 2007. 345</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th ACL Meeting and 17th ICCL Conference.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="685" citStr="[1]" startWordPosition="111" endWordPosition="111">, Emanuele Pianta Abstract In this paper we present a novel approach to mapping FrameNet lexical units to WordNet synsets in order to automatically enrich the lexical unit set of a given frame. While the mapping approaches proposed in the past mainly rely on the semantic similarity between lexical units in a frame and lemmas in a synset, we exploit the definition of the lexical entries in FrameNet and the WordNet glosses to find the best candidate synset(s) for the mapping. Evaluation results are also reported and discussed. 1 FrameNet and the existing mapping approaches The FrameNet database [1] is a lexical resource of English describing some prototypical situations, the frames, and the frame-evoking words or expressions associated with them, the lexical units (LU). Every frame corresponds to a scenario involving a set of participants, the frame elements, that are typically the syntactic dependents of the lexical units. The FrameNet resource is corpus-based, i.e. every lexical unit should be instantiated by at least one example sentence, even if at the moment the definition and annotation step is still incomplete for several LUs. FrameNet has proved to be useful in a number of NLP t</context>
</contexts>
<marker>[1]</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The Berkeley FrameNet Project. In Proceedings of the 36th ACL Meeting and 17th ICCL Conference. Morgan Kaufmann, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego De Cao</author>
<author>Danilo Croce</author>
<author>Marco Pennacchiotti</author>
<author>Roberto Basili</author>
</authors>
<title>Combining Word Sense and Usage for modeling Frame Semantics.</title>
<date>2008</date>
<booktitle>In Proceedings of STEP 2008,</booktitle>
<location>Venice, Italy,</location>
<contexts>
<context position="2629" citStr="[2]" startWordPosition="418" endWordPosition="418">omputational Semantics, pages 342–345, Tilburg, January 2009. c�2009 International Conference on Computational Semantics Several experiments have been carried out in this direction. Johansson and Nugues [5] created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. Crespo and Buitelaar [3] carried out an automatic mapping of medical-oriented frames to WordNet synsets, trying to select synsets attached to a LU that were statistically significant in a given reference corpus. De Cao et al. [2] proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame. For all above mentioned approaches, a real evaluation on randomly selected frames is missing, and accuracy was mainly computed over the new lexical units obtained for a frame, not on a gold standard where one or more synsets are assigned to every lexical unit in a frame. Besides, it seems that the most common approach to carry out the mapping relies on some similarity measures that perform better on richer sets </context>
</contexts>
<marker>[2]</marker>
<rawString>Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili. Combining Word Sense and Usage for modeling Frame Semantics. In Proceedings of STEP 2008, Venice, Italy, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Crespo</author>
<author>Paul Buitelaar</author>
</authors>
<title>Domain-specific English-to-Spanish Translation of FrameNet.</title>
<date>2008</date>
<booktitle>In Proc. of LREC</booktitle>
<location>Marrakech,</location>
<contexts>
<context position="2424" citStr="[3]" startWordPosition="384" endWordPosition="384">nyms from the mapped synset, and would allow to exploit the semantic and lexical relations in WordNet to enrich the information encoded in FrameNet. 342 Proceedings of the 8th International Conference on Computational Semantics, pages 342–345, Tilburg, January 2009. c�2009 International Conference on Computational Semantics Several experiments have been carried out in this direction. Johansson and Nugues [5] created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. Crespo and Buitelaar [3] carried out an automatic mapping of medical-oriented frames to WordNet synsets, trying to select synsets attached to a LU that were statistically significant in a given reference corpus. De Cao et al. [2] proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame. For all above mentioned approaches, a real evaluation on randomly selected frames is missing, and accuracy was mainly computed over the new lexical units obtained for a frame, not on a gold standard where one o</context>
</contexts>
<marker>[3]</marker>
<rawString>Mario Crespo and Paul Buitelaar. Domain-specific English-to-Spanish Translation of FrameNet. In Proc. of LREC 2008, Marrakech, 2008.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="1560" citStr="[4]" startWordPosition="252" endWordPosition="252">are typically the syntactic dependents of the lexical units. The FrameNet resource is corpus-based, i.e. every lexical unit should be instantiated by at least one example sentence, even if at the moment the definition and annotation step is still incomplete for several LUs. FrameNet has proved to be useful in a number of NLP tasks, from textual entailment to question answering, but its coverage is still a major problem. In order to expand the resource, it would be a good solution to acquire lexical knowledge encoded in other existing resources and import it into the FrameNet database. WordNet [4], for instance, covers the majority of nouns, verbs, adjectives and adverbs in the English language, organized in synonym sets called synsets. Mapping FrameNet LUs to WordNet synsets would automatically increase the number of LUs per frame by importing all synonyms from the mapped synset, and would allow to exploit the semantic and lexical relations in WordNet to enrich the information encoded in FrameNet. 342 Proceedings of the 8th International Conference on Computational Semantics, pages 342–345, Tilburg, January 2009. c�2009 International Conference on Computational Semantics Several exper</context>
</contexts>
<marker>[4]</marker>
<rawString>Christiane Fellbaum, editor. WordNet: An Electronic Lexical Database. MIT Press, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Using WordNet to extend FrameNet coverage.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA,</booktitle>
<location>Tartu,</location>
<contexts>
<context position="2232" citStr="[5]" startWordPosition="350" endWordPosition="350">dverbs in the English language, organized in synonym sets called synsets. Mapping FrameNet LUs to WordNet synsets would automatically increase the number of LUs per frame by importing all synonyms from the mapped synset, and would allow to exploit the semantic and lexical relations in WordNet to enrich the information encoded in FrameNet. 342 Proceedings of the 8th International Conference on Computational Semantics, pages 342–345, Tilburg, January 2009. c�2009 International Conference on Computational Semantics Several experiments have been carried out in this direction. Johansson and Nugues [5] created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. Crespo and Buitelaar [3] carried out an automatic mapping of medical-oriented frames to WordNet synsets, trying to select synsets attached to a LU that were statistically significant in a given reference corpus. De Cao et al. [2] proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame. For all above ment</context>
</contexts>
<marker>[5]</marker>
<rawString>R. Johansson and P. Nugues. Using WordNet to extend FrameNet coverage. In Proc. of the Workshop on Building Frame-semantic Resources for Scandinavian and Baltic Languages, at NODALIDA, Tartu, 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>