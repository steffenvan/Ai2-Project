<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.997461">
A Discriminative Global Training Algorithm for Statistical MT
</title>
<author confidence="0.903098">
Christoph Tillmann Tong Zhang
</author>
<affiliation confidence="0.5615">
IBM T.J. Watson Research Center Yahoo! Research
</affiliation>
<address confidence="0.505114">
Yorktown Heights, N.Y. 10598 New York City, N.Y. 10011
</address>
<email confidence="0.982614">
ctill@us.ibm.com tzhang@yahoo-inc.com
</email>
<sectionHeader confidence="0.99344" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995980625">
This paper presents a novel training al-
gorithm for a linearly-scored block se-
quence translation model. The key com-
ponent is a new procedure to directly op-
timize the global scoring function used by
a SMT decoder. No translation, language,
or distortion model probabilities are used
as in earlier work on SMT. Therefore
our method, which employs less domain
specific knowledge, is both simpler and
more extensible than previous approaches.
Moreover, the training procedure treats the
decoder as a black-box, and thus can be
used to optimize any decoding scheme.
The training algorithm is evaluated on a
standard Arabic-English translation task.
</bodyText>
<sectionHeader confidence="0.984344" genericHeader="introduction">
1 Introduction
</sectionHeader>
<figureCaption confidence="0.796711333333333">
Figure 1: An Arabic-English block translation ex-
ample, where the Arabic words are romanized.
The following orientation sequence is generated:
</figureCaption>
<bodyText confidence="0.995977277777778">
.
This paper presents a view of phrase-based SMT
as a sequential process that generates block ori-
entation sequences. A block is a pair of phrases
which are translations of each other. For example,
Figure 1 shows an Arabic-English translation ex-
ample that uses four blocks. During decoding, we
view translation as a block segmentation process,
where the input sentence is segmented from left
to right and the target sentence is generated from
bottom to top, one block at a time. A monotone
block sequence is generated except for the possi-
bility to handle some local phrase re-ordering. In
this local re-ordering model (Tillmann and Zhang,
2005; Kumar and Byrne, 2005) a block with
orientation is generated relative to its predeces-
sor block . During decoding, we maximize the
score of a block orientation sequence
</bodyText>
<equation confidence="0.728956">
):
(1)
</equation>
<bodyText confidence="0.998690636363636">
where is a block, is its predecessor block,
and eft ight eutral is a three-
valued orientation component linked to the block
: a block is generated to the left or the right of
its predecessor block , where the orientation
of the predecessor block is ignored. Here,
is the number of blocks in the translation. We are
interested in learning the weight vector from the
training data. is a high-dimensional
binary feature representation of the block orienta-
tion pair . The block orientation se-
</bodyText>
<page confidence="0.964557">
721
</page>
<note confidence="0.532863">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 721–728,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999868404761905">
❱quence is generated under the restriction that the
concatenated source phrases of the blocks yield
the input sentence. In modeling a block sequence,
we emphasize adjacent block neighbors that have
right or left orientation, since in the current exper-
iments only local block swapping is handled (neu-
tral orientation is used for ’detached’ blocks as de-
scribed in (Tillmann and Zhang, 2005)).
This paper focuses on the discriminative train-
ing of the weight vector used in Eq. 1. The de-
coding process is decomposed into local decision
steps based on Eq. 1, but the model is trained in
a global setting as shown below. The advantage
of this approach is that it can easily handle tens of
millions of features, e.g. up to million features
for the experiments in this paper. Moreover, under
this view, SMT becomes quite similar to sequen-
tial natural language annotation problems such as
part-of-speech tagging and shallow parsing, and
the novel training algorithm presented in this pa-
per is actually most similar to work on training al-
gorithms presented for these task, e.g. the on-line
training algorithm presented in (McDonald et al.,
2005) and the perceptron training algorithm pre-
sented in (Collins, 2002). The current approach
does not use specialized probability features as in
(Och, 2003) in any stage during decoder parame-
ter training. Such probability features include lan-
guage model, translation or distortion probabili-
ties, which are commonly used in current SMT
approaches 1. We are able to achieve comparable
performance to (Tillmann and Zhang, 2005). The
novel algorithm differs computationally from ear-
lier work in discriminative training algorithms for
SMT (Och, 2003) as follows:
No additional development data set is neces-
sary as the weight vector is trained on bilin-
gual training data only.
The paper is structured as follows: Section 2
presents the baseline block sequence model and
the feature representation. Section 3 presents
the discriminative training algorithm that learns
</bodyText>
<footnote confidence="0.985084">
1A translation and distortion model is used in generating
the block set used in the experiments, but these translation
probabilities are not used during decoding.
</footnote>
<bodyText confidence="0.99952025">
a good global ranking function used during de-
coding. Section 4 presents results on a standard
Arabic-English translation task. Finally, some dis-
cussion and future work is presented in Section 5.
</bodyText>
<sectionHeader confidence="0.91209" genericHeader="method">
2 Block Sequence Model
</sectionHeader>
<bodyText confidence="0.992593805555555">
This paper views phrase-based SMT as a block
sequence generation process. Blocks are phrase
pairs consisting of target and source phrases and
local phrase re-ordering is handled by including
so-called block orientation. Starting point for the
block-based translation model is a block set, e.g.
about million Arabic-English phrase pairs for
the experiments in this paper. This block set is
used to decode training sentence to obtain block
orientation sequences that are used in the discrim-
inative parameter training. Nothing but the block
set and the parallel training data is used to carry
out the training. We use the block set described
in (Al-Onaizan et al., 2004), the use of a different
block set may effect translation results.
Rather than predicting local block neighbors as in
(Tillmann and Zhang, 2005) , here the model pa-
rameters are trained in a global setting. Starting
with a simple model, the training data is decoded
multiple times: the weight vector is trained to
discriminate block sequences with a high trans-
lation score against block sequences with a high
BLEU score 2. The high BLEU scoring block
sequences are obtained as follows: the regular
phrase-based decoder is modified in a way that
it uses the BLEU score as optimization criterion
(independent of any translation model). Here,
searching for the highest BLEU scoring block se-
quence is restricted to local re-ordering as is the
model-based decoding (as shown in Fig. 1). The
BLEU score is computed with respect to the sin-
gle reference translation provided by the paral-
lel training data. A block sequence with an av-
erage BLEU score of about is obtained for
each training sentence 3. The ’true’ maximum
BLEU block sequence as well as the high scoring
</bodyText>
<footnote confidence="0.981973916666667">
2High scoring block sequences may contain translation er-
rors that are quantified by a lower BLEU score.
3The training BLEU score is computed for each train-
ing sentence pair separately (treating each sentence pair as
a single-sentence corpus with a single reference) and then av-
eraged over all training sentences. Although block sequences
are found with a high BLEU score on average there is no
guarantee to find the maximum BLEU block sequence for a
given sentence pair. The target word sequence correspond-
ing to a block sequence does not have to match the refer-
ence translation, i.e. maximum BLEU scores are quite low
for some training sentences.
</footnote>
<bodyText confidence="0.994369">
No computationally expensive -best lists
are generated during training: for each input
sentence a single block sequence is generated
on each iteration over the training data.
</bodyText>
<page confidence="0.995002">
722
</page>
<bodyText confidence="0.992964578947369">
❵block sequences are represented by high dimen-
sional feature vectors using the binary features de-
fined below and the translation process is handled
as a multi-class classification problem in which
each block sequence represents a possible class.
The effect of this training procedure can be seen
in Figure 2: each decoding step on the training
data adds a high-scoring block sequence to the dis-
criminative training and decoding performance on
the training data is improved after each iteration
(along with the test data decoding performance).
A theoretical justification for the novel training
procedure is given in Section 3.
We now define the feature components for the
block bigram feature vector in Eq. 1.
Although the training algorithm can handle real-
valued features as used in (Och, 2003; Tillmann
and Zhang, 2005) the current paper intentionally
excludes them. The current feature functions are
similar to those used in common phrase-based
translation systems: for them it has been shown
that good translation performance can be achieved
4. A systematic analysis of the novel training algo-
rithm will allow us to include much more sophis-
ticated features in future experiments, i.e. POS-
based features, syntactic or hierarchical features
(Chiang, 2005). The dimensionality of the fea-
ture vector depends on the number
of binary features. For illustration purposes, the
binary features are chosen such that they yield
on the example block sequence in Fig. 1. There
are phrase-based and word-based features:
The feature is a ’unigram’ phrase-based fea-
ture capturing the identity of a block. Addi-
tional phrase-based features include block orien-
tation, target and source phrase bigram features.
Word-based features are used as well, e.g. fea-
ture captures word-to-word translation de-
</bodyText>
<footnote confidence="0.873166">
4On our test set, (Tillmann and Zhang, 2005) reports a
</footnote>
<note confidence="0.548077">
BLEU score of and (Ittycheriah and Roukos, 2005) re-
</note>
<subsectionHeader confidence="0.341557">
ports a BLEU score of .
</subsectionHeader>
<bodyText confidence="0.9999768">
pendencies similar to the use of Model probabil-
ities in (Koehn et al., 2003). Additionally, we use
distortion features involving relative source word
position and -gram features for adjacent target
words. These features correspond to the use of
a language model, but the weights for theses fea-
tures are trained on the parallel training data only.
For the most complex model, the number of fea-
tures is about million (ignoring all features that
occur only once).
</bodyText>
<sectionHeader confidence="0.973124" genericHeader="method">
3 Approximate Relevant Set Method
</sectionHeader>
<bodyText confidence="0.9883672">
Throughout the section, we let . Each
block sequence corresponds to a can-
didate translation. In the training data where target
translations are given, a BLEU score can be
calculated for each against the tar-
get translations. In this set up, our goal is to find
a weight vector such that the higher
the higher the corresponding BLEU score
should be. If we can find such a weight vector,
then block decoding by searching for the high-
est will lead to good translation with high
BLEU score.
Formally, we denote a source sentence by ,
and let be the set of possible candidate ori-
ented block sequences that the de-
coder can generate from . For example, in a
monotone decoder, the set contains block
sequences that cover the source sentence
in the same order. For a decoder with lo-
cal re-ordering, the candidate set also in-
cludes additional block sequences with re-ordered
block configurations that the decoder can effi-
ciently search. Therefore depending on the spe-
cific implementation of the decoder, the set
can be different. In general, is a subset of all
possible oriented block sequences that
are consistent with input sentence .
Given a scoring function and an input sen-
tence , we can assume that the decoder imple-
ments the following decoding rule:
(2)
Let be a set of training sentences.
Each sentence is associated with a set
of possible translation block sequences that are
searchable by the decoder. Each translation block
sequence induces a translation, which
is then assigned a BLEU score (obtained
by comparing against the target translations). The
block consists of target phrase
’violate’ and source phrase ’tnthk’
otherwise
’Lebanese’ is a word in the target
phrase of block and ’AllbnAny’
is a word in the source phrase
otherwise
</bodyText>
<page confidence="0.927157">
723
</page>
<bodyText confidence="0.982843189655172">
❻goal of the training is to find a weight vector
such that for each training sentence , the corre-
sponding decoder outputs which has
the maximum BLEU score among all
based on Eq. 2. In other words, if maximizes the
scoring function , then also maximizes the
BLEU metric.
Based on the description, a simple idea is to
learn the BLEU score for each candidate
block sequence . That is, we would like to es-
timate such that . This can be
achieved through least squares regression. It is
easy to see that if we can find a weight vector
that approximates , then the decoding-rule in
Eq. 2 automatically maximizes the BLEU score.
However, it is usually difficult to estimate
reliably based only on a linear combination of the
feature vector as in Eq. 1. We note that a good de-
coder does not necessarily employ a scoring func-
tion that approximates the BLEU score. Instead,
we only need to make sure that the top-ranked
block sequence obtained by the decoder scoring
function has a high BLEU score. To formulate
this idea, we attempt to find a decoding parame-
ter such that for each sentence in the training
data, sequences in with the highest BLEU
scores should get scores higher than those
with low BLEU scores.
Denote by a set of block sequences
in with the highest BLEU scores. Our de-
coded result should lie in this set. We call them
the “truth”. The set of the remaining sequences
is , which we shall refer to as the
“alternatives”. We look for a weight vector that
minimize the following training criterion:
where are BLEU scores, are transla-
tion scores, and . We refer
to this formulation as ’costMargin’ (cost-sensitive
margin) method: for each training sentence
the ’costMargin’ between the
’true’ block sequence set and the ’alterna-
tive’ block sequence set is maximized. Note
that due to the truth and alternative set up, we al-
ways have . This loss function gives an up-
per bound of the error we will suffer if the order of
and is wrongly predicted (that is, if we predict
instead of ). It also has the property
that if for the BLEU scores holds, then the
loss value is small (proportional to ).
A major contribution of this work is a proce-
dure to solve Eq. 3 approximately. The main dif-
ficulty is that the search space covered by
the decoder can be extremely large. It cannot be
enumerated for practical purposes. Our idea is
to replace this large space by a small subspace
which we call relevant set. The
possibility of this reduction is based on the follow-
ing theoretical result.
</bodyText>
<figure confidence="0.729889875">
Lemma 1 Let be a non-negative con-
tinuous piece-wise differentiable function of ,
and let be a local solution of Eq. 3. Let
, and
define
s.t.
Then is a local solution of
(5)
</figure>
<bodyText confidence="0.9216941">
If is a convex function of (as in our choice),
then we know that the global optimal solution re-
mains the same if the whole decoding space is
replaced by the relevant set
where is a non-negative real-valued loss func-
tion (whose specific choice is not critical for the
purposes of this paper),and is a regular-
ization parameter. In our experiments, results are
obtained using the following convex loss
.
Each subspace will be significantly
smaller than . This is because it only in-
cludes those alternatives with score close
to one of the selected truth. These are the most im-
portant alternatives that are easily confused with
the truth. Essentially the lemma says that if the
decoder works well on these difficult alternatives
(relevant points), then it works well on the whole
space. The idea is closely related to active learn-
ing in standard classification problems, where we
</bodyText>
<page confidence="0.997736">
724
</page>
<tableCaption confidence="0.999636">
Table 1: Generic Approximate Relevant Set Method
</tableCaption>
<bodyText confidence="0.984488">
selectively pick the most important samples (often
based on estimation uncertainty) for labeling in or-
der to maximize classification performance (Lewis
and Catlett, 1994). In the active learning setting,
as long as we do well on the actively selected sam-
ples, we do well on the whole sample space. In our
case, as long as we do well on the relevant set, the
decoder will perform well.
Since the relevant set depends on the decoder
parameter , and the decoder parameter is opti-
mized on the relevant set, it is necessary to es-
timate them jointly using an iterative algorithm.
The basic idea is to start with a decoding parame-
ter , and estimate the corresponding relevant set;
we then update based on the relevant set, and it-
erate this process. The procedure is outlined in Ta-
ble 1. We intentionally leave the implementation
details of the (*) step and (**) step open. More-
over, in this general algorithm, we do not have to
assume that has the form of Eq. 1.
A natural question concerning the procedure is
its convergence behavior. It can be shown that un-
der mild assumptions, if we pick in (*) an alterna-
tive for each
( ) such that
</bodyText>
<equation confidence="0.787874">
(6)
</equation>
<bodyText confidence="0.998901">
then the procedure converges to the solution of
Eq. 3. Moreover, the rate of convergence depends
only on the property of the loss function, and not
on the size of . This property is critical as
it shows that as long as Eq. 6 can be computed
efficiently, then the Approximate Relevant Set al-
gorithm is efficient. Moreover, it gives a bound
on the size of an approximate relevant set with a
certain accuracy.5
</bodyText>
<footnote confidence="0.754662">
5Due to the space limitation, we will not include a for-
</footnote>
<bodyText confidence="0.9998315625">
The approximate solution of Eq. 5 in (**) can
be implemented using stochastic gradient descent
(SGD), where we may simply update as:
The parameter is a fixed constant often re-
ferred to as learning rate. Again, convergence re-
sults can be proved for this procedure. Due to the
space limitation, we skip the formal statement as
well as the corresponding analysis.
Up to this point, we have not assumed any spe-
cific form of the decoder scoring function in our
algorithm. Now consider Eq. 1 used in our model.
We may express it as:
where . Using this
feature representation and the loss function in
Eq. 4, we obtain the following costMargin SGD
update rule for each training data point and :
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.935654586206897">
We applied the novel discriminative training ap-
proach to a standard Arabic-to-English translation
task. The training data comes from UN news
sources. Some punctuation tokenization and some
number classing are carried out on the English
and the Arabic training data. We show transla-
tion results in terms of the automatic BLEU evalu-
ation metric (Papineni et al., 2002) on the MT03
Arabic-English DARPA evaluation test set con-
sisting of sentences with Arabic words
with reference translations. In order to speed
up the parameter training the original training data
is filtered according to the test set: all the Ara-
bic substrings that occur in the test set are com-
puted and the parallel training data is filtered to
include only those training sentence pairs that con-
tain at least one out of these phrases: the resulting
pre-filtered training data contains about thou-
sand sentence pairs ( million Arabic words
and million English words). The block set is
generated using a phrase-pair selection algorithm
similar to (Koehn et al., 2003; Al-Onaizan et al.,
2004), which includes some heuristic filtering to
mal statement here. A detailed theoretical investigation of
the method will be given in a journal paper.
for each data point
initialize truth and alternative
for each decoding iteration:
for each data point
</bodyText>
<figure confidence="0.976013">
select relevant points (*)
update
update by solving Eq. 5 approximately (**)
(7)
</figure>
<page confidence="0.995228">
725
</page>
<bodyText confidence="0.999825333333333">
increase phrase translation accuracy. Blocks that
occur only once in the training data are included
as well.
</bodyText>
<subsectionHeader confidence="0.947947">
4.1 Practical Implementation Details
</subsectionHeader>
<bodyText confidence="0.976618652173913">
The training algorithm in Table 2 is adapted from
Table 1. The training is carried out by running
times over the parallel training data, each time
decoding all the training sentences
and generating a single block translation sequence
for each training sentence. The top five block se-
quences with the highest BLEU score are
computed up-front for all training sentence pairs
and are stored separately as described in Sec-
tion 2. The score-based decoding of the
training sentence pairs is carried out in parallel on
-Bit Opteron machines. Here, the monotone
decoding is much faster than the decoding with
block swapping: the monotone decoding takes less
than hours and the decoding with swapping
takes about an hour. Since the training starts with
only the parallel training data and a block set,
some initial block sequences have to be generated
in order to initialize the global model training: for
each input sentence a simple bag of blocks trans-
lation is generated. For each input interval that is
matched by some block, a single block is added
to the bag-of-blocks translation . The order
in which the blocks are generated is ignored. For
this block set only block and word identity fea-
tures are generated, i.e. features of type and
in Section 2. This step does not require the
use of a decoder. The initial block sequence train-
ing data contains only a single alternative. The
training procedure proceeds by iteratively decod-
ing the training data. After each decoding step, the
resulting translation block sequences are stored on
disc in binary format. A block sequence gener-
ated at decoding step is used in all subsequent
training steps , where . The block se-
quence training data after the-th decoding step
is given as , where the
size of the relevant alternative set is
. Although in order to achieve fast conver-
gence with a theoretical guarantee, we should use
Eq. 6 to update the relevant set, in reality, this
idea is difficult to implement because it requires
a more costly decoding step. Therefore in Table 2,
we adopt an approximation, where the relevant set
is updated by adding the decoder output at each
stage. In this way, we are able to treat the decoding
</bodyText>
<tableCaption confidence="0.913451">
Table 2: Relevant set method: = number of decoding
iterations, = number of training sentences.
</tableCaption>
<bodyText confidence="0.99959621875">
scheme as a black box. One way to approximate
Eq. 6 is to generate multiple decoding outputs
and pick the most relevant points based on Eq. 6.
Since the -best list generation is computation-
ally costly, only a single block sequence is gener-
ated for each training sentence pair, reducing the
memory requirements for the training algorithm
as well. Although we are not able to rigorously
prove fast convergence rate for this approximation,
it works well in practice, as Figure 2 shows. Theo-
retically this is because points achieving large val-
ues in Eq. 6 tend to have higher chances to become
the top-ranked decoder output as well. The SGD-
based on-line training algorithm described in Sec-
tion 3, is carried out after each decoding step to
generate the weight vector for the subsequent
decoding step. Since this training step is carried
out on a single machine, it dominates the overall
computation time. Since each iteration adds a sin-
gle relevant alternative to the set , com-
putation time increases with the number of train-
ing iterations: the initial model is trained in a few
minutes, while training the model after the -th
iteration takes up to hours for the most complex
models.
Table 3 presents experimental results in terms of
uncased BLEU 6. Two re-ordering restrictions are
tested, i.e. monotone decoding (’MON’), and lo-
cal block re-ordering where neighbor blocks can
be swapped (’SWAP’). The ’SWAP’ re-ordering
uses the same features as the monotone models
plus additional orientation-based and distortion-
</bodyText>
<footnote confidence="0.9414135">
6Translation performance in terms of cased BLEU is typ-
ically reduced by about %.
</footnote>
<table confidence="0.965633">
for each input sentence
initialize truth and alter-
native
for each decoding iteration:
train using SGD on training
data
for each input sentence
select top-scoring sequence and
update
</table>
<page confidence="0.968596">
726
</page>
<tableCaption confidence="0.999717">
Table 3: Translation results in terms of uncased
</tableCaption>
<bodyText confidence="0.5297595">
BLEU on the training data ( sentences)
and the MT03 test data (670 sentences).
</bodyText>
<table confidence="0.996027777777778">
Re-ordering Features train test
’MON’ bleu -
phrase
word
both
’SWAP’ bleu -
phrase
word
both
</table>
<bodyText confidence="0.999569470588235">
based features. Different feature sets include
word-based features, phrase-based features, and
the combination of both. For the results with
word-based features, the decoder still generates
phrase-to-phrase translations, but all the scoring
is done on the word level. Line shows a BLEU
score of for the best performing system which
uses all word-based and phrase-based features 7.
Line and line of Table 3 show the training
data averaged BLEU score obtained by searching
for the highest BLEU scoring block sequence for
each training sentence pair as described in Sec-
tion 2. Allowing local block swapping in this
search procedure yields a much improved BLEU
score of . The experimental results show
that word-based models significantly outperform
phrase-based models, the combination of word-
based and phrase-based features performs better
than those features types taken separately. Addi-
tionally, swap-based re-ordering slightly improves
performance over monotone decoding. For all
experiments, the training BLEU score remains
significantly lower than the maximum obtainable
BLEU score shown in line and line . In this re-
spect, there is significant room for improvements
in terms of feature functions and alternative set
generation. The word-based models perform sur-
prisingly well, i.e. the model in line uses only
three feature types: model features like in
Section 2, distortion features, and target language
m-gram features up to . Training speed
varies depending on the feature types used: for
the simplest model shown in line of Table 3, the
training takes about hours, for the models us-
</bodyText>
<footnote confidence="0.480583">
7With a margin of , the differences between the
results in line, line, and line are not statistically signifi-
cant, but the other result differences are.
</footnote>
<figure confidence="0.920076">
0 5 10 15 20 25 30
</figure>
<figureCaption confidence="0.9705755">
Figure 2: BLEU performance on the training set
(upper graph; averaged BLEU with single refer-
</figureCaption>
<bodyText confidence="0.924581583333333">
ence) and the test set (lower graph; BLEU with
four references) as a function of the training iter-
ation for the model corresponding to line in
Table 3.
ing word-based features shown in line and line
training takes less than days. Finally, the training
for the most complex model in line takes about
days.
Figure 2 shows the BLEU performance for the
model corresponding to line in Table 3 as a
function of the number of training iterations. By
adding top scoring alternatives in the training al-
gorithm in Table 2, the BLEU performance on the
training data improves from about for the ini-
tial model to about for the best model after
iterations. After each training iteration the test
data is decoded as well. Here, the BLEU perfor-
mance improves from for the initial model to
about for the final model (we do not include
the test data block sequences in the training). Ta-
ble 3 shows a typical learning curve for the experi-
ments in Table 3: the training BLEU score is much
higher than the test set BLEU score despite the fact
that the test set uses reference translations.
</bodyText>
<sectionHeader confidence="0.997859" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999889222222222">
The work in this paper substantially differs from
previous work in SMT based on the noisy chan-
nel approach presented in (Brown et al., 1993).
While error-driven training techniques are com-
monly used to improve the performance of phrase-
based translation systems (Chiang, 2005; Och,
2003), this paper presents a novel block sequence
translation approach to SMT that is similar to
sequential natural language annotation problems
</bodyText>
<figure confidence="0.996667666666667">
0.6
0.5
0.4
0.3
0.2
0.1
0
’SWAP.TRAINING’
’SWAP.TEST’
</figure>
<page confidence="0.981403">
727
</page>
<bodyText confidence="0.999993697674418">
such as part-of-speech tagging or shallow parsing,
both in modeling and parameter training. Unlike
earlier approaches to SMT training, which either
rely heavily on domain knowledge, or can only
handle a small number of features, this approach
treats the decoding process as a black box, and
can optimize tens millions of parameters automat-
ically, which makes it applicable to other problems
as well. The choice of our formulation is convex,
which ensures that we are able to find the global
optimum even for large scale problems. The loss
function in Eq. 4 may not be optimal, and us-
ing different choices may lead to future improve-
ments. Another important direction for perfor-
mance improvement is to design methods that bet-
ter approximate Eq. 6. Although at this stage the
system performance is not yet better than previous
approaches, good translation results are achieved
on a standard translation task. While being similar
to (Tillmann and Zhang, 2005), the current proce-
dure is more automated with comparable perfor-
mance. The latter approach requires a decompo-
sition of the decoding scheme into local decision
steps with the inherent difficulty acknowledged in
(Tillmann and Zhang, 2005). Since such limitation
is not present in the current model, improved re-
sults may be obtained in the future. A perceptron-
like algorithm that handles global features in the
context of re-ranking is also presented in (Shen et
al., 2004).
The computational requirements for the training
algorithm in Table 2 can be significantly reduced.
While the global training approach presented in
this paper is simple, after iterations or so, the
alternatives that are being added to the relevant set
differ very little from each other, slowing down
the training considerably such that the set of possi-
ble block translations might not be fully ex-
plored. As mentioned in Section 2, the current ap-
proach is still able to handle real-valued features,
e.g. the language model probability. This is im-
portant since the language model can be trained
on a much larger monolingual corpus.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
6 Acknowledgment
</sectionHeader>
<bodyText confidence="0.9999738">
This work was partially supported by the GALE
project under the DARPA contract No. HR0011-
06-2-0001. The authors would like to thank the
anonymous reviewers for their detailed criticism
on this paper.
</bodyText>
<sectionHeader confidence="0.99644" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999900176470588">
Yaser Al-Onaizan, Niyu Ge, Young-Suk Lee, Kishore
Papineni, Fei Xia, and Christoph Tillmann. 2004.
IBM Site Report. In NIST 2004 MT Workshop,
Alexandria, VA, June. IBM.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. CL, 19(2):263–311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc. of
ACL 2005), pages 263–270, Ann Arbor, Michigan,
June.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP’02, Philadelphia,PA.
A. Ittycheriah and S. Roukos. 2005. A Maximum
Entropy Word Aligner for Arabic-English MT. In
Proc. of HLT-EMNLP 06, pages 89–96, Vancouver,
British Columbia, Canada, October.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL
2003: Main Proceedings, pages 127–133, Edmon-
ton, Alberta, Canada, May 27 - June 1.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proc. ofHLT-EMNLP 05, pages 161–
168, Vancouver, British Columbia, Canada, October.
D. Lewis and J. Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Pro-
ceedings of the Eleventh International Conference
on Machine Learning, pages 148–156.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings ofACL’05, pages
91–98, Ann Arbor, Michigan, June.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL’03, pages 160–167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of machine translation. In In Proc. of
ACL’02, pages 311–318, Philadelphia, PA, July.
Libin Shen, Anoop Sarkar, and Franz-Josef Och. 2004.
Discriminative Reranking of Machine Translation.
In Proceedings of the Joint HLT and NAACL Confer-
ence (HLT 04), pages 177–184, Boston, MA, May.
Christoph Tillmann and Tong Zhang. 2005. A local-
ized prediction model for statistical machine trans-
lation. In Proceedings of ACL’05, pages 557–564,
Ann Arbor, Michigan, June.
</reference>
<page confidence="0.996819">
728
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950976">
<title confidence="0.998888">A Discriminative Global Training Algorithm for Statistical MT</title>
<author confidence="0.999306">Christoph Tillmann Tong Zhang</author>
<affiliation confidence="0.999366">IBM T.J. Watson Research Center Yahoo! Research</affiliation>
<address confidence="0.966676">Yorktown Heights, N.Y. 10598 New York City, N.Y. 10011</address>
<email confidence="0.996186">ctill@us.ibm.comtzhang@yahoo-inc.com</email>
<abstract confidence="0.999306529411765">This paper presents a novel training algorithm for a linearly-scored block sequence translation model. The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder. No translation, language, or distortion model probabilities are used as in earlier work on SMT. Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches. Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme. The training algorithm is evaluated on a standard Arabic-English translation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Niyu Ge</author>
<author>Young-Suk Lee</author>
<author>Kishore Papineni</author>
<author>Fei Xia</author>
<author>Christoph Tillmann</author>
</authors>
<title>IBM Site Report.</title>
<date>2004</date>
<booktitle>In NIST 2004 MT Workshop,</booktitle>
<publisher>IBM.</publisher>
<location>Alexandria, VA,</location>
<contexts>
<context position="5590" citStr="Al-Onaizan et al., 2004" startWordPosition="883" endWordPosition="886">k sequence generation process. Blocks are phrase pairs consisting of target and source phrases and local phrase re-ordering is handled by including so-called block orientation. Starting point for the block-based translation model is a block set, e.g. about million Arabic-English phrase pairs for the experiments in this paper. This block set is used to decode training sentence to obtain block orientation sequences that are used in the discriminative parameter training. Nothing but the block set and the parallel training data is used to carry out the training. We use the block set described in (Al-Onaizan et al., 2004), the use of a different block set may effect translation results. Rather than predicting local block neighbors as in (Tillmann and Zhang, 2005) , here the model parameters are trained in a global setting. Starting with a simple model, the training data is decoded multiple times: the weight vector is trained to discriminate block sequences with a high translation score against block sequences with a high BLEU score 2. The high BLEU scoring block sequences are obtained as follows: the regular phrase-based decoder is modified in a way that it uses the BLEU score as optimization criterion (indepe</context>
<context position="18571" citStr="Al-Onaizan et al., 2004" startWordPosition="3105" endWordPosition="3108">sentences with Arabic words with reference translations. In order to speed up the parameter training the original training data is filtered according to the test set: all the Arabic substrings that occur in the test set are computed and the parallel training data is filtered to include only those training sentence pairs that contain at least one out of these phrases: the resulting pre-filtered training data contains about thousand sentence pairs ( million Arabic words and million English words). The block set is generated using a phrase-pair selection algorithm similar to (Koehn et al., 2003; Al-Onaizan et al., 2004), which includes some heuristic filtering to mal statement here. A detailed theoretical investigation of the method will be given in a journal paper. for each data point initialize truth and alternative for each decoding iteration: for each data point select relevant points (*) update update by solving Eq. 5 approximately (**) (7) 725 increase phrase translation accuracy. Blocks that occur only once in the training data are included as well. 4.1 Practical Implementation Details The training algorithm in Table 2 is adapted from Table 1. The training is carried out by running times over the para</context>
</contexts>
<marker>Al-Onaizan, Ge, Lee, Papineni, Xia, Tillmann, 2004</marker>
<rawString>Yaser Al-Onaizan, Niyu Ge, Young-Suk Lee, Kishore Papineni, Fei Xia, and Christoph Tillmann. 2004. IBM Site Report. In NIST 2004 MT Workshop, Alexandria, VA, June. IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>CL,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="26411" citStr="Brown et al., 1993" startWordPosition="4413" endWordPosition="4416">model after iterations. After each training iteration the test data is decoded as well. Here, the BLEU performance improves from for the initial model to about for the final model (we do not include the test data block sequences in the training). Table 3 shows a typical learning curve for the experiments in Table 3: the training BLEU score is much higher than the test set BLEU score despite the fact that the test set uses reference translations. 5 Discussion and Future Work The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993). While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems (Chiang, 2005; Och, 2003), this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems 0.6 0.5 0.4 0.3 0.2 0.1 0 ’SWAP.TRAINING’ ’SWAP.TEST’ 727 such as part-of-speech tagging or shallow parsing, both in modeling and parameter training. Unlike earlier approaches to SMT training, which either rely heavily on domain knowledge, or can only handle a small number of features, this approach treats the de</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. CL, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="8733" citStr="Chiang, 2005" startWordPosition="1396" endWordPosition="1397"> feature components for the block bigram feature vector in Eq. 1. Although the training algorithm can handle realvalued features as used in (Och, 2003; Tillmann and Zhang, 2005) the current paper intentionally excludes them. The current feature functions are similar to those used in common phrase-based translation systems: for them it has been shown that good translation performance can be achieved 4. A systematic analysis of the novel training algorithm will allow us to include much more sophisticated features in future experiments, i.e. POSbased features, syntactic or hierarchical features (Chiang, 2005). The dimensionality of the feature vector depends on the number of binary features. For illustration purposes, the binary features are chosen such that they yield on the example block sequence in Fig. 1. There are phrase-based and word-based features: The feature is a ’unigram’ phrase-based feature capturing the identity of a block. Additional phrase-based features include block orientation, target and source phrase bigram features. Word-based features are used as well, e.g. feature captures word-to-word translation de4On our test set, (Tillmann and Zhang, 2005) reports a BLEU score of and (I</context>
<context position="26545" citStr="Chiang, 2005" startWordPosition="4434" endWordPosition="4435">ial model to about for the final model (we do not include the test data block sequences in the training). Table 3 shows a typical learning curve for the experiments in Table 3: the training BLEU score is much higher than the test set BLEU score despite the fact that the test set uses reference translations. 5 Discussion and Future Work The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993). While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems (Chiang, 2005; Och, 2003), this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems 0.6 0.5 0.4 0.3 0.2 0.1 0 ’SWAP.TRAINING’ ’SWAP.TEST’ 727 such as part-of-speech tagging or shallow parsing, both in modeling and parameter training. Unlike earlier approaches to SMT training, which either rely heavily on domain knowledge, or can only handle a small number of features, this approach treats the decoding process as a black box, and can optimize tens millions of parameters automatically, which makes it applicable to other problems</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of ACL 2005), pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP’02,</booktitle>
<location>Philadelphia,PA.</location>
<contexts>
<context position="3751" citStr="Collins, 2002" startWordPosition="599" endWordPosition="600">al setting as shown below. The advantage of this approach is that it can easily handle tens of millions of features, e.g. up to million features for the experiments in this paper. Moreover, under this view, SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing, and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task, e.g. the on-line training algorithm presented in (McDonald et al., 2005) and the perceptron training algorithm presented in (Collins, 2002). The current approach does not use specialized probability features as in (Och, 2003) in any stage during decoder parameter training. Such probability features include language model, translation or distortion probabilities, which are commonly used in current SMT approaches 1. We are able to achieve comparable performance to (Tillmann and Zhang, 2005). The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT (Och, 2003) as follows: No additional development data set is necessary as the weight vector is trained on bilingual training data only.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP’02, Philadelphia,PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<title>A Maximum Entropy Word Aligner for Arabic-English MT.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP 06,</booktitle>
<pages>89--96</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="9361" citStr="Ittycheriah and Roukos, 2005" startWordPosition="1493" endWordPosition="1496">). The dimensionality of the feature vector depends on the number of binary features. For illustration purposes, the binary features are chosen such that they yield on the example block sequence in Fig. 1. There are phrase-based and word-based features: The feature is a ’unigram’ phrase-based feature capturing the identity of a block. Additional phrase-based features include block orientation, target and source phrase bigram features. Word-based features are used as well, e.g. feature captures word-to-word translation de4On our test set, (Tillmann and Zhang, 2005) reports a BLEU score of and (Ittycheriah and Roukos, 2005) reports a BLEU score of . pendencies similar to the use of Model probabilities in (Koehn et al., 2003). Additionally, we use distortion features involving relative source word position and -gram features for adjacent target words. These features correspond to the use of a language model, but the weights for theses features are trained on the parallel training data only. For the most complex model, the number of features is about million (ignoring all features that occur only once). 3 Approximate Relevant Set Method Throughout the section, we let . Each block sequence corresponds to a candidat</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>A. Ittycheriah and S. Roukos. 2005. A Maximum Entropy Word Aligner for Arabic-English MT. In Proc. of HLT-EMNLP 06, pages 89–96, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003: Main Proceedings,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Alberta, Canada,</location>
<contexts>
<context position="9464" citStr="Koehn et al., 2003" startWordPosition="1514" endWordPosition="1517">the binary features are chosen such that they yield on the example block sequence in Fig. 1. There are phrase-based and word-based features: The feature is a ’unigram’ phrase-based feature capturing the identity of a block. Additional phrase-based features include block orientation, target and source phrase bigram features. Word-based features are used as well, e.g. feature captures word-to-word translation de4On our test set, (Tillmann and Zhang, 2005) reports a BLEU score of and (Ittycheriah and Roukos, 2005) reports a BLEU score of . pendencies similar to the use of Model probabilities in (Koehn et al., 2003). Additionally, we use distortion features involving relative source word position and -gram features for adjacent target words. These features correspond to the use of a language model, but the weights for theses features are trained on the parallel training data only. For the most complex model, the number of features is about million (ignoring all features that occur only once). 3 Approximate Relevant Set Method Throughout the section, we let . Each block sequence corresponds to a candidate translation. In the training data where target translations are given, a BLEU score can be calculated</context>
<context position="18545" citStr="Koehn et al., 2003" startWordPosition="3101" endWordPosition="3104">t set consisting of sentences with Arabic words with reference translations. In order to speed up the parameter training the original training data is filtered according to the test set: all the Arabic substrings that occur in the test set are computed and the parallel training data is filtered to include only those training sentence pairs that contain at least one out of these phrases: the resulting pre-filtered training data contains about thousand sentence pairs ( million Arabic words and million English words). The block set is generated using a phrase-pair selection algorithm similar to (Koehn et al., 2003; Al-Onaizan et al., 2004), which includes some heuristic filtering to mal statement here. A detailed theoretical investigation of the method will be given in a journal paper. for each data point initialize truth and alternative for each decoding iteration: for each data point select relevant points (*) update update by solving Eq. 5 approximately (**) (7) 725 increase phrase translation accuracy. Blocks that occur only once in the training data are included as well. 4.1 Practical Implementation Details The training algorithm in Table 2 is adapted from Table 1. The training is carried out by r</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL 2003: Main Proceedings, pages 127–133, Edmonton, Alberta, Canada, May 27 - June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofHLT-EMNLP 05,</booktitle>
<pages>161--168</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="1708" citStr="Kumar and Byrne, 2005" startWordPosition="261" endWordPosition="264"> SMT as a sequential process that generates block orientation sequences. A block is a pair of phrases which are translations of each other. For example, Figure 1 shows an Arabic-English translation example that uses four blocks. During decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time. A monotone block sequence is generated except for the possibility to handle some local phrase re-ordering. In this local re-ordering model (Tillmann and Zhang, 2005; Kumar and Byrne, 2005) a block with orientation is generated relative to its predecessor block . During decoding, we maximize the score of a block orientation sequence ): (1) where is a block, is its predecessor block, and eft ight eutral is a threevalued orientation component linked to the block : a block is generated to the left or the right of its predecessor block , where the orientation of the predecessor block is ignored. Here, is the number of blocks in the translation. We are interested in learning the weight vector from the training data. is a high-dimensional binary feature representation of the block ori</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proc. ofHLT-EMNLP 05, pages 161– 168, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proceedings of the Eleventh International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="15376" citStr="Lewis and Catlett, 1994" startWordPosition="2541" endWordPosition="2544">y includes those alternatives with score close to one of the selected truth. These are the most important alternatives that are easily confused with the truth. Essentially the lemma says that if the decoder works well on these difficult alternatives (relevant points), then it works well on the whole space. The idea is closely related to active learning in standard classification problems, where we 724 Table 1: Generic Approximate Relevant Set Method selectively pick the most important samples (often based on estimation uncertainty) for labeling in order to maximize classification performance (Lewis and Catlett, 1994). In the active learning setting, as long as we do well on the actively selected samples, we do well on the whole sample space. In our case, as long as we do well on the relevant set, the decoder will perform well. Since the relevant set depends on the decoder parameter , and the decoder parameter is optimized on the relevant set, it is necessary to estimate them jointly using an iterative algorithm. The basic idea is to start with a decoding parameter , and estimate the corresponding relevant set; we then update based on the relevant set, and iterate this process. The procedure is outlined in</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the Eleventh International Conference on Machine Learning, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL’05,</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3684" citStr="McDonald et al., 2005" startWordPosition="587" endWordPosition="590">nto local decision steps based on Eq. 1, but the model is trained in a global setting as shown below. The advantage of this approach is that it can easily handle tens of millions of features, e.g. up to million features for the experiments in this paper. Moreover, under this view, SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing, and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task, e.g. the on-line training algorithm presented in (McDonald et al., 2005) and the perceptron training algorithm presented in (Collins, 2002). The current approach does not use specialized probability features as in (Och, 2003) in any stage during decoder parameter training. Such probability features include language model, translation or distortion probabilities, which are commonly used in current SMT approaches 1. We are able to achieve comparable performance to (Tillmann and Zhang, 2005). The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT (Och, 2003) as follows: No additional development data set is necessa</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings ofACL’05, pages 91–98, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3837" citStr="Och, 2003" startWordPosition="612" endWordPosition="613">s of millions of features, e.g. up to million features for the experiments in this paper. Moreover, under this view, SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing, and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task, e.g. the on-line training algorithm presented in (McDonald et al., 2005) and the perceptron training algorithm presented in (Collins, 2002). The current approach does not use specialized probability features as in (Och, 2003) in any stage during decoder parameter training. Such probability features include language model, translation or distortion probabilities, which are commonly used in current SMT approaches 1. We are able to achieve comparable performance to (Tillmann and Zhang, 2005). The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT (Och, 2003) as follows: No additional development data set is necessary as the weight vector is trained on bilingual training data only. The paper is structured as follows: Section 2 presents the baseline block sequence mo</context>
<context position="8270" citStr="Och, 2003" startWordPosition="1326" endWordPosition="1327">em in which each block sequence represents a possible class. The effect of this training procedure can be seen in Figure 2: each decoding step on the training data adds a high-scoring block sequence to the discriminative training and decoding performance on the training data is improved after each iteration (along with the test data decoding performance). A theoretical justification for the novel training procedure is given in Section 3. We now define the feature components for the block bigram feature vector in Eq. 1. Although the training algorithm can handle realvalued features as used in (Och, 2003; Tillmann and Zhang, 2005) the current paper intentionally excludes them. The current feature functions are similar to those used in common phrase-based translation systems: for them it has been shown that good translation performance can be achieved 4. A systematic analysis of the novel training algorithm will allow us to include much more sophisticated features in future experiments, i.e. POSbased features, syntactic or hierarchical features (Chiang, 2005). The dimensionality of the feature vector depends on the number of binary features. For illustration purposes, the binary features are c</context>
<context position="26557" citStr="Och, 2003" startWordPosition="4436" endWordPosition="4437">bout for the final model (we do not include the test data block sequences in the training). Table 3 shows a typical learning curve for the experiments in Table 3: the training BLEU score is much higher than the test set BLEU score despite the fact that the test set uses reference translations. 5 Discussion and Future Work The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993). While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems (Chiang, 2005; Och, 2003), this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems 0.6 0.5 0.4 0.3 0.2 0.1 0 ’SWAP.TRAINING’ ’SWAP.TEST’ 727 such as part-of-speech tagging or shallow parsing, both in modeling and parameter training. Unlike earlier approaches to SMT training, which either rely heavily on domain knowledge, or can only handle a small number of features, this approach treats the decoding process as a black box, and can optimize tens millions of parameters automatically, which makes it applicable to other problems as well. Th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL’03, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of machine translation. In</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="17879" citStr="Papineni et al., 2002" startWordPosition="2991" endWordPosition="2994">r algorithm. Now consider Eq. 1 used in our model. We may express it as: where . Using this feature representation and the loss function in Eq. 4, we obtain the following costMargin SGD update rule for each training data point and : 4 Experimental Results We applied the novel discriminative training approach to a standard Arabic-to-English translation task. The training data comes from UN news sources. Some punctuation tokenization and some number classing are carried out on the English and the Arabic training data. We show translation results in terms of the automatic BLEU evaluation metric (Papineni et al., 2002) on the MT03 Arabic-English DARPA evaluation test set consisting of sentences with Arabic words with reference translations. In order to speed up the parameter training the original training data is filtered according to the test set: all the Arabic substrings that occur in the test set are computed and the parallel training data is filtered to include only those training sentence pairs that contain at least one out of these phrases: the resulting pre-filtered training data contains about thousand sentence pairs ( million Arabic words and million English words). The block set is generated usin</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of machine translation. In In Proc. of ACL’02, pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Franz-Josef Och</author>
</authors>
<title>Discriminative Reranking of Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint HLT and NAACL Conference (HLT 04),</booktitle>
<pages>177--184</pages>
<location>Boston, MA,</location>
<contexts>
<context position="28180" citStr="Shen et al., 2004" startWordPosition="4696" endWordPosition="4699">er than previous approaches, good translation results are achieved on a standard translation task. While being similar to (Tillmann and Zhang, 2005), the current procedure is more automated with comparable performance. The latter approach requires a decomposition of the decoding scheme into local decision steps with the inherent difficulty acknowledged in (Tillmann and Zhang, 2005). Since such limitation is not present in the current model, improved results may be obtained in the future. A perceptronlike algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004). The computational requirements for the training algorithm in Table 2 can be significantly reduced. While the global training approach presented in this paper is simple, after iterations or so, the alternatives that are being added to the relevant set differ very little from each other, slowing down the training considerably such that the set of possible block translations might not be fully explored. As mentioned in Section 2, the current approach is still able to handle real-valued features, e.g. the language model probability. This is important since the language model can be trained on a </context>
</contexts>
<marker>Shen, Sarkar, Och, 2004</marker>
<rawString>Libin Shen, Anoop Sarkar, and Franz-Josef Och. 2004. Discriminative Reranking of Machine Translation. In Proceedings of the Joint HLT and NAACL Conference (HLT 04), pages 177–184, Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A localized prediction model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>557--564</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1684" citStr="Tillmann and Zhang, 2005" startWordPosition="257" endWordPosition="260">nts a view of phrase-based SMT as a sequential process that generates block orientation sequences. A block is a pair of phrases which are translations of each other. For example, Figure 1 shows an Arabic-English translation example that uses four blocks. During decoding, we view translation as a block segmentation process, where the input sentence is segmented from left to right and the target sentence is generated from bottom to top, one block at a time. A monotone block sequence is generated except for the possibility to handle some local phrase re-ordering. In this local re-ordering model (Tillmann and Zhang, 2005; Kumar and Byrne, 2005) a block with orientation is generated relative to its predecessor block . During decoding, we maximize the score of a block orientation sequence ): (1) where is a block, is its predecessor block, and eft ight eutral is a threevalued orientation component linked to the block : a block is generated to the left or the right of its predecessor block , where the orientation of the predecessor block is ignored. Here, is the number of blocks in the translation. We are interested in learning the weight vector from the training data. is a high-dimensional binary feature represe</context>
<context position="2937" citStr="Tillmann and Zhang, 2005" startWordPosition="461" endWordPosition="464">tion pair . The block orientation se721 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 721–728, Sydney, July 2006. c�2006 Association for Computational Linguistics ❱quence is generated under the restriction that the concatenated source phrases of the blocks yield the input sentence. In modeling a block sequence, we emphasize adjacent block neighbors that have right or left orientation, since in the current experiments only local block swapping is handled (neutral orientation is used for ’detached’ blocks as described in (Tillmann and Zhang, 2005)). This paper focuses on the discriminative training of the weight vector used in Eq. 1. The decoding process is decomposed into local decision steps based on Eq. 1, but the model is trained in a global setting as shown below. The advantage of this approach is that it can easily handle tens of millions of features, e.g. up to million features for the experiments in this paper. Moreover, under this view, SMT becomes quite similar to sequential natural language annotation problems such as part-of-speech tagging and shallow parsing, and the novel training algorithm presented in this paper is actu</context>
<context position="5734" citStr="Tillmann and Zhang, 2005" startWordPosition="906" endWordPosition="909">ding so-called block orientation. Starting point for the block-based translation model is a block set, e.g. about million Arabic-English phrase pairs for the experiments in this paper. This block set is used to decode training sentence to obtain block orientation sequences that are used in the discriminative parameter training. Nothing but the block set and the parallel training data is used to carry out the training. We use the block set described in (Al-Onaizan et al., 2004), the use of a different block set may effect translation results. Rather than predicting local block neighbors as in (Tillmann and Zhang, 2005) , here the model parameters are trained in a global setting. Starting with a simple model, the training data is decoded multiple times: the weight vector is trained to discriminate block sequences with a high translation score against block sequences with a high BLEU score 2. The high BLEU scoring block sequences are obtained as follows: the regular phrase-based decoder is modified in a way that it uses the BLEU score as optimization criterion (independent of any translation model). Here, searching for the highest BLEU scoring block sequence is restricted to local re-ordering as is the model-</context>
<context position="8297" citStr="Tillmann and Zhang, 2005" startWordPosition="1328" endWordPosition="1331"> each block sequence represents a possible class. The effect of this training procedure can be seen in Figure 2: each decoding step on the training data adds a high-scoring block sequence to the discriminative training and decoding performance on the training data is improved after each iteration (along with the test data decoding performance). A theoretical justification for the novel training procedure is given in Section 3. We now define the feature components for the block bigram feature vector in Eq. 1. Although the training algorithm can handle realvalued features as used in (Och, 2003; Tillmann and Zhang, 2005) the current paper intentionally excludes them. The current feature functions are similar to those used in common phrase-based translation systems: for them it has been shown that good translation performance can be achieved 4. A systematic analysis of the novel training algorithm will allow us to include much more sophisticated features in future experiments, i.e. POSbased features, syntactic or hierarchical features (Chiang, 2005). The dimensionality of the feature vector depends on the number of binary features. For illustration purposes, the binary features are chosen such that they yield </context>
<context position="27710" citStr="Tillmann and Zhang, 2005" startWordPosition="4620" endWordPosition="4623">s automatically, which makes it applicable to other problems as well. The choice of our formulation is convex, which ensures that we are able to find the global optimum even for large scale problems. The loss function in Eq. 4 may not be optimal, and using different choices may lead to future improvements. Another important direction for performance improvement is to design methods that better approximate Eq. 6. Although at this stage the system performance is not yet better than previous approaches, good translation results are achieved on a standard translation task. While being similar to (Tillmann and Zhang, 2005), the current procedure is more automated with comparable performance. The latter approach requires a decomposition of the decoding scheme into local decision steps with the inherent difficulty acknowledged in (Tillmann and Zhang, 2005). Since such limitation is not present in the current model, improved results may be obtained in the future. A perceptronlike algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004). The computational requirements for the training algorithm in Table 2 can be significantly reduced. While the global training app</context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2005. A localized prediction model for statistical machine translation. In Proceedings of ACL’05, pages 557–564, Ann Arbor, Michigan, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>