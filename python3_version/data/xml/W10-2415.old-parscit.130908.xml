<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004247">
<title confidence="0.9993045">
Assessing the Challenge of Fine-Grained
Named Entity Recognition and Classification
</title>
<author confidence="0.994655">
Asif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo Ponzetto
</author>
<affiliation confidence="0.9011285">
Department of Computational Linguistics
Heidelberg University, Germany
</affiliation>
<email confidence="0.997992">
{ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.994778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999270736842106">
Named Entity Recognition and Classi-
fication (NERC) is a well-studied NLP
task typically focused on coarse-grained
named entity (NE) classes. NERC for
more fine-grained semantic NE classes has
not been systematically studied. This pa-
per quantifies the difficulty of fine-grained
NERC (FG-NERC) when performed at
large scale on the people domain. We
apply unsupervised acquisition methods
to construct a gold standard dataset for
FG-NERC. This dataset is used to bench-
mark methods for classifying NEs at var-
ious levels of fine-grainedness using clas-
sical NERC techniques and global contex-
tual information inspired from Word Sense
Disambiguation approaches. Our results
indicate high difficulty of the task and pro-
vide a ‘strong’ baseline for future research.
</bodyText>
<sectionHeader confidence="0.99876" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999689542372882">
Named Entity Recognition and Classification (cf.
Nadeau and Sekine (2007)) is a well-established
NLP task relevant for nearly all semantic process-
ing and information access applications. NERC
has been investigated using supervised (McCallum
and Li, 2003), unsupervised (Etzioni et al., 2005)
and semi-supervised (Pas¸ca et al., 2006b) learning
methods. It has been investigated in multilingual
settings (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003) and special domains, e.g.
biomedicine (Ananiadou et al., 2004).
The classical NERC task is confined to coarse-
grained named entity (NE) classes established
in the MUC (MUC-7, 1998) or CoNLL (Tjong
Kim Sang, 2002) competitions, typically PERS,
LOC, ORG, MISC. While most recent work con-
centrates on feature engineering and robust statis-
tical models for various domains, few researchers
addressed the problem of recognizing and catego-
rizing fine-grained NE classes (such as biologist,
composer, or athlete) in an open-domain setting.
Fine-grained NERC is expected to be benefi-
cial for a wide spectrum of applications, includ-
ing Information Retrieval (Mandl and Womser-
Hacker, 2005), Information Extraction (Pas¸ca et
al., 2006a) or Question-Answering (Pizzato et
al., 2006). However, manually compiling wide-
coverage gazetteers for fine-grained NE classes is
time-consuming and error-prone. Also, without an
extrinsic evaluation, it is difficult to define a priori
which classes are relevant for a particular domain
or task. Finally, prior research in FG-NERC is dif-
ficult to evaluate, due to the diversity of NE classes
and datasets used.
Accordingly, in the interest of a general ap-
proach, we address the challenge of capturing a
broad range of NE classes at various levels of con-
ceptual granularity. By turning FG-NERC into
a widely applicable task, applications are free to
choose relevant NE categories for specific needs.
Also, establishing a gold standard dataset for this
task enables comparative benchmarking of meth-
ods. However, the envisaged task is far from triv-
ial, given that the set of possible semantic classes
for a given NE comprises the full space of NE
classes, whereas descriptive nouns may be am-
biguous between a fixed set of meanings only.
The paper aims to establish a general frame-
work for FG-NERC by addressing two goals: (i)
we automatically build a gold standard dataset of
NE instances classified in context with fine-grain-
ed semantic class labels; (ii) we develop strong
baseline methods, to assess the aptness of standard
NLP approaches for this task. The two efforts are
strongly interleaved: a standardized dataset is not
only essential for (comparative) evaluation, but
also a prerequisite for classification approaches
based on supervised learning, the most successful
techniques for sequential labeling problems.
</bodyText>
<page confidence="0.984651">
93
</page>
<note confidence="0.9872225">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93–101,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.997488" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99998767816092">
An early approach to FG-NERC is Alfonseca and
Manandhar (2002), who identify it as a problem
related to Word Sense Disambiguation (WSD).
They jointly address concept hierarchy learning
and instance classification using topic signatures,
yet the experiments are restricted to a small on-
tology of 9 classes. Similarly, Fleischman and
Hovy (2002) extend previous work from Fleis-
chman (2001) on locations and address the ac-
quisition of instances for 8 fine-grained person
classes. For supervised training they compile a
web corpus which is filtered using high-confident
classifications from an initial classifier trained on
seeds. Due to the limitations of their method to
create a good sample of training data, the perfor-
mance could not be generalized to held-out data.
Recent work takes the task of FG-NERC one
step further by (i) extending the number of classes,
(ii) relating them to reference concept hierar-
chies and (iii) exploring methods for building
training and evaluation data, or applying weakly
and unsupervised learning based on high-volume
data. Tanev and Magnini (2006) selected 10 NE-
subclasses of person and location using Word-
Net as a reference. Datasets were automatically
acquired and manually filtered. They compare
word and pattern-based supervised and a semi-
supervised approach based on syntactic features.
Giuliano &amp; Gliozzo (2007, 2008) perform NE
classification against the People Ontology, an ex-
cerpt of the WordNet hierarchy, comprising 21
people classes populated with at least 40 instances.
Using minimally supervised lexical substitution
methods, they cast NE classification as an ontol-
ogy population task – as opposed to recognition
and classification in context. In a similar setting,
Giuliano (2009) explores semi-supervised classifi-
cation of the People Ontology classes using latent
semantic kernels, comparing models built from
Wikipedia and from a news corpus. In a differ-
ent line of research Pas¸ca (2007) and Pas¸ca and
van Durme (2008) make use of query logs to ac-
quire NEs on a large scale. While Pas¸ca (2007)
extracts NEs for 10 target classes, Pas¸ca and van
Durme (2008) combine web query logs and web
documents to acquire both NE-concept pairs and
concept attributes using seeds.
But while these more recent approaches all of-
fer substantially novel contributions for many NE
acquisition subtasks, none of them addresses the
full task of FG-NERC, i.e., recognition and clas-
sification of NE tokens in context. Compared to
ontology population, focusing on types, classifica-
tion in raw texts needs to consider any token and
cannot rely on special contexts offering indicative
clues for class membership.
Bunescu and Pas¸ca (2006) also perform dis-
ambiguation and classification of NEs in context,
yet in a different setup. Disambiguation is per-
formed into one of the known possible classes
for a NE, as determined from Wikipedia disam-
biguation pages. Contexts for training and testing
are acquired from Wikipedia pages, as opposed
to general text. Disambiguation is performed us-
ing vectors of co-occurring terms and a taxonomy-
based kernel that integrates word-category corre-
lations. Evaluation is performed on the task of
predicting, for a given NE in a Wikipedia page
context, the correct class from among its known
classes, including one experiment that included
10% of out-of-Wikipedia entities. The category
space was confined to People by occupation, with
8,202 subclasses. Classification considered 110
broad classes, 540 highly populated classes (w/o
out-of-Wikipedia entities), and 2,847 classes in-
cluding less populated ones. This setup is diffi-
cult to compare given the sense granularities em-
ployed and the special Wikipedia text genre. Even
though classification is performed in context, the
task does not evaluate recognition.
To summarize, the field has developed robust
methods for acquisition and fine-grained classifi-
cation of NEs on a large scale. But, the full task
of NE recognition and classification in context still
remains to be addressed for a wide-coverage, fine-
grained semantic class inventory that can serve as
a common benchmark for future research.
</bodyText>
<sectionHeader confidence="0.994888" genericHeader="method">
3 Fine-grained NERC on a large-scale
</sectionHeader>
<bodyText confidence="0.99996425">
We present experiments that assess the difficulty
of open-domain FG-NERC pursued at a large
scale. We concentrate on instances and classes re-
ferring to people, since it is a well-studied domain
(see Section 2) and structured fine-grained infor-
mation can be readily applied to a well-defined
end-user task such as IR, cf. the Web People
Search task (Artiles et al., 2008). Our method
is general in that it requires only a (PoS tagged
and chunked) corpus and a reference taxonomy
to provide a concept hierarchy. Given a map-
ping between automatically extracted class labels
</bodyText>
<page confidence="0.998912">
94
</page>
<bodyText confidence="0.993646416666667">
and concepts in a taxonomic resource, it can be
further extended to other domains, e.g. locations
or the biomedical domain by leveraging open-
domain taxonomies such as Yago (Suchanek et
al., 2008) or WikiTaxonomy (Ponzetto and Strube,
2007). The contribution of this work is two-fold:
(i) We develop an unsupervised method for ac-
quiring a comprehensive dataset for FG-NERC by
applying linguistically motivated patterns to a cor-
pus harvested from the Web (Section 4). Large
amounts of NEs are acquired together with their
contexts of occurrence and with their fine-grained
class labels which are mapped to synsets in Word-
Net. The controlled sense inventory and the tax-
onomic structure offered by WordNet enables an
evaluation of FG-NERC performance at different
levels of concept granularity, as given by the depth
at which the concepts are found. As our extraction
patterns reflect a wide-spread grammatical con-
struct, the method can be applied to many lan-
guages and extended to other domains.
(ii) Given this automatically acquired dataset,
we assess the problem of FG-NERC in a sys-
tematic series of experiments, exploring the per-
formance of NERC methods on different levels
of granularities. For recognition and classifica-
tion we apply standard sequential labeling tech-
niques – i.e. a Maximum Entropy (MaxEnt) tag-
ger (Section 5.1) – which we adapt to this hier-
archical classification problem (Section 5.2). To
test the hypothesis of whether a sequential la-
beler represents a valid choice to perform FG-
NERC, we compare the latter to a MaxEnt system
trained on a more semantically informed feature
set, and a gloss-overlap method inspired by WSD
approaches (Section 5.3).
</bodyText>
<sectionHeader confidence="0.552168" genericHeader="method">
4 Acquisition of a FG-NERC dataset
</sectionHeader>
<bodyText confidence="0.999905875">
We present an unsupervised method that simul-
taneously acquires NEs, their semantic class and
contexts of occurrence from large textual re-
sources. In order to develop a clean resource of
properly disambiguated NEs, we develop acqui-
sition patterns for a grammatical construction that
unambiguously associates proper names with their
corresponding semantic class.
</bodyText>
<subsectionHeader confidence="0.629357">
Pattern-based extraction of NE-concept pairs.
</subsectionHeader>
<bodyText confidence="0.999836166666667">
NEs are often introduced by so-called apposi-
tional structures as in (1), which overtly ex-
press which semantic class (here, painter) the NE
(Kandinsky) belongs to. Appositions involving
proper names can be captured by extraction pat-
terns as given in (2).
</bodyText>
<listItem confidence="0.898328166666667">
(1) ... writings of the abstract painter Kandinsky
frequently explored similarities between ...
(2) a. [thelThe]? [JJINN]* [NN] [NP]
the abstract painter Kandinsky
b. [NP] [,]? [alanithe]* [JJINN]* [NN]
W. Kandinsky, a Russian-born painter,..
</listItem>
<bodyText confidence="0.996273684210526">
Contexts like (2.a) provide a less noisy se-
quence for extraction, due to the class and instance
labels being adjacent – in contrast to (2.b) where
any number of modifiers can intervene between
the two. Accordingly, we apply in our experiments
only a restricted version of (2.a) – with a deter-
miner – to UKWAC, an English web-based cor-
pus (Baroni et al., 2009) that comes in a cleaned,
PoS-tagged and lemmatized form. Due to its size
(&gt;2 billion tokens) and mixed genres, the corpus
is ideally suited for acquiring large quantities of
NEs pertaining to a broad variety of open-domain
semantic classes.
Filtering heuristics. The apposition patterns are
subject to noise, due to PoS-tagging errors, as
well as special constructions, e.g. reduced rela-
tive clauses. The former can be controlled by fre-
quency filters, the latter can be circumvented by
using chunk boundary information1. A more chal-
lenging problem is to recognize whether an ex-
tracted nominal is in fact a valid semantic class for
NEs. Besides, class labels can be ambiguous, so
there is uncertainty as to which class an extracted
entity should be assigned to. We apply two fil-
tering strategies: we set a frequency threshold ft
on the number of extracted NE tokens per class,
to remove infrequent class label extractions; we
then filter invalid semantic classes using informa-
tion from WordNet: given the WordNet PERSON
supersense, i.e. the lexicographer file for nouns de-
noting people, we check whether the first sense of
the class label candidate is found in PERSON.
Mapping to the WordNet person domain. In
order to perform a hierarchical classification of
people, we need a taxonomy for the domain at
hand. We achieve this by mapping the extracted
class labels to WordNet synsets. In our setting, we
map against all synsets found under person#n#1,
</bodyText>
<footnote confidence="0.774685">
1We use YamCha (Kudo and Matsumoto, 2000) to per-
form phrase chunking.
</footnote>
<page confidence="0.997585">
95
</page>
<bodyText confidence="0.999984166666667">
which are direct hypernyms of at least one in-
stance in WordNet (CWN pers+Inst).2 Since our
goal is to map class labels to synsets (i.e. our fu-
ture NE classes), we check each class label candi-
date against all synonyms contained in the synset.
At this point we have to deal with two cases: two
extracted class label candidates (synonyms such
as doctor, physician) will map to a single synset,
while ambiguous class labels (e.g. director) can be
mapped to more than one synset. In the latter case,
we heuristically choose the synset which domi-
nates the highest number of instances in WordNet.
Mapping evaluation. We evaluated the cover-
age of our mapping for two sets of class labels
extracted for two different frequency thresholds:
ft = 40 and ft = 1. With ft = 40, we cover
31.1% of the synsets found under person#n#1 in
WordNet, i.e. the set of classes CWN pers+Inst;
conversely, 45.8% of the extracted class labels can
be successfully mapped to CWN pers+Inst. For
threshold ft = 1, we are able to map to 87.9%
of CWN pers+Inst, with only 20.1% of extracted
classes mapped to CWN pers+Inst. For the re-
maining 79.9% of class labels (e.g. goalkeeper,
chancellor, superstar) that have no instances in
WordNet, we manually inspected 20 classes, in 20
contexts each, and established that 76% of them
are appropriate NE person classes.
For threshold ft = 40, we obtain 153 class labels
which are mapped to 146 synsets. Ten class labels
are mapped to more than one synset. Using our
mapping heuristic based on the majority instance
class, we successfully disambiguate all of them.
However, since we only map to CWN pers+Inst,
we introduce errors for 5 classes. E.g. ‘manager’
incorrectly gets mapped to manager#n#2, since
the latter is the only synset containing instances.
For these cases we manually corrected the auto-
matic mapping.
A taxonomy for FG-NERC. We create our gold
standard taxonomy of semantic classes by start-
ing with the 146 synsets obtained from the map-
ping, including the 5 classes that were manually
corrected. Since we concentrate on the people
domain, we additionally remove 5 classes that
can refer to other domains as well (e.g. carrier,
guide). Given the remaining 141 synsets, we se-
lect the portion of WordNet rooted at person#n#1
</bodyText>
<footnote confidence="0.845026333333333">
2We use WordNet version 3.0. With w#p#i we denote the
i-th sense of a word w with part of speech p. E.g., person#n#1
is defined as { person, individual ...1).
</footnote>
<table confidence="0.9996708">
Level #C #C w/inst #inst #inst/C % of inst
1 1 0 0 - -
2 29 8 2,662 332 5.49
3 57 37 18,229 493 37.58
4 63 46 18,422 401 37.94
5 37 30 6,231 208 12.84
6 18 13 2,366 182 4.88
7 6 5 423 85 0.87
8 2 2 179 90 0.36
all 213 141 48,512 344 100
</table>
<tableCaption confidence="0.8197765">
Table 1: Level-wise statistics of classes and in-
stances across the FG-NERC person taxonomy.
</tableCaption>
<bodyText confidence="0.999636102564103">
which contains them, together with any interven-
ing synset found along the WordNet hierarchy.
Given this WordNet excerpt, the extracted NE to-
kens are then appended to the respective synsets in
the hierarchy. Statistics of the resulting WordNet
fragment augmented with instances are given in
Table 1. The taxonomy has a maximum depth of 8,
and contains 213 synsets, i.e. NE classes (see col-
umn 2). 83.5% of the 31,819 extracted instances
(type-level) sit in leaf nodes. The classes automat-
ically refer back to the acquired appositional con-
texts. Table 1 gives statistics about the number of
instances (token-level) acquired for classes at dif-
ferent embedding levels. In total we have at our
disposal 48,512 instances (token-level) in apposi-
tional contexts. The type-token ratio is 1.52.
Gold standard validation. To create a gold
standard dataset of entities in context labeled with
fine-grained classes, we first randomly select 20
classes, as well as an additional 18 which are
also found in the People Ontology (Giuliano and
Gliozzo, 2008). For each class, we randomly se-
lect 40 occurrences of instances in context, i.e.
the words co-occurring in a window of 60 tokens
before and after the instance. We asked four an-
notators to label these extractions for correctness,
and to provide the correct label for the incorrect
cases, if one was available. Only 52 contexts out
of 1520 were labeled as incorrect, thus giving us
96.58% accuracy on our automatically extracted
data. The manually validated dataset is used to
provide a ground-truth for FG-NERC. However,
the noun (e.g. hunter) denoting the NE class is re-
moved from these contexts for training and testing
in all experiments. This is because, due to the ex-
traction method based on POS-patterns denoting
appositions, class labels are known a priori to oc-
cur in the context of an instance and thus identify
them with high precision.
</bodyText>
<page confidence="0.98623">
96
</page>
<sectionHeader confidence="0.997935" genericHeader="method">
5 Methodology for FG-NERC
</sectionHeader>
<bodyText confidence="0.999934533333333">
We develop methods to perform FG-NERC using
standard techniques developed for coarse-grained
NERC and WSD. These are applied to our dataset
from Section 4, in order to measure performance
at different levels of semantic class granularity, i.e.
corresponding to the depth of the semantic classes
found in our WordNet fragment. We start in Sec-
tion 5.1 to present a Maximum Entropy model to
perform coarse-grained NERC and we extend it
to perform multiclass classification in a hierarchi-
cal taxonomy (Section 5.2). We then present in
Section 5.3 an alternative proposal to perform FG-
NERC using global context information, as found
in state-of-the-art approaches to supervised and
unsupervised WSD.
</bodyText>
<subsectionHeader confidence="0.872075">
5.1 NERC using a MaxEnt tagger
</subsectionHeader>
<bodyText confidence="0.999968666666667">
Our baseline system is modeled following a Maxi-
mum Entropy approach (Bender et al., 2003, inter
alia). The MaxEnt model produces a probability
for each class label t (the NE tag) of a classifica-
tion instance, conditioned on its context of occur-
rence h. This probability is calculated by:
</bodyText>
<equation confidence="0.994087">
� �
1n
P(t |h) = Z(h) exp �Ajfj(h, t) �(1)
j=1
</equation>
<bodyText confidence="0.9994842">
where fj(h, t) is the j-th feature with associated
weight Aj and Z(h) is a normalization constant to
ensure a proper probability distribution.3 Given a
word wi to be classified as Beginning, Inside or
Outside (IOB) of a NE, we extract as features:
</bodyText>
<listItem confidence="0.967417">
1. Context words. The words occurring within
the context window wi+2
</listItem>
<equation confidence="0.694055">
i−2 = wi−2 ... wi+2.
</equation>
<listItem confidence="0.999334833333333">
2. Word prefix and suffix. Word prefix and suffix
character sequences of length up to n.
3. Infrequent word. A feature that fires if wi oc-
curs in the training set less frequently than a
given threshold (i.e. below 10 occurrences).
4. Part-of-Speech (PoS) and chunk informa-
tion. The PoS and chunk labels of wi.
5. Capitalization. A binary feature that checks
whether wi starts with a capital letter or not.
6. Word length. A binary feature that fires if
the length of wi is smaller than a pre-defined
threshold (i.e. less than 5 characters).
</listItem>
<footnote confidence="0.8254715">
3In our implementation, we use the OpenNLP MaxEnt li-
brary (http://maxent.sourceforge.net).
</footnote>
<listItem confidence="0.9944492">
7. Digit and symbol features. Three features
check whether wi contains digit strings, non-
characters (e.g. slashes) or number expressions.
8. Dynamic feature. The tag ti−1 of the word
wi−1 preceding wi in the sequence wn1.
</listItem>
<subsectionHeader confidence="0.990695">
5.2 MaxEnt extension for FG-NERC
</subsectionHeader>
<bodyText confidence="0.999669926829268">
Extension to hierarchical classification. We
apply our baseline NERC system to FG-NERC.
Given a word in context, the task consists of recog-
nizing it as a NE, and classifying it into the appro-
priate semantic class from our person taxonomy.
We approach this as a hierarchical classification
task by generating a binary classifier4 with sepa-
rate training and test sets for each node in the tree.
To perform level-wise classification from coarse
to fine-grained classes, we need to adjust the class
labels and their corresponding training and test in-
stances for each experiment. For classification at
the deepest level, each concept contains the in-
stances of the original dataset. For classification
at higher levels we leverage the semantics of the
WordNet hyponym relations and expand the set
of target classes (i.e. synsets) of a given level to
contain all instances of hyponym synsets. Given
a set I of classification instances for a given tar-
get class c, we add all instances labeled with the
hyponyms of c to I. All other instances (not in
that subtree) are labeled as being Outside (O-) a
NE. This approach ensures that, for each node, the
dataset contains two classes (NE and O) only, and
implicitly ‘propagates’ the instances up the tree.
As a result, non-leaf nodes that did not have any
instance in the original dataset become populated.
Also, the classification of classes at higher levels
is based on larger datasets.
Extension to multiclass classification. Since
we train a binary classifier for each node of the
tree, we apply two methods to infer multiclass de-
cisions from these binary classifiers, namely level-
wise and global multiclass classification. In both
paradigms, we combine the single decisions of
the individual classifiers with the winner-takes-all
strategy, using weighted voting. The weights are
calculated based on the confidence value for the
corresponding class, i.e., its conditional probabil-
ity according to Equation (1). The output label is
selected randomly in case of ties.
</bodyText>
<footnote confidence="0.94758475">
4The IOB tagging scheme normally assigns three different
labels, i.e. Inside (I-), Outside (O-) and Beginning (B-) of
a chunk. However, our dataset does not have any instance
labeled as B-, since it does not contain any adjacent NEs.
</footnote>
<page confidence="0.999724">
97
</page>
<bodyText confidence="0.999771666666667">
For level-wise classi�cation, we combine only
classifiers at the same level of embedding. Given
n concepts at level l, we have n possible out-
put labels for each word. The output label for a
classification instance is determined by the highest
weighted vote among all binary classifiers at level
l. For global classi�cation we combine all binary
classifiers of the entire tree using weighted voting
to determine the winning class label. The weights
are calculated based on the product of confidence
value and depth of the corresponding class in the
tree.
</bodyText>
<subsectionHeader confidence="0.985143">
5.3 FG-NERC using global contexts
</subsectionHeader>
<bodyText confidence="0.999894954545454">
FG-NERC is a more demanding task than ‘classi-
cal’ NERC, due to the larger amount of classes,
the paucity of examples for each class, and the
increasingly subtle semantic differences between
these classes. For such a task contextual informa-
tion is expected to be very informative – e.g. if an
entity co-occurs in context with ’Nobel prize’, this
provides evidence that it is likely to be a scien-
tist or scholar. However, the context window used
by our baseline MaxEnt tagger is very local, in-
cluding at most the two preceding and succeeding
words. Hence, the classifier is not able to capture
informative contextual clues in a larger context.
Previous work has related FG-NERC to WSD
approaches (Alfonseca and Manandhar, 2002).
Accordingly, we investigate two context-sensitive
approaches inspired from WSD proposals, which
consider a more global context for classification.
We first define a new feature set to induce a new
MaxEnt model (MaxEnt-B) which only uses lexi-
cal features from a larger context window, as used
in standard supervised WSD (Lee and Ng, 2002):
</bodyText>
<listItem confidence="0.894397">
1. PoS context. The part-of-speech occur-
</listItem>
<bodyText confidence="0.98125">
ring within the context window posi+3
</bodyText>
<equation confidence="0.9964075">
i−3 =
posi−3 ... posi+3.
</equation>
<listItem confidence="0.879114">
2. Local collocation. Local collocations C,,,m sur-
rounding wi. We use C−2,−1 and C1,2.
3. Content words in surrounding context. We
consider all unigrams in contexts wi+3
</listItem>
<equation confidence="0.937732">
i−3 =
</equation>
<bodyText confidence="0.997344176470588">
wi−3 ... wi+3 of wi (crossing sentence bound-
aries) for the entire training data. We convert to-
kens to lower case, remove stopwords, numbers
and punctuation symbols. We define a feature
vector of length 10 using the 10 most frequent
content words. Given a classification instance,
the feature corresponding to token t is set to 1
iff the context wi+3 i−3of wi contains t.
In addition, we use a Lesk-like method (Lesk,
1986) which labels instances in context with the
WordNet synset whose gloss has the maximum
overlap with the glosses of the senses of its words
in context. Given the small context provided by
the WordNet glosses, we follow Banerjee and Ped-
ersen (2003) and expand these to also include the
words from the glosses of the hypernym and hy-
ponym synsets.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999977">
6.1 Benchmarking on coarse-grained NERC
</subsectionHeader>
<bodyText confidence="0.999941692307692">
We benchmark the performance of our baseline
MaxEnt classifier using the feature set from Sec-
tion 5.1 (MaxEnt-A henceforth) on the CoNLL-
2003 shared task dataset (Tjong Kim Sang and
De Meulder, 2003), the de-facto standard for eval-
uating coarse-grained NERC systems.
In MaxEnt modeling, feature selection is a cru-
cial problem and key to improving classification
performance. MaxEnt, however, does not provide
methods for automatic feature selection. We there-
fore experimented with various combinations of
features standardly used for NERC (1-8 of Section
5.1). Model parameters are computed with 200
iterations without feature frequency cutoff. The
best configuration is found by optimizing the F1
measure on the development data with various fea-
ture representations. The chosen features are: 1, 2
(with n = 3), 4, 5, 6, 7 and 8. Evaluation on the
test set is performed blindly, using this feature set.
The results are presented in Table 2.
The MaxEnt labeler achieves performance com-
parable with the CoNLL-2003 task participants,
ranking 121h among the 16 systems participating
in the task, with a 2 point margin off the F1 of the
most similar system of Bender et al. (2003) and
7 points below the best-performing system (Flo-
rian et al., 2003). The former used a relatively
complex set of features and different gazetteers
extracted from unannotated data. The latter com-
bined four diverse classifiers, namely a robust lin-
ear classifier, maximum entropy, transformation-
based learning and a hidden Markov model. They
used different feature sets, unannotated data and
an additional NE tagger. In comparison, our
NERC system is simpler and based on a small set
of features that can be easily obtained for many
languages. Besides, it does not make use of any
external resources and still shows state-of-the-art
performance on the overall data.
</bodyText>
<page confidence="0.980593">
98
</page>
<table confidence="0.9999235">
Recall Precision F,3_1
PER 83.02% 81.40% 82.21%
LOC 88.47% 88.19% 88.23%
ORG 77.20% 68.03% 72.23%
MISC 81.20% 83.92% 82.54%
Overall 83.11% 80.47% 81.77%
</table>
<tableCaption confidence="0.994377">
Table 2: Results on the CoNLL-2003 test data.
</tableCaption>
<table confidence="0.99937425">
Set # tokens # NEs
Training 2,431,041 38,810
Development 478,871 9,702
Test 181,490 1,520
</table>
<tableCaption confidence="0.994631">
Table 3: Statistics for training, dev and test sets.
</tableCaption>
<subsectionHeader confidence="0.994644">
6.2 Evaluating FG-NERC
</subsectionHeader>
<bodyText confidence="0.999928117647059">
Experimental setting. For the task of FG-
NERC, we compare the performance of MaxEnt-
A with the MaxEnt-B model from Section 5.3 and
the Lesk method. The data is partitioned into train-
ing and development sets by randomly selecting
80%-20% of the contexts in which the NEs occur.
We use the held-out, manually validated gold stan-
dard from Section 4 for blind evaluation. Statistics
for the dataset are reported in Table 3.
We build a MaxEnt model for each FG-NE
class, using the features that performed best on
the CoNLL task, except the digit and dynamic
NE features (MaxEnt-A), and context features 1-
3 of Section 5.3 (MaxEnt-B). Model parameters
are computed in the same way as for coarse-
grained NERC. Table 3 shows that our training set
is highly unbalanced. The ratio between positive
(NEs) and negative examples (i.e. O classification
instances) at the topmost level is 63:1. Also, with
increasing levels of fine-grainedness, the number
of negative (-O) NE classes is increasing for each
binary classifier. We observed on the develop-
ment set that this skewed distribution heavily bi-
ases the classifiers towards the negative category,
and accordingly investigated sampling techniques
to make the ratio of positive and negative examples
more balanced. We experiment with a sampling
strategy that over-samples the positive examples
and under-samples the negative ones. We define
various ratios of over-sampling depending upon
the number of positive examples in the original
training set. Table 4 lists the factors (f) of over-
sampling applied to the original positive samples
(P), with minimum and maximum sizes of the ob-
</bodyText>
<table confidence="0.992022428571429">
factor f size of P min P&apos; max P&apos;
20 x P 1 – 2K 20 40K
15 x P 2K – 5K 30K 75K
10 x P 5K – 10K 50K 100K
5 x P 10K – 20K 50K 100K
2 x P 20K – 50K 40K 100K
P 50K – ... 50K &gt;50K
</table>
<tableCaption confidence="0.994746">
Table 4: Oversampling of positive samples.
</tableCaption>
<table confidence="0.999928666666667">
Level MaxEnt-A Fl R MaxEnt-B Fl
R P P
1 98.7 85.0 91.4 95.1 83.0 88.6
2 96.0 65.5 77.9 48.1 46.3 47.2
3 95.3 54.3 69.3 43.3 41.1 42.2
4 86.8 52.8 65.6 41.1 37.2 39.1
5 90.4 45.9 60.9 49.2 21.5 29.9
6 91.6 36.9 52.6 51.7 13.2 21.1
7 89.5 31.8 46.9 42.2 10.2 16.4
8 100.0 19.9 66.7 87.1 8.1 14.7
global 85.1 43.2 57.3 61.9 26.6 37.2
hierarchical 87.7 44.8 59.4 64.5 29.5 40.5
</table>
<tableCaption confidence="0.9423035">
Table 6: Level-wise NE recognition &amp; classifica-
tion evaluation (in %).
</tableCaption>
<bodyText confidence="0.998688172413793">
tained oversampled sets P&apos; for different ranges of
original sizes of P.5 Oversampling is done with-
out replacement. The number of negative instan-
ces is always downsampled on the basis of P&apos; to
yield a 1:5 ratio of positive and negative samples,
a ratio we estimated from the CoNLL-2003 data.
Level-wise evaluation results on the FG-NE
classification-only (NEC) task for the MaxEnt
classifiers and Lesk are given in Table 5. Table
6 reports results for the evaluation of the MaxEnt
model performing both classification and recog-
nition. As for coarse-grained NERC, we evaluate
using the standard metrics of recall (R), precision
(P) and balanced F-measure (F1). As baseline, we
use a majority class assignment – i.e. at each level,
we label all instances with the most frequent class
label. For global FG-NE classification, reported in
Table 5, the original fine-grained classes are con-
sidered, across the entire class hierarchy. Global
evaluation is performed by counting exact label
predictions on the entire hierarchy (global) and us-
ing the evaluation metric of Melamed and Resnik
(2000, hierarchical). As baseline we assume the
most frequent class label in the training set.
Discussion. All methods perform reasonably
well, indicating the feasibility of the task. For the
MaxEnt models, Table 5 shows a general high re-
call and decreasing precision as we move down the
hierarchy. Degradation in the overall F1 score is
</bodyText>
<footnote confidence="0.863961">
5Sampling ratios are determined on the development set.
</footnote>
<page confidence="0.991912">
99
</page>
<table confidence="0.9995205">
Level R Baseline Fl R MaxEnt-A Fl R MaxEnt-B Fl R Lesk Fl
P P P P
1 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0
2 28.4 25.9 27.1 85.8 88.6 87.0 79.5 84.9 82.2 16.4 19.7 17.9
3 27.9 23.1 25.2 83.9 88.1 85.9 75.5 79.8 77.5 16.2 16.2 16.2
4 18.8 20.4 19.5 74.6 85.0 79.5 65.4 71.3 68.2 11.3 11.3 11.3
5 25.8 19.0 21.9 78.8 83.4 80.9 78.6 74.1 76.3 13.5 14 13.8
6 24.7 7.8 11.9 88.5 73.6 80.4 78.7 74.1 75.7 33.2 37.5 35.2
7 19.1 5.34 8.3 79.2 76.5 77.8 78.1 72.7 75.3 49.4 49.4 49.4
8 34.2 2.9 5.5 82.8 73.8 78.1 81.1 71.1 75.8 0.1 0.1 0.1
global 34.6 18.5 24.1 81.1 84.2 82.6 78.0 74.2 76.6 36.5 38.6 37.5
hierarchical 33.0 21.2 25.8 83.5 86.2 84.8 78.2 77.8 78.1 36.6 38.7 37.6
</table>
<tableCaption confidence="0.999653">
Table 5: Level-wise evaluation of fine-grained NE classification techniques (in %).
</tableCaption>
<bodyText confidence="0.999983266666667">
given by the increasingly limited amount of class
instances found towards the low regions of the tree
(down to an average of 85 and 90 instances per
class for levels 7 and 8, respectively) (cf. Table 1).
The ’classical’ feature set (MaxEnt-A) yields bet-
ter performance compared to the semantic feature
set (MaxEnt-B). However MaxEnt-B still achieves
a respectable performance, given that it contains a
few semantic features only.
The MaxEnt classifiers achieve a far better per-
formance than Lesk. This is in-line with previ-
ous findings in WSD, namely unsupervised fine-
grained disambiguation methods rarely perform-
ing above the baseline, and suggests that Lesk can
be merely used as a ‘strong’ baseline. Error anal-
ysis showed that it performs poorly due to the lim-
ited context provided by the WordNet glosses, and
the limited impact of gloss expansions deriving
from the low connectivity between synsets.
Comparison of Tables 5 and 6 shows that per-
formance decreases considerably for a classifier
that not only assigns fine-grained classes, but also
detects which tokens actually are NEs. As for
the classification-only task, the performance de-
creases as one moves to lower levels. This in-
dicates that the complexity of the task is propor-
tional to the fine-grainedness of the class inven-
tory. MaxEnt-B, lacking ’classical’ NER features,
shows dramatic losses, compared to MaxEnt-A.
Comparison to other work. We compared the
performance of our system based on global classi-
fication (one vs. rest) against the figures reported
for individual categories in Giuliano (2009). The
MaxEnt-A system compares favorably, although it
considers (i) more classes at each level – i.e. 213
vs. 21 – and (ii) classifies NEs at finer-grained lev-
els – i.e. 8 vs. 4 maximum depth in the respec-
tive WordNet fragments. We achieve overall mi-
cro average R, P and F1 values of 87.5%, 85.7%
and 86.6%, respectively, compared to Giuliano’s
79.6%, 80.9% and 80.2%. Due to the different se-
tups and data used, these figures do not offer a ba-
sis for true comparison. However, the figures sug-
gest that our system achieves respectable perfor-
mance on a more complex classification problem.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999980379310345">
We presented a method to perform FG-NERC on
a large scale. Our contribution lies in the def-
inition of a benchmarking setup for this task in
terms of gold standard datasets and strong base-
line methods provided by a MaxEnt classifier. We
proposed a pattern-based approach for the acqui-
sition of fined-grained NE semantic classes and
instances. This corpus-based method relies only
on the availability of large text corpora, such as
the WaCky corpora, in contrast to resources diffi-
cult to obtain, such as query-logs (Pas¸ca and van
Durme, 2008). It makes use of a very large Web
corpus to extract instances from open-domain con-
texts – in contrast to standard NERC approaches,
which are tailored for newswire data and do not
generalize well across domains. Our gold stan-
dard training and test datasets are currently based
only on appositional patterns6. Therefore, it does
not include the full spectrum of constructions in
which instances can be found in context. Future
work will investigate semi-supervised and heuris-
tics (e.g. ‘one sense per discourse’) methods to ex-
pand the data with examples from follow-up men-
tions, e.g. co-occurring in the same document.
Our MaxEnt models still perform very local
classification decisions, relying on separate mod-
els for each semantic class. We accordingly plan to
explore both global models operating on the over-
all hierarchy, and more informative feature sets.
</bodyText>
<footnote confidence="0.997281">
6The data are available for research purposes at http:
//www.cl.uni-heidelberg.de/fgnerc.
</footnote>
<page confidence="0.986308">
100
</page>
<sectionHeader confidence="0.979408" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997301862068965">
Enrique Alfonseca and Suresh Manandhar. 2002.
An unsupervised method for general named entity
recognition and automated concept discovery. In
Proc. of GWC-02.
S. Ananiadou, C. Friedman, and J.I. Tsujii. 2004.
Special issue on named entity recognition in
biomedicine. Journal of Biomedical Informatics,
37(6).
Javier Artiles, Satoshi Sekine, and Julio Gonzalo.
2008. Web people search. In Proc. of LREC ’08.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlap as a measure of semantic relatedness.
In Proc. of IJCAI-03, pages 805–810.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide web:
A collection of very large linguistically processed
web-crawled corpora. Journal of Language Re-
sources and Evaluation, 43(3):209–226.
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum entropy models for named entity
recognition. In Proc. of CoNLL-03, pages 148–151.
Razvan Bunescu and Marius Pas¸ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proc. of EACL-06, pages 9–16.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91–134.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING-02, pages 1–7.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001
Student Workshop.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Proc. of CoNLL-
03, pages 168–171.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proc. of ACL-07, pages 248–256.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proc. of COLING-ACL-08, pages
265–272.
Claudio Giuliano. 2009. Fine-grained classification of
named entities exploiting latent semantic kernels. In
Proc. of CoNLL-09, pages 201–209.
Taku Kudo and Yuji Matsumoto. 2000. Use of Support
Vector Machines for chunk identification. In Proc.
of CoNLL-00, pages 142–144.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning
algorithms for word sense disambiguation. In Proc.
of EMNLP-02, pages 41–48.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings
of the ACL-SIGDOC Conference, pages 24–26.
Thomas Mandl and Christa Womser-Hacker. 2005.
The effect of named entities on effectiveness in
cross-language information retrieval evaluation. In
Proc. of ACM SAC 2005, pages 1059–1064.
Andrew McCallum and Andrew Li. 2003. Early re-
sults for named entity recognition with conditional
random fields, features induction and web-enhanced
lexicons. In Proc. of CoNLL-03, pages 188–191.
Dan Melamed and Philip Resnik. 2000. Tagger evalu-
ation given hierarchical tag sets. Computers and the
Humanities, pages 79–84.
MUC-7. 1998. Proceedings of the Seventh Mes-
sage Understanding Conference (MUC-7). Morgan
Kaufmann, San Francisco, Cal.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Journal
of Linguisticae Investigationes, 30(1).
Marius Pas¸ca and Benjamin van Durme. 2008.
Weakly-supervised acquisition of open-domain
classes and class attributes from web documents and
query logs. In Proc. of ACL-08, pages 19–27.
M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006a. Names and similarities on the web: Fact ex-
traction in the fast lane. In Proc. of COLING-ACL-
06, pages 809–816.
Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, and Alpa Jain. 2006b. Organizing and
searching the world wide web of facts – Step one:
The one-million fact extraction challenge. In Proc.
ofAAAI-06, pages 1400–1405.
Marius Pas¸ca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
CIKM-2007, pages 683–690.
Luiz Augusto Pizzato, Diego Molla, and C´ecile Paris.
2006. Pseudo relevance feedback using named enti-
ties for question answering. In Proc. ofALTW-2006,
pages 83–90.
Simone Paolo Ponzetto and Michael Strube. 2007. De-
riving a large scale taxonomy from Wikipedia. In
Proc. of AAAI-07, pages 1440–1445.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A Large Ontology from
Wikipedia and WordNet. Elsevier Journal of Web
Semantics, 6(3):203–217.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proc. of EACL-06, pages 17–24.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition. In Proc. of CoNLL-03, pages 127–132.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-independent
Named Entity Recognition. In Proc. of CoNLL-02,
pages 155–158.
</reference>
<page confidence="0.998599">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966079">
<title confidence="0.99805">the Challenge of Entity Recognition</title>
<author confidence="0.998382">Eva Sourjikova Ekbal</author>
<author confidence="0.998382">Anette Frank Paolo</author>
<affiliation confidence="0.9999115">Department of Computational Heidelberg University,</affiliation>
<abstract confidence="0.9985061">Named Entity Recognition and Classification (NERC) is a well-studied NLP task typically focused on coarse-grained named entity (NE) classes. NERC for more fine-grained semantic NE classes has not been systematically studied. This paper quantifies the difficulty of fine-grained NERC (FG-NERC) when performed at large scale on the people domain. We apply unsupervised acquisition methods to construct a gold standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Suresh Manandhar</author>
</authors>
<title>An unsupervised method for general named entity recognition and automated concept discovery.</title>
<date>2002</date>
<booktitle>In Proc. of GWC-02.</booktitle>
<contexts>
<context position="4098" citStr="Alfonseca and Manandhar (2002)" startWordPosition="605" endWordPosition="608">ed semantic class labels; (ii) we develop strong baseline methods, to assess the aptness of standard NLP approaches for this task. The two efforts are strongly interleaved: a standardized dataset is not only essential for (comparative) evaluation, but also a prerequisite for classification approaches based on supervised learning, the most successful techniques for sequential labeling problems. 93 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93–101, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2 Related work An early approach to FG-NERC is Alfonseca and Manandhar (2002), who identify it as a problem related to Word Sense Disambiguation (WSD). They jointly address concept hierarchy learning and instance classification using topic signatures, yet the experiments are restricted to a small ontology of 9 classes. Similarly, Fleischman and Hovy (2002) extend previous work from Fleischman (2001) on locations and address the acquisition of instances for 8 fine-grained person classes. For supervised training they compile a web corpus which is filtered using high-confident classifications from an initial classifier trained on seeds. Due to the limitations of their met</context>
<context position="23753" citStr="Alfonseca and Manandhar, 2002" startWordPosition="3823" endWordPosition="3826">e paucity of examples for each class, and the increasingly subtle semantic differences between these classes. For such a task contextual information is expected to be very informative – e.g. if an entity co-occurs in context with ’Nobel prize’, this provides evidence that it is likely to be a scientist or scholar. However, the context window used by our baseline MaxEnt tagger is very local, including at most the two preceding and succeeding words. Hence, the classifier is not able to capture informative contextual clues in a larger context. Previous work has related FG-NERC to WSD approaches (Alfonseca and Manandhar, 2002). Accordingly, we investigate two context-sensitive approaches inspired from WSD proposals, which consider a more global context for classification. We first define a new feature set to induce a new MaxEnt model (MaxEnt-B) which only uses lexical features from a larger context window, as used in standard supervised WSD (Lee and Ng, 2002): 1. PoS context. The part-of-speech occurring within the context window posi+3 i−3 = posi−3 ... posi+3. 2. Local collocation. Local collocations C,,,m surrounding wi. We use C−2,−1 and C1,2. 3. Content words in surrounding context. We consider all unigrams in </context>
</contexts>
<marker>Alfonseca, Manandhar, 2002</marker>
<rawString>Enrique Alfonseca and Suresh Manandhar. 2002. An unsupervised method for general named entity recognition and automated concept discovery. In Proc. of GWC-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ananiadou</author>
<author>C Friedman</author>
<author>J I Tsujii</author>
</authors>
<title>Special issue on named entity recognition in biomedicine.</title>
<date>2004</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>37</volume>
<issue>6</issue>
<contexts>
<context position="1583" citStr="Ananiadou et al., 2004" startWordPosition="217" endWordPosition="220">e high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically PERS, LOC, ORG, MISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers addressed the problem of recognizing and categorizing fine-grained NE classes (such as biologist, composer, or athlete) in an open-domain setting. Fine-grained NERC is expected to be beneficial for a wide spectrum of applications, including Information Retrieval (Mandl and Woms</context>
</contexts>
<marker>Ananiadou, Friedman, Tsujii, 2004</marker>
<rawString>S. Ananiadou, C. Friedman, and J.I. Tsujii. 2004. Special issue on named entity recognition in biomedicine. Journal of Biomedical Informatics, 37(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Satoshi Sekine</author>
<author>Julio Gonzalo</author>
</authors>
<title>Web people search.</title>
<date>2008</date>
<booktitle>In Proc. of LREC ’08.</booktitle>
<contexts>
<context position="8558" citStr="Artiles et al., 2008" startWordPosition="1299" endWordPosition="1302"> But, the full task of NE recognition and classification in context still remains to be addressed for a wide-coverage, finegrained semantic class inventory that can serve as a common benchmark for future research. 3 Fine-grained NERC on a large-scale We present experiments that assess the difficulty of open-domain FG-NERC pursued at a large scale. We concentrate on instances and classes referring to people, since it is a well-studied domain (see Section 2) and structured fine-grained information can be readily applied to a well-defined end-user task such as IR, cf. the Web People Search task (Artiles et al., 2008). Our method is general in that it requires only a (PoS tagged and chunked) corpus and a reference taxonomy to provide a concept hierarchy. Given a mapping between automatically extracted class labels 94 and concepts in a taxonomic resource, it can be further extended to other domains, e.g. locations or the biomedical domain by leveraging opendomain taxonomies such as Yago (Suchanek et al., 2008) or WikiTaxonomy (Ponzetto and Strube, 2007). The contribution of this work is two-fold: (i) We develop an unsupervised method for acquiring a comprehensive dataset for FG-NERC by applying linguistical</context>
</contexts>
<marker>Artiles, Sekine, Gonzalo, 2008</marker>
<rawString>Javier Artiles, Satoshi Sekine, and Julio Gonzalo. 2008. Web people search. In Proc. of LREC ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlap as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proc. of IJCAI-03,</booktitle>
<pages>805--810</pages>
<contexts>
<context position="25041" citStr="Banerjee and Pedersen (2003)" startWordPosition="4036" endWordPosition="4040">oundaries) for the entire training data. We convert tokens to lower case, remove stopwords, numbers and punctuation symbols. We define a feature vector of length 10 using the 10 most frequent content words. Given a classification instance, the feature corresponding to token t is set to 1 iff the context wi+3 i−3of wi contains t. In addition, we use a Lesk-like method (Lesk, 1986) which labels instances in context with the WordNet synset whose gloss has the maximum overlap with the glosses of the senses of its words in context. Given the small context provided by the WordNet glosses, we follow Banerjee and Pedersen (2003) and expand these to also include the words from the glosses of the hypernym and hyponym synsets. 6 Experiments 6.1 Benchmarking on coarse-grained NERC We benchmark the performance of our baseline MaxEnt classifier using the feature set from Section 5.1 (MaxEnt-A henceforth) on the CoNLL2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), the de-facto standard for evaluating coarse-grained NERC systems. In MaxEnt modeling, feature selection is a crucial problem and key to improving classification performance. MaxEnt, however, does not provide methods for automatic feature selection.</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlap as a measure of semantic relatedness. In Proc. of IJCAI-03, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: A collection of very large linguistically processed web-crawled corpora.</title>
<date>2009</date>
<journal>Journal of Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>3</issue>
<contexts>
<context position="11731" citStr="Baroni et al., 2009" startWordPosition="1803" endWordPosition="1806"> as given in (2). (1) ... writings of the abstract painter Kandinsky frequently explored similarities between ... (2) a. [thelThe]? [JJINN]* [NN] [NP] the abstract painter Kandinsky b. [NP] [,]? [alanithe]* [JJINN]* [NN] W. Kandinsky, a Russian-born painter,.. Contexts like (2.a) provide a less noisy sequence for extraction, due to the class and instance labels being adjacent – in contrast to (2.b) where any number of modifiers can intervene between the two. Accordingly, we apply in our experiments only a restricted version of (2.a) – with a determiner – to UKWAC, an English web-based corpus (Baroni et al., 2009) that comes in a cleaned, PoS-tagged and lemmatized form. Due to its size (&gt;2 billion tokens) and mixed genres, the corpus is ideally suited for acquiring large quantities of NEs pertaining to a broad variety of open-domain semantic classes. Filtering heuristics. The apposition patterns are subject to noise, due to PoS-tagging errors, as well as special constructions, e.g. reduced relative clauses. The former can be controlled by frequency filters, the latter can be circumvented by using chunk boundary information1. A more challenging problem is to recognize whether an extracted nominal is in </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: A collection of very large linguistically processed web-crawled corpora. Journal of Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Bender</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Maximum entropy models for named entity recognition.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL-03,</booktitle>
<pages>148--151</pages>
<contexts>
<context position="18723" citStr="Bender et al., 2003" startWordPosition="2989" endWordPosition="2992">erent levels of semantic class granularity, i.e. corresponding to the depth of the semantic classes found in our WordNet fragment. We start in Section 5.1 to present a Maximum Entropy model to perform coarse-grained NERC and we extend it to perform multiclass classification in a hierarchical taxonomy (Section 5.2). We then present in Section 5.3 an alternative proposal to perform FGNERC using global context information, as found in state-of-the-art approaches to supervised and unsupervised WSD. 5.1 NERC using a MaxEnt tagger Our baseline system is modeled following a Maximum Entropy approach (Bender et al., 2003, inter alia). The MaxEnt model produces a probability for each class label t (the NE tag) of a classification instance, conditioned on its context of occurrence h. This probability is calculated by: � � 1n P(t |h) = Z(h) exp �Ajfj(h, t) �(1) j=1 where fj(h, t) is the j-th feature with associated weight Aj and Z(h) is a normalization constant to ensure a proper probability distribution.3 Given a word wi to be classified as Beginning, Inside or Outside (IOB) of a NE, we extract as features: 1. Context words. The words occurring within the context window wi+2 i−2 = wi−2 ... wi+2. 2. Word prefix </context>
<context position="26367" citStr="Bender et al. (2003)" startWordPosition="4252" endWordPosition="4255">5.1). Model parameters are computed with 200 iterations without feature frequency cutoff. The best configuration is found by optimizing the F1 measure on the development data with various feature representations. The chosen features are: 1, 2 (with n = 3), 4, 5, 6, 7 and 8. Evaluation on the test set is performed blindly, using this feature set. The results are presented in Table 2. The MaxEnt labeler achieves performance comparable with the CoNLL-2003 task participants, ranking 121h among the 16 systems participating in the task, with a 2 point margin off the F1 of the most similar system of Bender et al. (2003) and 7 points below the best-performing system (Florian et al., 2003). The former used a relatively complex set of features and different gazetteers extracted from unannotated data. The latter combined four diverse classifiers, namely a robust linear classifier, maximum entropy, transformationbased learning and a hidden Markov model. They used different feature sets, unannotated data and an additional NE tagger. In comparison, our NERC system is simpler and based on a small set of features that can be easily obtained for many languages. Besides, it does not make use of any external resources a</context>
</contexts>
<marker>Bender, Och, Ney, 2003</marker>
<rawString>Oliver Bender, Franz Josef Och, and Hermann Ney. 2003. Maximum entropy models for named entity recognition. In Proc. of CoNLL-03, pages 148–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proc. of EACL-06,</booktitle>
<pages>9--16</pages>
<marker>Bunescu, Pas¸ca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pas¸ca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proc. of EACL-06, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1349" citStr="Etzioni et al., 2005" startWordPosition="182" endWordPosition="185">dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically PERS, LOC, ORG, MISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers addressed the problem of recognizing and cat</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-02,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="4379" citStr="Fleischman and Hovy (2002)" startWordPosition="647" endWordPosition="650">ation approaches based on supervised learning, the most successful techniques for sequential labeling problems. 93 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93–101, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2 Related work An early approach to FG-NERC is Alfonseca and Manandhar (2002), who identify it as a problem related to Word Sense Disambiguation (WSD). They jointly address concept hierarchy learning and instance classification using topic signatures, yet the experiments are restricted to a small ontology of 9 classes. Similarly, Fleischman and Hovy (2002) extend previous work from Fleischman (2001) on locations and address the acquisition of instances for 8 fine-grained person classes. For supervised training they compile a web corpus which is filtered using high-confident classifications from an initial classifier trained on seeds. Due to the limitations of their method to create a good sample of training data, the performance could not be generalized to held-out data. Recent work takes the task of FG-NERC one step further by (i) extending the number of classes, (ii) relating them to reference concept hierarchies and (iii) exploring methods f</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>Michael Fleischman and Eduard Hovy. 2002. Fine grained classification of named entities. In Proc. of COLING-02, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
</authors>
<title>Automated subcategorization of named entities.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL 2001 Student Workshop.</booktitle>
<contexts>
<context position="4423" citStr="Fleischman (2001)" startWordPosition="655" endWordPosition="657">st successful techniques for sequential labeling problems. 93 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93–101, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2 Related work An early approach to FG-NERC is Alfonseca and Manandhar (2002), who identify it as a problem related to Word Sense Disambiguation (WSD). They jointly address concept hierarchy learning and instance classification using topic signatures, yet the experiments are restricted to a small ontology of 9 classes. Similarly, Fleischman and Hovy (2002) extend previous work from Fleischman (2001) on locations and address the acquisition of instances for 8 fine-grained person classes. For supervised training they compile a web corpus which is filtered using high-confident classifications from an initial classifier trained on seeds. Due to the limitations of their method to create a good sample of training data, the performance could not be generalized to held-out data. Recent work takes the task of FG-NERC one step further by (i) extending the number of classes, (ii) relating them to reference concept hierarchies and (iii) exploring methods for building training and evaluation data, or</context>
</contexts>
<marker>Fleischman, 2001</marker>
<rawString>Michael Fleischman. 2001. Automated subcategorization of named entities. In Proc. of the ACL 2001 Student Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Tong Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL03,</booktitle>
<pages>168--171</pages>
<contexts>
<context position="26436" citStr="Florian et al., 2003" startWordPosition="4263" endWordPosition="4267">ure frequency cutoff. The best configuration is found by optimizing the F1 measure on the development data with various feature representations. The chosen features are: 1, 2 (with n = 3), 4, 5, 6, 7 and 8. Evaluation on the test set is performed blindly, using this feature set. The results are presented in Table 2. The MaxEnt labeler achieves performance comparable with the CoNLL-2003 task participants, ranking 121h among the 16 systems participating in the task, with a 2 point margin off the F1 of the most similar system of Bender et al. (2003) and 7 points below the best-performing system (Florian et al., 2003). The former used a relatively complex set of features and different gazetteers extracted from unannotated data. The latter combined four diverse classifiers, namely a robust linear classifier, maximum entropy, transformationbased learning and a hidden Markov model. They used different feature sets, unannotated data and an additional NE tagger. In comparison, our NERC system is simpler and based on a small set of features that can be easily obtained for many languages. Besides, it does not make use of any external resources and still shows state-of-the-art performance on the overall data. 98 R</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang. 2003. Named entity recognition through classifier combination. In Proc. of CoNLL03, pages 168–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Instance based lexical entailment for ontology population.</title>
<date>2007</date>
<booktitle>In Proc. of ACL-07,</booktitle>
<pages>248--256</pages>
<contexts>
<context position="5386" citStr="Giuliano &amp; Gliozzo (2007" startWordPosition="803" endWordPosition="806">uld not be generalized to held-out data. Recent work takes the task of FG-NERC one step further by (i) extending the number of classes, (ii) relating them to reference concept hierarchies and (iii) exploring methods for building training and evaluation data, or applying weakly and unsupervised learning based on high-volume data. Tanev and Magnini (2006) selected 10 NEsubclasses of person and location using WordNet as a reference. Datasets were automatically acquired and manually filtered. They compare word and pattern-based supervised and a semisupervised approach based on syntactic features. Giuliano &amp; Gliozzo (2007, 2008) perform NE classification against the People Ontology, an excerpt of the WordNet hierarchy, comprising 21 people classes populated with at least 40 instances. Using minimally supervised lexical substitution methods, they cast NE classification as an ontology population task – as opposed to recognition and classification in context. In a similar setting, Giuliano (2009) explores semi-supervised classification of the People Ontology classes using latent semantic kernels, comparing models built from Wikipedia and from a news corpus. In a different line of research Pas¸ca (2007) and Pas¸ca</context>
</contexts>
<marker>Giuliano, Gliozzo, 2007</marker>
<rawString>Claudio Giuliano and Alfio Gliozzo. 2007. Instance based lexical entailment for ontology population. In Proc. of ACL-07, pages 248–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Instancebased ontology population exploiting named-entity substitution.</title>
<date>2008</date>
<booktitle>In Proc. of COLING-ACL-08,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="17033" citStr="Giuliano and Gliozzo, 2008" startWordPosition="2712" endWordPosition="2715">19 extracted instances (type-level) sit in leaf nodes. The classes automatically refer back to the acquired appositional contexts. Table 1 gives statistics about the number of instances (token-level) acquired for classes at different embedding levels. In total we have at our disposal 48,512 instances (token-level) in appositional contexts. The type-token ratio is 1.52. Gold standard validation. To create a gold standard dataset of entities in context labeled with fine-grained classes, we first randomly select 20 classes, as well as an additional 18 which are also found in the People Ontology (Giuliano and Gliozzo, 2008). For each class, we randomly select 40 occurrences of instances in context, i.e. the words co-occurring in a window of 60 tokens before and after the instance. We asked four annotators to label these extractions for correctness, and to provide the correct label for the incorrect cases, if one was available. Only 52 contexts out of 1520 were labeled as incorrect, thus giving us 96.58% accuracy on our automatically extracted data. The manually validated dataset is used to provide a ground-truth for FG-NERC. However, the noun (e.g. hunter) denoting the NE class is removed from these contexts for</context>
</contexts>
<marker>Giuliano, Gliozzo, 2008</marker>
<rawString>Claudio Giuliano and Alfio Gliozzo. 2008. Instancebased ontology population exploiting named-entity substitution. In Proc. of COLING-ACL-08, pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
</authors>
<title>Fine-grained classification of named entities exploiting latent semantic kernels.</title>
<date>2009</date>
<booktitle>In Proc. of CoNLL-09,</booktitle>
<pages>201--209</pages>
<contexts>
<context position="5765" citStr="Giuliano (2009)" startWordPosition="861" endWordPosition="862">on and location using WordNet as a reference. Datasets were automatically acquired and manually filtered. They compare word and pattern-based supervised and a semisupervised approach based on syntactic features. Giuliano &amp; Gliozzo (2007, 2008) perform NE classification against the People Ontology, an excerpt of the WordNet hierarchy, comprising 21 people classes populated with at least 40 instances. Using minimally supervised lexical substitution methods, they cast NE classification as an ontology population task – as opposed to recognition and classification in context. In a similar setting, Giuliano (2009) explores semi-supervised classification of the People Ontology classes using latent semantic kernels, comparing models built from Wikipedia and from a news corpus. In a different line of research Pas¸ca (2007) and Pas¸ca and van Durme (2008) make use of query logs to acquire NEs on a large scale. While Pas¸ca (2007) extracts NEs for 10 target classes, Pas¸ca and van Durme (2008) combine web query logs and web documents to acquire both NE-concept pairs and concept attributes using seeds. But while these more recent approaches all offer substantially novel contributions for many NE acquisition </context>
<context position="33515" citStr="Giuliano (2009)" startWordPosition="5475" endWordPosition="5476">ce decreases considerably for a classifier that not only assigns fine-grained classes, but also detects which tokens actually are NEs. As for the classification-only task, the performance decreases as one moves to lower levels. This indicates that the complexity of the task is proportional to the fine-grainedness of the class inventory. MaxEnt-B, lacking ’classical’ NER features, shows dramatic losses, compared to MaxEnt-A. Comparison to other work. We compared the performance of our system based on global classification (one vs. rest) against the figures reported for individual categories in Giuliano (2009). The MaxEnt-A system compares favorably, although it considers (i) more classes at each level – i.e. 213 vs. 21 – and (ii) classifies NEs at finer-grained levels – i.e. 8 vs. 4 maximum depth in the respective WordNet fragments. We achieve overall micro average R, P and F1 values of 87.5%, 85.7% and 86.6%, respectively, compared to Giuliano’s 79.6%, 80.9% and 80.2%. Due to the different setups and data used, these figures do not offer a basis for true comparison. However, the figures suggest that our system achieves respectable performance on a more complex classification problem. 7 Conclusion</context>
</contexts>
<marker>Giuliano, 2009</marker>
<rawString>Claudio Giuliano. 2009. Fine-grained classification of named entities exploiting latent semantic kernels. In Proc. of CoNLL-09, pages 201–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of Support Vector Machines for chunk identification.</title>
<date>2000</date>
<booktitle>In Proc. of CoNLL-00,</booktitle>
<pages>142--144</pages>
<contexts>
<context position="13224" citStr="Kudo and Matsumoto, 2000" startWordPosition="2048" endWordPosition="2051">r class, to remove infrequent class label extractions; we then filter invalid semantic classes using information from WordNet: given the WordNet PERSON supersense, i.e. the lexicographer file for nouns denoting people, we check whether the first sense of the class label candidate is found in PERSON. Mapping to the WordNet person domain. In order to perform a hierarchical classification of people, we need a taxonomy for the domain at hand. We achieve this by mapping the extracted class labels to WordNet synsets. In our setting, we map against all synsets found under person#n#1, 1We use YamCha (Kudo and Matsumoto, 2000) to perform phrase chunking. 95 which are direct hypernyms of at least one instance in WordNet (CWN pers+Inst).2 Since our goal is to map class labels to synsets (i.e. our future NE classes), we check each class label candidate against all synonyms contained in the synset. At this point we have to deal with two cases: two extracted class label candidates (synonyms such as doctor, physician) will map to a single synset, while ambiguous class labels (e.g. director) can be mapped to more than one synset. In the latter case, we heuristically choose the synset which dominates the highest number of </context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Use of Support Vector Machines for chunk identification. In Proc. of CoNLL-00, pages 142–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP-02,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="24092" citStr="Lee and Ng, 2002" startWordPosition="3876" endWordPosition="3879">our baseline MaxEnt tagger is very local, including at most the two preceding and succeeding words. Hence, the classifier is not able to capture informative contextual clues in a larger context. Previous work has related FG-NERC to WSD approaches (Alfonseca and Manandhar, 2002). Accordingly, we investigate two context-sensitive approaches inspired from WSD proposals, which consider a more global context for classification. We first define a new feature set to induce a new MaxEnt model (MaxEnt-B) which only uses lexical features from a larger context window, as used in standard supervised WSD (Lee and Ng, 2002): 1. PoS context. The part-of-speech occurring within the context window posi+3 i−3 = posi−3 ... posi+3. 2. Local collocation. Local collocations C,,,m surrounding wi. We use C−2,−1 and C1,2. 3. Content words in surrounding context. We consider all unigrams in contexts wi+3 i−3 = wi−3 ... wi+3 of wi (crossing sentence boundaries) for the entire training data. We convert tokens to lower case, remove stopwords, numbers and punctuation symbols. We define a feature vector of length 10 using the 10 most frequent content words. Given a classification instance, the feature corresponding to token t is</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proc. of EMNLP-02, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the ACL-SIGDOC Conference,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="24795" citStr="Lesk, 1986" startWordPosition="3997" endWordPosition="3998"> ... posi+3. 2. Local collocation. Local collocations C,,,m surrounding wi. We use C−2,−1 and C1,2. 3. Content words in surrounding context. We consider all unigrams in contexts wi+3 i−3 = wi−3 ... wi+3 of wi (crossing sentence boundaries) for the entire training data. We convert tokens to lower case, remove stopwords, numbers and punctuation symbols. We define a feature vector of length 10 using the 10 most frequent content words. Given a classification instance, the feature corresponding to token t is set to 1 iff the context wi+3 i−3of wi contains t. In addition, we use a Lesk-like method (Lesk, 1986) which labels instances in context with the WordNet synset whose gloss has the maximum overlap with the glosses of the senses of its words in context. Given the small context provided by the WordNet glosses, we follow Banerjee and Pedersen (2003) and expand these to also include the words from the glosses of the hypernym and hyponym synsets. 6 Experiments 6.1 Benchmarking on coarse-grained NERC We benchmark the performance of our baseline MaxEnt classifier using the feature set from Section 5.1 (MaxEnt-A henceforth) on the CoNLL2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), th</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the ACL-SIGDOC Conference, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mandl</author>
<author>Christa Womser-Hacker</author>
</authors>
<title>The effect of named entities on effectiveness in cross-language information retrieval evaluation.</title>
<date>2005</date>
<booktitle>In Proc. of ACM SAC</booktitle>
<pages>1059--1064</pages>
<marker>Mandl, Womser-Hacker, 2005</marker>
<rawString>Thomas Mandl and Christa Womser-Hacker. 2005. The effect of named entities on effectiveness in cross-language information retrieval evaluation. In Proc. of ACM SAC 2005, pages 1059–1064.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Andrew Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, features induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL-03,</booktitle>
<pages>188--191</pages>
<contexts>
<context position="1312" citStr="McCallum and Li, 2003" startWordPosition="177" endWordPosition="180">ld standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically PERS, LOC, ORG, MISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers address</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Andrew Li. 2003. Early results for named entity recognition with conditional random fields, features induction and web-enhanced lexicons. In Proc. of CoNLL-03, pages 188–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
<author>Philip Resnik</author>
</authors>
<title>Tagger evaluation given hierarchical tag sets. Computers and the Humanities,</title>
<date>2000</date>
<pages>79--84</pages>
<contexts>
<context position="30766" citStr="Melamed and Resnik (2000" startWordPosition="5002" endWordPosition="5005">ion of the MaxEnt model performing both classification and recognition. As for coarse-grained NERC, we evaluate using the standard metrics of recall (R), precision (P) and balanced F-measure (F1). As baseline, we use a majority class assignment – i.e. at each level, we label all instances with the most frequent class label. For global FG-NE classification, reported in Table 5, the original fine-grained classes are considered, across the entire class hierarchy. Global evaluation is performed by counting exact label predictions on the entire hierarchy (global) and using the evaluation metric of Melamed and Resnik (2000, hierarchical). As baseline we assume the most frequent class label in the training set. Discussion. All methods perform reasonably well, indicating the feasibility of the task. For the MaxEnt models, Table 5 shows a general high recall and decreasing precision as we move down the hierarchy. Degradation in the overall F1 score is 5Sampling ratios are determined on the development set. 99 Level R Baseline Fl R MaxEnt-A Fl R MaxEnt-B Fl R Lesk Fl P P P P 1 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 2 28.4 25.9 27.1 85.8 88.6 87.0 79.5 84.9 82.2 16.4 19.7 17.9 3 27.9</context>
</contexts>
<marker>Melamed, Resnik, 2000</marker>
<rawString>Dan Melamed and Philip Resnik. 2000. Tagger evaluation given hierarchical tag sets. Computers and the Humanities, pages 79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, Cal.</location>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7). Morgan Kaufmann, San Francisco, Cal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Journal of Linguisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1131" citStr="Nadeau and Sekine (2007)" startWordPosition="151" endWordPosition="154">ed. This paper quantifies the difficulty of fine-grained NERC (FG-NERC) when performed at large scale on the people domain. We apply unsupervised acquisition methods to construct a gold standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Journal of Linguisticae Investigationes, 30(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Benjamin van Durme</author>
</authors>
<title>Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-08,</booktitle>
<pages>pages</pages>
<marker>Pas¸ca, van Durme, 2008</marker>
<rawString>Marius Pas¸ca and Benjamin van Durme. 2008. Weakly-supervised acquisition of open-domain classes and class attributes from web documents and query logs. In Proc. of ACL-08, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and similarities on the web: Fact extraction in the fast lane.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL06,</booktitle>
<pages>809--816</pages>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006a. Names and similarities on the web: Fact extraction in the fast lane. In Proc. of COLING-ACL06, pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Dekang Lin</author>
<author>Jeffrey Bigham</author>
<author>Andrei Lifchits</author>
<author>Alpa Jain</author>
</authors>
<title>Organizing and searching the world wide web of facts – Step one: The one-million fact extraction challenge.</title>
<date>2006</date>
<booktitle>In Proc. ofAAAI-06,</booktitle>
<pages>1400--1405</pages>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>Marius Pas¸ca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits, and Alpa Jain. 2006b. Organizing and searching the world wide web of facts – Step one: The one-million fact extraction challenge. In Proc. ofAAAI-06, pages 1400–1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
</authors>
<title>Weakly-supervised discovery of named entities using web search queries.</title>
<date>2007</date>
<booktitle>In Proc. of CIKM-2007,</booktitle>
<pages>683--690</pages>
<marker>Pas¸ca, 2007</marker>
<rawString>Marius Pas¸ca. 2007. Weakly-supervised discovery of named entities using web search queries. In Proc. of CIKM-2007, pages 683–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luiz Augusto Pizzato</author>
<author>Diego Molla</author>
<author>C´ecile Paris</author>
</authors>
<title>Pseudo relevance feedback using named entities for question answering.</title>
<date>2006</date>
<booktitle>In Proc. ofALTW-2006,</booktitle>
<pages>83--90</pages>
<contexts>
<context position="2290" citStr="Pizzato et al., 2006" startWordPosition="323" endWordPosition="326">lished in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically PERS, LOC, ORG, MISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers addressed the problem of recognizing and categorizing fine-grained NE classes (such as biologist, composer, or athlete) in an open-domain setting. Fine-grained NERC is expected to be beneficial for a wide spectrum of applications, including Information Retrieval (Mandl and WomserHacker, 2005), Information Extraction (Pas¸ca et al., 2006a) or Question-Answering (Pizzato et al., 2006). However, manually compiling widecoverage gazetteers for fine-grained NE classes is time-consuming and error-prone. Also, without an extrinsic evaluation, it is difficult to define a priori which classes are relevant for a particular domain or task. Finally, prior research in FG-NERC is difficult to evaluate, due to the diversity of NE classes and datasets used. Accordingly, in the interest of a general approach, we address the challenge of capturing a broad range of NE classes at various levels of conceptual granularity. By turning FG-NERC into a widely applicable task, applications are free</context>
</contexts>
<marker>Pizzato, Molla, Paris, 2006</marker>
<rawString>Luiz Augusto Pizzato, Diego Molla, and C´ecile Paris. 2006. Pseudo relevance feedback using named entities for question answering. In Proc. ofALTW-2006, pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Deriving a large scale taxonomy from Wikipedia.</title>
<date>2007</date>
<booktitle>In Proc. of AAAI-07,</booktitle>
<pages>1440--1445</pages>
<contexts>
<context position="9001" citStr="Ponzetto and Strube, 2007" startWordPosition="1371" endWordPosition="1374">domain (see Section 2) and structured fine-grained information can be readily applied to a well-defined end-user task such as IR, cf. the Web People Search task (Artiles et al., 2008). Our method is general in that it requires only a (PoS tagged and chunked) corpus and a reference taxonomy to provide a concept hierarchy. Given a mapping between automatically extracted class labels 94 and concepts in a taxonomic resource, it can be further extended to other domains, e.g. locations or the biomedical domain by leveraging opendomain taxonomies such as Yago (Suchanek et al., 2008) or WikiTaxonomy (Ponzetto and Strube, 2007). The contribution of this work is two-fold: (i) We develop an unsupervised method for acquiring a comprehensive dataset for FG-NERC by applying linguistically motivated patterns to a corpus harvested from the Web (Section 4). Large amounts of NEs are acquired together with their contexts of occurrence and with their fine-grained class labels which are mapped to synsets in WordNet. The controlled sense inventory and the taxonomic structure offered by WordNet enables an evaluation of FG-NERC performance at different levels of concept granularity, as given by the depth at which the concepts are </context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007. Deriving a large scale taxonomy from Wikipedia. In Proc. of AAAI-07, pages 1440–1445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A Large Ontology from Wikipedia and WordNet.</title>
<date>2008</date>
<journal>Elsevier Journal of Web Semantics,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="8957" citStr="Suchanek et al., 2008" startWordPosition="1365" endWordPosition="1368">g to people, since it is a well-studied domain (see Section 2) and structured fine-grained information can be readily applied to a well-defined end-user task such as IR, cf. the Web People Search task (Artiles et al., 2008). Our method is general in that it requires only a (PoS tagged and chunked) corpus and a reference taxonomy to provide a concept hierarchy. Given a mapping between automatically extracted class labels 94 and concepts in a taxonomic resource, it can be further extended to other domains, e.g. locations or the biomedical domain by leveraging opendomain taxonomies such as Yago (Suchanek et al., 2008) or WikiTaxonomy (Ponzetto and Strube, 2007). The contribution of this work is two-fold: (i) We develop an unsupervised method for acquiring a comprehensive dataset for FG-NERC by applying linguistically motivated patterns to a corpus harvested from the Web (Section 4). Large amounts of NEs are acquired together with their contexts of occurrence and with their fine-grained class labels which are mapped to synsets in WordNet. The controlled sense inventory and the taxonomic structure offered by WordNet enables an evaluation of FG-NERC performance at different levels of concept granularity, as g</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A Large Ontology from Wikipedia and WordNet. Elsevier Journal of Web Semantics, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hristo Tanev</author>
<author>Bernardo Magnini</author>
</authors>
<title>Weakly supervised approaches for ontology population.</title>
<date>2006</date>
<booktitle>In Proc. of EACL-06,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="5117" citStr="Tanev and Magnini (2006)" startWordPosition="763" endWordPosition="766">ned person classes. For supervised training they compile a web corpus which is filtered using high-confident classifications from an initial classifier trained on seeds. Due to the limitations of their method to create a good sample of training data, the performance could not be generalized to held-out data. Recent work takes the task of FG-NERC one step further by (i) extending the number of classes, (ii) relating them to reference concept hierarchies and (iii) exploring methods for building training and evaluation data, or applying weakly and unsupervised learning based on high-volume data. Tanev and Magnini (2006) selected 10 NEsubclasses of person and location using WordNet as a reference. Datasets were automatically acquired and manually filtered. They compare word and pattern-based supervised and a semisupervised approach based on syntactic features. Giuliano &amp; Gliozzo (2007, 2008) perform NE classification against the People Ontology, an excerpt of the WordNet hierarchy, comprising 21 people classes populated with at least 40 instances. Using minimally supervised lexical substitution methods, they cast NE classification as an ontology population task – as opposed to recognition and classification i</context>
</contexts>
<marker>Tanev, Magnini, 2006</marker>
<rawString>Hristo Tanev and Bernardo Magnini. 2006. Weakly supervised approaches for ontology population. In Proc. of EACL-06, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL-03,</booktitle>
<pages>127--132</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-independent Named Entity Recognition. In Proc. of CoNLL-03, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 Shared Task: Language-independent Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-02,</booktitle>
<pages>155--158</pages>
<contexts>
<context position="1482" citStr="Sang, 2002" startWordPosition="203" endWordPosition="204">tual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically PERS, LOC, ORG, MISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers addressed the problem of recognizing and categorizing fine-grained NE classes (such as biologist, composer, or athlete) in an open-domain setting. Fine-grained NERC is expected </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 Shared Task: Language-independent Named Entity Recognition. In Proc. of CoNLL-02, pages 155–158.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>