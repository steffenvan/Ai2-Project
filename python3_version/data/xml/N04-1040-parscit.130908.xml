<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000188">
<title confidence="0.997519">
Multiple Similarity Measures and Source-Pair Information
in Story Link Detection
</title>
<author confidence="0.968542">
Francine Chen Ayman Farahat
</author>
<affiliation confidence="0.981419">
Palo Alto Research Center
</affiliation>
<address confidence="0.74314">
3333 Coyote Hill Rd.
Palo Alto, CA 94304
</address>
<email confidence="0.932508">
fchen, farahat @parc.com, thorsten@brants.net
</email>
<author confidence="0.351851">
Thorsten Brants
</author>
<sectionHeader confidence="0.97402" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913375">
State-of-the-art story link detection systems,
that is, systems that determine whether two sto-
ries are about the same event or linked, are usu-
ally based on the cosine-similarity measured
between two stories. This paper presents a
method for improving the performance of a link
detection system by using a variety of simi-
larity measures and using source-pair specific
statistical information. The utility of a num-
ber of different similarity measures, including
cosine, Hellinger, Tanimoto, and clarity, both
alone and in combination, was investigated.
We also compared several machine learning
techniques for combining the different types
of information. The techniques investigated
were SVMs, voting, and decision trees, each
of which makes use of similarity and statisti-
cal information differently. Our experimental
results indicate that the combination of similar-
ity measures and source-pair specific statistical
information using an SVM provides the largest
improvement in estimating whether two stories
are linked; the resulting system was the best-
performing link detection system at TDT-2002.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919896551724">
Story link detection, as defined in the Topic Detection and
Tracking (TDT) competition sponsored by the DARPA
TIDES program, is the task of determining whether two
stories, such as news articles and/or radio broadcasts, are
about the same event, or linked. In TDT an event is de-
fined as “something that happens at some specific time
and place” (TDT, 2002). For example, a story about a tor-
nado in Kansas in May and another story about a tornado
in Nebraska in June should not be classified as linked be-
cause they are about different events, although they both
fall under the same general “topic” of natural disasters.
But a story about damage due to a tornado in Kansas and
a story about the clean-up and repairs due to the same tor-
nado in Kansas are considered linked events.
In the TDT link detection task, a link detection sys-
tem is given a sequence of time-ordered sets of stories,
where each set is from one news source. The system can
“look ahead” N source files from the current source file
being processed when deciding whether the current pair
is linked. Because the TDT link detection task is focused
on streams of news stories, one of the primary differences
between link detection and the more traditional IR catego-
rization task is that new events occur relatively frequently
and comparisons of interest are focused on events that are
not known in advance. One consequence of this is that the
best-performing systems usually adapt to new input. Link
detection is thought of as the basis for other event-based
topic analysis tasks, such as topic tracking, topic detec-
tion, and first-story detection (TDT, 2002).
</bodyText>
<sectionHeader confidence="0.890578" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999854">
The DARPA TDT story link detection task requires iden-
tifying pairs of linked stories. The original language of
the stories are in English, Mandarin and Arabic. The
sources include broadcast news and newswire. For the
required story link detection task, the research groups
tested their systems on a processed version of the data in
which the story boundaries have been manually identified,
the Arabic and Mandarin stories have been automatically
translated to English, and the broadcast news stories have
been converted to text by an automatic speech recognition
(ASR) system.
A number of research groups have developed story
link detection systems. The best current technology for
link detection relies on the use of cosine similarity be-
tween document terms vectors with TF-IDF term weight-
ing. In a TF-IDF model, the frequency of a term in a docu-
ment (TF) is weighted by the inverse document frequency
(IDF), the inverse of the number of documents containing
a term. UMass (Allan et al., 2000) has examined a num-
ber of similarity measures in the link detection task, in-
cluding weighted sum, language modeling and Kullback-
Leibler divergence, and found that the cosine similarity
produced the best results. More recently, in Lavrenko et
al. (2002), UMass found that the clarity similarity mea-
sure performed best for the link detection task. In this
paper, we also examine a number of similarity measures,
both separately, as in Allan et al. (2000), and in combina-
tion. In the machine learning field, classifier combination
has been shown to provide accuracy gains (e.g., Belkin et
al.(1995); Kittler et al. (1998); Brill and Wu (1998); Di-
etterich (2000)). Motivated by the performance improve-
ment observed in these studies, we explored the combina-
tion of similarity measures for improving Story Link De-
tection.
CMU hypothesized that the similarity between a pair
of stories is influenced by the source of each story. For
example, sources in a language that is translated to En-
glish will consistently use the same terminology, result-
ing in greater similarity between linked documents with
the same native language. In contrast, sources from radio
broadcasts may be transcribed much less consistently than
text sources due to recognition errors, so that the expected
similarity of a radio broadcast and a text source is less than
that of two text sources. They found that similarity thresh-
olds that were dependent on the type of the story-pair
sources (e.g., English/non-English language and broad-
cast news/newswire) improved story-link detection re-
sults by 15% (Carbonell et al., 2001). We also investigate
how to make use of differences in similarity that are de-
pendent on the types of sources composing a story pair.
We refer to the statistics characterizing story pairs with the
same source types as source-pair specific information. In
contrast to the source-specific thresholds used by CMU,
we normalize the similarity measures based on the source-
pair specific information, simultaneously with combining
different similarity measures.
Other researchers have successfully used machine
learning algorithms such as support vector machines
(SVM) (Cristianini and Shawe-Taylor, 2000; Joachims,
1998) and boosted decision stumps (Schapire and Singer,
2000) for text categorization. SVM-based systems, such
as that described in (Joachims,1998), are typically among
the best performers for the categorization task. However,
attempts to directly apply SVMs to TDT tasks such as
tracking and link detection have not been successful; this
has been attributed in part to the lack of enough data for
training the SVMI. In these systems, the input was the
set of term vectors characterizing each document, similar
to the input used for the categorization task. In this pa-
</bodyText>
<footnote confidence="0.9448385">
1http://www.ldc.upenn.edu/Projects/TDT3/email/email 402.
html, accessed Mar 11, 2004.
</footnote>
<bodyText confidence="0.9999569">
per, we present a method for using SVMs to improve link
detection performance by combining heterogeneous in-
put features, composed of multiple similarity metrics and
statistical characterization of the story sources. We addi-
tionally examine the utility of the statistical information
by comparing against decision trees, where the statistical
characterization is not utilized. We also examine the util-
ity of the similarity values by comparing against voting,
where the classification based on each similarity measure
is combined.
</bodyText>
<sectionHeader confidence="0.983573" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.873726">
To determine whether two documents are linked, state-of-
the-art link detection systems perform three primary pro-
cessing steps:
</bodyText>
<listItem confidence="0.926979">
1. preprocessing to create a normalized set of terms
for representing each document as a vector of term
counts, or term vector
2. adapting model parameters (i.e., IDF) as new story
sets are introduced and computing the similarity of
the term vectors
3. determining whether a pair of stories are linked
based on the similarity score.
</listItem>
<bodyText confidence="0.999880388888889">
In this paper, we describe our investigations in improv-
ing the basic story link detection systems by using source
specific information and combining a number of similar-
ity measures. As in the basic story link detection system, a
similarity score between two stories is computed. In con-
trast to the basic story link detection system, a variety of
similarity measures is computed and the prediction mod-
els use source-pair-specific statistics (i.e., median, aver-
age, and variance of the story pair similarity scores). We
do this in a post-processing step using machine learning
classifiers (i.e., SVMs, decision trees, or voting) to pro-
duce a decision with an associated confidence score as to
whether a pair of stories are linked. Source-pair-specific
statistics and multiple similarity measures are used as in-
put features to the machine learning based techniques in
post-processing the similarity scores. In the next sections,
we describe the components and processing performed by
our system.
</bodyText>
<subsectionHeader confidence="0.998683">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999743">
For preprocessing, we tokenize the data, remove stop-
words, replace spelled-out numbers by digits, replace the
tokens by their stems using the Inxight LinguistX mor-
phological analyzer, and then generate a term-frequency
vector to represent each story. For text where the original
source is Mandarin, some of the terms are untranslated.
In our experiments, we retain these terms because many
are content words. Both the training data and test data are
preprocessed in the same way.
</bodyText>
<subsectionHeader confidence="0.89481">
3.1.1 Stop Words
</subsectionHeader>
<bodyText confidence="0.999985636363636">
Our base stoplist is composed of 577 terms. We extend
the stoplist with terms that are represented differently by
ASR systems and text documents. For example, in the
broadcast news documents in the TDT collection “30” is
spelled out as “thirty” and “CNN” is represented as three
separate tokens “C”, “N”, and “N”. To handle these differ-
ences, an “ASR stoplist” was automatically created. Chen
et al. (2003) found that the use of an enhanced stoplist,
formed from the union of a base stoplist and ASR stoplist,
was very effective in improving performance and empir-
ically better than normalizing ASR abbreviations.
</bodyText>
<subsectionHeader confidence="0.60877">
3.1.2 Source-specific Incremental TF-IDF Model
</subsectionHeader>
<bodyText confidence="0.998548961538461">
The training data is used to compute the initial docu-
ment frequency over the corpus for each term. The docu-
ment frequency of term, is defined to be:
Separate document term counts, , and document
counts, , are computed for each type of source.
Our similarity calculations of documents are based on
an incremental TF-IDF model. Term vectors are created
for each story, and the vectors are weighted by the in-
verse document frequency, IDF, i.e., . In the in-
cremental model, and are updated with each new
set of stories in a source file. When the set of test
documents, , is added to the model, the document term
counts are updated as:
where denotes the document count for termin the
newly added set of documents . The initial document
counts were generated from a training set. In a static
TF-IDF model, new words (i.e., those words, that did not
occur in the training set) are ignored in further computa-
tions. An incremental TF-IDF model uses the new vocab-
ulary in similarity calculations, which is an advantage for
the TDT task because new events often contain new vo-
cabulary.
Since very low frequency termstend to be uninfor-
mative, we set a threshold such that only terms with
are used with sources up through . For
these experiments, we used .
</bodyText>
<subsectionHeader confidence="0.802686">
3.1.3 Term Weighting
</subsectionHeader>
<bodyText confidence="0.999046">
The document frequencies, , the number of doc-
uments containing term, and document term frequen-
cies, , are used to calculate TF-IDF based weights
for the terms in a document (or story):
where is the total number of documents and is
a normalization value. For the Hellinger, Tanimoto, and
clarity measures, it is computed as:
For cosine distance it is computed as:
</bodyText>
<subsectionHeader confidence="0.999538">
3.2 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.99988440625">
In addition to the cosine similarity measure used in base-
line systems, we compute story pair similarity over a set
of measures, motivated by the accuracy gains obtained
by others when combining classifiers (see Section 2). A
vector composed of the similarity values is created and
is given to a trained classifier, which emits a score. The
score can be used as a measure of confidence that the story
pairs are linked.
The similarity measures that we examined are cosine,
Hellinger, Tanimoto, and clarity. Each of the measures
captures a different aspect of the similarity of the terms
in a document. Classifier combination has been observed
to perform best when the classifiers produce independent
judgments. The cosine distance between the word distri-
bution for documents andis computed as:
This measure has been found to perform well and was
used by all the TDT 2002 link detection systems (unpub-
lished presentations at the TDT2002 workshop).
In contrast to the Euclidean distance based cosine mea-
sure, the Hellinger measure is a probabilistic measure.
The Hellinger measure between the word distributions for
documents andis computed as:
whereranges over the terms that occur inor. In
Brants et al. (2002), the Hellinger measure was used in a
text segmentation application and was found to be supe-
rior to the cosine similarity.
The Tanimoto (Duda and Hart, 1973) measure is a mea-
sure of the ratio of the number of shared terms between
two documents to the number possessed by one document
only. We modified it to use frequency counts, instead of a
binary indicator as to whether a term is present and com-
puted it as:
</bodyText>
<equation confidence="0.495841">
(1)
</equation>
<bodyText confidence="0.998029714285714">
The clarity measure was introduced by Croft et al.
(2001) and shown to improve link detection performance
by Lavrenko et al. (2002). It gets its name from the dis-
tance to general English, which is called Clarity. We used
a symmetric version that is computed as:
where is the probability distribution of words for
“general English” as derived from the training corpus,
and KL is the Kullback-Leibler divergence:
In computing the clarity measure, the
✳
term frequencies were smoothed with the General English
model using a weight of 0.01. This enables the KL diver-
gence to be defined when or is 0. The
idea behind the clarity measure is to give credit to similar
pairs of documents with term distributions that are very
different from general English, and to discount similar
pairs of documents with term distributions that are close
to general English, which can be interpreted as being non-
topical.
We also defined the “source-pair normalized cosine”
distance as the cosine distance normalized by dividing by
the running median of the similarity values corresponding
to the source-pair:
where is the running median of the
similarity values of all processed story pairs where the
source of is the same asand the source of is the
same as. This is a finer-grained use of source pair infor-
mation than what was used by CMU, which used decision
thresholds conditioned on whether or not the sources were
cross-language or cross-ASR/newswire (Carbonell et al.,
2001).
In a base system employing a single similarity measure,
the system computes the similarity measure for each story
pair, which is given to the evaluation program (see Sec-
tion 4.2).
</bodyText>
<subsectionHeader confidence="0.999754">
3.3 Improving Story Link Detection Performance
</subsectionHeader>
<bodyText confidence="0.998848638888889">
We examined a number of methods for improving link de-
tection, including:
compare the 5 similarity measures alone
combine subsets of similarity scores using a support
vector machine (SVM)
combine source-pair statistics with the correspond-
ing similarity score using an SVM, for each of the 5
similarity measures
combine subsets of similarity scores with source pair
information using an SVM
compare SVMs, decision trees, and majority voting
as alternative methods for combining scores
In contrast to earlier attempts that applied the machine
learning categorization paradigm of using the term vec-
tors as input features (Joachims, 1998) to the link detec-
tion task, we believed that the use of document term vec-
tors is too fine-grained for the SVMs to develop good gen-
eralization with a limited amount of labeled training data.
Furthermore, the use of terms as input to a learner, as was
done in the categorization task (see Section 2), would re-
quire frequent retraining of a link detection system since
new stories often discuss new topics and introduce new
terms. For our work, we used more general character-
istics of a document pair, the similarity between a pair
of documents, as input to the machine learning systems.
Thus, in contrast to the term-based systems, the machine
learning techniques are used in a post-processing step af-
ter the similarity scores are computed. Additionally, to
normalize differences in expected similarity among pairs
of source types, source-pair statistics are used as features
in deciding whether two stories are linked and in estimat-
ing the confidence of the decision.
In the next sections, we describe our methods for
combining the similarity scores using machine learning
techniques, and for combining the similarity scores with
source-pair specific information.
</bodyText>
<subsectionHeader confidence="0.870542">
3.3.1 Combining Similarity Scores with SVMs
</subsectionHeader>
<bodyText confidence="0.9998673">
We used an SVM to combine sets of similarity mea-
sures for predicting whether two stories are linked be-
cause theoretically it has good generalization properties
(Cristianini and Shawe-Taylor, 2000), it has been shown
to be a competitive classifier for a variety of tasks (e.g.,
(Cristianini and Shawe-Taylor, 2000; Gestal et al., 2000),
and it makes full use of the similarity scores and statistical
characterizations. We also empirically show in Section
4.3.2 that it provides better performance than decision
trees and voting for this task. The SVM is first trained on
a set of labeled data where the input features are the sets of
similarity measures and the class labels are the manually
assigned decisions as to whether a pair of documents are
linked. The trained model is then used to automatically
decide whether a new pair of stories are linked. For the
support vector machine, we used SVM-light (Joachims,
1999). A polynomial kernel was used in all the reported
SVM experiments. In addition to making a decision as to
whether two stories are linked, we use the value of the de-
cision function produced by SVM-light as a measure of
</bodyText>
<tableCaption confidence="0.995864">
Table 1: Source Pair Groups
</tableCaption>
<table confidence="0.999345714285714">
asr:asr asr:text text:text
English:English a b c
English:Arabic d e f
English:Mandarin g h i
Arabic:Arabic j k l
Arabic:Mandarin m n o
Mandarin:Mandarin p q r
</table>
<bodyText confidence="0.999159125">
confidence, which serves as input to the evaluation pro-
gram.
Training SVM-light on a 20,000 story-pair training cor-
pus usually requires less than five minutes on a 1.8 GHz
Linux machine, although the time is quite variable de-
pending on the corpus characteristics. However, once the
system is trained, testing new story pair similarities re-
quires less than 1 min for over 20,000 story pairs.
</bodyText>
<subsectionHeader confidence="0.909225">
3.3.2 Source-Pair Specific Information
</subsectionHeader>
<bodyText confidence="0.999737941176471">
Source-pair-specific information that statistically char-
acterizes each of the similarity measures is used in a post-
processing step. In particular, we compute statistics from
the training data similarity scores for different combina-
tions of source modalities and languages. The modal-
ity pairs that we considered are: asr:asr, asr:text, and
text:text, where asr represents “automatic speech recog-
nition”. The combinations of languages that we used
are: English:English, English:Arabic, English:Mandarin,
Arabic:Arabic, Arabic:Mandarin, Mandarin:Mandarin.
The rows of Table 1 represent possible combinations
of source language for the story pairs; the columns rep-
resent different combinations of source modality. The al-
phabetic characters in the cells represent the pair similar-
ity statistics of mean, median, and variance for that con-
dition obtained from the training corpus. For conditions
where training data was not available, we used the statis-
tics of a coarser grouping. For example, if there is no data
for the cell with languages Mandarin:Arabic and modal-
ity pair asr:asr, we would use statistics from the language
pair non-English:non-English and modality pair asr:asr.
Prior to use in link detection, an SVM is trained on a set
of features computed for each story pair. These include
the similarity measures described in Section 3.2 and cor-
responding source-pair specific statistics (average, me-
dian and variance) for the similarity measures. The mo-
tivation for using the statistical values is to inform the
SVM about the type of source pairs that are being con-
sidered. Rather than using categorical labels, the source-
pair statistics provide a natural ordering to the source-pair
types and can be used for normalization. When a new pair
of stories is post-processed, the computed similarity mea-
sures and the corresponding source-pair statistics are used
as input to the trained SVM.
</bodyText>
<sectionHeader confidence="0.605884" genericHeader="method">
3.3.3 Other Methods for Combining Similarity
Scores
</sectionHeader>
<bodyText confidence="0.9998692">
In addition to SVMs, we investigated the utility of de-
cision trees (Breiman et al., 1984) and majority voting
(Kittler et al., 1998) as techniques to combine similarity
measures and statistical information in a post-processing
step. The simplest method that we examined for combin-
ing similarity scores is to create a separate classifier for
each similarity measure and then classify based a combi-
nation of the votes of the different classifiers (Kittler et
al., 1998). This method does not utilize statistical infor-
mation. The single measure classifiers use an empirically
determined threshold based on training data.
Decision trees and SVMs are classifiers that use the
similarity scores directly. Decision trees such as C4.5
easily handle categorical data. In our experiments, we
noted that although source-pair specific statistics were
used as an input feature to the decision tree, the decision
trees treated the source-pair based statistical information
as categorical features. For the decision trees we used
the WEKA implementation of C4.5 (Witten and Frank,
1999).
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999937">
We conducted a set of experiments to compare the utility
of combining similarity measures and the use of normal-
ization statistics. We also compared the utility of different
statistical learners.
</bodyText>
<subsectionHeader confidence="0.967954">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.997943285714286">
For our studies, we used corpora developed by the LDC
for the TDT tasks (Cieri et al, 2003). The TDT3 corpus
contains 40,000 news articles and broadcast news stories
with 120 labeled events in English, Mandarin and Ara-
bic from October through December 1998. For our com-
parative evaluations, we initialized the document term
counts and document counts using the TDT2 data (from
TDT 1998). Our “post-processing” system was trained
on the TDT3pub partition of the TDT2001 story pairs,
and tested on the TDT3unp partition of the TDT2001
test story pairs. The source-pair statistics were computed
from the linked story pairs in the TDT2002 dry run test
set. There are 20,966, 27,541, and 20,191 labeled story
pairs in TDT3pub, TDT2002 dry run, and TDT3unp, re-
spectively. The preprocessing and similarity computa-
tions do not require training, although adaptation is per-
formed by incrementally updating the document counts,
document frequencies, and the source-specific similari-
ties, and using the updated values in the computations.
The training data is used to compute similarity data for
training the post-processing systems.
</bodyText>
<tableCaption confidence="0.734265">
Table 2: Topic-weighted Min Detection Cost for Different
Systems
</tableCaption>
<figure confidence="0.742256">
system min DET
A 0.2368
B 0.3439
C 0.3175
D 0.2606
E 0.2342
</figure>
<subsectionHeader confidence="0.955613">
4.2 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.9985690625">
The goal of link detection is to minimize the cost, or
penalty, due to errors by the system. The TDT tasks are
evaluated by computing a “detection cost”:
where is the cost of a miss, is the estimated
probability of a miss, is the prior probability that
a pair of stories are linked, is the cost of a false
alarm, is the estimated probability of a false alarm,
and is the prior probability that a pair of sto-
ries are not linked. A miss occurs when a linked story pair
is not identified as linked by the system. A false alarm oc-
curs when a pair of stories that are not linked are identified
as linked by the system. A target is a pair of linked sto-
ries; conversely, a non-target is a pair of stories that are
not linked. For the link detection task these parameters
are set as follows: is 1.0, is 0.02, and
is 0.1. The cost for each topic is equally weighted (i.e.,
the cost is topic-weighted, rather than story-weighted)and
normalized so that for a given system, “ can
be no less than one without extracting information from
the source data” (TDT, 2002):
and where
the sum is over topics. A detection curve (DET curve)
is computed by sweeping a threshold over the range of
scores, and the minimum cost over the DET curve is iden-
tified as the minimum detection cost or min DET. The
topic-weighted DET cost, or score, is dependent on both
a good minimum cost over the DET curve, and a good
method for selecting an operating point, which is usually
implemented by selecting a threshold. A system with a
very low min DET score can have a much larger topic-
weighted DET score. Therefore, we focus on the mini-
mum DET score for our experiments.
</bodyText>
<subsectionHeader confidence="0.85451">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.9718035">
We present results comparing the utility of different
similarity measures and use of source-pair statistics, as
</bodyText>
<tableCaption confidence="0.758736666666667">
Table 3: Topic-weighted Min Detection Cost: Combined
Similarity Measures and Source-Pair Specific Informa-
tion (baseline system performance shown in bold)
</tableCaption>
<table confidence="0.991753111111111">
similarity measures used min DET Cost
source-pair info used?
no yes
cos 0.2801 0.2532
normcos 0.2732 0.2533
Hel 0.3216 0.2657
Tan 0.3008 0.2748
cla 0.2706 0.2496
cos, Hel 0.2791 0.2467
normcos, cla 0.2631 0.2462
cos, normcos, cla 0.2626 0.2430
Hel, normcos, Tan 0.2714 0.2429
cos, normcos, Hel 0.2725 0.2421
cos, Hel, Tan, cla 0.2615 0.2452
cos, normcos, Hel, Tan 0.2736 0.2418
cos, normcos, Hel, cla 0.2614 0.2431
cos, normcos, Tan, cla 0.2623 0.2431
cos, normcos, Hel, Tan, cla 0.2608 0.2431
</table>
<bodyText confidence="0.999748227272727">
well as different learners for combining the similarity
measures and source-pair statistics. Our system was the
best performing Link Detection system at TDT2002.
We cannot compare our results with the other TDT2002
Link Detection systems because participants in TDT
agree not to publish results from another site. We did
not participate in TDT2001, but can compare our best
system on the TDT2001 test data (TDT3unp) against
the results of other TDT2001 systems (we extracted
the Primary Link Detection results from slides from the
TDT 2001 workshop, available (as of Mar 11, 2004) at:
http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt2001/Pape
rPres/Nist-pres/NIST-presentation-v6 files/v3 document
.htm. These results are shown in Table 2. The minimum
cost for our system, , is less (better) than that of the
TDT2001 systems. For this comparison, we set the bias
parameter to 0.2, which reflects the expected number of
linked stories computed from the training data. For the
following comparative results, we set the bias parameter
to reflect the probability of a linked story specified in the
task definition (TDT2002), which resulted in a somewhat
higher cost.
</bodyText>
<subsectionHeader confidence="0.968606">
4.3.1 Comparison of Similarity Measures and
Utility of Source-Pair Information
</subsectionHeader>
<bodyText confidence="0.999727813559322">
In this section, the effect of combining similarity met-
rics using an SVM and the effect of using source-pair
information is examined. The results in Table 3 are di-
vided into four sections. The upper sections show perfor-
mance as measured by min DET for single similarity met-
rics and the lower sections show performance for com-
bined similarity metrics using an SVM. The columns la-
beled “source-pair info used?” indicate whether source-
pair specific statistics were used as input features to the
SVM. The cosine, normalized cosine, Hellinger, Tani-
moto, and clarity similarity measures are represented as
“cos”, “normcos”, “Hel”, “Tan”, and “cla”, respectively.
The baseline model for comparison is the normalized co-
sine similarity without source-pair information (bolded),
which is very similar to the most successful story link de-
tection models (Carbonell et al., 2001; Allan et al., 2002).
To assess whether the observed differences were signifi-
cant, we compared models at the .005 significance level
using a paired two-sided t-test where the data was ran-
domly partitioned into 10 mutually exclusive sets.
In the upper sections, note that the clarity measure
with source-pair specific statistics exhibits the best per-
formance of the five measures, and is competitive with the
combination measures in the lower right of the table. Note
that the best-performing combination (italicized) did not
include clarity, which may be due in part to redundancy
with the other measures (Kittler et al., 1998). Compared
to the normalized cosine performance of 0.2732, the im-
proved performance of the cosine and normalized cosine
measures when source-pair specific information is used
(0.2532 and 0.2533, respectively; p .005 for both com-
parisons) indicates that simple threshold normalization by
the running mean is not optimal.
Comparison of the upper and lower sections of the table
indicates that combination of similarity measures gener-
ally yields somewhat improved performance over single
similarity link detection systems; the difference between
the best upper model vs the best lower model, i.e., “cla”’
vs “cos, normcos, Hel, Tan, cla” without source-pair info,
and “cla” vs “cos, normcos, Hel, Tan” with source-pair
info, was significant at p .005 for both comparisons.
And comparison of the left and right sections of the ta-
ble indicates that the use of source-pair specific statistics
noticeably improves performance (all models significant
at p .005). The lower right section of Table 3 shows a
generally modest improvement over the best single metric
(i.e., clarity) when using a combination of features with
source-pair information.
These results indicate that although combination of
similarity measures improves performance, the use of
source-pair specific statistics are a larger factor in im-
proving performance. The SVM effectively uses the the
source-pair information to normalize and combine the
scores. Once the scores have been normalized, for some
measures there is little additional information to be gained
from adding additional features, although the combina-
tion of at least two measures removes the necessity of se-
lecting the “best” measure. For reference to the IR met-
rics of precision and recall, we present the results for a
</bodyText>
<tableCaption confidence="0.9070225">
Table 4: Precision and Recall: Combined Similarity Mea-
sures and Source-Pair Specific Information
</tableCaption>
<table confidence="0.993588">
similarity measures used source pair precision recall
info used?
cos no 87.45 85.33
cla yes 87.07 88.06
cos, Hel yes 88.83 86.75
cos, normcos, Hel, Tan yes 88.37 87.17
</table>
<tableCaption confidence="0.944766">
Table 5: Topic-weighted Min Detection Cost: Different
Learners for Combining Similarities
</tableCaption>
<bodyText confidence="0.310593857142857">
models
similarity measures used voting decision SVM
tree
cos, normcos, Hel 0.2802 0.2708 0.2421
cos, normcos, Hel, Tan 0.2810 0.2516 0.2418
cos, normcos, Hel, Tan, cla 0.2632 0.2574 0.2431
selected number of conditions in Table 4.
</bodyText>
<subsectionHeader confidence="0.947176">
4.3.2 Comparison of Combination Models
</subsectionHeader>
<bodyText confidence="0.999972310344827">
We also investigated the use of other methods for com-
bining similarity measures and using source-pair specific
information. Table 5 compares the performance ofvoting,
a C4.5 decision tree, and an SVM. Three sets of similarity
measures were compared: 1) cosine, normalized cosine,
and Hellinger, 2) cosine, normalized cosine, Hellinger,
and Tanimoto (the best performing system in Table 3) and
3) the full set of similarity measures. All the SVM sys-
tems show significant improved performance at p .005
over the baseline normalized cosine model, which had a
cost of 0.2732 (Table 3); only one of the decision trees
was significantly better at p .05. The poorer perfor-
mance of voting compared to the baseline may be due in
part to dependencies among the different measures. None
of the decision tree systems were significantly better than
voting at p .05; in comparison, the performance of all
SVMs were significantly better than the corresponding
voting and tree models at p .005. The voting systems
did not use any source-pair information. The decision
trees used source-pair information categorically, but did
not make use of source-pair statistics. The SVMs used the
source-pair statistics, plus categorical source-pair infor-
mation as input features. Thus, the performance of these
systems tends to support the hypothesis that source-pair
information, and more specifically, source-pair similar-
ity statistics, contains useful information for the link de-
tection task. That is, the statistics not only differentiate
the source pairs, but provide additional information to the
classifier.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999992772727273">
We have presented a set of enhancements for improving
story link detection over the best baseline systems. The
enhancements include the combination of different sim-
ilarity scores and statistical characterization of source-
pair information using machine learning techniques. We
observed that the use of statistical characterization of
source-pair information had a larger effect in improving
the performance of our system than the specific set of sim-
ilarity measures used. Comparing different methods for
combining similarity scores and source-pair information,
we observed that simple voting did not always provide
improvement over the best cosine similarity based sys-
tem, decision trees tended to provide better performance,
and SVMs provided the best performance of all combina-
tion methods evaluated. Our method can be used as post-
processing to the methods developedby other researchers,
such as topic-specific models, to create a system with
even better performance. Our investigations have fo-
cused on one collection drawn from broadcast news and
newswire stories in three languages; experiments on a va-
riety of collections would allow for assessment of our re-
sults more generally.
</bodyText>
<sectionHeader confidence="0.999018" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999362243902439">
James Allan, Victor Lavrenko, Daniella Malin, and Rus-
sell Swan. 2000. Detections, Bounds, and Timelines:
UMass and TDT-3. In Proceedings of Topic Detection
and Tracking Workshop (TDT-3), Vienna, Virginia.
James Allan, Victor Lavernko, and Ramesh Nallapti.
2002. UMass at TDT 2002. Proceedings of the Topic
Detection &amp; Tracking Workshop.
Nicholas J. Belkin, Paul B. Kantor, Edward A. Fox, and
J.A. Shaw. 1995. Combining the Evidence of Multiple
Query Representations for Information Retrieval. In-
formation Processing and Management, 33:3, pp. 431-
448.
Thorsten Brants, Francine Chen, and Ioannis Tsochan-
taridis. 2002. Topic-Based Document Segmentation
with Probabilistic Latent Semantic analysis. In Interna-
tional conference on Information and Knowledge Man-
agement (CIKM), McLean, VA, pp. 211-218.
Leo Breiman, Jerome H. Friedman, Richard A. Olshen,
and Charles J. Stone. 1984. Classification and Regres-
sion Trees, Wasworth International Group.
Eric Brill and Jun Wu. 1998. Classifier Combination for
Improved Lexical Disambiguation. In Proceedings of
COLING/ACL. pp. 191-195.
Jaime Carbonell, Yiming Yang, Ralf Brown,
Chun Jin and Jian Zhang. 2001. CMU TDT
Report. Slides at the TDT-2001 Meeting.
http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt2001/
paperpres.htm (and select the CMU presentation)
Francine Chen, Ayman Farahat and Thorsten Brants.
2003. Story Link Detection and New Even Detection
are Asymmetic. Proceedings of HLT-NAACL 2003,
Companion Volume, pp. 13-15.
Christopher Cieri, David Graff, Nii Martey,
and Stephanie Strassel. 2003. The TDT-3
Text and Speech Corpus. Proceedings Topic
Detection and Tracking Workshop, 2000.
http://www.itl.nist.gov/iaui/894.01/tests/tdt/research
links/index.htm
Nello Cristianini and John Shawe-Taylor. 2000. Support
Vector Machines, Cambridge University Press, Cam-
bridge, U.K.
W. Bruce Croft, Stephen Cronon-Townsend, and Victor
Lavrenko. 2001. Relevance Feedback and Personaliza-
tion: A Language Modeling Perspective. In DELOS
Workshop: Personalization and Recommender Sys-
tems in Digital Libraries, pp. 49-54.
Thomas G. Dietterich. 2000. Ensemble Methods in Ma-
chine Learning. In Multiple Classier Systems, Cagliari,
Italy.
Richard O. Duda and Peter E. Hart. 1973. Pattern Classi-
fication and Scene Analysis, John Wiley &amp; Sons, Inc.
Tony van Gestel, Johan A.K. Suykens, Bart Baesens,
Stijn Viaene, Jan Vanthienen, Guido Dedene, Bart de
Moor and Joos Vandewalle. 2000. Benchmarking Least
Squares Support Vector Machine Classifiers. Internal
Report 00-37, ESAT-SISTA, K.U.Leuven.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. In Advances in Kernel Methods -
Support Vector Learning, B. Schlkopf and C. Burges
and A. Smola (ed.), MIT-Press.
Thorsten Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Relevant
Features. Proceedings of the European Conference on
Machine Learning (ECML), Springer, pp. 137-142.
Josef Kittler, Mohamad Hatef, Robert P.W. Duin, and Jiri
Matas. 1998. On Combining Classifiers. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
20(3), pp. 226-239.
Victor Lavrenko, James Allan, E. DeGuzman, D.
LaFlamme, V. Pollard, and S. Thomas. 2002. Rele-
vance Models for Topic Detection and Tracking. In
Proceedings ofHLT-2002, San Diego, CA.
Robert E. Schapire and Yoram Singer. 2000. BoosTexter:
A Boosting-based System for Text Categorization. Ma-
chine Learning, 39(2/3), pp. 135-168.
(TDT2002) The 2002 Topic Detection and Track-
ing Task Definition and Evaluation Plan
http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt2002/
evalplan.htm
Ian H. Witten and Eibe Frank. 1999. Data Mining: Practi-
cal Machine Learning Tools and Techniques with Java
Implementations, Morgan Kaufman.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.316685">
<title confidence="0.9936845">Multiple Similarity Measures and Source-Pair in Story Link Detection</title>
<author confidence="0.959815">Francine Chen Ayman Farahat</author>
<affiliation confidence="0.751785">Palo Alto Research</affiliation>
<address confidence="0.903861">3333 Coyote Hill Palo Alto, CA</address>
<email confidence="0.983558">fchen,farahat@parc.com,thorsten@brants.net</email>
<author confidence="0.98109">Thorsten Brants</author>
<abstract confidence="0.99974275">State-of-the-art story link detection systems, that is, systems that determine whether two stoare about the same event or are usually based on the cosine-similarity measured between two stories. This paper presents a method for improving the performance of a link detection system by using a variety of similarity measures and using source-pair specific statistical information. The utility of a number of different similarity measures, including cosine, Hellinger, Tanimoto, and clarity, both alone and in combination, was investigated. We also compared several machine learning techniques for combining the different types of information. The techniques investigated were SVMs, voting, and decision trees, each of which makes use of similarity and statistical information differently. Our experimental results indicate that the combination of similarity measures and source-pair specific statistical information using an SVM provides the largest improvement in estimating whether two stories are linked; the resulting system was the best-</abstract>
<note confidence="0.512671">performing link detection system at TDT-2002.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Victor Lavrenko</author>
<author>Daniella Malin</author>
<author>Russell Swan</author>
</authors>
<title>Detections, Bounds, and Timelines: UMass and TDT-3.</title>
<date>2000</date>
<booktitle>In Proceedings of Topic Detection and Tracking Workshop (TDT-3),</booktitle>
<location>Vienna, Virginia.</location>
<contexts>
<context position="4007" citStr="Allan et al., 2000" startWordPosition="637" endWordPosition="640">ly identified, the Arabic and Mandarin stories have been automatically translated to English, and the broadcast news stories have been converted to text by an automatic speech recognition (ASR) system. A number of research groups have developed story link detection systems. The best current technology for link detection relies on the use of cosine similarity between document terms vectors with TF-IDF term weighting. In a TF-IDF model, the frequency of a term in a document (TF) is weighted by the inverse document frequency (IDF), the inverse of the number of documents containing a term. UMass (Allan et al., 2000) has examined a number of similarity measures in the link detection task, including weighted sum, language modeling and KullbackLeibler divergence, and found that the cosine similarity produced the best results. More recently, in Lavrenko et al. (2002), UMass found that the clarity similarity measure performed best for the link detection task. In this paper, we also examine a number of similarity measures, both separately, as in Allan et al. (2000), and in combination. In the machine learning field, classifier combination has been shown to provide accuracy gains (e.g., Belkin et al.(1995); Kit</context>
</contexts>
<marker>Allan, Lavrenko, Malin, Swan, 2000</marker>
<rawString>James Allan, Victor Lavrenko, Daniella Malin, and Russell Swan. 2000. Detections, Bounds, and Timelines: UMass and TDT-3. In Proceedings of Topic Detection and Tracking Workshop (TDT-3), Vienna, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Victor Lavernko</author>
<author>Ramesh Nallapti</author>
</authors>
<title>UMass at TDT</title>
<date>2002</date>
<booktitle>Proceedings of the Topic Detection &amp; Tracking Workshop.</booktitle>
<contexts>
<context position="27638" citStr="Allan et al., 2002" startWordPosition="4434" endWordPosition="4437">ty metrics and the lower sections show performance for combined similarity metrics using an SVM. The columns labeled “source-pair info used?” indicate whether sourcepair specific statistics were used as input features to the SVM. The cosine, normalized cosine, Hellinger, Tanimoto, and clarity similarity measures are represented as “cos”, “normcos”, “Hel”, “Tan”, and “cla”, respectively. The baseline model for comparison is the normalized cosine similarity without source-pair information (bolded), which is very similar to the most successful story link detection models (Carbonell et al., 2001; Allan et al., 2002). To assess whether the observed differences were significant, we compared models at the .005 significance level using a paired two-sided t-test where the data was randomly partitioned into 10 mutually exclusive sets. In the upper sections, note that the clarity measure with source-pair specific statistics exhibits the best performance of the five measures, and is competitive with the combination measures in the lower right of the table. Note that the best-performing combination (italicized) did not include clarity, which may be due in part to redundancy with the other measures (Kittler et al.</context>
</contexts>
<marker>Allan, Lavernko, Nallapti, 2002</marker>
<rawString>James Allan, Victor Lavernko, and Ramesh Nallapti. 2002. UMass at TDT 2002. Proceedings of the Topic Detection &amp; Tracking Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas J Belkin</author>
<author>Paul B Kantor</author>
<author>Edward A Fox</author>
<author>J A Shaw</author>
</authors>
<date>1995</date>
<booktitle>Combining the Evidence of Multiple Query Representations for Information Retrieval. Information Processing and Management, 33:3,</booktitle>
<pages>431--448</pages>
<marker>Belkin, Kantor, Fox, Shaw, 1995</marker>
<rawString>Nicholas J. Belkin, Paul B. Kantor, Edward A. Fox, and J.A. Shaw. 1995. Combining the Evidence of Multiple Query Representations for Information Retrieval. Information Processing and Management, 33:3, pp. 431-448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Francine Chen</author>
<author>Ioannis Tsochantaridis</author>
</authors>
<title>Topic-Based Document Segmentation with Probabilistic Latent Semantic analysis.</title>
<date>2002</date>
<booktitle>In International conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>211--218</pages>
<location>McLean, VA,</location>
<contexts>
<context position="12896" citStr="Brants et al. (2002)" startWordPosition="2056" endWordPosition="2059">a document. Classifier combination has been observed to perform best when the classifiers produce independent judgments. The cosine distance between the word distribution for documents andis computed as: This measure has been found to perform well and was used by all the TDT 2002 link detection systems (unpublished presentations at the TDT2002 workshop). In contrast to the Euclidean distance based cosine measure, the Hellinger measure is a probabilistic measure. The Hellinger measure between the word distributions for documents andis computed as: whereranges over the terms that occur inor. In Brants et al. (2002), the Hellinger measure was used in a text segmentation application and was found to be superior to the cosine similarity. The Tanimoto (Duda and Hart, 1973) measure is a measure of the ratio of the number of shared terms between two documents to the number possessed by one document only. We modified it to use frequency counts, instead of a binary indicator as to whether a term is present and computed it as: (1) The clarity measure was introduced by Croft et al. (2001) and shown to improve link detection performance by Lavrenko et al. (2002). It gets its name from the distance to general Engli</context>
</contexts>
<marker>Brants, Chen, Tsochantaridis, 2002</marker>
<rawString>Thorsten Brants, Francine Chen, and Ioannis Tsochantaridis. 2002. Topic-Based Document Segmentation with Probabilistic Latent Semantic analysis. In International conference on Information and Knowledge Management (CIKM), McLean, VA, pp. 211-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
<author>Jerome H Friedman</author>
<author>Richard A Olshen</author>
<author>Charles J Stone</author>
</authors>
<title>Classification and Regression Trees,</title>
<date>1984</date>
<institution>Wasworth International Group.</institution>
<contexts>
<context position="20625" citStr="Breiman et al., 1984" startWordPosition="3303" endWordPosition="3306"> variance) for the similarity measures. The motivation for using the statistical values is to inform the SVM about the type of source pairs that are being considered. Rather than using categorical labels, the sourcepair statistics provide a natural ordering to the source-pair types and can be used for normalization. When a new pair of stories is post-processed, the computed similarity measures and the corresponding source-pair statistics are used as input to the trained SVM. 3.3.3 Other Methods for Combining Similarity Scores In addition to SVMs, we investigated the utility of decision trees (Breiman et al., 1984) and majority voting (Kittler et al., 1998) as techniques to combine similarity measures and statistical information in a post-processing step. The simplest method that we examined for combining similarity scores is to create a separate classifier for each similarity measure and then classify based a combination of the votes of the different classifiers (Kittler et al., 1998). This method does not utilize statistical information. The single measure classifiers use an empirically determined threshold based on training data. Decision trees and SVMs are classifiers that use the similarity scores </context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Classification and Regression Trees, Wasworth International Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Jun Wu</author>
</authors>
<title>Classifier Combination for Improved Lexical Disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<pages>191--195</pages>
<contexts>
<context position="4646" citStr="Brill and Wu (1998)" startWordPosition="741" endWordPosition="744">er of similarity measures in the link detection task, including weighted sum, language modeling and KullbackLeibler divergence, and found that the cosine similarity produced the best results. More recently, in Lavrenko et al. (2002), UMass found that the clarity similarity measure performed best for the link detection task. In this paper, we also examine a number of similarity measures, both separately, as in Allan et al. (2000), and in combination. In the machine learning field, classifier combination has been shown to provide accuracy gains (e.g., Belkin et al.(1995); Kittler et al. (1998); Brill and Wu (1998); Dietterich (2000)). Motivated by the performance improvement observed in these studies, we explored the combination of similarity measures for improving Story Link Detection. CMU hypothesized that the similarity between a pair of stories is influenced by the source of each story. For example, sources in a language that is translated to English will consistently use the same terminology, resulting in greater similarity between linked documents with the same native language. In contrast, sources from radio broadcasts may be transcribed much less consistently than text sources due to recognitio</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>Eric Brill and Jun Wu. 1998. Classifier Combination for Improved Lexical Disambiguation. In Proceedings of COLING/ACL. pp. 191-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Yiming Yang</author>
<author>Ralf Brown</author>
<author>Chun Jin</author>
<author>Jian Zhang</author>
</authors>
<date>2001</date>
<booktitle>CMU TDT Report. Slides at the TDT-2001 Meeting. http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt2001/ paperpres.htm (and select the CMU presentation)</booktitle>
<contexts>
<context position="5596" citStr="Carbonell et al., 2001" startWordPosition="891" endWordPosition="894">ranslated to English will consistently use the same terminology, resulting in greater similarity between linked documents with the same native language. In contrast, sources from radio broadcasts may be transcribed much less consistently than text sources due to recognition errors, so that the expected similarity of a radio broadcast and a text source is less than that of two text sources. They found that similarity thresholds that were dependent on the type of the story-pair sources (e.g., English/non-English language and broadcast news/newswire) improved story-link detection results by 15% (Carbonell et al., 2001). We also investigate how to make use of differences in similarity that are dependent on the types of sources composing a story pair. We refer to the statistics characterizing story pairs with the same source types as source-pair specific information. In contrast to the source-specific thresholds used by CMU, we normalize the similarity measures based on the sourcepair specific information, simultaneously with combining different similarity measures. Other researchers have successfully used machine learning algorithms such as support vector machines (SVM) (Cristianini and Shawe-Taylor, 2000; J</context>
<context position="14769" citStr="Carbonell et al., 2001" startWordPosition="2375" endWordPosition="2378"> general English, which can be interpreted as being nontopical. We also defined the “source-pair normalized cosine” distance as the cosine distance normalized by dividing by the running median of the similarity values corresponding to the source-pair: where is the running median of the similarity values of all processed story pairs where the source of is the same asand the source of is the same as. This is a finer-grained use of source pair information than what was used by CMU, which used decision thresholds conditioned on whether or not the sources were cross-language or cross-ASR/newswire (Carbonell et al., 2001). In a base system employing a single similarity measure, the system computes the similarity measure for each story pair, which is given to the evaluation program (see Section 4.2). 3.3 Improving Story Link Detection Performance We examined a number of methods for improving link detection, including: compare the 5 similarity measures alone combine subsets of similarity scores using a support vector machine (SVM) combine source-pair statistics with the corresponding similarity score using an SVM, for each of the 5 similarity measures combine subsets of similarity scores with source pair informa</context>
<context position="27617" citStr="Carbonell et al., 2001" startWordPosition="4430" endWordPosition="4433"> DET for single similarity metrics and the lower sections show performance for combined similarity metrics using an SVM. The columns labeled “source-pair info used?” indicate whether sourcepair specific statistics were used as input features to the SVM. The cosine, normalized cosine, Hellinger, Tanimoto, and clarity similarity measures are represented as “cos”, “normcos”, “Hel”, “Tan”, and “cla”, respectively. The baseline model for comparison is the normalized cosine similarity without source-pair information (bolded), which is very similar to the most successful story link detection models (Carbonell et al., 2001; Allan et al., 2002). To assess whether the observed differences were significant, we compared models at the .005 significance level using a paired two-sided t-test where the data was randomly partitioned into 10 mutually exclusive sets. In the upper sections, note that the clarity measure with source-pair specific statistics exhibits the best performance of the five measures, and is competitive with the combination measures in the lower right of the table. Note that the best-performing combination (italicized) did not include clarity, which may be due in part to redundancy with the other mea</context>
</contexts>
<marker>Carbonell, Yang, Brown, Jin, Zhang, 2001</marker>
<rawString>Jaime Carbonell, Yiming Yang, Ralf Brown, Chun Jin and Jian Zhang. 2001. CMU TDT Report. Slides at the TDT-2001 Meeting. http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt2001/ paperpres.htm (and select the CMU presentation)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francine Chen</author>
</authors>
<title>Ayman Farahat and Thorsten Brants.</title>
<date>2003</date>
<booktitle>Proceedings of HLT-NAACL 2003, Companion Volume,</booktitle>
<pages>13--15</pages>
<marker>Chen, 2003</marker>
<rawString>Francine Chen, Ayman Farahat and Thorsten Brants. 2003. Story Link Detection and New Even Detection are Asymmetic. Proceedings of HLT-NAACL 2003, Companion Volume, pp. 13-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Cieri</author>
<author>David Graff</author>
<author>Nii Martey</author>
<author>Stephanie Strassel</author>
</authors>
<date>2003</date>
<booktitle>The TDT-3 Text and Speech Corpus. Proceedings Topic Detection and Tracking Workshop,</booktitle>
<note>http://www.itl.nist.gov/iaui/894.01/tests/tdt/research links/index.htm</note>
<contexts>
<context position="21921" citStr="Cieri et al, 2003" startWordPosition="3503" endWordPosition="3506">xperiments, we noted that although source-pair specific statistics were used as an input feature to the decision tree, the decision trees treated the source-pair based statistical information as categorical features. For the decision trees we used the WEKA implementation of C4.5 (Witten and Frank, 1999). 4 Experiments We conducted a set of experiments to compare the utility of combining similarity measures and the use of normalization statistics. We also compared the utility of different statistical learners. 4.1 Corpora For our studies, we used corpora developed by the LDC for the TDT tasks (Cieri et al, 2003). The TDT3 corpus contains 40,000 news articles and broadcast news stories with 120 labeled events in English, Mandarin and Arabic from October through December 1998. For our comparative evaluations, we initialized the document term counts and document counts using the TDT2 data (from TDT 1998). Our “post-processing” system was trained on the TDT3pub partition of the TDT2001 story pairs, and tested on the TDT3unp partition of the TDT2001 test story pairs. The source-pair statistics were computed from the linked story pairs in the TDT2002 dry run test set. There are 20,966, 27,541, and 20,191 l</context>
</contexts>
<marker>Cieri, Graff, Martey, Strassel, 2003</marker>
<rawString>Christopher Cieri, David Graff, Nii Martey, and Stephanie Strassel. 2003. The TDT-3 Text and Speech Corpus. Proceedings Topic Detection and Tracking Workshop, 2000. http://www.itl.nist.gov/iaui/894.01/tests/tdt/research links/index.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Support Vector Machines,</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, U.K.</location>
<contexts>
<context position="6193" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="977" endWordPosition="980">sults by 15% (Carbonell et al., 2001). We also investigate how to make use of differences in similarity that are dependent on the types of sources composing a story pair. We refer to the statistics characterizing story pairs with the same source types as source-pair specific information. In contrast to the source-specific thresholds used by CMU, we normalize the similarity measures based on the sourcepair specific information, simultaneously with combining different similarity measures. Other researchers have successfully used machine learning algorithms such as support vector machines (SVM) (Cristianini and Shawe-Taylor, 2000; Joachims, 1998) and boosted decision stumps (Schapire and Singer, 2000) for text categorization. SVM-based systems, such as that described in (Joachims,1998), are typically among the best performers for the categorization task. However, attempts to directly apply SVMs to TDT tasks such as tracking and link detection have not been successful; this has been attributed in part to the lack of enough data for training the SVMI. In these systems, the input was the set of term vectors characterizing each document, similar to the input used for the categorization task. In this pa1http://www.ldc.upen</context>
<context position="17035" citStr="Cristianini and Shawe-Taylor, 2000" startWordPosition="2731" endWordPosition="2734">fferences in expected similarity among pairs of source types, source-pair statistics are used as features in deciding whether two stories are linked and in estimating the confidence of the decision. In the next sections, we describe our methods for combining the similarity scores using machine learning techniques, and for combining the similarity scores with source-pair specific information. 3.3.1 Combining Similarity Scores with SVMs We used an SVM to combine sets of similarity measures for predicting whether two stories are linked because theoretically it has good generalization properties (Cristianini and Shawe-Taylor, 2000), it has been shown to be a competitive classifier for a variety of tasks (e.g., (Cristianini and Shawe-Taylor, 2000; Gestal et al., 2000), and it makes full use of the similarity scores and statistical characterizations. We also empirically show in Section 4.3.2 that it provides better performance than decision trees and voting for this task. The SVM is first trained on a set of labeled data where the input features are the sets of similarity measures and the class labels are the manually assigned decisions as to whether a pair of documents are linked. The trained model is then used to automa</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. Support Vector Machines, Cambridge University Press, Cambridge, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bruce Croft</author>
<author>Stephen Cronon-Townsend</author>
<author>Victor Lavrenko</author>
</authors>
<title>Relevance Feedback and Personalization: A Language Modeling Perspective.</title>
<date>2001</date>
<booktitle>In DELOS Workshop: Personalization and Recommender Systems in Digital Libraries,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="13369" citStr="Croft et al. (2001)" startWordPosition="2143" endWordPosition="2146">linger measure between the word distributions for documents andis computed as: whereranges over the terms that occur inor. In Brants et al. (2002), the Hellinger measure was used in a text segmentation application and was found to be superior to the cosine similarity. The Tanimoto (Duda and Hart, 1973) measure is a measure of the ratio of the number of shared terms between two documents to the number possessed by one document only. We modified it to use frequency counts, instead of a binary indicator as to whether a term is present and computed it as: (1) The clarity measure was introduced by Croft et al. (2001) and shown to improve link detection performance by Lavrenko et al. (2002). It gets its name from the distance to general English, which is called Clarity. We used a symmetric version that is computed as: where is the probability distribution of words for “general English” as derived from the training corpus, and KL is the Kullback-Leibler divergence: In computing the clarity measure, the ✳ term frequencies were smoothed with the General English model using a weight of 0.01. This enables the KL divergence to be defined when or is 0. The idea behind the clarity measure is to give credit to simi</context>
</contexts>
<marker>Croft, Cronon-Townsend, Lavrenko, 2001</marker>
<rawString>W. Bruce Croft, Stephen Cronon-Townsend, and Victor Lavrenko. 2001. Relevance Feedback and Personalization: A Language Modeling Perspective. In DELOS Workshop: Personalization and Recommender Systems in Digital Libraries, pp. 49-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<date>2000</date>
<booktitle>Ensemble Methods in Machine Learning. In Multiple Classier Systems,</booktitle>
<location>Cagliari, Italy.</location>
<contexts>
<context position="4665" citStr="Dietterich (2000)" startWordPosition="745" endWordPosition="747">ures in the link detection task, including weighted sum, language modeling and KullbackLeibler divergence, and found that the cosine similarity produced the best results. More recently, in Lavrenko et al. (2002), UMass found that the clarity similarity measure performed best for the link detection task. In this paper, we also examine a number of similarity measures, both separately, as in Allan et al. (2000), and in combination. In the machine learning field, classifier combination has been shown to provide accuracy gains (e.g., Belkin et al.(1995); Kittler et al. (1998); Brill and Wu (1998); Dietterich (2000)). Motivated by the performance improvement observed in these studies, we explored the combination of similarity measures for improving Story Link Detection. CMU hypothesized that the similarity between a pair of stories is influenced by the source of each story. For example, sources in a language that is translated to English will consistently use the same terminology, resulting in greater similarity between linked documents with the same native language. In contrast, sources from radio broadcasts may be transcribed much less consistently than text sources due to recognition errors, so that t</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. Ensemble Methods in Machine Learning. In Multiple Classier Systems, Cagliari, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard O Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis,</title>
<date>1973</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="13053" citStr="Duda and Hart, 1973" startWordPosition="2083" endWordPosition="2086"> distribution for documents andis computed as: This measure has been found to perform well and was used by all the TDT 2002 link detection systems (unpublished presentations at the TDT2002 workshop). In contrast to the Euclidean distance based cosine measure, the Hellinger measure is a probabilistic measure. The Hellinger measure between the word distributions for documents andis computed as: whereranges over the terms that occur inor. In Brants et al. (2002), the Hellinger measure was used in a text segmentation application and was found to be superior to the cosine similarity. The Tanimoto (Duda and Hart, 1973) measure is a measure of the ratio of the number of shared terms between two documents to the number possessed by one document only. We modified it to use frequency counts, instead of a binary indicator as to whether a term is present and computed it as: (1) The clarity measure was introduced by Croft et al. (2001) and shown to improve link detection performance by Lavrenko et al. (2002). It gets its name from the distance to general English, which is called Clarity. We used a symmetric version that is computed as: where is the probability distribution of words for “general English” as derived</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Richard O. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis, John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony van Gestel</author>
<author>Johan A K Suykens</author>
<author>Bart Baesens</author>
<author>Stijn Viaene</author>
<author>Jan Vanthienen</author>
<author>Guido Dedene</author>
<author>Bart de Moor</author>
<author>Joos Vandewalle</author>
</authors>
<title>Benchmarking Least Squares Support Vector Machine Classifiers.</title>
<date>2000</date>
<tech>Internal Report 00-37, ESAT-SISTA, K.U.Leuven.</tech>
<marker>van Gestel, Suykens, Baesens, Viaene, Vanthienen, Dedene, de Moor, Vandewalle, 2000</marker>
<rawString>Tony van Gestel, Johan A.K. Suykens, Bart Baesens, Stijn Viaene, Jan Vanthienen, Guido Dedene, Bart de Moor and Joos Vandewalle. 2000. Benchmarking Least Squares Support Vector Machine Classifiers. Internal Report 00-37, ESAT-SISTA, K.U.Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods -Support Vector Learning,</booktitle>
<pages>MIT-Press.</pages>
<editor>B. Schlkopf and C. Burges and A. Smola (ed.),</editor>
<contexts>
<context position="17758" citStr="Joachims, 1999" startWordPosition="2854" endWordPosition="2855">or, 2000; Gestal et al., 2000), and it makes full use of the similarity scores and statistical characterizations. We also empirically show in Section 4.3.2 that it provides better performance than decision trees and voting for this task. The SVM is first trained on a set of labeled data where the input features are the sets of similarity measures and the class labels are the manually assigned decisions as to whether a pair of documents are linked. The trained model is then used to automatically decide whether a new pair of stories are linked. For the support vector machine, we used SVM-light (Joachims, 1999). A polynomial kernel was used in all the reported SVM experiments. In addition to making a decision as to whether two stories are linked, we use the value of the decision function produced by SVM-light as a measure of Table 1: Source Pair Groups asr:asr asr:text text:text English:English a b c English:Arabic d e f English:Mandarin g h i Arabic:Arabic j k l Arabic:Mandarin m n o Mandarin:Mandarin p q r confidence, which serves as input to the evaluation program. Training SVM-light on a 20,000 story-pair training corpus usually requires less than five minutes on a 1.8 GHz Linux machine, althoug</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-Scale SVM Learning Practical. In Advances in Kernel Methods -Support Vector Learning, B. Schlkopf and C. Burges and A. Smola (ed.), MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of the European Conference on Machine Learning (ECML),</booktitle>
<pages>137--142</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="6210" citStr="Joachims, 1998" startWordPosition="981" endWordPosition="982">). We also investigate how to make use of differences in similarity that are dependent on the types of sources composing a story pair. We refer to the statistics characterizing story pairs with the same source types as source-pair specific information. In contrast to the source-specific thresholds used by CMU, we normalize the similarity measures based on the sourcepair specific information, simultaneously with combining different similarity measures. Other researchers have successfully used machine learning algorithms such as support vector machines (SVM) (Cristianini and Shawe-Taylor, 2000; Joachims, 1998) and boosted decision stumps (Schapire and Singer, 2000) for text categorization. SVM-based systems, such as that described in (Joachims,1998), are typically among the best performers for the categorization task. However, attempts to directly apply SVMs to TDT tasks such as tracking and link detection have not been successful; this has been attributed in part to the lack of enough data for training the SVMI. In these systems, the input was the set of term vectors characterizing each document, similar to the input used for the categorization task. In this pa1http://www.ldc.upenn.edu/Projects/TD</context>
<context position="15631" citStr="Joachims, 1998" startWordPosition="2509" endWordPosition="2510">er of methods for improving link detection, including: compare the 5 similarity measures alone combine subsets of similarity scores using a support vector machine (SVM) combine source-pair statistics with the corresponding similarity score using an SVM, for each of the 5 similarity measures combine subsets of similarity scores with source pair information using an SVM compare SVMs, decision trees, and majority voting as alternative methods for combining scores In contrast to earlier attempts that applied the machine learning categorization paradigm of using the term vectors as input features (Joachims, 1998) to the link detection task, we believed that the use of document term vectors is too fine-grained for the SVMs to develop good generalization with a limited amount of labeled training data. Furthermore, the use of terms as input to a learner, as was done in the categorization task (see Section 2), would require frequent retraining of a link detection system since new stories often discuss new topics and introduce new terms. For our work, we used more general characteristics of a document pair, the similarity between a pair of documents, as input to the machine learning systems. Thus, in contr</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Proceedings of the European Conference on Machine Learning (ECML), Springer, pp. 137-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Kittler</author>
<author>Mohamad Hatef</author>
<author>Robert P W Duin</author>
<author>Jiri Matas</author>
</authors>
<title>On Combining Classifiers.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>3</issue>
<pages>226--239</pages>
<contexts>
<context position="4625" citStr="Kittler et al. (1998)" startWordPosition="737" endWordPosition="740">00) has examined a number of similarity measures in the link detection task, including weighted sum, language modeling and KullbackLeibler divergence, and found that the cosine similarity produced the best results. More recently, in Lavrenko et al. (2002), UMass found that the clarity similarity measure performed best for the link detection task. In this paper, we also examine a number of similarity measures, both separately, as in Allan et al. (2000), and in combination. In the machine learning field, classifier combination has been shown to provide accuracy gains (e.g., Belkin et al.(1995); Kittler et al. (1998); Brill and Wu (1998); Dietterich (2000)). Motivated by the performance improvement observed in these studies, we explored the combination of similarity measures for improving Story Link Detection. CMU hypothesized that the similarity between a pair of stories is influenced by the source of each story. For example, sources in a language that is translated to English will consistently use the same terminology, resulting in greater similarity between linked documents with the same native language. In contrast, sources from radio broadcasts may be transcribed much less consistently than text sour</context>
<context position="20668" citStr="Kittler et al., 1998" startWordPosition="3310" endWordPosition="3313"> motivation for using the statistical values is to inform the SVM about the type of source pairs that are being considered. Rather than using categorical labels, the sourcepair statistics provide a natural ordering to the source-pair types and can be used for normalization. When a new pair of stories is post-processed, the computed similarity measures and the corresponding source-pair statistics are used as input to the trained SVM. 3.3.3 Other Methods for Combining Similarity Scores In addition to SVMs, we investigated the utility of decision trees (Breiman et al., 1984) and majority voting (Kittler et al., 1998) as techniques to combine similarity measures and statistical information in a post-processing step. The simplest method that we examined for combining similarity scores is to create a separate classifier for each similarity measure and then classify based a combination of the votes of the different classifiers (Kittler et al., 1998). This method does not utilize statistical information. The single measure classifiers use an empirically determined threshold based on training data. Decision trees and SVMs are classifiers that use the similarity scores directly. Decision trees such as C4.5 easil</context>
<context position="28245" citStr="Kittler et al., 1998" startWordPosition="4530" endWordPosition="4533"> et al., 2002). To assess whether the observed differences were significant, we compared models at the .005 significance level using a paired two-sided t-test where the data was randomly partitioned into 10 mutually exclusive sets. In the upper sections, note that the clarity measure with source-pair specific statistics exhibits the best performance of the five measures, and is competitive with the combination measures in the lower right of the table. Note that the best-performing combination (italicized) did not include clarity, which may be due in part to redundancy with the other measures (Kittler et al., 1998). Compared to the normalized cosine performance of 0.2732, the improved performance of the cosine and normalized cosine measures when source-pair specific information is used (0.2532 and 0.2533, respectively; p .005 for both comparisons) indicates that simple threshold normalization by the running mean is not optimal. Comparison of the upper and lower sections of the table indicates that combination of similarity measures generally yields somewhat improved performance over single similarity link detection systems; the difference between the best upper model vs the best lower model, i.e., “cla”</context>
</contexts>
<marker>Kittler, Hatef, Duin, Matas, 1998</marker>
<rawString>Josef Kittler, Mohamad Hatef, Robert P.W. Duin, and Jiri Matas. 1998. On Combining Classifiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(3), pp. 226-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>James Allan</author>
<author>E DeGuzman</author>
<author>D LaFlamme</author>
<author>V Pollard</author>
<author>S Thomas</author>
</authors>
<title>Relevance Models for Topic Detection and Tracking.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT-2002,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="4259" citStr="Lavrenko et al. (2002)" startWordPosition="677" endWordPosition="680">y link detection systems. The best current technology for link detection relies on the use of cosine similarity between document terms vectors with TF-IDF term weighting. In a TF-IDF model, the frequency of a term in a document (TF) is weighted by the inverse document frequency (IDF), the inverse of the number of documents containing a term. UMass (Allan et al., 2000) has examined a number of similarity measures in the link detection task, including weighted sum, language modeling and KullbackLeibler divergence, and found that the cosine similarity produced the best results. More recently, in Lavrenko et al. (2002), UMass found that the clarity similarity measure performed best for the link detection task. In this paper, we also examine a number of similarity measures, both separately, as in Allan et al. (2000), and in combination. In the machine learning field, classifier combination has been shown to provide accuracy gains (e.g., Belkin et al.(1995); Kittler et al. (1998); Brill and Wu (1998); Dietterich (2000)). Motivated by the performance improvement observed in these studies, we explored the combination of similarity measures for improving Story Link Detection. CMU hypothesized that the similarity</context>
<context position="13443" citStr="Lavrenko et al. (2002)" startWordPosition="2155" endWordPosition="2158">ted as: whereranges over the terms that occur inor. In Brants et al. (2002), the Hellinger measure was used in a text segmentation application and was found to be superior to the cosine similarity. The Tanimoto (Duda and Hart, 1973) measure is a measure of the ratio of the number of shared terms between two documents to the number possessed by one document only. We modified it to use frequency counts, instead of a binary indicator as to whether a term is present and computed it as: (1) The clarity measure was introduced by Croft et al. (2001) and shown to improve link detection performance by Lavrenko et al. (2002). It gets its name from the distance to general English, which is called Clarity. We used a symmetric version that is computed as: where is the probability distribution of words for “general English” as derived from the training corpus, and KL is the Kullback-Leibler divergence: In computing the clarity measure, the ✳ term frequencies were smoothed with the General English model using a weight of 0.01. This enables the KL divergence to be defined when or is 0. The idea behind the clarity measure is to give credit to similar pairs of documents with term distributions that are very different fro</context>
</contexts>
<marker>Lavrenko, Allan, DeGuzman, LaFlamme, Pollard, Thomas, 2002</marker>
<rawString>Victor Lavrenko, James Allan, E. DeGuzman, D. LaFlamme, V. Pollard, and S. Thomas. 2002. Relevance Models for Topic Detection and Tracking. In Proceedings ofHLT-2002, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A Boosting-based System for Text Categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>39</volume>
<issue>2</issue>
<pages>135--168</pages>
<contexts>
<context position="6266" citStr="Schapire and Singer, 2000" startWordPosition="987" endWordPosition="990">ferences in similarity that are dependent on the types of sources composing a story pair. We refer to the statistics characterizing story pairs with the same source types as source-pair specific information. In contrast to the source-specific thresholds used by CMU, we normalize the similarity measures based on the sourcepair specific information, simultaneously with combining different similarity measures. Other researchers have successfully used machine learning algorithms such as support vector machines (SVM) (Cristianini and Shawe-Taylor, 2000; Joachims, 1998) and boosted decision stumps (Schapire and Singer, 2000) for text categorization. SVM-based systems, such as that described in (Joachims,1998), are typically among the best performers for the categorization task. However, attempts to directly apply SVMs to TDT tasks such as tracking and link detection have not been successful; this has been attributed in part to the lack of enough data for training the SVMI. In these systems, the input was the set of term vectors characterizing each document, similar to the input used for the categorization task. In this pa1http://www.ldc.upenn.edu/Projects/TDT3/email/email 402. html, accessed Mar 11, 2004. per, we</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. BoosTexter: A Boosting-based System for Text Categorization. Machine Learning, 39(2/3), pp. 135-168.</rawString>
</citation>
<citation valid="true">
<title>Topic Detection and Tracking Task Definition and Evaluation Plan</title>
<date>2002</date>
<booktitle>(TDT2002) The</booktitle>
<note>http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt2002/ evalplan.htm</note>
<contexts>
<context position="4259" citStr="(2002)" startWordPosition="680" endWordPosition="680"> systems. The best current technology for link detection relies on the use of cosine similarity between document terms vectors with TF-IDF term weighting. In a TF-IDF model, the frequency of a term in a document (TF) is weighted by the inverse document frequency (IDF), the inverse of the number of documents containing a term. UMass (Allan et al., 2000) has examined a number of similarity measures in the link detection task, including weighted sum, language modeling and KullbackLeibler divergence, and found that the cosine similarity produced the best results. More recently, in Lavrenko et al. (2002), UMass found that the clarity similarity measure performed best for the link detection task. In this paper, we also examine a number of similarity measures, both separately, as in Allan et al. (2000), and in combination. In the machine learning field, classifier combination has been shown to provide accuracy gains (e.g., Belkin et al.(1995); Kittler et al. (1998); Brill and Wu (1998); Dietterich (2000)). Motivated by the performance improvement observed in these studies, we explored the combination of similarity measures for improving Story Link Detection. CMU hypothesized that the similarity</context>
<context position="12896" citStr="(2002)" startWordPosition="2059" endWordPosition="2059">assifier combination has been observed to perform best when the classifiers produce independent judgments. The cosine distance between the word distribution for documents andis computed as: This measure has been found to perform well and was used by all the TDT 2002 link detection systems (unpublished presentations at the TDT2002 workshop). In contrast to the Euclidean distance based cosine measure, the Hellinger measure is a probabilistic measure. The Hellinger measure between the word distributions for documents andis computed as: whereranges over the terms that occur inor. In Brants et al. (2002), the Hellinger measure was used in a text segmentation application and was found to be superior to the cosine similarity. The Tanimoto (Duda and Hart, 1973) measure is a measure of the ratio of the number of shared terms between two documents to the number possessed by one document only. We modified it to use frequency counts, instead of a binary indicator as to whether a term is present and computed it as: (1) The clarity measure was introduced by Croft et al. (2001) and shown to improve link detection performance by Lavrenko et al. (2002). It gets its name from the distance to general Engli</context>
</contexts>
<marker>2002</marker>
<rawString>(TDT2002) The 2002 Topic Detection and Tracking Task Definition and Evaluation Plan http://www.itl.nist.gov/iaui/894.01/tests/tdt/tdt2002/ evalplan.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations,</booktitle>
<location>Morgan Kaufman.</location>
<contexts>
<context position="21607" citStr="Witten and Frank, 1999" startWordPosition="3451" endWordPosition="3454">ttler et al., 1998). This method does not utilize statistical information. The single measure classifiers use an empirically determined threshold based on training data. Decision trees and SVMs are classifiers that use the similarity scores directly. Decision trees such as C4.5 easily handle categorical data. In our experiments, we noted that although source-pair specific statistics were used as an input feature to the decision tree, the decision trees treated the source-pair based statistical information as categorical features. For the decision trees we used the WEKA implementation of C4.5 (Witten and Frank, 1999). 4 Experiments We conducted a set of experiments to compare the utility of combining similarity measures and the use of normalization statistics. We also compared the utility of different statistical learners. 4.1 Corpora For our studies, we used corpora developed by the LDC for the TDT tasks (Cieri et al, 2003). The TDT3 corpus contains 40,000 news articles and broadcast news stories with 120 labeled events in English, Mandarin and Arabic from October through December 1998. For our comparative evaluations, we initialized the document term counts and document counts using the TDT2 data (from </context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations, Morgan Kaufman.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>