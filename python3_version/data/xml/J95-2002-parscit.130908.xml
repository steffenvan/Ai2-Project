<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998667">
An Efficient Probabilistic Context-Free
Parsing Algorithm that Computes Prefix
Probabilities
</title>
<author confidence="0.999235">
Andreas Stolcke*
</author>
<affiliation confidence="0.990352">
University of California at Berkeley
</affiliation>
<author confidence="0.130616">
and
</author>
<affiliation confidence="0.290541">
International Computer Science Institute
</affiliation>
<bodyText confidence="0.999742916666667">
We describe an extension of Earley&apos;s parser for stochastic context-free grammars that computes the
following quantities given a stochastic context-free grammar and an input string: a) probabilities
of successive prefixes being generated by the grammar; b) probabilities of substrings being gen-
erated by the nonterminals, including the entire string being generated by the grammar; c) most
likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar
production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed
incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to
standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars
by making use of Earley&apos;s top-down control structure. It can process any context-free rule format
without conversion to some normal form, and combines computations for (a) through (d) in a
single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed
inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.
</bodyText>
<sectionHeader confidence="0.992128" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999940823529412">
Context-free grammars are widely used as models of natural language syntax. In
their probabilistic version, which defines a language as a probability distribution over
strings, they have been used in a variety of applications: for the selection of parses
for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during
parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing
(Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play
a central role in integrating low-level word models with higher-level language mod-
els (Ney 1992), as well as in non—finite-state acoustic and phonotactic modeling (Lani
and Young 1991). In some work, context-free grammars are combined with scoring
functions that are not strictly probabilistic (Nakagawa 1987), or they are used with
context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Mager-
man and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993).
Although clearly not a perfect model of natural language, stochastic context-free
grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory pro-
viding a sound theoretical basis for ranking and pruning of parses, as well as for
integration with models for nonsyntactic aspects of language. All of the applications
listed above involve (or could potentially make use of) one or more of the following
</bodyText>
<note confidence="0.8211625">
* Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park,
CA 94025. E-mail: stolcke@speech.sri.com.
C) 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.429928">
standard tasks, compiled by Jelinek and Lafferty (1991))
</bodyText>
<listItem confidence="0.996938714285714">
1. What is the probability that a given string x is generated by a grammar
G?
2. What is the single most likely parse (or derivation) for x?
3. What is the probability that x occurs as a prefix of some string generated
by G (the prefix probability of x)?
4. How should the parameters (e.g., rule probabilities) in G be chosen to
maximize the probability over a training set of strings?
</listItem>
<bodyText confidence="0.999771611111111">
The algorithm described in this article can compute solutions to all four of these
problems in a single framework, with a number of additional advantages over previ-
ously presented isolated solutions.
Most probabilistic parsers are based on a generalization of bottom-up chart pars-
ing, such as the CYK algorithm. Partial parses are assembled just as in nonprobabilistic
parsing (modulo possible pruning based on probabilities), while substring probabili-
ties (also known as &amp;quot;inside&amp;quot; probabilities) can be computed in a straightforward way.
Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4)
(Baker 1979), as well as (2) (Jelinek 1985). While the Jelinek and Lafferty (1991) solu-
tion to problem (3) is not a direct extension of CYK parsing, the authors nevertheless
present their algorithm in terms of its similarities to the computation of inside proba-
bilities.
In our algorithm, computations for tasks (1) and (3) proceed incrementally, as the
parser scans its input from left to right; in particular, prefix probabilities are available
as soon as the prefix has been seen, and are updated incrementally as it is extended.
Tasks (2) and (4) require one more (reverse) pass over the chart constructed from the
input.
Incremental, left-to-right computation of prefix probabilities is particularly impor-
tant since that is a necessary condition for using SCFGs as a replacement for finite-state
language models in many applications, such a speech decoding. As pointed out by Je-
linek and Lafferty (1991), knowing probabilities P(x0 . xi) for arbitrary prefixes xo . xi
enables probabilistic prediction of possible follow-words xi+i, as P(xi-Fi Ixo . • • xi) =
P(xo )/P(xo • .. xi). These conditional probabilities can then be used as word
transition probabilities in a Viterbi-style decoder or to incrementally compute the cost
function for a stack decoder (Bahl, Jelinek, and Mercer 1983).
Another application in which prefix probabilities play a central role is the extrac-
tion of n-gram probabilities from SCFGs (Stolcke and Segal 1994). Here, too, efficient
incremental computation saves time, since the work for common prefix strings can be
shared.
The key to most of the features of our algorithm is that it is based on the top-
down parsing method for nonprobabilistic CFGs developed by Earley (1970). Earley&apos;s
algorithm is appealing because it runs with best-known complexity on a number of
special classes of grammars. In particular, Earley parsing is more efficient than the
bottom-up methods in cases where top-down prediction can rule out potential parses
of substrings. The worst-case computational expense of the algorithm (either for the
complete input, or incrementally for each new word) is as good as that of the other
</bodyText>
<footnote confidence="0.9761685">
1 Their paper phrases these problem in terms of context-free probabilistic grammars, but they generalize
in obvious ways to other classes of models.
</footnote>
<page confidence="0.990767">
166
</page>
<note confidence="0.597728">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</note>
<bodyText confidence="0.999583555555556">
known specialized algorithms, but can be substantially better on well-known grammar
classes.
Earley&apos;s parser (and hence ours) also deals with any context-free rule format in
a seamless way, without requiring conversions to Chomsky Normal Form (CNF), as
is often assumed. Another advantage is that our probabilistic Earley parser has been
extended to take advantage of partially bracketed input, and to return partial parses
on ungrammatical input. The latter extension removes one of the common objections
against top-down, predictive (as opposed to bottom-up) parsing approaches (Mager-
man and Weir 1992).
</bodyText>
<sectionHeader confidence="0.98474" genericHeader="keywords">
2. Overview
</sectionHeader>
<bodyText confidence="0.999883454545455">
The remainder of the article proceeds as follows. Section 3 briefly reviews the workings
of an Earley parser without regard to probabilities. Section 4 describes how the parser
needs to be extended to compute sentence and prefix probabilities. Section 5 deals with
further modifications for solving the Viterbi and training tasks, for processing partially
bracketed inputs, and for finding partial parses. Section 6 discusses miscellaneous
issues and relates our work to the literature on the subject. In Section 7 we summarize
and draw some conclusions.
To get an overall idea of probabilistic Earley parsing it should be sufficient to read
Sections 3, 4.2, and 4.4. Section 4.5 deals with a crucial technicality, and later sections
mostly fill in details and add optional features.
We assume the reader is familiar with the basics of context-free grammar the-
ory, such as given in Aho and Ullman (1972, Chapter 2). Some prior familiarity with
probabilistic context-free grammars will also be helpful. Jelinek, Lafferty, and Mercer
(1992) provide a tutorial introduction covering the standard algorithms for the four
tasks mentioned in the introduction.
Notation. The input string is denoted by x. Ix&apos; is the length of x. Individual input
symbols are identified by indices starting at 0: x0, x1,.. ,x1_1. The input alphabet
is denoted by E. Substrings are identified by beginning and end positions x..1. The
variables i,j,k are reserved for integers referring to positions in input strings. Latin
capital letters X, Y, Z denote nonterminal symbols. Latin lowercase letters a, b,... are
used for terminal symbols. Strings of mixed nonterminal and terminal symbols are
written using lowercase Greek letters A, it, v. The empty string is denoted by E.
</bodyText>
<sectionHeader confidence="0.908645" genericHeader="introduction">
3. Earley Parsing
</sectionHeader>
<bodyText confidence="0.999775888888889">
An Earley parser is essentially a generator that builds left-most derivations of strings,
using a given set of context-free productions. The parsing functionality arises because
the generator keeps track of all possible derivations that are consistent with the input
string up to a certain point. As more and more of the input is revealed, the set of
possible derivations (each of which corresponds to a parse) can either expand as new
choices are introduced, or shrink as a result of resolved ambiguities. In describing the
parser it is thus appropriate and convenient to use generation terminology.
The parser keeps a set of states for each position in the input, describing all
pending derivations.2 These state sets together form the Earley chart. A state is of the
</bodyText>
<footnote confidence="0.8848185">
2 Earley states are also known as items in LR parsing; see Aho and Ullman (1972, Section 5.2) and
Section 6.2.
</footnote>
<page confidence="0.984949">
167
</page>
<figure confidence="0.344262333333333">
Computational Linguistics Volume 21, Number 2
form
1: kX
</figure>
<bodyText confidence="0.998789333333333">
where X is a nonterminal of the grammar, A and it are strings of nonterminals and/or
terminals, and i and k are indices into the input string. States are derived from pro-
ductions in the grammar. The above state is derived from a corresponding production
</bodyText>
<equation confidence="0.495131">
X
</equation>
<bodyText confidence="0.782935">
with the following semantics:
</bodyText>
<listItem confidence="0.992263090909091">
• The current position in the input is i, i.e., xo xi_1 have been processed
so far.3 The states describing the parser state at position i are collectively
called state set i. Note that there is one more state set than input
symbols: set 0 describes the parser state before any input is processed,
while set Ix&apos; contains the states after all input symbols have been
processed.
• Nonterminal X was expanded starting at position k in the input, i.e., X
generates some substring starting at position k.
• The expansion of X proceeded using the production X —&gt; Am, and has
expanded the right-hand side (RHS) Apt up to the position indicated by
the dot. The dot thus refers to the current position i.
</listItem>
<bodyText confidence="0.9967055">
A state with the dot to the right of the entire RHS is called a complete state, since
it indicates that the left-hand side (LHS) nonterminal has been fully expanded.
Our description of Earley parsing omits an optional feature of Earley states, the
lookahead string. Earley&apos;s algorithm allows for an adjustable amount of lookahead
during parsing, in order to process LR(k) grammars deterministically (and obtain the
same computational complexity as specialized LR(k) parsers where possible). The ad-
dition of lookahead is orthogonal to our extension to probabilistic grammars, so we
will not include it here.
The operation of the parser is defined in terms of three operations that consult the
current set of states and the current input symbol, and add new states to the chart. This
is strongly suggestive of state transitions in finite-state models of language, parsing,
etc. This analogy will be explored further in the probabilistic formulation later on.
The three types of transitions operate as follows.
Prediction. For each state
</bodyText>
<sectionHeader confidence="0.3426" genericHeader="method">
1: kX A.Y
</sectionHeader>
<bodyText confidence="0.9782702">
where Y is a nonterminal anywhere in the RHS, and for all rules Y v expanding Y,
add states
i: jY—.v.
A state produced by prediction is called a predicted state. Each prediction corresponds
to a potential expansion of a nonterminal in a left-most derivation.
</bodyText>
<page confidence="0.729747">
3 This index is implicit in Earley (1970). We include it here for clarity.
168
</page>
<note confidence="0.566799">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
Scanning. For each state
: kX A.a
</note>
<bodyText confidence="0.947692">
where a is a terminal symbol that matches the current input x1, add the state
</bodyText>
<equation confidence="0.602689">
i 1 : kX ---&gt; Aa.p,
</equation>
<bodyText confidence="0.99094125">
(move the dot over the current symbol). A state produced by scanning is called a
scanned state. Scanning ensures that the terminals produced in a derivation match
the input string.
Completion. For each complete state
</bodyText>
<equation confidence="0.935929">
i: 1Y v.
and each state in set j, j &lt; i, that has Y to the right of the dot,
j: kX --&gt; A.Y[t,
add the state
kX -4 AY .11,
</equation>
<bodyText confidence="0.992070217391304">
(move the dot over the current nonterminal). A state produced by completion is called
a completed state.&apos; Each completion corresponds to the end of a nonterminal expan-
sion started by a matching prediction step.
For each input symbol and corresponding state set, an Earley parser performs
all three operations exhaustively, i.e., until no new states are generated. One crucial
insight into the working of the algorithm is that, although both prediction and com-
pletion feed themselves, there are only a finite number of states that can possibly
be produced. Therefore recursive prediction and completion at each position have to
terminate eventually, and the parser can proceed to the next input via scanning.
To complete the description we need only specify the initial and final states. The
parser starts out with
0: o —.S,
where S is the sentence nonterminal (note the empty left-hand side). After processing
the last symbol, the parser verifies that
1: 0
has been produced (among possibly others), where 1 is the length of the input x. If at
any intermediate stage a state set remains empty (because no states from the previous
stage permit scanning), the parse can be aborted because an impossible prefix has been
detected.
States with empty LHS such as those above are useful in other contexts, as will
be shown in Section 5.4. We will refer to them collectively as dummy states. Dummy
states enter the chart only as a result of initialization, as opposed to being derived
from grammar productions.
</bodyText>
<footnote confidence="0.947188333333333">
4 Note the difference between &amp;quot;complete&amp;quot; and &amp;quot;completed&amp;quot; states: complete states (those with the dot to
the right of the entire RHS) are the result of a completion or scanning step, but completion also
produces states that are not yet complete.
</footnote>
<page confidence="0.993337">
169
</page>
<note confidence="0.780943">
Computational Linguistics Volume 21, Number 2
</note>
<tableCaption confidence="0.883716">
Table 1
</tableCaption>
<figure confidence="0.985848444444444">
(a) Example grammar for a tiny fragment of English. (b) Earley parser processing the sentence
a circle touches a triangle.
S ---, NP VP Det a
NP ---* Det N VT circle I square I triangle
VP —, VT NP VI touches
VP .— VI PP is
PP PNP above I below
touches a square
a circle
</figure>
<equation confidence="0.952600315789474">
O —› -S
predicted
oS —&gt; .NP VP
oNP —&gt; .Det N
oDet --* .a
scanned
oDet —&gt; a.
completed
oNP —&gt; Det.N
predicted
iN .circle
iN —&gt; .square
iN —&gt; .triangle
scanned
1N —&gt; circle.
completed
oNP Det N.
0S —&gt; NP.VP
predicted
</equation>
<footnote confidence="0.950819181818182">
2VP .VT NP
2VP PP
2VT —&gt; .touches
2VI —&gt; .is
scanned
2VT —4 touches.
completed
2VP —&gt; VT.NP
predicted
3NP .Det N
3Det —&gt; .a
</footnote>
<equation confidence="0.7929121875">
scanned
3Det —&gt; a.
completed
3NP —4 Det.N
predicted
5N —&gt; .circle
4N —4 .square
4N —&gt; .triangle
scanned
4N —4 triangle.
completed
4NP —&gt; Det N.
3VP —&gt; VT NP.
oS NP VP.
0 -*5.
State set 0 1 2 3 4 5
</equation>
<bodyText confidence="0.999880722222222">
It is easy to see that Earley parser operations are correct, in the sense that each
chain of transitions (predictions, scanning steps, completions) corresponds to a pos-
sible (partial) derivation. Intuitively, it is also true that a parser that performs these
transitions exhaustively is complete, i.e., it finds all possible derivations. Formal proofs
of these properties are given in the literature; e.g., Aho and Ullman (1972). The rela-
tionship between Earley transitions and derivations will be stated more formally in
the next section.
The parse trees for sentences can be reconstructed from the chart contents. We will
illustrate this in Section 5 when discussing Viterbi parses.
Table 1 shows a simple grammar and a trace of Earley parser operation on a
sample sentence.
Earley&apos;s parser can deal with any type of context-free rule format, even with
null or &amp;productions, i.e., those that replace a nonterminal with the empty string.
Such productions do, however, require special attention, and make the algorithm and
its description more complicated than otherwise necessary. In the following sections
we assume that no null productions have to be dealt with, and then summarize the
necessary changes in Section 4.7. One might choose to simply preprocess the grammar
to eliminate null productions, a process which is also described.
</bodyText>
<sectionHeader confidence="0.816046" genericHeader="method">
4. Probabilistic Earley Parsing
</sectionHeader>
<subsectionHeader confidence="0.971856">
4.1 Stochastic Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.759727">
A stochastic context-free grammar (SCFG) extends the standard context-free formalism
by adding probabilities to each production:
[p],
</bodyText>
<page confidence="0.983681">
170
</page>
<subsectionHeader confidence="0.329984">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</subsectionHeader>
<bodyText confidence="0.946189090909091">
where the rule probability p is usually written as P(X --4 A). This notation to some
extent hides the fact that p is a conditional probability, of production X -4 A being
chosen, given that X is up for expansion. The probabilities of all rules with the same
nonterminal X on the LHS must therefore sum to unity. Context-freeness in a prob-
abilistic setting translates into conditional independence of rule choices. As a result,
complete derivations have joint probabilities that are simply the products of the rule
probabilities involved.
The probabilities of interest mentioned in Section 1 can now be defined formally.
Definition 1
The following quantities are defined relative to a SCFG G, a nonterminal X, and a
string x over the alphabet E of G.
</bodyText>
<equation confidence="0.624418">
a) The probability of a (partial) derivation vi v2 • • • vk is inductively
defined by
1) P(vi) = 1
2) P(vi • • • vk) = P(X A)P(v2 • • •= vk),
</equation>
<bodyText confidence="0.969358909090909">
where i, v2,. vk are strings of terminals and nonterminals, X -4 A is a
production of G, and 1)2 is derived from vi by replacing one occurrence
of X with A.
b) The string probability P(X 4 x) (of x given X) is the sum of the
probabilities of all left-most derivations X • • •= x producing x
from X.&apos;
c) The sentence probability P(S x) (of x given G) is the string
probability given the start symbol S of G. By definition, this is also the
probability P(x I G) assigned to x by the grammar G.
d) The prefix probability P(S 4L x) (of x given G) is the sum of the
probabilities of all sentence strings having x as a prefix,
</bodyText>
<equation confidence="0.991661333333333">
P(S L x) P(S xy).
yEE
(In particular, P(S E) = 1).
</equation>
<bodyText confidence="0.995099833333333">
In the following, we assume that the probabilities in a SCFG are proper and con-
sistent as defined in Booth and Thompson (1973), and that the grammar contains no
useless nonterminals (ones that can never appear in a derivation). These restrictions
ensure that all nonterminals define probability measures over strings; i.e., P(X x) is
a proper distribution over x for all X. Formal definitions of these conditions are given
in Appendix A.
</bodyText>
<footnote confidence="0.9086925">
5 In a left-most derivation each step replaces the nonterminal furthest to the left in the partially
expanded string. The order of expansion is actually irrelevant for this definition, because of the
multiplicative combination of production probabilities. We restrict summation to left-most derivations
to avoid counting duplicates, and because left-most derivations will play an important role later.
</footnote>
<page confidence="0.979062">
171
</page>
<note confidence="0.470531">
Computational Linguistics Volume 21, Number 2
</note>
<subsectionHeader confidence="0.996766">
4.2 Earley Paths and Their Probabilities
</subsectionHeader>
<bodyText confidence="0.9188832">
In order to define the probabilities associated with parser operation on a SCFG, we
need the concept of a path, or partial derivation, executed by the Earley parser.
Definition 2
a) An (unconstrained) Earley path, or simply path, is a sequence of Earley
states linked by prediction, scanning, or completion. For the purpose of
this definition, we allow scanning to operate in &amp;quot;generation mode,&amp;quot; i.e.,
all states with terminals to the right of the dot can be scanned, not just
those matching the input. (For completed states, the predecessor state is
defined to be the complete state from the same state set contributing to
the completion.)
</bodyText>
<listItem confidence="0.814420625">
b) A path is said to be constrained by, or to generate a string x if the
terminals immediately to the left of the dot in all scanned states, in
sequence, form the string x.
c) A path is complete if the last state on it matches the first, except that the
dot has moved to the end of the RHS.
d) We say that a path starts with nonterminal X if the first state on it is a
predicted state with X on the LHS.
e) The length of a path is defined as the number of scanned states on it.
</listItem>
<bodyText confidence="0.976606666666667">
Note that the definition of path length is somewhat counterintuitive, but is moti-
vated by the fact that only scanned states correspond directly to input symbols. Thus
the length of a path is always the same as the length of the input string it generates.
A constrained path starting with the initial state contains a sequence of states
from state set 0 derived by repeated prediction, followed by a single state from set 1
produced by scanning the first symbol, followed by a sequence of states produced by
completion, followed by a sequence of predicted states, followed by a state scanning
the second symbol, and so on. The significance of Earley paths is that they are in a
one-to-one correspondence with left-most derivations. This will allow us to talk about
probabilities of derivations, strings, and prefixes in terms of the actions performed by
Earley&apos;s parser. From now on, we will use &amp;quot;derivation&amp;quot; to imply a left-most derivation.
Lemma 1
</bodyText>
<listItem confidence="0.8905585">
a) An Earley parser generates state
i: kX
if and only if there is a partial derivation
S xo...k-iXv
</listItem>
<bodyText confidence="0.89720875">
deriving a prefix x0...,_1 of the input.
b) There is a one-to-one mapping between partial derivations and Earley
paths, such that each production X v applied in a derivation
corresponds to a predicted Earley state X —&gt; .v.
</bodyText>
<page confidence="0.99263">
172
</page>
<subsectionHeader confidence="0.310267">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</subsectionHeader>
<bodyText confidence="0.98298">
(a) is the invariant underlying the correctness and completeness of Earley&apos;s algo-
rithm; it can be proved by induction on the length of a derivation (Aho and Ullman
1972, Theorem 4.9). The slightly stronger form (b) follows from (a) and the way pos-
sible prediction steps are defined.
Since we have established that paths correspond to derivations, it is convenient
to associate derivation probabilities directly with paths. The uniqueness condition (b)
above, which is irrelevant to the correctness of a standard Earley parser, justifies (prob-
abilistic) counting of paths in lieu of derivations.
</bodyText>
<subsectionHeader confidence="0.919261">
Definition 3
</subsectionHeader>
<bodyText confidence="0.796901666666667">
The probability P(P) of a path P is the product of the probabilities of all rules used
in the predicted states occurring in P.
Lemma 2
</bodyText>
<listItem confidence="0.807045777777778">
a) For all paths P starting with a nonterminal X, P(P) gives the probability
of the (partial) derivation represented by P. In particular, the string
probability P(X x) is the sum of the probabilities of all paths starting
with X that are complete and constrained by x.
b) The sentence probability P(S 4 x) is the sum of the probabilities of all
complete paths starting with the initial state, constrained by x.
c) The prefix probability P(S L x) is the sum of the probabilities of all
paths P starting with the initial state, constrained by x, that end in a
scanned state.
</listItem>
<bodyText confidence="0.998893909090909">
Note that when summing over all paths &amp;quot;starting with the initial state,&amp;quot; summa-
tion is actually over all paths starting with S, by definition of the initial state 0 —&gt; .S.
(a) follows directly from our definitions of derivation probability, string probability,
path probability, and the one-to-one correspondence between paths and derivations
established by Lemma 1. (b) follows from (a) by using S as the start nonterminal. To
obtain the prefix probability in (c), we need to sum the probabilities of all complete
derivations that generate x as a prefix. The constrained paths ending in scanned states
represent exactly the beginnings of all such derivations. Since the grammar is assumed
to be consistent and without useless nonterminals, all partial derivations can be com-
pleted with probability one. Hence the sum over the constrained incomplete paths is
the sought-after sum over all complete derivations generating the prefix.
</bodyText>
<subsectionHeader confidence="0.99989">
4.3 Forward and Inner Probabilities
</subsectionHeader>
<bodyText confidence="0.999952">
Since string and prefix probabilities are the result of summing derivation probabilities,
the goal is to compute these sums efficiently by taking advantage of the Earley control
structure. This can be accomplished by attaching two probabilistic quantities to each
Earley state, as follows. The terminology is derived from analogous or similar quan-
tities commonly used in the literature on Hidden Markov Models (HMMs) (Rabiner
and Juang 1986) and in Baker (1979).
</bodyText>
<subsectionHeader confidence="0.892454">
Definition 4
</subsectionHeader>
<bodyText confidence="0.990497333333333">
The following definitions are relative to an implied input string x.
a) The forward probability a, (kX --&gt; A.A) is the sum of the probabilities of
all constrained paths of length i that end in state kX
</bodyText>
<page confidence="0.997984">
173
</page>
<subsectionHeader confidence="0.660663">
Computational Linguistics Volume 21, Number 2
</subsectionHeader>
<bodyText confidence="0.99602324137931">
b) The inner probability 7,(kX ).p) is the sum of the probabilities of all
paths of length i - k that start in state k: kX .Au and end in
kX A.p, and generate the input symbols xk • • • x1-1.
It helps to interpret these quantities in terms of an unconstrained Earley parser that
operates as a generator emitting—rather than recognizing—strings. Instead of tracking
all possible derivations, the generator traces along a single Earley path randomly
determined by always choosing among prediction steps according to the associated
rule probabilities. Notice that the scanning and completion steps are deterministic once
the rules have been chosen.
Intuitively, the forward probability ai(kX ).p) is the probability of an Earley
generator producing the prefix of the input up to position i -1 while passing through
state kX A•,u, at position i. However, due to left-recursion in productions the same
state may appear several times on a path, and each occurrence is counted toward
the total a,. Thus, a, is really the expected number of occurrences of the given state
in state set i. Having said that, we will refer to a simply as a probability, both for
the sake of brevity, and to keep the analogy to the HMM terminology of which this
is a generalization.&apos; Note that for scanned states, a is always a probability, since by
definition a scanned state can occur only once along a path.
The inner probabilities, on the other hand, represent the probability of generating
a substring of the input from a given nonterminal, using a particular production.
Inner probabilities are thus conditional on the presence of a given nonterminal X with
expansion starting at position k, unlike the forward probabilities, which include the
generation history starting with the initial state. The inner probabilities as defined here
correspond closely to the quantities of the same name in Baker (1979). The sum of -y
of all states with a given LHS X is exactly Baker&apos;s inner probability for X.
The following is essentially a restatement of Lemma 2 in terms of forward and
inner probabilities. It shows how to obtain the sentence and string probabilities we
are interested in, provided that forward and inner probabilities can be computed ef-
fectively.
</bodyText>
<subsectionHeader confidence="0.57723">
Lemma 3
</subsectionHeader>
<bodyText confidence="0.879723555555556">
The following assumes an Earley chart constructed by the parser on an input string x
with Ix I = 1.
a) Provided that S L x0...k_iXv is a possible left-most derivation of the
grammar (for some v), the probability that a nonterminal X generates the
substring xk . . . x,_1 can be computed as the sum
P(X xk...k_i) = E -yickx —+ A.)
i:kx- A.
(sum of inner probabilities over all complete states with LHS X and start
index k).
</bodyText>
<footnote confidence="0.531550333333333">
6 The same technical complication was noticed by Wright (1990) in the computation of probabilistic LR
parser tables. The relation to LR parsing will be discussed in Section 6.2. Incidentally, a similar
interpretation of forward &amp;quot;probabilities&amp;quot; is required for HMMs with non-emitting states.
</footnote>
<page confidence="0.988654">
174
</page>
<subsectionHeader confidence="0.236129">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</subsectionHeader>
<bodyText confidence="0.245622">
b) In particular, the string probability P(S x) can be computed as7
</bodyText>
<equation confidence="0.86442725">
P(S x) = -y1(0 —S.)
al(0 S.)
c) The prefix probability P(S L x), with Ix = I, can be computed as
P(S x) =
</equation>
<bodyText confidence="0.995564571428571">
(sum of forward probabilities over all scanned states).
The restriction in (a) that X be preceded by a possible prefix is necessary, since
the Earley parser at position i will only pursue derivations that are consistent with
the input up to position i. This constitutes the main distinguishing feature of Earley
parsing compared to the strict bottom-up computation used in the standard inside
probability computation (Baker 1979). There, inside probabilities for all positions and
nonterminals are computed, regardless of possible prefixes.
</bodyText>
<subsectionHeader confidence="0.999921">
4.4 Computing Forward and Inner Probabilities
</subsectionHeader>
<bodyText confidence="0.999516384615385">
Forward and inner probabilities not only subsume the prefix and string probabilities,
they are also straightforward to compute during a run of Earley&apos;s algorithm. In fact, if
it weren&apos;t for left-recursive and unit productions their computation would be trivial.
For the purpose of exposition we will therefore ignore the technical complications
introduced by these productions for a moment, and then return to them once the
overall picture has become clear.
During a run of the parser both forward and inner probabilities will be attached
to each state, and updated incrementally as new states are created through one of the
three types of transitions. Both probabilities are set to unity for the initial state 0 .S.
This is consistent with the interpretation that the initial state is derived from a dummy
production -4 S for which no alternatives exist.
Parsing then proceeds as usual, with the probabilistic computations detailed below.
The probabilities associated with new states will be computed as sums of various
combinations of old probabilities. As new states are generated by prediction, scanning,
and completion, certain probabilities have to be accumulated, corresponding to the
multiple paths leading to a state. That is, if the same state is generated multiple
times, the previous probability associated with it has to be incremented by the new
contribution just computed. States and probability contributions can be generated in
any order, as long as the summation for one state is finished before its probability
enters into the computation of some successor state. Appendix B.2 suggests a way to
implement this incremental summation.
Notation. A few intuitive abbreviations are used from here on to describe Earley
transitions succinctly. (1) To avoid unwieldy E notation we adopt the following con-
vention. The expression x += y means that x is computed incrementally as a sum of
various y terms, which are computed in some order and accumulated to finally yield
the value of x.8 (2) Transitions are denoted by , with predecessor states on the left
</bodyText>
<footnote confidence="0.918658">
7 The definitions of forward and inner probabilities coincide for the final state.
8 This notation suggests a simple implementation, being obviously borrowed from the programming
language C.
</footnote>
<page confidence="0.989802">
175
</page>
<note confidence="0.450899">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.99310375">
and successor states on the right. (3) The forward and inner probabilities of states are
notated in brackets after each state, e.g.,
kX A.Ytt [a,7]
is shorthand for a = a,(kX ).Yu), 7 = -y,(kX
</bodyText>
<subsectionHeader confidence="0.947324">
Prediction (probabilistic).
</subsectionHeader>
<bodyText confidence="0.9948785">
i: kX —+ A.Yft [a, 7] &gt; i: ,Y .v [a&apos;,-y&apos;]
for all productions Y v. The new probabilities can be computed as
a&apos; += a • P(Y v)
= P(Y v)
Note that only the forward probability is accumulated; 7 is not used in this step.
Rationale, a&apos; is the sum of all path probabilities leading up to kX A.Yit, times
the probability of choosing production Y v. The value -y&apos; is just a special case of the
definition.
</bodyText>
<subsectionHeader confidence="0.926888">
Scanning (probabilistic).
</subsectionHeader>
<bodyText confidence="0.956418">
i: kX A.ap, [a, 7] i +1 : kX—&gt; Aa.P, [a&apos;,71
for all states with terminal a matching input at position i. Then
=-- a
7
Rationale. Scanning does not involve any new choices, since the terminal was al-
ready selected as part of the production during prediction.&apos;
</bodyText>
<subsectionHeader confidence="0.416035">
Completion (probabilistic).
</subsectionHeader>
<bodyText confidence="0.665673625">
Then i: j1( //. [a&amp;quot;, -y&amp;quot;] i: kX XY.ti [a&apos;, 71
j kX —&gt; A.Yit [a,7] j
a&apos; += a
+= 7.7&amp;quot; (12)
Note that a&amp;quot; is not used.
Rationale. To update the old forward/inner probabilities a and -y to a&apos; and -y&apos;,
respectively, the probabilities of all paths expanding Y v have to be factored in.
These are exactly the paths summarized by the inner probability -y&amp;quot;.
</bodyText>
<footnote confidence="0.449941">
9 In different parsing scenarios the scanning step may well modify probabilities. For example, if the
input symbols themselves have attached likelihoods, these can be integrated by multiplying them onto
a and &apos;y when a symbol is scanned. That way it is possible to perform efficient Earley parsing with
integrated joint probability computation directly on weighted lattices describing ambiguous inputs.
</footnote>
<page confidence="0.988558">
176
</page>
<note confidence="0.213184">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</note>
<subsectionHeader confidence="0.999854">
4.5 Coping with Recursion
</subsectionHeader>
<bodyText confidence="0.903106777777778">
The standard Earley algorithm, together with the probability computations described
in the previous section, would be sufficient if it weren&apos;t for the problem of recursion
in the prediction and completion steps.
The nonprobabilistic Earley algorithm can stop recursing as soon as all predic-
tions/completions yield states already contained in the current state set. For the com-
putation of probabilities, however, this would mean truncating the probabilities re-
sulting from the repeated summing of contributions.
4.5.1 Prediction loops. As an example, consider the following simple left-recursive
SCFG.
</bodyText>
<equation confidence="0.5580635">
S —pa [p]
S Sb [q],
</equation>
<bodyText confidence="0.9880335">
where q = 1 — p. Nonprobabilistically, the prediction loop at position 0 would stop
after producing the states
</bodyText>
<equation confidence="0.844497666666666">
o .S
05 .a
OS .Sb.
This would leave the forward probabilities at
ao(oS .a) = p
ao(oS —+ .Sb) = q,
</equation>
<bodyText confidence="0.99703">
corresponding to just two out of an infinity of possible paths. The correct forward
probabilities are obtained as a sum of infinitely many terms, accounting for all possible
paths of length 1.
</bodyText>
<equation confidence="0.9799415">
ao(oS .a) = p + qp + q2p + • • • =p(1 —q)1 = 1
ao(oS .Sb) = q + q2 + q3 + • • • =q(1 —q)1 =
</equation>
<bodyText confidence="0.999972111111111">
In these sums each p corresponds to a choice of the first production, each q to a choice
of the second production. If we didn&apos;t care about finite computation the resulting
geometric series could be computed by letting the prediction loop (and hence the
summation) continue indefinitely.
Fortunately, all repeated prediction steps, including those due to left-recursion
in the productions, can be collapsed into a single, modified prediction step, and the
corresponding sums computed in closed form. For this purpose we need a probabilistic
version of the well-known parsing concept of a left corner, which is also at the heart
of the prefix probability algorithm of Jelinek and Lafferty (1991).
</bodyText>
<subsectionHeader confidence="0.773442">
Definition 5
</subsectionHeader>
<bodyText confidence="0.8664355">
The following definitions are relative to a given SCFG G.
a) Two nonterminals X and Y are said to be in a left-corner relation
</bodyText>
<equation confidence="0.9949965">
X —3L Y if there exists a production for X that has a RHS starting with Y,
X —&gt; YA.
</equation>
<page confidence="0.97963">
177
</page>
<subsectionHeader confidence="0.319589">
Computational Linguistics Volume 21, Number 2
</subsectionHeader>
<bodyText confidence="0.739497333333333">
b) The probabilistic left-corner relationl° PL -= PL(G) is the matrix of
probabilities P(X Y), defined as the total probability of choosing a
production for X that has Y as a left corner:
</bodyText>
<equation confidence="0.9229966">
P(X L Y) = E P(X -&gt; YA).
X—&gt;YAEG
c) The relation X 4L Y is defined as the reflexive, transitive closure of
X Y, i.e., X Y iff X = Y or there is a nonterminal Z such that
X Z and Z L Y.
</equation>
<bodyText confidence="0.974751666666667">
d) The probabilistic reflexive, transitive left-corner relation RL = RL(G) is
a matrix of probability sums R(X 4L Y). Each R(X Y) is defined as a
series
</bodyText>
<equation confidence="0.997445">
R(X Y) = P(X =Y)
P(X L Y)
+E P(X -&gt;L Zi)P(Zi L Y)
ZI
+ E P(X L zop(Zi L z2)P(z2 L Y)
zi,z2
+
Alternatively, RL is defined by the recurrence relation
R(X L Y) = 6(X, Y) E P(X L Z)R(Z LY),
</equation>
<bodyText confidence="0.997658">
where we use the delta function, defined as S(X, Y) = 1 if X = Y, and
6 (X, Y) =0 if X Y.
The recurrence for RL can be conveniently written in matrix notation
</bodyText>
<equation confidence="0.992193">
RL = I + PLRL,
</equation>
<bodyText confidence="0.959218">
from which the closed-form solution is derived:
</bodyText>
<equation confidence="0.990408">
RL = (I - PL)-1.
</equation>
<bodyText confidence="0.996499727272727">
An existence proof for RL is given in Appendix A. Appendix B.3.1 shows how to speed
up the computation of RL by inverting only a reduced version of the matrix I - PL.
The significance of the matrix RL for the Earley algorithm is that its elements are
the sums of the probabilities of the potentially infinitely many prediction paths leading
from a state kX -› ).Zp, to a predicted state ,Y .v, via any number of intermediate
states.
RL can be computed once for each grammar, and used for table-lookup in the
following, modified prediction step.
10 If a probabilistic relation R is replaced by its set-theoretic version R&apos;, i.e., (x, y) E R&apos; iff R(x,y) 0,
then the closure operations used here reduce to their traditional discrete counterparts; hence the choice
of terminology.
</bodyText>
<page confidence="0.980737">
178
</page>
<table confidence="0.3154315">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
Prediction (probabilistic, transitive).
</table>
<bodyText confidence="0.699048">
i: kX A.Zit [a, 7] i: iY .v
for all productions Y v such that R(Z =L Y) is nonzero. Then
</bodyText>
<equation confidence="0.968217">
a&apos; += a • R(Z L Y)P(Y --&gt; v) (13)
= P(Y -4 v) (14)
</equation>
<bodyText confidence="0.985036666666667">
The new R(Z 41, Y) factor in the updated forward probability accounts for the
sum of all path probabilities linking Z to Y. For Z = Y this covers the case of a single
step of prediction; R(Y ZL Y) &gt; 1 always, since RL is defined as a reflexive closure.
</bodyText>
<subsubsectionHeader confidence="0.53039">
4.5.2 Completion loops. As in prediction, the completion step in the Earley algorithm
</subsubsectionHeader>
<bodyText confidence="0.830332">
may imply an infinite summation, and could lead to an infinite loop if computed
naively. However, only unit productions&amp;quot; can give rise to cyclic completions.
</bodyText>
<equation confidence="0.83008575">
The problem is best explained by studying an example. Consider the grammar
S a [p]
S T [q]
T S [1],
</equation>
<bodyText confidence="0.99801">
where q = 1 - p. Presented with the input a (the only string the grammar generates),
after one cycle of prediction, the Earley chart contains the following states.
</bodyText>
<equation confidence="0.9996005">
0 : o .S a = 1, 7 = 1
0 : 0S .T a= -y =q
0 : 0T .S a -= -y = 1
0 : 0S .a a = rip -= 1, -y = p.
</equation>
<bodyText confidence="0.979554555555556">
The 1)-1 factors are a result of the left-corner sum 1 ± q q2 + • • • = (1 - q) 1.
After scanning 0S -&gt; .a, completion without truncation would enter an infinite
loop. First oT .S is completed, yielding a complete state oT -4 S., which allows 0S -&gt;
.T to be completed, leading to another complete state for S. etc. The nonprobabilistic
Earley parser can just stop here, but as in prediction, this would lead to truncated
probabilities. The sum of probabilities that needs to be computed to arrive at the correct
result contains infinitely many terms, one for each possible loop through the T S
production. Each such loop adds a factor of q to the forward and inner probabilities.
The summations for all completed states turn out as
</bodyText>
<equation confidence="0.9987305">
1 : 0S -&gt; a. a = 1, -y = p
1 : 0T -&gt; S. a = r 1 q (p ± pq + pq 2 + . . . ) ... r 1 q, ey ___ p ± pq + pq2 ± . . . ..__ 1
1 : 0 S . a =__ p ± pq + pq2 ± • . • = -1., 7 = p+pq±pq2 + • • • i
1 : 0S -- T. a = r 1 q (p ± pq + pq2 ± . . . ) ir lq, ey q(p ± pq ± pq2 ± . . . ) q
</equation>
<page confidence="0.8578125">
11 Unit productions are also called &amp;quot;chain productions&amp;quot; or &amp;quot;single productions&amp;quot; in the literature.
179
</page>
<note confidence="0.653216">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.934476181818182">
The approach taken here to compute exact probabilities in cyclic completions is
mostly analogous to that for left-recursive predictions. The main difference is that unit
productions, rather than left-corners, form the underlying transitive relation. Before
proceeding we can convince ourselves that this is indeed the only case we have to
worry about.
Lemma 4
Let
ki X1 Al X2. &gt; k2X2 —&gt; A2X3. -&gt; &amp;quot; &gt; k,X, —&gt; A,X,+1 •
be a completion cycle, i.e., ki A1 =-- Ac, X2 = Xc+1. Then it must be the
case that Ai = A2 = • • • = A, €, i.e., all productions involved are unit productions
xl —&gt; x2,. .,x—&gt; x,+1
Proof
For all completion chains it is true that the start indices of the states are monotonically
increasing, ki &gt; k2 &gt; ... (a state can only complete an expansion that started at
the same or a previous position). From ki = /cc, it follows that ki = k2 -= • • • = kc.
Because the current position (dot) also refers to the same input index in all states,
all nonterminals Xi, X2, ..., X, have been expanded into the same substring of the
input between ki and the current position. By assumption the grammar contains no
nonterminals that generate 6,12 therefore we must have Ai = A2 = • • • = A, = €, q.e.d.
0
We now formally define the relation between nonterminals mediated by unit pro-
ductions, analogous to the left-corner relation.
</bodyText>
<sectionHeader confidence="0.436817" genericHeader="method">
Definition 6
</sectionHeader>
<bodyText confidence="0.993328">
The following definitions are relative to a given SCFG G.
</bodyText>
<listItem confidence="0.7204847">
a) Two nonterminals X and Y are said to be in a unit-production relation
X Y iff there exists a production for X that has Y as its RHS.
b) The probabilistic unit-production relation Pu = Pu(G) is the matrix of
probabilities P(X —&gt; Y).
c) The relation X Y is defined as the reflexive, transitive closure of
X —&gt; Y, i.e., X Y iff X = Y or there is a nonterminal Z such that X Z
and Z 4 Y.
d) The probabilistic reflexive, transitive unit-production relation
Ru = Ru(G) is the matrix of probability sums R(X 11. Each R(X Y)
is defined as a series
</listItem>
<equation confidence="0.94988325">
R(X Y) P(X Y)
P(X Y)
+E P(X --&gt; Zi)P(Zi Y)
ZI
</equation>
<page confidence="0.6389435">
12 Even with null productions, these would not be used for Earley transitions; see Section 4.7.
180
</page>
<table confidence="0.9726995">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
+ E P(X -&gt; 11)P(Zi -&gt; Z2)P(Z2
Z1,Z2
P(X Z)R(Z Y).
</table>
<bodyText confidence="0.746823">
As before, a matrix inversion can compute the relation Ru in closed form:
</bodyText>
<equation confidence="0.909913">
Ru = (I - Purl .
</equation>
<bodyText confidence="0.98469675">
The existence of Ru is shown in Appendix A.
The modified completion loop in the probabilistic Earley parser can now use the
Ru matrix to collapse all unit completions into a single step. Note that we still have
to do iterative completion on non-unit productions.
</bodyText>
<subsectionHeader confidence="0.46136">
Completion (probabilistic, transitive).
</subsectionHeader>
<bodyText confidence="0.733222">
: jy. v. [arr, i: kX AZ.A [ctr,,yri
kX A.Zit [a, &apos;y]
for all Y, Z such that R(Z Y) is nonzero, and Y v is not a unit production
or v e E). Then
</bodyText>
<equation confidence="0.701007">
af += a • 7nR(Z Y)
+= &apos;y • -y&amp;quot;R(Z Y)
</equation>
<subsectionHeader confidence="0.995716">
4.6 An Example
</subsectionHeader>
<bodyText confidence="0.985662">
Consider the grammar
</bodyText>
<equation confidence="0.9722825">
S -› a [p]
S -&gt; SS [q]
</equation>
<bodyText confidence="0.964268857142857">
where q =1 - p. This highly ambiguous grammar generates strings of any number of
a&apos;s, using all possible binary parse trees over the given number of terminals. The states
involved in parsing the string aaa are listed in Table 2, along with their forward and
inner probabilities. The example illustrates how the parser deals with left-recursion
and the merging of alternative sub-parses during completion.
Since the grammar has only a single nonterminal, the left-corner matrix PL has
rank 1:
</bodyText>
<equation confidence="0.840526333333333">
PL = [q].
Its transitive closure is
RL =- (I- PL)&apos; = [p]-1 [--1].
</equation>
<bodyText confidence="0.998758">
Consequently, the example trace shows the factor p-1 being introduced into the for-
ward probability terms in the prediction steps.
The sample string can be parsed as either (a(aa)) or ((aa)a), each parse having a
probability of p3q2. The total string probability is thus 2p3q2, the computed a and -y
values for the final state. The a values for the scanned states in sets 1, 2, and 3 are
the prefix probabilities for a, aa, and aaa, respectively: P(S 41, a) = 1, P(S L aa) = q,
</bodyText>
<equation confidence="0.895985">
P(S aaa) = (1 + p)q2.
</equation>
<page confidence="0.98136">
181
</page>
<note confidence="0.440772">
Computational Linguistics Volume 21, Number 2
</note>
<tableCaption confidence="0.984988">
Table 2
</tableCaption>
<bodyText confidence="0.77520875">
Earley chart as constructed during the parse of aaa with the grammar in (a). The two columns
to the right in (b) list the forward and inner probabilities, respectively, for each state. In both a
and columns, the • separates old factors from new ones (as per equations 11, 12 and 13).
Addition indicates multiple derivations of the same state.
</bodyText>
<figure confidence="0.932171548387097">
S —&gt; a [p]
S --&gt; SS [q]
State set 0
.S
predicted
0S —&gt; .a
OS —&gt; .SS
State set 1
scanned
oS —&gt; a.
completed
OS —&gt; S.S
predicted
iS —&gt; .a
iS —&gt; .SS
State set 2
scanned
—&gt; a.
completed
15 S.S
05 —&gt; SS.
oS —&gt; S.S
0 -&gt; S.
predicted
25 —&gt; .a
25 .SS
State set 3
scanned
25 —&gt; a.
completed
25 S.S
</figure>
<equation confidence="0.988932586206897">
iS —&gt; SS.
iS —&gt; S.S
oS —&gt; SS.
0S —&gt; S.S
O —&gt; S.
1 • 1,-1 p = 1
1 • p- q = p-1 q
rip = 1
riq • p = q
q p-lp = q
q p-lq = p-1q2
p-1q2 p = q2
q • P = pq
p-i p2q = p q2
• p2q = p2q
+ pq2 ) rip = (1±p )q2
+ pq2 r 1 q = + r1)(33
(1 + p)q2
(1 + p-1)q3-p= (1 + p)q3 q P = Pq
q2 p pq2 pq p = p2 q
p-1q2 p2q pq3 q p2 q = p2 q2
pq2 p q p2q = 2p2q2 p2q2 p pq p2q 2p3q2
p-1 q • 2p3q2 2p2q3 q • 2p3q2 = 2p3q3
1 2p3q2 = 2p3q2 1 • 2p3q2 = 2p3q2
q • P = pq
q • q = pq
pq p p2q
q p2q p2q2
1 • pq = p2q
</equation>
<page confidence="0.981747">
182
</page>
<note confidence="0.330292">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</note>
<subsectionHeader confidence="0.977471">
4.7 Null Productions
</subsectionHeader>
<bodyText confidence="0.998370631578947">
Null productions X —&gt; E introduce some complications into the relatively straight-
forward parser operation described so far, some of which are due specifically to the
probabilistic aspects of parsing. This section summarizes the necessary modifications
to process null productions correctly, using the previous description as a baseline. Our
treatment of null productions follows the (nonprobabilistic) formulation of Graham,
Harrison, and Ruzzo (1980), rather than the original one in Earley (1970).
4.7.1 Computing 1-expansion probabilities. The main problem with null productions
is that they allow multiple prediction-completion cycles in between scanning steps
(since null productions do not have to be matched against one or more input symbols).
Our strategy will be to collapse all predictions and completions due to chains of null
productions into the regular prediction and completion steps, not unlike the way
recursive predictions/completions were handled in Section 4.5.
A prerequisite for this approach is to precompute, for all nonterminals X, the prob-
ability that X expands to the empty string. Note that this is another recursive problem,
since X itself may not have a null production, but expand to some nonterminal Y that
does.
Computation of P(X E) for all X can be cast as a system of non-linear equations,
as follows. For each X, let ex be an abbreviation for P(X z E). For example, let X have
productions
</bodyText>
<equation confidence="0.997129666666667">
X -&gt; [Pu]
- Y1Y2 [p21
-&gt; Y3 Y4 Y5 [P3]
</equation>
<bodyText confidence="0.999741666666667">
The semantics of context-free rules imply that X can only expand to E if all the RHS
nonterminals in one of X&apos;s productions expand to E. Translating to probabilities, we
obtain the equation
</bodyText>
<equation confidence="0.816443">
ex =-- Pu + p2ey,ey2 + p3ey3ey4ey5 + • • •
</equation>
<bodyText confidence="0.983615882352941">
In other words, each production contributes a term in which the rule probability is
multiplied by the product of the e variables corresponding to the RHS nonterminals,
unless the RHS contains a terminal (in which case the production contributes nothing
to ex because it cannot possibly lead to E).
The resulting nonlinear system can be solved by iterative approximation. Each
variable ex is initialized to P(X -4 6), and then repeatedly updated by substituting in
the equation right-hand sides, until the desired level of accuracy is attained. Conver-
gence is guaranteed, since the ex values are monotonically increasing and bounded
above by the true values P(X e) &lt; 1. For grammars without cyclic dependen-
cies among &amp;producing nonterminals, this procedure degenerates to simple backward
substitution. Obviously the system has to be solved only once for each grammar.
The probability ex can be seen as the precomputed inner probability of an expan-
sion of X to the empty string; i.e., it sums the probabilities of all Earley paths that
derive c from X. This is the justification for the way these probabilities can be used in
modified prediction and completion steps, described next.
4.7.2 Prediction with null productions. Prediction is mediated by the left-corner re-
lation. For each X occurring to the right of a dot, we generate states for all Y that
</bodyText>
<page confidence="0.993873">
183
</page>
<note confidence="0.655008">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.9852094">
are reachable from X by way of the X —&gt;L Y relation. This reachability criterion has
to be extended in the presence of null productions. Specifically, if X has a production
X ---&gt; Y,_i Y,A then Y, is a left corner of X iff Yi, ••• , Y,_1 all have a nonzero
probability of expanding to €. The contribution of such a production to the left-corner
probability P(X --&gt;L Y1) is
</bodyText>
<equation confidence="0.844758">
P(X —&gt; YI_lY)fleyk
</equation>
<bodyText confidence="0.96276970967742">
The old prediction procedure can now be modified in two steps. First, replace
the old PL relation by the one that takes into account null productions, as sketched
above. From the resulting PL compute the reflexive transitive closure RL, and use it to
generate predictions as before.
Second, when predicting a left corner Y with a production Y Y,_1Y,A, add
states for all dot positions up to the first RHS nonterminal that cannot expand to E,
say from X —&gt; .Y1 ... Y,_i Y,A through X —&gt; Yi_i .Y,A. We will call this procedure
&amp;quot;spontaneous dot shifting.&amp;quot; It accounts precisely for those derivations that expand the
RHS prefix Y1 ... Y,_1 without consuming any of the input symbols.
The forward and inner probabilities of the states thus created are those of the
first state X —&gt; .Y1 ... Y,_1Y,A, multiplied by factors that account for the implied E-
expansions. This factor is just the product [Li ey„ where j is the dot position.
4.7.3 Completion with null productions. Modification of the completion step follows
a similar pattern. First, the unit production relation has to be extended to allow for
unit production chains due to null productions. A rule X Yi • • • Y1-1 Y/Yi+i • .. Yi can
effectively act as a unit production that links X and Y, if all other nonterminals on the
RHS can expand to E. Its contribution to the unit production relation P(X —&gt; Y,) will
then be
P(X —&gt; yiyi+i • .. y,) ey,
koi
From the resulting revised Pu matrix we compute the closure Ru as usual.
The second modification is another instance of spontaneous dot shifting. When
completing a state X —&gt; A.Yit and moving the dot to get X —&gt; AY.it, additional states
have to be added, obtained by moving the dot further over any nonterminals in that
have nonzero &amp;expansion probability. As in prediction, forward and inner probabilities
are multiplied by the corresponding &amp;expansion probabilities.
4.7.4 Eliminating null productions. Given these added complications one might con-
sider simply eliminating all &amp;productions in a preprocessing step. This is mostly
straightforward and analogous to the corresponding procedure for nonprobabilistic
CFGs (Aho and Ullman 1972, Algorithm 2.10). The main difference is the updating of
rule probabilities, for which the &amp;expansion probabilities are again needed.
</bodyText>
<listItem confidence="0.996368">
1. Delete all null productions, except on the start symbol (in case the
grammar as a whole produces E with nonzero probability). Scale the
remaining production probabilities to sum to unity.
2. For each original rule X —&gt; AY/..t that contains a nonterminal Y such that
Y E:
</listItem>
<page confidence="0.995318">
184
</page>
<bodyText confidence="0.17711">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</bodyText>
<listItem confidence="0.894031">
(a) Create a variant rule X —+ Ap,
(b) Set the rule probability of the new rule to eyP(X -- )Y/1). If the
rule X -- Apt already exists, sum the probabilities.
(c) Decrement the old rule probability by the same amount.
Iterate these steps for all RHS occurrences of a null-able nonterminal.
</listItem>
<bodyText confidence="0.9997744">
The crucial step in this procedure is the addition of variants of the original produc-
tions that simulate the null productions by deleting the corresponding nonterminals
from the RHS. The spontaneous dot shifting described in the previous sections effec-
tively performs the same operation on the fly as the rules are used in prediction and
completion.
</bodyText>
<subsectionHeader confidence="0.994848">
4.8 Complexity Issues
</subsectionHeader>
<bodyText confidence="0.99998784">
The probabilistic extension of Earley&apos;s parser preserves the original control structure
in most aspects, the major exception being the collapsing of cyclic predictions and unit
completions, which can only make these steps more efficient. Therefore the complexity
analysis from Earley (1970) applies, and we only summarize the most important results
here.
The worst-case complexity for Earley&apos;s parser is dominated by the completion step,
which takes 0(12) for each input position, 1 being the length of the current prefix. The
total time is therefore 0(13) for an input of length 1, which is also the complexity of the
standard Inside/Outside (Baker 1979) and LRI (Jelinek and Lafferty 1991) algorithms.
For grammars of bounded ambiguity, the incremental per-word cost reduces to 0(1),
0(12) total. For deterministic CFGs the incremental cost is constant, 0(1) total. Because
of the possible start indices each state set can contain 0(1) Earley states, giving 0(12)
worst-case space complexity overall.
Apart from input length, complexity is also determined by grammar size. We
will not try to give a precise characterization in the case of sparse grammars (Ap-
pendix B.3 gives some hints on how to implement the algorithm efficiently for such
grammars). However, for fully parameterized grammars in CNF we can verify the
scaling of the algorithm in terms of the number of nonterminals n, and verify that it
has the same 0(n3) time and space requirements as the Inside/Outside (I/O) and LRI
algorithms.
The completion step again dominates the computation, which has to compute
probabilities for at most 0(n3) states. By organizing summations (11) and (12) so that
7&amp;quot; are first summed by LHS nonterminals, the entire completion operation can be
accomplished in 0(n3). The one-time cost for the matrix inversions to compute the
left-corner and unit production relation matrices is also 0(n3).
</bodyText>
<sectionHeader confidence="0.982436" genericHeader="method">
5. Extensions
</sectionHeader>
<bodyText confidence="0.9999162">
This section discusses extensions to the Earley algorithm that go beyond simple parsing
and the computation of prefix and string probabilities. These extensions are all quite
straightforward and well supported by the original Earley chart structure, which leads
us to view them as part of a single, unified algorithm for solving the tasks mentioned
in the introduction.
</bodyText>
<page confidence="0.992269">
185
</page>
<figure confidence="0.558404666666667">
Computational Linguistics Volume 21, Number 2
5.1 Viterbi Parses
Definition 7
</figure>
<bodyText confidence="0.997738125">
A Viterbi parse for a string x, in a grammar G, is a left-most derivation that assigns
maximal probability to x, among all possible derivations for x.
Both the definition of Viterbi parse and its computation are straightforward gener-
alizations of the corresponding notion for Hidden Markov Models (Rabiner and juang
1986), where one computes the Viterbi path (state sequence) through an HMM. Pre-
cisely the same approach can be used in the Earley parser, using the fact that each
derivation corresponds to a path.
The standard computational technique for Viterbi parses is applicable here. Wher-
ever the original parsing procedure sums probabilities that correspond to alternative
derivations of a grammatical entity, the summation is replaced by a maximization.
Thus, during the forward pass each state must keep track of the maximal path prob-
ability leading to it, as well as the predecessor states associated with that maximum
probability path. Once the final state is reached, the maximum probability parse can
be recovered by tracing back the path of &amp;quot;best&amp;quot; predecessor states.
The following modifications to the probabilistic Earley parser implement the for-
ward phase of the Viterbi computation.
</bodyText>
<listItem confidence="0.998146333333333">
• Each state computes an additional probability, its Viterbi probability v.
• Viterbi probabilities are propagated in the same way as inner
probabilities, except that during completion the summation is replaced
by maximization: v,(kX —&gt; )Y.p) is the maximum of all products
v, (I Y —&gt; v.)vj(kX A.Yp) that contribute to the completed state
kX AY.p. The same-position predecessor 1Y v. associated with the
maximum is recorded as the Viterbi path predecessor of kX —&gt; AY.p, (the
other predecessor state kX —&gt; A.Yp, can be inferred).
• The completion step uses the original recursion without collapsing unit
production loops. Loops are simply avoided, since they can only lower a
path&apos;s probability. Collapsing unit production completions has to be
avoided to maintain a continuous chain of predecessors for later
backtracing and parse construction.
• The prediction step does not need to be modified for the Viterbi
computation.
</listItem>
<bodyText confidence="0.902555833333333">
Once the final state is reached, a recursive procedure can recover the parse tree
associated with the Viterbi parse. This procedure takes an Earley state i : kX A.p
as input and produces the Viterbi parse for the substring between k and i as output.
(If the input state is not complete (p €), the result will be a partial parse tree with
children missing from the root node.)
Viterbi parse (i: kX A.p):
</bodyText>
<listItem confidence="0.959457">
1. If A = E, return a parse tree with root labeled X and no children.
</listItem>
<page confidence="0.9148">
186
</page>
<bodyText confidence="0.187558">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</bodyText>
<listItem confidence="0.8515155">
2. Otherwise, if A ends in a terminal a, let A&apos;a = A, and call this procedure
recursively to obtain the parse tree
T -= Viterbi-parse(i — 1: kX A/.46)
Adjoin a leaf node labeled a as the right-most child to the root of T and
return T.
3. Otherwise, if A ends in a nonterminal Y, let A&apos; Y --= A. Find the Viterbi
predecessor state 1Y —&gt; v. for the current state. Call this procedure
recursively to compute
</listItem>
<equation confidence="0.819561">
T = Viterbi-parse(j : kX A&apos;.Y/-1)
</equation>
<bodyText confidence="0.749852666666667">
as well as
T&apos; = Viterbi-parse(i : Y v.)
Adjoin T&apos; to T as the right-most child at the root, and return T.
</bodyText>
<subsectionHeader confidence="0.999643">
5.2 Rule Probability Estimation
</subsectionHeader>
<bodyText confidence="0.999077">
The rule probabilities in a SCFG can be iteratively estimated using the EM (Expectation-
Maximization) algorithm (Dempster et al. 1977). Given a sample corpus D, the esti-
mation procedure finds a set of parameters that represent a local maximum of the
grammar likelihood function P(D G), which is given by the product of the string
probabilities
</bodyText>
<equation confidence="0.99038">
P(D I G) = P(S X),
xED
</equation>
<bodyText confidence="0.999515863636364">
i.e., the samples are assumed to be distributed identically and independently.
The two steps of this algorithm can be briefly characterized as follows.
E-step: Compute expectations for how often each grammar rule is used, given the
corpus D and the current grammar parameters (rule probabilities).
M-step: Reset the parameters so as to maximize the likelihood relative to the
expected rule counts found in the E-step.
This procedure is iterated until the parameter values (as well as the likelihood) con-
verge. It can be shown that each round in the algorithm produces a likelihood that is
at least as high as the previous one; the EM algorithm is therefore guaranteed to find
at least a local maximum of the likelihood function.
EM is a generalization of the well-known Baum—Welch algorithm for HMM esti-
mation (Baum et al. 1970); the original formulation for the case of SCFGs is attributable
to Baker (1979). For SCFGs, the E-step involves computing the expected number of
times each production is applied in generating the training corpus. After that, the M-
step consists of a simple normalization of these counts to yield the new production
probabilities.
In this section we examine the computation of production count expectations re-
quired for the E-step. The crucial notion introduced by Baker (1979) for this purpose
is the &amp;quot;outer probability&amp;quot; of a nonterminal, or the joint probability that the nonter-
minal is generated with a given prefix and suffix of terminals. Essentially the same
method can be used in the Earley framework, after extending the definition of outer
probabilities to apply to arbitrary Earley states.
</bodyText>
<page confidence="0.984797">
187
</page>
<note confidence="0.45558">
Computational Linguistics Volume 21, Number 2
</note>
<subsectionHeader confidence="0.440804">
Definition 8
</subsectionHeader>
<bodyText confidence="0.9975775">
Given a string x, Ix1 = I, the outer probability 0,(kX A./t) of an Earley state is the
sum of the probabilities of all paths that
</bodyText>
<listItem confidence="0.99944">
1. start with the initial state,
2. generate the prefix xo ...xk-i
3. pass through kX —&gt; .vi, for some v,
4. generate the suffix x starting with state kX
5. end in the final state.
</listItem>
<bodyText confidence="0.990092">
Outer probabilities complement inner probabilities in that they refer precisely to
those parts of complete paths generating x not covered by the corresponding inner
probability -y,(kX —&gt; A.p). Therefore, the choice of the production X —&gt; Apt is not part
of the outer probability associated with a state kX A.A. In fact, the definition makes
no reference to the first part A of the RHS: all states sharing the same k, X, and it will
have identical outer probabilities.
Intuitively, f3 (kX —&gt; A.,u) is the probability that an Earley parser operating as a
string generator yields the prefix xo...k-1 and the suffix while passing through
state kX A././ at position i (which is independent of A). As was the case for forward
probabilities, ,C3 is actually an expectation of the number of such states in the path, as
unit production cycles can result in multiple occurrences for a single state. Again, we
gloss over this technicality in our terminology. The name is motivated by the fact that
,3 reduces to the &amp;quot;outer probability&amp;quot; of X, as defined in Baker (1979), if the dot is in
final position.
5.2.1 Computing expected production counts. Before going into the details of com-
puting outer probabilities, we describe their use in obtaining the expected rule counts
needed for the E-step in grammar estimation.
Let c(X —&gt; A x) denote the expected number of uses of production X —&gt; A in the
derivation of string x. Alternatively, c(X A I x) is the expected number of times that
X —&gt; A is used for prediction in a complete Earley path generating x. Let c(X A I P)
be the number of occurrences of predicted states based on production X —&gt; A along a
path P.
</bodyText>
<equation confidence="0.999338">
c(X —&gt; A x) E p(pis x.)c(x—, A I 2)
P derives x
1
E P(p, s x)c(x A I 1&apos;)
P(S x) P derives x
1
P(S xo...i-
P(S x
</equation>
<bodyText confidence="0.6465906">
The last summation is over all predicted states based on production X A. The
quantity P(S x0.../-1Xv x) is the sum of the probabilities of all paths passing
through i : ,X ---+ .A. Inner and outer probabilities have been defined such that this
quantity is obtained precisely as the product of the corresponding of and 13,. Thus,
v x).
</bodyText>
<page confidence="0.806676">
188
</page>
<figure confidence="0.58402975">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
the expected usage count for a rule can be computed as
1
c(X A I x)= AGX P(S x) .A)7,(,X .A).
</figure>
<bodyText confidence="0.945597">
The sum can be computed after completing both forward and backward passes (or
during the backward pass itself) by scanning the chart for predicted states.
5.2.2 Computing outer probabilities. The outer probabilities are computed by tracing
the complete paths from the final state to the start state, in a single backward pass over
the Earley chart. Only completion and scanning steps need to be traced back. Reverse
scanning leaves outer probabilities unchanged, so the only operation of concern is
reverse completion.
We describe reverse transitions using the same notation as for their forward coun-
terparts, annotating each state with its outer and inner probabilities.
Reverse completion. I /Y v• ]/3&amp;quot;,7&amp;quot;]
i: kX AY.,u [0,7] j kX -4 A.Yp, [r3C-Y1
for all pairs of states 1Y -4 v. and kX A.Ybt in the chart. Then
0/ +=
0/,
The inner probability 7 is not used.
Rationale. Relative to )3, 0&apos; is missing the probability of expanding Y, which is
filled in from 7&amp;quot;. The probability of the surrounding of Y(/3&amp;quot;) is the probability of the
surrounding of X(0), plus the choice of the rule of production for X and the expansion
of the partial LHS A, which are together given by 7&apos;.
Note that the computation makes use of the inner probabilities computed in the
forward pass. The particular way in which 7 and ,3 were defined turns out to be
convenient here, as no reference to the production probabilities themselves needs to
be made in the computation.
As in the forward pass, simple reverse completion would not terminate in the pres-
ence of cyclic unit productions. A version that collapses all such chains of productions
is given below.
</bodyText>
<subsectionHeader confidence="0.678256">
Reverse completion (transitive).
</subsectionHeader>
<equation confidence="0.57798">
i: )Y —&gt; v. 7&amp;quot;]
1 j: kX A.Z,u
</equation>
<bodyText confidence="0.8973055">
for all pairs of states 1Y -4 v. and kX A.Zp, in the chart, such that the unit production
relation R(Z 4 Y) is nonzero. Then
</bodyText>
<equation confidence="0.9816975">
0/
/3&amp;quot; += 7&apos; • OR(Z 4 Y)
</equation>
<bodyText confidence="0.870503">
The first summation is carried out once for each state j : kX A.Z,u, whereas the
second summation is applied for each choice of Z, but only if X AZp, is not itself a
unit production, i.e., Aft c.
</bodyText>
<equation confidence="0.670377">
i: kX —&gt; AZ,Lt [0,7]
</equation>
<page confidence="0.988044">
189
</page>
<note confidence="0.743897">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.99762975">
Rationale. This increments 13&amp;quot; the equivalent of R(Z 4. Y) times, accounting for
the infinity of surroundings in which Y can occur if it can be derived through cyclic
productions. Note that the computation of 0&apos; is unchanged, since ry&amp;quot; already includes
an infinity of cyclically generated subtrees for Y, where appropriate.
</bodyText>
<subsectionHeader confidence="0.999982">
5.3 Parsing Bracketed Inputs
</subsectionHeader>
<bodyText confidence="0.9999657">
The estimation procedure described above (and EM-based estimators in general) are
only guaranteed to find locally optimal parameter estimates. Unfortunately, it seems
that in the case of unconstrained SCFG estimation local maxima present a very real
problem, and make success dependent on chance and initial conditions (Lan i and
Young 1990). Pereira and Schabes (1992) showed that partially bracketed input samples
can alleviate the problem in certain cases. The bracketing information constrains the
parse of the inputs, and therefore the parameter estimates, steering it clear from some
of the suboptimal solutions that could otherwise be found.
An Earley parser can be minimally modified to take advantage of bracketed strings
by invoking itself recursively when a left parenthesis is encountered. The recursive in-
stance of the parser is passed any predicted states at that position, processes the input
up to the matching right parenthesis, and hands complete states back to the invoking
instance. This technique is efficient, as it never explicitly rejects parses not consistent
with the bracketing. It is also convenient, as it leaves the basic parser operations,
including the left-to-right processing and the probabilistic computations, unchanged.
For example, prefix probabilities conditioned on partial bracketings could be computed
easily this way.
Parsing bracketed inputs is described in more detail in Stolcke (1993), where it is
also shown that bracketing gives the expected improved efficiency. For example, the
modified Earley parser processes fully bracketed inputs in linear time.
</bodyText>
<subsectionHeader confidence="0.999022">
5.4 Robust Parsing
</subsectionHeader>
<bodyText confidence="0.999686470588235">
In many applications ungrammatical input has to be dealt with in some way. Tra-
ditionally it has been seen as a drawback of top-down parsing algorithms such as
Earley&apos;s that they sacrifice &amp;quot;robustness,&amp;quot; i.e., the ability to find partial parses in an
ungrammatical input, for the efficiency gained from top-down prediction (Magerman
and Weir 1992).
One approach to the problem is to build robustness into the grammar itself. In the
simplest case one could add top-level productions
where X can expand to any nonterminal, including an &amp;quot;unknown word&amp;quot; category. This
grammar will cause the Earley parser to find all partial parses of substrings, effectively
behaving like a bottom-up parser constructing the chart in left-to-right fashion. More
refined variations are possible: the top-level productions could be used to model which
phrasal categories (sentence fragments) can likely follow each other. This probabilistic
information can then be used in a pruning version of the Earley parser (Section 6.1)
to arrive at a compromise between robust and expectation-driven parsing.
An alternative method for making Earley parsing more robust is to modify the
parser itself so as to accept arbitrary input and find all or a chosen subset of pos-
sible substring parses. In the case of Earley&apos;s parser there is a simple extension to
</bodyText>
<page confidence="0.988457">
190
</page>
<note confidence="0.421469">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</note>
<bodyText confidence="0.946860153846154">
accomplish just that, based on the notion of a wildcard state
where the wildcard ? stands for an arbitrary continuation of the RHS.
During prediction, a wildcard to the left of the dot causes the chart to be seeded
with dummy states .X for each phrasal category X of interest. Conversely, a minimal
modification to the standard completion step allows the wildcard states to collect all
abutting substring parses:
i: 11( —› it. 1 i : —&gt; AY.?
j: k
for all Y. This way each partial parse will be represented by exactly one wildcard state
in the final chart position.
A detailed account of this technique is given in Stolcke (1993). One advantage
over the grammar-modifying approach is that it can be tailored to use various criteria
at runtime to decide which partial parses to follow.
</bodyText>
<sectionHeader confidence="0.998471" genericHeader="method">
6. Discussion
</sectionHeader>
<subsectionHeader confidence="0.999149">
6.1 Online Pruning
</subsectionHeader>
<bodyText confidence="0.999958590909091">
In finite-state parsing (especially speech decoding) one often makes use of the forward
probabilities for pruning partial parses before having seen the entire input. Pruning
is formally straightforward in Earley parsers: in each state set, rank states according
to their a values, then remove those states with small probabilities compared to the
current best candidate, or simply those whose rank exceeds a given limit. Notice this
will not only omit certain parses, but will also underestimate the forward and inner
probabilities of the derivations that remain. Pruning procedures have to be evaluated
empirically since they invariably sacrifice completeness and, in the case of the Viterbi
algorithm, optimality of the result.
While Earley-based on-line pruning awaits further study, there is reason to be-
lieve the Earley framework has inherent advantages over strategies based only on
bottom-up information (including so-called &amp;quot;over-the-top&amp;quot; parsers). Context-free for-
ward probabilities include all available probabilistic information (subject to assump-
tions implicit in the SCFG formalism) available from an input prefix, whereas the
usual inside probabilities do not take into account the nonterminal prior probabilities
that result from the top-down relation to the start state. Using top-down constraints
does not necessarily mean sacrificing robustness, as discussed in Section 5.4. On the
contrary, by using Earley-style parsing with a set of carefully designed and estimated
&amp;quot;fault-tolerant&amp;quot; top-level productions, it should be possible to use probabilities to bet-
ter advantage in robust parsing. This approach is a subject of ongoing work, in the
context of tight-coupling SCFGs with speech decoders (Jurafsky, Wooters, Segal, Stol-
cke, Fosler, Tajchman, and Morgan 1995).
</bodyText>
<subsectionHeader confidence="0.999738">
6.2 Relation to Probabilistic LR Parsing
</subsectionHeader>
<bodyText confidence="0.99973625">
One of the major alternative context-free parsing paradigms besides Earley&apos;s algo-
rithm is LR parsing (Aho and Ullman 1972). A comparison of the two approaches,
both in their probabilistic and nonprobabilistic aspects, is interesting and provides
useful insights. The following remarks assume familiarity with both approaches. We
</bodyText>
<page confidence="0.995213">
191
</page>
<note confidence="0.749924">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.958115058823529">
sketch the fundamental relations, as well as the important tradeoffs between the two
frameworks.13
Like an Earley parser, LR parsing uses dotted productions, called items, to keep
track of the progress of derivations as the input is processed. The start indices are not
part of LR items: we may therefore use the term &amp;quot;item&amp;quot; to refer to both LR items and
Earley states without start indices. An Earley parser constructs sets of possible items
on the fly, by following all possible partial derivations. An LR parser, on the other
hand, has access to a complete list of sets of possible items computed beforehand, and at
runtime simply follows transitions between these sets. The item sets are known as the
&amp;quot;states&amp;quot; of the LR parser.&apos; A grammar is suitable for LR parsing if these transitions can
be performed deterministically by considering only the next input and the contents
of a shift-reduce stack. Generalized LR parsing is an extension that allows parallel
tracking of multiple state transitions and stack actions by using a graph-structured
stack (Tomita 1986).
Probabilistic LR parsing (Wright 1990) is based on LR items augmented with
certain conditional probabilities. Specifically, the probability p associated with an LR
item X -4 A.,tt is, in our terminology, a normalized forward probability:
</bodyText>
<equation confidence="0.997576666666667">
a,(X A.p)
P=
P(S xo
</equation>
<bodyText confidence="0.999983210526316">
where the denominator is the probability of the current prefix.&apos; LR item probabilities,
are thus conditioned forward probabilities, and can be used to compute conditional
probabilities of next words: P(x, xo...1_1) is the sum of the p&apos;s of all items having x,
to the right of the dot (extra work is required if the item corresponds to a &amp;quot;reduce&amp;quot;
state, i.e., if the dot is in final position).
Notice that the definition of p is independent of i as well as the start index of
the corresponding Earley state. Therefore, to ensure that item probabilities are correct
independent of input position, item sets would have to be constructed so that their
probabilities are unique within each set. However, this may be impossible given that
the probabilities can take on infinitely many values and in general depend on the his-
tory of the parse. The solution used by Wright (1990) is to collapse items whose prob-
abilities are within a small tolerance E and are otherwise identical. The same threshold
is used to simplify a number of other technical problems, e.g., left-corner probabilities
are computed by iterated prediction, until the resulting changes in probabilities are
smaller than E. Subject to these approximations, then, a probabilistic LR parser can
compute prefix probabilities by multiplying successive conditional probabilities for
the words it sees.16
As an alternative to the computation of LR transition probabilities from a given
SCFG, one might instead estimate such probabilities directly from traces of parses
</bodyText>
<footnote confidence="0.901723">
13 Like Earley parsers, LR parsers can be built using various amounts of /ookahead to make the operation
of the parser (more) deterministic, and hence more efficient. Only the case of zero-lookahead, LR(0), is
considered here; the correspondence between LR(k) parsers and k-lookahead Earley parsers is
discussed in the literature (Earley 1970; Aho and Ullman 1972).
14 Once again, it is helpful to compare this to a closely related finite-state concept: the states of the LR
parser correspond to sets of Earley states, similar to the way the states of a deterministic FSA
correspond to sets of states of an equivalent nondeterministic FSA under the standard subset
construction.
15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction
on the steps performed to compute the p&apos;s, as shown in Stolcke (1993).
16 It is not clear what the numerical properties of this approximation are, e.g., how the errors will
accumulate over longer parses.
</footnote>
<page confidence="0.988028">
192
</page>
<bodyText confidence="0.973070458333333">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
on a training corpus. Because of the imprecise relationship between LR probabilities
and SCFG probabilities, it is not clear if the model thus estimated corresponds to any
particular SCFG in the usual sense.
Briscoe and Carroll (1993) turn this incongruity into an advantage by using the
LR parser as a probabilistic model in its own right, and show how LR probabilities
can be extended to capture non—context-free contingencies. The problem of capturing
more complex distributional constraints in natural language is clearly important, but
well beyond the scope of this article. We simply remark that it should be possible to
define &amp;quot;interesting&amp;quot; nonstandard probabilities in terms of Earley parser actions so as
to better model non—context-free phenomena.
Apart from such considerations, the choice between LR methods and Earley pars-
ing is a typical space-time tradeoff. Even though an Earley parser runs with the same
linear time and space complexity as an LR parser on grammars of the appropriate LR
class, the constant factors involved will be much in favor of the LR parser, as almost all
the work has already been compiled into its transition and action table. However, the
size of LR parser tables can be exponential in the size of the grammar (because of the
number of potential item subsets). Furthermore, if the generalized LR method is used
for dealing with nondeterministic grammars (Tomita 1986) the runtime on arbitrary
inputs may also grow exponentially. The bottom line is that each application&apos;s needs
have to be evaluated against the pros and cons of both approaches to find the best
solution. From a theoretical point of view, the Earley approach has the inherent appeal
of being the more general (and exact) solution to the computation of the various SCFG
probabilities.
</bodyText>
<subsectionHeader confidence="0.999837">
6.3 Other Related Work
</subsectionHeader>
<bodyText confidence="0.99934312">
The literature on Earley-based probabilistic parsers is sparse, presumably because of
the precedent set by the Inside/Outside algorithm, which is more naturally formulated
as a bottom-up algorithm.
Both Nakagawa (1987) and Easeler (1988) use a nonprobabilistic Earley parser aug-
mented with &amp;quot;word match&amp;quot; scoring. Though not truly probabilistic, these algorithms
are similar to the Viterbi version described here, in that they find a parse that optimizes
the accumulated matching scores (without regard to rule probabilities). Prediction and
completion loops do not come into play since no precise inner or forward probabilities
are computed.
Magerman and Marcus (1991) are interested primarily in scoring functions to guide
a parser efficiently to the most promising parses. Earley-style top-down prediction is
used only to suggest worthwhile parses, not to compute precise probabilities, which
they argue would be an inappropriate metric for natural language parsing.
Casacuberta and Vidal (1988) exhibit an Earley parser that processes weighted (not
necessarily probabilistic) CFGs and performs a computation that is isomorphic to that
of inside probabilities shown here. Schabes (1991) adds both inner and outer proba-
bilities to Earley&apos;s algorithm, with the purpose of obtaining a generalized estimation
algorithm for SCFGs. Both of these approaches are restricted to grammars without
unbounded ambiguities, which can arise from unit or null productions.
Dan Jurafsky (personal communication) wrote an Earley parser for the Berke-
ley Restaurant Project (BeRP) speech understanding system that originally computed
forward probabilities for restricted grammars (without left-corner or unit production
recursion). The parser now uses the method described here to provide exact SCFG pre-
fix and next-word probabilities to a tightly coupled speech decoder (Jurafsky, Wooters,
Segal, Stolcke, Fosler, Tajchman, and Morgan 1995).
</bodyText>
<page confidence="0.997589">
193
</page>
<note confidence="0.751039">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.999951233333333">
An essential idea in the probabilistic formulation of Earley&apos;s algorithm is the
collapsing of recursive predictions and unit completion chains, replacing both with
lookups in precomputed matrices. This idea arises in our formulation out of the need
to compute probability sums given as infinite series. Graham, Harrison, and Ruzzo
(1980) use a nonprobabilistic version of the same technique to create a highly opti-
mized Earley-like parser for general CFGs that implements prediction and completion
by operations on Boolean matrices.&apos;
The matrix inversion method for dealing with left-recursive prediction is borrowed
from the LRI algorithm of Jelinek and Lafferty (1991) for computing prefix probabilities
for SCFGs in CNF.18 We then use that idea a second time to deal with the similar
recursion arising from unit productions in the completion step. We suspect, but have
not proved, that the Earley computation of forward probabilities when applied to a
CNF grammar performs a computation that is isomorphic to that of the LRI algorithm.
In any case, we believe that the parser-oriented view afforded by the Earley framework
makes for a very intuitive solution to the prefix probability problem, with the added
advantage that it is not restricted to CNF grammars.
Algorithms for probabilistic CFGs can be broadly characterized along several di-
mensions. One such dimension is whether the quantities entered into the parser chart
are defined in a bottom-up (CYK) fashion, or whether left-to-right constraints are an
inherent part of their definition.19 The probabilistic Earley parser shares the inherent
left-to-right character of the LRI algorithm, and contrasts with the bottom-up I/O
algorithm.
Probabilistic parsing algorithms may also be classified as to whether they are for-
mulated for fully parameterized CNF grammars or arbitrary context-free rules (typ-
ically taking advantage of grammar sparseness). In this respect the Earley approach
contrasts with both the CNF-oriented I/O and LRI algorithms. Another approach to
avoiding the CNF constraint is a formulation based on probabilistic Recursive Tran-
sition Networks (RTNs) (Kupiec 1992). The similarity goes further, as both Kupiec&apos;s
and our approach is based on state transitions, and dotted productions (Earley states)
turn out to be equivalent to RTN states if the RTN is constructed from a CFG.
</bodyText>
<sectionHeader confidence="0.977527" genericHeader="method">
7. Conclusions
</sectionHeader>
<bodyText confidence="0.9999904">
We have presented an Earley-based parser for stochastic context-free grammars that
is appealing for its combination of advantages over existing methods. Earley&apos;s control
structure lets the algorithm run with best-known complexity on a number of gram-
mar subclasses, and no worse than standard bottom-up probabilistic chart parsers on
general SCFGs and fully parameterized CNF grammars.
Unlike bottom-up parsers it also computes accurate prefix probabilities incremen-
tally while scanning its input, along with the usual substring (inside) probabilities. The
chart constructed during parsing supports both Viterbi parse extraction and Baum—
Welch type rule probability estimation by way of a backward pass over the parser
chart. If the input comes with (partial) bracketing to indicate phrase structure, this
</bodyText>
<footnote confidence="0.837494333333333">
17 This connection to the CHR algorithm was pointed out by Fernando Pereira. Exploration of this link
then led to the extension of our algorithm to handle &amp;productions, as described in Section 4.7.
18 Their method uses the transitive (but not reflexive) closure over the left-corner relation PL, for which
they chose the symbol QL. We chose the symbol RL in this article to point to this difference.
19 Of course a CYK-style parser can operate left-to-right, right-to-left, or otherwise by reordering the
computation of chart entries.
</footnote>
<page confidence="0.994425">
194
</page>
<bodyText confidence="0.925413">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
information can be easily incorporated to restrict the allowable parses. A simple ex-
tension of the Earley chart allows finding partial parses of ungrammatical input.
The computation of probabilities is conceptually simple, and follows directly Ear-
ley&apos;s parsing framework, while drawing heavily on the analogy to finite-state language
models. It does not require rewriting the grammar into normal form. Thus, the present
algorithm fills a gap in the existing array of algorithms for SCFGs, efficiently combin-
ing the functionalities and advantages of several previous approaches.
</bodyText>
<sectionHeader confidence="0.948697" genericHeader="method">
Appendix A: Existence of RL and Ru
</sectionHeader>
<bodyText confidence="0.999874166666667">
In Section 4.5 we defined the probabilistic left-corner and unit-production matrices RL
and Ru, respectively, to collapse recursions in the prediction and completion steps. It
was shown how these matrices could be obtained as the result of matrix inversions.
In this appendix we give a proof that the existence of these inverses is assured if the
grammar is well-defined in the following three senses. The terminology used here is
taken from Booth and Thompson (1973).
</bodyText>
<sectionHeader confidence="0.675841" genericHeader="method">
Definition 9
</sectionHeader>
<bodyText confidence="0.893911333333333">
For an SCFG G over an alphabet E, with start symbol S, we say that&apos;
a) G is proper iff for all nonterminals X the rule&amp;quot; probabilities sum to unity,
i.e.,
</bodyText>
<equation confidence="0.856867">
A:(X—■A)EG
b) G is consistent iff it defines a probability distribution over finite strings,
i.e.,
E P(S x) = 1,
xcE*
</equation>
<bodyText confidence="0.935286692307692">
where P(S 4 x) is induced by the rule probabilities according to
Definition 1(a).
c) G has no useless nonterminals iff all nonterminals X appear in at least
one derivation of some string x E E* with nonzero probability, i.e.,
P(S AX 4&apos; x) &gt; 0.
It is useful to translate consistency into &amp;quot;process&amp;quot; terms. We can view an SCFG as
a stochastic string-rewriting process, in which each step consists of simultaneously re-
placing all nonterminals in a sentential form with the right-hand sides of productions,
randomly drawn according to the rule probabilities. Booth and Thompson (1973) show
that the grammar is consistent if and only if the probability that stochastic rewriting
of the start symbol S leaves nonterminals remaining after n steps, goes to 0 as n oo.
More loosely speaking, rewriting S has to terminate after a finite number of steps with
probability 1, or else the grammar is inconsistent.
</bodyText>
<footnote confidence="0.9617008">
20 Unfortunately, the terminology used in the literature is not uniform. For example, Jelinek and Lafferty
(1991) use the term &amp;quot;proper&amp;quot; to mean (c), and &amp;quot;well-defined&amp;quot; for (b). They also state mistakenly that (a)
and (c) together are a sufficient condition for (b). Booth and Thompson (1973) show that one can write
a SCFG that satisfies (a) and (c) but generates derivations that do not terminate with probability 1, and
give necessary and sufficient conditions for (b).
</footnote>
<page confidence="0.992177">
195
</page>
<note confidence="0.725936">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.999474">
We observe that the same property holds not only for S, but for all nontermi-
nals, if the grammar has no useless terminals. If any nonterminal X admitted infinite
derivations with nonzero probability, then S itself would have such derivations, since
by assumption X is reachable from S with nonzero probability.
To prove the existence of RL and Ru, it is sufficient to show that the corresponding
geometric series converge:
</bodyText>
<equation confidence="0.9219605">
RL = I + + + ••• = (I - PL)l
Ru I + +131+ • • • = (I - Pu)l.
</equation>
<bodyText confidence="0.82439175">
Lemma 5
If G is a proper, consistent SCFG without useless nonterminals, then the powers FL&apos;
of the left-corner relation, and PIZ of the unit production relation, converge to zero as
n oo.
</bodyText>
<subsectionHeader confidence="0.741551">
Proof
</subsectionHeader>
<bodyText confidence="0.999632">
Entry (X, Y) in the left-corner matrix PL is the probability of generating Y as the
immediately succeeding left-corner below X. Similarly, entry (X, Y) in the nth power
PI! is the probability of generating Y as the left-corner of X with n - 1 intermediate
nonterminals. Certainly .13&apos;1&amp;quot;(X, Y) is bounded above by the probability that the entire
derivation starting at X terminates after n steps, since a derivation couldn&apos;t terminate
without expanding the left-most symbol to a terminal (as opposed to a nonterminal).
But that probability tends to 0 as n oo, and hence so must each entry in P.
For the unit production matrix Pu a similar argument applies, since the length of
a derivation is at least as long as it takes to terminate any initial unit production chain.
</bodyText>
<subsectionHeader confidence="0.664792">
Lemma 6
</subsectionHeader>
<bodyText confidence="0.9980955">
If G is a proper, consistent SCFG without useless nonterminals, then the series for RL
and Ru as defined above converge to finite, non-negative values.
</bodyText>
<subsectionHeader confidence="0.891863">
Proof
</subsectionHeader>
<bodyText confidence="0.999653166666667">
PI converging to 0 implies that the magnitude of PL&apos;s largest eigenvalue (its spectral
radius) is &lt; 1, which in turn implies that the series E,`&amp;quot;f 0 13&apos;L converges (similarly for
Pu). The elements of RL and Ru are non-negative since they are the result of adding
and multiplying among the non-negative elements of PL and Pu, respectively.
Interestingly, a SCFG may be inconsistent and still have converging left-corner
and/or unit production matrices, i.e., consistency is a stronger constraint. For example
</bodyText>
<equation confidence="0.4779135">
S -÷ a [p]
S SS [q]
</equation>
<bodyText confidence="0.94283675">
is inconsistent for any choice of q &gt; but the left-corner relation (a single number
in this case) is well defined for all q &lt; 1, namely (1 - q)-1 p-1. In this case the left
fringe of the derivation is guaranteed to result in a terminal after finitely many steps,
but the derivation as a whole may never terminate.
</bodyText>
<page confidence="0.995395">
196
</page>
<note confidence="0.417461">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</note>
<sectionHeader confidence="0.68082" genericHeader="discussions">
Appendix B: Implementation Notes
</sectionHeader>
<bodyText confidence="0.989286024390244">
This appendix discusses some of the experiences gained from implementing the prob-
abilistic Earley parser.
B.1 Prediction
Because of the collapse of transitive predictions, this step can be implemented in a very
efficient and straightforward manner. As explained in Section 4.5, one has to perform
a single pass over the current state set, identifying all nonterminals Z occurring to the
right of dots, and add states corresponding to all productions Y v that are reachable
through the left-corner relation Z =L Y. As indicated in equation (13), contributions
to the forward probabilities of new states have to be summed when several paths lead
to the same state. However, the summation in equation (13) can be optimized if the
a values for all old states with the same nonterminal Z are summed first, and then
multiplied by R(Z 4L Y). These quantities are then summed over all nonterminals Z,
and the result is once multiplied by the rule probability P(Y v) to give the forward
probability for the predicted state.
B.2 Completion
Unlike prediction, the completion step still involves iteration. Each complete state de-
rived by completion can potentially feed other completions. An important detail here
is to ensure that all contributions to a state&apos;s a and -y are summed before proceeding
with using that state as input to further completion steps.
One approach to this problem is to insert complete states into a prioritized queue.
The queue orders states by their start indices, highest first. This is because states
corresponding to later expansions always have to be completed first before they can
lead to the completion of expansions earlier on in the derivation. For each start index,
the entries are managed as a first-in, first-out queue, ensuring that the dependency
graph formed by the states is traversed in breadth-first order.
The completion pass can now be implemented as follows. Initially, all complete
states from the previous scanning step are inserted in the queue. States are then re-
moved from the front of the queue and used to complete other states. Among the new
states thus produced, complete ones are again added to the queue. The process iterates
until no more states remain in the queue. Because the computation of probabilities al-
ready includes chains of unit productions, states derived from such productions need
not be queued, which also ensures that the iteration terminates.
A similar queuing scheme, with the start index order reversed, can be used for the
reverse completion step needed in the computation of outer probabilities (Section 5.2).
B.3 Efficient Parsing with Large Sparse Grammars
During work with a moderate-sized, application-specific natural language grammar
taken from the BeRP speech system (Jurafsky, Wooters, Tajchman, Segal, Stolcke, Foster,
and Morgan 1994) we had an opportunity to optimize our implementation of the
algorithm. Below we relate some of the lessons learned in the process.
B.3.1 Speeding up matrix inversions. Both prediction and completion steps make use
of a matrix R defined as a geometric series derived from a matrix P.
</bodyText>
<equation confidence="0.948303">
R = I + P +P2 + • • • = (I — P)-1.
</equation>
<page confidence="0.981655">
197
</page>
<note confidence="0.716344">
Computational Linguistics Volume 21, Number 2
</note>
<bodyText confidence="0.999802380952381">
Both P and R are indexed by the nonterminals in the grammar. The matrix P is de-
rived from the SCFG rules and probabilities (either the left-corner relation or the unit
production relation).
For an application using a fixed grammar the time taken by the precomputation
of left-corner and unit production matrices may not be crucial, since it occurs off-
line. There are cases, however, when that cost should be minimized, e.g., when rule
probabilities are iteratively reestimated.
Even if the matrix P is sparse, the matrix inversion can be prohibitive for large
numbers of nonterminals n. Empirically, matrices of rank n with a bounded number
p of nonzero entries in each row (i.e., p is independent of n) can be inverted in time
0(n2), whereas a full matrix of size n x n would require time 0(n3).
In many cases the grammar has a relatively small number of nonterminals that
have productions involving other nonterminals in a left-corner (or the RHS of a unit
production). Only those nonterminals can have nonzero contributions to the higher
powers of the matrix P. This fact can be used to substantially reduce the cost of the
matrix inversion needed to compute R.
Let P&apos; be a subset of the entries of P. namely, only those elements indexed by non-
terminals that have a nonempty row in P. For example, for the left-corner computation,
P&apos; is obtained from P by deleting all rows and columns indexed by nonterminals that
do not have productions starting with nonterminals. Let I&apos; be the identity matrix over
the same set of nonterminals as P&apos;. Then R can be computed as
</bodyText>
<equation confidence="0.999727666666667">
R = I+(/+P+P2+..-)P
= / + (r + P&apos; + P&apos;2 + . - .)*P
= I + (r - ID&apos; yi * P
</equation>
<bodyText confidence="0.979356095238095">
= I+IV*P.
Here R&apos; is the inverse of I&apos; —P&apos;, and * denotes a matrix multiplication in which the left
operand is first augmented with zero elements to match the dimensions of the right
operand, P.
The speedups obtained with this technique can be substantial. For a grammar
with 789 nonterminals, of which only 132 have nonterminal productions, the left-
corner matrix was computed in 12 seconds (including the final multiply with P and
addition of I). Inversion of the full matrix I — P took 4 minutes, 28 seconds.21
B.3.2 Linking and bottom-up filtering. As discussed in Section 4.8, the worst-case
run-time on fully parameterized CNF grammars is dominated by the completion step.
However, this is not necessarily true of sparse grammars. Our experiments showed that
the computation is dominated by the generation of Earley states during the prediction
steps.
It is therefore worthwhile to minimize the total number of predicted states gen-
erated by the parser. Since predicted states only affect the derivation if they lead to
subsequent scanning, we can use the next input symbol to constrain the relevant pre-
dictions. To this end, we compute the extended left-corner relation Rif, indicating
which terminals can appear as left corners of which nonterminals. Ru- is a Boolean
21 These figures are not very meaningful for their absolute values. All measurements were obtained on a
Sun SPARCstation 2 with a CommonLisp/CLOS implementation of generic sparse matrices that was
not particularly optimized for this task.
</bodyText>
<page confidence="0.990427">
198
</page>
<note confidence="0.42344">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
</note>
<bodyText confidence="0.984403">
matrix with rows indexed by nonterminals and columns indexed by terminals. It can
be computed as the product
</bodyText>
<equation confidence="0.795307">
RLT = RLPLT
</equation>
<bodyText confidence="0.999906705882353">
where Pu has a nonzero entry at (X, a) iff there is a production for nonterminal X that
starts with terminal a. RL is the old left-corner relation.
During the prediction step we can ignore incoming states whose RHS nonterminal
following the dot cannot have the current input as a left-corner, and then eliminate
from the remaining predictions all those whose LHS cannot produce the current input
as a left-corner. These filtering steps are very fast, as they involve only table lookup.
This technique for speeding up Earley prediction is the exact converse of the
&amp;quot;linking&amp;quot; method described by Pereira and Shieber (1987, chapter 6) for improving
the efficiency of bottom-up parsers. There, the extended left-corner relation is used
for top-down filtering the bottom-up application of grammar rules. In our case, we use
linking to provide bottom-up filtering for top-down application of productions.
On a test corpus this technique cut the number of generated predictions to al-
most one-fourth and speeded up parsing by a factor of 3.3. The corpus consisted of
1,143 sentence with an average length of 4.65 words. The top-down prediction alone
generated 991,781 states and parsed at a rate of 590 milliseconds per sentence. With
bottom-up filtered prediction only 262,287 states were generated, resulting in 180 mil-
liseconds per sentence.
</bodyText>
<sectionHeader confidence="0.99149" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997632625">
Thanks are due Dan Jurafsky and Steve
Omohundro for extensive discussions on
the topics in this paper, and Fernando
Pereira for helpful advice and pointers.
Jerry Feldman, Terry Regier, Jonathan Segal,
Kevin Thompson, and the anonymous
reviewers provided valuable comments for
improving content and presentation.
</bodyText>
<sectionHeader confidence="0.995037" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850586206897">
Aho, Alfred V., and Ullman, Jeffrey D.
(1972). The Theory of Parsing, Translation,
and Compiling, Volume 1: Parsing.
Prentice-Hall.
Bahl, Lalit R.; Jelinek, Frederick; and Mercer,
Robert L. (1983). &amp;quot;A maximum likelihood
approach to continuous speech
recognition.&amp;quot; IEEE Transactions on Pattern
Analysis and Machine Intelligence, 5(2),
179-190.
Baker, James K. (1979). &amp;quot;Trainable grammars
for speech recognition.&amp;quot; In Speech
Communication Papers for the 97th Meeting of
the Acoustical Society of America, edited by
Jared J. Wolf and Dennis H. Klatt,
547-550.
Baum, Leonard E.; Petrie, Ted; Soules,
George; and Weiss, Norman (1970). &amp;quot;A
maximization technique occuring in the
statistical analysis of probabilistic
functions in Markov chains.&amp;quot; The Annals of
Mathematical Statistics, 41(1), 164-171.
Booth, Taylor L., and Thompson, Richard A.
(1973). &amp;quot;Applying probability measures to
abstract languages.&amp;quot; IEEE Transactions on
Computers, C-22(5), 442-450.
Briscoe, Ted, and Carroll, John (1993).
&amp;quot;Generalized probabilistic LR parsing of
natural language (corpora) with
unification-based grammars.&amp;quot;
Computational Linguistics, 19(1), 25-59.
Casacuberta, F., and Vidal, E. (1988). &amp;quot;A
parsing algorithm for weighted grammars
and substring recognition.&amp;quot; In Syntactic
and Structural Pattern Recognition, Volume
F45, NATO ASI Series, edited by Gabriel
Ferrate, Theo Pavlidis, Alberto Sanfeliu,
and Horst Bunke, 51-67. Springer Verlag.
Corazza, Anna; De Mori, Renato; Gretter,
Roberto; and Satta, Giorgio (1991).
&amp;quot;Computation of probabilities for an
island-driven parser.&amp;quot; IEEE Transactions on
Pattern Analysis and Machine Intelligence,
13(9), 936-950.
Dempster, A. P.; Laird, N. M.; and Rubin,
D. B. (1977). &amp;quot;Maximum likelihood from
incomplete data via the EM algorithm.&amp;quot;
Journal of the Royal Statistical Society, Series
B, 34,1-38.
Earley, Jay (1970). &amp;quot;An efficient context-free
parsing algorithm.&amp;quot; Communications of the
ACM, 6(8), 451-455.
Fujisaki, T.; Jelinek, F.; Cocke, J.; Black, E.;
and Nishino, T. (1991). &amp;quot;A probabilistic
parsing method for sentence
disambiguation.&amp;quot; In Current Issues in
Parsing Technology, edited by Masaru
Tomita, 139-152. Kluwer Academic
</reference>
<page confidence="0.97665">
199
</page>
<note confidence="0.293786">
Computational Linguistics Volume 21, Number 2
</note>
<reference confidence="0.997424073770492">
Publishers.
Graham, Susan L.; Harrison, Michael A.;
and Ruzzo, Walter L. (1980). &amp;quot;An
improved context-free recognizer.&amp;quot; ACM
Transactions on Programming Languages and
Systems, 2(3), 415-462.
Jelinek, Frederick (1985). &amp;quot;Markov source
modeling of text generation.&amp;quot; In The
Impact of Processing Techniques on
Communications, Volume E91, NATO ASI
Series, edited by J. K. Skwirzynski,
569-598. Nijhoff.
Jelinek, Frederick, and Lafferty, John D.
(1991). &amp;quot;Computation of the probability of
initial substring generation by stochastic
context-free grammars.&amp;quot; Computational
Linguistics, 17(3), 315-323.
Jelinek, Frederick; Lafferty, John D.; and
Mercer, Robert L. (1992). &amp;quot;Basic methods
of probabilistic context free grammars.&amp;quot;
In Speech Recognition and Understanding.
Recent Advances, Trends, and Applications,
Volume F75, NATO ASI Series, edited by
Pietro Laface and Renato De Mori,
345-360. Springer Verlag.
Jones, Mark A., and Eisner, Jason M. (1992).
&amp;quot;A probabilistic parser and its
applications.&amp;quot; In AAAI Workshop on
Statistically-Based NLP Techniques, 20-27.
Jurafsky, Daniel; Wooters, Chuck; Segal,
Jonathan; Stolcke, Andreas; Fosler, Eric;
Tajchman, Gary; and Morgan, Nelson
(1995). &amp;quot;Using a stochastic context-free
grammar as a language model for speech
recognition.&amp;quot; In Proceedings, IEEE
Conference on Acoustics, Speech and Signal
Processing, 189-192. Detroit, Michigan.
Jurafsky, Daniel; Wooters, Chuck; Tajchman,
Gary; Segal, Jonathan; Stolcke, Andreas;
Fosler, Eric; and Morgan, Nelson (1994).
&amp;quot;The Berkeley restaurant project.&amp;quot; In
Proceedings, International Conference on
Spoken Language Processing, 4,2139-2142.
Yokohama, Japan.
Kupiec, Julian (1992). &amp;quot;Hidden Markov
estimation for unrestricted stochastic
context-free grammars.&amp;quot; In Proceedings,
IEEE Conference on Acoustics, Speech and
Signal Processing, 1, 177-180, San
Francisco, California.
Lan, K., and Young, S. J. (1990). &amp;quot;The
estimation of stochastic context-free
grammars using the Inside-Outside
algorithm.&amp;quot; Computer Speech and Language,
4,35-56.
Lan, K., and Young, S. J. (1991).
&amp;quot;Applications of stochastic context-free
grammars using the Inside-Outside
algorithm.&amp;quot; Computer Speech and Language,
5,237-257.
Magerman, David M., and Marcus,
Mitchell P. (1991). &amp;quot;Pearl: A probabilistic
chart parser.&amp;quot; In Proceedings, Second
International Workshop on Parsing
Technologies. Cancun, Mexico, 193-199.
Magerman, David M., and Weir, Carl (1992).
&amp;quot;Efficiency, robustness and accuracy in
Picky chart parsing.&amp;quot; In Proceedings, 30th
Annual Meeting of the Association for
Computational Linguistics. Newark,
Delaware, 40-47.
Nakagawa, Sei-ichi (1987). &amp;quot;Spoken
sentence recognition by time-synchronous
parsing algorithm of context-free
grammar.&amp;quot; In Proceedings, IEEE Conference
on Acoustics, Speech and Signal Processing, 2,
829-832. Dallas, Texas.
Ney, Hermann (1992). &amp;quot;Stochastic grammars
and pattern recognition.&amp;quot; In Speech
Recognition and Understanding. Recent
Advances, Trends, and Applications, volume
F75 of NATO ASI Series, edited by Pietro
Laface and Renato De Mori, 319-344.
Springer Verlag.
Paseler, Annedore (1988). &amp;quot;Modification of
Earley&apos;s algorithm for speech
recognition.&amp;quot; In Recent Advances in Speech
Understanding and Dialog Systems, volume
F46 of NATO ASI Series, edited by
H. Niemann, M. Lang, and G. Sagerer,
466-472. Springer Verlag.
Pereira, Fernando C. N., and Schabes, Yves
(1992). &amp;quot;Inside-outside reestimation from
partially bracketed corpora.&amp;quot; In
Proceedings, 30th Annual Meeting of the
Association for Computational Linguistics.
Newark, Delaware, 128-135.
Pereira, Fernando C. N., and Shieber,
Stuart M. (1987). Prolog and
Natural-Language Analysis. CLSI Lecture
Notes Series, Number 10. Center for the
Study of Language and Information,
Stanford, California.
Rabiner, L. R., and Juang, B. H. (1986). &amp;quot;An
introduction to hidden Markov models.&amp;quot;
IEEE ASSP Magazine, 3(1), 4-16.
Schabes, Yves (1991). &amp;quot;An inside-outside
algorithm for estimating the parameters
of a hidden stochastic context-free
grammar based on Earley&apos;s algorithm.&amp;quot;
Paper presented at the Second Workshop
on Mathematics of Language, Tarrytown,
New York.
Stolcke, Andreas, and Segal, Jonathan
(1994). &amp;quot;Precise n-gram probabilities from
stochastic context-free grammars.&amp;quot; In
Proceedings, 31th Annual Meeting of the
Association for Computational Linguistics.
Las Cruces, New Mexico, 74-79.
Stolcke, Andreas (1993). &amp;quot;An efficient
probabilistic context-free parsing
algorithm that computes prefix
</reference>
<page confidence="0.85869">
200
</page>
<reference confidence="0.957118583333333">
Andreas Stolcke Efficient Probabilistic Context-Free Parsing
probabilities.&amp;quot; Technical Report
TR-93-065, International Computer
Science Institute, Berkeley, CA. Revised
1994.
Tomita, Masaru (1986). Efficient Parsing for
Natural Language. Kluwer Academic
Publishers.
Wright, J. H. (1990). &amp;quot;LR parsing of
probabilistic grammars with input
uncertainty for speech recognition.&amp;quot;
Computer Speech and Language, 4, 297-323.
</reference>
<page confidence="0.998402">
201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881225">
<title confidence="0.999695666666667">An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities</title>
<author confidence="0.999835">Andreas Stolcke</author>
<affiliation confidence="0.974957333333333">University of California at Berkeley and International Computer Science Institute</affiliation>
<abstract confidence="0.995448416666667">We describe an extension of Earley&apos;s parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley&apos;s top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing.</booktitle>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="7952" citStr="Aho and Ullman (1972" startWordPosition="1209" endWordPosition="1212">ions for solving the Viterbi and training tasks, for processing partially bracketed inputs, and for finding partial parses. Section 6 discusses miscellaneous issues and relates our work to the literature on the subject. In Section 7 we summarize and draw some conclusions. To get an overall idea of probabilistic Earley parsing it should be sufficient to read Sections 3, 4.2, and 4.4. Section 4.5 deals with a crucial technicality, and later sections mostly fill in details and add optional features. We assume the reader is familiar with the basics of context-free grammar theory, such as given in Aho and Ullman (1972, Chapter 2). Some prior familiarity with probabilistic context-free grammars will also be helpful. Jelinek, Lafferty, and Mercer (1992) provide a tutorial introduction covering the standard algorithms for the four tasks mentioned in the introduction. Notation. The input string is denoted by x. Ix&apos; is the length of x. Individual input symbols are identified by indices starting at 0: x0, x1,.. ,x1_1. The input alphabet is denoted by E. Substrings are identified by beginning and end positions x..1. The variables i,j,k are reserved for integers referring to positions in input strings. Latin capit</context>
<context position="9666" citStr="Aho and Ullman (1972" startWordPosition="1486" endWordPosition="1489">ns that are consistent with the input string up to a certain point. As more and more of the input is revealed, the set of possible derivations (each of which corresponds to a parse) can either expand as new choices are introduced, or shrink as a result of resolved ambiguities. In describing the parser it is thus appropriate and convenient to use generation terminology. The parser keeps a set of states for each position in the input, describing all pending derivations.2 These state sets together form the Earley chart. A state is of the 2 Earley states are also known as items in LR parsing; see Aho and Ullman (1972, Section 5.2) and Section 6.2. 167 Computational Linguistics Volume 21, Number 2 form 1: kX where X is a nonterminal of the grammar, A and it are strings of nonterminals and/or terminals, and i and k are indices into the input string. States are derived from productions in the grammar. The above state is derived from a corresponding production X with the following semantics: • The current position in the input is i, i.e., xo xi_1 have been processed so far.3 The states describing the parser state at position i are collectively called state set i. Note that there is one more state set than inp</context>
<context position="15696" citStr="Aho and Ullman (1972)" startWordPosition="2540" endWordPosition="2543">ned 3Det —&gt; a. completed 3NP —4 Det.N predicted 5N —&gt; .circle 4N —4 .square 4N —&gt; .triangle scanned 4N —4 triangle. completed 4NP —&gt; Det N. 3VP —&gt; VT NP. oS NP VP. 0 -*5. State set 0 1 2 3 4 5 It is easy to see that Earley parser operations are correct, in the sense that each chain of transitions (predictions, scanning steps, completions) corresponds to a possible (partial) derivation. Intuitively, it is also true that a parser that performs these transitions exhaustively is complete, i.e., it finds all possible derivations. Formal proofs of these properties are given in the literature; e.g., Aho and Ullman (1972). The relationship between Earley transitions and derivations will be stated more formally in the next section. The parse trees for sentences can be reconstructed from the chart contents. We will illustrate this in Section 5 when discussing Viterbi parses. Table 1 shows a simple grammar and a trace of Earley parser operation on a sample sentence. Earley&apos;s parser can deal with any type of context-free rule format, even with null or &amp;productions, i.e., those that replace a nonterminal with the empty string. Such productions do, however, require special attention, and make the algorithm and its d</context>
<context position="21993" citStr="Aho and Ullman 1972" startWordPosition="3617" endWordPosition="3620">will use &amp;quot;derivation&amp;quot; to imply a left-most derivation. Lemma 1 a) An Earley parser generates state i: kX if and only if there is a partial derivation S xo...k-iXv deriving a prefix x0...,_1 of the input. b) There is a one-to-one mapping between partial derivations and Earley paths, such that each production X v applied in a derivation corresponds to a predicted Earley state X —&gt; .v. 172 Andreas Stolcke Efficient Probabilistic Context-Free Parsing (a) is the invariant underlying the correctness and completeness of Earley&apos;s algorithm; it can be proved by induction on the length of a derivation (Aho and Ullman 1972, Theorem 4.9). The slightly stronger form (b) follows from (a) and the way possible prediction steps are defined. Since we have established that paths correspond to derivations, it is convenient to associate derivation probabilities directly with paths. The uniqueness condition (b) above, which is irrelevant to the correctness of a standard Earley parser, justifies (probabilistic) counting of paths in lieu of derivations. Definition 3 The probability P(P) of a path P is the product of the probabilities of all rules used in the predicted states occurring in P. Lemma 2 a) For all paths P starti</context>
<context position="49554" citStr="Aho and Ullman 1972" startWordPosition="8548" endWordPosition="8551">spontaneous dot shifting. When completing a state X —&gt; A.Yit and moving the dot to get X —&gt; AY.it, additional states have to be added, obtained by moving the dot further over any nonterminals in that have nonzero &amp;expansion probability. As in prediction, forward and inner probabilities are multiplied by the corresponding &amp;expansion probabilities. 4.7.4 Eliminating null productions. Given these added complications one might consider simply eliminating all &amp;productions in a preprocessing step. This is mostly straightforward and analogous to the corresponding procedure for nonprobabilistic CFGs (Aho and Ullman 1972, Algorithm 2.10). The main difference is the updating of rule probabilities, for which the &amp;expansion probabilities are again needed. 1. Delete all null productions, except on the start symbol (in case the grammar as a whole produces E with nonzero probability). Scale the remaining production probabilities to sum to unity. 2. For each original rule X —&gt; AY/..t that contains a nonterminal Y such that Y E: 184 Andreas Stolcke Efficient Probabilistic Context-Free Parsing (a) Create a variant rule X —+ Ap, (b) Set the rule probability of the new rule to eyP(X -- )Y/1). If the rule X -- Apt alread</context>
<context position="69142" citStr="Aho and Ullman 1972" startWordPosition="11769" endWordPosition="11772">sarily mean sacrificing robustness, as discussed in Section 5.4. On the contrary, by using Earley-style parsing with a set of carefully designed and estimated &amp;quot;fault-tolerant&amp;quot; top-level productions, it should be possible to use probabilities to better advantage in robust parsing. This approach is a subject of ongoing work, in the context of tight-coupling SCFGs with speech decoders (Jurafsky, Wooters, Segal, Stolcke, Fosler, Tajchman, and Morgan 1995). 6.2 Relation to Probabilistic LR Parsing One of the major alternative context-free parsing paradigms besides Earley&apos;s algorithm is LR parsing (Aho and Ullman 1972). A comparison of the two approaches, both in their probabilistic and nonprobabilistic aspects, is interesting and provides useful insights. The following remarks assume familiarity with both approaches. We 191 Computational Linguistics Volume 21, Number 2 sketch the fundamental relations, as well as the important tradeoffs between the two frameworks.13 Like an Earley parser, LR parsing uses dotted productions, called items, to keep track of the progress of derivations as the input is processed. The start indices are not part of LR items: we may therefore use the term &amp;quot;item&amp;quot; to refer to both L</context>
<context position="72613" citStr="Aho and Ullman 1972" startWordPosition="12320" endWordPosition="12323"> probabilities by multiplying successive conditional probabilities for the words it sees.16 As an alternative to the computation of LR transition probabilities from a given SCFG, one might instead estimate such probabilities directly from traces of parses 13 Like Earley parsers, LR parsers can be built using various amounts of /ookahead to make the operation of the parser (more) deterministic, and hence more efficient. Only the case of zero-lookahead, LR(0), is considered here; the correspondence between LR(k) parsers and k-lookahead Earley parsers is discussed in the literature (Earley 1970; Aho and Ullman 1972). 14 Once again, it is helpful to compare this to a closely related finite-state concept: the states of the LR parser correspond to sets of Earley states, similar to the way the states of a deterministic FSA correspond to sets of states of an equivalent nondeterministic FSA under the standard subset construction. 15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p&apos;s, as shown in Stolcke (1993). 16 It is not clear what the numerical properties of this approximation are, e.g., how the errors will accum</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, Alfred V., and Ullman, Jeffrey D. (1972). The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.&amp;quot;</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>179--190</pages>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, Lalit R.; Jelinek, Frederick; and Mercer, Robert L. (1983). &amp;quot;A maximum likelihood approach to continuous speech recognition.&amp;quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2), 179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.&amp;quot; In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, edited by</title>
<date>1979</date>
<pages>547--550</pages>
<contexts>
<context position="4091" citStr="Baker 1979" startWordPosition="604" endWordPosition="605"> in this article can compute solutions to all four of these problems in a single framework, with a number of additional advantages over previously presented isolated solutions. Most probabilistic parsers are based on a generalization of bottom-up chart parsing, such as the CYK algorithm. Partial parses are assembled just as in nonprobabilistic parsing (modulo possible pruning based on probabilities), while substring probabilities (also known as &amp;quot;inside&amp;quot; probabilities) can be computed in a straightforward way. Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4) (Baker 1979), as well as (2) (Jelinek 1985). While the Jelinek and Lafferty (1991) solution to problem (3) is not a direct extension of CYK parsing, the authors nevertheless present their algorithm in terms of its similarities to the computation of inside probabilities. In our algorithm, computations for tasks (1) and (3) proceed incrementally, as the parser scans its input from left to right; in particular, prefix probabilities are available as soon as the prefix has been seen, and are updated incrementally as it is extended. Tasks (2) and (4) require one more (reverse) pass over the chart constructed fr</context>
<context position="24570" citStr="Baker (1979)" startWordPosition="4036" endWordPosition="4037">over the constrained incomplete paths is the sought-after sum over all complete derivations generating the prefix. 4.3 Forward and Inner Probabilities Since string and prefix probabilities are the result of summing derivation probabilities, the goal is to compute these sums efficiently by taking advantage of the Earley control structure. This can be accomplished by attaching two probabilistic quantities to each Earley state, as follows. The terminology is derived from analogous or similar quantities commonly used in the literature on Hidden Markov Models (HMMs) (Rabiner and Juang 1986) and in Baker (1979). Definition 4 The following definitions are relative to an implied input string x. a) The forward probability a, (kX --&gt; A.A) is the sum of the probabilities of all constrained paths of length i that end in state kX 173 Computational Linguistics Volume 21, Number 2 b) The inner probability 7,(kX ).p) is the sum of the probabilities of all paths of length i - k that start in state k: kX .Au and end in kX A.p, and generate the input symbols xk • • • x1-1. It helps to interpret these quantities in terms of an unconstrained Earley parser that operates as a generator emitting—rather than recognizi</context>
<context position="26719" citStr="Baker (1979)" startWordPosition="4395" endWordPosition="4396">canned states, a is always a probability, since by definition a scanned state can occur only once along a path. The inner probabilities, on the other hand, represent the probability of generating a substring of the input from a given nonterminal, using a particular production. Inner probabilities are thus conditional on the presence of a given nonterminal X with expansion starting at position k, unlike the forward probabilities, which include the generation history starting with the initial state. The inner probabilities as defined here correspond closely to the quantities of the same name in Baker (1979). The sum of -y of all states with a given LHS X is exactly Baker&apos;s inner probability for X. The following is essentially a restatement of Lemma 2 in terms of forward and inner probabilities. It shows how to obtain the sentence and string probabilities we are interested in, provided that forward and inner probabilities can be computed effectively. Lemma 3 The following assumes an Earley chart constructed by the parser on an input string x with Ix I = 1. a) Provided that S L x0...k_iXv is a possible left-most derivation of the grammar (for some v), the probability that a nonterminal X generates</context>
<context position="28459" citStr="Baker 1979" startWordPosition="4692" endWordPosition="4693">Parsing b) In particular, the string probability P(S x) can be computed as7 P(S x) = -y1(0 —S.) al(0 S.) c) The prefix probability P(S L x), with Ix = I, can be computed as P(S x) = (sum of forward probabilities over all scanned states). The restriction in (a) that X be preceded by a possible prefix is necessary, since the Earley parser at position i will only pursue derivations that are consistent with the input up to position i. This constitutes the main distinguishing feature of Earley parsing compared to the strict bottom-up computation used in the standard inside probability computation (Baker 1979). There, inside probabilities for all positions and nonterminals are computed, regardless of possible prefixes. 4.4 Computing Forward and Inner Probabilities Forward and inner probabilities not only subsume the prefix and string probabilities, they are also straightforward to compute during a run of Earley&apos;s algorithm. In fact, if it weren&apos;t for left-recursive and unit productions their computation would be trivial. For the purpose of exposition we will therefore ignore the technical complications introduced by these productions for a moment, and then return to them once the overall picture ha</context>
<context position="51345" citStr="Baker 1979" startWordPosition="8837" endWordPosition="8838">er preserves the original control structure in most aspects, the major exception being the collapsing of cyclic predictions and unit completions, which can only make these steps more efficient. Therefore the complexity analysis from Earley (1970) applies, and we only summarize the most important results here. The worst-case complexity for Earley&apos;s parser is dominated by the completion step, which takes 0(12) for each input position, 1 being the length of the current prefix. The total time is therefore 0(13) for an input of length 1, which is also the complexity of the standard Inside/Outside (Baker 1979) and LRI (Jelinek and Lafferty 1991) algorithms. For grammars of bounded ambiguity, the incremental per-word cost reduces to 0(1), 0(12) total. For deterministic CFGs the incremental cost is constant, 0(1) total. Because of the possible start indices each state set can contain 0(1) Earley states, giving 0(12) worst-case space complexity overall. Apart from input length, complexity is also determined by grammar size. We will not try to give a precise characterization in the case of sparse grammars (Appendix B.3 gives some hints on how to implement the algorithm efficiently for such grammars). H</context>
<context position="57561" citStr="Baker (1979)" startWordPosition="9861" endWordPosition="9862">. M-step: Reset the parameters so as to maximize the likelihood relative to the expected rule counts found in the E-step. This procedure is iterated until the parameter values (as well as the likelihood) converge. It can be shown that each round in the algorithm produces a likelihood that is at least as high as the previous one; the EM algorithm is therefore guaranteed to find at least a local maximum of the likelihood function. EM is a generalization of the well-known Baum—Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is attributable to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus. After that, the Mstep consists of a simple normalization of these counts to yield the new production probabilities. In this section we examine the computation of production count expectations required for the E-step. The crucial notion introduced by Baker (1979) for this purpose is the &amp;quot;outer probability&amp;quot; of a nonterminal, or the joint probability that the nonterminal is generated with a given prefix and suffix of terminals. Essentially the same method can be us</context>
<context position="59711" citStr="Baker (1979)" startWordPosition="10227" endWordPosition="10228">l outer probabilities. Intuitively, f3 (kX —&gt; A.,u) is the probability that an Earley parser operating as a string generator yields the prefix xo...k-1 and the suffix while passing through state kX A././ at position i (which is independent of A). As was the case for forward probabilities, ,C3 is actually an expectation of the number of such states in the path, as unit production cycles can result in multiple occurrences for a single state. Again, we gloss over this technicality in our terminology. The name is motivated by the fact that ,3 reduces to the &amp;quot;outer probability&amp;quot; of X, as defined in Baker (1979), if the dot is in final position. 5.2.1 Computing expected production counts. Before going into the details of computing outer probabilities, we describe their use in obtaining the expected rule counts needed for the E-step in grammar estimation. Let c(X —&gt; A x) denote the expected number of uses of production X —&gt; A in the derivation of string x. Alternatively, c(X A I x) is the expected number of times that X —&gt; A is used for prediction in a complete Earley path generating x. Let c(X A I P) be the number of occurrences of predicted states based on production X —&gt; A along a path P. c(X —&gt; A </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, James K. (1979). &amp;quot;Trainable grammars for speech recognition.&amp;quot; In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, edited by Jared J. Wolf and Dennis H. Klatt, 547-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
<author>Ted Petrie</author>
<author>George Soules</author>
<author>Norman Weiss</author>
</authors>
<title>A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains.&amp;quot;</title>
<date>1970</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>41</volume>
<issue>1</issue>
<pages>164--171</pages>
<contexts>
<context position="57481" citStr="Baum et al. 1970" startWordPosition="9846" endWordPosition="9849">e is used, given the corpus D and the current grammar parameters (rule probabilities). M-step: Reset the parameters so as to maximize the likelihood relative to the expected rule counts found in the E-step. This procedure is iterated until the parameter values (as well as the likelihood) converge. It can be shown that each round in the algorithm produces a likelihood that is at least as high as the previous one; the EM algorithm is therefore guaranteed to find at least a local maximum of the likelihood function. EM is a generalization of the well-known Baum—Welch algorithm for HMM estimation (Baum et al. 1970); the original formulation for the case of SCFGs is attributable to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus. After that, the Mstep consists of a simple normalization of these counts to yield the new production probabilities. In this section we examine the computation of production count expectations required for the E-step. The crucial notion introduced by Baker (1979) for this purpose is the &amp;quot;outer probability&amp;quot; of a nonterminal, or the joint probability that the nonterminal is generated wi</context>
</contexts>
<marker>Baum, Petrie, Soules, Weiss, 1970</marker>
<rawString>Baum, Leonard E.; Petrie, Ted; Soules, George; and Weiss, Norman (1970). &amp;quot;A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains.&amp;quot; The Annals of Mathematical Statistics, 41(1), 164-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
<author>Richard A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.&amp;quot;</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>22</volume>
<issue>5</issue>
<pages>442--450</pages>
<contexts>
<context position="18565" citStr="Booth and Thompson (1973)" startWordPosition="3038" endWordPosition="3041">obability P(X 4 x) (of x given X) is the sum of the probabilities of all left-most derivations X • • •= x producing x from X.&apos; c) The sentence probability P(S x) (of x given G) is the string probability given the start symbol S of G. By definition, this is also the probability P(x I G) assigned to x by the grammar G. d) The prefix probability P(S 4L x) (of x given G) is the sum of the probabilities of all sentence strings having x as a prefix, P(S L x) P(S xy). yEE (In particular, P(S E) = 1). In the following, we assume that the probabilities in a SCFG are proper and consistent as defined in Booth and Thompson (1973), and that the grammar contains no useless nonterminals (ones that can never appear in a derivation). These restrictions ensure that all nonterminals define probability measures over strings; i.e., P(X x) is a proper distribution over x for all X. Formal definitions of these conditions are given in Appendix A. 5 In a left-most derivation each step replaces the nonterminal furthest to the left in the partially expanded string. The order of expansion is actually irrelevant for this definition, because of the multiplicative combination of production probabilities. We restrict summation to left-mo</context>
<context position="81950" citStr="Booth and Thompson (1973)" startWordPosition="13746" endWordPosition="13749">ng array of algorithms for SCFGs, efficiently combining the functionalities and advantages of several previous approaches. Appendix A: Existence of RL and Ru In Section 4.5 we defined the probabilistic left-corner and unit-production matrices RL and Ru, respectively, to collapse recursions in the prediction and completion steps. It was shown how these matrices could be obtained as the result of matrix inversions. In this appendix we give a proof that the existence of these inverses is assured if the grammar is well-defined in the following three senses. The terminology used here is taken from Booth and Thompson (1973). Definition 9 For an SCFG G over an alphabet E, with start symbol S, we say that&apos; a) G is proper iff for all nonterminals X the rule&amp;quot; probabilities sum to unity, i.e., A:(X—■A)EG b) G is consistent iff it defines a probability distribution over finite strings, i.e., E P(S x) = 1, xcE* where P(S 4 x) is induced by the rule probabilities according to Definition 1(a). c) G has no useless nonterminals iff all nonterminals X appear in at least one derivation of some string x E E* with nonzero probability, i.e., P(S AX 4&apos; x) &gt; 0. It is useful to translate consistency into &amp;quot;process&amp;quot; terms. We can vi</context>
<context position="83427" citStr="Booth and Thompson (1973)" startWordPosition="13999" endWordPosition="14002">hompson (1973) show that the grammar is consistent if and only if the probability that stochastic rewriting of the start symbol S leaves nonterminals remaining after n steps, goes to 0 as n oo. More loosely speaking, rewriting S has to terminate after a finite number of steps with probability 1, or else the grammar is inconsistent. 20 Unfortunately, the terminology used in the literature is not uniform. For example, Jelinek and Lafferty (1991) use the term &amp;quot;proper&amp;quot; to mean (c), and &amp;quot;well-defined&amp;quot; for (b). They also state mistakenly that (a) and (c) together are a sufficient condition for (b). Booth and Thompson (1973) show that one can write a SCFG that satisfies (a) and (c) but generates derivations that do not terminate with probability 1, and give necessary and sufficient conditions for (b). 195 Computational Linguistics Volume 21, Number 2 We observe that the same property holds not only for S, but for all nonterminals, if the grammar has no useless terminals. If any nonterminal X admitted infinite derivations with nonzero probability, then S itself would have such derivations, since by assumption X is reachable from S with nonzero probability. To prove the existence of RL and Ru, it is sufficient to s</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Booth, Taylor L., and Thompson, Richard A. (1973). &amp;quot;Applying probability measures to abstract languages.&amp;quot; IEEE Transactions on Computers, C-22(5), 442-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>25--59</pages>
<contexts>
<context position="2358" citStr="Briscoe and Carroll 1993" startWordPosition="333" endWordPosition="336">); to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non—finite-state acoustic and phonotactic modeling (Lani and Young 1991). In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Magerman and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993). Although clearly not a perfect model of natural language, stochastic context-free grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory providing a sound theoretical basis for ranking and pruning of parses, as well as for integration with models for nonsyntactic aspects of language. All of the applications listed above involve (or could potentially make use of) one or more of the following * Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail: stolcke@speech.sri.com. C) 1995 Association for Computational Li</context>
<context position="73538" citStr="Briscoe and Carroll (1993)" startWordPosition="12469" endWordPosition="12472">t construction. 15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p&apos;s, as shown in Stolcke (1993). 16 It is not clear what the numerical properties of this approximation are, e.g., how the errors will accumulate over longer parses. 192 Andreas Stolcke Efficient Probabilistic Context-Free Parsing on a training corpus. Because of the imprecise relationship between LR probabilities and SCFG probabilities, it is not clear if the model thus estimated corresponds to any particular SCFG in the usual sense. Briscoe and Carroll (1993) turn this incongruity into an advantage by using the LR parser as a probabilistic model in its own right, and show how LR probabilities can be extended to capture non—context-free contingencies. The problem of capturing more complex distributional constraints in natural language is clearly important, but well beyond the scope of this article. We simply remark that it should be possible to define &amp;quot;interesting&amp;quot; nonstandard probabilities in terms of Earley parser actions so as to better model non—context-free phenomena. Apart from such considerations, the choice between LR methods and Earley par</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, Ted, and Carroll, John (1993). &amp;quot;Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.&amp;quot; Computational Linguistics, 19(1), 25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>E Vidal</author>
</authors>
<title>A parsing algorithm for weighted grammars and substring recognition.&amp;quot;</title>
<date>1988</date>
<booktitle>In Syntactic and Structural Pattern Recognition, Volume F45,</booktitle>
<pages>51--67</pages>
<publisher>Springer Verlag.</publisher>
<location>NATO</location>
<note>ASI Series, edited by</note>
<contexts>
<context position="76115" citStr="Casacuberta and Vidal (1988)" startWordPosition="12869" endWordPosition="12872">ilar to the Viterbi version described here, in that they find a parse that optimizes the accumulated matching scores (without regard to rule probabilities). Prediction and completion loops do not come into play since no precise inner or forward probabilities are computed. Magerman and Marcus (1991) are interested primarily in scoring functions to guide a parser efficiently to the most promising parses. Earley-style top-down prediction is used only to suggest worthwhile parses, not to compute precise probabilities, which they argue would be an inappropriate metric for natural language parsing. Casacuberta and Vidal (1988) exhibit an Earley parser that processes weighted (not necessarily probabilistic) CFGs and performs a computation that is isomorphic to that of inside probabilities shown here. Schabes (1991) adds both inner and outer probabilities to Earley&apos;s algorithm, with the purpose of obtaining a generalized estimation algorithm for SCFGs. Both of these approaches are restricted to grammars without unbounded ambiguities, which can arise from unit or null productions. Dan Jurafsky (personal communication) wrote an Earley parser for the Berkeley Restaurant Project (BeRP) speech understanding system that or</context>
</contexts>
<marker>Casacuberta, Vidal, 1988</marker>
<rawString>Casacuberta, F., and Vidal, E. (1988). &amp;quot;A parsing algorithm for weighted grammars and substring recognition.&amp;quot; In Syntactic and Structural Pattern Recognition, Volume F45, NATO ASI Series, edited by Gabriel Ferrate, Theo Pavlidis, Alberto Sanfeliu, and Horst Bunke, 51-67. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Corazza</author>
<author>Renato De Mori</author>
<author>Roberto Gretter</author>
<author>Giorgio Satta</author>
</authors>
<title>Computation of probabilities for an island-driven parser.&amp;quot;</title>
<date>1991</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>13</volume>
<issue>9</issue>
<pages>936--950</pages>
<marker>Corazza, De Mori, Gretter, Satta, 1991</marker>
<rawString>Corazza, Anna; De Mori, Renato; Gretter, Roberto; and Satta, Giorgio (1991). &amp;quot;Computation of probabilities for an island-driven parser.&amp;quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(9), 936-950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.&amp;quot;</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>34--1</pages>
<contexts>
<context position="56420" citStr="Dempster et al. 1977" startWordPosition="9666" endWordPosition="9669"> to obtain the parse tree T -= Viterbi-parse(i — 1: kX A/.46) Adjoin a leaf node labeled a as the right-most child to the root of T and return T. 3. Otherwise, if A ends in a nonterminal Y, let A&apos; Y --= A. Find the Viterbi predecessor state 1Y —&gt; v. for the current state. Call this procedure recursively to compute T = Viterbi-parse(j : kX A&apos;.Y/-1) as well as T&apos; = Viterbi-parse(i : Y v.) Adjoin T&apos; to T as the right-most child at the root, and return T. 5.2 Rule Probability Estimation The rule probabilities in a SCFG can be iteratively estimated using the EM (ExpectationMaximization) algorithm (Dempster et al. 1977). Given a sample corpus D, the estimation procedure finds a set of parameters that represent a local maximum of the grammar likelihood function P(D G), which is given by the product of the string probabilities P(D I G) = P(S X), xED i.e., the samples are assumed to be distributed identically and independently. The two steps of this algorithm can be briefly characterized as follows. E-step: Compute expectations for how often each grammar rule is used, given the corpus D and the current grammar parameters (rule probabilities). M-step: Reset the parameters so as to maximize the likelihood relativ</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P.; Laird, N. M.; and Rubin, D. B. (1977). &amp;quot;Maximum likelihood from incomplete data via the EM algorithm.&amp;quot; Journal of the Royal Statistical Society, Series B, 34,1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.&amp;quot;</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>6</volume>
<issue>8</issue>
<pages>451--455</pages>
<contexts>
<context position="5788" citStr="Earley (1970)" startWordPosition="878" endWordPosition="879">ditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder (Bahl, Jelinek, and Mercer 1983). Another application in which prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs (Stolcke and Segal 1994). Here, too, efficient incremental computation saves time, since the work for common prefix strings can be shared. The key to most of the features of our algorithm is that it is based on the topdown parsing method for nonprobabilistic CFGs developed by Earley (1970). Earley&apos;s algorithm is appealing because it runs with best-known complexity on a number of special classes of grammars. In particular, Earley parsing is more efficient than the bottom-up methods in cases where top-down prediction can rule out potential parses of substrings. The worst-case computational expense of the algorithm (either for the complete input, or incrementally for each new word) is as good as that of the other 1 Their paper phrases these problem in terms of context-free probabilistic grammars, but they generalize in obvious ways to other classes of models. 166 Andreas Stolcke E</context>
<context position="12085" citStr="Earley (1970)" startWordPosition="1898" endWordPosition="1899">put symbol, and add new states to the chart. This is strongly suggestive of state transitions in finite-state models of language, parsing, etc. This analogy will be explored further in the probabilistic formulation later on. The three types of transitions operate as follows. Prediction. For each state 1: kX A.Y where Y is a nonterminal anywhere in the RHS, and for all rules Y v expanding Y, add states i: jY—.v. A state produced by prediction is called a predicted state. Each prediction corresponds to a potential expansion of a nonterminal in a left-most derivation. 3 This index is implicit in Earley (1970). We include it here for clarity. 168 Andreas Stolcke Efficient Probabilistic Context-Free Parsing Scanning. For each state : kX A.a where a is a terminal symbol that matches the current input x1, add the state i 1 : kX ---&gt; Aa.p, (move the dot over the current symbol). A state produced by scanning is called a scanned state. Scanning ensures that the terminals produced in a derivation match the input string. Completion. For each complete state i: 1Y v. and each state in set j, j &lt; i, that has Y to the right of the dot, j: kX --&gt; A.Y[t, add the state kX -4 AY .11, (move the dot over the current</context>
<context position="44418" citStr="Earley (1970)" startWordPosition="7677" endWordPosition="7678"> p2q q p2q p2q2 1 • pq = p2q 182 Andreas Stolcke Efficient Probabilistic Context-Free Parsing 4.7 Null Productions Null productions X —&gt; E introduce some complications into the relatively straightforward parser operation described so far, some of which are due specifically to the probabilistic aspects of parsing. This section summarizes the necessary modifications to process null productions correctly, using the previous description as a baseline. Our treatment of null productions follows the (nonprobabilistic) formulation of Graham, Harrison, and Ruzzo (1980), rather than the original one in Earley (1970). 4.7.1 Computing 1-expansion probabilities. The main problem with null productions is that they allow multiple prediction-completion cycles in between scanning steps (since null productions do not have to be matched against one or more input symbols). Our strategy will be to collapse all predictions and completions due to chains of null productions into the regular prediction and completion steps, not unlike the way recursive predictions/completions were handled in Section 4.5. A prerequisite for this approach is to precompute, for all nonterminals X, the probability that X expands to the emp</context>
<context position="50980" citStr="Earley (1970)" startWordPosition="8777" endWordPosition="8778">n of variants of the original productions that simulate the null productions by deleting the corresponding nonterminals from the RHS. The spontaneous dot shifting described in the previous sections effectively performs the same operation on the fly as the rules are used in prediction and completion. 4.8 Complexity Issues The probabilistic extension of Earley&apos;s parser preserves the original control structure in most aspects, the major exception being the collapsing of cyclic predictions and unit completions, which can only make these steps more efficient. Therefore the complexity analysis from Earley (1970) applies, and we only summarize the most important results here. The worst-case complexity for Earley&apos;s parser is dominated by the completion step, which takes 0(12) for each input position, 1 being the length of the current prefix. The total time is therefore 0(13) for an input of length 1, which is also the complexity of the standard Inside/Outside (Baker 1979) and LRI (Jelinek and Lafferty 1991) algorithms. For grammars of bounded ambiguity, the incremental per-word cost reduces to 0(1), 0(12) total. For deterministic CFGs the incremental cost is constant, 0(1) total. Because of the possibl</context>
<context position="72591" citStr="Earley 1970" startWordPosition="12318" endWordPosition="12319">ompute prefix probabilities by multiplying successive conditional probabilities for the words it sees.16 As an alternative to the computation of LR transition probabilities from a given SCFG, one might instead estimate such probabilities directly from traces of parses 13 Like Earley parsers, LR parsers can be built using various amounts of /ookahead to make the operation of the parser (more) deterministic, and hence more efficient. Only the case of zero-lookahead, LR(0), is considered here; the correspondence between LR(k) parsers and k-lookahead Earley parsers is discussed in the literature (Earley 1970; Aho and Ullman 1972). 14 Once again, it is helpful to compare this to a closely related finite-state concept: the states of the LR parser correspond to sets of Earley states, similar to the way the states of a deterministic FSA correspond to sets of states of an equivalent nondeterministic FSA under the standard subset construction. 15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p&apos;s, as shown in Stolcke (1993). 16 It is not clear what the numerical properties of this approximation are, e.g., how</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay (1970). &amp;quot;An efficient context-free parsing algorithm.&amp;quot; Communications of the ACM, 6(8), 451-455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisaki</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A probabilistic parsing method for sentence disambiguation.&amp;quot;</title>
<date>1991</date>
<booktitle>In Current Issues in Parsing Technology, edited by Masaru Tomita,</booktitle>
<pages>139--152</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1657" citStr="Fujisaki et al. 1991" startWordPosition="232" endWordPosition="235">ss any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. 1. Introduction Context-free grammars are widely used as models of natural language syntax. In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non—finite-state acoustic and phonotactic modeling (Lani and Young 1991). In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa 1987), or they are used with context-sensitive and/or semantic probabilitie</context>
</contexts>
<marker>Fujisaki, Jelinek, Cocke, Black, Nishino, 1991</marker>
<rawString>Fujisaki, T.; Jelinek, F.; Cocke, J.; Black, E.; and Nishino, T. (1991). &amp;quot;A probabilistic parsing method for sentence disambiguation.&amp;quot; In Current Issues in Parsing Technology, edited by Masaru Tomita, 139-152. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan L Graham</author>
<author>Michael A Harrison</author>
<author>Walter L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.&amp;quot;</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>415--462</pages>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Graham, Susan L.; Harrison, Michael A.; and Ruzzo, Walter L. (1980). &amp;quot;An improved context-free recognizer.&amp;quot; ACM Transactions on Programming Languages and Systems, 2(3), 415-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Markov source modeling of text generation.&amp;quot;</title>
<date>1985</date>
<booktitle>In The Impact of Processing Techniques on Communications, Volume E91, NATO ASI Series, edited</booktitle>
<pages>569--598</pages>
<publisher>Nijhoff.</publisher>
<contexts>
<context position="4122" citStr="Jelinek 1985" startWordPosition="610" endWordPosition="611">solutions to all four of these problems in a single framework, with a number of additional advantages over previously presented isolated solutions. Most probabilistic parsers are based on a generalization of bottom-up chart parsing, such as the CYK algorithm. Partial parses are assembled just as in nonprobabilistic parsing (modulo possible pruning based on probabilities), while substring probabilities (also known as &amp;quot;inside&amp;quot; probabilities) can be computed in a straightforward way. Thus, the CYK chart parser underlies the standard solutions to problems (1) and (4) (Baker 1979), as well as (2) (Jelinek 1985). While the Jelinek and Lafferty (1991) solution to problem (3) is not a direct extension of CYK parsing, the authors nevertheless present their algorithm in terms of its similarities to the computation of inside probabilities. In our algorithm, computations for tasks (1) and (3) proceed incrementally, as the parser scans its input from left to right; in particular, prefix probabilities are available as soon as the prefix has been seen, and are updated incrementally as it is extended. Tasks (2) and (4) require one more (reverse) pass over the chart constructed from the input. Incremental, left</context>
</contexts>
<marker>Jelinek, 1985</marker>
<rawString>Jelinek, Frederick (1985). &amp;quot;Markov source modeling of text generation.&amp;quot; In The Impact of Processing Techniques on Communications, Volume E91, NATO ASI Series, edited by J. K. Skwirzynski, 569-598. Nijhoff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.&amp;quot;</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<pages>315--323</pages>
<contexts>
<context position="3069" citStr="Jelinek and Lafferty (1991)" startWordPosition="434" endWordPosition="437">grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory providing a sound theoretical basis for ranking and pruning of parses, as well as for integration with models for nonsyntactic aspects of language. All of the applications listed above involve (or could potentially make use of) one or more of the following * Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail: stolcke@speech.sri.com. C) 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 2 standard tasks, compiled by Jelinek and Lafferty (1991)) 1. What is the probability that a given string x is generated by a grammar G? 2. What is the single most likely parse (or derivation) for x? 3. What is the probability that x occurs as a prefix of some string generated by G (the prefix probability of x)? 4. How should the parameters (e.g., rule probabilities) in G be chosen to maximize the probability over a training set of strings? The algorithm described in this article can compute solutions to all four of these problems in a single framework, with a number of additional advantages over previously presented isolated solutions. Most probabi</context>
<context position="4987" citStr="Jelinek and Lafferty (1991)" startWordPosition="745" endWordPosition="749">orithm, computations for tasks (1) and (3) proceed incrementally, as the parser scans its input from left to right; in particular, prefix probabilities are available as soon as the prefix has been seen, and are updated incrementally as it is extended. Tasks (2) and (4) require one more (reverse) pass over the chart constructed from the input. Incremental, left-to-right computation of prefix probabilities is particularly important since that is a necessary condition for using SCFGs as a replacement for finite-state language models in many applications, such a speech decoding. As pointed out by Jelinek and Lafferty (1991), knowing probabilities P(x0 . xi) for arbitrary prefixes xo . xi enables probabilistic prediction of possible follow-words xi+i, as P(xi-Fi Ixo . • • xi) = P(xo )/P(xo • .. xi). These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder (Bahl, Jelinek, and Mercer 1983). Another application in which prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs (Stolcke and Segal 1994). Here, too, efficient incremental computation saves time, since </context>
<context position="34506" citStr="Jelinek and Lafferty (1991)" startWordPosition="5689" endWordPosition="5692">on, each q to a choice of the second production. If we didn&apos;t care about finite computation the resulting geometric series could be computed by letting the prediction loop (and hence the summation) continue indefinitely. Fortunately, all repeated prediction steps, including those due to left-recursion in the productions, can be collapsed into a single, modified prediction step, and the corresponding sums computed in closed form. For this purpose we need a probabilistic version of the well-known parsing concept of a left corner, which is also at the heart of the prefix probability algorithm of Jelinek and Lafferty (1991). Definition 5 The following definitions are relative to a given SCFG G. a) Two nonterminals X and Y are said to be in a left-corner relation X —3L Y if there exists a production for X that has a RHS starting with Y, X —&gt; YA. 177 Computational Linguistics Volume 21, Number 2 b) The probabilistic left-corner relationl° PL -= PL(G) is the matrix of probabilities P(X Y), defined as the total probability of choosing a production for X that has Y as a left corner: P(X L Y) = E P(X -&gt; YA). X—&gt;YAEG c) The relation X 4L Y is defined as the reflexive, transitive closure of X Y, i.e., X Y iff X = Y or t</context>
<context position="51381" citStr="Jelinek and Lafferty 1991" startWordPosition="8841" endWordPosition="8844">inal control structure in most aspects, the major exception being the collapsing of cyclic predictions and unit completions, which can only make these steps more efficient. Therefore the complexity analysis from Earley (1970) applies, and we only summarize the most important results here. The worst-case complexity for Earley&apos;s parser is dominated by the completion step, which takes 0(12) for each input position, 1 being the length of the current prefix. The total time is therefore 0(13) for an input of length 1, which is also the complexity of the standard Inside/Outside (Baker 1979) and LRI (Jelinek and Lafferty 1991) algorithms. For grammars of bounded ambiguity, the incremental per-word cost reduces to 0(1), 0(12) total. For deterministic CFGs the incremental cost is constant, 0(1) total. Because of the possible start indices each state set can contain 0(1) Earley states, giving 0(12) worst-case space complexity overall. Apart from input length, complexity is also determined by grammar size. We will not try to give a precise characterization in the case of sparse grammars (Appendix B.3 gives some hints on how to implement the algorithm efficiently for such grammars). However, for fully parameterized gram</context>
<context position="77762" citStr="Jelinek and Lafferty (1991)" startWordPosition="13106" endWordPosition="13109">ulation of Earley&apos;s algorithm is the collapsing of recursive predictions and unit completion chains, replacing both with lookups in precomputed matrices. This idea arises in our formulation out of the need to compute probability sums given as infinite series. Graham, Harrison, and Ruzzo (1980) use a nonprobabilistic version of the same technique to create a highly optimized Earley-like parser for general CFGs that implements prediction and completion by operations on Boolean matrices.&apos; The matrix inversion method for dealing with left-recursive prediction is borrowed from the LRI algorithm of Jelinek and Lafferty (1991) for computing prefix probabilities for SCFGs in CNF.18 We then use that idea a second time to deal with the similar recursion arising from unit productions in the completion step. We suspect, but have not proved, that the Earley computation of forward probabilities when applied to a CNF grammar performs a computation that is isomorphic to that of the LRI algorithm. In any case, we believe that the parser-oriented view afforded by the Earley framework makes for a very intuitive solution to the prefix probability problem, with the added advantage that it is not restricted to CNF grammars. Algor</context>
<context position="83249" citStr="Jelinek and Lafferty (1991)" startWordPosition="13969" endWordPosition="13972">consists of simultaneously replacing all nonterminals in a sentential form with the right-hand sides of productions, randomly drawn according to the rule probabilities. Booth and Thompson (1973) show that the grammar is consistent if and only if the probability that stochastic rewriting of the start symbol S leaves nonterminals remaining after n steps, goes to 0 as n oo. More loosely speaking, rewriting S has to terminate after a finite number of steps with probability 1, or else the grammar is inconsistent. 20 Unfortunately, the terminology used in the literature is not uniform. For example, Jelinek and Lafferty (1991) use the term &amp;quot;proper&amp;quot; to mean (c), and &amp;quot;well-defined&amp;quot; for (b). They also state mistakenly that (a) and (c) together are a sufficient condition for (b). Booth and Thompson (1973) show that one can write a SCFG that satisfies (a) and (c) but generates derivations that do not terminate with probability 1, and give necessary and sufficient conditions for (b). 195 Computational Linguistics Volume 21, Number 2 We observe that the same property holds not only for S, but for all nonterminals, if the grammar has no useless terminals. If any nonterminal X admitted infinite derivations with nonzero prob</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>Jelinek, Frederick, and Lafferty, John D. (1991). &amp;quot;Computation of the probability of initial substring generation by stochastic context-free grammars.&amp;quot; Computational Linguistics, 17(3), 315-323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
</authors>
<title>Basic methods of probabilistic context free grammars.&amp;quot;</title>
<date>1992</date>
<booktitle>In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, Volume F75, NATO ASI Series, edited by Pietro Laface and Renato De Mori,</booktitle>
<pages>345--360</pages>
<publisher>Springer Verlag.</publisher>
<marker>Jelinek, Lafferty, Mercer, 1992</marker>
<rawString>Jelinek, Frederick; Lafferty, John D.; and Mercer, Robert L. (1992). &amp;quot;Basic methods of probabilistic context free grammars.&amp;quot; In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, Volume F75, NATO ASI Series, edited by Pietro Laface and Renato De Mori, 345-360. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Jones</author>
<author>Jason M Eisner</author>
</authors>
<title>A probabilistic parser and its applications.&amp;quot;</title>
<date>1992</date>
<booktitle>In AAAI Workshop on Statistically-Based NLP Techniques,</booktitle>
<contexts>
<context position="1734" citStr="Jones and Eisner 1992" startWordPosition="244" endWordPosition="247">combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs. 1. Introduction Context-free grammars are widely used as models of natural language syntax. In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non—finite-state acoustic and phonotactic modeling (Lani and Young 1991). In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Magerman and Weir 1992; Jones and Eisner 1992; B</context>
</contexts>
<marker>Jones, Eisner, 1992</marker>
<rawString>Jones, Mark A., and Eisner, Jason M. (1992). &amp;quot;A probabilistic parser and its applications.&amp;quot; In AAAI Workshop on Statistically-Based NLP Techniques, 20-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Chuck Wooters</author>
<author>Jonathan Segal</author>
<author>Andreas Stolcke</author>
<author>Eric Fosler</author>
<author>Gary Tajchman</author>
<author>Morgan</author>
</authors>
<title>Using a stochastic context-free grammar as a language model for speech recognition.&amp;quot;</title>
<date>1995</date>
<booktitle>In Proceedings, IEEE Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>189--192</pages>
<location>Nelson</location>
<marker>Jurafsky, Wooters, Segal, Stolcke, Fosler, Tajchman, Morgan, 1995</marker>
<rawString>Jurafsky, Daniel; Wooters, Chuck; Segal, Jonathan; Stolcke, Andreas; Fosler, Eric; Tajchman, Gary; and Morgan, Nelson (1995). &amp;quot;Using a stochastic context-free grammar as a language model for speech recognition.&amp;quot; In Proceedings, IEEE Conference on Acoustics, Speech and Signal Processing, 189-192. Detroit, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Chuck Wooters</author>
<author>Gary Tajchman</author>
<author>Jonathan Segal</author>
<author>Andreas Stolcke</author>
<author>Eric Fosler</author>
<author>Morgan</author>
</authors>
<title>The Berkeley restaurant project.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, International Conference on Spoken Language Processing,</booktitle>
<pages>4--2139</pages>
<location>Nelson</location>
<marker>Jurafsky, Wooters, Tajchman, Segal, Stolcke, Fosler, Morgan, 1994</marker>
<rawString>Jurafsky, Daniel; Wooters, Chuck; Tajchman, Gary; Segal, Jonathan; Stolcke, Andreas; Fosler, Eric; and Morgan, Nelson (1994). &amp;quot;The Berkeley restaurant project.&amp;quot; In Proceedings, International Conference on Spoken Language Processing, 4,2139-2142. Yokohama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Hidden Markov estimation for unrestricted stochastic context-free grammars.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, IEEE Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>177--180</pages>
<location>San Francisco, California.</location>
<contexts>
<context position="79236" citStr="Kupiec 1992" startWordPosition="13333" endWordPosition="13334">their definition.19 The probabilistic Earley parser shares the inherent left-to-right character of the LRI algorithm, and contrasts with the bottom-up I/O algorithm. Probabilistic parsing algorithms may also be classified as to whether they are formulated for fully parameterized CNF grammars or arbitrary context-free rules (typically taking advantage of grammar sparseness). In this respect the Earley approach contrasts with both the CNF-oriented I/O and LRI algorithms. Another approach to avoiding the CNF constraint is a formulation based on probabilistic Recursive Transition Networks (RTNs) (Kupiec 1992). The similarity goes further, as both Kupiec&apos;s and our approach is based on state transitions, and dotted productions (Earley states) turn out to be equivalent to RTN states if the RTN is constructed from a CFG. 7. Conclusions We have presented an Earley-based parser for stochastic context-free grammars that is appealing for its combination of advantages over existing methods. Earley&apos;s control structure lets the algorithm run with best-known complexity on a number of grammar subclasses, and no worse than standard bottom-up probabilistic chart parsers on general SCFGs and fully parameterized C</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Kupiec, Julian (1992). &amp;quot;Hidden Markov estimation for unrestricted stochastic context-free grammars.&amp;quot; In Proceedings, IEEE Conference on Acoustics, Speech and Signal Processing, 1, 177-180, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the Inside-Outside algorithm.&amp;quot;</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<marker>Lan, Young, 1990</marker>
<rawString>Lan, K., and Young, S. J. (1990). &amp;quot;The estimation of stochastic context-free grammars using the Inside-Outside algorithm.&amp;quot; Computer Speech and Language, 4,35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan</author>
<author>S J Young</author>
</authors>
<title>Applications of stochastic context-free grammars using the Inside-Outside algorithm.&amp;quot;</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--237</pages>
<marker>Lan, Young, 1991</marker>
<rawString>Lan, K., and Young, S. J. (1991). &amp;quot;Applications of stochastic context-free grammars using the Inside-Outside algorithm.&amp;quot; Computer Speech and Language, 5,237-257.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David M Magerman</author>
<author>Marcus</author>
</authors>
<marker>Magerman, Marcus, </marker>
<rawString>Magerman, David M., and Marcus,</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mitchell</author>
</authors>
<title>Pearl: A probabilistic chart parser.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, Second International Workshop on Parsing Technologies. Cancun,</booktitle>
<marker>Mitchell, 1991</marker>
<rawString>Mitchell P. (1991). &amp;quot;Pearl: A probabilistic chart parser.&amp;quot; In Proceedings, Second International Workshop on Parsing Technologies. Cancun, Mexico, 193-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
<author>Carl Weir</author>
</authors>
<title>Efficiency, robustness and accuracy in Picky chart parsing.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 30th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>40--47</pages>
<location>Newark, Delaware,</location>
<contexts>
<context position="2308" citStr="Magerman and Weir 1992" startWordPosition="324" endWordPosition="328">ficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non—finite-state acoustic and phonotactic modeling (Lani and Young 1991). In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Magerman and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993). Although clearly not a perfect model of natural language, stochastic context-free grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory providing a sound theoretical basis for ranking and pruning of parses, as well as for integration with models for nonsyntactic aspects of language. All of the applications listed above involve (or could potentially make use of) one or more of the following * Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025. E-mail: stolcke@speech</context>
<context position="7036" citStr="Magerman and Weir 1992" startWordPosition="1061" endWordPosition="1065">ntext-Free Parsing known specialized algorithms, but can be substantially better on well-known grammar classes. Earley&apos;s parser (and hence ours) also deals with any context-free rule format in a seamless way, without requiring conversions to Chomsky Normal Form (CNF), as is often assumed. Another advantage is that our probabilistic Earley parser has been extended to take advantage of partially bracketed input, and to return partial parses on ungrammatical input. The latter extension removes one of the common objections against top-down, predictive (as opposed to bottom-up) parsing approaches (Magerman and Weir 1992). 2. Overview The remainder of the article proceeds as follows. Section 3 briefly reviews the workings of an Earley parser without regard to probabilities. Section 4 describes how the parser needs to be extended to compute sentence and prefix probabilities. Section 5 deals with further modifications for solving the Viterbi and training tasks, for processing partially bracketed inputs, and for finding partial parses. Section 6 discusses miscellaneous issues and relates our work to the literature on the subject. In Section 7 we summarize and draw some conclusions. To get an overall idea of proba</context>
<context position="65336" citStr="Magerman and Weir 1992" startWordPosition="11180" endWordPosition="11183">d easily this way. Parsing bracketed inputs is described in more detail in Stolcke (1993), where it is also shown that bracketing gives the expected improved efficiency. For example, the modified Earley parser processes fully bracketed inputs in linear time. 5.4 Robust Parsing In many applications ungrammatical input has to be dealt with in some way. Traditionally it has been seen as a drawback of top-down parsing algorithms such as Earley&apos;s that they sacrifice &amp;quot;robustness,&amp;quot; i.e., the ability to find partial parses in an ungrammatical input, for the efficiency gained from top-down prediction (Magerman and Weir 1992). One approach to the problem is to build robustness into the grammar itself. In the simplest case one could add top-level productions where X can expand to any nonterminal, including an &amp;quot;unknown word&amp;quot; category. This grammar will cause the Earley parser to find all partial parses of substrings, effectively behaving like a bottom-up parser constructing the chart in left-to-right fashion. More refined variations are possible: the top-level productions could be used to model which phrasal categories (sentence fragments) can likely follow each other. This probabilistic information can then be used</context>
</contexts>
<marker>Magerman, Weir, 1992</marker>
<rawString>Magerman, David M., and Weir, Carl (1992). &amp;quot;Efficiency, robustness and accuracy in Picky chart parsing.&amp;quot; In Proceedings, 30th Annual Meeting of the Association for Computational Linguistics. Newark, Delaware, 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sei-ichi Nakagawa</author>
</authors>
<title>Spoken sentence recognition by time-synchronous parsing algorithm of context-free grammar.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, IEEE Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>2</volume>
<pages>829--832</pages>
<location>Dallas, Texas.</location>
<contexts>
<context position="2187" citStr="Nakagawa 1987" startWordPosition="309" endWordPosition="310">pplications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non—finite-state acoustic and phonotactic modeling (Lani and Young 1991). In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Magerman and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993). Although clearly not a perfect model of natural language, stochastic context-free grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory providing a sound theoretical basis for ranking and pruning of parses, as well as for integration with models for nonsyntactic aspects of language. All of the applications listed above involve (or could potentially make use of) one or more of the following * Speech</context>
<context position="75337" citStr="Nakagawa (1987)" startWordPosition="12757" endWordPosition="12758">ime on arbitrary inputs may also grow exponentially. The bottom line is that each application&apos;s needs have to be evaluated against the pros and cons of both approaches to find the best solution. From a theoretical point of view, the Earley approach has the inherent appeal of being the more general (and exact) solution to the computation of the various SCFG probabilities. 6.3 Other Related Work The literature on Earley-based probabilistic parsers is sparse, presumably because of the precedent set by the Inside/Outside algorithm, which is more naturally formulated as a bottom-up algorithm. Both Nakagawa (1987) and Easeler (1988) use a nonprobabilistic Earley parser augmented with &amp;quot;word match&amp;quot; scoring. Though not truly probabilistic, these algorithms are similar to the Viterbi version described here, in that they find a parse that optimizes the accumulated matching scores (without regard to rule probabilities). Prediction and completion loops do not come into play since no precise inner or forward probabilities are computed. Magerman and Marcus (1991) are interested primarily in scoring functions to guide a parser efficiently to the most promising parses. Earley-style top-down prediction is used onl</context>
</contexts>
<marker>Nakagawa, 1987</marker>
<rawString>Nakagawa, Sei-ichi (1987). &amp;quot;Spoken sentence recognition by time-synchronous parsing algorithm of context-free grammar.&amp;quot; In Proceedings, IEEE Conference on Acoustics, Speech and Signal Processing, 2, 829-832. Dallas, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
</authors>
<title>Stochastic grammars and pattern recognition.&amp;quot; In Speech Recognition and Understanding. Recent Advances, Trends, and Applications,</title>
<date>1992</date>
<booktitle>F75 of NATO ASI Series, edited by Pietro Laface and Renato De Mori,</booktitle>
<volume>volume</volume>
<pages>319--344</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="1974" citStr="Ney 1992" startWordPosition="279" endWordPosition="280">t-free grammars are widely used as models of natural language syntax. In their probabilistic version, which defines a language as a probability distribution over strings, they have been used in a variety of applications: for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones and Eisner 1992); to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non—finite-state acoustic and phonotactic modeling (Lani and Young 1991). In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic (Nakagawa 1987), or they are used with context-sensitive and/or semantic probabilities (Magerman and Marcus 1991; Magerman and Weir 1992; Jones and Eisner 1992; Briscoe and Carroll 1993). Although clearly not a perfect model of natural language, stochastic context-free grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory providing a sound theoretical basis for ranking and </context>
</contexts>
<marker>Ney, 1992</marker>
<rawString>Ney, Hermann (1992). &amp;quot;Stochastic grammars and pattern recognition.&amp;quot; In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, volume F75 of NATO ASI Series, edited by Pietro Laface and Renato De Mori, 319-344. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annedore Paseler</author>
</authors>
<title>Modification of Earley&apos;s algorithm for speech recognition.&amp;quot;</title>
<date>1988</date>
<booktitle>In Recent Advances in Speech Understanding and Dialog Systems,</booktitle>
<volume>46</volume>
<pages>466--472</pages>
<publisher>Springer Verlag.</publisher>
<note>ASI Series, edited by</note>
<marker>Paseler, 1988</marker>
<rawString>Paseler, Annedore (1988). &amp;quot;Modification of Earley&apos;s algorithm for speech recognition.&amp;quot; In Recent Advances in Speech Understanding and Dialog Systems, volume F46 of NATO ASI Series, edited by H. Niemann, M. Lang, and G. Sagerer, 466-472. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Inside-outside reestimation from partially bracketed corpora.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings, 30th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>128--135</pages>
<location>Newark, Delaware,</location>
<contexts>
<context position="63740" citStr="Pereira and Schabes (1992)" startWordPosition="10940" endWordPosition="10943"> surroundings in which Y can occur if it can be derived through cyclic productions. Note that the computation of 0&apos; is unchanged, since ry&amp;quot; already includes an infinity of cyclically generated subtrees for Y, where appropriate. 5.3 Parsing Bracketed Inputs The estimation procedure described above (and EM-based estimators in general) are only guaranteed to find locally optimal parameter estimates. Unfortunately, it seems that in the case of unconstrained SCFG estimation local maxima present a very real problem, and make success dependent on chance and initial conditions (Lan i and Young 1990). Pereira and Schabes (1992) showed that partially bracketed input samples can alleviate the problem in certain cases. The bracketing information constrains the parse of the inputs, and therefore the parameter estimates, steering it clear from some of the suboptimal solutions that could otherwise be found. An Earley parser can be minimally modified to take advantage of bracketed strings by invoking itself recursively when a left parenthesis is encountered. The recursive instance of the parser is passed any predicted states at that position, processes the input up to the matching right parenthesis, and hands complete stat</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Pereira, Fernando C. N., and Schabes, Yves (1992). &amp;quot;Inside-outside reestimation from partially bracketed corpora.&amp;quot; In Proceedings, 30th Annual Meeting of the Association for Computational Linguistics. Newark, Delaware, 128-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Stuart M Shieber</author>
</authors>
<title>Prolog and Natural-Language Analysis.</title>
<date>1987</date>
<booktitle>CLSI Lecture Notes Series, Number 10. Center for the Study of Language and Information,</booktitle>
<location>Stanford, California.</location>
<contexts>
<context position="93352" citStr="Pereira and Shieber (1987" startWordPosition="15674" endWordPosition="15677">RLPLT where Pu has a nonzero entry at (X, a) iff there is a production for nonterminal X that starts with terminal a. RL is the old left-corner relation. During the prediction step we can ignore incoming states whose RHS nonterminal following the dot cannot have the current input as a left-corner, and then eliminate from the remaining predictions all those whose LHS cannot produce the current input as a left-corner. These filtering steps are very fast, as they involve only table lookup. This technique for speeding up Earley prediction is the exact converse of the &amp;quot;linking&amp;quot; method described by Pereira and Shieber (1987, chapter 6) for improving the efficiency of bottom-up parsers. There, the extended left-corner relation is used for top-down filtering the bottom-up application of grammar rules. In our case, we use linking to provide bottom-up filtering for top-down application of productions. On a test corpus this technique cut the number of generated predictions to almost one-fourth and speeded up parsing by a factor of 3.3. The corpus consisted of 1,143 sentence with an average length of 4.65 words. The top-down prediction alone generated 991,781 states and parsed at a rate of 590 milliseconds per sentenc</context>
</contexts>
<marker>Pereira, Shieber, 1987</marker>
<rawString>Pereira, Fernando C. N., and Shieber, Stuart M. (1987). Prolog and Natural-Language Analysis. CLSI Lecture Notes Series, Number 10. Center for the Study of Language and Information, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>B H Juang</author>
</authors>
<title>An introduction to hidden Markov models.&amp;quot;</title>
<date>1986</date>
<journal>IEEE ASSP Magazine,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>4--16</pages>
<contexts>
<context position="24550" citStr="Rabiner and Juang 1986" startWordPosition="4030" endWordPosition="4033">probability one. Hence the sum over the constrained incomplete paths is the sought-after sum over all complete derivations generating the prefix. 4.3 Forward and Inner Probabilities Since string and prefix probabilities are the result of summing derivation probabilities, the goal is to compute these sums efficiently by taking advantage of the Earley control structure. This can be accomplished by attaching two probabilistic quantities to each Earley state, as follows. The terminology is derived from analogous or similar quantities commonly used in the literature on Hidden Markov Models (HMMs) (Rabiner and Juang 1986) and in Baker (1979). Definition 4 The following definitions are relative to an implied input string x. a) The forward probability a, (kX --&gt; A.A) is the sum of the probabilities of all constrained paths of length i that end in state kX 173 Computational Linguistics Volume 21, Number 2 b) The inner probability 7,(kX ).p) is the sum of the probabilities of all paths of length i - k that start in state k: kX .Au and end in kX A.p, and generate the input symbols xk • • • x1-1. It helps to interpret these quantities in terms of an unconstrained Earley parser that operates as a generator emitting—r</context>
</contexts>
<marker>Rabiner, Juang, 1986</marker>
<rawString>Rabiner, L. R., and Juang, B. H. (1986). &amp;quot;An introduction to hidden Markov models.&amp;quot; IEEE ASSP Magazine, 3(1), 4-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>An inside-outside algorithm for estimating the parameters of a hidden stochastic context-free grammar based on Earley&apos;s algorithm.&amp;quot;</title>
<date>1991</date>
<booktitle>Paper presented at the Second Workshop on Mathematics of Language,</booktitle>
<location>Tarrytown, New York.</location>
<contexts>
<context position="76306" citStr="Schabes (1991)" startWordPosition="12898" endWordPosition="12899"> play since no precise inner or forward probabilities are computed. Magerman and Marcus (1991) are interested primarily in scoring functions to guide a parser efficiently to the most promising parses. Earley-style top-down prediction is used only to suggest worthwhile parses, not to compute precise probabilities, which they argue would be an inappropriate metric for natural language parsing. Casacuberta and Vidal (1988) exhibit an Earley parser that processes weighted (not necessarily probabilistic) CFGs and performs a computation that is isomorphic to that of inside probabilities shown here. Schabes (1991) adds both inner and outer probabilities to Earley&apos;s algorithm, with the purpose of obtaining a generalized estimation algorithm for SCFGs. Both of these approaches are restricted to grammars without unbounded ambiguities, which can arise from unit or null productions. Dan Jurafsky (personal communication) wrote an Earley parser for the Berkeley Restaurant Project (BeRP) speech understanding system that originally computed forward probabilities for restricted grammars (without left-corner or unit production recursion). The parser now uses the method described here to provide exact SCFG prefix </context>
</contexts>
<marker>Schabes, 1991</marker>
<rawString>Schabes, Yves (1991). &amp;quot;An inside-outside algorithm for estimating the parameters of a hidden stochastic context-free grammar based on Earley&apos;s algorithm.&amp;quot; Paper presented at the Second Workshop on Mathematics of Language, Tarrytown, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jonathan Segal</author>
</authors>
<title>Precise n-gram probabilities from stochastic context-free grammars.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, 31th Annual Meeting of the Association for Computational Linguistics. Las</booktitle>
<pages>74--79</pages>
<location>Cruces, New</location>
<contexts>
<context position="5522" citStr="Stolcke and Segal 1994" startWordPosition="831" endWordPosition="834">many applications, such a speech decoding. As pointed out by Jelinek and Lafferty (1991), knowing probabilities P(x0 . xi) for arbitrary prefixes xo . xi enables probabilistic prediction of possible follow-words xi+i, as P(xi-Fi Ixo . • • xi) = P(xo )/P(xo • .. xi). These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder (Bahl, Jelinek, and Mercer 1983). Another application in which prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs (Stolcke and Segal 1994). Here, too, efficient incremental computation saves time, since the work for common prefix strings can be shared. The key to most of the features of our algorithm is that it is based on the topdown parsing method for nonprobabilistic CFGs developed by Earley (1970). Earley&apos;s algorithm is appealing because it runs with best-known complexity on a number of special classes of grammars. In particular, Earley parsing is more efficient than the bottom-up methods in cases where top-down prediction can rule out potential parses of substrings. The worst-case computational expense of the algorithm (eit</context>
</contexts>
<marker>Stolcke, Segal, 1994</marker>
<rawString>Stolcke, Andreas, and Segal, Jonathan (1994). &amp;quot;Precise n-gram probabilities from stochastic context-free grammars.&amp;quot; In Proceedings, 31th Annual Meeting of the Association for Computational Linguistics. Las Cruces, New Mexico, 74-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix Andreas Stolcke Efficient Probabilistic Context-Free Parsing probabilities.&amp;quot;</title>
<date>1993</date>
<tech>Technical Report TR-93-065,</tech>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA. Revised</location>
<contexts>
<context position="64802" citStr="Stolcke (1993)" startWordPosition="11099" endWordPosition="11100">nstance of the parser is passed any predicted states at that position, processes the input up to the matching right parenthesis, and hands complete states back to the invoking instance. This technique is efficient, as it never explicitly rejects parses not consistent with the bracketing. It is also convenient, as it leaves the basic parser operations, including the left-to-right processing and the probabilistic computations, unchanged. For example, prefix probabilities conditioned on partial bracketings could be computed easily this way. Parsing bracketed inputs is described in more detail in Stolcke (1993), where it is also shown that bracketing gives the expected improved efficiency. For example, the modified Earley parser processes fully bracketed inputs in linear time. 5.4 Robust Parsing In many applications ungrammatical input has to be dealt with in some way. Traditionally it has been seen as a drawback of top-down parsing algorithms such as Earley&apos;s that they sacrifice &amp;quot;robustness,&amp;quot; i.e., the ability to find partial parses in an ungrammatical input, for the efficiency gained from top-down prediction (Magerman and Weir 1992). One approach to the problem is to build robustness into the gram</context>
<context position="67005" citStr="Stolcke (1993)" startWordPosition="11456" endWordPosition="11457">st that, based on the notion of a wildcard state where the wildcard ? stands for an arbitrary continuation of the RHS. During prediction, a wildcard to the left of the dot causes the chart to be seeded with dummy states .X for each phrasal category X of interest. Conversely, a minimal modification to the standard completion step allows the wildcard states to collect all abutting substring parses: i: 11( —› it. 1 i : —&gt; AY.? j: k for all Y. This way each partial parse will be represented by exactly one wildcard state in the final chart position. A detailed account of this technique is given in Stolcke (1993). One advantage over the grammar-modifying approach is that it can be tailored to use various criteria at runtime to decide which partial parses to follow. 6. Discussion 6.1 Online Pruning In finite-state parsing (especially speech decoding) one often makes use of the forward probabilities for pruning partial parses before having seen the entire input. Pruning is formally straightforward in Earley parsers: in each state set, rank states according to their a values, then remove those states with small probabilities compared to the current best candidate, or simply those whose rank exceeds a giv</context>
<context position="73104" citStr="Stolcke (1993)" startWordPosition="12405" endWordPosition="12406">ence between LR(k) parsers and k-lookahead Earley parsers is discussed in the literature (Earley 1970; Aho and Ullman 1972). 14 Once again, it is helpful to compare this to a closely related finite-state concept: the states of the LR parser correspond to sets of Earley states, similar to the way the states of a deterministic FSA correspond to sets of states of an equivalent nondeterministic FSA under the standard subset construction. 15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p&apos;s, as shown in Stolcke (1993). 16 It is not clear what the numerical properties of this approximation are, e.g., how the errors will accumulate over longer parses. 192 Andreas Stolcke Efficient Probabilistic Context-Free Parsing on a training corpus. Because of the imprecise relationship between LR probabilities and SCFG probabilities, it is not clear if the model thus estimated corresponds to any particular SCFG in the usual sense. Briscoe and Carroll (1993) turn this incongruity into an advantage by using the LR parser as a probabilistic model in its own right, and show how LR probabilities can be extended to capture no</context>
</contexts>
<marker>Stolcke, 1993</marker>
<rawString>Stolcke, Andreas (1993). &amp;quot;An efficient probabilistic context-free parsing algorithm that computes prefix Andreas Stolcke Efficient Probabilistic Context-Free Parsing probabilities.&amp;quot; Technical Report TR-93-065, International Computer Science Institute, Berkeley, CA. Revised 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="70466" citStr="Tomita 1986" startWordPosition="11981" endWordPosition="11982">by following all possible partial derivations. An LR parser, on the other hand, has access to a complete list of sets of possible items computed beforehand, and at runtime simply follows transitions between these sets. The item sets are known as the &amp;quot;states&amp;quot; of the LR parser.&apos; A grammar is suitable for LR parsing if these transitions can be performed deterministically by considering only the next input and the contents of a shift-reduce stack. Generalized LR parsing is an extension that allows parallel tracking of multiple state transitions and stack actions by using a graph-structured stack (Tomita 1986). Probabilistic LR parsing (Wright 1990) is based on LR items augmented with certain conditional probabilities. Specifically, the probability p associated with an LR item X -4 A.,tt is, in our terminology, a normalized forward probability: a,(X A.p) P= P(S xo where the denominator is the probability of the current prefix.&apos; LR item probabilities, are thus conditioned forward probabilities, and can be used to compute conditional probabilities of next words: P(x, xo...1_1) is the sum of the p&apos;s of all items having x, to the right of the dot (extra work is required if the item corresponds to a &amp;quot;re</context>
<context position="74713" citStr="Tomita 1986" startWordPosition="12660" endWordPosition="12661">ice between LR methods and Earley parsing is a typical space-time tradeoff. Even though an Earley parser runs with the same linear time and space complexity as an LR parser on grammars of the appropriate LR class, the constant factors involved will be much in favor of the LR parser, as almost all the work has already been compiled into its transition and action table. However, the size of LR parser tables can be exponential in the size of the grammar (because of the number of potential item subsets). Furthermore, if the generalized LR method is used for dealing with nondeterministic grammars (Tomita 1986) the runtime on arbitrary inputs may also grow exponentially. The bottom line is that each application&apos;s needs have to be evaluated against the pros and cons of both approaches to find the best solution. From a theoretical point of view, the Earley approach has the inherent appeal of being the more general (and exact) solution to the computation of the various SCFG probabilities. 6.3 Other Related Work The literature on Earley-based probabilistic parsers is sparse, presumably because of the precedent set by the Inside/Outside algorithm, which is more naturally formulated as a bottom-up algorit</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>Tomita, Masaru (1986). Efficient Parsing for Natural Language. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
</authors>
<title>LR parsing of probabilistic grammars with input uncertainty for speech recognition.&amp;quot;</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<volume>4</volume>
<pages>297--323</pages>
<contexts>
<context position="27562" citStr="Wright (1990)" startWordPosition="4548" endWordPosition="4549">d string probabilities we are interested in, provided that forward and inner probabilities can be computed effectively. Lemma 3 The following assumes an Earley chart constructed by the parser on an input string x with Ix I = 1. a) Provided that S L x0...k_iXv is a possible left-most derivation of the grammar (for some v), the probability that a nonterminal X generates the substring xk . . . x,_1 can be computed as the sum P(X xk...k_i) = E -yickx —+ A.) i:kx- A. (sum of inner probabilities over all complete states with LHS X and start index k). 6 The same technical complication was noticed by Wright (1990) in the computation of probabilistic LR parser tables. The relation to LR parsing will be discussed in Section 6.2. Incidentally, a similar interpretation of forward &amp;quot;probabilities&amp;quot; is required for HMMs with non-emitting states. 174 Andreas Stolcke Efficient Probabilistic Context-Free Parsing b) In particular, the string probability P(S x) can be computed as7 P(S x) = -y1(0 —S.) al(0 S.) c) The prefix probability P(S L x), with Ix = I, can be computed as P(S x) = (sum of forward probabilities over all scanned states). The restriction in (a) that X be preceded by a possible prefix is necessary,</context>
<context position="70506" citStr="Wright 1990" startWordPosition="11986" endWordPosition="11987">tions. An LR parser, on the other hand, has access to a complete list of sets of possible items computed beforehand, and at runtime simply follows transitions between these sets. The item sets are known as the &amp;quot;states&amp;quot; of the LR parser.&apos; A grammar is suitable for LR parsing if these transitions can be performed deterministically by considering only the next input and the contents of a shift-reduce stack. Generalized LR parsing is an extension that allows parallel tracking of multiple state transitions and stack actions by using a graph-structured stack (Tomita 1986). Probabilistic LR parsing (Wright 1990) is based on LR items augmented with certain conditional probabilities. Specifically, the probability p associated with an LR item X -4 A.,tt is, in our terminology, a normalized forward probability: a,(X A.p) P= P(S xo where the denominator is the probability of the current prefix.&apos; LR item probabilities, are thus conditioned forward probabilities, and can be used to compute conditional probabilities of next words: P(x, xo...1_1) is the sum of the p&apos;s of all items having x, to the right of the dot (extra work is required if the item corresponds to a &amp;quot;reduce&amp;quot; state, i.e., if the dot is in fina</context>
<context position="73007" citStr="Wright (1990)" startWordPosition="12387" endWordPosition="12388">hence more efficient. Only the case of zero-lookahead, LR(0), is considered here; the correspondence between LR(k) parsers and k-lookahead Earley parsers is discussed in the literature (Earley 1970; Aho and Ullman 1972). 14 Once again, it is helpful to compare this to a closely related finite-state concept: the states of the LR parser correspond to sets of Earley states, similar to the way the states of a deterministic FSA correspond to sets of states of an equivalent nondeterministic FSA under the standard subset construction. 15 The identity of this expression with the item probabilities of Wright (1990) can be proved by induction on the steps performed to compute the p&apos;s, as shown in Stolcke (1993). 16 It is not clear what the numerical properties of this approximation are, e.g., how the errors will accumulate over longer parses. 192 Andreas Stolcke Efficient Probabilistic Context-Free Parsing on a training corpus. Because of the imprecise relationship between LR probabilities and SCFG probabilities, it is not clear if the model thus estimated corresponds to any particular SCFG in the usual sense. Briscoe and Carroll (1993) turn this incongruity into an advantage by using the LR parser as a </context>
</contexts>
<marker>Wright, 1990</marker>
<rawString>Wright, J. H. (1990). &amp;quot;LR parsing of probabilistic grammars with input uncertainty for speech recognition.&amp;quot; Computer Speech and Language, 4, 297-323.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>