<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008035">
<title confidence="0.96968">
Analysing Wikipedia and Gold-Standard Corpora for NER Training
</title>
<author confidence="0.995426">
Joel Nothman and Tara Murphy and James R. Curran
</author>
<affiliation confidence="0.9954105">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.43686">
NSW 2006, Australia
</address>
<email confidence="0.996797">
{jnot4610,tm,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.997358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999812533333334">
Named entity recognition (NER) for En-
glish typically involves one of three gold
standards: MUC, CoNLL, or BBN, all created
by costly manual annotation. Recent work
has used Wikipedia to automatically cre-
ate a massive corpus of named entity an-
notated text.
We present the first comprehensive cross-
corpus evaluation of NER. We identify
the causes of poor cross-corpus perfor-
mance and demonstrate ways of making
them more compatible. Using our process,
we develop a Wikipedia corpus which out-
performs gold standard corpora on cross-
corpus evaluation by up to 11%.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999656622222223">
Named Entity Recognition (NER), the task of iden-
tifying and classifying the names of people, organ-
isations and other entities within text, is central
to many NLP systems. NER developed from in-
formation extraction in the Message Understand-
ing Conferences (MUC) of the 1990s. By MUC 6
and 7, NER had become a distinct task: tagging
proper names, and temporal and numerical expres-
sions (Chinchor, 1998).
Statistical machine learning systems have
proven successful for NER. These learn patterns
associated with individual entity classes, mak-
ing use of many contextual, orthographic, linguis-
tic and external knowledge features. However,
they rely heavily on large annotated training cor-
pora. This need for costly expert annotation hin-
ders the creation of more task-adaptable, high-
performance named entity recognisers.
In acquiring new sources for annotated corpora,
we require an analysis of training data as a variable
in NER. This paper compares the three main gold-
standard corpora. We found that tagging mod-
els built on each corpus perform relatively poorly
when tested on the others. We therefore present
three methods for analysing internal and inter-
corpus inconsistencies. Our analysis demonstrates
that seemingly minor variations in the text itself,
starting right from tokenisation can have a huge
impact on practical NER performance.
We take this experience and apply it to a corpus
created automatically using Wikipedia. This cor-
pus was created following the method of Nothman
et al. (2008). By training the C&amp;C tagger (Curran
and Clark, 2003) on the gold-standard corpora and
our new Wikipedia-derived training data, we eval-
uate the usefulness of the latter and explore the
nature of the training corpus as a variable in NER.
Our Wikipedia-derived corpora exceed the per-
formance of non-corresponding training and test
sets by up to 11% F-score, and can be engineered
to automatically produce models consistent with
various NE-annotation schema. We show that it is
possible to automatically create large, free, named
entity-annotated corpora for general or domain
specific tasks.
</bodyText>
<sectionHeader confidence="0.946606" genericHeader="introduction">
2 NER and annotated corpora
</sectionHeader>
<bodyText confidence="0.999919375">
Research into NER has rarely considered the im-
pact of training corpora. The CoNLL evalua-
tions focused on machine learning methods (Tjong
Kim Sang, 2002; Tjong Kim Sang and De Meul-
der, 2003) while more recent work has often in-
volved the use of external knowledge. Since many
tagging systems utilise gazetteers of known enti-
ties, some research has focused on their automatic
extraction from the web (Etzioni et al., 2005) or
Wikipedia (Toral et al., 2008), although Mikheev
et al. (1999) and others have shown that larger
NE lists do not necessarily correspond to increased
NER performance. Nadeau et al. (2006) use such
lists in an unsupervised NE recogniser, outper-
forming some entrants of the MUC Named Entity
Task. Unlike statistical approaches which learn
</bodyText>
<note confidence="0.9230485">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612–620,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.995488">
612
</page>
<bodyText confidence="0.999972538461539">
patterns associated with a particular type of entity,
these unsupervised approaches are limited to iden-
tifying common entities present in lists or those
caught by hand-built rules.
External knowledge has also been used to aug-
ment supervised NER approaches. Kazama and
Torisawa (2007) improve their F-score by 3% by
including a Wikipedia-based feature in their ma-
chine learner. Such approaches are limited by the
gold-standard data already available.
Less common is the automatic creation of train-
ing data. An et al. (2003) extracted sentences con-
taining listed entities from the web, and produced
a 1.8 million word Korean corpus that gave sim-
ilar results to manually-annotated training data.
Richman and Schone (2008) used a method sim-
ilar to Nothman et al. (2008) in order to derive
NE-annotated corpora in languages other than En-
glish. They classify Wikipedia articles in foreign
languages by transferring knowledge from English
Wikipedia via inter-language links. With these
classifications they automatically annotate entire
articles for NER training, and suggest that their re-
sults with a 340k-word Spanish corpus are compa-
rable to 20k-40k words of gold-standard training
data when using MUC-style evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.988413">
2.1 Gold-standard corpora
</subsectionHeader>
<bodyText confidence="0.9973725">
We evaluate our Wikipedia-derived corpora
against three sets of manually-annotated data from
</bodyText>
<listItem confidence="0.892409555555556">
(a) the MUC-7 Named Entity Task (MUC, 2001);
(b) the English CoNLL-03 Shared Task (Tjong
Kim Sang and De Meulder, 2003); (c) the
BBN Pronoun Coreference and Entity Type
Corpus (Weischedel and Brunstein, 2005). We
consider only the generic newswire NER task,
although domain-specific annotated corpora have
been developed for applications such as bio-text
mining (Kim et al., 2003).
</listItem>
<bodyText confidence="0.999606">
Stylistic and genre differences between the
source texts affect compatibility for NER evalua-
tion e.g., the CoNLL corpus formats headlines in
all-caps, and includes non-sentential data such as
tables of sports scores.
Each corpus uses a different set of entity labels.
MUC marks locations (LOC), organisations (ORG)
and personal names (PER), in addition to numeri-
cal and time information. The CoNLL NER shared
tasks (Tjong Kim Sang, 2002; Tjong Kim Sang
and De Meulder, 2003) mark PER, ORG and LOC
entities, as well as a broad miscellaneous class
</bodyText>
<table confidence="0.4960108">
Number of tokens
TRAIN DEV TEST
83601 18655 60436
203621 51362 46435
901894 142218 129654
</table>
<tableCaption confidence="0.996436">
Table 1: Gold-standard NE-annotated corpora
</tableCaption>
<bodyText confidence="0.999928571428571">
(MISC; e.g. events, artworks and nationalities).
BBN annotates the entire Penn Treebank corpus
with 105 fine-grained tags (Brunstein, 2002): 54
corresponding to CoNLL entities; 21 for numeri-
cal and time data; and 30 for other classes. For
our evaluation, BBN’s tags were reduced to the
equivalent CoNLL tags, with extra tags in the BBN
and MUC data removed. Since no MISC tags are
marked in MUC, they need to be removed from
CoNLL, BBN and Wikipedia data for comparison.
We transformed all three corpora into a com-
mon format and annotated them with part-of-
speech tags using the Penn Treebank-trained
C&amp;C POS tagger. We altered the default MUC
tokenisation to attach periods to abbreviations
when sentence-internal. While standard training
(TRAIN), development (DEV) and final test (TEST)
set divisions were available for CoNLL and MUC,
the BBN corpus was split at our discretion: sec-
tions 03–21 for TRAIN, 00–02 for DEV and 22-24
for TEST. Corpus sizes are compared in Table 1.
</bodyText>
<subsectionHeader confidence="0.999283">
2.2 Evaluating NER performance
</subsectionHeader>
<bodyText confidence="0.999938045454545">
One challenge for NER research is establishing an
appropriate evaluation metric (Nadeau and Sekine,
2007). In particular, entities may be correctly
delimited but mis-classified, or entity boundaries
may be mismatched.
MUC (Chinchor, 1998) awarded equal score for
matching type, where an entity’s class is identi-
fied with at least one boundary matching, and text,
where an entity’s boundaries are precisely delim-
ited, irrespective of the classification. This equal
weighting is unrealistic, as some boundary errors
are highly significant, while others are arbitrary.
CoNLL awarded exact (type and text) phrasal
matches, ignoring boundary issues entirely and
providing a lower-bound measure of NER per-
formance. Manning (2006) argues that CoNLL-
style evaluation is biased towards systems which
leave entities with ambiguous boundaries un-
tagged, since boundary errors amount simultane-
ously to false positives and false negatives. In both
MUC and CoNLL, micro-averaged precision, recall
and F1 score summarise the results.
</bodyText>
<figure confidence="0.9772975">
Corpus # tags
MUC-7 3
CoNLL-03 4
BBN 54
</figure>
<page confidence="0.998483">
613
</page>
<bodyText confidence="0.999491285714286">
Tsai et al. (2006) compares a number of meth-
ods for relaxing boundary requirements: matching
only the left or right boundary, any tag overlap,
per-token measures, or more semantically-driven
matching. ACE evaluations instead use a customiz-
able evaluation metric with weights specified for
different types of error (NIST-ACE, 2008).
</bodyText>
<sectionHeader confidence="0.942571" genericHeader="method">
3 Corpus and error analysis approaches
</sectionHeader>
<bodyText confidence="0.994154142857143">
To evaluate the performance impact of a corpus
we may analyse (a) the annotations themselves; or
(b) the model built on those annotations and its
performance. A corpus can be considered in isola-
tion or by comparison with other corpora. We use
three methods to explore intra- and inter-corpus
consistency in MUC, CoNLL, and BBN in Section 4.
</bodyText>
<subsectionHeader confidence="0.998871">
3.1 N-gram tag variation
</subsectionHeader>
<bodyText confidence="0.999415133333333">
Dickinson and Meurers (2003) present a clever
method for finding inconsistencies within POS an-
notated corpora, which we apply to NER corpora.
Their approach finds all n-grams in a corpus which
appear multiple times, albeit with variant tags for
some sub-sequence, the nucleus (see e.g. Table
3). To remove valid ambiguity, they suggest us-
ing (a) a minimum n-gram length; (b) a minimum
margin of invariant terms around the nucleus.
For example, the BBN TRAIN corpus includes
eight occurrences of the 6-gram the San Francisco
Bay area,. Six instances of area are tagged as non-
entities, but two instances are tagged as part of the
LOC that precedes it. The other five tokens in this
n-gram are consistently labelled.
</bodyText>
<subsectionHeader confidence="0.998682">
3.2 Entity type frequency
</subsectionHeader>
<bodyText confidence="0.999520735294118">
An intuitive approach to finding discrepancies be-
tween corpora is to compare the distribution of en-
tities within each corpus. To make this manage-
able, instances need to be grouped by more than
their class labels. We used the following groups:
POS sequences: Types of candidate entities may
often be distinguished by their POS tags, e.g.
nationalities are often JJ or NNPS.
Wordtypes: Collins (2002) proposed wordtypes
where all uppercase characters map to A, low-
ercase to a, and digits to 0. Adjacent charac-
ters in the same orthographic class were col-
lapsed. However, we distinguish single from
multiple characters by duplication. e.g. USS
Nimitz (CVN-68) has wordtype AA Aaa (AA-00).
Wordtype with functions: We also map content
words to wordtypes only—function words
are retained, e.g. Bank of New England Corp.
maps to Aaa of Aaa Aaa Aaa..
No approach provides sufficient discrimination
alone: wordtype patterns are able to distinguish
within common POS tags and vice versa. Each
method can be further simplified by merging re-
peated tokens, NNP NNP becoming NNP.
By calculating the distribution of entities over
these groupings, we can find anomalies between
corpora. For instance, 4% of MUC’s and 5.9%
of BBN’s PER entities have wordtype Aaa A. Aaa,
e.g. David S. Black, while CoNLL has only 0.05% of
PERs like this. Instead, CoNLL has many names of
form A. Aaa, e.g. S. Waugh, while BBN and MUC
have none. We can therefore predict incompatibil-
ities between systems trained on BBN and evalu-
ated on CoNLL or vice-versa.
</bodyText>
<subsectionHeader confidence="0.999277">
3.3 Tag sequence confusion
</subsectionHeader>
<bodyText confidence="0.99999592">
A confusion matrix between predicted and correct
classes is an effective method of error analysis.
For phrasal sequence tagging, this can be applied
to either exact boundary matches or on a per-token
basis, ignoring entity bounds. We instead compile
two matrices: C/P comparing correct entity classes
against predicted tag sequences; and P/C compar-
ing predicted classes to correct tag sequences.
C/P equates oversized boundaries to correct
matches, and tabulates cases of undersized bound-
aries. For example, if [ORG Johnson and Johnson] is
tagged [PER Johnson] and [PER Johnson], it is marked
in matrix coordinates (ORG, PER O PER). P/C em-
phasises oversized boundaries: if gold-standard
Mr. [PER Ross] is tagged PER, it is counted as con-
fusion between PER and O PER. To further dis-
tinguish classes of error, the entity type groupings
from Section 3.2 are also used.
This analysis is useful for both tagger evalua-
tion and cross-corpus evaluation, e.g. BBN versus
CoNLL on a BBN test set. This involves finding
confusion matrix entries where BBN and CoNLL’s
performance differs significantly, identifying com-
mon errors related to difficult instances in the test
corpus as well as errors in the NER model.
</bodyText>
<sectionHeader confidence="0.793587" genericHeader="method">
4 Comparing gold-standard corpora
</sectionHeader>
<bodyText confidence="0.998094666666667">
We trained the C&amp;C NER tagger (Curran and Clark,
2003) to build separate models for each gold-
standard corpus. The C&amp;C tagger utilises a number
</bodyText>
<page confidence="0.996115">
614
</page>
<table confidence="0.999523333333333">
MUC — — 73.5 55.5 67.5
CoNLL 81.2 62.3 65.9 82.1 62.4
BBN 54.7 86.7 77.9 53.9 88.4
</table>
<tableCaption confidence="0.999611">
Table 2: Gold standard F-scores (exact-match)
</tableCaption>
<bodyText confidence="0.99948824137931">
of orthographic, contextual and in-document fea-
tures, as well as gazetteers for personal names. Ta-
ble 2 shows that each training set performs much
better on corresponding (same corpus) test sets
(italics) than on test sets from other sources, also
identified by (Ciaramita and Altun, 2005). NER
research typically deals with small improvements
(-1% F-score). The 12-32% mismatch between
training and test corpora suggests that an appropri-
ate training corpus is a much greater concern. The
exception is BBN on MUC, due to differing TEST
and DEV subject matter. Here we analyse the vari-
ation within and between the gold standards.
Table 3 lists some n-gram tag variations for BBN
and CoNLL (TRAIN + DEV). These include cases of
schematic variations (e.g. the period in Co .) and
tagging errors. Some n-grams have three variants,
e.g. the Standard &amp; Poor ’s 500 which appears un-
tagged, as the [ORG Standard &amp; Poor] ’s 500, or the
[ORG Standard &amp; Poor ’s] 500. MUC is too small for
this method. CoNLL only provides only a few ex-
amples, echoing BBN in the ambiguities of trailing
periods and leading determiners or modifiers.
Wordtype distributions were also used to com-
pare the three gold standards. We investigated all
wordtypes which occur with at least twice the fre-
quency in one corpus as in another, if that word-
type was sufficiently frequent. Among the differ-
ences recovered from this analysis are:
</bodyText>
<listItem confidence="0.999885285714286">
• CoNLL has an over-representation of uppercase words
due to all-caps headlines.
• Since BBN also annotates common nouns, some have
been mistakenly labelled as proper-noun entities.
• BBN tags text like Munich-based as LOC; CoNLL
tags it as MISC; MUC separates the hyphen as a token.
• CoNLL is biased to sports and has many event names
in the form of 1990 World Cup.
• BBN separates organisation names from their products
as in [ORG Commodore] [MISC 64].
• CoNLL has few references to abbreviated US states.
• CoNLL marks conjunctions of people (e.g. Ruth and
Edwin Brooks) as a single PER entity.
• CoNLL text has Co Ltd instead of Co. Ltd.
</listItem>
<bodyText confidence="0.999923">
We analysed the tag sequence confusion when
training with each corpus and testing on BBN DEV.
While full confusion matrices are too large for this
paper, Table 4 shows some examples where the
</bodyText>
<figureCaption confidence="0.998963">
Figure 1: Deriving training data from Wikipedia
</figureCaption>
<bodyText confidence="0.999933833333333">
NER models disagree. MUC fails to correctly tag
U.K. and U.S.. U.K. only appears once in MUC, and
U.S. appears 22 times as ORG and 77 times as LOC.
CoNLL has only three instances of Mr., so it often
mis-labels Mr. as part of a PER entity. The MUC
model also has trouble recognising ORG names
ending with corporate abbreviations, and may fail
to identify abbreviated US state names.
Our analysis demonstrates that seemingly mi-
nor orthographic variations in the text, tokenisa-
tion and annotation schemes can have a huge im-
pact on practical NER performance.
</bodyText>
<sectionHeader confidence="0.988835" genericHeader="method">
5 From Wikipedia to NE-annotated text
</sectionHeader>
<bodyText confidence="0.9979967">
Wikipedia is a collaborative, multilingual, online
encyclopedia which includes over 2.3 million arti-
cles in English alone. Our baseline approach de-
tailed in Nothman et al. (2008) exploits the hyper-
linking between articles to derive a NE corpus.
Since —74% of Wikipedia articles describe top-
ics covering entity classes, many of Wikipedia’s
links correspond to entity annotations in gold-
standard NE corpora. We derive a NE-annotated
corpus by the following steps:
</bodyText>
<listItem confidence="0.972376">
1. Classify all articles into entity classes
2. Split Wikipedia articles into sentences
3. Label NEs according to link targets
4. Select sentences for inclusion in a corpus
</listItem>
<figure confidence="0.7435286">
With MISC
CoNLL BBN
TRAIN
Without MISC
MUC CoNLL BBN
</figure>
<page confidence="0.797963">
615
</page>
<table confidence="0.999906583333333">
N-gram Tag # Tag #
Co . - 52 ORG 111
Smith Barney, Harris Upham &amp; Co. - 1 ORG 9
the Contra rebels MISC 1 ORG 2
in the West is - 1 LOC 1
that the Constitution MISC 2 - 1
Chancellor of the Exchequer Nigel Lawson - 11 ORG 2
the world ’s - 80 LOC 1
1993 BellSouth Classic - 1 MISC 1
Atlanta Games LOC 1 MISC 1
Justice Minister - 1 ORG 1
GOLF - GERMAN OPEN - 2 LOC 1
</table>
<tableCaption confidence="0.992105">
Table 3: Examples of n-gram tag variations in BBN (top) and CoNLL (bottom). Nucleus is in bold.
</tableCaption>
<table confidence="0.999403571428571">
Tag sequence Grouping # if trained on BBN Example
Correct Pred. MUC CoNLL
LOC LOC A.A. 101 349 343 U.K.
-PER PER Aa. Aaa 9 242 0 Mr. Watson
- LOC Aa. 16 109 0 Mr.
ORG ORG Aaa Aaa. 118 214 218 Campeau Corp.
LOC - Aaa. 20 0 3 Calif.
</table>
<tableCaption confidence="0.999887">
Table 4: Tag sequence confusion on BBN DEV when training on gold-standard corpora (no MISC)
</tableCaption>
<bodyText confidence="0.99943934375">
In Figure 1, a sentence introducing Holden as an
Australian car maker based in Port Melbourne has
links to separate articles about each entity. Cues
in the linked article about Holden indicate that it is
an organisation, and the article on Port Melbourne
is likewise classified as a location. The original
sentence can then be automatically annotated with
these facts. We thus extract millions of sentences
from Wikipedia to form a new NER corpus.
We classify each article in a bootstrapping pro-
cess using its category head nouns, definitional
nouns from opening sentences, and title capital-
isation. Each article is classified as one of: un-
known; a member of a NE category (LOC, ORG,
PER, MISC, as per CoNLL); a disambiguation page
(these list possible referent articles for a given ti-
tle); or a non-entity (NON). This classifier classi-
fier achieves 89% F-score.
A sentence is selected for our corpus when all
of its capitalised words are linked to articles with a
known class. Exceptions are made for common ti-
tlecase words, e.g. I, Mr., June, and sentence-initial
words. We also infer additional links — variant ti-
tles are collected for each Wikipedia topic and are
marked up in articles which link to them — which
Nothman et al. (2008) found increases coverage.
Transforming links into annotations that con-
form to a gold standard is far from trivial. Link
boundaries need to be adjusted, e.g. to remove ex-
cess punctuation. Adjectival forms of entities (e.g.
American, Islamic) generally link to nominal arti-
cles. However, they are treated by CoNLL and our
</bodyText>
<table confidence="0.9841638">
Tag # Tag #
MISC 2 PER 5
- 58 LOC 4
- 8 LOC 3
LOC 1 MISC 2
</table>
<tableCaption confidence="0.999349">
Table 5: N-gram variations in the Wiki baseline
</tableCaption>
<bodyText confidence="0.999643571428571">
BBN mapping as MISC. POS tagging the corpus and
relabelling entities ending with JJ as MISC solves
this heuristically. Although they are capitalised in
English, personal titles (e.g. Prime Minister) are not
typically considered entities. Initially we assume
that all links immediately preceding PER entities
are titles and delete their entity classification.
</bodyText>
<sectionHeader confidence="0.979083" genericHeader="method">
6 Improving Wikipedia performance
</sectionHeader>
<bodyText confidence="0.988379952380952">
The baseline system described above achieves
only 58.9% and 62.3% on the CoNLL and
BBN TEST sets (exact-match scoring) with 3.5-
million training tokens. We apply methods pro-
posed in Section 3 to to identify and minimise
Wikipedia errors on the BBN DEV corpus.
We begin by considering Wikipedia’s internal
consistency using n-gram tag variation (Table 5).
The breadth of Wikipedia leads to greater genuine
ambiguity, e.g. Batman (a character or a comic
strip). It also shares gold-standard inconsistencies
like leading modifiers. Variations in American and
Chicago, Illinois indicate errors in adjectival entity
labels and in correcting link boundaries.
Some errors identified with tag sequence confu-
sion are listed in Table 6. These correspond to re-
N-gram
of Batman ’s
in the Netherlands
Chicago, Illinois
the American and
</bodyText>
<page confidence="0.996154">
616
</page>
<table confidence="0.9994985">
Tag sequence Grouping # if trained on Example
Correct Pred. BBN Wiki
LOC LOC Aaa. 103 14 Calif.
LOC - LOC ORG Aaa , Aaa. 0 15 Norwalk, Conn.
LOC LOC Aaa-aa 23 0 Texas-based
- PER PER Aa. Aaa 4 208 Mr. Yamamoto
-PER PER Aaa Aaa 1 49 Judge Keenan
- PER Aaa 7 58 President
MISC MISC A. 25 1 R.
MISC LOC NNPS 0 39 Soviets
</table>
<tableCaption confidence="0.999688">
Table 6: Tag sequence confusion on BBN DEV with training on BBN and the Wikipedia baseline
</tableCaption>
<bodyText confidence="0.998888875">
sults of an entity type frequency analysis and mo-
tivate many of our Wikipedia extensions presented
below. In particular, personal titles are tagged as
PER rather than unlabelled; plural nationalities are
tagged LOC, not MISC; LOCs hyphenated to fol-
lowing words are not identified; nor are abbrevi-
ated US state names. Using R. to abbreviate Re-
publican in BBN is also a high-frequency error.
</bodyText>
<subsectionHeader confidence="0.96429">
6.1 Inference from disambiguation pages
</subsectionHeader>
<bodyText confidence="0.999977117647059">
Our baseline system infers extra links using a set
of alternative titles identified for each article. We
extract the alternatives from the article and redirect
titles, the text of all links to the article, and the first
and last word of the article title if it is labelled PER.
Our extension is to extract additional inferred ti-
tles from Wikipedia’s disambiguation pages. Most
disambiguation pages are structured as lists of ar-
ticles that are often referred to by the title D being
disambiguated. For each link with target A that
appears at the start of a list item on D’s page, D
and its redirect aliases are added to the list of al-
ternative titles for A.
Our new source of alternative titles includes
acronyms and abbreviations (AMP links to AMP
Limited and Ampere), and given or family names
(Howard links to Howard Dean and John Howard).
</bodyText>
<subsectionHeader confidence="0.999601">
6.2 Personal titles
</subsectionHeader>
<bodyText confidence="0.998399636363636">
Personal titles (e.g. Brig. Gen., Prime Minister-
elect) are capitalised in English. Titles are some-
times linked in Wikipedia, but the target articles,
e.g. U.S. President, are in Wikipedia categories like
Presidents of the United States, causing their incor-
rect classification as PER.
Our initial implementation assumed that links
immediately preceding PER entity links are titles.
While this feature improved performance, it only
captured one context for personal titles and failed
to handle instances where the title was only a
portion of the link text, such as Australian Prime
Minister-elect or Prime Minister of Australia.
To handle titles more comprehensively, we
compiled a list of the terms most frequently linked
immediately prior to PER links. These were man-
ually filtered, removing LOC or ORG mentions and
complemented with abbreviated titles extracted
from BBN, producing a list of 384 base title forms,
11 prefixes (e.g. Vice) and 3 suffixes (e.g. -elect).
Using these gazetteers, titles are stripped of erro-
neous NE tags.
</bodyText>
<subsectionHeader confidence="0.997035">
6.3 Adjectival forms
</subsectionHeader>
<bodyText confidence="0.999659533333333">
In English, capitalisation is retained in adjectival
entity forms, such as American or Islamic. While
these are not exactly entities, both CoNLL and BBN
annotate them as MISC. Our baseline approach
POS tagged the corpus and marked all adjectival
entities as MISC. This missed instances where na-
tionalities are used nominally, e.g. five Italians.
We extracted 339 frequent LOC and ORG ref-
erences with POS tag JJ. Words from this list
(e.g. Italian) are relabelled MISC, irrespective of
POS tag or pluralisation (e.g. Italian/JJ, Italian/NNP,
Italian/NNPS). This unfiltered list includes some er-
rors from POS tagging, e.g. First, Emmy; and others
where MISC is rarely the appropriate tag, e.g. the
Democrats (an ORG).
</bodyText>
<subsectionHeader confidence="0.999315">
6.4 Miscellaneous changes
</subsectionHeader>
<bodyText confidence="0.999829153846154">
Entity-word aliases Longest-string matching for
inferred links often adds redundant words, e.g.
both Australian and Australian people are redirects to
Australia. We therefore exclude from inference ti-
tles of form X Y where X is an alias of the same
article and Y is lowercase.
State abbreviations A gold standard may use
stylistic forms which are rare in Wikipedia. For
instance, the Wall Street Journal (BBN) uses US
state abbreviations, while Wikipedia nearly al-
ways refers to states in full. We boosted perfor-
mance by substituting a random selection of US
state names in Wikipedia with their abbreviations.
</bodyText>
<page confidence="0.994124">
617
</page>
<table confidence="0.9989417">
TRAIN With MISC MUC No MISC BBN
CoN. BBN CoN.
MUC — — 82.3 54.9 69.3
CoNLL 85.9 61.9 69.9 86.9 60.2
BBN 59.4 86.5 80.2 59.0 88.0
WP0 – no inf. 62.8 69.7 69.7 64.7 70.0
WP1 67.2 73.4 75.3 67.7 73.6
WP2 69.0 74.0 76.6 69.4 75.1
WP3 68.9 73.5 77.2 69.5 73.7
WP4 – all inf. 66.2 72.3 75.6 67.3 73.3
</table>
<tableCaption confidence="0.999321">
Table 7: Exact-match DEV F-scores
</tableCaption>
<bodyText confidence="0.9990186">
Removing rare cases We explicitly removed
sentences containing title abbreviations (e.g. Mr.)
appearing in non-PER entities such as movie titles.
Compared to newswire, these forms as personal
titles are rare in Wikipedia, so their appearance in
entities causes tagging errors. We used a similar
approach to personal names including of, which
also act as noise.
Fixing tokenization Hyphenation is a problem
in tokenisation: should London-based be one token,
two, or three? Both BBN and CoNLL treat it as one
token, but BBN labels it a LOC and CoNLL a MISC.
Our baseline had split hyphenated portions from
entities. Fixing this to match the BBN approach
improved performance significantly.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.999948611111111">
We evaluated our annotation process by build-
ing separate NER models learned from Wikipedia-
derived and gold-standard data. Our results are
given as micro-averaged precision, recall and F-
scores both in terms of MUC-style and CoNLL-style
(exact-match) scoring. We evaluated all experi-
ments with and without the MISC category.
Wikipedia’s articles are freely available for
download.1 We have used data from the 2008
May 22 dump of English Wikipedia which in-
cludes 2.3 million articles. Splitting this into sen-
tences and tokenising produced 32 million sen-
tences each containing an average of 24 tokens.
Our experiments were performed with a
Wikipedia corpus of 3.5 million tokens. Although
we had up to 294 million tokens available, we
were limited by the RAM required by the C&amp;C tag-
ger training software.
</bodyText>
<sectionHeader confidence="0.999208" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.6869415">
Tables 7 and 8 show F-scores on the MUC, CoNLL,
and BBN development sets for CoNLL-style exact
</bodyText>
<footnote confidence="0.988179">
1http://download.wikimedia.org/
</footnote>
<table confidence="0.998374">
TRAIN With MISC MUC No MISC BBN
CoN. BBN CoN.
MUC — — 89.0 68.2 79.2
CoNLL 91.0 75.1 81.4 90.9 72.6
BBN 72.7 91.1 87.6 71.8 91.5
WP0 – no inf. 71.0 79.3 76.3 71.1 78.7
WP1 74.9 82.3 81.4 73.1 81.0
WP2 76.1 82.7 81.6 74.5 81.9
WP3 76.3 82.2 81.9 74.7 80.7
WP4 – all inf. 74.3 81.4 80.9 73.1 80.7
</table>
<tableCaption confidence="0.9996325">
Table 8: MUC-style DEV F-scores
Table 9: Wikipedia as additional training data
</tableCaption>
<table confidence="0.999601833333333">
TRAIN With MISC MUC No MISC BBN
CoN. BBN CoN.
MUC — — 73.5 55.5 67.5
CoNLL 81.2 62.3 65.9 82.1 62.4
BBN 54.7 86.7 77.9 53.9 88.4
WP2 60.9 69.3 76.9 61.5 69.9
</table>
<tableCaption confidence="0.983938">
Table 10: Exact-match TEST results for WP2
</tableCaption>
<table confidence="0.9996735">
TRAIN With MISC MUC No MISC BBN
CoN. BBN CoN.
MUC — — 81.0 68.5 77.6
CoNLL 87.8 75.0 76.2 87.9 74.1
BBN 69.3 91.1 83.6 68.5 91.9
WP2 70.2 79.1 81.3 68.6 77.3
</table>
<tableCaption confidence="0.999371">
Table 11: MUC-eval TEST results for WP2
</tableCaption>
<bodyText confidence="0.999940681818182">
match and MUC-style evaluations (which are typi-
cally a few percent higher). The cross-corpus gold
standard experiments on the DEV sets are shown
first in both tables. As in Table 2, the performance
drops significantly when the training and test cor-
pus are from different sources. The corresponding
TEST set scores are given in Tables 9 and 10.
The second group of experiments in these ta-
bles show the performance of Wikipedia corpora
with increasing levels of link inference (described
in Section 6.1). Links inferred upon match-
ing article titles (WP1) and disambiguation ti-
tles (WP2) consistently increase F-score by -5%,
while surnames for PER entities (WP3) and all link
texts (WP4) tend to introduce error. A key re-
sult of our work is that the performance of non-
corresponding gold standards is often significantly
exceeded by our Wikipedia training data.
Our third group of experiments combined our
Wikipedia corpora with gold-standard data to im-
prove performance beyond traditional train-test
pairs. Table 9 shows that this approach may lead
</bodyText>
<figure confidence="0.982665857142857">
Training corpus
DEV (MUC-style F)
MUC CoNLL BBN
Corresponding TRAIN
TRAIN + WP2
89.0 91.0 91.1
90.6 91.7 91.2
</figure>
<page confidence="0.974256">
618
</page>
<table confidence="0.999316">
Token Corr. Pred. Count Why?
. ORG - 90 Inconsistencies in BBN
House ORG LOC 56 Article White House is a LOC due to classification bootstrapping
Wall - LOC 33 Wall Street is ambiguously a location and a concept
Gulf ORG LOC 29 Georgia Gulf is common in BBN, but Gulf indicates LOC
, ORG - 26 A difficult NER ambiguity in e.g. Robertson , Stephens &amp; Co.
’s ORG - 25 Unusually high frequency of ORGs ending ’s in BBN
Senate ORG LOC 20 Classification bootstrapping identifies Senate as a house, i.e. LOC
S&amp;P - MISC 20 Rare in Wikipedia, and inconsistently labelled in BBN
D. MISC PER 14 BBN uses D. to abbreviate Democrat
</table>
<tableCaption confidence="0.998538">
Table 12: Tokens in BBN DEV that our Wikipedia model frequently mis-tagged
</tableCaption>
<table confidence="0.999781857142857">
Class By exact phrase P By token F
P R F R
LOC 66.7 87.9 75.9 64.4 89.8 75.0
MISC 48.8 58.7 53.3 46.5 61.6 53.0
ORG 76.9 56.5 65.1 88.9 68.1 77.1
PER 67.3 91.4 77.5 70.5 93.6 80.5
All 68.6 69.9 69.3 80.9 75.3 78.0
</table>
<tableCaption confidence="0.999059">
Table 13: Category results for WP2 on BBN TEST
</tableCaption>
<bodyText confidence="0.998276735294117">
to small F-score increases.
Our per-class Wikipedia results are shown in
Table 13. LOC and PER entities are relatively easy
to identify, although a low precision for PER sug-
gests that many other entities have been marked
erroneously as people, unlike the high precision
and low recall of ORG. As an ill-defined category,
with uncertain mapping between BBN and CoNLL
classes, MISC precision is unsurprisingly low. We
also show results evaluating the correct labelling
of each token, where Nothman et al. (2008) had
reported results 13% higher than phrasal match-
ing, reflecting a failure to correctly identify entity
boundaries. We have reduced this difference to
9%. A BBN-trained model gives only 5% differ-
ence between phrasal and token F-score.
Among common tagging errors, we identified:
tags continuing over additional words as in New
York-based Loews Corp. all being marked as a sin-
gle ORG; nationalities marked as LOC rather than
MISC; White House a LOC rather than ORG, as
with many sports teams; single-word ORG entities
marked as PER; titles such as Dr. included in PER
tags; mis-labelling un-tagged title-case terms and
tagged lowercase terms in the gold-standard.
The corpus analysis methods described in
Section 3 show greater similarity between our
Wikipedia-derived corpus and BBN after imple-
menting our extensions. There is nonetheless
much scope for further analysis and improvement.
Notably, the most commonly mis-tagged tokens in
BBN (see Table 12) relate more often to individual
entities and stylistic differences than to a general-
isable class of errors.
</bodyText>
<sectionHeader confidence="0.996678" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99996771875">
We have demonstrated the enormous variability in
performance between using NER models trained
and tested on the same corpus versus tested on
other gold standards. This variability arises from
not only mismatched annotation schemes but also
stylistic conventions, tokenisation, and missing
frequent lexical items. Therefore, NER corpora
must be carefully matched to the target text for rea-
sonable performance. We demonstrate three ap-
proaches for gauging corpus and annotation mis-
match, and apply them to MUC, CoNLL and BBN,
and our automatically-derived Wikipedia corpora.
There is much room for improving the results of
our Wikipedia-based NE annotations. In particu-
lar, a more careful approach to link inference may
further reduce incorrect boundaries of tagged en-
tities. We plan to increase the largest training set
the C&amp;C tagger can support so that we can fully
exploit the enormous Wikipedia corpus.
However, we have shown that Wikipedia can
be used a source of free annotated data for train-
ing NER systems. Although such corpora need
to be engineered specifically to a desired appli-
cation, Wikipedia’s breadth may permit the pro-
duction of large corpora even within specific do-
mains. Our results indicate that Wikipedia data
can perform better (up to 11% for CoNLL on MUC)
than training data that is not matched to the eval-
uation, and hence is widely applicable. Trans-
forming Wikipedia into training data thus provides
a free and high-yield alternative to the laborious
manual annotation required for NER.
</bodyText>
<sectionHeader confidence="0.998814" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.979700666666667">
We would like to thank the Language Technol-
ogy Research Group and the anonymous review-
ers for their feedback. This project was sup-
ported by Australian Research Council Discovery
Project DP0665973 and Nothman was supported
by a University of Sydney Honours Scholarship.
</bodyText>
<page confidence="0.998745">
619
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989707070707">
Joohui An, Seungwoo Lee, and Gary Geunbae Lee.
2003. Automatic acquisition of named entity tagged
corpus from world wide web. In The Companion
Volume to the Proceedings of 41st Annual Meeting
of the Association for Computational Linguistics,
pages 165–168.
Ada Brunstein. 2002. Annotation guidelines for an-
swer types. LDC2005T33.
Nancy Chinchor. 1998. Overview of MUC-7. In Proc.
of the 7th Message Understanding Conference.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Proceedings of the
NIPS Workshop on Advances in Structured Learning
for Text and Speech Processing.
Michael Collins. 2002. Ranking algorithms for
named-entity extraction: boosting and the voted per-
ceptron. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
489–496, Morristown, NJ, USA.
James R. Curran and Stephen Clark. 2003. Language
independent NER using a maximum entropy tagger.
In Proceedings of the 7th Conference on Natural
Language Learning, pages 164–167.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 107–114, Budapest, Hungary.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91–134.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 698–707.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun’ichi Tsujii. 2003. GENIA corpus—a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19(suppl. 1):i180–i182.
Christopher Manning. 2006. Doing named entity
recognition? Don’t optimize for Fl. In NLPers
Blog, 25 August. http://nlpers.blogspot.
com.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of the 9th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 1–8, Bergen, Norway.
2001. Message Understanding Conference (MUC) 7.
Linguistic Data Consortium, Philadelphia.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30:3–26.
David Nadeau, Peter D. Turney, and Stan Matwin.
2006. Unsupervised named-entity recognition:
Generating gazetteers and resolving ambiguity. In
Proceedings of the 19th Canadian Conference on
Artificial Intelligence, volume 4013 of LNCS, pages
266–277.
NIST-ACE. 2008. Automatic content extraction 2008
evaluation plan (ACE08). NIST, April 7.
Joel Nothman, James R. Curran, and Tara Murphy.
2008. Transforming Wikipedia into named entity
training data. In Proceedings of the Australian Lan-
guage Technology Workshop, pages 124–132, Ho-
bart.
Alexander E. Richman and Patrick Schone. 2008.
Mining wiki resources for multilingual named entity
recognition. In 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1–9, Columbus, Ohio.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of the 7th Conference on Natural Lan-
guage Learning, pages 142–147.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: language-independent
named entity recognition. In Proceedings of the 6th
Conference on Natural Language Learning, pages
1–4.
Antonio Toral, Rafael Mu˜noz, and Monica Monachini.
2008. Named entity WordNet. In Proceedings of the
6th International Language Resources and Evalua-
tion Conference.
Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi
Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, Ting-
Yi Sung, and Wen-Lian Hsu. 2006. Various criteria
in the evaluation of biomedical named entity recog-
nition. BMC Bioinformatics, 7:96–100.
Ralph Weischedel and Ada Brunstein. 2005. BBN Pro-
noun Coreference and Entity Type Corpus. Linguis-
tic Data Consortium, Philadelphia.
</reference>
<page confidence="0.997617">
620
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767035">
<title confidence="0.999917">Analysing Wikipedia and Gold-Standard Corpora for NER Training</title>
<author confidence="0.999498">Nothman Murphy R Curran</author>
<affiliation confidence="0.9997075">School of Information Technologies University of Sydney</affiliation>
<address confidence="0.983872">NSW 2006, Australia</address>
<abstract confidence="0.9855341875">entity recognition for English typically involves one of three gold or all created by costly manual annotation. Recent work has used Wikipedia to automatically create a massive corpus of named entity annotated text. We present the first comprehensive crossevaluation of We identify the causes of poor cross-corpus performance and demonstrate ways of making them more compatible. Using our process, we develop a Wikipedia corpus which outperforms gold standard corpora on crosscorpus evaluation by up to 11%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joohui An</author>
<author>Seungwoo Lee</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Automatic acquisition of named entity tagged corpus from world wide web.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>165--168</pages>
<contexts>
<context position="4381" citStr="An et al. (2003)" startWordPosition="682" endWordPosition="685">Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 612 patterns associated with a particular type of entity, these unsupervised approaches are limited to identifying common entities present in lists or those caught by hand-built rules. External knowledge has also been used to augment supervised NER approaches. Kazama and Torisawa (2007) improve their F-score by 3% by including a Wikipedia-based feature in their machine learner. Such approaches are limited by the gold-standard data already available. Less common is the automatic creation of training data. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million word Korean corpus that gave similar results to manually-annotated training data. Richman and Schone (2008) used a method similar to Nothman et al. (2008) in order to derive NE-annotated corpora in languages other than English. They classify Wikipedia articles in foreign languages by transferring knowledge from English Wikipedia via inter-language links. With these classifications they automatically annotate entire articles for NER training, and suggest that their results with a 340k-word Spanish corpus ar</context>
</contexts>
<marker>An, Lee, Lee, 2003</marker>
<rawString>Joohui An, Seungwoo Lee, and Gary Geunbae Lee. 2003. Automatic acquisition of named entity tagged corpus from world wide web. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 165–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ada Brunstein</author>
</authors>
<title>Annotation guidelines for answer types.</title>
<date>2002</date>
<pages>2005--33</pages>
<contexts>
<context position="6403" citStr="Brunstein, 2002" startWordPosition="990" endWordPosition="991">ports scores. Each corpus uses a different set of entity labels. MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and time information. The CoNLL NER shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) mark PER, ORG and LOC entities, as well as a broad miscellaneous class Number of tokens TRAIN DEV TEST 83601 18655 60436 203621 51362 46435 901894 142218 129654 Table 1: Gold-standard NE-annotated corpora (MISC; e.g. events, artworks and nationalities). BBN annotates the entire Penn Treebank corpus with 105 fine-grained tags (Brunstein, 2002): 54 corresponding to CoNLL entities; 21 for numerical and time data; and 30 for other classes. For our evaluation, BBN’s tags were reduced to the equivalent CoNLL tags, with extra tags in the BBN and MUC data removed. Since no MISC tags are marked in MUC, they need to be removed from CoNLL, BBN and Wikipedia data for comparison. We transformed all three corpora into a common format and annotated them with part-ofspeech tags using the Penn Treebank-trained C&amp;C POS tagger. We altered the default MUC tokenisation to attach periods to abbreviations when sentence-internal. While standard training </context>
</contexts>
<marker>Brunstein, 2002</marker>
<rawString>Ada Brunstein. 2002. Annotation guidelines for answer types. LDC2005T33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Overview of MUC-7.</title>
<date>1998</date>
<booktitle>In Proc. of the 7th Message Understanding Conference.</booktitle>
<contexts>
<context position="1206" citStr="Chinchor, 1998" startWordPosition="186" endWordPosition="187">corpus performance and demonstrate ways of making them more compatible. Using our process, we develop a Wikipedia corpus which outperforms gold standard corpora on crosscorpus evaluation by up to 11%. 1 Introduction Named Entity Recognition (NER), the task of identifying and classifying the names of people, organisations and other entities within text, is central to many NLP systems. NER developed from information extraction in the Message Understanding Conferences (MUC) of the 1990s. By MUC 6 and 7, NER had become a distinct task: tagging proper names, and temporal and numerical expressions (Chinchor, 1998). Statistical machine learning systems have proven successful for NER. These learn patterns associated with individual entity classes, making use of many contextual, orthographic, linguistic and external knowledge features. However, they rely heavily on large annotated training corpora. This need for costly expert annotation hinders the creation of more task-adaptable, highperformance named entity recognisers. In acquiring new sources for annotated corpora, we require an analysis of training data as a variable in NER. This paper compares the three main goldstandard corpora. We found that taggi</context>
<context position="7511" citStr="Chinchor, 1998" startWordPosition="1169" endWordPosition="1170">efault MUC tokenisation to attach periods to abbreviations when sentence-internal. While standard training (TRAIN), development (DEV) and final test (TEST) set divisions were available for CoNLL and MUC, the BBN corpus was split at our discretion: sections 03–21 for TRAIN, 00–02 for DEV and 22-24 for TEST. Corpus sizes are compared in Table 1. 2.2 Evaluating NER performance One challenge for NER research is establishing an appropriate evaluation metric (Nadeau and Sekine, 2007). In particular, entities may be correctly delimited but mis-classified, or entity boundaries may be mismatched. MUC (Chinchor, 1998) awarded equal score for matching type, where an entity’s class is identified with at least one boundary matching, and text, where an entity’s boundaries are precisely delimited, irrespective of the classification. This equal weighting is unrealistic, as some boundary errors are highly significant, while others are arbitrary. CoNLL awarded exact (type and text) phrasal matches, ignoring boundary issues entirely and providing a lower-bound measure of NER performance. Manning (2006) argues that CoNLLstyle evaluation is biased towards systems which leave entities with ambiguous boundaries untagge</context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>Nancy Chinchor. 1998. Overview of MUC-7. In Proc. of the 7th Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Named-entity recognition in novel domains with external lexical knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of the NIPS Workshop on Advances in Structured Learning for Text and Speech Processing.</booktitle>
<contexts>
<context position="13163" citStr="Ciaramita and Altun, 2005" startWordPosition="2081" endWordPosition="2084">s errors in the NER model. 4 Comparing gold-standard corpora We trained the C&amp;C NER tagger (Curran and Clark, 2003) to build separate models for each goldstandard corpus. The C&amp;C tagger utilises a number 614 MUC — — 73.5 55.5 67.5 CoNLL 81.2 62.3 65.9 82.1 62.4 BBN 54.7 86.7 77.9 53.9 88.4 Table 2: Gold standard F-scores (exact-match) of orthographic, contextual and in-document features, as well as gazetteers for personal names. Table 2 shows that each training set performs much better on corresponding (same corpus) test sets (italics) than on test sets from other sources, also identified by (Ciaramita and Altun, 2005). NER research typically deals with small improvements (-1% F-score). The 12-32% mismatch between training and test corpora suggests that an appropriate training corpus is a much greater concern. The exception is BBN on MUC, due to differing TEST and DEV subject matter. Here we analyse the variation within and between the gold standards. Table 3 lists some n-gram tag variations for BBN and CoNLL (TRAIN + DEV). These include cases of schematic variations (e.g. the period in Co .) and tagging errors. Some n-grams have three variants, e.g. the Standard &amp; Poor ’s 500 which appears untagged, as the</context>
</contexts>
<marker>Ciaramita, Altun, 2005</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2005. Named-entity recognition in novel domains with external lexical knowledge. In Proceedings of the NIPS Workshop on Advances in Structured Learning for Text and Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>489--496</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10209" citStr="Collins (2002)" startWordPosition="1599" endWordPosition="1600">Bay area,. Six instances of area are tagged as nonentities, but two instances are tagged as part of the LOC that precedes it. The other five tokens in this n-gram are consistently labelled. 3.2 Entity type frequency An intuitive approach to finding discrepancies between corpora is to compare the distribution of entities within each corpus. To make this manageable, instances need to be grouped by more than their class labels. We used the following groups: POS sequences: Types of candidate entities may often be distinguished by their POS tags, e.g. nationalities are often JJ or NNPS. Wordtypes: Collins (2002) proposed wordtypes where all uppercase characters map to A, lowercase to a, and digits to 0. Adjacent characters in the same orthographic class were collapsed. However, we distinguish single from multiple characters by duplication. e.g. USS Nimitz (CVN-68) has wordtype AA Aaa (AA-00). Wordtype with functions: We also map content words to wordtypes only—function words are retained, e.g. Bank of New England Corp. maps to Aaa of Aaa Aaa Aaa.. No approach provides sufficient discrimination alone: wordtype patterns are able to distinguish within common POS tags and vice versa. Each method can be f</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for named-entity extraction: boosting and the voted perceptron. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 489–496, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent NER using a maximum entropy tagger.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning,</booktitle>
<pages>164--167</pages>
<contexts>
<context position="2355" citStr="Curran and Clark, 2003" startWordPosition="360" endWordPosition="363">is paper compares the three main goldstandard corpora. We found that tagging models built on each corpus perform relatively poorly when tested on the others. We therefore present three methods for analysing internal and intercorpus inconsistencies. Our analysis demonstrates that seemingly minor variations in the text itself, starting right from tokenisation can have a huge impact on practical NER performance. We take this experience and apply it to a corpus created automatically using Wikipedia. This corpus was created following the method of Nothman et al. (2008). By training the C&amp;C tagger (Curran and Clark, 2003) on the gold-standard corpora and our new Wikipedia-derived training data, we evaluate the usefulness of the latter and explore the nature of the training corpus as a variable in NER. Our Wikipedia-derived corpora exceed the performance of non-corresponding training and test sets by up to 11% F-score, and can be engineered to automatically produce models consistent with various NE-annotation schema. We show that it is possible to automatically create large, free, named entity-annotated corpora for general or domain specific tasks. 2 NER and annotated corpora Research into NER has rarely consid</context>
<context position="12652" citStr="Curran and Clark, 2003" startWordPosition="1995" endWordPosition="1998">: if gold-standard Mr. [PER Ross] is tagged PER, it is counted as confusion between PER and O PER. To further distinguish classes of error, the entity type groupings from Section 3.2 are also used. This analysis is useful for both tagger evaluation and cross-corpus evaluation, e.g. BBN versus CoNLL on a BBN test set. This involves finding confusion matrix entries where BBN and CoNLL’s performance differs significantly, identifying common errors related to difficult instances in the test corpus as well as errors in the NER model. 4 Comparing gold-standard corpora We trained the C&amp;C NER tagger (Curran and Clark, 2003) to build separate models for each goldstandard corpus. The C&amp;C tagger utilises a number 614 MUC — — 73.5 55.5 67.5 CoNLL 81.2 62.3 65.9 82.1 62.4 BBN 54.7 86.7 77.9 53.9 88.4 Table 2: Gold standard F-scores (exact-match) of orthographic, contextual and in-document features, as well as gazetteers for personal names. Table 2 shows that each training set performs much better on corresponding (same corpus) test sets (italics) than on test sets from other sources, also identified by (Ciaramita and Altun, 2005). NER research typically deals with small improvements (-1% F-score). The 12-32% mismatch</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Language independent NER using a maximum entropy tagger. In Proceedings of the 7th Conference on Natural Language Learning, pages 164–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting errors in part-of-speech annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>107--114</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="9099" citStr="Dickinson and Meurers (2003)" startWordPosition="1413" endWordPosition="1416">rlap, per-token measures, or more semantically-driven matching. ACE evaluations instead use a customizable evaluation metric with weights specified for different types of error (NIST-ACE, 2008). 3 Corpus and error analysis approaches To evaluate the performance impact of a corpus we may analyse (a) the annotations themselves; or (b) the model built on those annotations and its performance. A corpus can be considered in isolation or by comparison with other corpora. We use three methods to explore intra- and inter-corpus consistency in MUC, CoNLL, and BBN in Section 4. 3.1 N-gram tag variation Dickinson and Meurers (2003) present a clever method for finding inconsistencies within POS annotated corpora, which we apply to NER corpora. Their approach finds all n-grams in a corpus which appear multiple times, albeit with variant tags for some sub-sequence, the nucleus (see e.g. Table 3). To remove valid ambiguity, they suggest using (a) a minimum n-gram length; (b) a minimum margin of invariant terms around the nucleus. For example, the BBN TRAIN corpus includes eight occurrences of the 6-gram the San Francisco Bay area,. Six instances of area are tagged as nonentities, but two instances are tagged as part of the </context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Markus Dickinson and W. Detmar Meurers. 2003. Detecting errors in part-of-speech annotation. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 107–114, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="3339" citStr="Etzioni et al., 2005" startWordPosition="518" endWordPosition="521">tent with various NE-annotation schema. We show that it is possible to automatically create large, free, named entity-annotated corpora for general or domain specific tasks. 2 NER and annotated corpora Research into NER has rarely considered the impact of training corpora. The CoNLL evaluations focused on machine learning methods (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) while more recent work has often involved the use of external knowledge. Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al., 2005) or Wikipedia (Toral et al., 2008), although Mikheev et al. (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance. Nadeau et al. (2006) use such lists in an unsupervised NE recogniser, outperforming some entrants of the MUC Named Entity Task. Unlike statistical approaches which learn Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612–620, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 612 patterns associated with a particular type of entity, these unsupervised approac</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>698--707</pages>
<contexts>
<context position="4142" citStr="Kazama and Torisawa (2007)" startWordPosition="643" endWordPosition="646">et al. (2006) use such lists in an unsupervised NE recogniser, outperforming some entrants of the MUC Named Entity Task. Unlike statistical approaches which learn Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612–620, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 612 patterns associated with a particular type of entity, these unsupervised approaches are limited to identifying common entities present in lists or those caught by hand-built rules. External knowledge has also been used to augment supervised NER approaches. Kazama and Torisawa (2007) improve their F-score by 3% by including a Wikipedia-based feature in their machine learner. Such approaches are limited by the gold-standard data already available. Less common is the automatic creation of training data. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million word Korean corpus that gave similar results to manually-annotated training data. Richman and Schone (2008) used a method similar to Nothman et al. (2008) in order to derive NE-annotated corpora in languages other than English. They classify Wikipedia articles in foreign </context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 698–707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus—a semantically annotated corpus for bio-textmining.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<pages>1--180</pages>
<contexts>
<context position="5582" citStr="Kim et al., 2003" startWordPosition="861" endWordPosition="864">panish corpus are comparable to 20k-40k words of gold-standard training data when using MUC-style evaluation metrics. 2.1 Gold-standard corpora We evaluate our Wikipedia-derived corpora against three sets of manually-annotated data from (a) the MUC-7 Named Entity Task (MUC, 2001); (b) the English CoNLL-03 Shared Task (Tjong Kim Sang and De Meulder, 2003); (c) the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005). We consider only the generic newswire NER task, although domain-specific annotated corpora have been developed for applications such as bio-text mining (Kim et al., 2003). Stylistic and genre differences between the source texts affect compatibility for NER evaluation e.g., the CoNLL corpus formats headlines in all-caps, and includes non-sentential data such as tables of sports scores. Each corpus uses a different set of entity labels. MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and time information. The CoNLL NER shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) mark PER, ORG and LOC entities, as well as a broad miscellaneous class Number of tokens TRAIN DEV TEST 83601 18655 60436 20</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2003. GENIA corpus—a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl. 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
</authors>
<title>Doing named entity recognition? Don’t optimize for Fl.</title>
<date>2006</date>
<booktitle>In NLPers Blog, 25</booktitle>
<note>http://nlpers.blogspot. com.</note>
<contexts>
<context position="7996" citStr="Manning (2006)" startWordPosition="1241" endWordPosition="1242"> particular, entities may be correctly delimited but mis-classified, or entity boundaries may be mismatched. MUC (Chinchor, 1998) awarded equal score for matching type, where an entity’s class is identified with at least one boundary matching, and text, where an entity’s boundaries are precisely delimited, irrespective of the classification. This equal weighting is unrealistic, as some boundary errors are highly significant, while others are arbitrary. CoNLL awarded exact (type and text) phrasal matches, ignoring boundary issues entirely and providing a lower-bound measure of NER performance. Manning (2006) argues that CoNLLstyle evaluation is biased towards systems which leave entities with ambiguous boundaries untagged, since boundary errors amount simultaneously to false positives and false negatives. In both MUC and CoNLL, micro-averaged precision, recall and F1 score summarise the results. Corpus # tags MUC-7 3 CoNLL-03 4 BBN 54 613 Tsai et al. (2006) compares a number of methods for relaxing boundary requirements: matching only the left or right boundary, any tag overlap, per-token measures, or more semantically-driven matching. ACE evaluations instead use a customizable evaluation metric </context>
</contexts>
<marker>Manning, 2006</marker>
<rawString>Christopher Manning. 2006. Doing named entity recognition? Don’t optimize for Fl. In NLPers Blog, 25 August. http://nlpers.blogspot. com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
<author>Claire Grover</author>
</authors>
<title>Named entity recognition without gazetteers.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Bergen,</location>
<contexts>
<context position="3405" citStr="Mikheev et al. (1999)" startWordPosition="529" endWordPosition="532">e to automatically create large, free, named entity-annotated corpora for general or domain specific tasks. 2 NER and annotated corpora Research into NER has rarely considered the impact of training corpora. The CoNLL evaluations focused on machine learning methods (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) while more recent work has often involved the use of external knowledge. Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al., 2005) or Wikipedia (Toral et al., 2008), although Mikheev et al. (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance. Nadeau et al. (2006) use such lists in an unsupervised NE recogniser, outperforming some entrants of the MUC Named Entity Task. Unlike statistical approaches which learn Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612–620, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 612 patterns associated with a particular type of entity, these unsupervised approaches are limited to identifying common entities present in lists or</context>
</contexts>
<marker>Mikheev, Moens, Grover, 1999</marker>
<rawString>Andrei Mikheev, Marc Moens, and Claire Grover. 1999. Named entity recognition without gazetteers. In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics, pages 1–8, Bergen, Norway. 2001. Message Understanding Conference (MUC) 7. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<booktitle>Lingvisticae Investigationes,</booktitle>
<pages>30--3</pages>
<contexts>
<context position="7378" citStr="Nadeau and Sekine, 2007" startWordPosition="1149" endWordPosition="1152">three corpora into a common format and annotated them with part-ofspeech tags using the Penn Treebank-trained C&amp;C POS tagger. We altered the default MUC tokenisation to attach periods to abbreviations when sentence-internal. While standard training (TRAIN), development (DEV) and final test (TEST) set divisions were available for CoNLL and MUC, the BBN corpus was split at our discretion: sections 03–21 for TRAIN, 00–02 for DEV and 22-24 for TEST. Corpus sizes are compared in Table 1. 2.2 Evaluating NER performance One challenge for NER research is establishing an appropriate evaluation metric (Nadeau and Sekine, 2007). In particular, entities may be correctly delimited but mis-classified, or entity boundaries may be mismatched. MUC (Chinchor, 1998) awarded equal score for matching type, where an entity’s class is identified with at least one boundary matching, and text, where an entity’s boundaries are precisely delimited, irrespective of the classification. This equal weighting is unrealistic, as some boundary errors are highly significant, while others are arbitrary. CoNLL awarded exact (type and text) phrasal matches, ignoring boundary issues entirely and providing a lower-bound measure of NER performan</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Lingvisticae Investigationes, 30:3–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Peter D Turney</author>
<author>Stan Matwin</author>
</authors>
<title>Unsupervised named-entity recognition: Generating gazetteers and resolving ambiguity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 19th Canadian Conference on Artificial Intelligence,</booktitle>
<volume>4013</volume>
<pages>266--277</pages>
<contexts>
<context position="3529" citStr="Nadeau et al. (2006)" startWordPosition="549" endWordPosition="552">ed corpora Research into NER has rarely considered the impact of training corpora. The CoNLL evaluations focused on machine learning methods (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) while more recent work has often involved the use of external knowledge. Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al., 2005) or Wikipedia (Toral et al., 2008), although Mikheev et al. (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance. Nadeau et al. (2006) use such lists in an unsupervised NE recogniser, outperforming some entrants of the MUC Named Entity Task. Unlike statistical approaches which learn Proceedings of the 12th Conference of the European Chapter of the ACL, pages 612–620, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 612 patterns associated with a particular type of entity, these unsupervised approaches are limited to identifying common entities present in lists or those caught by hand-built rules. External knowledge has also been used to augment supervised NER approaches. Kazama and To</context>
</contexts>
<marker>Nadeau, Turney, Matwin, 2006</marker>
<rawString>David Nadeau, Peter D. Turney, and Stan Matwin. 2006. Unsupervised named-entity recognition: Generating gazetteers and resolving ambiguity. In Proceedings of the 19th Canadian Conference on Artificial Intelligence, volume 4013 of LNCS, pages 266–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST-ACE</author>
</authors>
<title>Automatic content extraction</title>
<date>2008</date>
<note>evaluation plan (ACE08). NIST,</note>
<contexts>
<context position="8664" citStr="NIST-ACE, 2008" startWordPosition="1343" endWordPosition="1344">systems which leave entities with ambiguous boundaries untagged, since boundary errors amount simultaneously to false positives and false negatives. In both MUC and CoNLL, micro-averaged precision, recall and F1 score summarise the results. Corpus # tags MUC-7 3 CoNLL-03 4 BBN 54 613 Tsai et al. (2006) compares a number of methods for relaxing boundary requirements: matching only the left or right boundary, any tag overlap, per-token measures, or more semantically-driven matching. ACE evaluations instead use a customizable evaluation metric with weights specified for different types of error (NIST-ACE, 2008). 3 Corpus and error analysis approaches To evaluate the performance impact of a corpus we may analyse (a) the annotations themselves; or (b) the model built on those annotations and its performance. A corpus can be considered in isolation or by comparison with other corpora. We use three methods to explore intra- and inter-corpus consistency in MUC, CoNLL, and BBN in Section 4. 3.1 N-gram tag variation Dickinson and Meurers (2003) present a clever method for finding inconsistencies within POS annotated corpora, which we apply to NER corpora. Their approach finds all n-grams in a corpus which </context>
</contexts>
<marker>NIST-ACE, 2008</marker>
<rawString>NIST-ACE. 2008. Automatic content extraction 2008 evaluation plan (ACE08). NIST, April 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Nothman</author>
<author>James R Curran</author>
<author>Tara Murphy</author>
</authors>
<title>Transforming Wikipedia into named entity training data.</title>
<date>2008</date>
<booktitle>In Proceedings of the Australian Language Technology Workshop,</booktitle>
<pages>124--132</pages>
<location>Hobart.</location>
<contexts>
<context position="2302" citStr="Nothman et al. (2008)" startWordPosition="351" endWordPosition="354"> analysis of training data as a variable in NER. This paper compares the three main goldstandard corpora. We found that tagging models built on each corpus perform relatively poorly when tested on the others. We therefore present three methods for analysing internal and intercorpus inconsistencies. Our analysis demonstrates that seemingly minor variations in the text itself, starting right from tokenisation can have a huge impact on practical NER performance. We take this experience and apply it to a corpus created automatically using Wikipedia. This corpus was created following the method of Nothman et al. (2008). By training the C&amp;C tagger (Curran and Clark, 2003) on the gold-standard corpora and our new Wikipedia-derived training data, we evaluate the usefulness of the latter and explore the nature of the training corpus as a variable in NER. Our Wikipedia-derived corpora exceed the performance of non-corresponding training and test sets by up to 11% F-score, and can be engineered to automatically produce models consistent with various NE-annotation schema. We show that it is possible to automatically create large, free, named entity-annotated corpora for general or domain specific tasks. 2 NER and </context>
<context position="4624" citStr="Nothman et al. (2008)" startWordPosition="722" endWordPosition="725">or those caught by hand-built rules. External knowledge has also been used to augment supervised NER approaches. Kazama and Torisawa (2007) improve their F-score by 3% by including a Wikipedia-based feature in their machine learner. Such approaches are limited by the gold-standard data already available. Less common is the automatic creation of training data. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million word Korean corpus that gave similar results to manually-annotated training data. Richman and Schone (2008) used a method similar to Nothman et al. (2008) in order to derive NE-annotated corpora in languages other than English. They classify Wikipedia articles in foreign languages by transferring knowledge from English Wikipedia via inter-language links. With these classifications they automatically annotate entire articles for NER training, and suggest that their results with a 340k-word Spanish corpus are comparable to 20k-40k words of gold-standard training data when using MUC-style evaluation metrics. 2.1 Gold-standard corpora We evaluate our Wikipedia-derived corpora against three sets of manually-annotated data from (a) the MUC-7 Named En</context>
<context position="15929" citStr="Nothman et al. (2008)" startWordPosition="2555" endWordPosition="2558">NLL has only three instances of Mr., so it often mis-labels Mr. as part of a PER entity. The MUC model also has trouble recognising ORG names ending with corporate abbreviations, and may fail to identify abbreviated US state names. Our analysis demonstrates that seemingly minor orthographic variations in the text, tokenisation and annotation schemes can have a huge impact on practical NER performance. 5 From Wikipedia to NE-annotated text Wikipedia is a collaborative, multilingual, online encyclopedia which includes over 2.3 million articles in English alone. Our baseline approach detailed in Nothman et al. (2008) exploits the hyperlinking between articles to derive a NE corpus. Since —74% of Wikipedia articles describe topics covering entity classes, many of Wikipedia’s links correspond to entity annotations in goldstandard NE corpora. We derive a NE-annotated corpus by the following steps: 1. Classify all articles into entity classes 2. Split Wikipedia articles into sentences 3. Label NEs according to link targets 4. Select sentences for inclusion in a corpus With MISC CoNLL BBN TRAIN Without MISC MUC CoNLL BBN 615 N-gram Tag # Tag # Co . - 52 ORG 111 Smith Barney, Harris Upham &amp; Co. - 1 ORG 9 the Co</context>
<context position="18462" citStr="Nothman et al. (2008)" startWordPosition="3019" endWordPosition="3022">lassified as one of: unknown; a member of a NE category (LOC, ORG, PER, MISC, as per CoNLL); a disambiguation page (these list possible referent articles for a given title); or a non-entity (NON). This classifier classifier achieves 89% F-score. A sentence is selected for our corpus when all of its capitalised words are linked to articles with a known class. Exceptions are made for common titlecase words, e.g. I, Mr., June, and sentence-initial words. We also infer additional links — variant titles are collected for each Wikipedia topic and are marked up in articles which link to them — which Nothman et al. (2008) found increases coverage. Transforming links into annotations that conform to a gold standard is far from trivial. Link boundaries need to be adjusted, e.g. to remove excess punctuation. Adjectival forms of entities (e.g. American, Islamic) generally link to nominal articles. However, they are treated by CoNLL and our Tag # Tag # MISC 2 PER 5 - 58 LOC 4 - 8 LOC 3 LOC 1 MISC 2 Table 5: N-gram variations in the Wiki baseline BBN mapping as MISC. POS tagging the corpus and relabelling entities ending with JJ as MISC solves this heuristically. Although they are capitalised in English, personal ti</context>
<context position="29591" citStr="Nothman et al. (2008)" startWordPosition="4908" endWordPosition="4911"> 67.3 91.4 77.5 70.5 93.6 80.5 All 68.6 69.9 69.3 80.9 75.3 78.0 Table 13: Category results for WP2 on BBN TEST to small F-score increases. Our per-class Wikipedia results are shown in Table 13. LOC and PER entities are relatively easy to identify, although a low precision for PER suggests that many other entities have been marked erroneously as people, unlike the high precision and low recall of ORG. As an ill-defined category, with uncertain mapping between BBN and CoNLL classes, MISC precision is unsurprisingly low. We also show results evaluating the correct labelling of each token, where Nothman et al. (2008) had reported results 13% higher than phrasal matching, reflecting a failure to correctly identify entity boundaries. We have reduced this difference to 9%. A BBN-trained model gives only 5% difference between phrasal and token F-score. Among common tagging errors, we identified: tags continuing over additional words as in New York-based Loews Corp. all being marked as a single ORG; nationalities marked as LOC rather than MISC; White House a LOC rather than ORG, as with many sports teams; single-word ORG entities marked as PER; titles such as Dr. included in PER tags; mis-labelling un-tagged t</context>
</contexts>
<marker>Nothman, Curran, Murphy, 2008</marker>
<rawString>Joel Nothman, James R. Curran, and Tara Murphy. 2008. Transforming Wikipedia into named entity training data. In Proceedings of the Australian Language Technology Workshop, pages 124–132, Hobart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander E Richman</author>
<author>Patrick Schone</author>
</authors>
<title>Mining wiki resources for multilingual named entity recognition.</title>
<date>2008</date>
<booktitle>In 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1--9</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="4577" citStr="Richman and Schone (2008)" startWordPosition="712" endWordPosition="715">ed to identifying common entities present in lists or those caught by hand-built rules. External knowledge has also been used to augment supervised NER approaches. Kazama and Torisawa (2007) improve their F-score by 3% by including a Wikipedia-based feature in their machine learner. Such approaches are limited by the gold-standard data already available. Less common is the automatic creation of training data. An et al. (2003) extracted sentences containing listed entities from the web, and produced a 1.8 million word Korean corpus that gave similar results to manually-annotated training data. Richman and Schone (2008) used a method similar to Nothman et al. (2008) in order to derive NE-annotated corpora in languages other than English. They classify Wikipedia articles in foreign languages by transferring knowledge from English Wikipedia via inter-language links. With these classifications they automatically annotate entire articles for NER training, and suggest that their results with a 340k-word Spanish corpus are comparable to 20k-40k words of gold-standard training data when using MUC-style evaluation metrics. 2.1 Gold-standard corpora We evaluate our Wikipedia-derived corpora against three sets of manu</context>
</contexts>
<marker>Richman, Schone, 2008</marker>
<rawString>Alexander E. Richman and Patrick Schone. 2008. Mining wiki resources for multilingual named entity recognition. In 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1–9, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning,</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the 7th Conference on Natural Language Learning, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 shared task: language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="3071" citStr="Sang, 2002" startWordPosition="474" endWordPosition="475">he latter and explore the nature of the training corpus as a variable in NER. Our Wikipedia-derived corpora exceed the performance of non-corresponding training and test sets by up to 11% F-score, and can be engineered to automatically produce models consistent with various NE-annotation schema. We show that it is possible to automatically create large, free, named entity-annotated corpora for general or domain specific tasks. 2 NER and annotated corpora Research into NER has rarely considered the impact of training corpora. The CoNLL evaluations focused on machine learning methods (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) while more recent work has often involved the use of external knowledge. Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al., 2005) or Wikipedia (Toral et al., 2008), although Mikheev et al. (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance. Nadeau et al. (2006) use such lists in an unsupervised NE recogniser, outperforming some entrants of the MUC Named Entity Task. Unlike statistical approaches whic</context>
<context position="6020" citStr="Sang, 2002" startWordPosition="931" endWordPosition="932">. We consider only the generic newswire NER task, although domain-specific annotated corpora have been developed for applications such as bio-text mining (Kim et al., 2003). Stylistic and genre differences between the source texts affect compatibility for NER evaluation e.g., the CoNLL corpus formats headlines in all-caps, and includes non-sentential data such as tables of sports scores. Each corpus uses a different set of entity labels. MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and time information. The CoNLL NER shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) mark PER, ORG and LOC entities, as well as a broad miscellaneous class Number of tokens TRAIN DEV TEST 83601 18655 60436 203621 51362 46435 901894 142218 129654 Table 1: Gold-standard NE-annotated corpora (MISC; e.g. events, artworks and nationalities). BBN annotates the entire Penn Treebank corpus with 105 fine-grained tags (Brunstein, 2002): 54 corresponding to CoNLL entities; 21 for numerical and time data; and 30 for other classes. For our evaluation, BBN’s tags were reduced to the equivalent CoNLL tags, with extra tags in the BBN and MUC data removed</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: language-independent named entity recognition. In Proceedings of the 6th Conference on Natural Language Learning, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
<author>Rafael Mu˜noz</author>
<author>Monica Monachini</author>
</authors>
<title>Named entity WordNet.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Language Resources and Evaluation Conference.</booktitle>
<marker>Toral, Mu˜noz, Monachini, 2008</marker>
<rawString>Antonio Toral, Rafael Mu˜noz, and Monica Monachini. 2008. Named entity WordNet. In Proceedings of the 6th International Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Tzong-Han Tsai</author>
<author>Shih-Hung Wu</author>
<author>Wen-Chi Chou</author>
<author>Yu-Chun Lin</author>
<author>Ding He</author>
<author>Jieh Hsiang</author>
<author>TingYi Sung</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>Various criteria in the evaluation of biomedical named entity recognition.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<pages>7--96</pages>
<contexts>
<context position="8352" citStr="Tsai et al. (2006)" startWordPosition="1296" endWordPosition="1299">al weighting is unrealistic, as some boundary errors are highly significant, while others are arbitrary. CoNLL awarded exact (type and text) phrasal matches, ignoring boundary issues entirely and providing a lower-bound measure of NER performance. Manning (2006) argues that CoNLLstyle evaluation is biased towards systems which leave entities with ambiguous boundaries untagged, since boundary errors amount simultaneously to false positives and false negatives. In both MUC and CoNLL, micro-averaged precision, recall and F1 score summarise the results. Corpus # tags MUC-7 3 CoNLL-03 4 BBN 54 613 Tsai et al. (2006) compares a number of methods for relaxing boundary requirements: matching only the left or right boundary, any tag overlap, per-token measures, or more semantically-driven matching. ACE evaluations instead use a customizable evaluation metric with weights specified for different types of error (NIST-ACE, 2008). 3 Corpus and error analysis approaches To evaluate the performance impact of a corpus we may analyse (a) the annotations themselves; or (b) the model built on those annotations and its performance. A corpus can be considered in isolation or by comparison with other corpora. We use thre</context>
</contexts>
<marker>Tsai, Wu, Chou, Lin, He, Hsiang, Sung, Hsu, 2006</marker>
<rawString>Richard Tzong-Han Tsai, Shih-Hung Wu, Wen-Chi Chou, Yu-Chun Lin, Ding He, Jieh Hsiang, TingYi Sung, and Wen-Lian Hsu. 2006. Various criteria in the evaluation of biomedical named entity recognition. BMC Bioinformatics, 7:96–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<date>2005</date>
<booktitle>BBN Pronoun Coreference and Entity Type Corpus. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="5410" citStr="Weischedel and Brunstein, 2005" startWordPosition="836" endWordPosition="839">rom English Wikipedia via inter-language links. With these classifications they automatically annotate entire articles for NER training, and suggest that their results with a 340k-word Spanish corpus are comparable to 20k-40k words of gold-standard training data when using MUC-style evaluation metrics. 2.1 Gold-standard corpora We evaluate our Wikipedia-derived corpora against three sets of manually-annotated data from (a) the MUC-7 Named Entity Task (MUC, 2001); (b) the English CoNLL-03 Shared Task (Tjong Kim Sang and De Meulder, 2003); (c) the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005). We consider only the generic newswire NER task, although domain-specific annotated corpora have been developed for applications such as bio-text mining (Kim et al., 2003). Stylistic and genre differences between the source texts affect compatibility for NER evaluation e.g., the CoNLL corpus formats headlines in all-caps, and includes non-sentential data such as tables of sports scores. Each corpus uses a different set of entity labels. MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and time information. The CoNLL NER shared tasks (Tjong Kim </context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN Pronoun Coreference and Entity Type Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>