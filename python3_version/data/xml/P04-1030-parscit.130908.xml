<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001190">
<title confidence="0.927538">
Head-Driven Parsing for Word Lattices
</title>
<note confidence="0.90645725">
Bob Carpenter
Alias I, Inc.
Brooklyn, NY, USA
carp@alias-i.com
</note>
<author confidence="0.985173">
Christopher Collins
</author>
<affiliation confidence="0.9988805">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.59556">
Toronto, ON, Canada
</address>
<email confidence="0.998085">
ccollins@cs.utoronto.ca
</email>
<author confidence="0.995795">
Gerald Penn
</author>
<affiliation confidence="0.854906666666667">
Department of Computer Science
University of Toronto
Toronto, ON, Canada
</affiliation>
<email confidence="0.995561">
gpenn@cs.utoronto.ca
</email>
<sectionHeader confidence="0.993837" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999799133333333">
We present the first application of the head-driven
statistical parsing model of Collins (1999) as a si-
multaneous language model and parser for large-
vocabulary speech recognition. The model is
adapted to an online left to right chart-parser for
word lattices, integrating acoustic, n-gram, and
parser probabilities. The parser uses structural
and lexical dependencies not considered by n-
gram models, conditioning recognition on more
linguistically-grounded relationships. Experiments
on the Wall Street Journal treebank and lattice cor-
pora show word error rates competitive with the
standard n-gram language model while extracting
additional structural information useful for speech
understanding.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918413793104">
The question of how to integrate high-level knowl-
edge representations of language with automatic
speech recognition (ASR) is becoming more impor-
tant as (1) speech recognition technology matures,
(2) the rate of improvement of recognition accu-
racy decreases, and (3) the need for additional in-
formation (beyond simple transcriptions) becomes
evident. Most of the currently best ASR systems use
an n-gram language model of the type pioneered by
Bahl et al. (1983). Recently, research has begun to
show progress towards application of new and bet-
ter models of spoken language (Hall and Johnson,
2003; Roark, 2001; Chelba and Jelinek, 2000).
Our goal is integration of head-driven lexical-
ized parsing with acoustic and n-gram models for
speech recognition, extracting high-level structure
from speech, while simultaneously selecting the
best path in a word lattice. Parse trees generated
by this process will be useful for automated speech
understanding, such as in higher semantic parsing
(Ng and Zelle, 1997).
Collins (1999) presents three lexicalized models
which consider long-distance dependencies within a
sentence. Grammar productions are conditioned on
headwords. The conditioning context is thus more
focused than that of a large n-gram covering the
same span, so the sparse data problems arising from
the sheer size of the parameter space are less press-
ing. However, sparse data problems arising from
the limited availability of annotated training data be-
come a problem.
We test the head-driven statistical lattice parser
with word lattices from the NIST HUB-1 corpus,
which has been used by others in related work (Hall
and Johnson, 2003; Roark, 2001; Chelba and Je-
linek, 2000). Parse accuracy and word error rates
are reported. We present an analysis of the ef-
fects of pruning and heuristic search on efficiency
and accuracy and note several simplifying assump-
tions common to other reported experiments in this
area, which present challenges for scaling up to real-
world applications.
This work shows the importance of careful al-
gorithm and data structure design and choice of
dynamic programming constraints to the efficiency
and accuracy of a head-driven probabilistic parser
for speech. We find that the parsing model of
Collins (1999) can be successfully adapted as a lan-
guage model for speech recognition.
In the following section, we present a review of
recent works in high-level language modelling for
speech recognition. We describe the word lattice
parser developed in this work in Section 3. Sec-
tion 4 is a description of current evaluation metrics,
and suggestions for new metrics. Experiments on
strings and word lattices are reported in Section 5,
and conclusions and opportunities for future work
are outlined in Section 6.
</bodyText>
<sectionHeader confidence="0.997697" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.992695066666667">
The largest improvements in word error rate (WER)
have been seen with n-best list rescoring. The best
n hypotheses of a simple speech recognizer are pro-
cessed by a more sophisticated language model and
re-ranked. This method is algorithmically simpler
than parsing lattices, as one can use a model de-
veloped for strings, which need not operate strictly
*
left to right. However, we confirm the observa-
tion of (Ravishankar, 1997; Hall and Johnson, 2003)
that parsing word lattices saves computation time by
only parsing common substrings once.
Chelba (2000) reports WER reduction by rescor-
ing word lattices with scores of a structured lan-
guage model (Chelba and Jelinek, 2000), interpo-
lated with trigram scores. Word predictions of the
structured language model are conditioned on the
two previous phrasal heads not yet contained in a
bigger constituent. This is a computationally inten-
sive process, as the dependencies considered can be
of arbitrarily long distances. All possible sentence
prefixes are considered at each extension step.
Roark (2001) reports on the use of a lexical-
ized probabilistic top-down parser for word lattices,
evaluated both on parse accuracy and WER. Our
work is different from Roark (2001) in that we use
a bottom-up parsing algorithm with dynamic pro-
gramming based on the parsing model II of Collins
(1999).
Bottom-up chart parsing, through various forms
of extensions to the CKY algorithm, has been ap-
plied to word lattices for speech recognition (Hall
and Johnson, 2003; Chappelier and Rajman, 1998;
Chelba and Jelinek, 2000). Full acoustic and n-best
lattices filtered by trigram scores have been parsed.
Hall and Johnson (2003) use a best-first probabilis-
tic context free grammar (PCFG) to parse the input
lattice, pruning to a set of local trees (candidate par-
tial parse trees), which are then passed to a version
of the parser of Charniak (2001) for more refined
parsing. Unlike (Roark, 2001; Chelba, 2000), Hall
and Johnson (2003) achieve improvement in WER
over the trigram model without interpolating its lat-
tice parser probabilities directly with trigram prob-
abilities.
</bodyText>
<sectionHeader confidence="0.951535" genericHeader="method">
3 Word Lattice Parser
</sectionHeader>
<bodyText confidence="0.999471363636364">
Parsing models based on headword dependency re-
lationships have been reported, such as the struc-
tured language model of Chelba and Jelinek (2000).
These models use much less conditioning informa-
tion than the parsing models of Collins (1999), and
do not provide Penn Treebank format parse trees as
output. In this section we outline the adaptation of
the Collins (1999) parsing model to word lattices.
The intended action of the parser is illustrated
in Figure 1, which shows parse trees built directly
upon a word lattice.
</bodyText>
<subsectionHeader confidence="0.995609">
3.1 Parameterization
</subsectionHeader>
<bodyText confidence="0.9948535">
The parameterization of model II of Collins (1999)
is used in our word lattice parser. Parameters are
</bodyText>
<figure confidence="0.8635006">
S S*
NP NP VP
NN IN CC NNP AUX IN DT NN MD VB
rise
arise
</figure>
<figureCaption confidence="0.995521">
Figure 1: Example of a partially-parsed word lat-
tice. Different paths through the lattice are simul-
taneously parsed. The example shows two final
parses, one of low probability (S ) and one of high
probability (S).
</figureCaption>
<bodyText confidence="0.999425157894737">
maximum likelihood estimates of conditional prob-
abilities — the probability of some event of inter-
est (e.g., a left-modifier attachment) given a con-
text (e.g., parent non-terminal, distance, headword).
One notable difference between the word lattice
parser and the original implementation of Collins
(1999) is the handling of part-of-speech (POS) tag-
ging of unknown words (words seen fewer than 5
times in training). The conditioning context of the
parsing model parameters includes POS tagging.
Collins (1999) falls back to the POS tagging of Rat-
naparkhi (1996) for words seen fewer than 5 times
in the training corpus. As the tagger of Ratnaparkhi
(1996) cannot tag a word lattice, we cannot back off
to this tagging. We rely on the tag assigned by the
parsing model in all cases.
Edges created by the bottom-up parsing are as-
signed a score which is the product of the inside and
outside probabilities of the Collins (1999) model.
</bodyText>
<subsectionHeader confidence="0.999825">
3.2 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.97522524137931">
The algorithm is a variation of probabilistic
online, bottom-up, left-to-right Cocke-Kasami-
Younger parsing similar to Chappelier and Rajman
(1998).
Our parser produces trees (bottom-up) in a right-
branching manner, using unary extension and binary
adjunction. Starting with a proposed headword, left
modifiers are added first using right-branching, then
right modifiers using left-branching.
Word lattice edges are iteratively added to the
agenda. Complete closure is carried out, and the
next word edge is added to the agenda. This process
is repeated until all word edges are read from the
* speculation in tokyo was that the yen could
and
the
unit
lattice, and at least one complete parse is found.
Edges are each assigned a score, used to rank
parse candidates. For parsing of strings, the score
for a chart edge is the product of the scores of any
child edges and the score for the creation of the new
edge, as given by the model parameters. This score,
defined solely by the parsing model, will be referred
to as the parser score. The total score for chart
edges for the lattice parsing task is a combination
of the parser score, an acoustic model score, and a
trigram model score. Scaling factors follow those of
(Chelba and Jelinek, 2000; Roark, 2001).
</bodyText>
<subsectionHeader confidence="0.999795">
3.3 Smoothing and Pruning
</subsectionHeader>
<bodyText confidence="0.999961972972973">
The parameter estimation techniques (smoothing
and back-off) of Collins (1999) are reimplemented.
Additional techniques are required to prune the
search space of possible parses, due to the com-
plexity of the parsing algorithm and the size of the
word lattices. The main technique we employ is a
variation of the beam search of Collins (1999) to
restrict the chart size by excluding low probability
edges. The total score (combined acoustic and lan-
guage model scores) of candidate edges are com-
pared against edge with the same span and cate-
gory. Proposed edges with score outside the beam
are not added to the chart. The drawback to this
process is that we can no longer guarantee that a
model-optimal solution will be found. In practice,
these heuristics have a negative effect on parse accu-
racy, but the amount of pruning can be tuned to bal-
ance relative time and space savings against preci-
sion and recall degradation (Collins, 1999). Collins
(1999) uses a fixed size beam (10 000). We exper-
iment with several variable beam (ˆb) sizes, where
the beam is some function of a base beam (b) and
the edge width (the number of terminals dominated
by an edge). The base beam starts at a low beam
size and increases iteratively by a specified incre-
ment if no parse is found. This allows parsing to
operate quickly (with a minimal number of edges
added to the chart). However, if many iterations
are required to obtain a parse, the utility of starting
with a low beam and iterating becomes questionable
(Goodman, 1997). The base beam is limited to con-
trol the increase in the chart size. The selection of
the base beam, beam increment, and variable beam
function is governed by the familiar speed/accuracy
trade-off.1 The variable beam function found to al-
low fast convergence with minimal loss of accuracy
is:
</bodyText>
<equation confidence="0.9176415">
b
bˆ (1)
</equation>
<bodyText confidence="0.437872">
log w 2 2
</bodyText>
<subsectionHeader confidence="0.536269">
1Details of the optimization can be found in Collins (2004).
</subsectionHeader>
<bodyText confidence="0.999909666666667">
Charniak et al. (1998) introduce overparsing as a
technique to improve parse accuracy by continuing
parsing after the first complete parse tree is found.
The technique is employed by Hall and Johnson
(2003) to ensure that early stages of parsing do not
strongly bias later stages. We adapt this idea to
a single stage process. Due to the restrictions of
beam search and thresholds, the first parse found by
the model may not be the model optimal parse (i.e.,
we cannot guarantee best-first search). We there-
fore employ a form of overparsing — once a com-
plete parse tree is found, we further extend the base
beam by the beam increment and parse again. We
continue this process as long as extending the beam
results in an improved best parse score.
</bodyText>
<sectionHeader confidence="0.942437" genericHeader="method">
4 Expanding the Measures of Success
</sectionHeader>
<bodyText confidence="0.999989625">
Given the task of simply generating a transcription
of speech, WER is a useful and direct way to mea-
sure language model quality for ASR. WER is the
count of incorrect words in hypothesis Wˆ per word
in the true string W. For measurement, we must as-
sume prior knowledge of W and the best alignment
of the reference and hypothesis strings.2 Errors are
categorized as insertions, deletions, or substitutions.
</bodyText>
<sectionHeader confidence="0.630649" genericHeader="method">
Word Error Rate
</sectionHeader>
<subsectionHeader confidence="0.929326">
100Insertions Substitutions Deletions
Total Words in Correct Transcript
</subsectionHeader>
<bodyText confidence="0.999956227272727">
It is important to note that most models — Mangu
et al. (2000) is an innovative exception — minimize
sentence error. Sentence error rate is the percent-
age of sentences for which the proposed utterance
has at least one error. Models (such as ours) which
optimize prediction of test sentences Wt, generated
by the source, minimize the sentence error. Thus
even though WER is useful practically, it is formally
not the appropriate measure for the commonly used
language models. Unfortunately, as a practical mea-
sure, sentence error rate is not as useful — it is not
as fine-grained as WER.
Perplexity is another measure of language model
quality, measurable independent of ASR perfor-
mance (Jelinek, 1997). Perplexity is related to the
entropy of the source model which the language
model attempts to estimate.
These measures, while informative, do not cap-
ture success of extraction of high-level information
from speech. Task-specific measures should be used
in tandem with extensional measures such as per-
plexity and WER. Roark (2002), when reviewing
</bodyText>
<equation confidence="0.862847">
2SCLITE (http://www.nist.gov/speech/
tools/) by NIST is the most commonly used alignment tool.
(2)
</equation>
<bodyText confidence="0.99998584">
parsing for speech recognition, discusses a mod-
elling trade-off between producing parse trees and
producing strings. Most models are evaluated ei-
ther with measures of success for parsing or for
word recognition, but rarely both. Parsing mod-
els are difficult to implement as word-predictive
language models due to their complexity. Gener-
ative random sampling is equally challenging, so
the parsing correlate of perplexity is not easy to
measure. Traditional (i.e., n-gram) language mod-
els do not produce parse trees, so parsing metrics
are not useful. However, Roark (2001) argues for
using parsing metrics, such as labelled precision
and recall,3 along with WER, for parsing applica-
tions in ASR. Weighted WER (Weber et al., 1997)
is also a useful measurement, as the most often
ill-recognized words are short, closed-class words,
which are not as important to speech understanding
as phrasal head words. We will adopt the testing
strategy of Roark (2001), but find that measurement
of parse accuracy and WER on the same data set is
not possible given currently available corpora. Use
of weighted WER and development of methods to
simultaneously measure WER and parse accuracy
remain a topic for future research.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.97692936">
The word lattice parser was evaluated with sev-
eral metrics — WER, labelled precision and recall,
crossing brackets, and time and space resource us-
age. Following Roark (2001), we conducted evalu-
ations using two experimental sets — strings and
word lattices. We optimized settings (thresholds,
variable beam function, base beam value) for pars-
ing using development test data consisting of strings
for which we have annotated parse trees.
The parsing accuracy for parsing word lattices
was not directly evaluated as we did not have an-
notated parse trees for comparison. Furthermore,
standard parsing measures such as labelled preci-
sion and recall are not directly applicable in cases
where the number of words differs between the pro-
posed parse tree and the gold standard. Results
show scores for parsing strings which are lower than
the original implementation of Collins (1999). The
WER scores for this, the first application of the
Collins (1999) model to parsing word lattices, are
comparable to other recent work in syntactic lan-
guage modelling, and better than a simple trigram
model trained on the same data.
3Parse trees are commonly scored with the PARSEVAL set
of metrics (Black et al., 1991).
</bodyText>
<subsectionHeader confidence="0.998251">
5.1 Parsing Strings
</subsectionHeader>
<bodyText confidence="0.999993107142857">
The lattice parser can parse strings by creating a
single-path lattice from the input (all word transi-
tions are assigned an input score of 1.0). The lat-
tice parser was trained on sections 02-21 of the Wall
Street Journal portion of the Penn Treebank (Tay-
lor et al., 2003) Development testing was carried
out on section 23 in order to select model thresh-
olds and variable beam functions. Final testing was
carried out on section 00, and the PARSEVAL mea-
sures (Black et al., 1991) were used to evaluate the
performance.
The scores for our experiments are lower than the
scores of the original implementation of model II
(Collins, 1999). This difference is likely due in part
to differences in POS tagging. Tag accuracy for our
model was 93.2%, whereas for the original imple-
mentation of Collins (1999), model II achieved tag
accuracy of 96.75%. In addition to different tagging
strategies for unknown words, mentioned above, we
restrict the tag-set considered by the parser for each
word to those suggested by a simple first-stage tag-
ger.4 By reducing the tag-set considered by the pars-
ing model, we reduce the search space and increase
the speed. However, the simple tagger used to nar-
row the search also introduces tagging error.
The utility of the overparsing extension can be
seen in Table 1. Each of the PARSEVAL measures
improves when overparsing is used.
</bodyText>
<subsectionHeader confidence="0.999329">
5.2 Parsing Lattices
</subsectionHeader>
<bodyText confidence="0.9999068">
The success of the parsing model as a language
model for speech recognition was measured both
by parsing accuracy (parsing strings with annotated
reference parses), and by WER. WER is measured
by parsing word lattices and comparing the sentence
yield of the highest scoring parse tree to the refer-
ence transcription (using NIST SCLITE for align-
ment and error calculation).5 We assume the pars-
ing performance achieved by parsing strings carries
over approximately to parsing word lattices.
Two different corpora were used in training the
parsing model on word lattices:
sections 02-21 of the WSJ Penn Treebank (the
same sections as used to train the model for
parsing strings) [1 million words]
</bodyText>
<footnote confidence="0.969232888888889">
4The original implementation (Collins, 1999) of this model
considered all tags for all words.
5To properly model language using a parser, one should sum
parse tree scores for each sentence hypothesis, and choose the
sentence with the best sum of parse tree scores. We choose the
yield of the parse tree with the highest score. Summation is too
computationally expensive given the model —we do not even
generate all possible parse trees, but instead restrict generation
using dynamic programming.
</footnote>
<table confidence="0.99903125">
Exp. OP LP (%) LR (%) CB 0 CB (%) 2 CB (%)
Ref N 88.7 89.0 0.95 65.7 85.6
1 N 79.4 80.6 1.89 46.2 74.5
2 Y 80.8 81.4 1.70 44.3 80.4
</table>
<tableCaption confidence="0.993846">
Table 1: Results for parsing section 0 ( 40 words) of the WSJ Penn Treebank: OP = overparsing, LP/LR
</tableCaption>
<bodyText confidence="0.993185209302326">
= labelled precision/recall. CB is the average number of crossing brackets per sentence. 0 CB, 2 CB are
the percentage of sentences with 0 or 2 crossing brackets respectively. Ref is model II of (Collins, 1999).
section “1987” of the BLLIP corpus (Charniak
et al., 1999) [20 million words]
The BLLIP corpus is a collection of Penn
Treebank-style parses of the three-year (1987-1989)
Wall Street Journal collection from the ACL/DCI
corpus (approximately 30 million words).6 The
parses were automatically produced by the parser
of Charniak (2001). As the memory usage of our
model corresponds directly to the amount of train-
ing data used, we were restricted by available mem-
ory to use only one section (1987) of the total cor-
pus. Using the BLLIP corpus, we expected to get
lower quality parse results due to the higher parse
error of the corpus, when compared to the manually
annotated Penn Treebank. The WER was expected
to improve, as the BLLIP corpus has much greater
lexical coverage.
The training corpora were modified using a utility
by Brian Roark to convert newspaper text to speech-
like text, before being used as training input to the
model. Specifically, all numbers were converted to
words (60 sixty) and all punctuation was re-
moved.
We tested the performance of our parser on the
word lattices from the NIST HUB-1 evaluation task
of 1993. The lattices are derived from a set of
utterances produced from Wall Street Journal text
— the same domain as the Penn Treebank and the
BLLIP training data. The word lattices were previ-
ously pruned to the 50-best paths by Brian Roark,
using the A* decoding of Chelba (2000). The word
lattices of the HUB-1 corpus are directed acyclic
graphs in the HTK Standard Lattice Format (SLF),
consisting of a set of vertices and a set of edges.
Vertices, or nodes, are defined by a time-stamp and
labelled with a word. The set of labelled, weighted
edges, represents the word utterances. A word w is
hypothesized over edge e if e ends at a vertex v la-
belled w. Edges are associated with transition prob-
abilities and are labelled with an acoustic score and
a language model score. The lattices of the HUB-
</bodyText>
<footnote confidence="0.97404">
6The sentences of the HUB-1 corpus are a subset of those
in BLLIP. We removed all HUB-1 sentences from the BLLIP
corpus used in training.
</footnote>
<bodyText confidence="0.984617981132076">
1 corpus are annotated with trigram scores trained
using a 20 thousand word vocabulary and 40 mil-
lion word training sample. The word lattices have a
unique start and end point, and each complete path
through a lattice represents an utterance hypothesis.
As the parser operates in a left-to-right manner, and
closure is performed at each node, the input lattice
edges must be processed in topological order. Input
lattices were sorted before parsing. This corpus has
been used in other work on syntactic language mod-
elling (Chelba, 2000; Roark, 2001; Hall and John-
son, 2003).
The word lattices of the HUB-1 corpus are anno-
tated with an acoustic score, a, and a trigram proba-
bility, lm, for each edge. The input edge score stored
in the word lattice is:
log PZnpyd alog a 0log lm (3)
where a is the acoustic score and lm is the trigram
score stored in the lattice. The total edge weight in
the parser is a scaled combination of these scores
with the parser score derived with the model param-
eters:
log w alog a 0log lm s (4)
where w is the edge weight, and s is the score as-
signed by the parameters of the parsing model. We
optimized performance on a development subset of
test data, yielding a 1 16 and 0 1.
There is an important difference in the tokeniza-
tion of the HUB-1 corpus and the Penn Treebank
format. Clitics (i.e., he’s, wasn’t) are split
from their hosts in the Penn Treebank (i.e., he ’s,
was n’t), but not in the word lattices. The Tree-
bank format cannot easily be converted into the lat-
tice format, as often the two parts fall into different
parse constituents. We used the lattices modified by
Chelba (2000) in dealing with this problem — con-
tracted words are split into two parts and the edge
scores redistributed. We followed Hall and John-
son (2003) and used the Treebank tokenization for
measuring the WER. The model was tested with and
without overparsing.
We see from Table 2 that overparsing has little
effect on the WER. The word sequence most easily
parsed by the model (i.e., generating the first com-
plete parse tree) is likely also the word sequence
found by overparsing. Although overparsing may
have little effect on WER, we know from the exper-
iments on strings that overparsing increases parse
accuracy. This introduces a speed-accuracy trade-
off: depending on what type of output is required
from the model (parse trees or strings), the addi-
tional time and resource requirements of overpars-
ing may or may not be warranted.
</bodyText>
<subsectionHeader confidence="0.999854">
5.3 Parsing N-Best Lattices vs. N-Best Lists
</subsectionHeader>
<bodyText confidence="0.99999152631579">
The application of the model to 50-best word lat-
tices was compared to rescoring the 50-best paths
individually (50-best list parsing). The results are
presented in Table 2.
The cumulative number of edges added to the
chart per word for n-best lists is an order of mag-
nitude larger than for corresponding n-best lattices,
in all cases. As the WERs are similar, we conclude
that parsing n-best lists requires more work than
parsing n-best lattices, for the same result. There-
fore, parsing lattices is more efficient. This is be-
cause common substrings are only considered once
per lattice. The amount of computational savings is
dependent on the density of the lattices — for very
dense lattices, the equivalent n-best list parsing will
parse common substrings up to n times. In the limit
of lowest density, a lattice may have paths without
overlap, and the number of edges per word would
be the same for the lattice and lists.
</bodyText>
<subsectionHeader confidence="0.996735">
5.4 Time and Space Requirements
</subsectionHeader>
<bodyText confidence="0.999916">
The algorithms and data structures were designed to
minimize parameter lookup times and memory us-
age by the chart and parameter set (Collins, 2004).
To increase parameter lookup speed, all parameter
values are calculated for all levels of back-off at
training time. By contrast, (Collins, 1999) calcu-
lates parameter values by looking up event counts
at run-time. The implementation was then opti-
mized using a memory and processor profiler and
debugger. Parsing the complete set of HUB-1 lat-
tices (213 sentences, a total of 3,446 words) on av-
erage takes approximately 8 hours, on an Intel Pen-
tium 4 (1.6GHz) Linux system, using 1GB memory.
Memory requirements for parsing lattices is vastly
greater than equivalent parsing of a single sentence,
as chart size increases with the number of divergent
paths in a lattice. Additional analysis of resource
issues can be found in Collins (2004).
</bodyText>
<subsectionHeader confidence="0.997594">
5.5 Comparison to Previous Work
</subsectionHeader>
<bodyText confidence="0.99992034">
The results of our best experiments for lattice- and
list-parsing are compared with previous results in
Table 3. The oracle WER7 for the HUB-1 corpus
is 3.4%. For the pruned 50-best lattices, the oracle
WER is 7.8%. We see that by pruning the lattices
using the trigram model, we already introduce addi-
tional error. Because of the memory usage and time
required for parsing word lattices, we were unable
to test our model on the original “acoustic” HUB-1
lattices, and are thus limited by the oracle WER of
the 50-best lattices, and the bias introduced by prun-
ing using a trigram model. Where available, we also
present comparative scores of the sentence error rate
(SER) — the percentage of sentences in the test set
for which there was at least one recognition error.
Note that due to the small (213 samples) size of the
HUB-1 corpus, the differences seen in SER may not
be significant.
We see an improvement in WER for our pars-
ing model alone (a 0 0) trained on 1 million
words of the Penn Treebank compared to a trigram
model trained on the same data — the “Treebank
Trigram” noted in Table 3. This indicates that the
larger context considered by our model allows for
performance improvements over the trigram model
alone. Further improvement is seen with the com-
bination of acoustic, parsing, and trigram scores
(a 1 16 0 1). However, the combination of
the parsing model (trained on 1M words) with the
lattice trigram (trained on 40M words) resulted in
a higher WER than the lattice trigram alone. This
indicates that our 1M word training set is not suf-
ficient to permit effective combination with the lat-
tice trigram. When the training of the head-driven
parsing model was extended to the BLLIP 1987
corpus (20M words), the combination of models
(a 1 16 0 1) achieved additional improvement
in WER over the lattice trigram alone.
The current best-performing models, in terms of
WER, for the HUB-1 corpus, are the models of
Roark (2001), Charniak (2001) (applied to n-best
lists by Hall and Johnson (2003)), and the SLM of
Chelba and Jelinek (2000) (applied to n-best lists by
Xu et al. (2002)). However, n-best list parsing, as
seen in our evaluation, requires repeated analysis of
common subsequences, a less efficient process than
directly parsing the word lattice.
The reported results of (Roark, 2001) and
(Chelba, 2000) are for parsing models interpolated
with the lattice trigram probabilities. Hall and John-
</bodyText>
<footnote confidence="0.980357">
7The WER of the hypothesis which best matches the true
utterance, i.e., the lowest WER possible given the hypotheses
set.
</footnote>
<table confidence="0.999562454545455">
Training Size Lattice/List OP WER Number of Edges
(per word)
S D I T
1M Lattice N 10.4 3.3 1.5 15.2 1788
1M List N 10.4 3.2 1.4 15.0 10211
1M Lattice Y 10.3 3.2 1.4 14.9 2855
1M List Y 10.2 3.2 1.4 14.8 16821
20M Lattice N 9.0 3.1 1.0 13.1 1735
20M List N 9.0 3.1 1.0 13.1 9999
20M Lattice Y 9.0 3.1 1.0 13.1 2801
20M List Y 9.0 3.3 0.9 13.3 16030
</table>
<tableCaption confidence="0.897519666666667">
Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%),
D = deletions (%), I = insertions (%), T = total WER (%). Variable beam function: bˆ b log w 2 2 .
Training corpora: 1M = Penn Treebank sections 02-21; 20M = BLLIP section 1987.
</tableCaption>
<table confidence="0.999954875">
Model n-best List/Lattice Training Size WER (%) SER (%)
Oracle (50-best lattice) Lattice 7.8
Charniak (2001) List 40M 11.9
Xu (2002) List 20M 12.3
Roark (2001) (with EM) List 2M 12.7
Hall (2003) Lattice 30M 13.0
Chelba (2000) Lattice 20M 13.0
Current (α 1 16 β 1) List 20M 13.1 71.0
Current (α 1 16 β 1) Lattice 20M 13.1 70.4
Roark (2001) (no EM) List 1M 13.4
Lattice Trigram Lattice 40M 13.7 69.0
Current (α 1 16 β 1) List 1M 14.8 74.3
Current (α 1 16 β 1) Lattice 1M 14.9 74.0
Current (α β 0) Lattice 1M 16.0 75.5
Treebank Trigram Lattice 1M 16.5 79.8
No language model Lattice 16.8 84.0
</table>
<tableCaption confidence="0.998931">
Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER =
</tableCaption>
<bodyText confidence="0.996077913043478">
sentence error rate. WER = word error rate. “Speech-like” transformations were applied to all training
corpora. Xu (2002) is an implementation of the model of Chelba (2000) for n-best list parsing. Hall (2003)
is a lattice-parser related to Charniak (2001).
son (2003) does not use the lattice trigram scores
directly. However, as in other works, the lattice
trigram is used to prune the acoustic lattice to the
50 best paths. The difference in WER between
our parser and those of Charniak (2001) and Roark
(2001) applied to word lists may be due in part to the
lower PARSEVAL scores of our system. Xu et al.
(2002) report inverse correlation between labelled
precision/recall and WER. We achieve 73.2/76.5%
LP/LR on section 23 of the Penn Treebank, com-
pared to 82.9/82.4% LP/LR of Roark (2001) and
90.1/90.1% LP/LR of Charniak (2000). Another
contributing factor to the accuracy of Charniak
(2001) is the size of the training set — 20M words
larger than that used in this work. The low WER
of Roark (2001), a top-down probabilistic parsing
model, was achieved by training the model on 1 mil-
lion words of the Penn Treebank, then performing a
single pass of Expectation Maximization (EM) on a
further 1.2 million words.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999982">
In this work we present an adaptation of the parsing
model of Collins (1999) for application to ASR. The
system was evaluated over two sets of data: strings
and word lattices. As PARSEVAL measures are not
applicable to word lattices, we measured the pars-
ing accuracy using string input. The resulting scores
were lower than that original implementation of the
model. Despite this, the model was successful as a
language model for speech recognition, as measured
by WER and ability to extract high-level informa-
tion. Here, the system performs better than a simple
n-gram model trained on the same data, while si-
multaneously providing syntactic information in the
form of parse trees. WER scores are comparable to
related works in this area.
The large size of the parameter set of this parsing
model necessarily restricts the size of training data
that may be used. In addition, the resource require-
ments currently present a challenge for scaling up
from the relatively sparse word lattices of the NIST
HUB-1 corpus (created in a lab setting by profes-
sional readers) to lattices created with spontaneous
speech in non-ideal conditions. An investigation
into the relevant importance of each parameter for
the speech recognition task may allow a reduction in
the size of the parameter space, with minimal loss of
recognition accuracy. A speedup may be achieved,
and additional training data could be used. Tun-
ing of parameters using EM has lead to improved
WER for other models. We encourage investigation
of this technique for lexicalized head-driven lattice
parsing.
</bodyText>
<sectionHeader confidence="0.99605" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99551275">
This research was funded in part by the Natural Sci-
ences and Engineering Research Council (NSERC)
of Canada. Advice on training and test data was
provided by Keith Hall of Brown University.
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999400445783133">
L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A maxi-
mum likelihood approach to continuous speech recog-
nition. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 5:179–190.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of Fourth DARPA
Speech and Natural Language Workshop, pages 306–
311.
J.-C. Chappelier and M. Rajman. 1998. A practical
bottom-up algorithm for on-line parsing with stochas-
tic context-free grammars. Technical Report 98-284,
Swiss Federal Institute of Technology, July.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-Based Best-First Chart Parsing. In
6th Annual Workshop for Very Large Corpora, pages
127–133.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 1999. BLLIP 1987-89
WSJ Corpus Release 1. Linguistic Data Consortium.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 2000 Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 132–129, New
Brunswick, U.S.A.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the ACL.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14:283–332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure
for Natural Language Modeling. Ph.D. thesis, Johns
Hopkins University.
Christopher Collins. 2004. Head-Driven Probabilistic
Parsingfor Word Lattices. M.Sc. thesis, University of
Toronto.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the 2nd Con-
ference on Empirical Methods in Natural Language
Processing.
Keith Hall and Mark Johnson. 2003. Language mod-
eling using efficient best-first bottom-up parsing. In
Proceedings of the IEEE Automatic Speech Recogni-
tion and Understanding Workshop.
Frederick Jelinek. 1997. Information Extraction From
Speech And Text. MIT Press.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373–
400.
Hwee Tou Ng and John Zelle. 1997. Corpus-based
approaches to semantic interpretation in natural lan-
guage processing. AI Magazine, 18:45–54.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Conference on Empirical
Methods in Natural Language Processing, May.
Mosur K. Ravishankar. 1997. Some results on search
complexity vs accuracy. In DARPA Speech Recogni-
tion Workshop, pages 104–107, February.
Brian Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models, and Applica-
tions. Ph.D. thesis, Brown University.
Brian Roark. 2002. Markov parsing: Lattice rescoring
with a statistical parser. In Proceedings of the 40th
Annual Meeting of the ACL, pages 287–294.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini,
2003. The Penn TreeBank: An Overview, chapter 1.
Kluwer, Dordrecht, The Netherlands.
Hans Weber, J¨org Spilker, and G¨unther G¨orz. 1997.
Parsing n best trees from a word lattice. Kunstliche
Intelligenz, pages 279–288.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002.
A study on richer syntactic dependencies in structured
language modeling. In Proceedings of the 40th An-
nual Meeting of the ACL, pages 191–198.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.922523">
<title confidence="0.999951">Head-Driven Parsing for Word Lattices</title>
<author confidence="0.999757">Bob Carpenter</author>
<affiliation confidence="0.984361">Alias I, Inc.</affiliation>
<address confidence="0.999723">Brooklyn, NY, USA</address>
<email confidence="0.999792">carp@alias-i.com</email>
<author confidence="0.998268">Christopher Collins</author>
<affiliation confidence="0.9999815">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.999866">Toronto, ON, Canada</address>
<email confidence="0.996271">ccollins@cs.utoronto.ca</email>
<author confidence="0.999499">Gerald Penn</author>
<affiliation confidence="0.999974">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.999874">Toronto, ON, Canada</address>
<email confidence="0.998722">gpenn@cs.utoronto.ca</email>
<abstract confidence="0.9965424375">We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for largevocabulary speech recognition. The model is adapted to an online left to right chart-parser for lattices, integrating acoustic, and parser probabilities. The parser uses structural lexical dependencies not considered by gram models, conditioning recognition on more linguistically-grounded relationships. Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the language model while extracting additional structural information useful for speech understanding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>5--179</pages>
<contexts>
<context position="1507" citStr="Bahl et al. (1983)" startWordPosition="208" endWordPosition="211">rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding. 1 Introduction The question of how to integrate high-level knowledge representations of language with automatic speech recognition (ASR) is becoming more important as (1) speech recognition technology matures, (2) the rate of improvement of recognition accuracy decreases, and (3) the need for additional information (beyond simple transcriptions) becomes evident. Most of the currently best ASR systems use an n-gram language model of the type pioneered by Bahl et al. (1983). Recently, research has begun to show progress towards application of new and better models of spoken language (Hall and Johnson, 2003; Roark, 2001; Chelba and Jelinek, 2000). Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition, extracting high-level structure from speech, while simultaneously selecting the best path in a word lattice. Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing (Ng and Zelle, 1997). Collins (1999) presents three lexicalized models whic</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5:179–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proceedings of Fourth DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="15830" citStr="Black et al., 1991" startWordPosition="2555" endWordPosition="2558">standard parsing measures such as labelled precision and recall are not directly applicable in cases where the number of words differs between the proposed parse tree and the gold standard. Results show scores for parsing strings which are lower than the original implementation of Collins (1999). The WER scores for this, the first application of the Collins (1999) model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data. 3Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al., 1991). 5.1 Parsing Strings The lattice parser can parse strings by creating a single-path lattice from the input (all word transitions are assigned an input score of 1.0). The lattice parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Taylor et al., 2003) Development testing was carried out on section 23 in order to select model thresholds and variable beam functions. Final testing was carried out on section 00, and the PARSEVAL measures (Black et al., 1991) were used to evaluate the performance. The scores for our experiments are lower than the scores of </context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proceedings of Fourth DARPA Speech and Natural Language Workshop, pages 306– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Chappelier</author>
<author>M Rajman</author>
</authors>
<title>A practical bottom-up algorithm for on-line parsing with stochastic context-free grammars.</title>
<date>1998</date>
<tech>Technical Report 98-284,</tech>
<institution>Swiss Federal Institute of Technology,</institution>
<contexts>
<context position="5337" citStr="Chappelier and Rajman, 1998" startWordPosition="815" endWordPosition="818">the dependencies considered can be of arbitrarily long distances. All possible sentence prefixes are considered at each extension step. Roark (2001) reports on the use of a lexicalized probabilistic top-down parser for word lattices, evaluated both on parse accuracy and WER. Our work is different from Roark (2001) in that we use a bottom-up parsing algorithm with dynamic programming based on the parsing model II of Collins (1999). Bottom-up chart parsing, through various forms of extensions to the CKY algorithm, has been applied to word lattices for speech recognition (Hall and Johnson, 2003; Chappelier and Rajman, 1998; Chelba and Jelinek, 2000). Full acoustic and n-best lattices filtered by trigram scores have been parsed. Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. Unlike (Roark, 2001; Chelba, 2000), Hall and Johnson (2003) achieve improvement in WER over the trigram model without interpolating its lattice parser probabilities directly with trigram probabilities. 3 Word Lattice Parse</context>
<context position="7956" citStr="Chappelier and Rajman (1998)" startWordPosition="1245" endWordPosition="1248"> includes POS tagging. Collins (1999) falls back to the POS tagging of Ratnaparkhi (1996) for words seen fewer than 5 times in the training corpus. As the tagger of Ratnaparkhi (1996) cannot tag a word lattice, we cannot back off to this tagging. We rely on the tag assigned by the parsing model in all cases. Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the Collins (1999) model. 3.2 Parsing Algorithm The algorithm is a variation of probabilistic online, bottom-up, left-to-right Cocke-KasamiYounger parsing similar to Chappelier and Rajman (1998). Our parser produces trees (bottom-up) in a rightbranching manner, using unary extension and binary adjunction. Starting with a proposed headword, left modifiers are added first using right-branching, then right modifiers using left-branching. Word lattice edges are iteratively added to the agenda. Complete closure is carried out, and the next word edge is added to the agenda. This process is repeated until all word edges are read from the * speculation in tokyo was that the yen could and the unit lattice, and at least one complete parse is found. Edges are each assigned a score, used to rank</context>
</contexts>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>J.-C. Chappelier and M. Rajman. 1998. A practical bottom-up algorithm for on-line parsing with stochastic context-free grammars. Technical Report 98-284, Swiss Federal Institute of Technology, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>Edge-Based Best-First Chart Parsing.</title>
<date>1998</date>
<booktitle>In 6th Annual Workshop for Very Large Corpora,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="11002" citStr="Charniak et al. (1998)" startWordPosition="1771" endWordPosition="1774">sing to operate quickly (with a minimal number of edges added to the chart). However, if many iterations are required to obtain a parse, the utility of starting with a low beam and iterating becomes questionable (Goodman, 1997). The base beam is limited to control the increase in the chart size. The selection of the base beam, beam increment, and variable beam function is governed by the familiar speed/accuracy trade-off.1 The variable beam function found to allow fast convergence with minimal loss of accuracy is: b bˆ (1) log w 2 2 1Details of the optimization can be found in Collins (2004). Charniak et al. (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. The technique is employed by Hall and Johnson (2003) to ensure that early stages of parsing do not strongly bias later stages. We adapt this idea to a single stage process. Due to the restrictions of beam search and thresholds, the first parse found by the model may not be the model optimal parse (i.e., we cannot guarantee best-first search). We therefore employ a form of overparsing — once a complete parse tree is found, we further extend the base beam by the bea</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Eugene Charniak, Sharon Goldwater, and Mark Johnson. 1998. Edge-Based Best-First Chart Parsing. In 6th Annual Workshop for Very Large Corpora, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Don Blaheta</author>
<author>Niyu Ge</author>
<author>Keith Hall</author>
<author>John Hale</author>
<author>Mark Johnson</author>
</authors>
<date>1999</date>
<booktitle>BLLIP 1987-89 WSJ Corpus Release 1. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="18926" citStr="Charniak et al., 1999" startWordPosition="3082" endWordPosition="3085">he model —we do not even generate all possible parse trees, but instead restrict generation using dynamic programming. Exp. OP LP (%) LR (%) CB 0 CB (%) 2 CB (%) Ref N 88.7 89.0 0.95 65.7 85.6 1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1: Results for parsing section 0 ( 40 words) of the WSJ Penn Treebank: OP = overparsing, LP/LR = labelled precision/recall. CB is the average number of crossing brackets per sentence. 0 CB, 2 CB are the percentage of sentences with 0 or 2 crossing brackets respectively. Ref is model II of (Collins, 1999). section “1987” of the BLLIP corpus (Charniak et al., 1999) [20 million words] The BLLIP corpus is a collection of Penn Treebank-style parses of the three-year (1987-1989) Wall Street Journal collection from the ACL/DCI corpus (approximately 30 million words).6 The parses were automatically produced by the parser of Charniak (2001). As the memory usage of our model corresponds directly to the amount of training data used, we were restricted by available memory to use only one section (1987) of the total corpus. Using the BLLIP corpus, we expected to get lower quality parse results due to the higher parse error of the corpus, when compared to the manua</context>
</contexts>
<marker>Charniak, Blaheta, Ge, Hall, Hale, Johnson, 1999</marker>
<rawString>Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, John Hale, and Mark Johnson. 1999. BLLIP 1987-89 WSJ Corpus Release 1. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--129</pages>
<location>New Brunswick, U.S.A.</location>
<contexts>
<context position="30010" citStr="Charniak (2000)" startWordPosition="5023" endWordPosition="5024">ice-parser related to Charniak (2001). son (2003) does not use the lattice trigram scores directly. However, as in other works, the lattice trigram is used to prune the acoustic lattice to the 50 best paths. The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system. Xu et al. (2002) report inverse correlation between labelled precision/recall and WER. We achieve 73.2/76.5% LP/LR on section 23 of the Penn Treebank, compared to 82.9/82.4% LP/LR of Roark (2001) and 90.1/90.1% LP/LR of Charniak (2000). Another contributing factor to the accuracy of Charniak (2001) is the size of the training set — 20M words larger than that used in this work. The low WER of Roark (2001), a top-down probabilistic parsing model, was achieved by training the model on 1 million words of the Penn Treebank, then performing a single pass of Expectation Maximization (EM) on a further 1.2 million words. 6 Conclusions In this work we present an adaptation of the parsing model of Collins (1999) for application to ASR. The system was evaluated over two sets of data: strings and word lattices. As PARSEVAL measures are </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computational Linguistics, pages 132–129, New Brunswick, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="5688" citStr="Charniak (2001)" startWordPosition="876" endWordPosition="877">th dynamic programming based on the parsing model II of Collins (1999). Bottom-up chart parsing, through various forms of extensions to the CKY algorithm, has been applied to word lattices for speech recognition (Hall and Johnson, 2003; Chappelier and Rajman, 1998; Chelba and Jelinek, 2000). Full acoustic and n-best lattices filtered by trigram scores have been parsed. Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. Unlike (Roark, 2001; Chelba, 2000), Hall and Johnson (2003) achieve improvement in WER over the trigram model without interpolating its lattice parser probabilities directly with trigram probabilities. 3 Word Lattice Parser Parsing models based on headword dependency relationships have been reported, such as the structured language model of Chelba and Jelinek (2000). These models use much less conditioning information than the parsing models of Collins (1999), and do not provide Penn Treebank format parse trees as output. In this section we outline the adaptation of </context>
<context position="19200" citStr="Charniak (2001)" startWordPosition="3124" endWordPosition="3125">ection 0 ( 40 words) of the WSJ Penn Treebank: OP = overparsing, LP/LR = labelled precision/recall. CB is the average number of crossing brackets per sentence. 0 CB, 2 CB are the percentage of sentences with 0 or 2 crossing brackets respectively. Ref is model II of (Collins, 1999). section “1987” of the BLLIP corpus (Charniak et al., 1999) [20 million words] The BLLIP corpus is a collection of Penn Treebank-style parses of the three-year (1987-1989) Wall Street Journal collection from the ACL/DCI corpus (approximately 30 million words).6 The parses were automatically produced by the parser of Charniak (2001). As the memory usage of our model corresponds directly to the amount of training data used, we were restricted by available memory to use only one section (1987) of the total corpus. Using the BLLIP corpus, we expected to get lower quality parse results due to the higher parse error of the corpus, when compared to the manually annotated Penn Treebank. The WER was expected to improve, as the BLLIP corpus has much greater lexical coverage. The training corpora were modified using a utility by Brian Roark to convert newspaper text to speechlike text, before being used as training input to the mo</context>
<context position="27268" citStr="Charniak (2001)" startWordPosition="4527" endWordPosition="4528">combination of the parsing model (trained on 1M words) with the lattice trigram (trained on 40M words) resulted in a higher WER than the lattice trigram alone. This indicates that our 1M word training set is not sufficient to permit effective combination with the lattice trigram. When the training of the head-driven parsing model was extended to the BLLIP 1987 corpus (20M words), the combination of models (a 1 16 0 1) achieved additional improvement in WER over the lattice trigram alone. The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al. (2002)). However, n-best list parsing, as seen in our evaluation, requires repeated analysis of common subsequences, a less efficient process than directly parsing the word lattice. The reported results of (Roark, 2001) and (Chelba, 2000) are for parsing models interpolated with the lattice trigram probabilities. Hall and John7The WER of the hypothesis which best matches the true utterance, i.e., the lowest WER possible given the hypotheses set. Training Size La</context>
<context position="28594" citStr="Charniak (2001)" startWordPosition="4768" endWordPosition="4769"> 15.0 10211 1M Lattice Y 10.3 3.2 1.4 14.9 2855 1M List Y 10.2 3.2 1.4 14.8 16821 20M Lattice N 9.0 3.1 1.0 13.1 1735 20M List N 9.0 3.1 1.0 13.1 9999 20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%), D = deletions (%), I = insertions (%), T = total WER (%). Variable beam function: bˆ b log w 2 2 . Training corpora: 1M = Penn Treebank sections 02-21; 20M = BLLIP section 1987. Model n-best List/Lattice Training Size WER (%) SER (%) Oracle (50-best lattice) Lattice 7.8 Charniak (2001) List 40M 11.9 Xu (2002) List 20M 12.3 Roark (2001) (with EM) List 2M 12.7 Hall (2003) Lattice 30M 13.0 Chelba (2000) Lattice 20M 13.0 Current (α 1 16 β 1) List 20M 13.1 71.0 Current (α 1 16 β 1) Lattice 20M 13.1 70.4 Roark (2001) (no EM) List 1M 13.4 Lattice Trigram Lattice 40M 13.7 69.0 Current (α 1 16 β 1) List 1M 14.8 74.3 Current (α 1 16 β 1) Lattice 1M 14.9 74.0 Current (α β 0) Lattice 1M 16.0 75.5 Treebank Trigram Lattice 1M 16.5 79.8 No language model Lattice 16.8 84.0 Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER = sentence error rat</context>
<context position="30074" citStr="Charniak (2001)" startWordPosition="5032" endWordPosition="5033">he lattice trigram scores directly. However, as in other works, the lattice trigram is used to prune the acoustic lattice to the 50 best paths. The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system. Xu et al. (2002) report inverse correlation between labelled precision/recall and WER. We achieve 73.2/76.5% LP/LR on section 23 of the Penn Treebank, compared to 82.9/82.4% LP/LR of Roark (2001) and 90.1/90.1% LP/LR of Charniak (2000). Another contributing factor to the accuracy of Charniak (2001) is the size of the training set — 20M words larger than that used in this work. The low WER of Roark (2001), a top-down probabilistic parsing model, was achieved by training the model on 1 million words of the Penn Treebank, then performing a single pass of Expectation Maximization (EM) on a further 1.2 million words. 6 Conclusions In this work we present an adaptation of the parsing model of Collins (1999) for application to ASR. The system was evaluated over two sets of data: strings and word lattices. As PARSEVAL measures are not applicable to word lattices, we measured the parsing accurac</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<date>2000</date>
<booktitle>Structured language modeling. Computer Speech and Language,</booktitle>
<pages>14--283</pages>
<contexts>
<context position="1682" citStr="Chelba and Jelinek, 2000" startWordPosition="236" endWordPosition="239">n of how to integrate high-level knowledge representations of language with automatic speech recognition (ASR) is becoming more important as (1) speech recognition technology matures, (2) the rate of improvement of recognition accuracy decreases, and (3) the need for additional information (beyond simple transcriptions) becomes evident. Most of the currently best ASR systems use an n-gram language model of the type pioneered by Bahl et al. (1983). Recently, research has begun to show progress towards application of new and better models of spoken language (Hall and Johnson, 2003; Roark, 2001; Chelba and Jelinek, 2000). Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition, extracting high-level structure from speech, while simultaneously selecting the best path in a word lattice. Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing (Ng and Zelle, 1997). Collins (1999) presents three lexicalized models which consider long-distance dependencies within a sentence. Grammar productions are conditioned on headwords. The conditioning context is thus more focused than that of a large n</context>
<context position="4483" citStr="Chelba and Jelinek, 2000" startWordPosition="680" endWordPosition="683">have been seen with n-best list rescoring. The best n hypotheses of a simple speech recognizer are processed by a more sophisticated language model and re-ranked. This method is algorithmically simpler than parsing lattices, as one can use a model developed for strings, which need not operate strictly * left to right. However, we confirm the observation of (Ravishankar, 1997; Hall and Johnson, 2003) that parsing word lattices saves computation time by only parsing common substrings once. Chelba (2000) reports WER reduction by rescoring word lattices with scores of a structured language model (Chelba and Jelinek, 2000), interpolated with trigram scores. Word predictions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent. This is a computationally intensive process, as the dependencies considered can be of arbitrarily long distances. All possible sentence prefixes are considered at each extension step. Roark (2001) reports on the use of a lexicalized probabilistic top-down parser for word lattices, evaluated both on parse accuracy and WER. Our work is different from Roark (2001) in that we use a bottom-up parsing algorithm with dynamic</context>
<context position="6083" citStr="Chelba and Jelinek (2000)" startWordPosition="934" endWordPosition="937">son (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. Unlike (Roark, 2001; Chelba, 2000), Hall and Johnson (2003) achieve improvement in WER over the trigram model without interpolating its lattice parser probabilities directly with trigram probabilities. 3 Word Lattice Parser Parsing models based on headword dependency relationships have been reported, such as the structured language model of Chelba and Jelinek (2000). These models use much less conditioning information than the parsing models of Collins (1999), and do not provide Penn Treebank format parse trees as output. In this section we outline the adaptation of the Collins (1999) parsing model to word lattices. The intended action of the parser is illustrated in Figure 1, which shows parse trees built directly upon a word lattice. 3.1 Parameterization The parameterization of model II of Collins (1999) is used in our word lattice parser. Parameters are S S* NP NP VP NN IN CC NNP AUX IN DT NN MD VB rise arise Figure 1: Example of a partially-parsed wo</context>
<context position="9054" citStr="Chelba and Jelinek, 2000" startWordPosition="1434" endWordPosition="1437">at the yen could and the unit lattice, and at least one complete parse is found. Edges are each assigned a score, used to rank parse candidates. For parsing of strings, the score for a chart edge is the product of the scores of any child edges and the score for the creation of the new edge, as given by the model parameters. This score, defined solely by the parsing model, will be referred to as the parser score. The total score for chart edges for the lattice parsing task is a combination of the parser score, an acoustic model score, and a trigram model score. Scaling factors follow those of (Chelba and Jelinek, 2000; Roark, 2001). 3.3 Smoothing and Pruning The parameter estimation techniques (smoothing and back-off) of Collins (1999) are reimplemented. Additional techniques are required to prune the search space of possible parses, due to the complexity of the parsing algorithm and the size of the word lattices. The main technique we employ is a variation of the beam search of Collins (1999) to restrict the chart size by excluding low probability edges. The total score (combined acoustic and language model scores) of candidate edges are compared against edge with the same span and category. Proposed edge</context>
<context position="27363" citStr="Chelba and Jelinek (2000)" startWordPosition="4542" endWordPosition="4545">ned on 40M words) resulted in a higher WER than the lattice trigram alone. This indicates that our 1M word training set is not sufficient to permit effective combination with the lattice trigram. When the training of the head-driven parsing model was extended to the BLLIP 1987 corpus (20M words), the combination of models (a 1 16 0 1) achieved additional improvement in WER over the lattice trigram alone. The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al. (2002)). However, n-best list parsing, as seen in our evaluation, requires repeated analysis of common subsequences, a less efficient process than directly parsing the word lattice. The reported results of (Roark, 2001) and (Chelba, 2000) are for parsing models interpolated with the lattice trigram probabilities. Hall and John7The WER of the hypothesis which best matches the true utterance, i.e., the lowest WER possible given the hypotheses set. Training Size Lattice/List OP WER Number of Edges (per word) S D I T 1M Lattice N 10.4 3.3 1.5 15.2 1788 1M Lis</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14:283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="4364" citStr="Chelba (2000)" startWordPosition="662" endWordPosition="663">r future work are outlined in Section 6. 2 Previous Work The largest improvements in word error rate (WER) have been seen with n-best list rescoring. The best n hypotheses of a simple speech recognizer are processed by a more sophisticated language model and re-ranked. This method is algorithmically simpler than parsing lattices, as one can use a model developed for strings, which need not operate strictly * left to right. However, we confirm the observation of (Ravishankar, 1997; Hall and Johnson, 2003) that parsing word lattices saves computation time by only parsing common substrings once. Chelba (2000) reports WER reduction by rescoring word lattices with scores of a structured language model (Chelba and Jelinek, 2000), interpolated with trigram scores. Word predictions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent. This is a computationally intensive process, as the dependencies considered can be of arbitrarily long distances. All possible sentence prefixes are considered at each extension step. Roark (2001) reports on the use of a lexicalized probabilistic top-down parser for word lattices, evaluated both on pa</context>
<context position="5749" citStr="Chelba, 2000" startWordPosition="885" endWordPosition="886"> (1999). Bottom-up chart parsing, through various forms of extensions to the CKY algorithm, has been applied to word lattices for speech recognition (Hall and Johnson, 2003; Chappelier and Rajman, 1998; Chelba and Jelinek, 2000). Full acoustic and n-best lattices filtered by trigram scores have been parsed. Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. Unlike (Roark, 2001; Chelba, 2000), Hall and Johnson (2003) achieve improvement in WER over the trigram model without interpolating its lattice parser probabilities directly with trigram probabilities. 3 Word Lattice Parser Parsing models based on headword dependency relationships have been reported, such as the structured language model of Chelba and Jelinek (2000). These models use much less conditioning information than the parsing models of Collins (1999), and do not provide Penn Treebank format parse trees as output. In this section we outline the adaptation of the Collins (1999) parsing model to word lattices. The intend</context>
<context position="20278" citStr="Chelba (2000)" startWordPosition="3314" endWordPosition="3315">ere modified using a utility by Brian Roark to convert newspaper text to speechlike text, before being used as training input to the model. Specifically, all numbers were converted to words (60 sixty) and all punctuation was removed. We tested the performance of our parser on the word lattices from the NIST HUB-1 evaluation task of 1993. The lattices are derived from a set of utterances produced from Wall Street Journal text — the same domain as the Penn Treebank and the BLLIP training data. The word lattices were previously pruned to the 50-best paths by Brian Roark, using the A* decoding of Chelba (2000). The word lattices of the HUB-1 corpus are directed acyclic graphs in the HTK Standard Lattice Format (SLF), consisting of a set of vertices and a set of edges. Vertices, or nodes, are defined by a time-stamp and labelled with a word. The set of labelled, weighted edges, represents the word utterances. A word w is hypothesized over edge e if e ends at a vertex v labelled w. Edges are associated with transition probabilities and are labelled with an acoustic score and a language model score. The lattices of the HUB6The sentences of the HUB-1 corpus are a subset of those in BLLIP. We removed al</context>
<context position="22558" citStr="Chelba (2000)" startWordPosition="3727" endWordPosition="3728">0log lm s (4) where w is the edge weight, and s is the score assigned by the parameters of the parsing model. We optimized performance on a development subset of test data, yielding a 1 16 and 0 1. There is an important difference in the tokenization of the HUB-1 corpus and the Penn Treebank format. Clitics (i.e., he’s, wasn’t) are split from their hosts in the Penn Treebank (i.e., he ’s, was n’t), but not in the word lattices. The Treebank format cannot easily be converted into the lattice format, as often the two parts fall into different parse constituents. We used the lattices modified by Chelba (2000) in dealing with this problem — contracted words are split into two parts and the edge scores redistributed. We followed Hall and Johnson (2003) and used the Treebank tokenization for measuring the WER. The model was tested with and without overparsing. We see from Table 2 that overparsing has little effect on the WER. The word sequence most easily parsed by the model (i.e., generating the first complete parse tree) is likely also the word sequence found by overparsing. Although overparsing may have little effect on WER, we know from the experiments on strings that overparsing increases parse </context>
<context position="27640" citStr="Chelba, 2000" startWordPosition="4587" endWordPosition="4588"> words), the combination of models (a 1 16 0 1) achieved additional improvement in WER over the lattice trigram alone. The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al. (2002)). However, n-best list parsing, as seen in our evaluation, requires repeated analysis of common subsequences, a less efficient process than directly parsing the word lattice. The reported results of (Roark, 2001) and (Chelba, 2000) are for parsing models interpolated with the lattice trigram probabilities. Hall and John7The WER of the hypothesis which best matches the true utterance, i.e., the lowest WER possible given the hypotheses set. Training Size Lattice/List OP WER Number of Edges (per word) S D I T 1M Lattice N 10.4 3.3 1.5 15.2 1788 1M List N 10.4 3.2 1.4 15.0 10211 1M Lattice Y 10.3 3.2 1.4 14.9 2855 1M List Y 10.2 3.2 1.4 14.8 16821 20M Lattice N 9.0 3.1 1.0 13.1 1735 20M List N 9.0 3.1 1.0 13.1 9999 20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2: Results for parsing HUB-1 n-bes</context>
<context position="29348" citStr="Chelba (2000)" startWordPosition="4911" endWordPosition="4912">t (α 1 16 β 1) List 20M 13.1 71.0 Current (α 1 16 β 1) Lattice 20M 13.1 70.4 Roark (2001) (no EM) List 1M 13.4 Lattice Trigram Lattice 40M 13.7 69.0 Current (α 1 16 β 1) List 1M 14.8 74.3 Current (α 1 16 β 1) Lattice 1M 14.9 74.0 Current (α β 0) Lattice 1M 16.0 75.5 Treebank Trigram Lattice 1M 16.5 79.8 No language model Lattice 16.8 84.0 Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER = sentence error rate. WER = word error rate. “Speech-like” transformations were applied to all training corpora. Xu (2002) is an implementation of the model of Chelba (2000) for n-best list parsing. Hall (2003) is a lattice-parser related to Charniak (2001). son (2003) does not use the lattice trigram scores directly. However, as in other works, the lattice trigram is used to prune the acoustic lattice to the 50 best paths. The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system. Xu et al. (2002) report inverse correlation between labelled precision/recall and WER. We achieve 73.2/76.5% LP/LR on section 23 of the Penn Treebank, compared to 82.9/82.4%</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Ciprian Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Collins</author>
</authors>
<title>Head-Driven Probabilistic Parsingfor Word Lattices.</title>
<date>2004</date>
<tech>M.Sc. thesis,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="10978" citStr="Collins (2004)" startWordPosition="1769" endWordPosition="1770"> This allows parsing to operate quickly (with a minimal number of edges added to the chart). However, if many iterations are required to obtain a parse, the utility of starting with a low beam and iterating becomes questionable (Goodman, 1997). The base beam is limited to control the increase in the chart size. The selection of the base beam, beam increment, and variable beam function is governed by the familiar speed/accuracy trade-off.1 The variable beam function found to allow fast convergence with minimal loss of accuracy is: b bˆ (1) log w 2 2 1Details of the optimization can be found in Collins (2004). Charniak et al. (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. The technique is employed by Hall and Johnson (2003) to ensure that early stages of parsing do not strongly bias later stages. We adapt this idea to a single stage process. Due to the restrictions of beam search and thresholds, the first parse found by the model may not be the model optimal parse (i.e., we cannot guarantee best-first search). We therefore employ a form of overparsing — once a complete parse tree is found, we further extend </context>
<context position="24536" citStr="Collins, 2004" startWordPosition="4061" endWordPosition="4062">tices is more efficient. This is because common substrings are only considered once per lattice. The amount of computational savings is dependent on the density of the lattices — for very dense lattices, the equivalent n-best list parsing will parse common substrings up to n times. In the limit of lowest density, a lattice may have paths without overlap, and the number of edges per word would be the same for the lattice and lists. 5.4 Time and Space Requirements The algorithms and data structures were designed to minimize parameter lookup times and memory usage by the chart and parameter set (Collins, 2004). To increase parameter lookup speed, all parameter values are calculated for all levels of back-off at training time. By contrast, (Collins, 1999) calculates parameter values by looking up event counts at run-time. The implementation was then optimized using a memory and processor profiler and debugger. Parsing the complete set of HUB-1 lattices (213 sentences, a total of 3,446 words) on average takes approximately 8 hours, on an Intel Pentium 4 (1.6GHz) Linux system, using 1GB memory. Memory requirements for parsing lattices is vastly greater than equivalent parsing of a single sentence, as </context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Christopher Collins. 2004. Head-Driven Probabilistic Parsingfor Word Lattices. M.Sc. thesis, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2068" citStr="Collins (1999)" startWordPosition="295" endWordPosition="296">age model of the type pioneered by Bahl et al. (1983). Recently, research has begun to show progress towards application of new and better models of spoken language (Hall and Johnson, 2003; Roark, 2001; Chelba and Jelinek, 2000). Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition, extracting high-level structure from speech, while simultaneously selecting the best path in a word lattice. Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing (Ng and Zelle, 1997). Collins (1999) presents three lexicalized models which consider long-distance dependencies within a sentence. Grammar productions are conditioned on headwords. The conditioning context is thus more focused than that of a large n-gram covering the same span, so the sparse data problems arising from the sheer size of the parameter space are less pressing. However, sparse data problems arising from the limited availability of annotated training data become a problem. We test the head-driven statistical lattice parser with word lattices from the NIST HUB-1 corpus, which has been used by others in related work (</context>
<context position="3291" citStr="Collins (1999)" startWordPosition="488" endWordPosition="489">Johnson, 2003; Roark, 2001; Chelba and Jelinek, 2000). Parse accuracy and word error rates are reported. We present an analysis of the effects of pruning and heuristic search on efficiency and accuracy and note several simplifying assumptions common to other reported experiments in this area, which present challenges for scaling up to realworld applications. This work shows the importance of careful algorithm and data structure design and choice of dynamic programming constraints to the efficiency and accuracy of a head-driven probabilistic parser for speech. We find that the parsing model of Collins (1999) can be successfully adapted as a language model for speech recognition. In the following section, we present a review of recent works in high-level language modelling for speech recognition. We describe the word lattice parser developed in this work in Section 3. Section 4 is a description of current evaluation metrics, and suggestions for new metrics. Experiments on strings and word lattices are reported in Section 5, and conclusions and opportunities for future work are outlined in Section 6. 2 Previous Work The largest improvements in word error rate (WER) have been seen with n-best list r</context>
<context position="5143" citStr="Collins (1999)" startWordPosition="787" endWordPosition="788">ctions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent. This is a computationally intensive process, as the dependencies considered can be of arbitrarily long distances. All possible sentence prefixes are considered at each extension step. Roark (2001) reports on the use of a lexicalized probabilistic top-down parser for word lattices, evaluated both on parse accuracy and WER. Our work is different from Roark (2001) in that we use a bottom-up parsing algorithm with dynamic programming based on the parsing model II of Collins (1999). Bottom-up chart parsing, through various forms of extensions to the CKY algorithm, has been applied to word lattices for speech recognition (Hall and Johnson, 2003; Chappelier and Rajman, 1998; Chelba and Jelinek, 2000). Full acoustic and n-best lattices filtered by trigram scores have been parsed. Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. Unlike (Roark, 2001; Chelba,</context>
<context position="6532" citStr="Collins (1999)" startWordPosition="1009" endWordPosition="1010"> Word Lattice Parser Parsing models based on headword dependency relationships have been reported, such as the structured language model of Chelba and Jelinek (2000). These models use much less conditioning information than the parsing models of Collins (1999), and do not provide Penn Treebank format parse trees as output. In this section we outline the adaptation of the Collins (1999) parsing model to word lattices. The intended action of the parser is illustrated in Figure 1, which shows parse trees built directly upon a word lattice. 3.1 Parameterization The parameterization of model II of Collins (1999) is used in our word lattice parser. Parameters are S S* NP NP VP NN IN CC NNP AUX IN DT NN MD VB rise arise Figure 1: Example of a partially-parsed word lattice. Different paths through the lattice are simultaneously parsed. The example shows two final parses, one of low probability (S ) and one of high probability (S). maximum likelihood estimates of conditional probabilities — the probability of some event of interest (e.g., a left-modifier attachment) given a context (e.g., parent non-terminal, distance, headword). One notable difference between the word lattice parser and the original imp</context>
<context position="7780" citStr="Collins (1999)" startWordPosition="1224" endWordPosition="1225">he handling of part-of-speech (POS) tagging of unknown words (words seen fewer than 5 times in training). The conditioning context of the parsing model parameters includes POS tagging. Collins (1999) falls back to the POS tagging of Ratnaparkhi (1996) for words seen fewer than 5 times in the training corpus. As the tagger of Ratnaparkhi (1996) cannot tag a word lattice, we cannot back off to this tagging. We rely on the tag assigned by the parsing model in all cases. Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the Collins (1999) model. 3.2 Parsing Algorithm The algorithm is a variation of probabilistic online, bottom-up, left-to-right Cocke-KasamiYounger parsing similar to Chappelier and Rajman (1998). Our parser produces trees (bottom-up) in a rightbranching manner, using unary extension and binary adjunction. Starting with a proposed headword, left modifiers are added first using right-branching, then right modifiers using left-branching. Word lattice edges are iteratively added to the agenda. Complete closure is carried out, and the next word edge is added to the agenda. This process is repeated until all word edg</context>
<context position="9174" citStr="Collins (1999)" startWordPosition="1452" endWordPosition="1453">se candidates. For parsing of strings, the score for a chart edge is the product of the scores of any child edges and the score for the creation of the new edge, as given by the model parameters. This score, defined solely by the parsing model, will be referred to as the parser score. The total score for chart edges for the lattice parsing task is a combination of the parser score, an acoustic model score, and a trigram model score. Scaling factors follow those of (Chelba and Jelinek, 2000; Roark, 2001). 3.3 Smoothing and Pruning The parameter estimation techniques (smoothing and back-off) of Collins (1999) are reimplemented. Additional techniques are required to prune the search space of possible parses, due to the complexity of the parsing algorithm and the size of the word lattices. The main technique we employ is a variation of the beam search of Collins (1999) to restrict the chart size by excluding low probability edges. The total score (combined acoustic and language model scores) of candidate edges are compared against edge with the same span and category. Proposed edges with score outside the beam are not added to the chart. The drawback to this process is that we can no longer guarante</context>
<context position="15507" citStr="Collins (1999)" startWordPosition="2502" endWordPosition="2503">timized settings (thresholds, variable beam function, base beam value) for parsing using development test data consisting of strings for which we have annotated parse trees. The parsing accuracy for parsing word lattices was not directly evaluated as we did not have annotated parse trees for comparison. Furthermore, standard parsing measures such as labelled precision and recall are not directly applicable in cases where the number of words differs between the proposed parse tree and the gold standard. Results show scores for parsing strings which are lower than the original implementation of Collins (1999). The WER scores for this, the first application of the Collins (1999) model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data. 3Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al., 1991). 5.1 Parsing Strings The lattice parser can parse strings by creating a single-path lattice from the input (all word transitions are assigned an input score of 1.0). The lattice parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Tayl</context>
<context position="17971" citStr="Collins, 1999" startWordPosition="2911" endWordPosition="2912">trings with annotated reference parses), and by WER. WER is measured by parsing word lattices and comparing the sentence yield of the highest scoring parse tree to the reference transcription (using NIST SCLITE for alignment and error calculation).5 We assume the parsing performance achieved by parsing strings carries over approximately to parsing word lattices. Two different corpora were used in training the parsing model on word lattices: sections 02-21 of the WSJ Penn Treebank (the same sections as used to train the model for parsing strings) [1 million words] 4The original implementation (Collins, 1999) of this model considered all tags for all words. 5To properly model language using a parser, one should sum parse tree scores for each sentence hypothesis, and choose the sentence with the best sum of parse tree scores. We choose the yield of the parse tree with the highest score. Summation is too computationally expensive given the model —we do not even generate all possible parse trees, but instead restrict generation using dynamic programming. Exp. OP LP (%) LR (%) CB 0 CB (%) 2 CB (%) Ref N 88.7 89.0 0.95 65.7 85.6 1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1: Results</context>
<context position="24683" citStr="Collins, 1999" startWordPosition="4083" endWordPosition="4084">n the density of the lattices — for very dense lattices, the equivalent n-best list parsing will parse common substrings up to n times. In the limit of lowest density, a lattice may have paths without overlap, and the number of edges per word would be the same for the lattice and lists. 5.4 Time and Space Requirements The algorithms and data structures were designed to minimize parameter lookup times and memory usage by the chart and parameter set (Collins, 2004). To increase parameter lookup speed, all parameter values are calculated for all levels of back-off at training time. By contrast, (Collins, 1999) calculates parameter values by looking up event counts at run-time. The implementation was then optimized using a memory and processor profiler and debugger. Parsing the complete set of HUB-1 lattices (213 sentences, a total of 3,446 words) on average takes approximately 8 hours, on an Intel Pentium 4 (1.6GHz) Linux system, using 1GB memory. Memory requirements for parsing lattices is vastly greater than equivalent parsing of a single sentence, as chart size increases with the number of divergent paths in a lattice. Additional analysis of resource issues can be found in Collins (2004). 5.5 Co</context>
<context position="30485" citStr="Collins (1999)" startWordPosition="5106" endWordPosition="5107">e 73.2/76.5% LP/LR on section 23 of the Penn Treebank, compared to 82.9/82.4% LP/LR of Roark (2001) and 90.1/90.1% LP/LR of Charniak (2000). Another contributing factor to the accuracy of Charniak (2001) is the size of the training set — 20M words larger than that used in this work. The low WER of Roark (2001), a top-down probabilistic parsing model, was achieved by training the model on 1 million words of the Penn Treebank, then performing a single pass of Expectation Maximization (EM) on a further 1.2 million words. 6 Conclusions In this work we present an adaptation of the parsing model of Collins (1999) for application to ASR. The system was evaluated over two sets of data: strings and word lattices. As PARSEVAL measures are not applicable to word lattices, we measured the parsing accuracy using string input. The resulting scores were lower than that original implementation of the model. Despite this, the model was successful as a language model for speech recognition, as measured by WER and ability to extract high-level information. Here, the system performs better than a simple n-gram model trained on the same data, while simultaneously providing syntactic information in the form of parse </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Global thresholding and multiple-pass parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Goodman, 1997</marker>
<rawString>Joshua Goodman. 1997. Global thresholding and multiple-pass parsing. In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Mark Johnson</author>
</authors>
<title>Language modeling using efficient best-first bottom-up parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop.</booktitle>
<contexts>
<context position="1642" citStr="Hall and Johnson, 2003" startWordPosition="230" endWordPosition="233">rstanding. 1 Introduction The question of how to integrate high-level knowledge representations of language with automatic speech recognition (ASR) is becoming more important as (1) speech recognition technology matures, (2) the rate of improvement of recognition accuracy decreases, and (3) the need for additional information (beyond simple transcriptions) becomes evident. Most of the currently best ASR systems use an n-gram language model of the type pioneered by Bahl et al. (1983). Recently, research has begun to show progress towards application of new and better models of spoken language (Hall and Johnson, 2003; Roark, 2001; Chelba and Jelinek, 2000). Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition, extracting high-level structure from speech, while simultaneously selecting the best path in a word lattice. Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing (Ng and Zelle, 1997). Collins (1999) presents three lexicalized models which consider long-distance dependencies within a sentence. Grammar productions are conditioned on headwords. The conditioning context is </context>
<context position="4260" citStr="Hall and Johnson, 2003" startWordPosition="645" endWordPosition="648"> metrics. Experiments on strings and word lattices are reported in Section 5, and conclusions and opportunities for future work are outlined in Section 6. 2 Previous Work The largest improvements in word error rate (WER) have been seen with n-best list rescoring. The best n hypotheses of a simple speech recognizer are processed by a more sophisticated language model and re-ranked. This method is algorithmically simpler than parsing lattices, as one can use a model developed for strings, which need not operate strictly * left to right. However, we confirm the observation of (Ravishankar, 1997; Hall and Johnson, 2003) that parsing word lattices saves computation time by only parsing common substrings once. Chelba (2000) reports WER reduction by rescoring word lattices with scores of a structured language model (Chelba and Jelinek, 2000), interpolated with trigram scores. Word predictions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent. This is a computationally intensive process, as the dependencies considered can be of arbitrarily long distances. All possible sentence prefixes are considered at each extension step. Roark (2001) r</context>
<context position="5774" citStr="Hall and Johnson (2003)" startWordPosition="887" endWordPosition="890">-up chart parsing, through various forms of extensions to the CKY algorithm, has been applied to word lattices for speech recognition (Hall and Johnson, 2003; Chappelier and Rajman, 1998; Chelba and Jelinek, 2000). Full acoustic and n-best lattices filtered by trigram scores have been parsed. Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. Unlike (Roark, 2001; Chelba, 2000), Hall and Johnson (2003) achieve improvement in WER over the trigram model without interpolating its lattice parser probabilities directly with trigram probabilities. 3 Word Lattice Parser Parsing models based on headword dependency relationships have been reported, such as the structured language model of Chelba and Jelinek (2000). These models use much less conditioning information than the parsing models of Collins (1999), and do not provide Penn Treebank format parse trees as output. In this section we outline the adaptation of the Collins (1999) parsing model to word lattices. The intended action of the parser i</context>
<context position="11186" citStr="Hall and Johnson (2003)" startWordPosition="1800" endWordPosition="1803">erating becomes questionable (Goodman, 1997). The base beam is limited to control the increase in the chart size. The selection of the base beam, beam increment, and variable beam function is governed by the familiar speed/accuracy trade-off.1 The variable beam function found to allow fast convergence with minimal loss of accuracy is: b bˆ (1) log w 2 2 1Details of the optimization can be found in Collins (2004). Charniak et al. (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. The technique is employed by Hall and Johnson (2003) to ensure that early stages of parsing do not strongly bias later stages. We adapt this idea to a single stage process. Due to the restrictions of beam search and thresholds, the first parse found by the model may not be the model optimal parse (i.e., we cannot guarantee best-first search). We therefore employ a form of overparsing — once a complete parse tree is found, we further extend the base beam by the beam increment and parse again. We continue this process as long as extending the beam results in an improved best parse score. 4 Expanding the Measures of Success Given the task of simpl</context>
<context position="21509" citStr="Hall and Johnson, 2003" startWordPosition="3525" endWordPosition="3529">sentences from the BLLIP corpus used in training. 1 corpus are annotated with trigram scores trained using a 20 thousand word vocabulary and 40 million word training sample. The word lattices have a unique start and end point, and each complete path through a lattice represents an utterance hypothesis. As the parser operates in a left-to-right manner, and closure is performed at each node, the input lattice edges must be processed in topological order. Input lattices were sorted before parsing. This corpus has been used in other work on syntactic language modelling (Chelba, 2000; Roark, 2001; Hall and Johnson, 2003). The word lattices of the HUB-1 corpus are annotated with an acoustic score, a, and a trigram probability, lm, for each edge. The input edge score stored in the word lattice is: log PZnpyd alog a 0log lm (3) where a is the acoustic score and lm is the trigram score stored in the lattice. The total edge weight in the parser is a scaled combination of these scores with the parser score derived with the model parameters: log w alog a 0log lm s (4) where w is the edge weight, and s is the score assigned by the parameters of the parsing model. We optimized performance on a development subset of te</context>
<context position="27320" citStr="Hall and Johnson (2003)" startWordPosition="4534" endWordPosition="4537"> 1M words) with the lattice trigram (trained on 40M words) resulted in a higher WER than the lattice trigram alone. This indicates that our 1M word training set is not sufficient to permit effective combination with the lattice trigram. When the training of the head-driven parsing model was extended to the BLLIP 1987 corpus (20M words), the combination of models (a 1 16 0 1) achieved additional improvement in WER over the lattice trigram alone. The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al. (2002)). However, n-best list parsing, as seen in our evaluation, requires repeated analysis of common subsequences, a less efficient process than directly parsing the word lattice. The reported results of (Roark, 2001) and (Chelba, 2000) are for parsing models interpolated with the lattice trigram probabilities. Hall and John7The WER of the hypothesis which best matches the true utterance, i.e., the lowest WER possible given the hypotheses set. Training Size Lattice/List OP WER Number of Edges (per word) S D I T</context>
</contexts>
<marker>Hall, Johnson, 2003</marker>
<rawString>Keith Hall and Mark Johnson. 2003. Language modeling using efficient best-first bottom-up parsing. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Information Extraction From Speech And Text.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12958" citStr="Jelinek, 1997" startWordPosition="2103" endWordPosition="2104">on — minimize sentence error. Sentence error rate is the percentage of sentences for which the proposed utterance has at least one error. Models (such as ours) which optimize prediction of test sentences Wt, generated by the source, minimize the sentence error. Thus even though WER is useful practically, it is formally not the appropriate measure for the commonly used language models. Unfortunately, as a practical measure, sentence error rate is not as useful — it is not as fine-grained as WER. Perplexity is another measure of language model quality, measurable independent of ASR performance (Jelinek, 1997). Perplexity is related to the entropy of the source model which the language model attempts to estimate. These measures, while informative, do not capture success of extraction of high-level information from speech. Task-specific measures should be used in tandem with extensional measures such as perplexity and WER. Roark (2002), when reviewing 2SCLITE (http://www.nist.gov/speech/ tools/) by NIST is the most commonly used alignment tool. (2) parsing for speech recognition, discusses a modelling trade-off between producing parse trees and producing strings. Most models are evaluated either wit</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Information Extraction From Speech And Text. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
<author>Andreas Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>400</pages>
<contexts>
<context position="12319" citStr="Mangu et al. (2000)" startWordPosition="1998" endWordPosition="2001">improved best parse score. 4 Expanding the Measures of Success Given the task of simply generating a transcription of speech, WER is a useful and direct way to measure language model quality for ASR. WER is the count of incorrect words in hypothesis Wˆ per word in the true string W. For measurement, we must assume prior knowledge of W and the best alignment of the reference and hypothesis strings.2 Errors are categorized as insertions, deletions, or substitutions. Word Error Rate 100Insertions Substitutions Deletions Total Words in Correct Transcript It is important to note that most models — Mangu et al. (2000) is an innovative exception — minimize sentence error. Sentence error rate is the percentage of sentences for which the proposed utterance has at least one error. Models (such as ours) which optimize prediction of test sentences Wt, generated by the source, minimize the sentence error. Thus even though WER is useful practically, it is formally not the appropriate measure for the commonly used language models. Unfortunately, as a practical measure, sentence error rate is not as useful — it is not as fine-grained as WER. Perplexity is another measure of language model quality, measurable indepen</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000. Finding consensus in speech recognition: Word error minimization and other applications of confusion networks. Computer Speech and Language, 14(4):373– 400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>John Zelle</author>
</authors>
<title>Corpus-based approaches to semantic interpretation in natural language processing.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<pages>18--45</pages>
<contexts>
<context position="2052" citStr="Ng and Zelle, 1997" startWordPosition="291" endWordPosition="294">s use an n-gram language model of the type pioneered by Bahl et al. (1983). Recently, research has begun to show progress towards application of new and better models of spoken language (Hall and Johnson, 2003; Roark, 2001; Chelba and Jelinek, 2000). Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition, extracting high-level structure from speech, while simultaneously selecting the best path in a word lattice. Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing (Ng and Zelle, 1997). Collins (1999) presents three lexicalized models which consider long-distance dependencies within a sentence. Grammar productions are conditioned on headwords. The conditioning context is thus more focused than that of a large n-gram covering the same span, so the sparse data problems arising from the sheer size of the parameter space are less pressing. However, sparse data problems arising from the limited availability of annotated training data become a problem. We test the head-driven statistical lattice parser with word lattices from the NIST HUB-1 corpus, which has been used by others i</context>
</contexts>
<marker>Ng, Zelle, 1997</marker>
<rawString>Hwee Tou Ng and John Zelle. 1997. Corpus-based approaches to semantic interpretation in natural language processing. AI Magazine, 18:45–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="7417" citStr="Ratnaparkhi (1996)" startWordPosition="1156" endWordPosition="1158">f low probability (S ) and one of high probability (S). maximum likelihood estimates of conditional probabilities — the probability of some event of interest (e.g., a left-modifier attachment) given a context (e.g., parent non-terminal, distance, headword). One notable difference between the word lattice parser and the original implementation of Collins (1999) is the handling of part-of-speech (POS) tagging of unknown words (words seen fewer than 5 times in training). The conditioning context of the parsing model parameters includes POS tagging. Collins (1999) falls back to the POS tagging of Ratnaparkhi (1996) for words seen fewer than 5 times in the training corpus. As the tagger of Ratnaparkhi (1996) cannot tag a word lattice, we cannot back off to this tagging. We rely on the tag assigned by the parsing model in all cases. Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the Collins (1999) model. 3.2 Parsing Algorithm The algorithm is a variation of probabilistic online, bottom-up, left-to-right Cocke-KasamiYounger parsing similar to Chappelier and Rajman (1998). Our parser produces trees (bottom-up) in a rightbranching m</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Conference on Empirical Methods in Natural Language Processing, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mosur K Ravishankar</author>
</authors>
<title>Some results on search complexity vs accuracy.</title>
<date>1997</date>
<booktitle>In DARPA Speech Recognition Workshop,</booktitle>
<pages>104--107</pages>
<contexts>
<context position="4235" citStr="Ravishankar, 1997" startWordPosition="643" endWordPosition="644">suggestions for new metrics. Experiments on strings and word lattices are reported in Section 5, and conclusions and opportunities for future work are outlined in Section 6. 2 Previous Work The largest improvements in word error rate (WER) have been seen with n-best list rescoring. The best n hypotheses of a simple speech recognizer are processed by a more sophisticated language model and re-ranked. This method is algorithmically simpler than parsing lattices, as one can use a model developed for strings, which need not operate strictly * left to right. However, we confirm the observation of (Ravishankar, 1997; Hall and Johnson, 2003) that parsing word lattices saves computation time by only parsing common substrings once. Chelba (2000) reports WER reduction by rescoring word lattices with scores of a structured language model (Chelba and Jelinek, 2000), interpolated with trigram scores. Word predictions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent. This is a computationally intensive process, as the dependencies considered can be of arbitrarily long distances. All possible sentence prefixes are considered at each exten</context>
</contexts>
<marker>Ravishankar, 1997</marker>
<rawString>Mosur K. Ravishankar. 1997. Some results on search complexity vs accuracy. In DARPA Speech Recognition Workshop, pages 104–107, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and Applications.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="1655" citStr="Roark, 2001" startWordPosition="234" endWordPosition="235">n The question of how to integrate high-level knowledge representations of language with automatic speech recognition (ASR) is becoming more important as (1) speech recognition technology matures, (2) the rate of improvement of recognition accuracy decreases, and (3) the need for additional information (beyond simple transcriptions) becomes evident. Most of the currently best ASR systems use an n-gram language model of the type pioneered by Bahl et al. (1983). Recently, research has begun to show progress towards application of new and better models of spoken language (Hall and Johnson, 2003; Roark, 2001; Chelba and Jelinek, 2000). Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition, extracting high-level structure from speech, while simultaneously selecting the best path in a word lattice. Parse trees generated by this process will be useful for automated speech understanding, such as in higher semantic parsing (Ng and Zelle, 1997). Collins (1999) presents three lexicalized models which consider long-distance dependencies within a sentence. Grammar productions are conditioned on headwords. The conditioning context is thus more foc</context>
<context position="4858" citStr="Roark (2001)" startWordPosition="738" endWordPosition="739">Johnson, 2003) that parsing word lattices saves computation time by only parsing common substrings once. Chelba (2000) reports WER reduction by rescoring word lattices with scores of a structured language model (Chelba and Jelinek, 2000), interpolated with trigram scores. Word predictions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent. This is a computationally intensive process, as the dependencies considered can be of arbitrarily long distances. All possible sentence prefixes are considered at each extension step. Roark (2001) reports on the use of a lexicalized probabilistic top-down parser for word lattices, evaluated both on parse accuracy and WER. Our work is different from Roark (2001) in that we use a bottom-up parsing algorithm with dynamic programming based on the parsing model II of Collins (1999). Bottom-up chart parsing, through various forms of extensions to the CKY algorithm, has been applied to word lattices for speech recognition (Hall and Johnson, 2003; Chappelier and Rajman, 1998; Chelba and Jelinek, 2000). Full acoustic and n-best lattices filtered by trigram scores have been parsed. Hall and John</context>
<context position="9068" citStr="Roark, 2001" startWordPosition="1438" endWordPosition="1439">nit lattice, and at least one complete parse is found. Edges are each assigned a score, used to rank parse candidates. For parsing of strings, the score for a chart edge is the product of the scores of any child edges and the score for the creation of the new edge, as given by the model parameters. This score, defined solely by the parsing model, will be referred to as the parser score. The total score for chart edges for the lattice parsing task is a combination of the parser score, an acoustic model score, and a trigram model score. Scaling factors follow those of (Chelba and Jelinek, 2000; Roark, 2001). 3.3 Smoothing and Pruning The parameter estimation techniques (smoothing and back-off) of Collins (1999) are reimplemented. Additional techniques are required to prune the search space of possible parses, due to the complexity of the parsing algorithm and the size of the word lattices. The main technique we employ is a variation of the beam search of Collins (1999) to restrict the chart size by excluding low probability edges. The total score (combined acoustic and language model scores) of candidate edges are compared against edge with the same span and category. Proposed edges with score o</context>
<context position="13977" citStr="Roark (2001)" startWordPosition="2257" endWordPosition="2258"> most commonly used alignment tool. (2) parsing for speech recognition, discusses a modelling trade-off between producing parse trees and producing strings. Most models are evaluated either with measures of success for parsing or for word recognition, but rarely both. Parsing models are difficult to implement as word-predictive language models due to their complexity. Generative random sampling is equally challenging, so the parsing correlate of perplexity is not easy to measure. Traditional (i.e., n-gram) language models do not produce parse trees, so parsing metrics are not useful. However, Roark (2001) argues for using parsing metrics, such as labelled precision and recall,3 along with WER, for parsing applications in ASR. Weighted WER (Weber et al., 1997) is also a useful measurement, as the most often ill-recognized words are short, closed-class words, which are not as important to speech understanding as phrasal head words. We will adopt the testing strategy of Roark (2001), but find that measurement of parse accuracy and WER on the same data set is not possible given currently available corpora. Use of weighted WER and development of methods to simultaneously measure WER and parse accur</context>
<context position="21484" citStr="Roark, 2001" startWordPosition="3523" endWordPosition="3524">ed all HUB-1 sentences from the BLLIP corpus used in training. 1 corpus are annotated with trigram scores trained using a 20 thousand word vocabulary and 40 million word training sample. The word lattices have a unique start and end point, and each complete path through a lattice represents an utterance hypothesis. As the parser operates in a left-to-right manner, and closure is performed at each node, the input lattice edges must be processed in topological order. Input lattices were sorted before parsing. This corpus has been used in other work on syntactic language modelling (Chelba, 2000; Roark, 2001; Hall and Johnson, 2003). The word lattices of the HUB-1 corpus are annotated with an acoustic score, a, and a trigram probability, lm, for each edge. The input edge score stored in the word lattice is: log PZnpyd alog a 0log lm (3) where a is the acoustic score and lm is the trigram score stored in the lattice. The total edge weight in the parser is a scaled combination of these scores with the parser score derived with the model parameters: log w alog a 0log lm s (4) where w is the edge weight, and s is the score assigned by the parameters of the parsing model. We optimized performance on a</context>
<context position="27251" citStr="Roark (2001)" startWordPosition="4525" endWordPosition="4526"> However, the combination of the parsing model (trained on 1M words) with the lattice trigram (trained on 40M words) resulted in a higher WER than the lattice trigram alone. This indicates that our 1M word training set is not sufficient to permit effective combination with the lattice trigram. When the training of the head-driven parsing model was extended to the BLLIP 1987 corpus (20M words), the combination of models (a 1 16 0 1) achieved additional improvement in WER over the lattice trigram alone. The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al. (2002)). However, n-best list parsing, as seen in our evaluation, requires repeated analysis of common subsequences, a less efficient process than directly parsing the word lattice. The reported results of (Roark, 2001) and (Chelba, 2000) are for parsing models interpolated with the lattice trigram probabilities. Hall and John7The WER of the hypothesis which best matches the true utterance, i.e., the lowest WER possible given the hypotheses set.</context>
<context position="28645" citStr="Roark (2001)" startWordPosition="4778" endWordPosition="4779">t Y 10.2 3.2 1.4 14.8 16821 20M Lattice N 9.0 3.1 1.0 13.1 1735 20M List N 9.0 3.1 1.0 13.1 9999 20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%), D = deletions (%), I = insertions (%), T = total WER (%). Variable beam function: bˆ b log w 2 2 . Training corpora: 1M = Penn Treebank sections 02-21; 20M = BLLIP section 1987. Model n-best List/Lattice Training Size WER (%) SER (%) Oracle (50-best lattice) Lattice 7.8 Charniak (2001) List 40M 11.9 Xu (2002) List 20M 12.3 Roark (2001) (with EM) List 2M 12.7 Hall (2003) Lattice 30M 13.0 Chelba (2000) Lattice 20M 13.0 Current (α 1 16 β 1) List 20M 13.1 71.0 Current (α 1 16 β 1) Lattice 20M 13.1 70.4 Roark (2001) (no EM) List 1M 13.4 Lattice Trigram Lattice 40M 13.7 69.0 Current (α 1 16 β 1) List 1M 14.8 74.3 Current (α 1 16 β 1) Lattice 1M 14.9 74.0 Current (α β 0) Lattice 1M 16.0 75.5 Treebank Trigram Lattice 1M 16.5 79.8 No language model Lattice 16.8 84.0 Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER = sentence error rate. WER = word error rate. “Speech-like” transformat</context>
<context position="29970" citStr="Roark (2001)" startWordPosition="5017" endWordPosition="5018">t list parsing. Hall (2003) is a lattice-parser related to Charniak (2001). son (2003) does not use the lattice trigram scores directly. However, as in other works, the lattice trigram is used to prune the acoustic lattice to the 50 best paths. The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system. Xu et al. (2002) report inverse correlation between labelled precision/recall and WER. We achieve 73.2/76.5% LP/LR on section 23 of the Penn Treebank, compared to 82.9/82.4% LP/LR of Roark (2001) and 90.1/90.1% LP/LR of Charniak (2000). Another contributing factor to the accuracy of Charniak (2001) is the size of the training set — 20M words larger than that used in this work. The low WER of Roark (2001), a top-down probabilistic parsing model, was achieved by training the model on 1 million words of the Penn Treebank, then performing a single pass of Expectation Maximization (EM) on a further 1.2 million words. 6 Conclusions In this work we present an adaptation of the parsing model of Collins (1999) for application to ASR. The system was evaluated over two sets of data: strings and </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Robust Probabilistic Predictive Syntactic Processing: Motivations, Models, and Applications. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Markov parsing: Lattice rescoring with a statistical parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>287--294</pages>
<contexts>
<context position="13289" citStr="Roark (2002)" startWordPosition="2154" endWordPosition="2155">opriate measure for the commonly used language models. Unfortunately, as a practical measure, sentence error rate is not as useful — it is not as fine-grained as WER. Perplexity is another measure of language model quality, measurable independent of ASR performance (Jelinek, 1997). Perplexity is related to the entropy of the source model which the language model attempts to estimate. These measures, while informative, do not capture success of extraction of high-level information from speech. Task-specific measures should be used in tandem with extensional measures such as perplexity and WER. Roark (2002), when reviewing 2SCLITE (http://www.nist.gov/speech/ tools/) by NIST is the most commonly used alignment tool. (2) parsing for speech recognition, discusses a modelling trade-off between producing parse trees and producing strings. Most models are evaluated either with measures of success for parsing or for word recognition, but rarely both. Parsing models are difficult to implement as word-predictive language models due to their complexity. Generative random sampling is equally challenging, so the parsing correlate of perplexity is not easy to measure. Traditional (i.e., n-gram) language mod</context>
</contexts>
<marker>Roark, 2002</marker>
<rawString>Brian Roark. 2002. Markov parsing: Lattice rescoring with a statistical parser. In Proceedings of the 40th Annual Meeting of the ACL, pages 287–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Taylor</author>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<title>The Penn TreeBank: An Overview, chapter 1.</title>
<date>2003</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="16123" citStr="Taylor et al., 2003" startWordPosition="2606" endWordPosition="2610">999). The WER scores for this, the first application of the Collins (1999) model to parsing word lattices, are comparable to other recent work in syntactic language modelling, and better than a simple trigram model trained on the same data. 3Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al., 1991). 5.1 Parsing Strings The lattice parser can parse strings by creating a single-path lattice from the input (all word transitions are assigned an input score of 1.0). The lattice parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Taylor et al., 2003) Development testing was carried out on section 23 in order to select model thresholds and variable beam functions. Final testing was carried out on section 00, and the PARSEVAL measures (Black et al., 1991) were used to evaluate the performance. The scores for our experiments are lower than the scores of the original implementation of model II (Collins, 1999). This difference is likely due in part to differences in POS tagging. Tag accuracy for our model was 93.2%, whereas for the original implementation of Collins (1999), model II achieved tag accuracy of 96.75%. In addition to different tag</context>
</contexts>
<marker>Taylor, Marcus, Santorini, 2003</marker>
<rawString>Ann Taylor, Mitchell Marcus, and Beatrice Santorini, 2003. The Penn TreeBank: An Overview, chapter 1. Kluwer, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Weber</author>
<author>J¨org Spilker</author>
<author>G¨unther G¨orz</author>
</authors>
<title>Parsing n best trees from a word lattice. Kunstliche Intelligenz,</title>
<date>1997</date>
<pages>279--288</pages>
<marker>Weber, Spilker, G¨orz, 1997</marker>
<rawString>Hans Weber, J¨org Spilker, and G¨unther G¨orz. 1997. Parsing n best trees from a word lattice. Kunstliche Intelligenz, pages 279–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies in structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>191--198</pages>
<contexts>
<context position="27408" citStr="Xu et al. (2002)" startWordPosition="4551" endWordPosition="4554">ttice trigram alone. This indicates that our 1M word training set is not sufficient to permit effective combination with the lattice trigram. When the training of the head-driven parsing model was extended to the BLLIP 1987 corpus (20M words), the combination of models (a 1 16 0 1) achieved additional improvement in WER over the lattice trigram alone. The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al. (2002)). However, n-best list parsing, as seen in our evaluation, requires repeated analysis of common subsequences, a less efficient process than directly parsing the word lattice. The reported results of (Roark, 2001) and (Chelba, 2000) are for parsing models interpolated with the lattice trigram probabilities. Hall and John7The WER of the hypothesis which best matches the true utterance, i.e., the lowest WER possible given the hypotheses set. Training Size Lattice/List OP WER Number of Edges (per word) S D I T 1M Lattice N 10.4 3.3 1.5 15.2 1788 1M List N 10.4 3.2 1.4 15.0 10211 1M Lattice Y 10.3</context>
<context position="29791" citStr="Xu et al. (2002)" startWordPosition="4988" endWordPosition="4991"> sentence error rate. WER = word error rate. “Speech-like” transformations were applied to all training corpora. Xu (2002) is an implementation of the model of Chelba (2000) for n-best list parsing. Hall (2003) is a lattice-parser related to Charniak (2001). son (2003) does not use the lattice trigram scores directly. However, as in other works, the lattice trigram is used to prune the acoustic lattice to the 50 best paths. The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system. Xu et al. (2002) report inverse correlation between labelled precision/recall and WER. We achieve 73.2/76.5% LP/LR on section 23 of the Penn Treebank, compared to 82.9/82.4% LP/LR of Roark (2001) and 90.1/90.1% LP/LR of Charniak (2000). Another contributing factor to the accuracy of Charniak (2001) is the size of the training set — 20M words larger than that used in this work. The low WER of Roark (2001), a top-down probabilistic parsing model, was achieved by training the model on 1 million words of the Penn Treebank, then performing a single pass of Expectation Maximization (EM) on a further 1.2 million wor</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A study on richer syntactic dependencies in structured language modeling. In Proceedings of the 40th Annual Meeting of the ACL, pages 191–198.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>