<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000067">
<title confidence="0.980389">
Edit Machines for Robust Multimodal Language Processing
</title>
<author confidence="0.683973">
Srinivas Bangalore
</author>
<affiliation confidence="0.575672">
AT&amp;T Labs-Research
</affiliation>
<address confidence="0.8393535">
180 Park Ave
Florham Park, NJ 07932
</address>
<email confidence="0.99781">
srini@research.att.com
</email>
<author confidence="0.581341">
Michael Johnston
</author>
<affiliation confidence="0.485242">
AT&amp;T Labs-Research
</affiliation>
<address confidence="0.762968">
180 Park Ave
Florham Park, NJ 07932
</address>
<email confidence="0.998764">
johnston@research.att.com
</email>
<sectionHeader confidence="0.993922" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996718">
Multimodal grammars provide an expres-
sive formalism for multimodal integra-
tion and understanding. However, hand-
crafted multimodal grammars can be brit-
tle with respect to unexpected, erroneous,
or disfluent inputs. Spoken language
(speech-only) understanding systems have
addressed this issue of lack of robustness
of hand-crafted grammars by exploiting
classification techniques to extract fillers
of a frame representation. In this paper,
we illustrate the limitations of such clas-
sification approaches for multimodal in-
tegration and understanding and present
an approach based on edit machines that
combine the expressiveness of multimodal
grammars with the robustness of stochas-
tic language models of speech recognition.
We also present an approach where the
edit operations are trained from data using
a noisy channel model paradigm. We eval-
uate and compare the performance of the
hand-crafted and learned edit machines in
the context of a multimodal conversational
system (MATCH).
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971580645162">
Over the years, there have been several mul-
timodal systems that allow input and/or output
to be conveyed over multiple channels such as
speech, graphics, and gesture, for example, put
that there (Bolt, 1980), CUBRICON (Neal and
Shapiro, 1991), QuickSet (Cohen et al., 1998),
SmartKom (Wahlster, 2002), Match (Johnston et
al., 2002). Multimodal integration and interpre-
tation for such interfaces is elegantly expressed
using multimodal grammars (Johnston and Ban-
galore, 2000). These grammars support com-
posite multimodal inputs by aligning speech in-
put (words) and gesture input (represented as se-
quences of gesture symbols) while expressing the
relation between the speech and gesture input and
their combined semantic representation. In (Ban-
galore and Johnston, 2000; Johnston and Banga-
lore, 2005), we have shown that such grammars
can be compiled into finite-state transducers en-
abling effective processing of lattice input from
speech and gesture recognition and mutual com-
pensation for errors and ambiguities.
However, like other approaches based on hand-
crafted grammars, multimodal grammars can be
brittle with respect to extra-grammatical, erro-
neous and disfluent input. For speech recognition,
a corpus-driven stochastic language model (SLM)
with smoothing or a combination of grammar-
based and -gram model (Bangalore and John-
ston, 2004; Wang et al., 2002) can be built in order
to overcome the brittleness of a grammar-based
language model. Although the corpus-driven lan-
guage model might recognize a user’s utterance
correctly, the recognized utterance may not be
assigned a semantic representation by the multi-
modal grammar if the utterance is not part of the
grammar.
There have been two main approaches to im-
proving robustness of the understanding compo-
nent in the spoken language understanding litera-
ture. First, a parsing-based approach attempts to
recover partial parses from the parse chart when
the input cannot be parsed in its entirety due to
noise, in order to construct a (partial) semantic
representation (Dowding et al., 1993; Allen et al.,
2001; Ward, 1991). Second, a classification-based
approach views the problem of understanding as
extracting certain bits of information from the in-
put. It attempts to classify the utterance and iden-
tifies substrings of the input as slot-filler values
to construct a frame-like semantic representation.
Both approaches have shortcomings. Although in
the first approach, the grammar can encode richer
semantic representations, the method for combin-
ing the fragmented parses is quite ad hoc. In the
second approach, the robustness is derived from
training classifiers on annotated data, this data is
very expensive to collect and annotate, and the
semantic representation is fairly limited. Further-
more, it is not clear how to extend this approach to
apply on lattice input – an important requirement
for multimodal processing.
</bodyText>
<page confidence="0.997227">
361
</page>
<bodyText confidence="0.99975">
An alternative to these approaches is to edit
the recognized string to match the closest string
that can be accepted by the grammar. Essentially
the idea is that, if the recognized string cannot
be parsed, then we determine which in-grammar
string it is most like. For example, in Figure 1, the
recognized string is mapped to the closest string in
the grammar by deletion of the words restaurants
and in.
</bodyText>
<figure confidence="0.545741">
ASR: show cheap restaurants thai places in in chelsea
Edits: show cheapthai places inchelsea
Grammar: show cheap thai places in chelsea
</figure>
<figureCaption confidence="0.999271">
Figure 1: Editing Example
</figureCaption>
<bodyText confidence="0.999965580645161">
In this paper, we develop further this edit-based
approach to finite-state multimodal language un-
derstanding and show how when appropriately
tuned it can provide a substantial improvement in
concept accuracy. We also explore learning ed-
its from data and present an approach of model-
ing this process as a machine translation problem.
We learn a model to translate from out of grammar
or misrecognized language (such as ‘ASR:’ above)
to the closest language the system can understand
(‘Grammar:’ above). To this end, we adopt tech-
niques from statistical machine translation (Brown
et al., 1993; Och and Ney, 2003) and use statistical
alignment to learn the edit patterns. Here we eval-
uate these different techniques on data from the
MATCH multimodal conversational system (John-
ston et al., 2002) but the same techniques are more
broadly applicable to spoken language systems in
general whether unimodal or multimodal.
The layout of the paper is as follows. In Sec-
tions 2 and 3, we briefly describe the MATCH
application and the finite-state approach to mul-
timodal language understanding. In Section 4,
we discuss the limitations of the methods used
for robust understanding in spoken language un-
derstanding literature. In Section 5 we present
our approach to building hand-crafted edit ma-
chines. In Section 6, we describe our approach to
learning the edit operations using a noisy channel
paradigm. In Section 7, we describe our experi-
mental evaluation.
</bodyText>
<sectionHeader confidence="0.956572" genericHeader="introduction">
2 MATCH: A Multimodal Application
</sectionHeader>
<bodyText confidence="0.991066545454545">
MATCH (Multimodal Access To City Help) is a
working city guide and navigation system that en-
ables mobile users to access restaurant and sub-
way information for New York City and Washing-
ton, D.C. (Johnston et al., 2002). The user inter-
acts with an interface displaying restaurant list-
ings and a dynamic map showing locations and
street information. The inputs can be speech,
drawing/pointing on the display with a stylus, or
synchronous multimodal combinations of the two
modes. The user can ask for the review, cui-
sine, phone number, address, or other informa-
tion about restaurants and subway directions to lo-
cations. The system responds with graphical la-
bels on the display, synchronized with synthetic
speech output. For example, if the user says phone
numbers for these two restaurants and circles two
restaurants as in Figure 2 [A], the system will draw
a callout with the restaurant name and number and
say, for example Time Cafe can be reached at 212-
533-7000, for each restaurant in turn (Figure 2
[B]).
</bodyText>
<figureCaption confidence="0.990052">
Figure 2: MATCH Example
</figureCaption>
<sectionHeader confidence="0.965686" genericHeader="method">
3 Finite-state Multimodal Understanding
</sectionHeader>
<bodyText confidence="0.99997535483871">
Our approach to integrating and interpreting mul-
timodal inputs (Johnston et al., 2002) is an exten-
sion of the finite-state approach previously pro-
posed in (Bangalore and Johnston, 2000; John-
ston and Bangalore, 2005). In this approach, a
declarative multimodal grammar captures both the
structure and the interpretation of multimodal and
unimodal commands. The grammar consists of
a set of context-free rules. The multimodal as-
pects of the grammar become apparent in the ter-
minals, each of which is a triple W:G:M, consist-
ing of speech (words, W), gesture (gesture sym-
bols, G), and meaning (meaning symbols, M). The
multimodal grammar encodes not just multimodal
integration patterns but also the syntax of speech
and gesture, and the assignment of meaning, here
represented in XML. The symbol SEM is used to
abstract over specific content such as the set of
points delimiting an area or the identifiers of se-
lected objects (Johnston et al., 2002). In Figure 3,
we present a small simplified fragment from the
MATCH application capable of handling informa-
tion seeking requests such as phone for these three
restaurants. The epsilon symbol () indicates that
a stream is empty in a given terminal.
In the example above where the user says phone
for these two restaurants while circling two restau-
rants (Figure 2 [a]), assume the speech recognizer
returns the lattice in Figure 4 (Speech). The ges-
ture recognition component also returns a lattice
(Figure 4, Gesture) indicating that the user’s ink
</bodyText>
<page confidence="0.997686">
362
</page>
<table confidence="0.978494777777778">
CMD :: cmd INFO:: /cmd
INFO :: type TYPE:: /type
for:: :: obj DEICNP:: /obj
TYPE phone::phonereview::review
DEICNP DDETPL:area: :sel: NUM HEADPL
DDETPL these:G: those:G:
HEADPL restaurants:rest: rest :SEM:SEM
:: /rest
NUM two:2: three:3:... ten:10:
</table>
<figureCaption confidence="0.9998435">
Figure 3: Multimodal grammar fragment
Figure 4: Multimodal Example
</figureCaption>
<bodyText confidence="0.999713416666667">
is either a selection of two restaurants or a ge-
ographical area. In Figure 4 (Gesture) the spe-
cific content is indicated in parentheses after SEM.
This content is removed before multimodal pars-
ing and integration and replaced afterwards. For
detailed explanation of our technique for abstract-
ing over and then re-integrating specific gestural
content and our approach to the representation of
complex gestures see (Johnston et al., 2002). The
multimodal grammar (Figure 3) expresses the re-
lationship between what the user said, what they
drew with the pen, and their combined mean-
ing, in this case Figure 4 (Meaning). The mean-
ing is generated by concatenating the meaning
symbols and replacing SEM with the appropri-
ate specific content: cmd info type
phone /type obj rest [r12,r15] /rest
/obj /info /cmd .
For use in our system, the multimodal grammar
is compiled into a cascade of finite-state transduc-
ers (Johnston and Bangalore, 2000; Johnston et al.,
2002; Johnston and Bangalore, 2005). As a result,
processing of lattice inputs from speech and ges-
ture processing is straightforward and efficient.
</bodyText>
<subsectionHeader confidence="0.9554395">
3.1 Meaning Representation for Concept
Accuracy
</subsectionHeader>
<bodyText confidence="0.999186105263158">
The hierarchically nested XML representation
above is effective for processing by the backend
application, but is not well suited for the auto-
mated determination of the performance of the
language understanding mechanism. We adopt an
approach, similar to (Ciaramella, 1993; Boros et
al., 1996), in which the meaning representation,
in our case XML, is transformed into a sorted flat
list of attribute-value pairs indicating the core con-
tentful concepts of each command. The example
above yields:
(1)
This allows us to calculate the performance of the
understanding component using the same string
matching metrics used for speech recognition ac-
curacy. Concept Sentence Accuracy measures the
number of user inputs for which the system got the
meaning completely right (this is called Sentence
Understanding in (Ciaramella, 1993)).
</bodyText>
<sectionHeader confidence="0.987075" genericHeader="method">
4 Robust Understanding
</sectionHeader>
<bodyText confidence="0.999397846153846">
Robust understanding has been of great interest
in the spoken language understanding literature.
The issue of noisy output from the speech recog-
nizer and disfluencies that are inherent in spoken
input make it imperative for using mechanisms
to provide robust understanding. As discussed
in the introduction, there are two approaches to
addressing robustness – partial parsing approach
and classification approach. We have explored the
classification-based approach to multimodal un-
derstanding in earlier work. We briefly present
this approach and discuss its limitations for mul-
timodal language processing.
</bodyText>
<subsectionHeader confidence="0.988179">
4.1 Classification-based Approach
</subsectionHeader>
<bodyText confidence="0.992748636363636">
In previous work (Bangalore and Johnston, 2004),
we viewed multimodal understanding as a se-
quence of classification problems in order to de-
termine the predicate and arguments of an utter-
ance. The meaning representation shown in (1)
consists of an predicate (the command attribute)
and a sequence of one or more argument at-
tributes which are the parameters for the success-
ful interpretation of the user’s intent. For ex-
ample, in (1), is the predicate and
is the set of
arguments to the predicate.
We determine the predicate ( ) for a to-
ken multimodal utterance ( ) by maximizing the
posterior probability as shown in Equation 2.
(2)
We view the problem of identifying and extract-
ing arguments from a multimodal input as a prob-
lem of associating each token of the input with
a specific tag that encodes the label of the argu-
ment and the span of the argument. These tags
are drawn from a tagset which is constructed by
</bodyText>
<figure confidence="0.997378727272727">
for
two
these
restaurants
phone
ten
Speech:
G area loc
SEM(points...)
Gesture:
sel
2 SEM(r12,r15)
&lt;cmd&gt; &lt;info&gt;
&lt;rest&gt;
r12,r15
&lt;rest&gt;
&lt;type&gt;
&lt;/rest&gt; &lt;/obj&gt; &lt;/info&gt;
phone
&lt;/type&gt; &lt;obj&gt;
&lt;/cmd&gt;
Meaning:
</figure>
<page confidence="0.996766">
363
</page>
<bodyText confidence="0.998168333333333">
extending each argument label by three additional
symbols , following (Ramshaw and Mar-
cus, 1995). These symbols correspond to cases
when a token is inside () an argument span, out-
side ( ) an argument span or at the boundary of
two argument spans ( ) (See Table 1).
</bodyText>
<table confidence="0.999493125">
User cheap thai upper west side
Utterance
Argument price cheap /price cuisine
Annotation thai /cuisine place upper west
side /place
IOB cheap price B thai cuisine B
Encoding upper place I west place I
side place I
</table>
<tableCaption confidence="0.9574795">
Table 1: TheI,O,B encoding for argument ex-
traction.
</tableCaption>
<bodyText confidence="0.997063130434783">
Given this encoding, the problem of extracting
the arguments is a search for the most likely se-
quence of tags ( ) given the input multimodal ut-
terance as shown in Equation (3). We approx-
imate the posterior probability us-
ing independence assumptions as shown in Equa-
tion (4).
Owing to the large set of features that are used
for predicate identification and argument extrac-
tion, we estimate the probabilities using a classifi-
cation model. In particular, we use the Adaboost
classifier (Freund and Schapire, 1996) wherein a
highly accurate classifier is build by combining
many “weak” or “simple” base classifiers, each
of which may only be moderately accurate. The
selection of the weak classifiers proceeds itera-
tively picking the weak classifier that correctly
classifies the examples that are misclassified by
the previously selected weak classifiers. Each
weak classifier is associated with a weight ( )
that reflects its contribution towards minimizing
the classification error. The posterior probability
of is computed as in Equation 5.
</bodyText>
<subsectionHeader confidence="0.99806">
4.2 Limitations of this approach
</subsectionHeader>
<bodyText confidence="0.999979361111111">
Although, we have shown that the classification
approach works for unimodal and simple multi-
modal inputs, it is not clear how this approach
can be extended to work on lattice inputs. Mul-
timodal language processing requires the integra-
tion and joint interpretation of speech and gesture
input. Multimodal integration requires alignment
of the speech and gesture input. Given that the in-
put modalities are both noisy and can receive mul-
tiple within-modality interpretations (e.g. a circle
could be an “O” or an area gesture); it is neces-
sary for the input to be represented as a multiplic-
ity of hypotheses, which can be most compactly
represented as a lattice. The multiplicity of hy-
potheses is also required for exploiting the mu-
tual compensation between the two modalities as
shown in (Oviatt, 1999; Bangalore and Johnston,
2000). Furthermore, in order to provide the dialog
manager the best opportunity to recover the most
appropriate meaning given the dialog context, we
construct a lattice of semantic representations in-
stead of providing only one semantic representa-
tion.
In the multimodal grammar-based approach, the
alignment between speech and gesture along with
their combined interpretation is utilized in deriv-
ing the multimodal finite-state transducers. These
transducers are used to create a gesture-speech
aligned lattice and a lattice of semantic interpre-
tations. However, in the classification-based ap-
proach, it is not as yet clear how alignment be-
tween speech and gesture would be achieved es-
pecially when the inputs are lattice and how the
aligned speech-gesture lattices can be processed to
produce lattice of multimodal semantic represen-
tations.
</bodyText>
<sectionHeader confidence="0.9847555" genericHeader="method">
5 Hand-crafted Finite-State Edit
Machines
</sectionHeader>
<bodyText confidence="0.9603072">
A corpus trained SLM with smoothing is more ef-
fective at recognizing what the user says, but this
will not help system performance if coupled di-
rectly to a grammar-based understanding system
which can only assign meanings to in-grammar ut-
terances. In order to overcome the possible mis-
match between the user’s input and the language
encoded in the multimodal grammar ( ), we in-
troduce a weighted finite-state edit transducer to
the multimodal language processing cascade. This
transducer coerces the set of strings ( ) encoded
in the lattice resulting from ASR ( ) to closest
strings in the grammar that can be assigned an in-
terpretation. We are interested in the string with
the least costly number of edits ( ) that can
be assigned an interpretation by the grammars.
This can be achieved by composition () of trans-
ducers followed by a search for the least cost path
through a weighted transducer as shown below.
We first describe the edit machine introduced
in (Bangalore and Johnston, 2004) (Basic Edit)
then go on to describe a smaller edit machine with
higher performance (4-edit) and an edit machine
&apos;We note that the closest string according to the edit met-
ric may not be the closest string in meaning
</bodyText>
<page confidence="0.995906">
364
</page>
<bodyText confidence="0.9895385">
which incorporates additional heuristics (Smart
edit).
</bodyText>
<subsectionHeader confidence="0.992834">
5.1 Basic edit
</subsectionHeader>
<bodyText confidence="0.9998849">
Our baseline, the edit machine described in (Ban-
galore and Johnston, 2004), is essentially a finite-
state implementation of the algorithm to compute
the Levenshtein distance. It allows for unlimited
insertion, deletion, and substitution of any word
for another (Figure 5). The costs of insertion, dele-
tion, and substitution are set as equal, except for
members of classes such as price (cheap, expen-
sive), cuisine (turkish) etc., which are assigned a
higher cost for deletion and substitution.
</bodyText>
<figure confidence="0.493861">
wi:wj /scost
wi :wi /0
</figure>
<figureCaption confidence="0.993929">
Figure 5: Basic Edit Machine
</figureCaption>
<subsectionHeader confidence="0.633408">
5.2 4-edit
</subsectionHeader>
<bodyText confidence="0.999445785714286">
Basic edit is effective in increasing the number of
strings that are assigned an interpretation (Banga-
lore and Johnston, 2004) but is quite large (15mb,
1 state, 978120 arcs) and adds an unacceptable
amount of latency (5s on average). In order to
overcome this performance problem we experi-
mented with revising the topology of the edit ma-
chine so that it allows only a limited number of
edit operations (at most four) and removed the
substitution arcs, since they give rise to
arcs. For the same grammar, the resulting edit ma-
chine is about 300K with 4 states and 16796 arcs
and the average latency is (0.5s). The topology of
the 4-edit machine is shown in Figure 6.
</bodyText>
<figureCaption confidence="0.977068">
Figure 6: 4-edit machine
</figureCaption>
<subsectionHeader confidence="0.995257">
5.3 Smart edit
</subsectionHeader>
<bodyText confidence="0.988235271186441">
Smart edit is a 4-edit machine which incorporates
a number of additional heuristics and refinements
to improve performance:
1. Deletion of SLM-only words: Arcs were
added to the edit transducer to allow for free
deletion of any words in the SLM training
data which are not found in the grammar. For
example, listings in thai restaurant listings in
midtown thai restaurant in midtown.
2. Deletion of doubled words: A common er-
ror observed in SLM output was doubling of
monosyllabic words. For example: subway
to the cloisters recognized as subway to to
the cloisters. Arcs were added to the edit ma-
chine to allow for free deletion of any short
word when preceded by the same word.
3. Extended variable weighting of words: In-
sertion and deletion costs were further subdi-
vided from two to three classes: a low cost
for ‘dispensable’ words, (e.g. please, would,
looking, a, the), a high cost for special words
(slot fillers, e.g. chinese, cheap, downtown),
and a medium cost for all other words, (e.g.
restaurant, find).
4. Auto completion of place names: It is un-
likely that grammar authors will include all
of the different ways to refer to named en-
tities such as place names. For example, if
the grammar includes metropolitan museum
of art the user may just say metropolitan
museum. These changes can involve signif-
icant numbers of edits. A capability was
added to the edit machine to complete par-
tial specifications of place names in a single
edit. This involves a closed world assump-
tion over the set of place names. For ex-
ample, if the only metropolitan museum in
the database is the metropolitan museum of
art we assume that we can insert of art af-
ter metropolitan museum. The algorithm for
construction of these auto-completion edits
enumerates all possible substrings (both con-
tiguous and non-contiguous) for place names.
For each of these it checks to see if the sub-
string is found in more than one semantically
distinct member of the set. If not, an edit se-
quence is added to the edit machine which
freely inserts the words needed to complete
the placename. Figure 7 illustrates one of the
edit transductions that is added for the place
name metropolitan museum of art. The algo-
rithm which generates the autocomplete edits
also generates new strings to add to the place
name class for the SLM (expanded class). In
order to limit over-application of the comple-
tion mechanism substrings starting in prepo-
sitions (of art metropolitan museum of art)
or involving deletion of parts of abbreviations
are not considered for edits (b c building n
</bodyText>
<figure confidence="0.726706">
b c building).
</figure>
<figureCaption confidence="0.732146">
metropolitan: metropolitan museum: museum ε : of ε : art
Figure 7: Auto-completion Edits
</figureCaption>
<equation confidence="0.9750352">
wi: /0 wi :wi/0 wi:w /0 w :wi/0 wi :wi /0
wi i i
ε : /icost
wi
wi
:ε /dcost
ε : wi
wi
:ε
/dcost
/icost
ε : /icost
wi
w :ε /dcost
i
ε: wi
wi :ε/dcost
/icost
wi :e /dcost
e : wi /icost
</equation>
<page confidence="0.99517">
365
</page>
<bodyText confidence="0.999919285714286">
The average latency of SmartEdit is 0.68s. Note
that the application-specific structure and weight-
ing of SmartEdit (3,4 above) can be derived auto-
matically: 4. runs on the placename list for the
new application and the classification in 3. is pri-
marily determined by which words correspond to
fields in the underlying application database.
</bodyText>
<sectionHeader confidence="0.893516" genericHeader="method">
6 Learning Edit Patterns
</sectionHeader>
<bodyText confidence="0.999787">
In the previous section, we described an edit ap-
proach where the weights of the edit operations
have been set by exploiting the constraints from
the underlying application. In this section, we dis-
cuss an approach that learns these weights from
data.
</bodyText>
<subsectionHeader confidence="0.7594825">
6.1 Noisy Channel Model for Error
Correction
</subsectionHeader>
<bodyText confidence="0.999830033333333">
The edit machine serves the purpose of translating
user’s input to a string that can be assigned a mean-
ing representation by the grammar. One of the
possible shortcomings of the approach described
in the preceding section is that the weights for the
edit operations are set heuristically and are crafted
carefully for the particular application. This pro-
cess can be tedious and application-specific. In or-
der to provide a more general approach, we couch
the problem of error correction in the noisy chan-
nel modeling framework. In this regard, we fol-
low (Ringger and Allen, 1996; Ristad and Yian-
ilos, 1998), however, we encode the error cor-
rection model as a weighted Finite State Trans-
ducer (FST) so we can directly edit ASR input
lattices. Furthermore, unlike (Ringger and Allen,
1996), the language grammar from our application
filters out edited strings that cannot be assigned an
interpretation by the multimodal grammar. Also,
while in (Ringger and Allen, 1996) the goal is
to translate to the reference string and improve
recognition accuracy, in our approach the goal is
to translate in order to get the reference meaning
and improve concept accuracy.
We let be the string that can be assigned a
meaning representation by the grammar and be
the user’s input utterance. If we consider to be
the noisy version of the , we view the decoding
task as a search for the string that maximizes
the following equation.
</bodyText>
<equation confidence="0.640129">
(7)
</equation>
<bodyText confidence="0.92965385">
We then use a Markov approximation (trigram
for our purposes) to compute the joint probability
.
.
In order to compute the joint probability, we
need to construct an alignment between tokens
. We use the viterbi alignment provided
by GIZA++ toolkit (Och and Ney, 2003) for this
purpose. We convert the viterbi alignment into a
bilanguage representation that pairs words of the
string with words of . A few examples of
bilanguage strings are shown in Figure 8. We
compute the joint n-gram model using a language
modeling toolkit (Goffin et al., 2005). Equation 8
thus allows us to edit a user’s utterance to a string
that can be interpreted by the grammar.
show:show me:me the:map:of:midtown:midtown
no:find:find me:me french:french restaurants:around down-
town:downtown
I:need:subway:subway directions:directions
</bodyText>
<figureCaption confidence="0.984201">
Figure 8: A few examples of bilanguage strings
</figureCaption>
<subsectionHeader confidence="0.973486">
6.2 Deriving Translation Corpus
</subsectionHeader>
<bodyText confidence="0.999927875">
Since our multimodal grammar is implemented as
a finite-state transducer it is fully reversible and
can be used not just to provide a meaning for input
strings but can also be run in reverse to determine
possible input strings for a given meaning. Our
multimodal corpus was annotated for meaning us-
ing the multimodal annotation tools described in
(Ehlen et al., 2002). In order to train the transla-
tion model we build a corpus that pairs the refer-
ence speech string for each utterance in the train-
ing data with a target string. The target string is de-
rived in two steps. First, the multimodal grammar
is run in reverse on the reference meaning yield-
ing a lattice of possible input strings. Second, the
closest string in the lattice to the reference speech
string is selected as the target string.
</bodyText>
<subsectionHeader confidence="0.997791">
6.3 FST-based Decoder
</subsectionHeader>
<bodyText confidence="0.999855083333333">
In order to facilitate editing of ASR lattices, we
represent the edit model as a weighted finite-state
transducer. We first represent the joint n-gram
model as a finite-state acceptor (Allauzen et al.,
2004). We then interpret the symbols on each
arc of the acceptor as having two components –
a word from user’s utterance (input) and a word
from the edited string (output). This transforma-
tion makes a transducer out of an acceptor. In do-
ing so, we can directly compose the editing model
with ASR lattices to produce a weighted lattice
of edited strings. We further constrain the set of
</bodyText>
<equation confidence="0.499582">
(8)
</equation>
<bodyText confidence="0.793116">
where and
</bodyText>
<page confidence="0.986556">
366
</page>
<bodyText confidence="0.999851833333333">
edited strings to those that are interpretable by
the grammar. We achieve this by composing with
the language finite-state acceptor derived from the
multimodal grammar as shown in Equation 5. Fig-
ure 9 shows the input string and the resulting out-
put after editing with the trained model.
</bodyText>
<tableCaption confidence="0.974054285714286">
Input: I’m trying to find african restaurants
that are located west of midtown
Edited Output: find african around west midtown
Input: I’d like directions subway directions from
the metropolitan museum of art to the empire state building
Edited Output: subway directions from the
metropolitan museum of art to the empire state building
</tableCaption>
<figureCaption confidence="0.992604">
Figure 9: Edited output from the MT edit-model
</figureCaption>
<sectionHeader confidence="0.993377" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999358512820513">
To evaluate the approach, we collected a corpus of
multimodal utterances for the MATCH domain in
a laboratory setting from a set of sixteen first time
users (8 male, 8 female). A total of 833 user inter-
actions (218 multimodal / 491 speech-only / 124
pen-only) resulting from six sample task scenarios
were collected and annotated for speech transcrip-
tion, gesture, and meaning (Ehlen et al., 2002).
These scenarios involved finding restaurants of
various types and getting their names, phone num-
bers, addresses, or reviews, and getting subway
directions between locations. The data collected
was conversational speech where the users ges-
tured and spoke freely.
Since we are concerned here with editing er-
rors out of disfluent, misrecognized or unexpected
speech, we report results on the 709 inputs that in-
volve speech (491 unimodal speech and 218 mul-
timodal). Since there are only a small number of
scenarios performed by all users, we partitioned
the data six ways by scenario. This ensures that
the specific tasks in the test data for each parti-
tion are not also found in the training data for that
partition. For each scenario we built a class-based
trigram language model using the other five sce-
narios as training data. Averaging over the six par-
titions, ASR sentence accuracy was 49% and word
accuracy was 73.4%.
In order to evaluate the understanding perfor-
mance of the different edit machines, for each
partition of the data we first composed the out-
put from speech recognition with the edit machine
and the multimodal grammar, flattened the mean-
ing representation (as described in Section 3.1),
and computed the exact string match accuracy be-
tween the flattened meaning representation and the
reference meaning representation. We then aver-
aged this concept sentence accuracy measure over
all six partitions.
</bodyText>
<table confidence="0.994529888888889">
ConSentAcc
No edits 38.9%
Basic edit 51.5%
4-edit 53.0%
Smart edit 60.2%
Smart edit (lattice) 63.2%
MT-based edit 51.3%
(lattice)
Classifier 34.0%
</table>
<figureCaption confidence="0.964158">
Figure 10: Results of 6-fold cross validation
</figureCaption>
<bodyText confidence="0.999988147058823">
The results are tabulated in Figure 10. The
columns show the concept sentence accuracy
(ConSentAcc) and the relative improvement over
the the baseline of no edits. Compared to the base-
line of 38.9% concept sentence accuracy without
edits (No Edits), Basic Edit gave a relative im-
provement of 32%, yielding 51.5% concept sen-
tence accuracy. 4-edit further improved concept
sentence accuracy (53%) compared to Basic Edit.
The heuristics in Smart Edit brought the concept
sentence accuracy to 60.2%, a 55% improvement
over the baseline. Applying Smart edit to lat-
tice input improved performance from 60.2% to
63.2%.
The MT-based edit model yielded concept sen-
tence accuracy of 51.3% a 31.8% improvement
over the baseline with no edits, but still substan-
tially less than the edit model derived from the
application database. We believe that given the
lack of data for multimodal applications that an
approach that combines the two methods may be
most effective.
The Classification approach yielded only 34.0%
concept sentence accuracy. Unlike MT-based edit
this approach does not have the benefit of compo-
sition with the grammar to guide the understand-
ing process. The low performance of the classi-
fier is most likely due to the small size of the cor-
pus. Also, since the training/test split was by sce-
nario the specifics of the commands differed be-
tween training and test. In future work will ex-
plore the use of other classification techniques and
try combining the annotated data with the gram-
mar for training the classifier model.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999046375">
Robust understanding is a crucial feature of a
practical conversational system whether spoken
or multimodal. There have been two main ap-
proaches to addressing this issue for speech-only
dialog systems. In this paper, we present an al-
ternative approach based on edit machines that is
more suitable for multimodal systems where gen-
erally very little training data is available and data
</bodyText>
<page confidence="0.993851">
367
</page>
<bodyText confidence="0.99997368">
is costly to collect and annotate. We have shown
how edit machines enable integration of stochas-
tic speech recognition with hand-crafted multi-
modal understanding grammars. The resulting
multimodal understanding system is significantly
more robust 62% relative improvement in perfor-
mance compared to 38.9% concept accuracy with-
out edits. We have also presented an approach to
learning the edit operations and a classification-
based approach. The Learned edit approach pro-
vides a substantial improvement over the baseline,
performing similarly to the Basic edit machine,
but does not perform as well as the application-
tuned Smart edit machine. Given the small size
of the corpus, the classification-based approach
performs less well. This leads us to conclude
that given the lack of data for multimodal applica-
tions a combined strategy may be most effective.
Multimodal grammars coupled with edit machines
derived from the underlying application database
can provide sufficiently robust understanding per-
formance to bootstrap a multimodal service and
as more data become available data-driven tech-
niques such as Learned edit and the classification-
based approach can be brought into play.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999981964285715">
C. Allauzen, M. Mohri, M. Riley, and B. Roark. 2004. A
generalized construction of speech recognition transduc-
ers. In ICASSP, pages 761–764.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson, L. Galescu,
and A. Stent. 2001. Towards Conversational Human-
Computer Interaction. AI Magazine, 22(4), December.
S. Bangalore and M. Johnston. 2000. Tight-coupling of mul-
timodal language processing with speech recognition. In
Proceedings of ICSLP, pages 126–129, Beijing, China.
S. Bangalore and M. Johnston. 2004. Balancing data-driven
and rule-based approaches in the context of a multimodal
conversational system. In Proceedings of HLT-NAACL.
Robert A. Bolt. 1980. ”put-that-there”:voice and gesture at
the graphics interface. Computer Graphics, 14(3):262–
270.
M. Boros, W. Eckert, F. Gallwitz, G. G˘orz, G. Hanrieder, and
H. Niemann. 1996. Towards Understanding Spontaneous
Speech: Word Accuracy vs. Concept Accuracy. In Pro-
ceedings of ICSLP, Philadelphia.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The
Mathematics of Machine Translation: Parameter Estima-
tion. Computational Linguistics, 16(2):263–312.
A. Ciaramella. 1993. A Prototype Performance Evalua-
tion Report. Technical Report WP8000-D3, Project Esprit
2218 SUNDIAL.
Philip R. Cohen, M. Johnston, D. McGee, S. L. Oviatt,
J. Pittman, I. Smith, L. Chen, and J. Clow. 1998. Mul-
timodal interaction for distributed interactive simulation.
In M. Maybury and W. Wahlster, editors, Readings in In-
telligent Interfaces. Morgan Kaufmann Publishers.
J. Dowding, J. M. Gawron, D. E. Appelt, J. Bear, L. Cherny,
R. Moore, and D. B. Moran. 1993. GEMINI: A natural
language system for spoken-language understanding. In
Proceedings of ACL, pages 54–61.
P. Ehlen, M. Johnston, and G. Vasireddy. 2002. Collecting
mobile multimodal data for MATCH. In Proceedings of
ICSLP, Denver, Colorado.
Y. Freund and R. E. Schapire. 1996. Experiments with a new
boosting alogrithm. In Machine Learning: Proceedings of
the Thirteenth International Conference, pages 148–156.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi, and
M. Saraclar. 2005. The at&amp;t watson speech recognizer.
In Proceedings of ICASSP, Philadelphia, PA.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING, pages 369–375, Saarbr¨ucken, Germany.
M. Johnston and S. Bangalore. 2005. Finite-state multi-
modal integration and understanding. Journal of Natural
Language Engineering, 11(2):159–187.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialog systems. In Pro-
ceedings of ACL, pages 376–383, Philadelphia.
J. G. Neal and S. C. Shapiro. 1991. Intelligent multi-media
interface technology. In J. W. Sullivan and S. W. Tyler,
editors, Intelligent User Interfaces, pages 45–68. ACM
Press, Addison Wesley, New York.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19–51.
S. L. Oviatt. 1999. Mutual disambiguation of recognition
errors in a multimodal architecture. In CHI ’99, pages
576–583. ACM Press, New York.
L. Ramshaw and M. P. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings of the
Third Workshop on Very Large Corpora, MIT, Cambridge,
Boston.
E. K. Ringger and J. F. Allen. 1996. A fertility channel
model for post-correction of continuous speech recogni-
tion. In ICSLP.
E. S. Ristad and P. N. Yianilos. 1998. Learning string-edit
distance. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 20(5):522–532.
W. Wahlster. 2002. SmartKom: Fusion and fission of speech,
gestures, and facial expressions. In Proceedings of the 1st
International Workshop on Man-Machine Symbiotic Sys-
tems, pages 213–225, Kyoto, Japan.
Y. Wang, A. Acero, C. Chelba, B. Frey, and L. Wong. 2002.
Combination of statistical and rule-based approaches for
spoken language understanding. In Proceedings of the IC-
SLP, Denver, Colorado, September.
W. Ward. 1991. Understanding spontaneous speech: the
phoenix system. In ICASSP.
</reference>
<page confidence="0.998263">
368
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911533">
<title confidence="0.998945">Edit Machines for Robust Multimodal Language Processing</title>
<author confidence="0.973424">Srinivas Bangalore</author>
<affiliation confidence="0.999716">AT&amp;T Labs-Research</affiliation>
<address confidence="0.997797">180 Park Ave Florham Park, NJ 07932</address>
<email confidence="0.999798">srini@research.att.com</email>
<author confidence="0.999954">Michael Johnston</author>
<affiliation confidence="0.99995">AT&amp;T Labs-Research</affiliation>
<address confidence="0.9981305">180 Park Ave Florham Park, NJ 07932</address>
<email confidence="0.999864">johnston@research.att.com</email>
<abstract confidence="0.997757307692308">Multimodal grammars provide an expressive formalism for multimodal integration and understanding. However, handcrafted multimodal grammars can be brittle with respect to unexpected, erroneous, or disfluent inputs. Spoken language (speech-only) understanding systems have addressed this issue of lack of robustness of hand-crafted grammars by exploiting classification techniques to extract fillers of a frame representation. In this paper, we illustrate the limitations of such classification approaches for multimodal integration and understanding and present an approach based on edit machines that combine the expressiveness of multimodal grammars with the robustness of stochastic language models of speech recognition. We also present an approach where the edit operations are trained from data using a noisy channel model paradigm. We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Allauzen</author>
<author>M Mohri</author>
<author>M Riley</author>
<author>B Roark</author>
</authors>
<title>A generalized construction of speech recognition transducers.</title>
<date>2004</date>
<booktitle>In ICASSP,</booktitle>
<pages>761--764</pages>
<contexts>
<context position="25553" citStr="Allauzen et al., 2004" startWordPosition="4136" endWordPosition="4139">odel we build a corpus that pairs the reference speech string for each utterance in the training data with a target string. The target string is derived in two steps. First, the multimodal grammar is run in reverse on the reference meaning yielding a lattice of possible input strings. Second, the closest string in the lattice to the reference speech string is selected as the target string. 6.3 FST-based Decoder In order to facilitate editing of ASR lattices, we represent the edit model as a weighted finite-state transducer. We first represent the joint n-gram model as a finite-state acceptor (Allauzen et al., 2004). We then interpret the symbols on each arc of the acceptor as having two components – a word from user’s utterance (input) and a word from the edited string (output). This transformation makes a transducer out of an acceptor. In doing so, we can directly compose the editing model with ASR lattices to produce a weighted lattice of edited strings. We further constrain the set of (8) where and 366 edited strings to those that are interpretable by the grammar. We achieve this by composing with the language finite-state acceptor derived from the multimodal grammar as shown in Equation 5. Figure 9 </context>
</contexts>
<marker>Allauzen, Mohri, Riley, Roark, 2004</marker>
<rawString>C. Allauzen, M. Mohri, M. Riley, and B. Roark. 2004. A generalized construction of speech recognition transducers. In ICASSP, pages 761–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>D Byron</author>
<author>M Dzikovska</author>
<author>G Ferguson</author>
<author>L Galescu</author>
<author>A Stent</author>
</authors>
<title>Towards Conversational HumanComputer Interaction.</title>
<date>2001</date>
<journal>AI Magazine,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="3328" citStr="Allen et al., 2001" startWordPosition="490" endWordPosition="493">lthough the corpus-driven language model might recognize a user’s utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar. There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature. First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct a (partial) semantic representation (Dowding et al., 1993; Allen et al., 2001; Ward, 1991). Second, a classification-based approach views the problem of understanding as extracting certain bits of information from the input. It attempts to classify the utterance and identifies substrings of the input as slot-filler values to construct a frame-like semantic representation. Both approaches have shortcomings. Although in the first approach, the grammar can encode richer semantic representations, the method for combining the fragmented parses is quite ad hoc. In the second approach, the robustness is derived from training classifiers on annotated data, this data is very ex</context>
</contexts>
<marker>Allen, Byron, Dzikovska, Ferguson, Galescu, Stent, 2001</marker>
<rawString>J. Allen, D. Byron, M. Dzikovska, G. Ferguson, L. Galescu, and A. Stent. 2001. Towards Conversational HumanComputer Interaction. AI Magazine, 22(4), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>M Johnston</author>
</authors>
<title>Tight-coupling of multimodal language processing with speech recognition.</title>
<date>2000</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<pages>126--129</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2028" citStr="Bangalore and Johnston, 2000" startWordPosition="287" endWordPosition="291">ech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. However, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and disfluent input. For speech recognition, a corpus-driven stochastic language model (SLM) with smoothing or a combination of grammarbased and -gram model (Bangalore and Johnston, 2004; Wang et al., 2002) can </context>
<context position="7465" citStr="Bangalore and Johnston, 2000" startWordPosition="1155" endWordPosition="1158">ponds with graphical labels on the display, synchronized with synthetic speech output. For example, if the user says phone numbers for these two restaurants and circles two restaurants as in Figure 2 [A], the system will draw a callout with the restaurant name and number and say, for example Time Cafe can be reached at 212- 533-7000, for each restaurant in turn (Figure 2 [B]). Figure 2: MATCH Example 3 Finite-state Multimodal Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al., 2002) is an extension of the finite-state approach previously proposed in (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005). In this approach, a declarative multimodal grammar captures both the structure and the interpretation of multimodal and unimodal commands. The grammar consists of a set of context-free rules. The multimodal aspects of the grammar become apparent in the terminals, each of which is a triple W:G:M, consisting of speech (words, W), gesture (gesture symbols, G), and meaning (meaning symbols, M). The multimodal grammar encodes not just multimodal integration patterns but also the syntax of speech and gesture, and the assignment of meaning, here represented in XML. Th</context>
<context position="15294" citStr="Bangalore and Johnston, 2000" startWordPosition="2411" endWordPosition="2414">ge processing requires the integration and joint interpretation of speech and gesture input. Multimodal integration requires alignment of the speech and gesture input. Given that the input modalities are both noisy and can receive multiple within-modality interpretations (e.g. a circle could be an “O” or an area gesture); it is necessary for the input to be represented as a multiplicity of hypotheses, which can be most compactly represented as a lattice. The multiplicity of hypotheses is also required for exploiting the mutual compensation between the two modalities as shown in (Oviatt, 1999; Bangalore and Johnston, 2000). Furthermore, in order to provide the dialog manager the best opportunity to recover the most appropriate meaning given the dialog context, we construct a lattice of semantic representations instead of providing only one semantic representation. In the multimodal grammar-based approach, the alignment between speech and gesture along with their combined interpretation is utilized in deriving the multimodal finite-state transducers. These transducers are used to create a gesture-speech aligned lattice and a lattice of semantic interpretations. However, in the classification-based approach, it i</context>
</contexts>
<marker>Bangalore, Johnston, 2000</marker>
<rawString>S. Bangalore and M. Johnston. 2000. Tight-coupling of multimodal language processing with speech recognition. In Proceedings of ICSLP, pages 126–129, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>M Johnston</author>
</authors>
<title>Balancing data-driven and rule-based approaches in the context of a multimodal conversational system.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2603" citStr="Bangalore and Johnston, 2004" startWordPosition="372" endWordPosition="376">ntic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. However, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and disfluent input. For speech recognition, a corpus-driven stochastic language model (SLM) with smoothing or a combination of grammarbased and -gram model (Bangalore and Johnston, 2004; Wang et al., 2002) can be built in order to overcome the brittleness of a grammar-based language model. Although the corpus-driven language model might recognize a user’s utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar. There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature. First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in i</context>
<context position="11787" citStr="Bangalore and Johnston, 2004" startWordPosition="1825" endWordPosition="1828">en language understanding literature. The issue of noisy output from the speech recognizer and disfluencies that are inherent in spoken input make it imperative for using mechanisms to provide robust understanding. As discussed in the introduction, there are two approaches to addressing robustness – partial parsing approach and classification approach. We have explored the classification-based approach to multimodal understanding in earlier work. We briefly present this approach and discuss its limitations for multimodal language processing. 4.1 Classification-based Approach In previous work (Bangalore and Johnston, 2004), we viewed multimodal understanding as a sequence of classification problems in order to determine the predicate and arguments of an utterance. The meaning representation shown in (1) consists of an predicate (the command attribute) and a sequence of one or more argument attributes which are the parameters for the successful interpretation of the user’s intent. For example, in (1), is the predicate and is the set of arguments to the predicate. We determine the predicate ( ) for a token multimodal utterance ( ) by maximizing the posterior probability as shown in Equation 2. (2) We view the pro</context>
<context position="17166" citStr="Bangalore and Johnston, 2004" startWordPosition="2711" endWordPosition="2714">ar ( ), we introduce a weighted finite-state edit transducer to the multimodal language processing cascade. This transducer coerces the set of strings ( ) encoded in the lattice resulting from ASR ( ) to closest strings in the grammar that can be assigned an interpretation. We are interested in the string with the least costly number of edits ( ) that can be assigned an interpretation by the grammars. This can be achieved by composition () of transducers followed by a search for the least cost path through a weighted transducer as shown below. We first describe the edit machine introduced in (Bangalore and Johnston, 2004) (Basic Edit) then go on to describe a smaller edit machine with higher performance (4-edit) and an edit machine &apos;We note that the closest string according to the edit metric may not be the closest string in meaning 364 which incorporates additional heuristics (Smart edit). 5.1 Basic edit Our baseline, the edit machine described in (Bangalore and Johnston, 2004), is essentially a finitestate implementation of the algorithm to compute the Levenshtein distance. It allows for unlimited insertion, deletion, and substitution of any word for another (Figure 5). The costs of insertion, deletion, and </context>
</contexts>
<marker>Bangalore, Johnston, 2004</marker>
<rawString>S. Bangalore and M. Johnston. 2004. Balancing data-driven and rule-based approaches in the context of a multimodal conversational system. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Bolt</author>
</authors>
<title>put-that-there”:voice and gesture at the graphics interface.</title>
<date>1980</date>
<journal>Computer Graphics,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>270</pages>
<contexts>
<context position="1468" citStr="Bolt, 1980" startWordPosition="209" endWordPosition="210">hat combine the expressiveness of multimodal grammars with the robustness of stochastic language models of speech recognition. We also present an approach where the edit operations are trained from data using a noisy channel model paradigm. We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 1 Introduction Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have</context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>Robert A. Bolt. 1980. ”put-that-there”:voice and gesture at the graphics interface. Computer Graphics, 14(3):262– 270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Boros</author>
<author>W Eckert</author>
<author>F Gallwitz</author>
<author>G G˘orz</author>
<author>G Hanrieder</author>
<author>H Niemann</author>
</authors>
<title>Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept Accuracy.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<location>Philadelphia.</location>
<marker>Boros, Eckert, Gallwitz, G˘orz, Hanrieder, Niemann, 1996</marker>
<rawString>M. Boros, W. Eckert, F. Gallwitz, G. G˘orz, G. Hanrieder, and H. Niemann. 1996. Towards Understanding Spontaneous Speech: Word Accuracy vs. Concept Accuracy. In Proceedings of ICSLP, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>R Mercer</author>
</authors>
<title>The Mathematics of Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="5311" citStr="Brown et al., 1993" startWordPosition="804" endWordPosition="807"> Figure 1: Editing Example In this paper, we develop further this edit-based approach to finite-state multimodal language understanding and show how when appropriately tuned it can provide a substantial improvement in concept accuracy. We also explore learning edits from data and present an approach of modeling this process as a machine translation problem. We learn a model to translate from out of grammar or misrecognized language (such as ‘ASR:’ above) to the closest language the system can understand (‘Grammar:’ above). To this end, we adopt techniques from statistical machine translation (Brown et al., 1993; Och and Ney, 2003) and use statistical alignment to learn the edit patterns. Here we evaluate these different techniques on data from the MATCH multimodal conversational system (Johnston et al., 2002) but the same techniques are more broadly applicable to spoken language systems in general whether unimodal or multimodal. The layout of the paper is as follows. In Sections 2 and 3, we briefly describe the MATCH application and the finite-state approach to multimodal language understanding. In Section 4, we discuss the limitations of the methods used for robust understanding in spoken language </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics, 16(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ciaramella</author>
</authors>
<title>A Prototype Performance Evaluation Report.</title>
<date>1993</date>
<tech>Technical Report WP8000-D3, Project Esprit 2218 SUNDIAL.</tech>
<contexts>
<context position="10518" citStr="Ciaramella, 1993" startWordPosition="1641" endWordPosition="1642">or use in our system, the multimodal grammar is compiled into a cascade of finite-state transducers (Johnston and Bangalore, 2000; Johnston et al., 2002; Johnston and Bangalore, 2005). As a result, processing of lattice inputs from speech and gesture processing is straightforward and efficient. 3.1 Meaning Representation for Concept Accuracy The hierarchically nested XML representation above is effective for processing by the backend application, but is not well suited for the automated determination of the performance of the language understanding mechanism. We adopt an approach, similar to (Ciaramella, 1993; Boros et al., 1996), in which the meaning representation, in our case XML, is transformed into a sorted flat list of attribute-value pairs indicating the core contentful concepts of each command. The example above yields: (1) This allows us to calculate the performance of the understanding component using the same string matching metrics used for speech recognition accuracy. Concept Sentence Accuracy measures the number of user inputs for which the system got the meaning completely right (this is called Sentence Understanding in (Ciaramella, 1993)). 4 Robust Understanding Robust understandin</context>
</contexts>
<marker>Ciaramella, 1993</marker>
<rawString>A. Ciaramella. 1993. A Prototype Performance Evaluation Report. Technical Report WP8000-D3, Project Esprit 2218 SUNDIAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>M Johnston</author>
<author>D McGee</author>
<author>S L Oviatt</author>
<author>J Pittman</author>
<author>I Smith</author>
<author>L Chen</author>
<author>J Clow</author>
</authors>
<title>Multimodal interaction for distributed interactive simulation.</title>
<date>1998</date>
<booktitle>Readings in Intelligent Interfaces.</booktitle>
<editor>In M. Maybury and W. Wahlster, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="1534" citStr="Cohen et al., 1998" startWordPosition="217" endWordPosition="220"> the robustness of stochastic language models of speech recognition. We also present an approach where the edit operations are trained from data using a noisy channel model paradigm. We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 1 Introduction Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transd</context>
</contexts>
<marker>Cohen, Johnston, McGee, Oviatt, Pittman, Smith, Chen, Clow, 1998</marker>
<rawString>Philip R. Cohen, M. Johnston, D. McGee, S. L. Oviatt, J. Pittman, I. Smith, L. Chen, and J. Clow. 1998. Multimodal interaction for distributed interactive simulation. In M. Maybury and W. Wahlster, editors, Readings in Intelligent Interfaces. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dowding</author>
<author>J M Gawron</author>
<author>D E Appelt</author>
<author>J Bear</author>
<author>L Cherny</author>
<author>R Moore</author>
<author>D B Moran</author>
</authors>
<title>GEMINI: A natural language system for spoken-language understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>54--61</pages>
<contexts>
<context position="3308" citStr="Dowding et al., 1993" startWordPosition="486" endWordPosition="489">ased language model. Although the corpus-driven language model might recognize a user’s utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar. There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature. First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct a (partial) semantic representation (Dowding et al., 1993; Allen et al., 2001; Ward, 1991). Second, a classification-based approach views the problem of understanding as extracting certain bits of information from the input. It attempts to classify the utterance and identifies substrings of the input as slot-filler values to construct a frame-like semantic representation. Both approaches have shortcomings. Although in the first approach, the grammar can encode richer semantic representations, the method for combining the fragmented parses is quite ad hoc. In the second approach, the robustness is derived from training classifiers on annotated data, </context>
</contexts>
<marker>Dowding, Gawron, Appelt, Bear, Cherny, Moore, Moran, 1993</marker>
<rawString>J. Dowding, J. M. Gawron, D. E. Appelt, J. Bear, L. Cherny, R. Moore, and D. B. Moran. 1993. GEMINI: A natural language system for spoken-language understanding. In Proceedings of ACL, pages 54–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ehlen</author>
<author>M Johnston</author>
<author>G Vasireddy</author>
</authors>
<title>Collecting mobile multimodal data for MATCH.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="24894" citStr="Ehlen et al., 2002" startWordPosition="4022" endWordPosition="4025"> by the grammar. show:show me:me the:map:of:midtown:midtown no:find:find me:me french:french restaurants:around downtown:downtown I:need:subway:subway directions:directions Figure 8: A few examples of bilanguage strings 6.2 Deriving Translation Corpus Since our multimodal grammar is implemented as a finite-state transducer it is fully reversible and can be used not just to provide a meaning for input strings but can also be run in reverse to determine possible input strings for a given meaning. Our multimodal corpus was annotated for meaning using the multimodal annotation tools described in (Ehlen et al., 2002). In order to train the translation model we build a corpus that pairs the reference speech string for each utterance in the training data with a target string. The target string is derived in two steps. First, the multimodal grammar is run in reverse on the reference meaning yielding a lattice of possible input strings. Second, the closest string in the lattice to the reference speech string is selected as the target string. 6.3 FST-based Decoder In order to facilitate editing of ASR lattices, we represent the edit model as a weighted finite-state transducer. We first represent the joint n-gr</context>
<context position="27044" citStr="Ehlen et al., 2002" startWordPosition="4387" endWordPosition="4390">politan museum of art to the empire state building Edited Output: subway directions from the metropolitan museum of art to the empire state building Figure 9: Edited output from the MT edit-model 7 Experiments and Results To evaluate the approach, we collected a corpus of multimodal utterances for the MATCH domain in a laboratory setting from a set of sixteen first time users (8 male, 8 female). A total of 833 user interactions (218 multimodal / 491 speech-only / 124 pen-only) resulting from six sample task scenarios were collected and annotated for speech transcription, gesture, and meaning (Ehlen et al., 2002). These scenarios involved finding restaurants of various types and getting their names, phone numbers, addresses, or reviews, and getting subway directions between locations. The data collected was conversational speech where the users gestured and spoke freely. Since we are concerned here with editing errors out of disfluent, misrecognized or unexpected speech, we report results on the 709 inputs that involve speech (491 unimodal speech and 218 multimodal). Since there are only a small number of scenarios performed by all users, we partitioned the data six ways by scenario. This ensures that</context>
</contexts>
<marker>Ehlen, Johnston, Vasireddy, 2002</marker>
<rawString>P. Ehlen, M. Johnston, and G. Vasireddy. 2002. Collecting mobile multimodal data for MATCH. In Proceedings of ICSLP, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a new boosting alogrithm.</title>
<date>1996</date>
<booktitle>In Machine Learning: Proceedings of the Thirteenth International Conference,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="13901" citStr="Freund and Schapire, 1996" startWordPosition="2191" endWordPosition="2194">hai cuisine B Encoding upper place I west place I side place I Table 1: TheI,O,B encoding for argument extraction. Given this encoding, the problem of extracting the arguments is a search for the most likely sequence of tags ( ) given the input multimodal utterance as shown in Equation (3). We approximate the posterior probability using independence assumptions as shown in Equation (4). Owing to the large set of features that are used for predicate identification and argument extraction, we estimate the probabilities using a classification model. In particular, we use the Adaboost classifier (Freund and Schapire, 1996) wherein a highly accurate classifier is build by combining many “weak” or “simple” base classifiers, each of which may only be moderately accurate. The selection of the weak classifiers proceeds iteratively picking the weak classifier that correctly classifies the examples that are misclassified by the previously selected weak classifiers. Each weak classifier is associated with a weight ( ) that reflects its contribution towards minimizing the classification error. The posterior probability of is computed as in Equation 5. 4.2 Limitations of this approach Although, we have shown that the cla</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R. E. Schapire. 1996. Experiments with a new boosting alogrithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goffin</author>
<author>C Allauzen</author>
<author>E Bocchieri</author>
<author>D Hakkani-Tur</author>
<author>A Ljolje</author>
<author>S Parthasarathy</author>
<author>M Rahim</author>
<author>G Riccardi</author>
<author>M Saraclar</author>
</authors>
<title>The at&amp;t watson speech recognizer.</title>
<date>2005</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="24185" citStr="Goffin et al., 2005" startWordPosition="3917" endWordPosition="3920"> as a search for the string that maximizes the following equation. (7) We then use a Markov approximation (trigram for our purposes) to compute the joint probability . . In order to compute the joint probability, we need to construct an alignment between tokens . We use the viterbi alignment provided by GIZA++ toolkit (Och and Ney, 2003) for this purpose. We convert the viterbi alignment into a bilanguage representation that pairs words of the string with words of . A few examples of bilanguage strings are shown in Figure 8. We compute the joint n-gram model using a language modeling toolkit (Goffin et al., 2005). Equation 8 thus allows us to edit a user’s utterance to a string that can be interpreted by the grammar. show:show me:me the:map:of:midtown:midtown no:find:find me:me french:french restaurants:around downtown:downtown I:need:subway:subway directions:directions Figure 8: A few examples of bilanguage strings 6.2 Deriving Translation Corpus Since our multimodal grammar is implemented as a finite-state transducer it is fully reversible and can be used not just to provide a meaning for input strings but can also be run in reverse to determine possible input strings for a given meaning. Our multim</context>
</contexts>
<marker>Goffin, Allauzen, Bocchieri, Hakkani-Tur, Ljolje, Parthasarathy, Rahim, Riccardi, Saraclar, 2005</marker>
<rawString>V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi, and M. Saraclar. 2005. The at&amp;t watson speech recognizer. In Proceedings of ICASSP, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
</authors>
<title>Finite-state multimodal parsing and understanding.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>369--375</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="1735" citStr="Johnston and Bangalore, 2000" startWordPosition="243" endWordPosition="247">ate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 1 Introduction Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. However, like other approaches based on handcrafted grammars</context>
<context position="10031" citStr="Johnston and Bangalore, 2000" startWordPosition="1568" endWordPosition="1571">n re-integrating specific gestural content and our approach to the representation of complex gestures see (Johnston et al., 2002). The multimodal grammar (Figure 3) expresses the relationship between what the user said, what they drew with the pen, and their combined meaning, in this case Figure 4 (Meaning). The meaning is generated by concatenating the meaning symbols and replacing SEM with the appropriate specific content: cmd info type phone /type obj rest [r12,r15] /rest /obj /info /cmd . For use in our system, the multimodal grammar is compiled into a cascade of finite-state transducers (Johnston and Bangalore, 2000; Johnston et al., 2002; Johnston and Bangalore, 2005). As a result, processing of lattice inputs from speech and gesture processing is straightforward and efficient. 3.1 Meaning Representation for Concept Accuracy The hierarchically nested XML representation above is effective for processing by the backend application, but is not well suited for the automated determination of the performance of the language understanding mechanism. We adopt an approach, similar to (Ciaramella, 1993; Boros et al., 1996), in which the meaning representation, in our case XML, is transformed into a sorted flat li</context>
</contexts>
<marker>Johnston, Bangalore, 2000</marker>
<rawString>M. Johnston and S. Bangalore. 2000. Finite-state multimodal parsing and understanding. In Proceedings of COLING, pages 369–375, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
</authors>
<title>Finite-state multimodal integration and understanding.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="2059" citStr="Johnston and Bangalore, 2005" startWordPosition="292" endWordPosition="296">r example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. However, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and disfluent input. For speech recognition, a corpus-driven stochastic language model (SLM) with smoothing or a combination of grammarbased and -gram model (Bangalore and Johnston, 2004; Wang et al., 2002) can be built in order to overcome t</context>
<context position="7496" citStr="Johnston and Bangalore, 2005" startWordPosition="1159" endWordPosition="1163"> the display, synchronized with synthetic speech output. For example, if the user says phone numbers for these two restaurants and circles two restaurants as in Figure 2 [A], the system will draw a callout with the restaurant name and number and say, for example Time Cafe can be reached at 212- 533-7000, for each restaurant in turn (Figure 2 [B]). Figure 2: MATCH Example 3 Finite-state Multimodal Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al., 2002) is an extension of the finite-state approach previously proposed in (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005). In this approach, a declarative multimodal grammar captures both the structure and the interpretation of multimodal and unimodal commands. The grammar consists of a set of context-free rules. The multimodal aspects of the grammar become apparent in the terminals, each of which is a triple W:G:M, consisting of speech (words, W), gesture (gesture symbols, G), and meaning (meaning symbols, M). The multimodal grammar encodes not just multimodal integration patterns but also the syntax of speech and gesture, and the assignment of meaning, here represented in XML. The symbol SEM is used to abstrac</context>
<context position="10085" citStr="Johnston and Bangalore, 2005" startWordPosition="1576" endWordPosition="1579">proach to the representation of complex gestures see (Johnston et al., 2002). The multimodal grammar (Figure 3) expresses the relationship between what the user said, what they drew with the pen, and their combined meaning, in this case Figure 4 (Meaning). The meaning is generated by concatenating the meaning symbols and replacing SEM with the appropriate specific content: cmd info type phone /type obj rest [r12,r15] /rest /obj /info /cmd . For use in our system, the multimodal grammar is compiled into a cascade of finite-state transducers (Johnston and Bangalore, 2000; Johnston et al., 2002; Johnston and Bangalore, 2005). As a result, processing of lattice inputs from speech and gesture processing is straightforward and efficient. 3.1 Meaning Representation for Concept Accuracy The hierarchically nested XML representation above is effective for processing by the backend application, but is not well suited for the automated determination of the performance of the language understanding mechanism. We adopt an approach, similar to (Ciaramella, 1993; Boros et al., 1996), in which the meaning representation, in our case XML, is transformed into a sorted flat list of attribute-value pairs indicating the core conten</context>
</contexts>
<marker>Johnston, Bangalore, 2005</marker>
<rawString>M. Johnston and S. Bangalore. 2005. Finite-state multimodal integration and understanding. Journal of Natural Language Engineering, 11(2):159–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Vasireddy</author>
<author>A Stent</author>
<author>P Ehlen</author>
<author>M Walker</author>
<author>S Whittaker</author>
<author>P Maloor</author>
</authors>
<title>MATCH: An architecture for multimodal dialog systems.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>376--383</pages>
<location>Philadelphia.</location>
<contexts>
<context position="1592" citStr="Johnston et al., 2002" startWordPosition="225" endWordPosition="228"> recognition. We also present an approach where the edit operations are trained from data using a noisy channel model paradigm. We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 1 Introduction Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from </context>
<context position="5513" citStr="Johnston et al., 2002" startWordPosition="836" endWordPosition="840">tial improvement in concept accuracy. We also explore learning edits from data and present an approach of modeling this process as a machine translation problem. We learn a model to translate from out of grammar or misrecognized language (such as ‘ASR:’ above) to the closest language the system can understand (‘Grammar:’ above). To this end, we adopt techniques from statistical machine translation (Brown et al., 1993; Och and Ney, 2003) and use statistical alignment to learn the edit patterns. Here we evaluate these different techniques on data from the MATCH multimodal conversational system (Johnston et al., 2002) but the same techniques are more broadly applicable to spoken language systems in general whether unimodal or multimodal. The layout of the paper is as follows. In Sections 2 and 3, we briefly describe the MATCH application and the finite-state approach to multimodal language understanding. In Section 4, we discuss the limitations of the methods used for robust understanding in spoken language understanding literature. In Section 5 we present our approach to building hand-crafted edit machines. In Section 6, we describe our approach to learning the edit operations using a noisy channel paradi</context>
<context position="7367" citStr="Johnston et al., 2002" startWordPosition="1139" endWordPosition="1142">s, or other information about restaurants and subway directions to locations. The system responds with graphical labels on the display, synchronized with synthetic speech output. For example, if the user says phone numbers for these two restaurants and circles two restaurants as in Figure 2 [A], the system will draw a callout with the restaurant name and number and say, for example Time Cafe can be reached at 212- 533-7000, for each restaurant in turn (Figure 2 [B]). Figure 2: MATCH Example 3 Finite-state Multimodal Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al., 2002) is an extension of the finite-state approach previously proposed in (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005). In this approach, a declarative multimodal grammar captures both the structure and the interpretation of multimodal and unimodal commands. The grammar consists of a set of context-free rules. The multimodal aspects of the grammar become apparent in the terminals, each of which is a triple W:G:M, consisting of speech (words, W), gesture (gesture symbols, G), and meaning (meaning symbols, M). The multimodal grammar encodes not just multimodal integration patterns but</context>
<context position="9532" citStr="Johnston et al., 2002" startWordPosition="1484" endWordPosition="1487">l: NUM HEADPL DDETPL these:G: those:G: HEADPL restaurants:rest: rest :SEM:SEM :: /rest NUM two:2: three:3:... ten:10: Figure 3: Multimodal grammar fragment Figure 4: Multimodal Example is either a selection of two restaurants or a geographical area. In Figure 4 (Gesture) the specific content is indicated in parentheses after SEM. This content is removed before multimodal parsing and integration and replaced afterwards. For detailed explanation of our technique for abstracting over and then re-integrating specific gestural content and our approach to the representation of complex gestures see (Johnston et al., 2002). The multimodal grammar (Figure 3) expresses the relationship between what the user said, what they drew with the pen, and their combined meaning, in this case Figure 4 (Meaning). The meaning is generated by concatenating the meaning symbols and replacing SEM with the appropriate specific content: cmd info type phone /type obj rest [r12,r15] /rest /obj /info /cmd . For use in our system, the multimodal grammar is compiled into a cascade of finite-state transducers (Johnston and Bangalore, 2000; Johnston et al., 2002; Johnston and Bangalore, 2005). As a result, processing of lattice inputs fro</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen, M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH: An architecture for multimodal dialog systems. In Proceedings of ACL, pages 376–383, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Neal</author>
<author>S C Shapiro</author>
</authors>
<title>Intelligent multi-media interface technology.</title>
<date>1991</date>
<booktitle>Intelligent User Interfaces,</booktitle>
<pages>45--68</pages>
<editor>In J. W. Sullivan and S. W. Tyler, editors,</editor>
<publisher>ACM Press, Addison Wesley,</publisher>
<location>New York.</location>
<contexts>
<context position="1503" citStr="Neal and Shapiro, 1991" startWordPosition="212" endWordPosition="215">iveness of multimodal grammars with the robustness of stochastic language models of speech recognition. We also present an approach where the edit operations are trained from data using a noisy channel model paradigm. We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 1 Introduction Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be co</context>
</contexts>
<marker>Neal, Shapiro, 1991</marker>
<rawString>J. G. Neal and S. C. Shapiro. 1991. Intelligent multi-media interface technology. In J. W. Sullivan and S. W. Tyler, editors, Intelligent User Interfaces, pages 45–68. ACM Press, Addison Wesley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5331" citStr="Och and Ney, 2003" startWordPosition="808" endWordPosition="811">xample In this paper, we develop further this edit-based approach to finite-state multimodal language understanding and show how when appropriately tuned it can provide a substantial improvement in concept accuracy. We also explore learning edits from data and present an approach of modeling this process as a machine translation problem. We learn a model to translate from out of grammar or misrecognized language (such as ‘ASR:’ above) to the closest language the system can understand (‘Grammar:’ above). To this end, we adopt techniques from statistical machine translation (Brown et al., 1993; Och and Ney, 2003) and use statistical alignment to learn the edit patterns. Here we evaluate these different techniques on data from the MATCH multimodal conversational system (Johnston et al., 2002) but the same techniques are more broadly applicable to spoken language systems in general whether unimodal or multimodal. The layout of the paper is as follows. In Sections 2 and 3, we briefly describe the MATCH application and the finite-state approach to multimodal language understanding. In Section 4, we discuss the limitations of the methods used for robust understanding in spoken language understanding litera</context>
<context position="23904" citStr="Och and Ney, 2003" startWordPosition="3869" endWordPosition="3872">goal is to translate in order to get the reference meaning and improve concept accuracy. We let be the string that can be assigned a meaning representation by the grammar and be the user’s input utterance. If we consider to be the noisy version of the , we view the decoding task as a search for the string that maximizes the following equation. (7) We then use a Markov approximation (trigram for our purposes) to compute the joint probability . . In order to compute the joint probability, we need to construct an alignment between tokens . We use the viterbi alignment provided by GIZA++ toolkit (Och and Ney, 2003) for this purpose. We convert the viterbi alignment into a bilanguage representation that pairs words of the string with words of . A few examples of bilanguage strings are shown in Figure 8. We compute the joint n-gram model using a language modeling toolkit (Goffin et al., 2005). Equation 8 thus allows us to edit a user’s utterance to a string that can be interpreted by the grammar. show:show me:me the:map:of:midtown:midtown no:find:find me:me french:french restaurants:around downtown:downtown I:need:subway:subway directions:directions Figure 8: A few examples of bilanguage strings 6.2 Deriv</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Mutual disambiguation of recognition errors in a multimodal architecture.</title>
<date>1999</date>
<booktitle>In CHI ’99,</booktitle>
<pages>576--583</pages>
<publisher>ACM Press,</publisher>
<location>New York.</location>
<contexts>
<context position="15263" citStr="Oviatt, 1999" startWordPosition="2409" endWordPosition="2410">timodal language processing requires the integration and joint interpretation of speech and gesture input. Multimodal integration requires alignment of the speech and gesture input. Given that the input modalities are both noisy and can receive multiple within-modality interpretations (e.g. a circle could be an “O” or an area gesture); it is necessary for the input to be represented as a multiplicity of hypotheses, which can be most compactly represented as a lattice. The multiplicity of hypotheses is also required for exploiting the mutual compensation between the two modalities as shown in (Oviatt, 1999; Bangalore and Johnston, 2000). Furthermore, in order to provide the dialog manager the best opportunity to recover the most appropriate meaning given the dialog context, we construct a lattice of semantic representations instead of providing only one semantic representation. In the multimodal grammar-based approach, the alignment between speech and gesture along with their combined interpretation is utilized in deriving the multimodal finite-state transducers. These transducers are used to create a gesture-speech aligned lattice and a lattice of semantic interpretations. However, in the clas</context>
</contexts>
<marker>Oviatt, 1999</marker>
<rawString>S. L. Oviatt. 1999. Mutual disambiguation of recognition errors in a multimodal architecture. In CHI ’99, pages 576–583. ACM Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora, MIT,</booktitle>
<location>Cambridge, Boston.</location>
<contexts>
<context position="12955" citStr="Ramshaw and Marcus, 1995" startWordPosition="2025" endWordPosition="2029">r probability as shown in Equation 2. (2) We view the problem of identifying and extracting arguments from a multimodal input as a problem of associating each token of the input with a specific tag that encodes the label of the argument and the span of the argument. These tags are drawn from a tagset which is constructed by for two these restaurants phone ten Speech: G area loc SEM(points...) Gesture: sel 2 SEM(r12,r15) &lt;cmd&gt; &lt;info&gt; &lt;rest&gt; r12,r15 &lt;rest&gt; &lt;type&gt; &lt;/rest&gt; &lt;/obj&gt; &lt;/info&gt; phone &lt;/type&gt; &lt;obj&gt; &lt;/cmd&gt; Meaning: 363 extending each argument label by three additional symbols , following (Ramshaw and Marcus, 1995). These symbols correspond to cases when a token is inside () an argument span, outside ( ) an argument span or at the boundary of two argument spans ( ) (See Table 1). User cheap thai upper west side Utterance Argument price cheap /price cuisine Annotation thai /cuisine place upper west side /place IOB cheap price B thai cuisine B Encoding upper place I west place I side place I Table 1: TheI,O,B encoding for argument extraction. Given this encoding, the problem of extracting the arguments is a search for the most likely sequence of tags ( ) given the input multimodal utterance as shown in Eq</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, MIT, Cambridge, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E K Ringger</author>
<author>J F Allen</author>
</authors>
<title>A fertility channel model for post-correction of continuous speech recognition.</title>
<date>1996</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="22793" citStr="Ringger and Allen, 1996" startWordPosition="3680" endWordPosition="3683"> 6.1 Noisy Channel Model for Error Correction The edit machine serves the purpose of translating user’s input to a string that can be assigned a meaning representation by the grammar. One of the possible shortcomings of the approach described in the preceding section is that the weights for the edit operations are set heuristically and are crafted carefully for the particular application. This process can be tedious and application-specific. In order to provide a more general approach, we couch the problem of error correction in the noisy channel modeling framework. In this regard, we follow (Ringger and Allen, 1996; Ristad and Yianilos, 1998), however, we encode the error correction model as a weighted Finite State Transducer (FST) so we can directly edit ASR input lattices. Furthermore, unlike (Ringger and Allen, 1996), the language grammar from our application filters out edited strings that cannot be assigned an interpretation by the multimodal grammar. Also, while in (Ringger and Allen, 1996) the goal is to translate to the reference string and improve recognition accuracy, in our approach the goal is to translate in order to get the reference meaning and improve concept accuracy. We let be the stri</context>
</contexts>
<marker>Ringger, Allen, 1996</marker>
<rawString>E. K. Ringger and J. F. Allen. 1996. A fertility channel model for post-correction of continuous speech recognition. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
<author>P N Yianilos</author>
</authors>
<title>Learning string-edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="22821" citStr="Ristad and Yianilos, 1998" startWordPosition="3684" endWordPosition="3688">for Error Correction The edit machine serves the purpose of translating user’s input to a string that can be assigned a meaning representation by the grammar. One of the possible shortcomings of the approach described in the preceding section is that the weights for the edit operations are set heuristically and are crafted carefully for the particular application. This process can be tedious and application-specific. In order to provide a more general approach, we couch the problem of error correction in the noisy channel modeling framework. In this regard, we follow (Ringger and Allen, 1996; Ristad and Yianilos, 1998), however, we encode the error correction model as a weighted Finite State Transducer (FST) so we can directly edit ASR input lattices. Furthermore, unlike (Ringger and Allen, 1996), the language grammar from our application filters out edited strings that cannot be assigned an interpretation by the multimodal grammar. Also, while in (Ringger and Allen, 1996) the goal is to translate to the reference string and improve recognition accuracy, in our approach the goal is to translate in order to get the reference meaning and improve concept accuracy. We let be the string that can be assigned a me</context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>E. S. Ristad and P. N. Yianilos. 1998. Learning string-edit distance. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(5):522–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
</authors>
<title>SmartKom: Fusion and fission of speech, gestures, and facial expressions.</title>
<date>2002</date>
<booktitle>In Proceedings of the 1st International Workshop on Man-Machine Symbiotic Systems,</booktitle>
<pages>213--225</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1561" citStr="Wahlster, 2002" startWordPosition="222" endWordPosition="223">anguage models of speech recognition. We also present an approach where the edit operations are trained from data using a noisy channel model paradigm. We evaluate and compare the performance of the hand-crafted and learned edit machines in the context of a multimodal conversational system (MATCH). 1 Introduction Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002). Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000). These grammars support composite multimodal inputs by aligning speech input (words) and gesture input (represented as sequences of gesture symbols) while expressing the relation between the speech and gesture input and their combined semantic representation. In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective pr</context>
</contexts>
<marker>Wahlster, 2002</marker>
<rawString>W. Wahlster. 2002. SmartKom: Fusion and fission of speech, gestures, and facial expressions. In Proceedings of the 1st International Workshop on Man-Machine Symbiotic Systems, pages 213–225, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>A Acero</author>
<author>C Chelba</author>
<author>B Frey</author>
<author>L Wong</author>
</authors>
<title>Combination of statistical and rule-based approaches for spoken language understanding.</title>
<date>2002</date>
<booktitle>In Proceedings of the ICSLP,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="2623" citStr="Wang et al., 2002" startWordPosition="377" endWordPosition="380">lore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities. However, like other approaches based on handcrafted grammars, multimodal grammars can be brittle with respect to extra-grammatical, erroneous and disfluent input. For speech recognition, a corpus-driven stochastic language model (SLM) with smoothing or a combination of grammarbased and -gram model (Bangalore and Johnston, 2004; Wang et al., 2002) can be built in order to overcome the brittleness of a grammar-based language model. Although the corpus-driven language model might recognize a user’s utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar. There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature. First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to n</context>
</contexts>
<marker>Wang, Acero, Chelba, Frey, Wong, 2002</marker>
<rawString>Y. Wang, A. Acero, C. Chelba, B. Frey, and L. Wong. 2002. Combination of statistical and rule-based approaches for spoken language understanding. In Proceedings of the ICSLP, Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ward</author>
</authors>
<title>Understanding spontaneous speech: the phoenix system.</title>
<date>1991</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="3341" citStr="Ward, 1991" startWordPosition="494" endWordPosition="495">riven language model might recognize a user’s utterance correctly, the recognized utterance may not be assigned a semantic representation by the multimodal grammar if the utterance is not part of the grammar. There have been two main approaches to improving robustness of the understanding component in the spoken language understanding literature. First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct a (partial) semantic representation (Dowding et al., 1993; Allen et al., 2001; Ward, 1991). Second, a classification-based approach views the problem of understanding as extracting certain bits of information from the input. It attempts to classify the utterance and identifies substrings of the input as slot-filler values to construct a frame-like semantic representation. Both approaches have shortcomings. Although in the first approach, the grammar can encode richer semantic representations, the method for combining the fragmented parses is quite ad hoc. In the second approach, the robustness is derived from training classifiers on annotated data, this data is very expensive to co</context>
</contexts>
<marker>Ward, 1991</marker>
<rawString>W. Ward. 1991. Understanding spontaneous speech: the phoenix system. In ICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>