<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009491">
<title confidence="0.97511">
Crowdsourcing Annotation of Non-Local Semantic Roles
</title>
<author confidence="0.869011">
Sebastian Pad´o
</author>
<affiliation confidence="0.8130585">
Institut f¨ur Maschinelle Sprachverarbeitung
Stuttgart University
</affiliation>
<address confidence="0.5361">
70569 Stuttgart, Germany
</address>
<email confidence="0.965834">
pado@ims.uni-stuttgart.de
</email>
<author confidence="0.952589">
Parvin Sadat Feizabadi
</author>
<affiliation confidence="0.959423">
Institut f¨ur Computerlinguistik
Heidelberg University
</affiliation>
<address confidence="0.843633">
69120 Heidelberg, Germany
</address>
<email confidence="0.998759">
feizabadi@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998496">
This paper reports on a study of crowd-
sourcing the annotation of non-local (or
implicit) frame-semantic roles, i.e., roles
that are realized in the previous discourse
context. We describe two annotation se-
tups (marking and gap filling) and find that
gap filling works considerably better, attain-
ing an acceptable quality relatively cheaply.
The produced data is available for research
purposes.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989777777778">
In the last years, crowdsourcing, e.g., using Ama-
zon’s Mechanical Turk platform, has been used to
collect data for a range of NLP tasks, e.g., MT eval-
uation (Callison-Burch, 2009), sentiment analysis
(Mellebeek et al., 2010), and student answer rat-
ing (Heilman and Smith, 2010). Frame-semantic
role annotation (FSRA) is a task that requires more
linguistic expertise than most data collection tasks
realized with crowdsourcing; nevertheless it is also
a crucial prerequisite for high-performance frame-
semantic role labeling (SRL) systems (Das et al.,
2014). Thus, there are some studies that have in-
vestigated FSRA as a crowdsourcing task. It can be
separated into two parts: First, choosing the frame
evoked by a given predicate in a sentence; second,
assigning the semantic roles associated with the
chosen frame. Hong and Baker (2011) have re-
cently addressed the first step, experimenting with
various ways of presenting the task. Fossati et
al. (2013) have considered both steps and opera-
tionalized them separately and jointly, finding the
best results when a single annotation task is pre-
sented to turkers (due to the interdependence of the
two steps) and when the semantic role description
are simplified. Both studies conclude that crowd-
sourcing can produce usable results for FSRA but
requires careful design. Our study extends these
previous studies to the phenomenon of implicit
(non-locally realized) semantic roles where anno-
tators are presented with a target sentence in para-
graph context, and have to decide for every role
whether it is realized in the target sentence, else-
where in the paragraph, or not at all. Our results
shows that implicit roles can be annotated as well
as locally realized roles in a crowdsourcing setup,
again provided that good design choices are taken.
</bodyText>
<sectionHeader confidence="0.957732" genericHeader="method">
2 Implicit Semantic Roles
</sectionHeader>
<bodyText confidence="0.999875538461539">
Implicit or non-locally realized semantic roles oc-
cur when arguments of a predicate are understood
although not expressed in its direct syntactic neigh-
borhood. FrameNet (Fillmore et al., 2003) dis-
tinguishes between indefinite non-instantiations
(INIs), which are interpreted generically; definite
non-instantiations (DNIs), which can often be iden-
tified with expressions from the previous context;
and constructional non-instantiations (CNI), e.g.,
passives. For instance, in the following example,
the GOAL of the predicate “reached” is realized
locally, the SOURCE is a non-locally realized DNI,
and the PATH is an INI and not realized at all.
</bodyText>
<listItem confidence="0.850289">
(1) Phileas Fogg, having shut the door of
</listItem>
<bodyText confidence="0.997103444444444">
[SOURCE his house] at half-past eleven, and
having put his right foot before his left
five hundred and seventy-five times, and
his left foot before his right five hundred
and seventy-six times, reached [GOAL the
Reform Club].
Implicit roles play an important role in discourse
comprehension and coherence (Burchardt et al.,
2005) and have found increasing attention over the
</bodyText>
<page confidence="0.980908">
226
</page>
<note confidence="0.688152">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999633461538461">
last years. The development was kickstarted by the
creation of a corpus of non-local frame-semantic
roles for the SemEval 2010 Task 10 (Ruppenhofer
et al., 2010), which still serves as a de facto stan-
dard. A number of systems perform SRL for non-
local roles (Chen et al., 2010; Silberer and Frank,
2012; Laparra and Rigau, 2013), but the obtained
results are still far from satisfactory, with the best
reported F-Score at 0.19. The main reason is data
sparsity: Due to the small size of the dataset (just
438 sentences), every predicate occurs only a small
number of times. Crowdsourcing can be an attrac-
tive strategy to acquire more annotations.
</bodyText>
<sectionHeader confidence="0.997652" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.981513">
3.1 Domain
</subsectionHeader>
<bodyText confidence="0.999797933333333">
Our emphasis is on evaluating the annotation of
implicit roles. We reduce complexity by limiting
the number of frames and roles like earlier studies
(Hong and Baker, 2011; Fossati et al., 2013). We
focus on verbs from the MOTION and POSITION
frames, which realize a common set of location
roles (PLACE OF EVENT, SOURCE, GOAL, PATH).
This makes the task more uniform and allows us to
skip frame annotation. Information about spatial
relations, provided by such verbs, can be useful
for many NLP tasks which reason about spatial
information, e.g. systems generating textual de-
scriptions from visual data, robot navigation tasks,
and geographical information systems or GIS (Ko-
rdjamshidi et al., 2012).
</bodyText>
<subsectionHeader confidence="0.999102">
3.2 Corpus
</subsectionHeader>
<bodyText confidence="0.999996909090909">
We chose the novel “Around the World in Eighty
Days” by Jules Verne, annotating the ten most fre-
quent predicates meeting the conditions described
above for annotation (reach, arrive, descend, rush,
follow, approach, send, cross, escape, pass). A
post-hoc analysis later showed that each instance of
these predicates has on average 0.67 implicit roles
identifiable in previous context, which underlines
the relevance of annotating such cases. Metaphori-
cal uses were discarded before annotation, which
left an average 38.4 instances for each predicate.
</bodyText>
<sectionHeader confidence="0.988311" genericHeader="method">
4 Annotation and Agreement
</sectionHeader>
<bodyText confidence="0.999786">
We decided to present target sentences with three
sentences of previous context, as a compromise be-
tween reading overhead and coverage of non-local
roles: For nominalizations, the three previous sen-
tences cover over 85% of all non-local roles (Ger-
</bodyText>
<table confidence="0.99477">
Source Goal Path Place
Exact Match 0.35 0.44 0.48 0.24
Overlap 0.35 0.46 0.52 0.27
</table>
<tableCaption confidence="0.89027">
Table 1: Raw agreement among annotators in the
“marking” task
</tableCaption>
<bodyText confidence="0.999440888888889">
ber and Chai, 2012). An example and the detailed
description of the task were provided to the an-
notators through external links. We experimented
with two alternatives: annotation as a marking task
and as a gap filling task (explained below). Each
HIT was annotated by five turkers who were asked
to annotate both local and non-local roles, since
identification of local roles is necessary for reliable
tagging of non-local roles.
</bodyText>
<subsectionHeader confidence="0.999395">
4.1 Marking Task
</subsectionHeader>
<bodyText confidence="0.999981333333333">
Our rationale was to make the task as comprehen-
sible as possible for non-experts. In each HIT, the
target predicate in its context was shown in bold-
face and the annotators were asked to answer four
questions about “the event in bold”: (a) where does
the event take place?; (b) what is its starting point?;
(c) what is its end point?; (d) which path is used?
For every question, turkers were asked to either
mark a text span (shown in a non-editable field
below the question) or click a button labeled “not
found in the text”. The goals of this setup were (a)
to minimize annotation effort, and (b) to make the
task as layman-compatible as possible, following
Fossati et al.’s (2013) observation that linguistic
definitions can harm results.
After annotating some instances, we computed
raw inter-annotator agreement (IAA). Table 1
shows IAA among turkers in two conditions (aver-
age pairwise Exact Match and word-based Overlap)
overall annotations for the first 49 instances.1 The
overall IAA is 37.9% (Exact Match) and 40.1%
(Overlap). We found these results to be too low to
continue this approach. The low results for Overlap
indicate that the problems cannot be due mainly to
differences in the marked spans. Indeed, an analy-
sis showed that the main reason was that annotators
were often confused by the presence of multiple
predicates in the paragraph. Consequently, many
answers marked roles pertaining not to the bolded
target predicate but to other predicates, such as (2).
</bodyText>
<footnote confidence="0.810651666666667">
(2) Leaving Bombay, it passes through Sal-
1Kappa is not applicable since we have a large number of
disjoint annotators.
</footnote>
<page confidence="0.960336">
227
</page>
<table confidence="0.999640333333333">
Source Goal Path Place
Exact Match 0.46 0.46 0.56 0.30
Overlap 0.50 0.54 0.58 0.38
</table>
<tableCaption confidence="0.7870165">
Table 2: Raw agreement among annotators in the
“gap filling” task
</tableCaption>
<bodyText confidence="0.9993431">
cette, crossing to the continent opposite
Tannah, goes over the chain of the West-
ern Ghauts, [... ] and, descending south-
eastward by Burdivan and the French town
of Chandernagor, has its terminus at Cal-
cutta.
Annotators would be expected to annotate the
continent opposite Tannah as the goal of crossing,
but some annotated Calcutta, the final destination
of the chain of motion events described.
</bodyText>
<subsectionHeader confidence="0.98869">
4.2 Gap Filling Task
</subsectionHeader>
<bodyText confidence="0.99987525">
Seeing that the marking task did not constrain the
interpretation of the turkers sufficiently, we moved
to a second setup, gap filling, with the aim of fo-
cussing the turkers’ attention to a single predicate
rather than the complete set of predicates present
in the text shown. In this task, the annotators were
asked to complete the sentence by filling in the
blanks in two sentences:
</bodyText>
<listItem confidence="0.988277666666667">
1. [Agent] [Event+ed] from ... to ...
through ... path.
2. The whole event took place in/at ...
</listItem>
<bodyText confidence="0.999955851851852">
The first sentence corresponds to annotations of
the SOURCE, GOAL, and PATH roles; the second
one of the PLACE role. The rationale is that the
presence of the predicate in the sentence focuses
the turkers’ attention on the predicate’s actual roles.
Annotators could leave gaps empty (in the case of
unrealized roles), and we asked them to remain as
close to the original material as possible, that is,
avoid paraphrases. Perfect copying is not always
possible, due to grammatical constraints.
Table 2 shows the IAA for this design. We see
that even though the gap filling introduced a new
source of variability (namely, the need for annota-
tors to copy text), the IAA improves considerably,
by up to 11% in Exact Match and 15% in Over-
lap. The new overall IAAs are 44.7% (+6.8%) and
50.2% (+10.1%), respectively. Overall, the num-
bers are still fairly low. However, note that these
IAA numbers among turkers are a lower bound for
the agreement between a “canonical” version of
the turkers’ annotation (see Section 5) and an ideal
gold standard. Additionally, a data analysis showed
that in the gap filling setup, many of the disagree-
ments are more well-behaved: unsurprisingly, they
are often cases where annotators disagree on the ex-
act range of the string to fill into the gap. Consider
the following example:
</bodyText>
<listItem confidence="0.668917">
(3) Skillful detectives have been sent to all the
</listItem>
<bodyText confidence="0.997911826086957">
principal ports of America and the Conti-
nent, and he’ll be a clever fellow if he slips
through their fingers.”
Arguably, experts would annotate all the prin-
cipal ports of America and the Continent as the
GOAL role of sent. Turkers however annotated dif-
ferent spans, including all the principal ports of
America, ports, as well as the “correct” span. The
lowest IAA is found for the place role. While it is
possible that our setup which required turkers to
consider a second sentence to annotate place con-
tributes to the overall difficulty, our data analysis
indicates that the main problem is the more vague
nature of PLACE compared to the other roles which
made it more difficult for annotators to tag consis-
tently. Consider Example (1): the PLACE could
be, among other things, the City, London, England,
etc. The large number of locations in the novel is a
compounding factor. We found that for some pred-
icates (e.g. arrive, reach), many turkers attempted
to resolve the ambiguity by (erroneously) annotat-
ing the same text as both GOAL and PLACE, which
runs counter to the FrameNet guidelines.
</bodyText>
<sectionHeader confidence="0.995817" genericHeader="method">
5 Canonicalization
</sectionHeader>
<bodyText confidence="0.9999448125">
We still need to compute a “canonical” annotation
that combines the five turker’s annotations. First,
we need to decide whether a role should be realized
or left unrealized (i.e., INI, CNI, or DNI but not in
the presented context). Second, we need to decide
on a span for realized roles. Canonicalization in
crowdsourcing often assumes a majority principle,
accepting the analysis proposed by most turkers.
We found it necessary to be more flexible. Regard-
ing realization, a manual analysis of a few instances
showed that cases of two turker annotations with
non-empty overlap could be accepted as non-local
roles. That is, turkers frequently miss non-local
roles, but if two out of five annotate an overlapping
span with the same role, this is reasonable evidence.
Regarding the role’s span, we used the consensus
</bodyText>
<page confidence="0.995846">
228
</page>
<table confidence="0.999296">
Source Goal Path Place
Exact Match 0.72 0.67 0.82 0.50
Overlap 0.72 0.69 0.82 0.54
</table>
<tableCaption confidence="0.9934185">
Table 3: Raw agreement between canonical crowd-
sourcing annotation and expert annotation by role
</tableCaption>
<table confidence="0.999861666666667">
Local Non-Local Unrealized
Exact Match 0.66 0.66 0.69
Overlap 0.69 0.70 0.69
</table>
<tableCaption confidence="0.926602">
Table 4: Raw agreement between canonical anno-
tation and expert annotation by realization status
</tableCaption>
<bodyText confidence="0.9967738">
span if it existed, and the maximal (union) span oth-
erwise, given that some turkers filled the gaps just
with head words and not complete constituents. To
test the quality of the canonical annotation, one of
the authors had previously annotated 100 random
instances that were also presented to the turkers.
We consider the result to be an expert annotation
approximating a gold standard and use it to judge
the quality of the canonical turker annotations. The
results are shown in Table 3.
The overall raw agreement numbers are 67.80%
(Exact Match) and 69.34% (Overlap). As we had
hoped, the agreement between the canonical crowd-
sourcing annotation and the expert annotation is
again substantially higher than the IAA among turk-
ers. Again, we see the highest numbers for path
(the most specific role) and the lowest numbers for
place (the least specific role).
To assess whether the number obtained in table
3 are sensitive to realization status (explicit, im-
plicit or unrealized), we broke down the agreement
numbers by realization status. Somewhat to our
(positive) surprise, the results in Table 4 indicate
that non-locally realized roles are annotated ablut
as reliably as locally realized ones. Except for the
ill-defined PLACE role, our reliability is compara-
ble to Fossati et al. (2013). Given the more difficult
nature of the task (annotators are given more con-
text and have to make a more difficult decision),
we consider this a promising result.
</bodyText>
<sectionHeader confidence="0.936194" genericHeader="method">
6 Final Dataset and Cost
</sectionHeader>
<bodyText confidence="0.996335">
The final dataset consists of 384 predicate in-
stances.2 With four roles per predicate, a total
of 1536 roles could have been realized. We found
</bodyText>
<footnote confidence="0.922609333333333">
2It can be downloaded for research purposes
from http://www.cl.uni-heidelberg.de/
˜feizabadi/res.mhtml
</footnote>
<bodyText confidence="0.999929033333333">
that more than half (60%) of the roles remained
unrealized even in context. 23% of the roles were
realized locally, and 17% non-locally. The distri-
bution over locally realized, non-locally realized,
and unrealized roles varies considerably among the
four roles that we consider. GOAL has the high-
est percentage of realized roles overall (unrealized
only for 34% of all predicate instances), and at the
same time the highest ratio of locally realized roles
(48% locally realized, 18% non-locally). This cor-
responds well to FrameNet’s predictions about our
chosen predicates which realize the Goal role gen-
erally as the direct object (reach) or an obligatory
prepositional phrase (arrive). In contrast, SOURCE
is realized only for 36% of all instances, and then
predominantly non-locally (24% non-local vs. 12%
local). This shows once more that a substantial part
of predicate-argument structure must be recovered
from previous discourse context.
On average, each HIT page was annotated in 1
minute and 48 seconds, which means 27 seconds
per each role and a total of 60 hours for the whole
annotation. We paid 0.15 USD for each HIT. Since
the number of roles in all HITs was fixed to four
(source, goal, path and place), each role cost 0.04
USD, which corresponds to about USD 0.19 for
every canonical role annotation. This is about twice
the amount paid by Fossati et al. and reflects the
increased effort inherent in a task that involves
discourse context.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998239947368421">
This paper presented a study on crowdsourcing the
annotation of non-local semantic roles in discourse
context, comparing a marking and a gap filling
setup. We found that gap filling is the more reliable
choice since the repetition of the predicate helps
focusing the turkers’ attention on the roles at hand
rather than understanding of the global text. Thus,
the semantic role-based crowdsourcing approach of
Fossati et al. (2013) appears to be generalizable to
the area of non-locally realized roles, provided that
the task is defined suitably. Our results also support
Fossati et al.’s observation that reliable annotations
can be obtained without providing definitions of
semantic roles. However, we also find large differ-
ences among semantic roles. Some (like PATH) can
be annotated reliably and should be usable to train
or improve SRL systems. Others (like PLACE) are
defined so vaguely that it is unclear how usable
their annotations are.
</bodyText>
<page confidence="0.997864">
229
</page>
<sectionHeader confidence="0.990147" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998705">
Aljoscha Burchardt, Anette Frank, and Manfred Pinkal.
2005. Building text meaning representations from
contextually related frames – a case study. In Pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 66–77, Tilburg, Nether-
lands.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon’s
Mechanical Turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 286–295, Singapore.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 264–267, Uppsala, Sweden.
Dipanjan Das, Desai Chen, Andr´e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics. To appear.
Charles J Fillmore, Christopher R Johnson, and Miriam
R L Petruck. 2003. Background to FrameNet. Inter-
national Journal of Lexicography, 16(3):235–250.
Marco Fossati, Claudio Giuliano, Sara Tonelli, and
Fondazione Bruno Kessler. 2013. Outsourcing
FrameNet to the Crowd. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 742–747, Sofia, Bulgaria.
Matthew Gerber and Joyce Y Chai. 2012. Semantic
role labeling of implicit arguments for nominal pred-
icates. Computational Linguistics, 38(4):755–798.
Michael Heilman and Noah A Smith. 2010. Rat-
ing computer-generated questions with Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 35–40, Los An-
geles, CA.
Jisup Hong and Collin F. Baker. 2011. How good is
the crowd at ”real” WSD? In Proceedings of the
5th Linguistic Annotation Workshop, pages 30–37,
Portland, Oregon, USA.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012. Semeval-2012 task 3: Spa-
tial role labeling. In Proceedings of the First Joint
Conference on Lexical and Computational Seman-
tics, pages 365–373, Montr´eal, Canada.
Egoitz Laparra and German Rigau. 2013. Sources of
evidence for implicit argument resolution. In Pro-
ceedings of the 10th International Conference on
Computational Semantics (IWCS 2013) – Long Pa-
pers, pages 155–166, Potsdam, Germany.
Bart Mellebeek, Francesc Benavent, Jens Grivolla,
Joan Codina, Marta R Costa-Jussa, and Rafael
Banchs. 2010. Opinion mining of spanish cus-
tomer comments with non-expert annotations on me-
chanical turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 114–
121, Los Angeles, CA.
Josef Ruppenhofer, Caroline Sporleder, R. Morante,
Collin Baker, and Martha Palmer. 2010. Semeval-
2010 task 10: Linking events and their participants
in discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 45–50, Up-
psala, Sweden.
Carina Silberer and Anette Frank. 2012. Casting im-
plicit role linking as an anaphora resolution task.
In Proceedings of SEM 2012: The First Joint Con-
ference on Lexical and Computational Semantics,
pages 1–10, Montreal, Canada.
</reference>
<page confidence="0.997092">
230
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.592289">
<title confidence="0.999941">Crowdsourcing Annotation of Non-Local Semantic Roles</title>
<author confidence="0.980298">Sebastian</author>
<affiliation confidence="0.98947">Institut f¨ur Maschinelle</affiliation>
<address confidence="0.832324">Stuttgart 70569 Stuttgart,</address>
<email confidence="0.996504">pado@ims.uni-stuttgart.de</email>
<author confidence="0.997625">Parvin Sadat</author>
<affiliation confidence="0.997177">Institut f¨ur</affiliation>
<address confidence="0.977339">Heidelberg 69120 Heidelberg, Germany</address>
<email confidence="0.999759">feizabadi@cl.uni-heidelberg.de</email>
<abstract confidence="0.996205636363636">This paper reports on a study of crowdthe annotation of frame-semantic roles, i.e., roles that are realized in the previous discourse context. We describe two annotation setups (marking and gap filling) and find that gap filling works considerably better, attaining an acceptable quality relatively cheaply. The produced data is available for research purposes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Anette Frank</author>
<author>Manfred Pinkal</author>
</authors>
<title>Building text meaning representations from contextually related frames – a case study.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Computational Semantics,</booktitle>
<pages>66--77</pages>
<location>Tilburg, Netherlands.</location>
<contexts>
<context position="3576" citStr="Burchardt et al., 2005" startWordPosition="532" endWordPosition="535">xt; and constructional non-instantiations (CNI), e.g., passives. For instance, in the following example, the GOAL of the predicate “reached” is realized locally, the SOURCE is a non-locally realized DNI, and the PATH is an INI and not realized at all. (1) Phileas Fogg, having shut the door of [SOURCE his house] at half-past eleven, and having put his right foot before his left five hundred and seventy-five times, and his left foot before his right five hundred and seventy-six times, reached [GOAL the Reform Club]. Implicit roles play an important role in discourse comprehension and coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the ob</context>
</contexts>
<marker>Burchardt, Frank, Pinkal, 2005</marker>
<rawString>Aljoscha Burchardt, Anette Frank, and Manfred Pinkal. 2005. Building text meaning representations from contextually related frames – a case study. In Proceedings of the International Workshop on Computational Semantics, pages 66–77, Tilburg, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>286--295</pages>
<contexts>
<context position="919" citStr="Callison-Burch, 2009" startWordPosition="121" endWordPosition="122">i@cl.uni-heidelberg.de Abstract This paper reports on a study of crowdsourcing the annotation of non-local (or implicit) frame-semantic roles, i.e., roles that are realized in the previous discourse context. We describe two annotation setups (marking and gap filling) and find that gap filling works considerably better, attaining an acceptable quality relatively cheaply. The produced data is available for research purposes. 1 Introduction In the last years, crowdsourcing, e.g., using Amazon’s Mechanical Turk platform, has been used to collect data for a range of NLP tasks, e.g., MT evaluation (Callison-Burch, 2009), sentiment analysis (Mellebeek et al., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic rol</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using Amazon’s Mechanical Turk. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desai Chen</author>
<author>Nathan Schneider</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semafor: Frame argument resolution with log-linear models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>264--267</pages>
<location>Uppsala,</location>
<contexts>
<context position="4112" citStr="Chen et al., 2010" startWordPosition="616" endWordPosition="619"> important role in discourse comprehension and coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 Experimental Setup 3.1 Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We</context>
</contexts>
<marker>Chen, Schneider, Das, Smith, 2010</marker>
<rawString>Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A. Smith. 2010. Semafor: Frame argument resolution with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264–267, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>Andr´e F T Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Frame-semantic parsing. Computational Linguistics.</title>
<date>2014</date>
<note>To appear.</note>
<contexts>
<context position="1296" citStr="Das et al., 2014" startWordPosition="173" endWordPosition="176">uced data is available for research purposes. 1 Introduction In the last years, crowdsourcing, e.g., using Amazon’s Mechanical Turk platform, has been used to collect data for a range of NLP tasks, e.g., MT evaluation (Callison-Burch, 2009), sentiment analysis (Mellebeek et al., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and Baker (2011) have recently addressed the first step, experimenting with various ways of presenting the task. Fossati et al. (2013) have considered both steps and operationalized them separately and jointly, finding the best results when a single annotation task is presented to turkers (due to the interdependence of the two steps)</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>Dipanjan Das, Desai Chen, Andr´e F. T. Martins, Nathan Schneider, and Noah A. Smith. 2014. Frame-semantic parsing. Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="2751" citStr="Fillmore et al., 2003" startWordPosition="408" endWordPosition="411">(non-locally realized) semantic roles where annotators are presented with a target sentence in paragraph context, and have to decide for every role whether it is realized in the target sentence, elsewhere in the paragraph, or not at all. Our results shows that implicit roles can be annotated as well as locally realized roles in a crowdsourcing setup, again provided that good design choices are taken. 2 Implicit Semantic Roles Implicit or non-locally realized semantic roles occur when arguments of a predicate are understood although not expressed in its direct syntactic neighborhood. FrameNet (Fillmore et al., 2003) distinguishes between indefinite non-instantiations (INIs), which are interpreted generically; definite non-instantiations (DNIs), which can often be identified with expressions from the previous context; and constructional non-instantiations (CNI), e.g., passives. For instance, in the following example, the GOAL of the predicate “reached” is realized locally, the SOURCE is a non-locally realized DNI, and the PATH is an INI and not realized at all. (1) Phileas Fogg, having shut the door of [SOURCE his house] at half-past eleven, and having put his right foot before his left five hundred and s</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles J Fillmore, Christopher R Johnson, and Miriam R L Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3):235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Fossati</author>
<author>Claudio Giuliano</author>
<author>Sara Tonelli</author>
<author>Fondazione Bruno Kessler</author>
</authors>
<title>Outsourcing FrameNet to the Crowd.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>742--747</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1695" citStr="Fossati et al. (2013)" startWordPosition="239" endWordPosition="242">quires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and Baker (2011) have recently addressed the first step, experimenting with various ways of presenting the task. Fossati et al. (2013) have considered both steps and operationalized them separately and jointly, finding the best results when a single annotation task is presented to turkers (due to the interdependence of the two steps) and when the semantic role description are simplified. Both studies conclude that crowdsourcing can produce usable results for FSRA but requires careful design. Our study extends these previous studies to the phenomenon of implicit (non-locally realized) semantic roles where annotators are presented with a target sentence in paragraph context, and have to decide for every role whether it is real</context>
<context position="4708" citStr="Fossati et al., 2013" startWordPosition="714" endWordPosition="717"> roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 Experimental Setup 3.1 Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the MOTION and POSITION frames, which realize a common set of location roles (PLACE OF EVENT, SOURCE, GOAL, PATH). This makes the task more uniform and allows us to skip frame annotation. Information about spatial relations, provided by such verbs, can be useful for many NLP tasks which reason about spatial information, e.g. systems generating textual descriptions from visual data, robot navigation tasks, and geographical information systems or GIS (Kordjamshidi et al., 2012). 3.2 Corpus We chose the novel “Around the World in Eighty Days” by Jules Verne, annotating th</context>
<context position="14233" citStr="Fossati et al. (2013)" startWordPosition="2284" endWordPosition="2287">on is again substantially higher than the IAA among turkers. Again, we see the highest numbers for path (the most specific role) and the lowest numbers for place (the least specific role). To assess whether the number obtained in table 3 are sensitive to realization status (explicit, implicit or unrealized), we broke down the agreement numbers by realization status. Somewhat to our (positive) surprise, the results in Table 4 indicate that non-locally realized roles are annotated ablut as reliably as locally realized ones. Except for the ill-defined PLACE role, our reliability is comparable to Fossati et al. (2013). Given the more difficult nature of the task (annotators are given more context and have to make a more difficult decision), we consider this a promising result. 6 Final Dataset and Cost The final dataset consists of 384 predicate instances.2 With four roles per predicate, a total of 1536 roles could have been realized. We found 2It can be downloaded for research purposes from http://www.cl.uni-heidelberg.de/ ˜feizabadi/res.mhtml that more than half (60%) of the roles remained unrealized even in context. 23% of the roles were realized locally, and 17% non-locally. The distribution over locall</context>
</contexts>
<marker>Fossati, Giuliano, Tonelli, Kessler, 2013</marker>
<rawString>Marco Fossati, Claudio Giuliano, Sara Tonelli, and Fondazione Bruno Kessler. 2013. Outsourcing FrameNet to the Crowd. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 742–747, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Y Chai</author>
</authors>
<title>Semantic role labeling of implicit arguments for nominal predicates.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>4</issue>
<marker>Gerber, Chai, 2012</marker>
<rawString>Matthew Gerber and Joyce Y Chai. 2012. Semantic role labeling of implicit arguments for nominal predicates. Computational Linguistics, 38(4):755–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Rating computer-generated questions with Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>35--40</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="1017" citStr="Heilman and Smith, 2010" startWordPosition="134" endWordPosition="137">f non-local (or implicit) frame-semantic roles, i.e., roles that are realized in the previous discourse context. We describe two annotation setups (marking and gap filling) and find that gap filling works considerably better, attaining an acceptable quality relatively cheaply. The produced data is available for research purposes. 1 Introduction In the last years, crowdsourcing, e.g., using Amazon’s Mechanical Turk platform, has been used to collect data for a range of NLP tasks, e.g., MT evaluation (Callison-Burch, 2009), sentiment analysis (Mellebeek et al., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and Baker (2011) have recently addressed the first step,</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A Smith. 2010. Rating computer-generated questions with Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 35–40, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jisup Hong</author>
<author>Collin F Baker</author>
</authors>
<title>How good is the crowd at ”real” WSD?</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th Linguistic Annotation Workshop,</booktitle>
<pages>30--37</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1577" citStr="Hong and Baker (2011)" startWordPosition="220" endWordPosition="223">., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and Baker (2011) have recently addressed the first step, experimenting with various ways of presenting the task. Fossati et al. (2013) have considered both steps and operationalized them separately and jointly, finding the best results when a single annotation task is presented to turkers (due to the interdependence of the two steps) and when the semantic role description are simplified. Both studies conclude that crowdsourcing can produce usable results for FSRA but requires careful design. Our study extends these previous studies to the phenomenon of implicit (non-locally realized) semantic roles where anno</context>
<context position="4685" citStr="Hong and Baker, 2011" startWordPosition="710" endWordPosition="713">rform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 Experimental Setup 3.1 Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the MOTION and POSITION frames, which realize a common set of location roles (PLACE OF EVENT, SOURCE, GOAL, PATH). This makes the task more uniform and allows us to skip frame annotation. Information about spatial relations, provided by such verbs, can be useful for many NLP tasks which reason about spatial information, e.g. systems generating textual descriptions from visual data, robot navigation tasks, and geographical information systems or GIS (Kordjamshidi et al., 2012). 3.2 Corpus We chose the novel “Around the World in Eighty Days” by Jul</context>
</contexts>
<marker>Hong, Baker, 2011</marker>
<rawString>Jisup Hong and Collin F. Baker. 2011. How good is the crowd at ”real” WSD? In Proceedings of the 5th Linguistic Annotation Workshop, pages 30–37, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Semeval-2012 task 3: Spatial role labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>365--373</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5213" citStr="Kordjamshidi et al., 2012" startWordPosition="793" endWordPosition="797">e complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the MOTION and POSITION frames, which realize a common set of location roles (PLACE OF EVENT, SOURCE, GOAL, PATH). This makes the task more uniform and allows us to skip frame annotation. Information about spatial relations, provided by such verbs, can be useful for many NLP tasks which reason about spatial information, e.g. systems generating textual descriptions from visual data, robot navigation tasks, and geographical information systems or GIS (Kordjamshidi et al., 2012). 3.2 Corpus We chose the novel “Around the World in Eighty Days” by Jules Verne, annotating the ten most frequent predicates meeting the conditions described above for annotation (reach, arrive, descend, rush, follow, approach, send, cross, escape, pass). A post-hoc analysis later showed that each instance of these predicates has on average 0.67 implicit roles identifiable in previous context, which underlines the relevance of annotating such cases. Metaphorical uses were discarded before annotation, which left an average 38.4 instances for each predicate. 4 Annotation and Agreement We decide</context>
</contexts>
<marker>Kordjamshidi, Bethard, Moens, 2012</marker>
<rawString>Parisa Kordjamshidi, Steven Bethard, and MarieFrancine Moens. 2012. Semeval-2012 task 3: Spatial role labeling. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 365–373, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Sources of evidence for implicit argument resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers,</booktitle>
<pages>155--166</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="4164" citStr="Laparra and Rigau, 2013" startWordPosition="624" endWordPosition="627">d coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 Experimental Setup 3.1 Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the MOTION and POSITION frames,</context>
</contexts>
<marker>Laparra, Rigau, 2013</marker>
<rawString>Egoitz Laparra and German Rigau. 2013. Sources of evidence for implicit argument resolution. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) – Long Papers, pages 155–166, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Mellebeek</author>
<author>Francesc Benavent</author>
<author>Jens Grivolla</author>
<author>Joan Codina</author>
<author>Marta R Costa-Jussa</author>
<author>Rafael Banchs</author>
</authors>
<title>Opinion mining of spanish customer comments with non-expert annotations on mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>114--121</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="964" citStr="Mellebeek et al., 2010" startWordPosition="125" endWordPosition="128">reports on a study of crowdsourcing the annotation of non-local (or implicit) frame-semantic roles, i.e., roles that are realized in the previous discourse context. We describe two annotation setups (marking and gap filling) and find that gap filling works considerably better, attaining an acceptable quality relatively cheaply. The produced data is available for research purposes. 1 Introduction In the last years, crowdsourcing, e.g., using Amazon’s Mechanical Turk platform, has been used to collect data for a range of NLP tasks, e.g., MT evaluation (Callison-Burch, 2009), sentiment analysis (Mellebeek et al., 2010), and student answer rating (Heilman and Smith, 2010). Frame-semantic role annotation (FSRA) is a task that requires more linguistic expertise than most data collection tasks realized with crowdsourcing; nevertheless it is also a crucial prerequisite for high-performance framesemantic role labeling (SRL) systems (Das et al., 2014). Thus, there are some studies that have investigated FSRA as a crowdsourcing task. It can be separated into two parts: First, choosing the frame evoked by a given predicate in a sentence; second, assigning the semantic roles associated with the chosen frame. Hong and</context>
</contexts>
<marker>Mellebeek, Benavent, Grivolla, Codina, Costa-Jussa, Banchs, 2010</marker>
<rawString>Bart Mellebeek, Francesc Benavent, Jens Grivolla, Joan Codina, Marta R Costa-Jussa, and Rafael Banchs. 2010. Opinion mining of spanish customer comments with non-expert annotations on mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 114– 121, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>R Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval2010 task 10: Linking events and their participants in discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>45--50</pages>
<location>Uppsala,</location>
<contexts>
<context position="3998" citStr="Ruppenhofer et al., 2010" startWordPosition="593" endWordPosition="596"> his left foot before his right five hundred and seventy-six times, reached [GOAL the Reform Club]. Implicit roles play an important role in discourse comprehension and coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 Experimental Setup 3.1 Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexit</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, R. Morante, Collin Baker, and Martha Palmer. 2010. Semeval2010 task 10: Linking events and their participants in discourse. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 45–50, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Anette Frank</author>
</authors>
<title>Casting implicit role linking as an anaphora resolution task.</title>
<date>2012</date>
<booktitle>In Proceedings of SEM 2012: The First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>1--10</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4138" citStr="Silberer and Frank, 2012" startWordPosition="620" endWordPosition="623">discourse comprehension and coherence (Burchardt et al., 2005) and have found increasing attention over the 226 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 226–230, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics last years. The development was kickstarted by the creation of a corpus of non-local frame-semantic roles for the SemEval 2010 Task 10 (Ruppenhofer et al., 2010), which still serves as a de facto standard. A number of systems perform SRL for nonlocal roles (Chen et al., 2010; Silberer and Frank, 2012; Laparra and Rigau, 2013), but the obtained results are still far from satisfactory, with the best reported F-Score at 0.19. The main reason is data sparsity: Due to the small size of the dataset (just 438 sentences), every predicate occurs only a small number of times. Crowdsourcing can be an attractive strategy to acquire more annotations. 3 Experimental Setup 3.1 Domain Our emphasis is on evaluating the annotation of implicit roles. We reduce complexity by limiting the number of frames and roles like earlier studies (Hong and Baker, 2011; Fossati et al., 2013). We focus on verbs from the M</context>
</contexts>
<marker>Silberer, Frank, 2012</marker>
<rawString>Carina Silberer and Anette Frank. 2012. Casting implicit role linking as an anaphora resolution task. In Proceedings of SEM 2012: The First Joint Conference on Lexical and Computational Semantics, pages 1–10, Montreal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>