<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000712">
<title confidence="0.989889">
Evaluating the Inferential Utility of Lexical-Semantic Resources
</title>
<author confidence="0.996075">
Shachar Mirkin, Ido Dagan, Eyal Shnarch
</author>
<affiliation confidence="0.990964">
Computer Science Department, Bar-Ilan University
</affiliation>
<address confidence="0.592162">
Ramat-Gan 52900, Israel
</address>
<email confidence="0.993143">
{mirkins,dagan,shey}@cs.biu.ac.il
</email>
<sectionHeader confidence="0.997314" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99952752631579">
Lexical-semantic resources are used ex-
tensively for applied semantic inference,
yet a clear quantitative picture of their
current utility and limitations is largely
missing. We propose system- and
application-independent evaluation and
analysis methodologies for resources’ per-
formance, and systematically apply them
to seven prominent resources. Our find-
ings identify the currently limited recall of
available resources, and indicate the po-
tential to improve performance by exam-
ining non-standard relation types and by
distilling the output of distributional meth-
ods. Further, our results stress the need
to include auxiliary information regarding
the lexical and logical contexts in which
a lexical inference is valid, as well as its
prior validity likelihood.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971186440678">
Lexical information plays a major role in seman-
tic inference, as the meaning of one term is of-
ten inferred form another. Lexical-semantic re-
sources, which provide the needed knowledge for
lexical inference, are commonly utilized by ap-
plied inference systems (Giampiccolo et al., 2007)
and applications such as Information Retrieval and
Question Answering (Shah and Croft, 2004; Pasca
and Harabagiu, 2001). Beyond WordNet (Fell-
baum, 1998), a wide range of resources has been
developed and utilized, including extensions to
WordNet (Moldovan and Rus, 2001; Snow et al.,
2006) and resources based on automatic distri-
butional similarity methods (Lin, 1998; Pantel
and Lin, 2002). Recently, Wikipedia is emerg-
ing as a source for extracting semantic relation-
ships (Suchanek et al., 2007; Kazama and Tori-
sawa, 2007).
As of today, only a partial comparative picture
is available regarding the actual utility and limi-
tations of available resources for lexical-semantic
inference. Works that do provide quantitative
information regarding resources utility have fo-
cused on few particular resources (Kouylekov and
Magnini, 2006; Roth and Sammons, 2007) and
evaluated their impact on a specific system. Most
often, works which utilized lexical resources do
not provide information about their isolated con-
tribution; rather, they only report overall per-
formance for systems in which lexical resources
serve as components.
Our paper provides a step towards clarify-
ing this picture. We propose a system- and
application-independent evaluation methodology
that isolates resources’ performance, and sys-
tematically apply it to seven prominent lexical-
semantic resources. The evaluation and analysis
methodology is specified within the Textual En-
tailment framework, which has become popular in
recent years for modeling practical semantic infer-
ence in a generic manner (Dagan and Glickman,
2004). To that end, we assume certain definitions
that extend the textual entailment paradigm to the
lexical level.
The findings of our work provide useful insights
and suggested directions for two research com-
munities: developers of applied inference systems
and researchers addressing lexical acquisition and
resource construction. Beyond the quantitative
mapping of resources’ performance, our analysis
points at issues concerning their effective utiliza-
tion and major characteristics. Even more impor-
tantly, the results highlight current gaps in exist-
ing resources and point at directions towards fill-
ing them. We show that the coverage of most
resources is quite limited, where a substantial
part of recall is attributable to semantic relations
that are typically not available to inference sys-
tems. Notably, distributional acquisition methods
</bodyText>
<note confidence="0.9229455">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558–566,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997046">
558
</page>
<bodyText confidence="0.999947625">
are shown to provide many useful relationships
which are missing from other resources, but these
are embedded amongst many irrelevant ones. Ad-
ditionally, the results highlight the need to rep-
resent and inference over various aspects of con-
textual information, which affect the applicability
of lexical inferences. We suggest that these gaps
should be addressed by future research.
</bodyText>
<sectionHeader confidence="0.989567" genericHeader="method">
2 Sub-sentential Textual Entailment
</sectionHeader>
<bodyText confidence="0.999977966666667">
Textual entailment captures the relation between a
text t and a textual statement (termed hypothesis)
h, such that a person reading t would infer that h
is most likely correct (Dagan et al., 2005).
The entailment relation has been defined insofar
in terms of truth values, assuming that h is a com-
plete sentence (proposition). However, there are
major aspects of inference that apply to the sub-
sentential level. First, in certain applications, the
target hypotheses are often sub-sentential. For ex-
ample, search queries in IR, which play the hy-
pothesis role from an entailment perspective, typ-
ically consist of a single term, like drug legaliza-
tion. Such sub-sentential hypotheses are not re-
garded naturally in terms of truth values and there-
fore do not fit well within the scope of the textual
entailment definition. Second, many entailment
models apply a compositional process, through
which they try to infer each sub-part of the hy-
pothesis from some parts of the text (Giampiccolo
et al., 2007).
Although inferences over sub-sentential ele-
ments are being applied in practice, so far there
are no standard definitions for entailment at sub-
sentential levels. To that end, and as a prerequisite
of our evaluation methodology and our analysis,
we first establish two relevant definitions for sub-
sentential entailment relations: (a) entailment of a
sub-sentential hypothesis by a text, and (b) entail-
ment of one lexical element by another.
</bodyText>
<subsectionHeader confidence="0.998318">
2.1 Entailment of Sub-sentential Hypotheses
</subsectionHeader>
<bodyText confidence="0.991754">
We first seek a definition that would capture the
entailment relationship between a text and a sub-
sentential hypothesis. A similar goal was ad-
dressed in (Glickman et al., 2006), who defined
the notion of lexical reference to model the fact
that in order to entail a hypothesis, the text has
to entail each non-compositional lexical element
within it. We suggest that a slight adaptation of
their definition is suitable to capture the notion of
entailment for any sub-sentential hypotheses, in-
cluding compositional ones:
</bodyText>
<construct confidence="0.832182333333333">
Definition 1 A sub-sentential hypothesis h is en-
tailed by a text t if there is an explicit or implied
reference in t to a possible meaning of h.
</construct>
<bodyText confidence="0.999473214285714">
For example, the sentence “crude steel output
is likely to fall in 2000” entails the sub-sentential
hypotheses production, steel production and steel
output decrease.
Glickman et al., achieving good inter-annotator
agreement, empirically found that almost all non-
compositional terms in an entailed sentential hy-
pothesis are indeed referenced in the entailing text.
This finding suggests that the above definition is
consistent with the original definition of textual
entailment for sentential hypotheses and can thus
model compositional entailment inferences.
We use this definition in our annotation method-
ology described in Section 3.
</bodyText>
<subsectionHeader confidence="0.998165">
2.2 Entailment between Lexical Elements
</subsectionHeader>
<bodyText confidence="0.998914454545454">
In the majority of cases, the reference to an
“atomic” (non-compositional) lexical element e in
h stems from a particular lexical element e&apos; in t,
as in the example above where the word output
implies the meaning of production.
To identify this relationship, an entailment sys-
tem needs a knowledge resource that would spec-
ify that the meaning of e&apos; implies the meaning of
e, at least in some contexts. We thus suggest the
following definition to capture this relationship be-
tween e&apos; and e:
</bodyText>
<construct confidence="0.9890885">
Definition 2 A lexical element e’ entails another
lexical element e, denoted e’==&gt;-e, if there exist
some natural (non-anecdotal) texts containing e’
which entail e, such that the reference to the mean-
ing of e can be implied solely from the meaning of
e’ in the text.
</construct>
<bodyText confidence="0.988584454545455">
(Entailment of e by a text follows Definition 1).
We refer to this relation in this paper as lexical
entailment1, and call e’ � e a lexical entailment
rule. e&apos; is referred to as the rule’s left hand side
(LHS) and e as its right hand side (RHS).
Currently there are no knowledge resources de-
signed specifically for lexical entailment model-
ing. Hence, the types of relationships they cap-
ture do not fully coincide with entailment infer-
ence needs. Thus, the definition suggests a spec-
ification for the rules that should be provided by
</bodyText>
<footnote confidence="0.998999">
1Section 6 discusses other definitions of lexical entailment
</footnote>
<page confidence="0.998298">
559
</page>
<bodyText confidence="0.99996853125">
a lexical entailment resource, following an oper-
ative rationale: a rule e’ ⇒ e should be included
in an entailment knowledge resource if it would be
needed, as part of a compositional process, to infer
the meaning of e from some natural texts. Based
on this definition, we perform an analysis of the re-
lationships included in lexical-semantic resources,
as described in Section 5.
A rule need not apply in all contexts, as long
as it is appropriate for some texts. Two contex-
tual aspects affect rule applicability. First is the
“lexical context” specifying the meanings of the
text’s words. A rules is applicable in a certain con-
text only when the intended sense of its LHS term
matches the sense of that term in the text. For ex-
ample, the application of the rule lay ⇒ produce is
valid only in contexts where the producer is poul-
try and the products are eggs. This is a well known
issue observed, for instance, by Voorhees (1994).
A second contextual factor requiring validation
is the “logical context”. The logical context de-
termines the monotonicity of the LHS and is in-
duced by logical operators such as negation and
(explicit or implicit) quantifiers. For example, the
rule mammal ⇒ whale may not be valid in most
cases, but is applicable in universally quantified
texts like “mammals are warm-blooded”. This is-
sue has been rarely addressed in applied inference
systems (de Marneffe et al., 2006). The above
mentioned rules both comply with Definition 2
and should therefore be included in a lexical en-
tailment resource.
</bodyText>
<sectionHeader confidence="0.980609" genericHeader="method">
3 Evaluating Entailment Resources
</sectionHeader>
<bodyText confidence="0.999969636363636">
Our evaluation goal is to assess the utility of
lexical-semantic resources as sources for entail-
ment rules. An inference system applies a rule by
inferring the rule’s RHS from texts that match its
LHS. Thus, the utility of a resource depends on the
performance of its rule applications rather than on
the proportion of correct rules it contains. A rule,
whether correct or incorrect, has insignificant ef-
fect on the resource’s utility if it rarely matches
texts in real application settings. Additionally,
correct rules might produce incorrect applications
when applied in inappropriate contexts. There-
fore, we use an instance-based evaluation method-
ology, which simulates rule applications by col-
lecting texts that contain rules’ LHS and manually
assessing the correctness of their applications.
Systems typically handle lexical context either
implicitly or explicitly. Implicit context valida-
tion occurs when the different terms of a compos-
ite hypothesis disambiguate each other. For exam-
ple, the rule waterside ⇒ bank is unlikely to be
applied when trying to infer the hypothesis bank
loans, since texts that match waterside are unlikely
to contain also the meaning of loan. Explicit meth-
ods, such as word-sense disambiguation or sense
matching, validate each rule application according
to the broader context in the text. Few systems
also address logical context validation by handling
quantifiers and negation. As we aim for a system-
independent comparison of resources, and explicit
approaches are not standardized yet within infer-
ence systems, our evaluation uses only implicit
context validation.
</bodyText>
<subsectionHeader confidence="0.976913">
3.1 Evaluation Methodology
</subsectionHeader>
<figureCaption confidence="0.997563">
Figure 1: Evaluation methodology flow chart
</figureCaption>
<bodyText confidence="0.999948090909091">
The input for our evaluation methodology is a
lexical-semantic resource R, which contains lex-
ical entailment rules. We evaluate R’s utility by
testing how useful it is for inferring a sample of
test hypotheses H from a corpus. Each hypothesis
in H contains more than one lexical element in or-
der to provide implicit context validation for rule
applications, e.g. h: water pollution.
We next describe the steps of our evaluation
methodology, as illustrated in Figure 1. We refer
to the examples in the figure when needed:
</bodyText>
<listItem confidence="0.883930571428571">
1) Fetch rules: For each h ∈ H and each
lexical element e ∈ h (e.g. water), we fetch all
rules e’ ⇒ e in R that might be applied to entail e
(e.g. lake ⇒ water).
2) Generate intermediate hypotheses h’:
For each rule r: e’ ⇒ e, we generate an intermedi-
ate hypothesis h&apos; by replacing e in h with e&apos; (e.g.
</listItem>
<page confidence="0.975678">
560
</page>
<bodyText confidence="0.98862025">
h01: lake pollution). From a text t entailing h0, h
can be further entailed by the single application of
r. We thus simulate the process by which an en-
tailment system would infer h from t using r.
</bodyText>
<listItem confidence="0.970100433333334">
3) Retrieve matching texts: For each h0 we
retrieve from a corpus all texts that contain the
lemmatized words of h0 (not necessarily as a sin-
gle phrase). These texts may entail h0. We dis-
card texts that also match h since entailing h from
them might not require the application of any rule
from the evaluated resource. In our example, the
retrieved texts contain lake and pollution but do
not contain water.
4) Annotation: A sample of the retrieved texts
is presented to human annotators. The annotators
are asked to answer the following two questions
for each text, simulating the typical inference pro-
cess of an entailment system:
a) Does t entail h’? If t does not entail h0
then the text would not provide a useful example
for the application of r. For instance, t1 (in Fig-
ure 1) does not entail h01 and thus we cannot de-
duce h from it by applying the rule r. Such texts
are discarded from further evaluation.
b) Does t entail h? If t is annotated as en-
tailing h0, an entailment system would then infer
h from h0 by applying r. If h is not entailed from
t even though h0 is, the rule application is consid-
ered invalid. For instance, t2 does not entail h even
though it entails h02. Indeed, the application of r2:
*soil ==&gt;-water 2, from which h02 was constructed,
yields incorrect inference. If the answer is ’yes’,
as in the case of t3, the application of r for t is
considered valid.
</listItem>
<bodyText confidence="0.99976375">
The above process yields a sample of annotated
rule applications for each test hypothesis, from
which we can measure resources performance, as
described in Section 5.
</bodyText>
<sectionHeader confidence="0.991248" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.99299">
4.1 Dataset and Annotation
</subsectionHeader>
<bodyText confidence="0.999971571428571">
Current available state-of-the-art lexical-semantic
resources mainly deal with nouns. Therefore, we
used nominal hypotheses for our experiment3.
We chose TREC 1-8 (excluding 4) as our test
corpus and randomly sampled 25 ad-hoc queries
of two-word compounds as our hypotheses. We
did not use longer hypotheses to ensure that
</bodyText>
<footnote confidence="0.953090666666667">
2The asterisk marks an incorrect rule.
3We suggest that the definitions and methodologies can be
applied for other parts of speech as well.
</footnote>
<bodyText confidence="0.999904222222222">
enough texts containing the intermediate hypothe-
ses are found in the corpus. For annotation sim-
plicity, we retrieved single sentences as our texts.
For each rule applied for an hypothesis h, we
sampled 10 sentences from the sentences retrieved
for that rule. As a baseline, we also sampled 10
sentences for each original hypothesis h in which
both words of h are found. In total, 1550 unique
sentences were sampled and annotated by two an-
notators.
To assess the validity of our evaluation method-
ology, the annotators first judged a sample of 220
sentences. The Kappa scores for inter-annotator
agreement were 0.74 and 0.64 for judging h0 and
h, respectively. These figures correspond to sub-
stantial agreement (Landis and Koch, 1997) and
are comparable with related semantic annotations
(Szpektor et al., 2007; Bhagat et al., 2007).
</bodyText>
<subsectionHeader confidence="0.88291">
4.2 Lexical-Semantic Resources
</subsectionHeader>
<bodyText confidence="0.98844075862069">
We evaluated the following resources:
WordNet (WNd): There is no clear agreement
regarding which set of WordNet relations is use-
ful for entailment inference. We therefore took a
conservative approach using only synonymy and
hyponymy rules, which typically comply with the
lexical entailment relation and are commonly used
by textual entailment systems, e.g. (Herrera et al.,
2005; Bos and Markert, 2006). Given a term e,
we created a rule e’ ==&gt;- e for each e0 amongst the
synonyms or direct hyponyms for all senses of e
in WordNet 3.0.
Snow (Snow30k): Snow et al. (2006) pre-
sented a probabilistic model for taxonomy induc-
tion which considers as features paths in parse
trees between related taxonomy nodes. They show
that the best performing taxonomy was the one
adding 30,000 hyponyms to WordNet. We created
an entailment rule for each new hyponym added to
WordNet by their algorithm4.
LCC’s extended WordNet (XWN&apos;°): In
(Moldovan and Rus, 2001) WordNet glosses were
transformed into logical form axioms. From this
representation we created a rule e’ ==&gt;- e for each e0
in the gloss which was tagged as referring to the
same entity as e.
CBC: A knowledgebase of labeled clusters gen-
erated by the statistical clustering and labeling al-
gorithms in (Pantel and Lin, 2002; Pantel and
</bodyText>
<footnote confidence="0.998873">
4Available at http://ai.stanford.edu/˜rion/swn
</footnote>
<page confidence="0.994346">
561
</page>
<bodyText confidence="0.997023466666667">
Ravichandran, 2004)5. Given a cluster label e, an
entailment rule e’ ⇒ e is created for each member
e&apos; of the cluster.
Lin Dependency Similarity (Lin-dep): A
distributional word similarity resource based on
syntactic-dependency features (Lin, 1998). Given
a term e and its list of similar terms, we construct
for each e&apos; in the list the rule e’ ⇒ e. This resource
was previously used in textual entailment engines,
e.g. (Roth and Sammons, 2007).
Lin Proximity Similarity (Lin-prox): A
knowledgebase of terms with their cooccurrence-
based distributionally similar terms. Rules are cre-
ated from this resource as from the previous one6.
Wikipedia first sentence (WikiFS): Kazama
and Torisawa (2007) used Wikipedia as an exter-
nal knowledge to improve Named Entity Recog-
nition. Using the first step of their algorithm, we
extracted from the first sentence of each page a
noun that appears in a is-a pattern referring to the
title. For each such pair we constructed a rule title
⇒ noun (e.g. Michelle Pfeiffer ⇒ actress).
The above resources represent various meth-
ods for detecting semantic relatedness between
words: Manually and semi-automatically con-
structed (WNd and XWN*, respectively), automat-
ically constructed based on a lexical-syntactic pat-
tern (WikiFS), distributional methods (Lin-dep and
Lin-prox) and combinations of pattern-based and
distributional methods (CBC and Snow30k).
</bodyText>
<sectionHeader confidence="0.999714" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999933125">
The results and analysis described in this section
reveal new aspects concerning the utility of re-
sources for lexical entailment, and experimentally
quantify several intuitively-accepted notions re-
garding these resources and the lexical entailment
relation. Overall, our findings highlight where ef-
forts in developing future resources and inference
systems should be invested.
</bodyText>
<subsectionHeader confidence="0.973354">
5.1 Resources Performance
</subsectionHeader>
<bodyText confidence="0.9998705">
Each resource was evaluated using two measures -
Precision and Recall-share, macro averaged over
all hypotheses. The results achieved for each re-
source are summarized in Table 1.
</bodyText>
<footnote confidence="0.997626">
5Kindly provided to us by Patrick Pantel.
6Lin’s resources were downloaded from:
http://www.cs.ualberta.ca/˜lindek/demos.htm
</footnote>
<table confidence="0.998191125">
Resource Precision (%) Recall-share (%)
Snow30k 56 8
WNd 55 24
XWN* 51 9
WikiFS 45 7
CBC 33 9
Lin-dep 28 45
Lin-prox 24 36
</table>
<tableCaption confidence="0.992124">
Table 1: Lexical resources performance
</tableCaption>
<subsubsectionHeader confidence="0.620715">
5.1.1 Precision
</subsubsectionHeader>
<bodyText confidence="0.972140297297297">
The Precision of a resource R is the percentage of
valid rule applications for the resource. It is esti-
mated by the percentage of texts entailing h from
those that entail h&apos;: countR(entailing h=yes)
countR(entailing h�=yes).
Not surprisingly, resources such as WNd, XWN*
or WikiFS achieved relatively high precision
scores, due to their accurate construction meth-
ods. In contrast, Lin’s distributional resources are
not designed to include lexical entailment relation-
ships. They provide pairs of contextually simi-
lar words, of which many have non-entailing rela-
tionships, such as co-hyponyms7 (e.g. *doctor ⇒
journalist) or topically-related words, such as *ra-
diotherapy ⇒ outpatient. Hence their relatively
low precision.
One visible outcome is the large gap between
the perceived high accuracy of resources con-
structed by accurate methods, most notably WNd,
and their performance in practice. This finding
emphasizes the need for instance-based evalua-
tions, which capture the “real” contribution of a
resource. To better understand the reasons for
this gap we further assessed the three factors
that contribute to incorrect applications: incorrect
rules, lexical context and logical context (see Sec-
tion 2.2). This analysis is presented in Table 2.
From Table 2 we see that the gap for accurate
resources is mainly caused by applications of cor-
rect rules in inappropriate contexts. More inter-
estingly, the information in the table allows us to
asses the lexical “context-sensitivity” of resources.
When considering only the COR-LEX rules to re-
calculate resources precision, we find that Lin-dep
achieves precision of 71%( 15% ), while WNd
15%+6%
yields only 56% ( 55%
</bodyText>
<footnote confidence="0.71245375">
55%+44%). This result indicates
that correct Lin-dep rules are less sensitive to lexi-
cal context, meaning that their prior likelihoods to
7a.k.a. sister terms or coordinate terms
</footnote>
<page confidence="0.987559">
562
</page>
<table confidence="0.999718">
(%) Invalid Rule Applications Valid Rule Applications
INCOR COR-LOG COR-LEX Total INCOR COR-LOG COR-LEX Total (P)
WNd 1 0 44 45 0 0 55 55
WikiFS 13 0 42 55 3 0 42 45
XWN* 19 0 30 49 0 0 51 51
Snow30k 23 0 21 44 0 0 56 56
CBC 51 12 4 67 14 0 19 33
Lin-prox 59 4 13 76 8 3 13 24
Lin-dep 61 5 6 72 9 4 15 28
</table>
<tableCaption confidence="0.992468333333333">
Table 2: The distribution of invalid and valid rule applications by rule types: incorrect rules (INCOR), correct rules requiring
“logical context” validation (COR-LOG), and correct rules requiring “lexical context” matching (COR-LEX). The numbers of each
resource’s valid applications add up to the resource’s precision.
</tableCaption>
<bodyText confidence="0.999149142857143">
be correct are higher. This is explained by the fact
that Lin-dep’s rules are calculated across multiple
contexts and therefore capture the more frequent
usages of words. WordNet, on the other hand, in-
cludes many anecdotal rules whose application is
rare, and thus is very sensitive to context. Simi-
larly, WikiFS turns out to be very context-sensitive.
This resource contains many rules for polysemous
proper nouns that are scarce in their proper noun
sense, e.g. Captive ==&gt;.computer game. Snow30k,
when applied with the same calculation, reaches
73%, which explains how it achieved a compara-
ble result to WNd, even though it contains many
incorrect rules in comparison to WNd.
</bodyText>
<subsubsectionHeader confidence="0.560248">
5.1.2 Recall
</subsubsectionHeader>
<bodyText confidence="0.9999584375">
Absolute recall cannot be measured since the total
number of texts in the corpus that entail each hy-
pothesis is unknown. Instead, we measure recall-
share, the contribution of each resource to recall
relative to matching only the words of the origi-
nal hypothesis without any rules. We denote by
yield(h) the number of texts that match h directly
and are annotated as entailing h. This figure is es-
timated by the number of sampled texts annotated
as entailing h multiplied by the sampling propor-
tion. In the same fashion, for each resource R,
we estimate the number of texts entailing h ob-
tained through entailment rules of the resource R,
denoted yieldR(h). Recall-share of R for h is the
proportion of the yield obtained by the resource’s
rules relative to the overall yield with and without
</bodyText>
<equation confidence="0.625291">
yieldR(h)
the rules from R:
yield(h)+yieldR(h).
</equation>
<bodyText confidence="0.98603275">
From Table 1 we see that along with their rela-
tively low precision, Lin’s resources’ recall greatly
surpasses that of any other resource, including
WordNet8. The rest of the resources are even infe-
</bodyText>
<footnote confidence="0.859392">
8A preliminary experiment we conducted showed that re-
</footnote>
<bodyText confidence="0.996708928571428">
rior to WNd in that respect, indicating their limited
utility for inference systems.
As expected, synonyms and hyponyms in Word-
Net contributed a noticeable portion to recall in all
resources. Additional correct rules correspond to
hyponyms and synonyms missing from WordNet,
many of them proper names and some slang ex-
pressions. These rules were mainly provided by
WikiFS and Snow30k, significantly supplementing
WordNet, whose HasInstance relation is quite par-
tial. However, there are other interesting types of
entailment relations contributing to recall. These
are discussed in Sections 5.2 and 5.3. Examples
for various rule types are found in Table 3.
</bodyText>
<subsubsectionHeader confidence="0.820144">
5.1.3 Valid Applications of Incorrect Rules
</subsubsectionHeader>
<bodyText confidence="0.9999315">
We observed that many entailing sentences were
retrieved by inherently incorrect rules in the distri-
butional resources. Analysis of these rules reveals
they were matched in entailing texts when the LHS
has noticeable statistical correlation with another
term in the text that does entail the RHS. For ex-
ample, for the hypothesis wildlife extinction, the
rule *species ==&gt;. extinction yielded valid applica-
tions in contexts about threatened or endangered
species. Has the resource included a rule between
the entailing term in the text and the RHS, the
entailing text would have been matched without
needing the incorrect rule.
These correlations accounted for nearly a third
of Lin resources’ recall. Nonetheless, in princi-
ple, we suggest that such rules, which do not con-
form with Definition 2, should not be included in a
lexical entailment resource, since they also cause
invalid rule applications, while the entailing texts
they retrieve will hopefully be matched by addi-
</bodyText>
<footnote confidence="0.914278">
call does not dramatically improve when using the entire hy-
ponymy subtree from WordNet.
</footnote>
<page confidence="0.992776">
563
</page>
<table confidence="0.9785185">
Type Correct Rules
Snow30k
HYPO Shevardnadze ⇒ official
ANT efficacy ⇒ ineffectiveness Lin-dep
HOLO government ⇒ official Lin-prox
HYPER arms ⇒ gun Lin-prox
˜ childbirth ⇒ motherhood Lin-dep
˜ mortgage ⇒ bank Lin-prox
˜ Captive ⇒ computer WikiFS
˜ negligence ⇒ failure CBC
˜ beatification ⇒ pope XWN*
Type Incorrect Rules
CO-HYP alcohol ⇒ cigarette CBC
˜ radiotherapy ⇒ outpatient Lin-dep
Snow30k
˜ teen-ager ⇒ gun
˜ basic ⇒ paper WikiFS
˜ species ⇒ extinction Lin-prox
</table>
<tableCaption confidence="0.8484436">
Table 3: Examples of lexical resources rules by types.
HYPO: hyponymy, HYPER: hypernymy (class entailment of
its members), HOLO: holonymy, ANT: antonymy, CO-HYP: co-
hyponymy. The non-categorized relations do not correspond
to any WordNet relation.
</tableCaption>
<bodyText confidence="0.989082">
tional correct rules in a more comprehensive re-
source.
</bodyText>
<subsectionHeader confidence="0.999379">
5.2 Non-standard Entailment Relations
</subsectionHeader>
<bodyText confidence="0.9999795">
An important finding of our analysis is that some
less standard entailment relationships have a con-
siderable impact on recall (see Table 3). These
rules, which comply with Definition 2 but do
not conform to any WordNet relation type, were
mainly contributed by Lin’s distributional re-
sources and to a smaller degree are also included
in XWN*. In Lin-dep, for example, they accounted
for approximately a third of the recall.
Among the finer grained relations we identi-
fied in this set are topical entailment (e.g. IBM
as the company entailing the topic computers),
consequential relationships (pregnancy ⇒ mother-
hood) and an entailment of inherent arguments by
a predicate, or of essential participants by a sce-
nario description, e.g. beatification ⇒ pope. A
comprehensive typology of these relationships re-
quires further investigation, as well as the identi-
fication and development of additional resources
from which they can be extracted.
As opposed to hyponymy and synonymy rules,
these rules are typically non-substitutable, i.e. the
RHS of the rule is unlikely to have the exact same
role in the text as the LHS. Many inference sys-
tems perform rule-based transformations, substi-
tuting the LHS by the RHS. This finding suggests
that different methods may be required to utilize
such rules for inference.
</bodyText>
<subsectionHeader confidence="0.998569">
5.3 Logical Context
</subsectionHeader>
<bodyText confidence="0.999964454545455">
WordNet relations other than synonyms and hy-
ponyms, including antonyms, holonyms and hy-
pernyms (see Table 3), contributed a noticeable
share of valid rule applications for some resources.
Following common practice, these relations are
missing by construction from the other resources.
As shown in Table 2 (COR-LOG columns), such
relations accounted for a seventh of Lin-dep’s
valid rule applications, as much as was the con-
tribution of hyponyms and synonyms to this re-
source’s recall. Yet, using these rules resulted with
more erroneous applications than correct ones. As
discussed in Section 2.2, the rules induced by
these relations do conform with our lexical entail-
ment definition. However, a valid application of
these rules requires certain logical conditions to
occur, which is not the common case. We thus
suggest that such rules are included in lexical en-
tailment resources, as long as they are marked
properly by their types, allowing inference sys-
tems to utilize them only when appropriate mech-
anisms for handling logical context are in place.
</bodyText>
<subsectionHeader confidence="0.996414">
5.4 Rules Priors
</subsectionHeader>
<bodyText confidence="0.999968545454546">
In Section 5.1.1 we observed that some resources
are highly sensitive to context. Hence, when con-
sidering the validity of a rule’s application, two
factors should be regarded: the actual context in
which the rule is to be applied, as well as the rule’s
prior likelihood to be valid in an arbitrary con-
text. Somewhat indicative, yet mostly indirect, in-
formation about rules’ priors is contained in some
resources. This includes sense ranks in WordNet,
SemCor statistics (Miller et al., 1993), and similar-
ity scores and rankings in Lin’s resources. Infer-
ence systems often incorporated this information,
typically as top-k or threshold-based filters (Pan-
tel and Lin, 2003; Roth and Sammons, 2007). By
empirically assessing the effect of several such fil-
ters in our setting, we found that this type of data
is indeed informative in the sense that precision
increases as the threshold rises. Yet, no specific
filters were found to improve results in terms of
F1 score (where recall is measured relatively to
the yield of the unfiltered resource) due to a sig-
nificant drop in relative recall. For example, Lin-
</bodyText>
<page confidence="0.993104">
564
</page>
<bodyText confidence="0.9999854">
prox loses more than 40% of its recall when only
the top-50 rules for each hypothesis are exploited,
and using only the first sense of WNd costs the re-
source over 60% of its recall. We thus suggest a
better strategy might be to combine the prior in-
formation with context matching scores in order
to obtain overall likelihood scores for rule appli-
cations, as in (Szpektor et al., 2008). Furthermore,
resources should include explicit information re-
garding the prior likelihoods of of their rules.
</bodyText>
<subsectionHeader confidence="0.858292">
5.5 Operative Conclusions
</subsectionHeader>
<bodyText confidence="0.999992482758621">
Our findings highlight the currently limited re-
call of available resources for lexical inference.
The higher recall of Lin’s resources indicates
that many more entailment relationships can be
acquired, particularly when considering distribu-
tional evidence. Yet, available distributional ac-
quisition methods are not geared for lexical entail-
ment. This suggests the need to develop acqui-
sition methods for dedicated and more extensive
knowledge resources that would subsume the cor-
rect rules found by current distributional methods.
Furthermore, substantially better recall may be ob-
tained by acquiring non-standard lexical entail-
ment relationships, as discussed in Section 5.2, for
which a comprehensive typology is still needed.
At the same time, transformation-based inference
systems would need to handle these kinds of rules,
which are usually non-substitutable. Our results
also quantify and stress earlier findings regarding
the severe degradation in precision when rules are
applied in inappropriate contexts. This highlights
the need for resources to provide explicit informa-
tion about the suitable lexical and logical contexts
in which an entailment rule is applicable. In par-
allel, methods should be developed to utilize such
contextual information within inference systems.
Additional auxiliary information needed in lexical
resources is the prior likelihood for a given rule to
be correct in an arbitrary context.
</bodyText>
<sectionHeader confidence="0.99999" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9997916875">
Several prior works defined lexical entailment.
WordNet’s lexical entailment is a relationship be-
tween verbs only, defined for propositions (Fell-
baum, 1998). Geffet and Dagan (2004) defined
substitutable lexical entailment as a relation be-
tween substitutable terms. We find this definition
too restrictive as non-substitutable rules may also
be useful for entailment inference. Examples are
breastfeeding ==&gt;. baby and hospital ==&gt;. medical.
Hence, Definition 2 is more broadly applicable for
defining the desired contents of lexical entailment
resources. We empirically observed that the rules
satisfying their definition are a proper subset of
the rules covered by our definition. Dagan and
Glickman (2004) referred to entailment at the sub-
sentential level by assigning truth values to sub-
propositional text fragments through their existen-
tial meaning. We find this criterion too permissive.
For instance, the existence of country implies the
existence of its flag. Yet, the meaning of flag is
typically not implied by country.
Previous works assessing rule application via
human annotation include (Pantel et al., 2007;
Szpektor et al., 2007), which evaluate acquisition
methods for lexical-syntactic rules. They posed an
additional question to the annotators asking them
to filter out invalid contexts. In our methodology
implicit context matching for the full hypothesis
was applied instead. Other related instance-based
evaluations (Giuliano and Gliozzo, 2007; Connor
and Roth, 2007) performed lexical substitutions,
but did not handle the non-substitutable cases.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999956285714286">
This paper provides several methodological and
empirical contributions. We presented a novel
evaluation methodology for the utility of lexical-
semantic resources for semantic inference. To that
end we proposed definitions for entailment at sub-
sentential levels, addressing a gap in the textual
entailment framework. Our evaluation and analy-
sis provide a first quantitative comparative assess-
ment of the isolated utility of a range of prominent
potential resources for entailment rules. We have
shown various factors affecting rule applicability
and resources performance, while providing oper-
ative suggestions to address them in future infer-
ence systems and resources.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999278444444444">
The authors would like to thank Naomi Frankel
and Iddo Greental for their excellent annotation
work, as well as Roy Bar-Haim and Idan Szpektor
for helpful discussion and advice. This work was
partially supported by the Negev Consortium of
the Israeli Ministry of Industry, Trade and Labor,
the PASCAL-2 Network of Excellence of the Eu-
ropean Community FP7-ICT-2007-1-216886 and
the Israel Science Foundation grant 1095/05.
</bodyText>
<page confidence="0.997564">
565
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992632">
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
LEDIR: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
EMNLP-CoNLL.
J. Bos and K. Markert. 2006. When logical infer-
ence helps determining textual entailment (and when
it doesn’t). In Proceedings of the Second PASCAL
RTE Challenge.
Michael Connor and Dan Roth. 2007. Context sensi-
tive paraphrasing with a global unsupervised classi-
fier. In Proceedings of ECML.
Ido Dagan and Oren Glickman. 2004. Probabilistic
textual entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Joaquin Quinonero Candela, Ido Da-
gan, Bernardo Magnini, and Florence d’Alch´e Buc,
editors, MLCW, Lecture Notes in Computer Science.
Marie-Catherine de Marneffe, Bill MacCartney, Trond
Grenager, Daniel Cer, Anna Rafferty, and Christo-
pher D. Manning. 2006. Learning to distinguish
valid textual entailments. In Proceedings of the Sec-
ond PASCAL RTE Challenge.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature vector
quality and distributional similarity. In Proceedings
of COLING.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
ACL-WTEP Workshop.
Claudio Giuliano and Alfio Gliozzo. 2007. Instance
based lexical entailment for ontology population. In
Proceedings of EMNLP-CoNLL.
Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006.
Lexical reference: a semantic matching subtask. In
Proceedings of EMNLP.
Jes´us Herrera, Anselmo Pe˜nas, and Felisa Verdejo.
2005. Textual entailment recognition based on de-
pendency analysis and wordnet. In Proceedings of
the First PASCAL RTE Challenge.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL.
Milen Kouylekov and Bernardo Magnini. 2006. Build-
ing a large-scale repository of textual entailment
rules. In Proceedings of LREC.
J. R. Landis and G. G. Koch. 1997. The measurements
of observer agreement for categorical data. In Bio-
metrics, pages 33:159–174.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of HLT.
Dan Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of ACM
SIGKDD.
Patrick Pantel and Dekang Lin. 2003. Automatically
discovering word senses. In Proceedings of NAACL.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of HLT-NAACL.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard Hovy. 2007. ISP:
Learning inferential selectional preferences. In Pro-
ceedings of HLT.
Marius Pasca and Sanda M. Harabagiu. 2001. The in-
formative role of wordnet in open-domain question
answering. In Proceedings of NAACL Workshop on
WordNet and Other Lexical Resources.
Dan Roth and Mark Sammons. 2007. Semantic and
logical inference model for textual entailment. In
Proceedings ofACL-WTEP Workshop.
Chirag Shah and Bruce W. Croft. 2004. Evaluating
high accuracy retrieval techniques. In Proceedings
of SIGIR.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A core of semantic knowl-
edge - unifying wordnet and wikipedia. In Proceed-
ings of WWW.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proceedings of ACL.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In Pro-
ceedings of ACL.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of SIGIR.
</reference>
<page confidence="0.998507">
566
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926502">
<title confidence="0.999968">Evaluating the Inferential Utility of Lexical-Semantic Resources</title>
<author confidence="0.968928">Shachar Mirkin</author>
<author confidence="0.968928">Ido Dagan</author>
<author confidence="0.968928">Eyal Shnarch</author>
<affiliation confidence="0.999975">Computer Science Department, Bar-Ilan University</affiliation>
<address confidence="0.995736">Ramat-Gan 52900, Israel</address>
<abstract confidence="0.9979497">Lexical-semantic resources are used extensively for applied semantic inference, yet a clear quantitative picture of their current utility and limitations is largely missing. We propose systemand application-independent evaluation and analysis methodologies for resources’ performance, and systematically apply them to seven prominent resources. Our findings identify the currently limited recall of available resources, and indicate the potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>LEDIR: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="15657" citStr="Bhagat et al., 2007" startWordPosition="2511" endWordPosition="2514">nces from the sentences retrieved for that rule. As a baseline, we also sampled 10 sentences for each original hypothesis h in which both words of h are found. In total, 1550 unique sentences were sampled and annotated by two annotators. To assess the validity of our evaluation methodology, the annotators first judged a sample of 220 sentences. The Kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging h0 and h, respectively. These figures correspond to substantial agreement (Landis and Koch, 1997) and are comparable with related semantic annotations (Szpektor et al., 2007; Bhagat et al., 2007). 4.2 Lexical-Semantic Resources We evaluated the following resources: WordNet (WNd): There is no clear agreement regarding which set of WordNet relations is useful for entailment inference. We therefore took a conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ==&gt;- e for each e0 amongst the synonyms or direct hyponyms for all senses of e in WordNet 3.0. Snow (Snow30k): Snow et al. (2</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007. LEDIR: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>K Markert</author>
</authors>
<title>When logical inference helps determining textual entailment (and when it doesn’t).</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL RTE Challenge.</booktitle>
<contexts>
<context position="16093" citStr="Bos and Markert, 2006" startWordPosition="2575" endWordPosition="2578"> respectively. These figures correspond to substantial agreement (Landis and Koch, 1997) and are comparable with related semantic annotations (Szpektor et al., 2007; Bhagat et al., 2007). 4.2 Lexical-Semantic Resources We evaluated the following resources: WordNet (WNd): There is no clear agreement regarding which set of WordNet relations is useful for entailment inference. We therefore took a conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ==&gt;- e for each e0 amongst the synonyms or direct hyponyms for all senses of e in WordNet 3.0. Snow (Snow30k): Snow et al. (2006) presented a probabilistic model for taxonomy induction which considers as features paths in parse trees between related taxonomy nodes. They show that the best performing taxonomy was the one adding 30,000 hyponyms to WordNet. We created an entailment rule for each new hyponym added to WordNet by their algorithm4. LCC’s extended WordNet (XWN&apos;°): In (Moldovan and Rus, 2001) WordNet glosses were transformed into logical form axio</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>J. Bos and K. Markert. 2006. When logical inference helps determining textual entailment (and when it doesn’t). In Proceedings of the Second PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Connor</author>
<author>Dan Roth</author>
</authors>
<title>Context sensitive paraphrasing with a global unsupervised classifier.</title>
<date>2007</date>
<booktitle>In Proceedings of ECML.</booktitle>
<contexts>
<context position="33111" citStr="Connor and Roth, 2007" startWordPosition="5259" endWordPosition="5262"> criterion too permissive. For instance, the existence of country implies the existence of its flag. Yet, the meaning of flag is typically not implied by country. Previous works assessing rule application via human annotation include (Pantel et al., 2007; Szpektor et al., 2007), which evaluate acquisition methods for lexical-syntactic rules. They posed an additional question to the annotators asking them to filter out invalid contexts. In our methodology implicit context matching for the full hypothesis was applied instead. Other related instance-based evaluations (Giuliano and Gliozzo, 2007; Connor and Roth, 2007) performed lexical substitutions, but did not handle the non-substitutable cases. 7 Conclusions This paper provides several methodological and empirical contributions. We presented a novel evaluation methodology for the utility of lexicalsemantic resources for semantic inference. To that end we proposed definitions for entailment at subsentential levels, addressing a gap in the textual entailment framework. Our evaluation and analysis provide a first quantitative comparative assessment of the isolated utility of a range of prominent potential resources for entailment rules. We have shown vario</context>
</contexts>
<marker>Connor, Roth, 2007</marker>
<rawString>Michael Connor and Dan Roth. 2007. Context sensitive paraphrasing with a global unsupervised classifier. In Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modeling of language variability.</title>
<date>2004</date>
<booktitle>In PASCAL Workshop on Learning Methods for Text Understanding and Mining.</booktitle>
<contexts>
<context position="2873" citStr="Dagan and Glickman, 2004" startWordPosition="410" endWordPosition="413">rovide information about their isolated contribution; rather, they only report overall performance for systems in which lexical resources serve as components. Our paper provides a step towards clarifying this picture. We propose a system- and application-independent evaluation methodology that isolates resources’ performance, and systematically apply it to seven prominent lexicalsemantic resources. The evaluation and analysis methodology is specified within the Textual Entailment framework, which has become popular in recent years for modeling practical semantic inference in a generic manner (Dagan and Glickman, 2004). To that end, we assume certain definitions that extend the textual entailment paradigm to the lexical level. The findings of our work provide useful insights and suggested directions for two research communities: developers of applied inference systems and researchers addressing lexical acquisition and resource construction. Beyond the quantitative mapping of resources’ performance, our analysis points at issues concerning their effective utilization and major characteristics. Even more importantly, the results highlight current gaps in existing resources and point at directions towards fill</context>
<context position="32330" citStr="Dagan and Glickman (2004)" startWordPosition="5144" endWordPosition="5147">elationship between verbs only, defined for propositions (Fellbaum, 1998). Geffet and Dagan (2004) defined substitutable lexical entailment as a relation between substitutable terms. We find this definition too restrictive as non-substitutable rules may also be useful for entailment inference. Examples are breastfeeding ==&gt;. baby and hospital ==&gt;. medical. Hence, Definition 2 is more broadly applicable for defining the desired contents of lexical entailment resources. We empirically observed that the rules satisfying their definition are a proper subset of the rules covered by our definition. Dagan and Glickman (2004) referred to entailment at the subsentential level by assigning truth values to subpropositional text fragments through their existential meaning. We find this criterion too permissive. For instance, the existence of country implies the existence of its flag. Yet, the meaning of flag is typically not implied by country. Previous works assessing rule application via human annotation include (Pantel et al., 2007; Szpektor et al., 2007), which evaluate acquisition methods for lexical-syntactic rules. They posed an additional question to the annotators asking them to filter out invalid contexts. I</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Ido Dagan and Oren Glickman. 2004. Probabilistic textual entailment: Generic applied modeling of language variability. In PASCAL Workshop on Learning Methods for Text Understanding and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>Lecture Notes in Computer Science.</booktitle>
<editor>In Joaquin Quinonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors, MLCW,</editor>
<contexts>
<context position="4510" citStr="Dagan et al., 2005" startWordPosition="656" endWordPosition="659">s 558 are shown to provide many useful relationships which are missing from other resources, but these are embedded amongst many irrelevant ones. Additionally, the results highlight the need to represent and inference over various aspects of contextual information, which affect the applicability of lexical inferences. We suggest that these gaps should be addressed by future research. 2 Sub-sentential Textual Entailment Textual entailment captures the relation between a text t and a textual statement (termed hypothesis) h, such that a person reading t would infer that h is most likely correct (Dagan et al., 2005). The entailment relation has been defined insofar in terms of truth values, assuming that h is a complete sentence (proposition). However, there are major aspects of inference that apply to the subsentential level. First, in certain applications, the target hypotheses are often sub-sentential. For example, search queries in IR, which play the hypothesis role from an entailment perspective, typically consist of a single term, like drug legalization. Such sub-sentential hypotheses are not regarded naturally in terms of truth values and therefore do not fit well within the scope of the textual e</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Joaquin Quinonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors, MLCW, Lecture Notes in Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL RTE Challenge.</booktitle>
<marker>de Marneffe, MacCartney, Grenager, Cer, Rafferty, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty, and Christopher D. Manning. 2006. Learning to distinguish valid textual entailments. In Proceedings of the Second PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Feature vector quality and distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="31803" citStr="Geffet and Dagan (2004)" startWordPosition="5068" endWordPosition="5071">opriate contexts. This highlights the need for resources to provide explicit information about the suitable lexical and logical contexts in which an entailment rule is applicable. In parallel, methods should be developed to utilize such contextual information within inference systems. Additional auxiliary information needed in lexical resources is the prior likelihood for a given rule to be correct in an arbitrary context. 6 Related Work Several prior works defined lexical entailment. WordNet’s lexical entailment is a relationship between verbs only, defined for propositions (Fellbaum, 1998). Geffet and Dagan (2004) defined substitutable lexical entailment as a relation between substitutable terms. We find this definition too restrictive as non-substitutable rules may also be useful for entailment inference. Examples are breastfeeding ==&gt;. baby and hospital ==&gt;. medical. Hence, Definition 2 is more broadly applicable for defining the desired contents of lexical entailment resources. We empirically observed that the rules satisfying their definition are a proper subset of the rules covered by our definition. Dagan and Glickman (2004) referred to entailment at the subsentential level by assigning truth val</context>
</contexts>
<marker>Geffet, Dagan, 2004</marker>
<rawString>Maayan Geffet and Ido Dagan. 2004. Feature vector quality and distributional similarity. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL-WTEP Workshop.</booktitle>
<contexts>
<context position="1283" citStr="Giampiccolo et al., 2007" startWordPosition="175" endWordPosition="178">e potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the</context>
<context position="5312" citStr="Giampiccolo et al., 2007" startWordPosition="787" endWordPosition="790">that apply to the subsentential level. First, in certain applications, the target hypotheses are often sub-sentential. For example, search queries in IR, which play the hypothesis role from an entailment perspective, typically consist of a single term, like drug legalization. Such sub-sentential hypotheses are not regarded naturally in terms of truth values and therefore do not fit well within the scope of the textual entailment definition. Second, many entailment models apply a compositional process, through which they try to infer each sub-part of the hypothesis from some parts of the text (Giampiccolo et al., 2007). Although inferences over sub-sentential elements are being applied in practice, so far there are no standard definitions for entailment at subsentential levels. To that end, and as a prerequisite of our evaluation methodology and our analysis, we first establish two relevant definitions for subsentential entailment relations: (a) entailment of a sub-sentential hypothesis by a text, and (b) entailment of one lexical element by another. 2.1 Entailment of Sub-sentential Hypotheses We first seek a definition that would capture the entailment relationship between a text and a subsentential hypoth</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of ACL-WTEP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Instance based lexical entailment for ontology population.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="33087" citStr="Giuliano and Gliozzo, 2007" startWordPosition="5255" endWordPosition="5258">ential meaning. We find this criterion too permissive. For instance, the existence of country implies the existence of its flag. Yet, the meaning of flag is typically not implied by country. Previous works assessing rule application via human annotation include (Pantel et al., 2007; Szpektor et al., 2007), which evaluate acquisition methods for lexical-syntactic rules. They posed an additional question to the annotators asking them to filter out invalid contexts. In our methodology implicit context matching for the full hypothesis was applied instead. Other related instance-based evaluations (Giuliano and Gliozzo, 2007; Connor and Roth, 2007) performed lexical substitutions, but did not handle the non-substitutable cases. 7 Conclusions This paper provides several methodological and empirical contributions. We presented a novel evaluation methodology for the utility of lexicalsemantic resources for semantic inference. To that end we proposed definitions for entailment at subsentential levels, addressing a gap in the textual entailment framework. Our evaluation and analysis provide a first quantitative comparative assessment of the isolated utility of a range of prominent potential resources for entailment ru</context>
</contexts>
<marker>Giuliano, Gliozzo, 2007</marker>
<rawString>Claudio Giuliano and Alfio Gliozzo. 2007. Instance based lexical entailment for ontology population. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Lexical reference: a semantic matching subtask.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5973" citStr="Glickman et al., 2006" startWordPosition="890" endWordPosition="893">al elements are being applied in practice, so far there are no standard definitions for entailment at subsentential levels. To that end, and as a prerequisite of our evaluation methodology and our analysis, we first establish two relevant definitions for subsentential entailment relations: (a) entailment of a sub-sentential hypothesis by a text, and (b) entailment of one lexical element by another. 2.1 Entailment of Sub-sentential Hypotheses We first seek a definition that would capture the entailment relationship between a text and a subsentential hypothesis. A similar goal was addressed in (Glickman et al., 2006), who defined the notion of lexical reference to model the fact that in order to entail a hypothesis, the text has to entail each non-compositional lexical element within it. We suggest that a slight adaptation of their definition is suitable to capture the notion of entailment for any sub-sentential hypotheses, including compositional ones: Definition 1 A sub-sentential hypothesis h is entailed by a text t if there is an explicit or implied reference in t to a possible meaning of h. For example, the sentence “crude steel output is likely to fall in 2000” entails the sub-sentential hypotheses </context>
</contexts>
<marker>Glickman, Shnarch, Dagan, 2006</marker>
<rawString>Oren Glickman, Eyal Shnarch, and Ido Dagan. 2006. Lexical reference: a semantic matching subtask. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Herrera</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>Textual entailment recognition based on dependency analysis and wordnet.</title>
<date>2005</date>
<booktitle>In Proceedings of the First PASCAL RTE Challenge.</booktitle>
<marker>Herrera, Pe˜nas, Verdejo, 2005</marker>
<rawString>Jes´us Herrera, Anselmo Pe˜nas, and Felisa Verdejo. 2005. Textual entailment recognition based on dependency analysis and wordnet. In Proceedings of the First PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="1807" citStr="Kazama and Torisawa, 2007" startWordPosition="255" endWordPosition="259">e for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluated their impact on a specific system. Most often, works which utilized lexical resources do not provide information about their isolated contribution; rather, they only report overall performance for systems in which lexical resources serve as components. </context>
<context position="17717" citStr="Kazama and Torisawa (2007)" startWordPosition="2840" endWordPosition="2843"> ⇒ e is created for each member e&apos; of the cluster. Lin Dependency Similarity (Lin-dep): A distributional word similarity resource based on syntactic-dependency features (Lin, 1998). Given a term e and its list of similar terms, we construct for each e&apos; in the list the rule e’ ⇒ e. This resource was previously used in textual entailment engines, e.g. (Roth and Sammons, 2007). Lin Proximity Similarity (Lin-prox): A knowledgebase of terms with their cooccurrencebased distributionally similar terms. Rules are created from this resource as from the previous one6. Wikipedia first sentence (WikiFS): Kazama and Torisawa (2007) used Wikipedia as an external knowledge to improve Named Entity Recognition. Using the first step of their algorithm, we extracted from the first sentence of each page a noun that appears in a is-a pattern referring to the title. For each such pair we constructed a rule title ⇒ noun (e.g. Michelle Pfeiffer ⇒ actress). The above resources represent various methods for detecting semantic relatedness between words: Manually and semi-automatically constructed (WNd and XWN*, respectively), automatically constructed based on a lexical-syntactic pattern (WikiFS), distributional methods (Lin-dep and </context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Bernardo Magnini</author>
</authors>
<title>Building a large-scale repository of textual entailment rules.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="2114" citStr="Kouylekov and Magnini, 2006" startWordPosition="299" endWordPosition="302">utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluated their impact on a specific system. Most often, works which utilized lexical resources do not provide information about their isolated contribution; rather, they only report overall performance for systems in which lexical resources serve as components. Our paper provides a step towards clarifying this picture. We propose a system- and application-independent evaluation methodology that isolates resources’ performance, and systematically apply it to seven prominent lexicalsemantic resources. The evaluation and analysis methodology is specified within the </context>
</contexts>
<marker>Kouylekov, Magnini, 2006</marker>
<rawString>Milen Kouylekov and Bernardo Magnini. 2006. Building a large-scale repository of textual entailment rules. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurements of observer agreement for categorical data. In Biometrics,</title>
<date>1997</date>
<pages>33--159</pages>
<contexts>
<context position="15559" citStr="Landis and Koch, 1997" startWordPosition="2496" endWordPosition="2499">rieved single sentences as our texts. For each rule applied for an hypothesis h, we sampled 10 sentences from the sentences retrieved for that rule. As a baseline, we also sampled 10 sentences for each original hypothesis h in which both words of h are found. In total, 1550 unique sentences were sampled and annotated by two annotators. To assess the validity of our evaluation methodology, the annotators first judged a sample of 220 sentences. The Kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging h0 and h, respectively. These figures correspond to substantial agreement (Landis and Koch, 1997) and are comparable with related semantic annotations (Szpektor et al., 2007; Bhagat et al., 2007). 4.2 Lexical-Semantic Resources We evaluated the following resources: WordNet (WNd): There is no clear agreement regarding which set of WordNet relations is useful for entailment inference. We therefore took a conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ==&gt;- e for each e0 amongst </context>
</contexts>
<marker>Landis, Koch, 1997</marker>
<rawString>J. R. Landis and G. G. Koch. 1997. The measurements of observer agreement for categorical data. In Biometrics, pages 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1650" citStr="Lin, 1998" startWordPosition="232" endWordPosition="233">semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluated their impact on a specific system. Most often, works which utilized lexical resources do not pro</context>
<context position="17271" citStr="Lin, 1998" startWordPosition="2770" endWordPosition="2771">ansformed into logical form axioms. From this representation we created a rule e’ ==&gt;- e for each e0 in the gloss which was tagged as referring to the same entity as e. CBC: A knowledgebase of labeled clusters generated by the statistical clustering and labeling algorithms in (Pantel and Lin, 2002; Pantel and 4Available at http://ai.stanford.edu/˜rion/swn 561 Ravichandran, 2004)5. Given a cluster label e, an entailment rule e’ ⇒ e is created for each member e&apos; of the cluster. Lin Dependency Similarity (Lin-dep): A distributional word similarity resource based on syntactic-dependency features (Lin, 1998). Given a term e and its list of similar terms, we construct for each e&apos; in the list the rule e’ ⇒ e. This resource was previously used in textual entailment engines, e.g. (Roth and Sammons, 2007). Lin Proximity Similarity (Lin-prox): A knowledgebase of terms with their cooccurrencebased distributionally similar terms. Rules are created from this resource as from the previous one6. Wikipedia first sentence (WikiFS): Kazama and Torisawa (2007) used Wikipedia as an external knowledge to improve Named Entity Recognition. Using the first step of their algorithm, we extracted from the first sentenc</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="29042" citStr="Miller et al., 1993" startWordPosition="4642" endWordPosition="4645">ng inference systems to utilize them only when appropriate mechanisms for handling logical context are in place. 5.4 Rules Priors In Section 5.1.1 we observed that some resources are highly sensitive to context. Hence, when considering the validity of a rule’s application, two factors should be regarded: the actual context in which the rule is to be applied, as well as the rule’s prior likelihood to be valid in an arbitrary context. Somewhat indicative, yet mostly indirect, information about rules’ priors is contained in some resources. This includes sense ranks in WordNet, SemCor statistics (Miller et al., 1993), and similarity scores and rankings in Lin’s resources. Inference systems often incorporated this information, typically as top-k or threshold-based filters (Pantel and Lin, 2003; Roth and Sammons, 2007). By empirically assessing the effect of several such filters in our setting, we found that this type of data is indeed informative in the sense that precision increases as the threshold rises. Yet, no specific filters were found to improve results in terms of F1 score (where recall is measured relatively to the yield of the unfiltered resource) due to a significant drop in relative recall. Fo</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Vasile Rus</author>
</authors>
<title>Logic form transformation of wordnet and its applicability to question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1552" citStr="Moldovan and Rus, 2001" startWordPosition="215" endWordPosition="218">e is valid, as well as its prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluate</context>
<context position="16637" citStr="Moldovan and Rus, 2001" startWordPosition="2669" endWordPosition="2672"> textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ==&gt;- e for each e0 amongst the synonyms or direct hyponyms for all senses of e in WordNet 3.0. Snow (Snow30k): Snow et al. (2006) presented a probabilistic model for taxonomy induction which considers as features paths in parse trees between related taxonomy nodes. They show that the best performing taxonomy was the one adding 30,000 hyponyms to WordNet. We created an entailment rule for each new hyponym added to WordNet by their algorithm4. LCC’s extended WordNet (XWN&apos;°): In (Moldovan and Rus, 2001) WordNet glosses were transformed into logical form axioms. From this representation we created a rule e’ ==&gt;- e for each e0 in the gloss which was tagged as referring to the same entity as e. CBC: A knowledgebase of labeled clusters generated by the statistical clustering and labeling algorithms in (Pantel and Lin, 2002; Pantel and 4Available at http://ai.stanford.edu/˜rion/swn 561 Ravichandran, 2004)5. Given a cluster label e, an entailment rule e’ ⇒ e is created for each member e&apos; of the cluster. Lin Dependency Similarity (Lin-dep): A distributional word similarity resource based on syntact</context>
</contexts>
<marker>Moldovan, Rus, 2001</marker>
<rawString>Dan Moldovan and Vasile Rus. 2001. Logic form transformation of wordnet and its applicability to question answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD.</booktitle>
<contexts>
<context position="1673" citStr="Pantel and Lin, 2002" startWordPosition="234" endWordPosition="237">ference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluated their impact on a specific system. Most often, works which utilized lexical resources do not provide information about </context>
<context position="16959" citStr="Pantel and Lin, 2002" startWordPosition="2725" endWordPosition="2728">tures paths in parse trees between related taxonomy nodes. They show that the best performing taxonomy was the one adding 30,000 hyponyms to WordNet. We created an entailment rule for each new hyponym added to WordNet by their algorithm4. LCC’s extended WordNet (XWN&apos;°): In (Moldovan and Rus, 2001) WordNet glosses were transformed into logical form axioms. From this representation we created a rule e’ ==&gt;- e for each e0 in the gloss which was tagged as referring to the same entity as e. CBC: A knowledgebase of labeled clusters generated by the statistical clustering and labeling algorithms in (Pantel and Lin, 2002; Pantel and 4Available at http://ai.stanford.edu/˜rion/swn 561 Ravichandran, 2004)5. Given a cluster label e, an entailment rule e’ ⇒ e is created for each member e&apos; of the cluster. Lin Dependency Similarity (Lin-dep): A distributional word similarity resource based on syntactic-dependency features (Lin, 1998). Given a term e and its list of similar terms, we construct for each e&apos; in the list the rule e’ ⇒ e. This resource was previously used in textual entailment engines, e.g. (Roth and Sammons, 2007). Lin Proximity Similarity (Lin-prox): A knowledgebase of terms with their cooccurrencebased</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Automatically discovering word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="29221" citStr="Pantel and Lin, 2003" startWordPosition="4668" endWordPosition="4672">are highly sensitive to context. Hence, when considering the validity of a rule’s application, two factors should be regarded: the actual context in which the rule is to be applied, as well as the rule’s prior likelihood to be valid in an arbitrary context. Somewhat indicative, yet mostly indirect, information about rules’ priors is contained in some resources. This includes sense ranks in WordNet, SemCor statistics (Miller et al., 1993), and similarity scores and rankings in Lin’s resources. Inference systems often incorporated this information, typically as top-k or threshold-based filters (Pantel and Lin, 2003; Roth and Sammons, 2007). By empirically assessing the effect of several such filters in our setting, we found that this type of data is indeed informative in the sense that precision increases as the threshold rises. Yet, no specific filters were found to improve results in terms of F1 score (where recall is measured relatively to the yield of the unfiltered resource) due to a significant drop in relative recall. For example, Lin564 prox loses more than 40% of its recall when only the top-50 rules for each hypothesis are exploited, and using only the first sense of WNd costs the resource ove</context>
</contexts>
<marker>Pantel, Lin, 2003</marker>
<rawString>Patrick Pantel and Dekang Lin. 2003. Automatically discovering word senses. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Rahul Bhagat</author>
<author>Bonaventura Coppola</author>
<author>Timothy Chklovski</author>
<author>Eduard Hovy</author>
</authors>
<title>ISP: Learning inferential selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="32743" citStr="Pantel et al., 2007" startWordPosition="5208" endWordPosition="5211">the desired contents of lexical entailment resources. We empirically observed that the rules satisfying their definition are a proper subset of the rules covered by our definition. Dagan and Glickman (2004) referred to entailment at the subsentential level by assigning truth values to subpropositional text fragments through their existential meaning. We find this criterion too permissive. For instance, the existence of country implies the existence of its flag. Yet, the meaning of flag is typically not implied by country. Previous works assessing rule application via human annotation include (Pantel et al., 2007; Szpektor et al., 2007), which evaluate acquisition methods for lexical-syntactic rules. They posed an additional question to the annotators asking them to filter out invalid contexts. In our methodology implicit context matching for the full hypothesis was applied instead. Other related instance-based evaluations (Giuliano and Gliozzo, 2007; Connor and Roth, 2007) performed lexical substitutions, but did not handle the non-substitutable cases. 7 Conclusions This paper provides several methodological and empirical contributions. We presented a novel evaluation methodology for the utility of l</context>
</contexts>
<marker>Pantel, Bhagat, Coppola, Chklovski, Hovy, 2007</marker>
<rawString>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>The informative role of wordnet in open-domain question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources.</booktitle>
<contexts>
<context position="1403" citStr="Pasca and Harabagiu, 2001" startWordPosition="192" endWordPosition="195">nal methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitativ</context>
</contexts>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>Marius Pasca and Sanda M. Harabagiu. 2001. The informative role of wordnet in open-domain question answering. In Proceedings of NAACL Workshop on WordNet and Other Lexical Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Mark Sammons</author>
</authors>
<title>Semantic and logical inference model for textual entailment.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL-WTEP Workshop.</booktitle>
<contexts>
<context position="2139" citStr="Roth and Sammons, 2007" startWordPosition="303" endWordPosition="306">s to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluated their impact on a specific system. Most often, works which utilized lexical resources do not provide information about their isolated contribution; rather, they only report overall performance for systems in which lexical resources serve as components. Our paper provides a step towards clarifying this picture. We propose a system- and application-independent evaluation methodology that isolates resources’ performance, and systematically apply it to seven prominent lexicalsemantic resources. The evaluation and analysis methodology is specified within the Textual Entailment framew</context>
<context position="17467" citStr="Roth and Sammons, 2007" startWordPosition="2805" endWordPosition="2808">gebase of labeled clusters generated by the statistical clustering and labeling algorithms in (Pantel and Lin, 2002; Pantel and 4Available at http://ai.stanford.edu/˜rion/swn 561 Ravichandran, 2004)5. Given a cluster label e, an entailment rule e’ ⇒ e is created for each member e&apos; of the cluster. Lin Dependency Similarity (Lin-dep): A distributional word similarity resource based on syntactic-dependency features (Lin, 1998). Given a term e and its list of similar terms, we construct for each e&apos; in the list the rule e’ ⇒ e. This resource was previously used in textual entailment engines, e.g. (Roth and Sammons, 2007). Lin Proximity Similarity (Lin-prox): A knowledgebase of terms with their cooccurrencebased distributionally similar terms. Rules are created from this resource as from the previous one6. Wikipedia first sentence (WikiFS): Kazama and Torisawa (2007) used Wikipedia as an external knowledge to improve Named Entity Recognition. Using the first step of their algorithm, we extracted from the first sentence of each page a noun that appears in a is-a pattern referring to the title. For each such pair we constructed a rule title ⇒ noun (e.g. Michelle Pfeiffer ⇒ actress). The above resources represent</context>
<context position="29246" citStr="Roth and Sammons, 2007" startWordPosition="4673" endWordPosition="4676">o context. Hence, when considering the validity of a rule’s application, two factors should be regarded: the actual context in which the rule is to be applied, as well as the rule’s prior likelihood to be valid in an arbitrary context. Somewhat indicative, yet mostly indirect, information about rules’ priors is contained in some resources. This includes sense ranks in WordNet, SemCor statistics (Miller et al., 1993), and similarity scores and rankings in Lin’s resources. Inference systems often incorporated this information, typically as top-k or threshold-based filters (Pantel and Lin, 2003; Roth and Sammons, 2007). By empirically assessing the effect of several such filters in our setting, we found that this type of data is indeed informative in the sense that precision increases as the threshold rises. Yet, no specific filters were found to improve results in terms of F1 score (where recall is measured relatively to the yield of the unfiltered resource) due to a significant drop in relative recall. For example, Lin564 prox loses more than 40% of its recall when only the top-50 rules for each hypothesis are exploited, and using only the first sense of WNd costs the resource over 60% of its recall. We t</context>
</contexts>
<marker>Roth, Sammons, 2007</marker>
<rawString>Dan Roth and Mark Sammons. 2007. Semantic and logical inference model for textual entailment. In Proceedings ofACL-WTEP Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chirag Shah</author>
<author>Bruce W Croft</author>
</authors>
<title>Evaluating high accuracy retrieval techniques.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1375" citStr="Shah and Croft, 2004" startWordPosition="188" endWordPosition="191"> output of distributional methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works</context>
</contexts>
<marker>Shah, Croft, 2004</marker>
<rawString>Chirag Shah and Bruce W. Croft. 2004. Evaluating high accuracy retrieval techniques. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1572" citStr="Snow et al., 2006" startWordPosition="219" endWordPosition="222">ts prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluated their impact on a </context>
<context position="16261" citStr="Snow et al. (2006)" startWordPosition="2609" endWordPosition="2612"> et al., 2007). 4.2 Lexical-Semantic Resources We evaluated the following resources: WordNet (WNd): There is no clear agreement regarding which set of WordNet relations is useful for entailment inference. We therefore took a conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ==&gt;- e for each e0 amongst the synonyms or direct hyponyms for all senses of e in WordNet 3.0. Snow (Snow30k): Snow et al. (2006) presented a probabilistic model for taxonomy induction which considers as features paths in parse trees between related taxonomy nodes. They show that the best performing taxonomy was the one adding 30,000 hyponyms to WordNet. We created an entailment rule for each new hyponym added to WordNet by their algorithm4. LCC’s extended WordNet (XWN&apos;°): In (Moldovan and Rus, 2001) WordNet glosses were transformed into logical form axioms. From this representation we created a rule e’ ==&gt;- e for each e0 in the gloss which was tagged as referring to the same entity as e. CBC: A knowledgebase of labeled</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A core of semantic knowledge - unifying wordnet and wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="1779" citStr="Suchanek et al., 2007" startWordPosition="251" endWordPosition="254">ide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). As of today, only a partial comparative picture is available regarding the actual utility and limitations of available resources for lexical-semantic inference. Works that do provide quantitative information regarding resources utility have focused on few particular resources (Kouylekov and Magnini, 2006; Roth and Sammons, 2007) and evaluated their impact on a specific system. Most often, works which utilized lexical resources do not provide information about their isolated contribution; rather, they only report overall performance for systems in which lexical res</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A core of semantic knowledge - unifying wordnet and wikipedia. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Eyal Shnarch</author>
<author>Ido Dagan</author>
</authors>
<title>Instance-based evaluation of entailment rule acquisition.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15635" citStr="Szpektor et al., 2007" startWordPosition="2507" endWordPosition="2510"> h, we sampled 10 sentences from the sentences retrieved for that rule. As a baseline, we also sampled 10 sentences for each original hypothesis h in which both words of h are found. In total, 1550 unique sentences were sampled and annotated by two annotators. To assess the validity of our evaluation methodology, the annotators first judged a sample of 220 sentences. The Kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging h0 and h, respectively. These figures correspond to substantial agreement (Landis and Koch, 1997) and are comparable with related semantic annotations (Szpektor et al., 2007; Bhagat et al., 2007). 4.2 Lexical-Semantic Resources We evaluated the following resources: WordNet (WNd): There is no clear agreement regarding which set of WordNet relations is useful for entailment inference. We therefore took a conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ==&gt;- e for each e0 amongst the synonyms or direct hyponyms for all senses of e in WordNet 3.0. Snow (Sn</context>
<context position="32767" citStr="Szpektor et al., 2007" startWordPosition="5212" endWordPosition="5215">of lexical entailment resources. We empirically observed that the rules satisfying their definition are a proper subset of the rules covered by our definition. Dagan and Glickman (2004) referred to entailment at the subsentential level by assigning truth values to subpropositional text fragments through their existential meaning. We find this criterion too permissive. For instance, the existence of country implies the existence of its flag. Yet, the meaning of flag is typically not implied by country. Previous works assessing rule application via human annotation include (Pantel et al., 2007; Szpektor et al., 2007), which evaluate acquisition methods for lexical-syntactic rules. They posed an additional question to the annotators asking them to filter out invalid contexts. In our methodology implicit context matching for the full hypothesis was applied instead. Other related instance-based evaluations (Giuliano and Gliozzo, 2007; Connor and Roth, 2007) performed lexical substitutions, but did not handle the non-substitutable cases. 7 Conclusions This paper provides several methodological and empirical contributions. We presented a novel evaluation methodology for the utility of lexicalsemantic resources</context>
</contexts>
<marker>Szpektor, Shnarch, Dagan, 2007</marker>
<rawString>Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance-based evaluation of entailment rule acquisition. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
<author>Roy Bar-Haim</author>
<author>Jacob Goldberger</author>
</authors>
<title>Contextual preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="30044" citStr="Szpektor et al., 2008" startWordPosition="4815" endWordPosition="4818"> threshold rises. Yet, no specific filters were found to improve results in terms of F1 score (where recall is measured relatively to the yield of the unfiltered resource) due to a significant drop in relative recall. For example, Lin564 prox loses more than 40% of its recall when only the top-50 rules for each hypothesis are exploited, and using only the first sense of WNd costs the resource over 60% of its recall. We thus suggest a better strategy might be to combine the prior information with context matching scores in order to obtain overall likelihood scores for rule applications, as in (Szpektor et al., 2008). Furthermore, resources should include explicit information regarding the prior likelihoods of of their rules. 5.5 Operative Conclusions Our findings highlight the currently limited recall of available resources for lexical inference. The higher recall of Lin’s resources indicates that many more entailment relationships can be acquired, particularly when considering distributional evidence. Yet, available distributional acquisition methods are not geared for lexical entailment. This suggests the need to develop acquisition methods for dedicated and more extensive knowledge resources that woul</context>
</contexts>
<marker>Szpektor, Dagan, Bar-Haim, Goldberger, 2008</marker>
<rawString>Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Goldberger. 2008. Contextual preferences. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Query expansion using lexical-semantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="9425" citStr="Voorhees (1994)" startWordPosition="1472" endWordPosition="1473">ncluded in lexical-semantic resources, as described in Section 5. A rule need not apply in all contexts, as long as it is appropriate for some texts. Two contextual aspects affect rule applicability. First is the “lexical context” specifying the meanings of the text’s words. A rules is applicable in a certain context only when the intended sense of its LHS term matches the sense of that term in the text. For example, the application of the rule lay ⇒ produce is valid only in contexts where the producer is poultry and the products are eggs. This is a well known issue observed, for instance, by Voorhees (1994). A second contextual factor requiring validation is the “logical context”. The logical context determines the monotonicity of the LHS and is induced by logical operators such as negation and (explicit or implicit) quantifiers. For example, the rule mammal ⇒ whale may not be valid in most cases, but is applicable in universally quantified texts like “mammals are warm-blooded”. This issue has been rarely addressed in applied inference systems (de Marneffe et al., 2006). The above mentioned rules both comply with Definition 2 and should therefore be included in a lexical entailment resource. 3 E</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Ellen M. Voorhees. 1994. Query expansion using lexical-semantic relations. In Proceedings of SIGIR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>