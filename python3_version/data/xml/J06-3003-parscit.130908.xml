<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993607">
Similarity of Semantic Relations
</title>
<author confidence="0.997893">
Peter D. Turney*
</author>
<affiliation confidence="0.96536">
National Research Council Canada
</affiliation>
<bodyText confidence="0.998536352941176">
There are at least two kinds of similarity. Relational similarity is correspondence between re-
lations, in contrast with attributional similarity, which is correspondence between attributes.
When two words have a high degree of attributional similarity, we call them synonyms. When
two pairs of words have a high degree of relational similarity, we say that their relations are
analogous. For example, the word pair mason:stone is analogous to the pair carpenter:wood.
This article introduces Latent Relational Analysis (LRA), a method for measuring relational
similarity. LRA has potential applications in many areas, including information extraction,
word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM)
of information retrieval has been adapted to measuring relational similarity, achieving a score
of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the
VSM approach, the relation between a pair of words is characterized by a vector offrequencies
of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1)
The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition
(SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are
used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions,
statistically equivalent to the average human score of 57%. On the related problem of classifying
semantic relations, LRA achieves similar gains over the VSM.
</bodyText>
<sectionHeader confidence="0.996143" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999907461538462">
There are at least two kinds of similarity. Attributional similarity is correspondence be-
tween attributes and relational similarity is correspondence between relations (Medin,
Goldstone, and Gentner 1990). When two words have a high degree of attributional
similarity, we call them synonyms. When two word pairs have a high degree of relational
similarity, we say they are analogous.
Verbal analogies are often written in the form A:B::C:D, meaning A is to B as C is to
D; for example, traffic:street::water:riverbed. Traffic flows over a street; water flows over
a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of
relational similarity between the word pair traffic:street and the word pair water:riverbed.
In fact, this analogy is the basis of several mathematical theories of traffic flow (Daganzo
1994).
In Section 2, we look more closely at the connections between attributional and
relational similarity. In analogies such as mason:stone::carpenter:wood, it seems that
</bodyText>
<note confidence="0.796452333333333">
* Institute for Information Technology, National Research Council Canada, M-50 Montreal Road, Ottawa,
Ontario, Canada K1A 0R6. E-mail: peter.turney@nrc-cnrc.gc.ca.
Submission received: 30 March 2005; revised submission received: 10 November 2005; accepted for
publication: 27 February 2006.
© 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999874930232558">
relational similarity can be reduced to attributional similarity, since mason and carpen-
ter are attributionally similar, as are stone and wood. In general, this reduction fails.
Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally
similar. Street and riverbed are only moderately attributionally similar.
Many algorithms have been proposed for measuring the attributional similar-
ity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang
and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and
Pedersen 2003). Measures of attributional similarity have been studied extensively, due
to their applications in problems such as recognizing synonyms (Landauer and Dumais
1997), information retrieval (Deerwester et al. 1990), determining semantic orientation
(Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion
(Morris and Hirst 1991), and word sense disambiguation (Lesk 1986).
On the other hand, since measures of relational similarity are not as well developed
as measures of attributional similarity, the potential applications of relational similarity
are not as well known. Many problems that involve semantic relations would benefit
from an algorithm for measuring relational similarity. We discuss related problems in
natural language processing, information retrieval, and information extraction in more
detail in Section 3.
This article builds on the Vector Space Model (VSM) of information retrieval. Given
a query, a search engine produces a ranked list of documents. The documents are
ranked in order of decreasing attributional similarity between the query and each
document. Almost all modern search engines measure attributional similarity using
the VSM (Baeza-Yates and Ribeiro-Neto 1999). Turney and Littman (2005) adapt the
VSM approach to measuring relational similarity. They used a vector of frequencies of
patterns in a corpus to represent the relation between a pair of words. Section 4 presents
the VSM approach to measuring similarity.
In Section 5, we present an algorithm for measuring relational similarity, which
we call Latent Relational Analysis (LRA). The algorithm learns from a large corpus
of unlabeled, unstructured text, without supervision. LRA extends the VSM approach
of Turney and Littman (2005) in three ways: (1) The connecting patterns are derived
automatically from the corpus, instead of using a fixed set of patterns. (2) Singular Value
Decomposition (SVD) is used to smooth the frequency data. (3) Given a word pair such
as traffic:street, LRA considers transformations of the word pair, generated by replacing
one of the words by synonyms, such as traffic:road or traffic:highway.
Section 6 presents our experimental evaluation of LRA with a collection of 374
multiple-choice word analogy questions from the SAT college entrance exam.1 An ex-
ample of a typical SAT question appears in Table 1. In the educational testing literature,
the first pair (mason:stone) is called the stem of the analogy. The correct choice is called the
solution and the incorrect choices are distractors. We evaluate LRA by testing its ability
to select the solution and avoid the distractors. The average performance of college-
bound senior high school students on verbal SAT questions corresponds to an accuracy
of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the
VSM attained 47%.
</bodyText>
<footnote confidence="0.9132474">
1 The College Board eliminated analogies from the SAT in 2005, apparently because it was believed that
analogy questions discriminate against minorities, although it has been argued by liberals (Goldenberg
2005) that dropping analogy questions has increased discrimination against minorities and by
conservatives (Kurtz 2002) that it has decreased academic standards. Analogy questions remain an
important component in many other tests, such as the GRE.
</footnote>
<page confidence="0.989352">
380
</page>
<note confidence="0.980851">
Turney Similarity of Semantic Relations
</note>
<tableCaption confidence="0.989903">
Table 1
</tableCaption>
<figure confidence="0.89875775">
An example of a typical SAT question, from the collection of 374 questions.
Stem: mason:stone
Choices: (a) teacher:chalk
(b) carpenter:wood
(c) soldier:gun
(d) photograph:camera
(e) book:word
Solution: (b) carpenter:wood
</figure>
<bodyText confidence="0.999851352941176">
One application for relational similarity is classifying semantic relations in noun-
modifier pairs (Turney and Littman 2005). In Section 7, we evaluate the performance
of LRA with a set of 600 noun-modifier pairs from Nastase and Szpakowicz (2003).
The problem is to classify a noun-modifier pair, such as “laser printer,” according to
the semantic relation between the head noun (printer) and the modifier (laser). The
600 pairs have been manually labeled with 30 classes of semantic relations. For example,
“laser printer” is classified as instrument; the printer uses the laser as an instrument for
printing.
We approach the task of classifying semantic relations in noun-modifier pairs as a
supervised learning problem. The 600 pairs are divided into training and testing sets
and a testing pair is classified according to the label of its single nearest neighbor in the
training set. LRA is used to measure distance (i.e., similarity, nearness). LRA achieves
an accuracy of 39.8% on the 30-class problem and 58.0% on the 5-class problem. On the
same 600 noun-modifier pairs, the VSM had accuracies of 27.8% (30-class) and 45.7%
(5-class) (Turney and Littman 2005).
We discuss the experimental results, limitations of LRA, and future work in Sec-
tion 8 and we conclude in Section 9.
</bodyText>
<sectionHeader confidence="0.966943" genericHeader="categories and subject descriptors">
2. Attributional and Relational Similarity
</sectionHeader>
<bodyText confidence="0.945187">
In this section, we explore connections between attributional and relational similarity.
</bodyText>
<subsectionHeader confidence="0.986124">
2.1 Types of Similarity
</subsectionHeader>
<bodyText confidence="0.994348454545455">
Medin, Goldstone, and Gentner (1990) distinguish attributes and relations as follows:
Attributes are predicates taking one argument (e.g., X is red, X is large), whereas
relations are predicates taking two or more arguments (e.g., X collides with Y, X is
larger than Y). Attributes are used to state properties of objects; relations express
relations between objects or propositions.
Gentner (1983) notes that what counts as an attribute or a relation can depend on the
context. For example, large can be viewed as an attribute of X, LARGE(X), or a relation
between X and some standard Y, LARGER THAN(X, Y).
The amount of attributional similarity between two words, A and B, depends
on the degree of correspondence between the properties of A and B. A measure of
attributional similarity is a function that maps two words, A and B, to a real number,
</bodyText>
<page confidence="0.994157">
381
</page>
<note confidence="0.804408">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999457456521739">
sima(A,B) E R. The more correspondence there is between the properties of A and B,
the greater their attributional similarity. For example, dog and wolf have a relatively
high degree of attributional similarity.
The amount of relational similarity between two pairs of words, A:B and C:D,
depends on the degree of correspondence between the relations between A and B
and the relations between C and D. A measure of relational similarity is a function
that maps two pairs, A:B and C:D, to a real number, simr(A:B,C:D) E R. The more
correspondence there is between the relations of A:B and C:D, the greater their relational
similarity. For example, dog:bark and cat:meow have a relatively high degree of relational
similarity.
Cognitive scientists distinguish words that are semantically associated (bee–honey)
from words that are semantically similar (deer–pony), although they recognize that some
words are both associated and similar (doctor–nurse) (Chiarello et al. 1990). Both of these
are types of attributional similarity, since they are based on correspondence between
attributes (e.g., bees and honey are both found in hives; deer and ponies are both
mammals).
Budanitsky and Hirst (2001) describe semantic relatedness as follows:
Recent research on the topic in computational linguistics has emphasized the
perspective of semantic relatedness of two lexemes in a lexical resource, or its inverse,
semantic distance. It’s important to note that semantic relatedness is a more general
concept than similarity; similar entities are usually assumed to be related by virtue of
their likeness (bank–trust company), but dissimilar entities may also be semantically
related by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold),
or just by any kind of functional relationship or frequent association (pencil–paper,
penguin–Antarctica).
As these examples show, semantic relatedness is the same as attributional similarity
(e.g., hot and cold are both kinds of temperature, pencil and paper are both used for
writing). Here we prefer to use the term attributional similarity because it emphasizes the
contrast with relational similarity. The term semantic relatedness may lead to confusion
when the term relational similarity is also under discussion.
Resnik (1995) describes semantic similarity as follows:
Semantic similarity represents a special case of semantic relatedness: for example, cars
and gasoline would seem to be more closely related than, say, cars and bicycles, but the
latter pair are certainly more similar. Rada et al. (1989) suggest that the assessment of
similarity in semantic networks can in fact be thought of as involving just taxonomic
(IS-A) links, to the exclusion of other link types; that view will also be taken here,
although admittedly it excludes some potentially useful information.
Thus semantic similarity is a specific type of attributional similarity. The term semantic
similarity is misleading, because it refers to a type of attributional similarity, yet rela-
tional similarity is not any less semantic than attributional similarity.
To avoid confusion, we will use the terms attributional similarity and relational
similarity, following Medin, Goldstone, and Gentner (1990). Instead of semantic sim-
ilarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term
taxonomical similarity, which we take to be a specific type of attributional similarity. We
interpret synonymy as a high degree of attributional similarity. Analogy is a high degree
of relational similarity.
</bodyText>
<page confidence="0.987043">
382
</page>
<note confidence="0.969624">
Turney Similarity of Semantic Relations
</note>
<subsectionHeader confidence="0.973724">
2.2 Measuring Attributional Similarity
</subsectionHeader>
<bodyText confidence="0.999666357142857">
Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986;
Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969;
Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik
1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect
that lexicon-based algorithms would be better at capturing synonymy than corpus-
based algorithms, since lexicons, such as WordNet, explicitly provide synonymy in-
formation that is only implicit in a corpus. However, experiments do not support this
intuition.
Several algorithms have been evaluated using 80 multiple-choice synonym ques-
tions taken from the Test of English as a Foreign Language (TOEFL). An example of
one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best performance
on the TOEFL questions for each type of attributional similarity algorithm. The results
support the claim that lexicon-based algorithms have no advantage over corpus-based
algorithms for recognizing synonymy.
</bodyText>
<subsectionHeader confidence="0.999">
2.3 Using Attributional Similarity to Solve Analogies
</subsectionHeader>
<bodyText confidence="0.9999034">
We may distinguish near analogies (mason:stone::carpenter:wood) from far anal-
ogies (traffic:street::water:riverbed) (Gentner 1983; Medin, Goldstone, and Gentner 1990).
In an analogy A:B::C:D, where there is a high degree of relational similarity between
A:B and C:D, if there is also a high degree of attributional similarity between A
and C, and between B and D, then A:B::C:D is a near analogy; otherwise, it is a far
analogy.
It seems possible that SAT analogy questions might consist largely of near analogies,
in which case they can be solved using attributional similarity measures. We could score
each candidate analogy by the average of the attributional similarity, sima, between A
and C and between B and D:
</bodyText>
<equation confidence="0.998739">
score(A:B::C:D) = 12(sima(A,C)+sima(B,D)) (1)
</equation>
<bodyText confidence="0.9729795">
This kind of approach was used in two of the thirteen modules in Turney et al. (2003)
(see Section 3.1).
</bodyText>
<tableCaption confidence="0.806479">
Table 2
</tableCaption>
<bodyText confidence="0.62036">
An example of a typical TOEFL question, from the collection of 80 questions.
Stem: Levied
</bodyText>
<listItem confidence="0.8487504">
Choices: (a) imposed
(b) believed
(c) requested
(d) correlated
Solution: (a) imposed
</listItem>
<page confidence="0.996742">
383
</page>
<note confidence="0.768887">
Computational Linguistics Volume 32, Number 3
</note>
<tableCaption confidence="0.975833">
Table 3
</tableCaption>
<note confidence="0.658535">
Performance of attributional similarity measures on the 80 TOEFL questions. (The average
non-English US college applicant’s performance is included in the bottom row, for comparison.)
</note>
<tableCaption confidence="0.3733102">
Reference Description Percent correct
Jarmasz and Szpakowicz (2003) Best lexicon-based algorithm 78.75
Terra and Clarke (2003) Best corpus-based algorithm 81.25
Turney et al. (2003) Best hybrid algorithm 97.50
Landauer and Dumais (1997) Average human score 64.50
</tableCaption>
<bodyText confidence="0.988249333333333">
To evaluate this approach, we applied several measures of attributional similarity to
our collection of 374 SAT questions. The performance of the algorithms was measured
by precision, recall, and F, defined as follows:
</bodyText>
<figure confidence="0.9686533">
number of correct guesses
precision =
(2)
total number of guesses made
recall =
number of correct guesses
(3)
maximum possible number correct
_ 2 × precision × recall ( )
F precision + recall 4
</figure>
<bodyText confidence="0.999911277777778">
Note that recall is the same as percent correct (for multiple-choice questions, with only
zero or one guesses allowed per question, but not in general).
Table 4 shows the experimental results for our set of 374 analogy questions. For
example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered
correctly, 224 incorrectly, and 30 questions were skipped. When the algorithm assigned
the same similarity to all of the choices for a given question, that question was skipped.
The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30).
The first five algorithms in Table 4 are implemented in Pedersen’s WordNet-
Similarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText
System (WMTS), as described in Terra and Clarke (2003).
The difference between the lowest performance (Jiang and Conrath 1997) and ran-
dom guessing is statistically significant with 95% confidence, according to the Fisher
Exact Test (Agresti 1990). However, the difference between the highest performance
(Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically
significant with 95% confidence. We conclude that there are enough near analogies
in the 374 SAT questions for attributional similarity to perform better than random
guessing, but not enough near analogies for attributional similarity to perform as well
as relational similarity.
</bodyText>
<footnote confidence="0.979458">
2 See http://www.d.umn.edu/∼tpederse/similarity.html.
</footnote>
<page confidence="0.997141">
384
</page>
<note confidence="0.923092">
Turney Similarity of Semantic Relations
</note>
<sectionHeader confidence="0.998773" genericHeader="related work">
3. Related Work
</sectionHeader>
<bodyText confidence="0.9988525">
This section is a brief survey of the many problems that involve semantic relations and
could potentially make use of an algorithm for measuring relational similarity.
</bodyText>
<subsectionHeader confidence="0.999926">
3.1 Recognizing Word Analogies
</subsectionHeader>
<bodyText confidence="0.99997265">
The problem of recognizing word analogies is, given a stem word pair and a finite list of
choice word pairs, selecting the choice that is most analogous to the stem. This problem
was first attempted by a system called Argus (Reitman 1965), using a small hand-built
semantic network. Argus could only solve the limited set of analogy questions that its
programmer had anticipated. Argus was based on a spreading activation model and
did not explicitly attempt to measure relational similarity.
Turney et al. (2003) combined 13 independent modules to answer SAT questions.
The final output of the system was based on a weighted combination of the outputs of
each individual module. The best of the 13 modules was the VSM, which is described
in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT
questions, achieving a score of 47%.
In contrast with the corpus-based approach of Turney and Littman (2005), Veale
(2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score
of 43%. Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths
in WordNet, joining A to B and C to D. The quality measure was based on the similarity
between the A:B paths and the C:D paths.
Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version
of the VSM approach, which reached 56% on the 374 SAT questions. Here we go
beyond Turney (2005) by describing LRA in more detail, performing more extensive
experiments, and analyzing the algorithm and related work in more depth.
</bodyText>
<subsectionHeader confidence="0.999233">
3.2 Structure Mapping Theory
</subsectionHeader>
<bodyText confidence="0.99992925">
French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its imple-
mentation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner
1989) as the most influential work on modeling of analogy making. The goal of com-
putational modeling of analogy making is to understand how people form complex,
</bodyText>
<tableCaption confidence="0.996196">
Table 4
</tableCaption>
<table confidence="0.973364666666667">
Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and
F are reported as percentages. (The bottom two rows are not attributional similarity measures.
They are included for comparison.)
Algorithm Type Precision Recall F
Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4
Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5
Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0
Lin (1998b) Hybrid 31.2 27.3 29.1
Resnik (1995) Hybrid 35.7 33.2 34.4
Turney (2001) Corpus-based 35.0 35.0 35.0
Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4
Random Random 20.0 20.0 20.0
</table>
<page confidence="0.962597">
385
</page>
<note confidence="0.293328">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999453428571428">
structured analogies. SME takes representations of a source domain and a target domain
and produces an analogical mapping between the source and target. The domains
are given structured propositional representations, using predicate logic. These de-
scriptions include attributes, relations, and higher-order relations (expressing relations
between relations). The analogical mapping connects source domain relations to target
domain relations.
For example, there is an analogy between the solar system and Rutherford’s model
of the atom (Falkenhainer, Forbus, and Gentner 1989). The solar system is the source
domain and Rutherford’s model of the atom is the target domain. The basic objects in
the source model are the planets and the sun. The basic objects in the target model are
the electrons and the nucleus. The planets and the sun have various attributes, such
as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and
attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as
charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus)
and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nu-
cleus) and attracts(sun, planet) to attracts(nucleus, electron).
Each individual connection (e.g., from revolve(planet, sun) to revolve(electron, nu-
cleus)) in an analogical mapping implies that the connected relations are similar; thus,
SMT requires a measure of relational similarity in order to form maps. Early versions
of SME only mapped identical relations, but later versions of SME allowed similar,
nonidentical relations to match (Falkenhainer 1990). However, the focus of research in
analogy making has been on the mapping process as a whole, rather than measuring
the similarity between any two particular relations; hence, the similarity measures used
in SME at the level of individual connections are somewhat rudimentary.
We believe that a more sophisticated measure of relational similarity, such as LRA,
may enhance the performance of SME. Likewise, the focus of our work here is on the
similarity between particular relations, and we ignore systematic mapping between sets
of relations, so LRA may also be enhanced by integration with SME.
</bodyText>
<subsectionHeader confidence="0.99067">
3.3 Metaphor
</subsectionHeader>
<bodyText confidence="0.999972214285714">
Metaphorical language is very common in our daily life, so common that we are usually
unaware of it (Lakoff and Johnson 1980). Gentner et al. (2001) argue that novel metaphors
are understood using analogy, but conventional metaphors are simply recalled from
memory. A conventional metaphor is a metaphor that has become entrenched in our
language (Lakoff and Johnson 1980). Dolan (1995) describes an algorithm that can
recognize conventional metaphors, but is not suited to novel metaphors. This suggests
that it may be fruitful to combine Dolan’s (1995) algorithm for handling conventional
metaphorical language with LRA and SME for handling novel metaphors.
Lakoff and Johnson (1980) give many examples of sentences in support of their
claim that metaphorical language is ubiquitous. The metaphors in their sample sen-
tences can be expressed using SAT-style verbal analogies of the form A:B::C:D. The first
column in Table 5 is a list of sentences from Lakoff and Johnson (1980) and the second
column shows how the metaphor that is implicit in each sentence may be made explicit
as a verbal analogy.
</bodyText>
<subsectionHeader confidence="0.993209">
3.4 Classifying Semantic Relations
</subsectionHeader>
<bodyText confidence="0.9996605">
The task of classifying semantic relations is to identify the relation between a pair
of words. Often the pairs are restricted to noun-modifier pairs, but there are many
</bodyText>
<page confidence="0.999026">
386
</page>
<note confidence="0.90546">
Turney Similarity of Semantic Relations
</note>
<tableCaption confidence="0.7822318">
Table 5
Metaphorical sentences from Lakoff and Johnson (1980), rendered as SAT-style verbal analogies.
Metaphorical sentence SAT-style verbal analogy
He shot down all of my arguments. aircraft:shoot down::argument:refute
I demolished his argument. building:demolish::argument:refute
You need to budget your time. money:budget::time:schedule
I’ve invested a lot of time in her. money:invest::time:allocate
My mind just isn’t operating today. machine:operate::mind:think
Life has cheated me. charlatan:cheat::life:disappoint
Inflation is eating up our profits. animal:eat::inflation:reduce
</tableCaption>
<bodyText confidence="0.999618419354839">
interesting relations, such as antonymy, that do not occur in noun-modifier pairs. How-
ever, noun-modifier pairs are interesting due to their high frequency in English. For
instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many
common noun-modifiers are not in WordNet, especially technical terms.
Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify noun-
modifier relations in the medical domain, using Medical Subject Headings (MeSH) and
Unified Medical Language System (UMLS) as lexical resources for representing each
noun-modifier pair with a feature vector. They trained a neural network to distinguish
13 classes of semantic relations. Nastase and Szpakowicz (2003) explore a similar ap-
proach to classifying general noun-modifier pairs (i.e., not restricted to a particular
domain, such as medicine), using WordNet and Roget’s Thesaurus as lexical resources.
Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to
classify noun-modifier pairs.
None of these approaches explicitly involved measuring relational similarity, but
any classification of semantic relations necessarily employs some implicit notion of re-
lational similarity since members of the same class must be relationally similar to some
extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used
a measure of relational similarity, but their measure was based on literal matching,
which limited its ability to generalize. Moldovan et al. (2004) also used a measure of
relational similarity based on mapping each noun and modifier into semantic classes
in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding
context in the corpus was used in a word sense disambiguation algorithm to improve
the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used
the VSM (as a component in a single nearest neighbor learning algorithm) to measure
relational similarity. We take the same approach here, substituting LRA for the VSM, in
Section 7.
Lauer (1995) used a corpus-based approach (using the BNC) to paraphrase noun–
modifier pairs by inserting the prepositions of, for, in, at, on, from, with, and about. For
example, reptile haven was paraphrased as haven for reptiles. Lapata and Keller (2004)
achieved improved results on this task by using the database of AltaVista’s search
engine as a corpus.
</bodyText>
<subsectionHeader confidence="0.970721">
3.5 Word Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.949008">
We believe that the intended sense of a polysemous word is determined by its semantic
relations with the other words in the surrounding text. If we can identify the semantic
</bodyText>
<page confidence="0.991495">
387
</page>
<note confidence="0.302836">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999807176470588">
relations between the given word and its context, then we can disambiguate the given
word. Yarowsky’s (1993) observation that collocations are almost always monosemous
is evidence for this view. Federici, Montemagni, and Pirrelli (1997) present an analogy-
based approach to word sense disambiguation.
For example, consider the word plant. Out of context, plant could refer to an in-
dustrial plant or a living organism. Suppose plant appears in some text near food. A
typical approach to disambiguating plant would compare the attributional similarity
of food and industrial plant to the attributional similarity of food and living organism
(Lesk 1986; Banerjee and Pedersen 2003). In this case, the decision may not be clear,
since industrial plants often produce food and living organisms often serve as food. It
would be very helpful to know the relation between food and plant in this example. In
the phrase “food for the plant,” the relation between food and plant strongly suggests
that the plant is a living organism, since industrial plants do not need food. In the
text “food at the plant,” the relation strongly suggests that the plant is an industrial
plant, since living organisms are not usually considered as locations. Thus, an algorithm
for classifying semantic relations (as in Section 7) should be helpful for word sense
disambiguation.
</bodyText>
<subsectionHeader confidence="0.978225">
3.6 Information Extraction
</subsectionHeader>
<bodyText confidence="0.999684413793104">
The problem of relation extraction is, given an input document and a specific relation R,
to extract all pairs of entities (if any) that have the relation R in the document. The prob-
lem was introduced as part of the Message Understanding Conferences (MUC) in 1998.
Zelenko, Aone, and Richardella (2003) present a kernel method for extracting the
relations person–affiliation and organization–location. For example, in the sentence John
Smith is the chief scientist of the Hardcom Corporation, there is a person–affiliation relation
between John Smith and Hardcom Corporation (Zelenko, Aone, and Richardella 2003).
This is similar to the problem of classifying semantic relations (Section 3.4), except
that information extraction focuses on the relation between a specific pair of entities
in a specific document, rather than a general pair of words in general text. There-
fore an algorithm for classifying semantic relations should be useful for information
extraction.
In the VSM approach to classifying semantic relations (Turney and Littman 2005),
we would have a training set of labeled examples of the relation person–affiliation,
for instance. Each example would be represented by a vector of pattern frequencies.
Given a specific document discussing John Smith and Hardcom Corporation, we could
construct a vector representing the relation between these two entities and then mea-
sure the relational similarity between this unlabeled vector and each of our labeled
training vectors. It would seem that there is a problem here because the training
vectors would be relatively dense, since they would presumably be derived from
a large corpus, but the new unlabeled vector for John Smith and Hardcom Corpora-
tion would be very sparse, since these entities might be mentioned only once in the
given document. However, this is not a new problem for the VSM; it is the standard
situation when the VSM is used for information retrieval. A query to a search en-
gine is represented by a very sparse vector, whereas a document is represented by
a relatively dense vector. There are well-known techniques in information retrieval
for coping with this disparity, such as weighting schemes for query vectors that
are different from the weighting schemes for document vectors (Salton and Buckley
1988).
</bodyText>
<page confidence="0.998166">
388
</page>
<note confidence="0.83524">
Turney Similarity of Semantic Relations
</note>
<subsectionHeader confidence="0.997094">
3.7 Question Answering
</subsectionHeader>
<bodyText confidence="0.999493666666667">
In their article on classifying semantic relations, Moldovan et al. (2004) suggest that an
important application of their work is question answering (QA). As defined in the Text
Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as
“Where have nuclear incidents occurred?”, by retrieving a relevant document from a
large corpus and then extracting a short string from the document, such as The Three
Mile Island nuclear incident caused a DOE policy crisis. Moldovan et al. (2004) propose to
map a given question to a semantic relation and then search for that relation in a corpus
of semantically tagged text. They argue that the desired semantic relation can easily be
inferred from the surface form of the question. A question of the form “Where... ?” is
likely to be looking for entities with a location relation and a question of the form “What
did ... make?” is likely to be looking for entities with a product relation. In Section 7, we
show how LRA can recognize relations such as location and product (see Table 19).
</bodyText>
<subsectionHeader confidence="0.993066">
3.8 Automatic Thesaurus Generation
</subsectionHeader>
<bodyText confidence="0.9999765">
Hearst (1992) presents an algorithm for learning hyponym (type of) relations from a
corpus and Berland and Charniak (1999) describe how to learn meronym (part of)
relations from a corpus. These algorithms could be used to automatically generate a
thesaurus or dictionary, but we would like to handle more relations than hyponymy
and meronymy. WordNet distinguishes more than a dozen semantic relations between
words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for
noun-modifier pairs. Hearst and Berland and Charniak (1999) use manually generated
rules to mine text for semantic relations. Turney and Littman (2005) also use a manually
generated set of 64 patterns.
LRA does not use a predefined set of patterns; it learns patterns from a large corpus.
Instead of manually generating new rules or patterns for each new semantic relation,
it is possible to automatically learn a measure of relational similarity that can handle
arbitrary semantic relations. A nearest neighbor algorithm can then use this relational
similarity measure to learn to classify according to any set of classes of relations, given
the appropriate labeled training data.
Girju, Badulescu, and Moldovan (2003) present an algorithm for learning meronym
relations from a corpus. Like Hearst (1992) and Berland and Charniak (1999), they
use manually generated rules to mine text for their desired relation. However, they
supplement their manual rules with automatically learned constraints, to increase the
precision of the rules.
</bodyText>
<subsectionHeader confidence="0.980568">
3.9 Information Retrieval
</subsectionHeader>
<bodyText confidence="0.999912333333333">
Veale (2003) has developed an algorithm for recognizing certain types of word analo-
gies, based on information in WordNet. He proposes to use the algorithm for analog-
ical information retrieval. For example, the query Muslim church should return
mosque and the query Hindu bible should return the Vedas. The algorithm was de-
signed with a focus on analogies of the form adjective:noun::adjective:noun, such as
Christian:church::Muslim:mosque.
A measure of relational similarity is applicable to this task. Given a pair of words,
A and B, the task is to return another pair of words, X and Y, such that there is
high relational similarity between the pair A:X and the pair Y:B. For example, given
</bodyText>
<page confidence="0.996082">
389
</page>
<note confidence="0.303351">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999424416666667">
A = Muslim and B = church, return X = mosque and Y = Christian. (The pair Muslim:mosque
has a high relational similarity to the pair Christian:church.)
Marx et al. (2002) developed an unsupervised algorithm for discovering analogies
by clustering words from two different corpora. Each cluster of words in one corpus
is coupled one-to-one with a cluster in the other corpus. For example, one experiment
used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of
words such as {Hindu, Mahayana, Zen, ...} from the Buddhist corpus was coupled with
a cluster of words such as {Catholic, Protestant, ...} from the Christian corpus. Thus the
algorithm appears to have discovered an analogical mapping between Buddhist schools
and traditions and Christian schools and traditions. This is interesting work, but it is not
directly applicable to SAT analogies, because it discovers analogies between clusters of
words rather than individual words.
</bodyText>
<subsectionHeader confidence="0.801963">
3.10 Identifying Semantic Roles
</subsectionHeader>
<bodyText confidence="0.999973666666667">
A semantic frame for an event such as judgement contains semantic roles such as judge,
evaluee, and reason, whereas an event such as statement contains roles such as speaker,
addressee, and message (Gildea and Jurafsky 2002). The task of identifying semantic roles
is to label the parts of a sentence according to their semantic roles. We believe that it
may be helpful to view semantic frames and their semantic roles as sets of semantic
relations; thus, a measure of relational similarity should help us to identify semantic
roles. Moldovan et al. (2004) argue that semantic roles are merely a special case of
semantic relations (Section 3.4), since semantic roles always involve verbs or predicates,
but semantic relations can involve words of any part of speech.
</bodyText>
<sectionHeader confidence="0.905274" genericHeader="method">
4. The Vector Space Model
</sectionHeader>
<bodyText confidence="0.9996185">
This section examines past work on measuring attributional and relational similarity
using the VSM.
</bodyText>
<subsectionHeader confidence="0.99932">
4.1 Measuring Attributional Similarity with the Vector Space Model
</subsectionHeader>
<bodyText confidence="0.986540647058824">
The VSM was first developed for information retrieval (Salton and McGill 1983; Salton
and Buckley 1988; Salton 1989) and it is at the core of most modern search engines
(Baeza-Yates and Ribeiro-Neto 1999). In the VSM approach to information retrieval,
queries and documents are represented by vectors. Elements in these vectors are based
on the frequencies of words in the corresponding queries and documents. The frequen-
cies are usually transformed by various formulas and weights, tailored to improve the
effectiveness of the search engine (Salton 1989). The attributional similarity between a
query and a document is measured by the cosine of the angle between their correspond-
ing vectors. For a given query, the search engine sorts the matching documents in order
of decreasing cosine.
The VSM approach has also been used to measure the attributional similarity of
words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered
words according to their attributional similarity, as measured by a VSM. Their algo-
rithm is able to discover the different senses of polysemous words, using unsupervised
learning.
Latent Semantic Analysis enhances the VSM approach to information retrieval by
using the Singular Value Decomposition (SVD) to smooth the vectors, which helps
</bodyText>
<page confidence="0.984502">
390
</page>
<note confidence="0.753289">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.9999194">
to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993;
Landauer and Dumais 1997). SVD improves both document-query attributional sim-
ilarity measures (Deerwester et al. 1990; Dumais 1993) and word–word attributional
similarity measures (Landauer and Dumais 1997). LRA also uses SVD to smooth vec-
tors, as we discuss in Section 5.
</bodyText>
<subsectionHeader confidence="0.999507">
4.2 Measuring Relational Similarity with the Vector Space Model
</subsectionHeader>
<bodyText confidence="0.99995675">
Let R1 be the semantic relation (or set of relations) between a pair of words, A and B,
and let R2 be the semantic relation (or set of relations) between another pair, C and D.
We wish to measure the relational similarity between R1 and R2. The relations R1 and R2
are not given to us; our task is to infer these hidden (latent) relations and then compare
them.
In the VSM approach to relational similarity (Turney and Littman 2005), we create
vectors, r1 and r2, that represent features of R1 and R2, and then measure the similarity
of R1 and R2 by the cosine of the angle 0 between r1 and r2:
</bodyText>
<equation confidence="0.998981571428572">
r1 = (r1,1, ... , r1,n) (5)
r2 = (r2,1,...r2,n) (6)
n
r1,i · r2,i
i=1
=
� n
i=1 i=1
n
(r1,i)2 · (r2,i)2
cosine(0) =
(7)
11r111
11r211
r1 · r2
=
·
r1
√r1·
·√r2· r2
r1 · r2
</equation>
<bodyText confidence="0.999912235294118">
We create a vector, r, to characterize the relationship between two words, X and Y,
by counting the frequencies of various short phrases containing X and Y. Turney and
Littman (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases
that contain X and Y, such as X of Y, Y of X, X for Y, Y for X, X to Y, and Y to X. These
phrases are then used as queries for a search engine and the number of hits (matching
documents) is recorded for each query. This process yields a vector of 128 numbers.
If the number of hits for a query is x, then the corresponding element in the vector r
is log(x + 1). Several authors report that the logarithmic transformation of frequencies
improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin
1998b).
Turney and Littman (2005) evaluated the VSM approach by its performance on 374
SAT analogy questions, achieving a score of 47%. Since there are five choices for each
question, the expected score for random guessing is 20%. To answer a multiple-choice
analogy question, vectors are created for the stem pair and each choice pair, and then
cosines are calculated for the angles between the stem pair and each choice pair. The
best guess is the choice pair with the highest cosine. We use the same set of analogy
questions to evaluate LRA in Secti on 6.
</bodyText>
<page confidence="0.951257">
391
</page>
<bodyText confidence="0.9840646875">
Computational Linguistics Volume 32, Number 3
The VSM was also evaluated by its performance as a distance (nearness) measure
in a supervised nearest neighbor classifier for noun-modifier semantic relations (Turney
and Littman 2005). The evaluation used 600 hand-labeled noun-modifier pairs from
Nastase and Szpakowicz (2003). A testing pair is classified by searching for its single
nearest neighbor in the labeled training data. The best guess is the label for the training
pair with the highest cosine. LRA is evaluated with the same set of noun-modifier pairs
in Section 7.
Turney and Littman (2005) used the AltaVista search engine to obtain the frequency
information required to build vectors for the VSM. Thus their corpus was the set of all
Web pages indexed by AltaVista. At the time, the English subset of this corpus consisted
of about 5 × 1011 words. Around April 2004, AltaVista made substantial changes to
their search engine, removing their advanced search operators. Their search engine no
longer supports the asterisk operator, which was used by Turney and Littman (2005)
for stemming and wild-card searching. AltaVista also changed their policy toward
automated searching, which is now forbidden.3
Turney and Littman (2005) used AltaVista’s hit count, which is the number of
documents (Web pages) matching a given query, but LRA uses the number of passages
(strings) matching a query. In our experiments with LRA (Sections 6 and 7), we use a lo-
cal copy of the Waterloo MultiText System (WMTS) (Clarke, Cormack, and Palmer 1998;
Terra and Clarke 2003), running on a 16 CPU Beowulf Cluster, with a corpus of about
5 × 1010 English words. The WMTS is a distributed (multiprocessor) search engine,
designed primarily for passage retrieval (although document retrieval is possible, as a
special case of passage retrieval). The text and index require approximately one terabyte
of disk space. Although AltaVista only gives a rough estimate of the number of match-
ing documents, the WMTS gives exact counts of the number of matching passages.
Turney et al. (2003) combine 13 independent modules to answer SAT questions. The
performance of LRA significantly surpasses this combined system, but there is no real
contest between these approaches, because we can simply add LRA to the combination,
as a fourteenth module. Since the VSM module had the best performance of the 13
modules (Turney et al. 2003), the following experiments focus on comparing VSM and
LRA.
</bodyText>
<sectionHeader confidence="0.859202" genericHeader="method">
5. Latent Relational Analysis
</sectionHeader>
<bodyText confidence="0.999937">
LRA takes as input a set of word pairs and produces as output a measure of the
relational similarity between any two of the input pairs. LRA relies on three resources, a
search engine with a very large corpus of text, a broad-coverage thesaurus of synonyms,
and an efficient implementation of SVD.
We first present a short description of the core algorithm. Later, in the following
subsections, we will give a detailed description of the algorithm, as it is applied in the
experiments in Sections 6 and 7.
</bodyText>
<listItem confidence="0.980668666666667">
• Given a set of word pairs as input, look in a thesaurus for synonyms for
each word in each word pair. For each input pair, make alternate pairs by
replacing the original words with their synonyms. The alternate pairs are
</listItem>
<footnote confidence="0.998945666666667">
3 See http://www.altavista.com/robots.txt for AltaVista’s current policy toward “robots” (software for
automatically gathering Web pages or issuing batches of queries). The protocol of the “robots.txt” file is
explained in http://www.robotstxt.org/wc/robots.html.
</footnote>
<page confidence="0.990468">
392
</page>
<note confidence="0.764787">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.9504505">
intended to form near analogies with the corresponding original pairs (see
Section 2.3).
</bodyText>
<listItem confidence="0.9262768125">
• Filter out alternate pairs that do not form near analogies by dropping
alternate pairs that co-occur rarely in the corpus. In the preceding step, if a
synonym replaced an ambiguous original word, but the synonym captures
the wrong sense of the original word, it is likely that there is no significant
relation between the words in the alternate pair, so they will rarely
co-occur.
• For each original and alternate pair, search in the corpus for short phrases
that begin with one member of the pair and end with the other. These
phrases characterize the relation between the words in each pair.
• For each phrase from the previous step, create several patterns, by
replacing words in the phrase with wild cards.
• Build a pair–pattern frequency matrix, in which each cell represents the
number of times that the corresponding pair (row) appears in the corpus
with the corresponding pattern (column). The number will usually be
zero, resulting in a sparse matrix.
• Apply the SVD to the matrix. This reduces noise in the matrix and helps
with sparse data.
• Suppose that we wish to calculate the relational similarity between any
two of the original pairs. Start by looking for the two row vectors in the
pair–pattern frequency matrix that correspond to the two original pairs.
Calculate the cosine of the angle between these two row vectors. Then
merge the cosine of the two original pairs with the cosines of their
corresponding alternate pairs, as follows. If an analogy formed with
alternate pairs has a higher cosine than the original pairs, we assume that
we have found a better way to express the analogy, but we have not
significantly changed its meaning. If the cosine is lower, we assume that
we may have changed the meaning by inappropriately replacing words
with synonyms. Filter out inappropriate alternates by dropping all
analogies formed of alternates, such that the cosines are less than the
cosine for the original pairs. The relational similarity between the two
original pairs is then calculated as the average of all of the remaining
cosines.
</listItem>
<bodyText confidence="0.9999752">
The motivation for the alternate pairs is to handle cases where the original pairs co-
occur rarely in the corpus. The hope is that we can find near analogies for the original
pairs, such that the near analogies co-occur more frequently in the corpus. The danger
is that the alternates may have different relations from the originals. The filtering steps
above aim to reduce this risk.
</bodyText>
<subsectionHeader confidence="0.957876">
5.1 Input and Output
</subsectionHeader>
<bodyText confidence="0.999984333333333">
In our experiments, the input set contains from 600 to 2,244 word pairs. The output
similarity measure is based on cosines, so the degree of similarity can range from −1
(dissimilar; 0 = 180°) to +1 (similar; 0 = 0°). Before applying SVD, the vectors are
</bodyText>
<page confidence="0.989115">
393
</page>
<note confidence="0.290524">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999716">
completely non-negative, which implies that the cosine can only range from 0 to +1, but
SVD introduces negative values, so it is possible for the cosine to be negative, although
we have never observed this in our experiments.
</bodyText>
<subsectionHeader confidence="0.999973">
5.2 Search Engine and Corpus
</subsectionHeader>
<bodyText confidence="0.999988142857143">
In the following experiments, we use a local copy of the WMTS (Clarke, Cormack, and
Palmer 1998; Terra and Clarke 2003).4 The corpus consists of about 5 x 1010 English
words, gathered by a Web crawler, mainly from US academic Web sites. The Web pages
cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is
well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our
case), it gives exact frequency counts (unlike most Web search engines), it is designed for
passage retrieval (rather than document retrieval), and it has a powerful query syntax.
</bodyText>
<subsectionHeader confidence="0.980995">
5.3 Thesaurus
</subsectionHeader>
<bodyText confidence="0.999873727272727">
As a source of synonyms, we use Lin’s (1998a) automatically generated thesaurus. This
thesaurus is available through an on-line interactive demonstration or it can be down-
loaded.5 We used the on-line demonstration, since the downloadable version seems to
contain fewer words. For each word in the input set of word pairs, we automatically
query the on-line demonstration and fetch the resulting list of synonyms. As a cour-
tesy to other users of Lin’s on-line system, we insert a 20-second delay between each
two queries.
Lin’s thesaurus was generated by parsing a corpus of about 5 x 107 English words,
consisting of text from the Wall Street Journal, San Jose Mercury, and AP Newswire (Lin
1998a). The parser was used to extract pairs of words and their grammatical relations.
Words were then clustered into synonym sets, based on the similarity of their grammat-
ical relations. Two words were judged to be highly similar when they tended to have
the same kinds of grammatical relations with the same sets of words. Given a word and
its part of speech, Lin’s thesaurus provides a list of words, sorted in order of decreasing
attributional similarity. This sorting is convenient for LRA, since it makes it possible
to focus on words with higher attributional similarity and ignore the rest. WordNet, in
contrast, given a word and its part of speech, provides a list of words grouped by the
possible senses of the given word, with groups sorted by the frequencies of the senses.
WordNet’s sorting does not directly correspond to sorting by degree of attributional
similarity, although various algorithms have been proposed for deriving attributional
similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst
2001; Banerjee and Pedersen 2003).
</bodyText>
<subsectionHeader confidence="0.978103">
5.4 Singular Value Decomposition
</subsectionHeader>
<bodyText confidence="0.9986975">
We use Rohde’s SVDLIBC implementation of the SVD, which is based on SVDPACKC
(Berry 1992).6 In LRA, SVD is used to reduce noise and compensate for sparseness.
</bodyText>
<footnote confidence="0.9990966">
4 See http://multitext.uwaterloo.ca/.
5 The online demonstration is at http://www.cs.ualberta.ca/∼lindek/demos/depsim.htm and the
downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz.
6 SVDLIBC is available at http://tedlab.mit.edu/∼dr/SVDLIBC/ and SVDPACKC is available at
http://www.netlib.org/svdpack/.
</footnote>
<page confidence="0.995336">
394
</page>
<note confidence="0.847012">
Turney Similarity of Semantic Relations
</note>
<subsectionHeader confidence="0.973305">
5.5 The Algorithm
</subsectionHeader>
<bodyText confidence="0.999502166666667">
We will go through each step of LRA, using an example to illustrate the steps. Assume
that the input to LRA is the 374 multiple-choice SAT word analogy questions of Turney
and Littman (2005). Since there are six word pairs per question (the stem and five
choices), the input consists of 2,244 word pairs. Let’s suppose that we wish to calculate
the relational similarity between the pair quart:volume and the pair mile:distance, taken
from the SAT question in Table 6. The LRA algorithm consists of the following 12 steps:
</bodyText>
<listItem confidence="0.987456571428571">
1. Find alternates: For each word pair A:B in the input set, look in Lin’s
(1998a) thesaurus for the top num sim words (in the following experiments,
num sim is 10) that are most similar to A. For each A&apos; that is similar to A,
make a new word pair A&apos;:B. Likewise, look for the top num sim words that
are most similar to B, and for each B&apos;, make a new word pair A:B&apos;. A:B is
called the original pair and each A&apos;:B or A:B&apos; is an alternate pair. The intent
is that alternates should have almost the same semantic relations as the
original. For each input pair, there will now be 2 × num sim alternate pairs.
When looking for similar words in Lin’s (1998a) thesaurus, avoid words
that seem unusual (e.g., hyphenated words, words with three characters or
less, words with non-alphabetical characters, multiword phrases, and
capitalized words). The first column in Table 7 shows the alternate pairs
that are generated for the original pair quart:volume.
2. Filter alternates: For each original pair A:B, filter the 2 × num sim
</listItem>
<bodyText confidence="0.994812">
alternates as follows. For each alternate pair, send a query to the WMTS, to
find the frequency of phrases that begin with one member of the pair and
end with the other. The phrases cannot have more than max phrase words
(we use max phrase = 5). Sort the alternate pairs by the frequency of their
phrases. Select the top num filter most frequent alternates and discard the
remainder (we use num filter = 3, so 17 alternates are dropped). This step
tends to eliminate alternates that have no clear semantic relation. The third
column in Table 7 shows the frequency with which each pair co-occurs in a
window of max phrase words. The last column in Table 7 shows the pairs
that are selected.
</bodyText>
<listItem confidence="0.9980335">
3. Find phrases: For each pair (originals and alternates), make a list of
phrases in the corpus that contain the pair. Query the WMTS for all
phrases that begin with one member of the pair and end with the other
(in either order). We ignore suffixes when searching for phrases that match
</listItem>
<tableCaption confidence="0.991127">
Table 6
</tableCaption>
<table confidence="0.747291666666667">
This SAT question, from Claman (2000), is used to illustrate the steps in the LRA algorithm.
Stem: quart:volume
Choices: (a) day:night
</table>
<listItem confidence="0.808214">
(b) mile:distance
(c) decade:century
(d) friction:heat
(e) part:whole
Solution: (b) mile:distance
</listItem>
<page confidence="0.899933">
395
</page>
<table confidence="0.590685">
Computational Linguistics Volume 32, Number 3
</table>
<tableCaption confidence="0.991845">
Table 7
</tableCaption>
<bodyText confidence="0.931637">
Alternate forms of the original pair quart:volume. The first column shows the original pair and
the alternate pairs. The second column shows Lin’s similarity score for the alternate word
compared to the original word. For example, the similarity between quart and pint is 0.210. The
third column shows the frequency of the pair in the WMTS corpus. The fourth column shows the
pairs that pass the filtering step (i.e., step 2).
</bodyText>
<table confidence="0.999638545454545">
Word pair Similarity Frequency Filtering step
quart:volume NA 632 Accept (original pair)
pint:volume 0.210 372
gallon:volume 0.159 1500 Accept (top alternate)
liter:volume 0.122 3323 Accept (top alternate)
squirt:volume 0.084 54
pail:volume 0.084 28
vial:volume 0.084 373
pumping:volume 0.073 1386 Accept (top alternate)
ounce:volume 0.071 430
spoonful:volume 0.070 42
tablespoon:volume 0.069 96
quart:turnover 0.229 0
quart:output 0.225 34
quart:export 0.206 7
quart:value 0.203 266
quart:import 0.186 16
quart:revenue 0.185 0
quart:sale 0.169 119
quart:investment 0.161 11
quart:earnings 0.156 0
quart:profit 0.156 24
</table>
<bodyText confidence="0.951609272727273">
a given pair. The phrases cannot have more than max phrase words and
there must be at least one word between the two members of the word
pair. These phrases give us information about the semantic relations
between the words in each pair. A phrase with no words between the two
members of the word pair would give us very little information about the
semantic relations (other than that the words occur together with a certain
frequency in a certain order). Table 8 gives some examples of phrases in
the corpus that match the pair quart:volume.
4. Find patterns: For each phrase found in the previous step, build patterns
from the intervening words. A pattern is constructed by replacing any or
all or none of the intervening words with wild cards (one wild card can
</bodyText>
<tableCaption confidence="0.897604">
Table 8
</tableCaption>
<bodyText confidence="0.9002895">
Some examples of phrases that contain quart:volume. Suffixes are ignored when searching for
matching phrases in the WMTS corpus. At least one word must occur between quart and
volume. At most max phrase words can appear in a phrase.
quarts liquid volume volume in quarts
quarts of volume volume capacity quarts
quarts in volume volume being about two quarts
quart total volume volume of milk in quarts
quart of spray volume volume include measures like quart
</bodyText>
<page confidence="0.995235">
396
</page>
<note confidence="0.914096">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.983709333333333">
replace only one word). If a phrase is n words long, there are n − 2
intervening words between the members of the given word pair (e.g.,
between quart and volume). Thus a phrase with n words generates 2(n−2)
patterns. (We use max phrase = 5, so a phrase generates at most eight
patterns.) For each pattern, count the number of pairs (originals and
alternates) with phrases that match the pattern (a wild card must match
exactly one word). Keep the top num patterns most frequent patterns and
discard the rest (we use num patterns = 4, 000). Typically there will be
millions of patterns, so it is not feasible to keep them all.
</bodyText>
<listItem confidence="0.867999551724138">
5. Map pairs to rows: In preparation for building the matrix X, create a
mapping of word pairs to row numbers. For each pair A:B, create a row for
A:B and another row for B:A. This will make the matrix more symmetrical,
reflecting our knowledge that the relational similarity between A:B and
C:D should be the same as the relational similarity between B:A and D:C.
This duplication of rows is examined in Section 6.6.
6. Map patterns to columns: Create a mapping of the top num patterns
patterns to column numbers. For each pattern P, create a column for
“word1 P word2” and another column for “word2 P word1.” Thus there will
be 2 × num patterns columns in X. This duplication of columns is
examined in Section 6.6.
7. Generate a sparse matrix: Generate a matrix X in sparse matrix format,
suitable for input to SVDLIBC. The value for the cell in row i and column j
is the frequency of the jth pattern (see step 6) in phrases that contain the
ith word pair (see step 5). Table 9 gives some examples of pattern
frequencies for quart:volume.
8. Calculate entropy: Apply log and entropy transformations to the sparse
matrix (Landauer and Dumais 1997). These transformations have been
found to be very helpful for information retrieval (Harman 1986; Dumais
1990). Let xi,j be the cell in row i and column j of the matrix X from step 7.
Let m be the number of rows in X and let n be the number of columns. We
wish to weight the cell xi,j by the entropy of the jth column. To calculate
the entropy of the column, we need to convert the column into a vector of
probabilities. Let pi,j be the probability of xi,j, calculated by normalizing the
column vector so that the sum of the elements is one, pi j = xi j/Emk= 1 xk ,j
The entropy of the jth column is then Hj = − Emk= 1 pk j log(pk j ). Entropy is
at its maximum when pi,j is a uniform distribution, pi,j = 1/m, in which
case Hj = log(m). Entropy is at its minimum when pi,j is 1 for some value
of i and 0 for all other values of i, in which case Hj = 0. We want to give
</listItem>
<tableCaption confidence="0.987927">
Table 9
</tableCaption>
<table confidence="0.681175875">
Frequencies of various patterns for quart:volume. The asterisk “*” represents the wildcard.
Suffixes are ignored, so quart matches quarts. For example, quarts in volume is one
of the four phrases that match quart P volume when P is in.
P = “in” P = “* of” P = “of *” P = “* *”
freq(“quart P volume”) 4 1 5 19
freq(“volume P quart”) 10 0 2 16
397
Computational Linguistics Volume 32, Number 3
</table>
<bodyText confidence="0.999773833333333">
more weight to columns (patterns) with frequencies that vary substantially
from one row (word pair) to the next, and less weight to columns that
are uniform. Therefore we weight the cell xi,j by wj = 1 − Hj/ log(m),
which varies from 0 when pi,j is uniform to 1 when entropy is minimal.
We also apply the log transformation to frequencies, log(xi,j + 1).
(Entropy is calculated with the original frequency values, before the
log transformation is applied.) For all i and all j, replace the original
value xi,j in X by the new value wj log(xi,j + 1). This is an instance of the
Term Frequency-Inverse Document Frequency (TF-IDF) family of
transformations, which is familiar in information retrieval (Salton and
Buckley 1988; Baeza-Yates and Ribeiro-Neto 1999): log(xi,j + 1) is the TF
term and wj is the IDF term.
</bodyText>
<listItem confidence="0.617960333333333">
9. Apply SVD: After the log and entropy transformations have been applied
to the matrix X, run SVDLIBC. SVD decomposes a matrix X into a product
of three matrices UEVT, where U and V are in column orthonormal form
(i.e., the columns are orthogonal and have unit length: UTU = VTV = I)
and E is a diagonal matrix of singular values (hence SVD) (Golub and Van
Loan 1996). If X is of rank r, then E is also of rank r. Let Ek, where k &lt; r,
be the diagonal matrix formed from the top k singular values, and let Uk
and Vk be the matrices produced by selecting the corresponding columns
from U and V. The matrix UkEkVkT is the matrix of rank k that best
</listItem>
<bodyText confidence="0.922839916666667">
approximates the original matrix X, in the sense that it minimizes the
� �
approximation errors. That is, Xˆ = UkEkVT k minimizes � Xˆ − X �F over all
matrices Xˆ of rank k, where 11. . .�F denotes the Frobenius norm (Golub and
Van Loan 1996). We may think of this matrix UkEkVTk as a “smoothed” or
“compressed” version of the original matrix. In the subsequent steps, we
will be calculating cosines for row vectors. For this purpose, we can
simplify calculations by dropping V. The cosine of two vectors is
their dot product, after they have been normalized to unit length. The
matrix XXT contains the dot products of all of the row vectors. We
can find the dot product of the ith and jth row vectors by looking at
the cell in row i, column j of the matrix XXT. Since VTV = I, we have
XXT = UEVT(UEVT)T = UEVTVETUT = UE(UE)T, which means that
we can calculate cosines with the smaller matrix UE, instead of using
X = UEVT (Deerwester et al. 1990).
10. Projection: Calculate UkEk (we use k = 300). This matrix has the same
number of rows as X, but only k columns (instead of 2 x num patterns
columns; in our experiments, that is 300 columns instead of 8,000). We can
compare two word pairs by calculating the cosine of the corresponding
row vectors in UkEk. The row vector for each word pair has been projected
from the original 8,000 dimensional space into a new 300 dimensional
space. The value k = 300 is recommended by Landauer and Dumais (1997)
for measuring the attributional similarity between words. We investigate
other values in Section 6.4.
</bodyText>
<listItem confidence="0.968858">
11. Evaluate alternates: Let A:B and C:D be any two word pairs in the input
set. From step 2, we have (num filter + 1) versions of A:B, the original and
num filter alternates. Likewise, we have (num filter + 1) versions of C:D.
</listItem>
<page confidence="0.995751">
398
</page>
<note confidence="0.913951">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.998906346153846">
Therefore we have (num filter + 1)2 ways to compare a version of A:B with
a version of C:D. Look for the row vectors in UkEk that correspond to the
versions of A:B and the versions of C:D and calculate the (num filter + 1)2
cosines (in our experiments, there are 16 cosines). For example, suppose
A:B is quart:volume and C:D is mile:distance. Table 10 gives the cosines for
the sixteen combinations.
12. Calculate relational similarity: The relational similarity between A:B and
C:D is the average of the cosines, among the (num filter + 1)2 cosines from
step 11, that are greater than or equal to the cosine of the original pairs,
A:B and C:D. The requirement that the cosine must be greater than or
equal to the original cosine is a way of filtering out poor analogies, which
may be introduced in step 1 and may have slipped through the filtering in
step 2. Averaging the cosines, as opposed to taking their maximum, is
intended to provide some resistance to noise. For quart:volume and
mile:distance, the third column in Table 10 shows which alternates are
used to calculate the average. For these two pairs, the average of the
selected cosines is 0.677. In Table 7, we see that pumping:volume has
slipped through the filtering in step 2, although it is not a good alternate
for quart:volume. However, Table 10 shows that all four analogies that
involve pumping:volume are dropped here, in step 12.
Steps 11 and 12 can be repeated for each two input pairs that are to be compared. This
completes the description of LRA.
Table 11 gives the cosines for the sample SAT question. The choice pair with the
highest average cosine (the choice with the largest value in column 1), choice (b), is
the solution for this question; LRA answers the question correctly. For comparison,
column 2 gives the cosines for the original pairs and column 3 gives the highest cosine.
</bodyText>
<tableCaption confidence="0.990898">
Table 10
</tableCaption>
<table confidence="0.916809933333333">
The 16 combinations and their cosines. A:B::C:D expresses the analogy A is to B as C is to D. The
third column indicates those combinations for which the cosine is greater than or equal
to the cosine of the original analogy, quart:volume::mile:distance.
Word pairs Cosine Cosine ≥ original pairs
quart:volume::mile:distance 0.525 Yes (original pairs)
quart:volume::feet:distance 0.464
quart:volume::mile:length 0.634 Yes
quart:volume::length:distance 0.499
liter:volume::mile:distance 0.736 Yes
liter:volume::feet:distance 0.687 Yes
liter:volume::mile:length 0.745 Yes
liter:volume::length:distance 0.576 Yes
gallon:volume::mile:distance 0.763 Yes
gallon:volume::feet:distance 0.710 Yes
gallon:volume::mile:length 0.781 Yes (highest cosine)
</table>
<footnote confidence="0.742979">
gallon:volume::length:distance 0.615 Yes
pumping:volume::mile:distance 0.412
pumping:volume::feet:distance 0.439
pumping:volume::mile:length 0.446
pumping:volume::length:distance 0.491
</footnote>
<page confidence="0.960445">
399
</page>
<note confidence="0.476667">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.991078">
For this particular SAT question, there is one choice that has the highest cosine for all
three columns, choice (b), although this is not true in general. Note that the gap between
the first choice (b) and the second choice (d) is largest for the average cosines (column
1). This suggests that the average of the cosines (column 1) is better at discriminating the
correct choice than either the original cosine (column 2) or the highest cosine (column 3).
</bodyText>
<sectionHeader confidence="0.981472" genericHeader="method">
6. Experiments with Word Analogy Questions
</sectionHeader>
<bodyText confidence="0.9981775">
This section presents various experiments with 374 multiple-choice SAT word analogy
questions.
</bodyText>
<subsectionHeader confidence="0.989355">
6.1 Baseline LRA System
</subsectionHeader>
<bodyText confidence="0.999904764705883">
Table 12 shows the performance of the baseline LRA system on the 374 SAT questions,
using the parameter settings and configuration described in Section 5. LRA correctly
answered 210 of the 374 questions; 160 questions were answered incorrectly and 4
questions were skipped, because the stem pair and its alternates were represented by
zero vectors. The performance of LRA is significantly better than the lexicon-based
approach of Veale (2004) (see Section 3.1) and the best performance using attributional
similarity (see Section 2.3), with 95% confidence, according to the Fisher Exact Test
(Agresti 1990).
As another point of reference, consider the simple strategy of always guessing the
choice with the highest co-occurrence frequency. The idea here is that the words in
the solution pair may occur together frequently, because there is presumably a clear
and meaningful relation between the solution words, whereas the distractors may only
occur together rarely because they have no meaningful relation. This strategy is signif-
cantly worse than random guessing. The opposite strategy, always guessing the choice
pair with the lowest co-occurrence frequency, is also worse than random guessing (but
not significantly). It appears that the designers of the SAT questions deliberately chose
distractors that would thwart these two strategies.
</bodyText>
<tableCaption confidence="0.996622428571429">
Table 11
Cosines for the sample SAT question given in Table 6. Column 1 gives the averages of the cosines
that are greater than or equal to the original cosines (e.g., the average of the cosines that are
marked Yes in Table 10 is 0.677; see choice (b) in column 1). Column 2 gives the cosine for the
original pairs (e.g., the cosine for the first pair in Table 10 is 0.525; see choice (b) in column 2).
Column 3 gives the maximum cosine for the 16 possible analogies (e.g., the maximum cosine in
Table 10 is 0.781; see choice (b) in column 3).
</tableCaption>
<table confidence="0.9999261">
Stem: quart:volume Average Original Highest
cosines cosines cosines
1 2 3
Choices: day:night 0.374 0.327 0.443
mile:distance 0.677 0.525 0.781
decade:century 0.389 0.327 0.470
friction:heat 0.428 0.336 0.552
part:whole 0.370 0.330 0.408
Solution: (b) mile:distance 0.677 0.525 0.781
Gap: (b)−(d) 0.249 0.189 0.229
</table>
<page confidence="0.716563">
400
</page>
<note confidence="0.689052">
Turney Similarity of Semantic Relations
</note>
<tableCaption confidence="0.995916">
Table 12
</tableCaption>
<table confidence="0.98255">
Performance of LRA on the 374 SAT questions. Precision, recall, and F are reported as
percentages. (The bottom five rows are included for comparison.)
Algorithm Precision Recall F
LRA 56.8 56.1 56.5
Veale (2004) 42.8 42.8 42.8
Best attributional similarity 35.0 35.0 35.0
Random guessing 20.0 20.0 20.0
Lowest co-occurrence frequency 16.8 16.8 16.8
Highest co-occurrence frequency 11.8 11.8 11.8
</table>
<bodyText confidence="0.9998844">
With 374 questions and six word pairs per question (one stem and five choices),
there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies
the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A:B, we add
B:A, yielding 17,952 pairs. However, some pairs are dropped because they correspond
to zero vectors (they do not appear together in a window of five words in the WMTS
corpus). Also, a few words do not appear in Lin’s thesaurus, and some word pairs
appear twice in the SAT questions (e.g., lion:cat). The sparse matrix (step 7) has 17,232
rows (word pairs) and 8,000 columns (patterns), with a density of 5.8% (percentage of
nonzero values).
Table 13 gives the time required for each step of LRA, a total of almost 9 days. All of
the steps used a single CPU on a desktop computer, except step 3, finding the phrases
for each word pair, which used a 16 CPU Beowulf cluster. Most of the other steps are
parallelizable; with a bit of programming effort, they could also be executed on the
Beowulf cluster. All CPUs (both desktop and cluster) were 2.4 GHz Intel Xeons. The
desktop computer had 2 GB of RAM and the cluster had a total of 16 GB of RAM.
</bodyText>
<subsectionHeader confidence="0.940277">
6.2 LRA versus VSM
</subsectionHeader>
<tableCaption confidence="0.885966">
Table 14 compares LRA to the VSM with the 374 analogy questions. VSM-AV refers
to the VSM using AltaVista’s database as a corpus. The VSM-AV results are taken
Table 13
</tableCaption>
<table confidence="0.957135533333334">
LRA elapsed run time.
Step Description Time H:M:S Hardware
1 Find alternates 24:56:00 1 CPU
2 Filter alternates 0:00:02 1 CPU
3 Find phrases 109:52:00 16 CPUs
4 Find patterns 33:41:00 1 CPU
5 Map pairs to rows 0:00:02 1 CPU
6 Map patterns to columns 0:00:02 1 CPU
7 Generate a sparse matrix 38:07:00 1 CPU
8 Calculate entropy 0:11:00 1 CPU
9 Apply SVD 0:43:28 1 CPU
10 Projection 0:08:00 1 CPU
11 Evaluate alternates 2:11:00 1 CPU
12 Calculate relational similarity 0:00:02 1 CPU
Total 209:49:36
</table>
<page confidence="0.925376">
401
</page>
<note confidence="0.463943">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999376956521739">
from Turney and Littman (2005). As mentioned in Section 4.2, we estimate this corpus
contained about 5 × 1011 English words at the time the VSM-AV experiments took place.
VSM-WMTS refers to the VSM using the WMTS, which contains about 5 × 1010 English
words. We generated the VSM-WMTS results by adapting the VSM to the WMTS.
The algorithm is slightly different from Turney and Littman’s (2005), because we used
passage frequencies instead of document frequencies.
All three pairwise differences in recall in Table 14 are statistically significant with
95% confidence, using the Fisher Exact Test (Agresti 1990). The pairwise differences in
precision between LRA and the two VSM variations are also significant, but the differ-
ence in precision between the two VSM variations (42.4% vs. 47.7%) is not significant.
Although VSM-AV has a corpus 10 times larger than LRA’s, LRA still performs better
than VSM-AV.
Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of
the VSM, but much of the drop is due to the larger number of questions that were
skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more
of the input word pairs simply do not appear together in short phrases in the corpus.
LRA is able to answer as many questions as VSM-AV, although it uses the same corpus
as VSM-WMTS, because Lin’s thesaurus allows LRA to substitute synonyms for words
that are not in the corpus.
VSM-AV required 17 days to process the 374 analogy questions (Turney and Littman
2005), compared to 9 days for LRA. As a courtesy to AltaVista, Turney and Littman
(2005) inserted a 5-second delay between each two queries. Since the WMTS is running
locally, there is no need for delays. VSM-WMTS processed the questions in only one day.
</bodyText>
<subsectionHeader confidence="0.999263">
6.3 Human Performance
</subsectionHeader>
<bodyText confidence="0.999995214285714">
The average performance of college-bound senior high school students on verbal SAT
questions corresponds to a recall (percent correct) of about 57% (Turney and Littman
2005). The SAT I test consists of 78 verbal questions and 60 math questions (there is
also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are
only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374
analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then
we can estimate that the average college-bound senior would correctly answer about
57% of the 374 analogy questions.
Of our 374 SAT questions, 190 are from a collection of ten official SAT tests (Claman
2000). On this subset of the questions, LRA has a recall of 61.1%, compared to a recall
of 51.1% on the other 184 questions. The 184 questions that are not from Claman (2000)
seem to be more difficult. This indicates that we may be underestimating how well
LRA performs, relative to college-bound senior high school students. Claman (2000)
suggests that the analogy questions may be somewhat harder than other verbal SAT
</bodyText>
<tableCaption confidence="0.996114">
Table 14
</tableCaption>
<table confidence="0.919435">
LRA versus VSM with 374 SAT analogy questions.
Algorithm Correct Incorrect Skipped Precision Recall F
VSM-AV 176 193 5 47.7 47.1 47.4
VSM-WMTS 144 196 34 42.4 38.5 40.3
LRA 210 160 4 56.8 56.1 56.5
</table>
<page confidence="0.996703">
402
</page>
<note confidence="0.902176">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.997529333333333">
questions, so we may be slightly overestimating the mean human score on the analogy
questions.
Table 15 gives the 95% confidence intervals for LRA, VSM-AV, and VSM-WMTS,
calculated by the Binomial Exact Test (Agresti 1990). There is no significant difference
between LRA and human performance, but VSM-AV and VSM-WMTS are significantly
below human-level performance.
</bodyText>
<subsectionHeader confidence="0.999281">
6.4 Varying the Parameters in LRA
</subsectionHeader>
<bodyText confidence="0.999985529411765">
There are several parameters in the LRA algorithm (see Section 5.5). The parameter
values were determined by trying a small number of possible values on a small set of
questions that were set aside. Since LRA is intended to be an unsupervised learning
algorithm, we did not attempt to tune the parameter values to maximize the precision
and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensitive
to the values of the parameters.
Table 16 shows the variation in the performance of LRA as the parameter values
are adjusted. We take the baseline parameter settings (given in Section 5.5) and vary
each parameter, one at a time, while holding the remaining parameters fixed at their
baseline values. None of the precision and recall values are significantly different from
the baseline, according to the Fisher Exact Test (Agresti 1990), at the 95% confidence
level. This supports the hypothesis that the algorithm is not sensitive to the parameter
values.
Although a full run of LRA on the 374 SAT questions takes 9 days, for some of
the parameters it is possible to reuse cached data from previous runs. We limited the
experiments with num sim and max phrase because caching was not as helpful for these
parameters, so experimenting with them required several weeks.
</bodyText>
<subsectionHeader confidence="0.993783">
6.5 Ablation Experiments
</subsectionHeader>
<bodyText confidence="0.9997845">
As mentioned in the introduction, LRA extends the VSM approach of Turney and
Littman (2005) by (1) exploring variations on the analogies by replacing words with
synonyms (step 1), (2) automatically generating connecting patterns (step 4), and (3)
smoothing the data with SVD (step 9). In this subsection, we ablate each of these three
components to assess their contribution to the performance of LRA. Table 17 shows the
results.
</bodyText>
<tableCaption confidence="0.4649044">
Table 15
Comparison with human SAT performance. The last column in the table indicates whether (YES)
or not (NO) the average human performance (57%) falls within the 95% confidence interval of
the corresponding algorithm’s performance. The confidence intervals are calculated using the
Binomial Exact Test (Agresti 1990).
</tableCaption>
<table confidence="0.99945">
System Recall 95% confidence Human-level
(% correct) interval for recall (57%)
VSM-AV 47.1 42.2–52.5 NO
VSM-WMTS 38.5 33.5–43.6 NO
LRA 56.1 51.0–61.2 YES
</table>
<page confidence="0.717667">
403
</page>
<table confidence="0.580895">
Computational Linguistics Volume 32, Number 3
</table>
<tableCaption confidence="0.990498">
Table 16
</tableCaption>
<table confidence="0.962802777777778">
Variation in performance with different parameter values. The Baseline column marks the
baseline parameter values. The Step column gives the step number in Section 5.5 where each
parameter is discussed.
Parameter Baseline Value Step Precision Recall F
num sim 5 1 54.2 53.5 53.8
num sim 10 1 56.8 56.1 56.5
num sim 15 1 54.1 53.5 53.8
max phrase 4 2 55.8 55.1 55.5
max phrase 5 2 56.8 56.1 56.5
max phrase 6 2 56.2 55.6 55.9
num filter 1 2 54.3 53.7 54.0
num filter 2 2 55.7 55.1 55.4
num filter 3 2 56.8 56.1 56.5
num filter 4 2 55.7 55.1 55.4
num filter 5 2 54.3 53.7 54.0
num patterns 1000 4 55.9 55.3 55.6
num patterns 2000 4 57.6 57.0 57.3
num patterns 3000 4 58.4 57.8 58.1
num patterns 4000 4 56.8 56.1 56.5
num patterns 5000 4 57.0 56.4 56.7
num patterns 6000 4 57.0 56.4 56.7
num patterns 7000 4 58.1 57.5 57.8
k 100 10 55.7 55.1 55.4
k 300 10 56.8 56.1 56.5
k 500 10 57.6 57.0 57.3
k 700 10 56.5 55.9 56.2
k 900 10 56.2 55.6 55.9
</table>
<bodyText confidence="0.99399525">
Without SVD (compare column 1 to 2 in Table 17), performance drops, but the
drop is not statistically significant with 95% confidence, according to the Fisher Exact
Test (Agresti 1990). However, we hypothesize that the drop in performance would be
significant with a larger set of word pairs. More word pairs would increase the sample
size, which would decrease the 95% confidence interval, which would likely show that
SVD is making a significant contribution. Furthermore, more word pairs would increase
the matrix size, which would give SVD more leverage. For example, Landauer and
Dumais (1997) apply SVD to a matrix of 30,473 columns by 60,768 rows, but our matrix
</bodyText>
<tableCaption confidence="0.996716">
Table 17
</tableCaption>
<table confidence="0.968063">
Results of ablation experiments.
LRA LRA LRA LRA VSM-WMTS
Baseline No SVD No synonyms No SVD, 5
system 2 3 no synonyms
1 4
Correct 210 198 185 178 144
Incorrect 160 172 167 173 196
Skipped 4 4 22 23 34
Precision 56.8 53.5 52.6 50.7 42.4
Recall 56.1 52.9 49.5 47.6 38.5
F 56.5 53.2 51.0 49.1 40.3
</table>
<page confidence="0.997792">
404
</page>
<note confidence="0.902747">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.999861086956522">
here is 8,000 columns by 17,232 rows. We are currently gathering more SAT questions
to test this hypothesis.
Without synonyms (compare column 1 to 3 in Table 17), recall drops significantly
(from 56.1% to 49.5%), but the drop in precision is not significant. When the synonym
component is dropped, the number of skipped questions rises from 4 to 22, which
demonstrates the value of the synonym component of LRA for compensating for sparse
data.
When both SVD and synonyms are dropped (compare column 1 to 4 in Table 17),
the decrease in recall is significant, but the decrease in precision is not significant.
Again, we believe that a larger sample size would show that the drop in precision is
significant.
If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from
VSM-WMTS is the patterns (step 4). The VSM approach uses a fixed list of 64 patterns
to generate 128 dimensional vectors (Turney and Littman 2005), whereas LRA uses a
dynamically generated set of 4,000 patterns, resulting in 8,000 dimensional vectors. We
can see the value of the automatically generated patterns by comparing LRA without
synonyms and SVD (column 4) to VSM-WMTS (column 5). The difference in both
precision and recall is statistically significant with 95% confidence, according to the
Fisher Exact Test (Agresti 1990).
The ablation experiments support the value of the patterns (step 4) and synonyms
(step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although
we believe more data will support its effectiveness. Nonetheless, the three components
together result in a 16% increase in F (compare column 1 to 5).
</bodyText>
<subsectionHeader confidence="0.996631">
6.6 Matrix Symmetry
</subsectionHeader>
<bodyText confidence="0.999926666666667">
We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as
carpenter is to wood implies stone is to mason as wood is to carpenter. Therefore, a good
measure of relational similarity, simr, should obey the following equation:
</bodyText>
<equation confidence="0.988575">
simr(A:B,C:D) = simr(B:A,D:C) (8)
</equation>
<bodyText confidence="0.999922352941176">
In steps 5 and 6 of the LRA algorithm (Section 5.5), we ensure that the matrix X is
symmetrical, so that equation (8) is necessarily true for LRA. The matrix is designed so
that the row vector for A:B is different from the row vector for B:A only by a permutation
of the elements. The same permutation distinguishes the row vectors for C:D and D:C.
Therefore the cosine of the angle between A:B and C:D must be identical to the cosine
of the angle between B:A and D:C (see equation (7)).
To discover the consequences of this design decision, we altered steps 5 and 6 so
that symmetry is no longer preserved. In step 5, for each word pair A:B that appears in
the input set, we only have one row. There is no row for B:A unless B:A also appears in
the input set. Thus the number of rows in the matrix dropped from 17,232 to 8,616.
In step 6, we no longer have two columns for each pattern P, one for “word1 P word2”
and another for “word2 P word1.” However, to be fair, we kept the total number of
columns at 8,000. In step 4, we selected the top 8,000 patterns (instead of the top 4,000),
distinguishing the pattern “word1 P word2” from the pattern “word2 P word1” (instead of
considering them equivalent). Thus a pattern P with a high frequency is likely to appear
in two columns, in both possible orders, but a lower frequency pattern might appear in
only one column, in only one possible order.
</bodyText>
<page confidence="0.996484">
405
</page>
<note confidence="0.498701">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.993769333333333">
These changes resulted in a slight decrease in performance. Recall dropped from
56.1% to 55.3% and precision dropped from 56.8% to 55.9%. The decrease is not sta-
tistically significant. However, the modified algorithm no longer obeys equation (8).
Although dropping symmetry appears to cause no significant harm to the performance
of the algorithm on the SAT questions, we prefer to retain symmetry, to ensure that
equation (8) is satisfied.
Note that, if A:B::C:D, it does not follow that B:A::C:D. For example, it is false that
“stone is to mason as carpenter is to wood.” In general (except when the semantic
relations between A and B are symmetrical), we have the following inequality:
</bodyText>
<equation confidence="0.973253">
simr(A:B,C:D) # simr(B:A,C:D) (9)
</equation>
<bodyText confidence="0.9980095">
Therefore we do not want A:B and B:A to be represented by identical row vectors,
although it would ensure that equation (8) is satisfied.
</bodyText>
<subsectionHeader confidence="0.989283">
6.7 All Alternates versus Better Alternates
</subsectionHeader>
<bodyText confidence="0.999505428571429">
In step 12 of LRA, the relational similarity between A:B and C:D is the average of the
cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal
to the cosine of the original pairs, A:B and C:D. That is, the average includes only those
alternates that are “better” than the originals. Taking all alternates instead of the better
alternates, recall drops from 56.1% to 40.4% and precision drops from 56.8% to 40.8%.
Both decreases are statistically significant with 95% confidence, according to the Fisher
Exact Test (Agresti 1990).
</bodyText>
<subsectionHeader confidence="0.986234">
6.8 Interpreting Vectors
</subsectionHeader>
<bodyText confidence="0.999868380952381">
Suppose a word pair A:B corresponds to a vector r in the matrix X. It would be con-
venient if inspection of r gave us a simple explanation or description of the relation
between A and B. For example, suppose the word pair ostrich:bird maps to the row
vector r. It would be pleasing to look in r and find that the largest element corresponds
to the pattern “is the largest” (i.e., “ostrich is the largest bird”). Unfortunately, inspection
of r reveals no such convenient patterns.
We hypothesize that the semantic content of a vector is distributed over the whole
vector; it is not concentrated in a few elements. To test this hypothesis, we modified
step 10 of LRA. Instead of projecting the 8,000 dimensional vectors into the 300 dimen-
sional space UkEk, we use the matrix UkEkVTk . This matrix yields the same cosines as
UkEk, but preserves the original 8,000 dimensions, making it easier to interpret the row
vectors. For each row vector in UkEkVTk , we select the N largest values and set all other
values to zero. The idea here is that we will only pay attention to the N most important
patterns in r; the remaining patterns will be ignored. This reduces the length of the
row vectors, but the cosine is the dot product of normalized vectors (all vectors are
normalized to unit length; see equation (7)), so the change to the vector lengths has no
impact; only the angle of the vectors is important. If most of the semantic content is in
the N largest elements of r, then setting the remaining elements to zero should have
relatively little impact.
Table 18 shows the performance as N varies from 1 to 3,000. The precision and recall
are significantly below the baseline LRA until N ≥ 300 (95% confidence, Fisher Exact
</bodyText>
<page confidence="0.998073">
406
</page>
<note confidence="0.916129">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.9813925">
Test). In other words, for a typical SAT analogy question, we need to examine the top
300 patterns to explain why LRA selected one choice instead of another.
We are currently working on an extension of LRA that will explain with a single
pattern why one choice is better than another. We have had some promising results, but
this work is not yet mature. However, we can confidently claim that interpreting the
vectors is not trivial.
</bodyText>
<subsectionHeader confidence="0.998095">
6.9 Manual Patterns versus Automatic Patterns
</subsectionHeader>
<bodyText confidence="0.999814653846154">
Turney and Littman (2005) used 64 manually generated patterns, whereas LRA uses
4,000 automatically generated patterns. We know from Section 6.5 that the automatically
generated patterns are significantly better than the manually generated patterns. It may
be interesting to see how many of the manually generated patterns appear within the
automatically generated patterns. If we require an exact match, 50 of the 64 manual
patterns can be found in the automatic patterns. If we are lenient about wildcards, and
count the pattern not the as matching * not the (for example), then 60 of the 64 manual
patterns appear within the automatic patterns. This suggests that the improvement in
performance with the automatic patterns is due to the increased quantity of patterns,
rather than a qualitative difference in the patterns.
Turney and Littman (2005) point out that some of their 64 patterns have been used
by other researchers. For example, Hearst (1992) used the pattern such as to discover hy-
ponyms and Berland and Charniak (1999) used the pattern of the to discover meronyms.
Both of these patterns are included in the 4,000 patterns automatically generated by
LRA.
The novelty in Turney and Littman (2005) is that their patterns are not used to
mine text for instances of word pairs that fit the patterns (Hearst 1992; Berland and
Charniak 1999); instead, they are used to gather frequency data for building vectors
that represent the relation between a given pair of words. The results in Section 6.8
show that a vector contains more information than any single pattern or small set of
patterns; a vector is a distributed representation. LRA is distinct from Hearst (1992) and
Berland and Charniak (1999) in its focus on distributed representations, which it shares
with Turney and Littman (2005), but LRA goes beyond Turney and Littman (2005) by
finding patterns automatically.
Riloff and Jones (1999) and Yangarber (2003) also find patterns automatically, but
their goal is to mine text for instances of word pairs; the same goal as Hearst (1992) and
</bodyText>
<tableCaption confidence="0.979414">
Table 18
</tableCaption>
<table confidence="0.9856391">
Performance as a function of N.
N Correct Incorrect Skipped Precision Recall F
1 114 179 81 38.9 30.5 34.2
3 146 206 22 41.5 39.0 40.2
10 167 201 6 45.4 44.7 45.0
30 174 196 4 47.0 46.5 46.8
100 178 192 4 48.1 47.6 47.8
300 192 178 4 51.9 51.3 51.6
1000 198 172 4 53.5 52.9 53.2
3000 207 163 4 55.9 55.3 55.6
</table>
<page confidence="0.890676">
407
</page>
<note confidence="0.537806">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999511333333333">
Berland and Charniak (1999). Because LRA uses patterns to build distributed vector
representations, it can exploit patterns that would be much too noisy and unreliable for
the kind of text mining instance extraction that is the objective of Hearst (1992), Berland
and Charniak (1999), Riloff and Jones (1999), and Yangarber (2003). Therefore LRA can
simply select the highest frequency patterns (step 4 in Section 5.5); it does not need the
more sophisticated selection algorithms of Riloff and Jones (1999) and Yangarber (2003).
</bodyText>
<sectionHeader confidence="0.613916" genericHeader="method">
7. Experiments with Noun-Modifier Relations
</sectionHeader>
<bodyText confidence="0.999762333333333">
This section describes experiments with 600 noun-modifier pairs, hand-labeled with
30 classes of semantic relations (Nastase and Szpakowicz 2003). In the following ex-
periments, LRA is used with the baseline parameter values, exactly as described in
Section 5.5. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is
used as a distance (nearness) measure in a single nearest neighbor supervised learning
algorithm.
</bodyText>
<subsectionHeader confidence="0.998864">
7.1 Classes of Relations
</subsectionHeader>
<bodyText confidence="0.999375421052631">
The following experiments use the 600 labeled noun-modifier pairs of Nastase and
Szpakowicz (2003). This data set includes information about the part of speech and
WordNet synset (synonym set; i.e., word sense tag) of each word, but our algorithm
does not use this information.
Table 19 lists the 30 classes of semantic relations. The table is based on Appendix A
of Nastase and Szpakowicz (2003), with some simplifications. The original table listed
several semantic relations for which there were no instances in the data set. These were
relations that are typically expressed with longer phrases (three or more words), rather
than noun-modifier word pairs. For clarity, we decided not to include these relations in
Table 19.
In this table, H represents the head noun and M represents the modifier. For exam-
ple, in flu virus, the head noun (H) is virus and the modifier (M) is flu (*). In English,
the modifier (typically a noun or adjective) usually precedes the head noun. In the
description of purpose, V represents an arbitrary verb. In concert hall, the hall is for
presenting concerts (V is present) or holding concerts (V is hold) (†).
Nastase and Szpakowicz (2003) organized the relations into groups. The five capi-
talized terms in the Relation column of Table 19 are the names of five groups of semantic
relations. (The original table had a sixth group, but there are no examples of this group
in the data set.) We make use of this grouping in the following experiments.
</bodyText>
<subsectionHeader confidence="0.96705">
7.2 Baseline LRA with Single Nearest Neighbor
</subsectionHeader>
<bodyText confidence="0.998284625">
The following experiments use single nearest neighbor classification with leave-one-out
cross-validation. For leave-one-out cross-validation, the testing set consists of a single
noun-modifier pair and the training set consists of the 599 remaining noun-modifiers.
The data set is split 600 times, so that each noun-modifier gets a turn as the testing word
pair. The predicted class of the testing pair is the class of the single nearest neighbor
in the training set. As the measure of nearness, we use LRA to calculate the relational
similarity between the testing pair and the training pairs. The single nearest neighbor
algorithm is a supervised learning algorithm (i.e., it requires a training set of labeled
</bodyText>
<page confidence="0.998171">
408
</page>
<note confidence="0.965769">
Turney Similarity of Semantic Relations
</note>
<tableCaption confidence="0.997359">
Table 19
</tableCaption>
<table confidence="0.955485434782609">
Classes of semantic relations, from Nastase and Szpakowicz (2003).
Relation Abbr. Example phrase Description
CAUSALITY
cause cs flu virus (*) H makes M occur or exist, H is
necessary and sufficient
effect eff exam anxiety M makes H occur or exist, M is
necessary and sufficient
purpose prp concert hall (†) H is for V-ing M, M does not
necessarily occur or exist
detraction detr headache pill H opposes M, H is not sufficient
to prevent M
TEMPORALITY
frequency freq daily exercise H occurs every time M occurs
time at tat morning exercise H occurs when M occurs
time through tthr six-hour meeting H existed while M existed, M is
an interval of time
SPATIAL
direction dir outgoing mail H is directed towards M, M is
not the final point
location loc home town H is the location of M
location at lat desert storm H is located at M
location from lfr foreign capital H originates at M
PARTICIPANT
</table>
<tableCaption confidence="0.8134718">
agent ag student protest M performs H, M is animate or
natural phenomenon
beneficiary ben student discount M benefits from H
instrument inst laser printer H uses M
object obj metal separator M is acted upon by H
</tableCaption>
<bodyText confidence="0.903020875">
object property obj prop sunken ship H underwent M
part part printer tray H is part of M
possessor posr national debt M has H
property prop blue book His M
product prod plum tree H produces M
source src olive oil M is the source of H
stative st sleeping dog H is in a state of M
whole whl daisy chain M is part of H
</bodyText>
<sectionHeader confidence="0.588171" genericHeader="method">
QUALITY
</sectionHeader>
<bodyText confidence="0.988972428571429">
container cntr film music M contains H
content cont apple cake M is contained in H
equative eq player coach H is also M
material mat brick house H is made of M
measure meas expensive book M is a measure of H
topic top weather report H is concerned with M
type type oak tree M is a type of H
</bodyText>
<page confidence="0.996423">
409
</page>
<note confidence="0.601291">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.999954833333333">
data), but we are using LRA to measure the distance between a pair and its potential
neighbors, and LRA is itself determined in an unsupervised fashion (i.e., LRA does not
need labeled data).
Each SAT question has five choices, so answering 374 SAT questions required cal-
culating 374 x 5 x 16 = 29,920 cosines. The factor of 16 comes from the alternate pairs,
step 11 in LRA. With the noun-modifier pairs, using leave-one-out cross-validation,
each test pair has 599 choices, so an exhaustive application of LRA would require
calculating 600 x 599 x 16 = 5,750,400 cosines. To reduce the amount of computation
required, we first find the 30 nearest neighbors for each pair, ignoring the alternate pairs
(600 x 599 = 359,400 cosines), and then apply the full LRA, including the alternates, to
just those 30 neighbors (600 x 30 x 16 = 288, 000 cosines), which requires calculating
only 359,400 + 288, 000 = 647,400 cosines.
There are 600 word pairs in the input set for LRA. In step 2, introducing alternate
pairs multiplies the number of pairs by four, resulting in 2,400 pairs. In step 5, for each
pair A:B, we add B:A, yielding 4,800 pairs. However, some pairs are dropped because
they correspond to zero vectors and a few words do not appear in Lin’s thesaurus. The
sparse matrix (step 7) has 4,748 rows and 8,000 columns, with a density of 8.4%.
Following Turney and Littman (2005), we evaluate the performance by accuracy
and also by the macroaveraged F measure (Lewis 1991). Macroaveraging calculates
the precision, recall, and F for each class separately, and then calculates the average
across all classes. Microaveraging combines the true positive, false positive, and false
negative counts for all of the classes, and then calculates precision, recall, and F from the
combined counts. Macroaveraging gives equal weight to all classes, but microaveraging
gives more weight to larger classes. We use macroaveraging (giving equal weight to all
classes), because we have no reason to believe that the class sizes in the data set reflect
the actual distribution of the classes in a real corpus.
Classification with 30 distinct classes is a hard problem. To make the task easier, we
can collapse the 30 classes to 5 classes, using the grouping that is given in Table 19. For
example, agent and beneficiary both collapse to participant. On the 30 class problem, LRA
with the single nearest neighbor algorithm achieves an accuracy of 39.8% (239/600)
and a macroaveraged F of 36.6%. Always guessing the majority class would result in
an accuracy of 8.2% (49/600). On the 5 class problem, the accuracy is 58.0% (348/600)
and the macroaveraged F is 54.6%. Always guessing the majority class would give an
accuracy of 43.3% (260/600). For both the 30 class and 5 class problems, LRA’s accuracy
is significantly higher than guessing the majority class, with 95% confidence, according
to the Fisher Exact Test (Agresti 1990).
</bodyText>
<subsectionHeader confidence="0.993448">
7.3 LRA versus VSM
</subsectionHeader>
<bodyText confidence="0.9988367">
Table 20 shows the performance of LRA and VSM on the 30 class problem. VSM-AV
is VSM with the AltaVista corpus and VSM-WMTS is VSM with the WMTS corpus.
The results for VSM-AV are taken from Turney and Littman (2005). All three pairwise
differences in the three F measures are statistically significant at the 95% level, according
to the Paired t-Test (Feelders and Verkooijen 1995). The accuracy of LRA is signifi-
cantly higher than the accuracies of VSM-AV and VSM-WMTS, according to the Fisher
Exact Test (Agresti 1990), but the difference between the two VSM accuracies is not
significant.
Table 21 compares the performance of LRA and VSM on the 5 class problem.
The accuracy and F measure of LRA are significantly higher than the accuracies and
</bodyText>
<page confidence="0.99812">
410
</page>
<note confidence="0.965582">
Turney Similarity of Semantic Relations
</note>
<tableCaption confidence="0.993672">
Table 20
</tableCaption>
<table confidence="0.992766555555555">
Comparison of LRA and VSM on the 30 class problem.
VSM-AV VSM-WMTS LRA
Correct 167 148 239
Incorrect 433 452 361
Total 600 600 600
Accuracy 27.8 24.7 39.8
Precision 27.9 24.0 41.0
Recall 26.8 20.9 35.9
F 26.5 20.3 36.6
</table>
<tableCaption confidence="0.986372">
Table 21
</tableCaption>
<table confidence="0.989359333333333">
Comparison of LRA and VSM on the 5 class problem.
VSM-AV VSM-WMTS LRA
Correct 274 264 348
Incorrect 326 336 252
Total 600 600 600
Accuracy 45.7 44.0 58.0
Precision 43.4 40.2 55.9
Recall 43.1 41.4 53.6
F 43.2 40.6 54.6
</table>
<tableCaption confidence="0.6213815">
F measures of VSM-AV and VSM-WMTS, but the differences between the two VSM
accuracies and F measures are not significant.
</tableCaption>
<sectionHeader confidence="0.82271" genericHeader="evaluation">
8. Discussion
</sectionHeader>
<bodyText confidence="0.9997726">
The experimental results in Sections 6 and 7 demonstrate that LRA performs signifi-
cantly better than the VSM, but it is also clear that there is room for improvement. The
accuracy might not yet be adequate for practical applications, although past work has
shown that it is possible to adjust the trade-off of precision versus recall (Turney and
Littman 2005). For some of the applications, such as information extraction, LRA might
be suitable if it is adjusted for high precision, at the expense of low recall.
Another limitation is speed; it took almost 9 days for LRA to answer 374 analogy
questions. However, with progress in computer hardware, speed will gradually become
less of a concern. Also, the software has not been optimized for speed; there are several
places where the efficiency could be increased and many operations are parallelizable.
It may also be possible to precompute much of the information for LRA, although this
would require substantial changes to the algorithm.
The difference in performance between VSM-AV and VSM-WMTS shows that VSM
is sensitive to the size of the corpus. Although LRA is able to surpass VSM-AV when the
WMTS corpus is only about one tenth the size of the AV corpus, it seems likely that LRA
would perform better with a larger corpus. The WMTS corpus requires one terabyte of
hard disk space, but progress in hardware will likely make 10 or even 100 terabytes
affordable in the relatively near future.
For noun-modifier classification, more labeled data should yield performance im-
provements. With 600 noun-modifier pairs and 30 classes, the average class has only
</bodyText>
<page confidence="0.994287">
411
</page>
<note confidence="0.582982">
Computational Linguistics Volume 32, Number 3
</note>
<bodyText confidence="0.995818483870968">
20 examples. We expect that the accuracy would improve substantially with 5 or
10 times more examples. Unfortunately, it is time consuming and expensive to acquire
hand-labeled data.
Another issue with noun-modifier classification is the choice of classification
scheme for the semantic relations. The 30 classes of Nastase and Szpakowicz (2003)
might not be the best scheme. Other researchers have proposed different schemes
(Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario,
Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine
learning than others. For some applications, 30 classes may not be necessary; the 5 class
scheme may be sufficient.
LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past
work suggests that a hybrid approach, combining multiple modules, some corpus-
based, some lexicon-based, will surpass any purebred approach (Turney et al. 2003).
In future work, it would be natural to combine the corpus-based approach of LRA with
the lexicon-based approach of Veale (2004), perhaps using the combination method of
Turney et al. (2003).
SVD is only one of many methods for handling sparse, noisy data. We have also
experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999),
Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Com-
ponents Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling
(IS) (Ando 2000). We had some interesting results with small matrices (around 2,000
rows by 1,000 columns), but none of these methods seemed substantially better than
SVD and none of them scaled up to the matrix sizes we are using here (e.g., 17,232 rows
and 8,000 columns; see Section 6.1).
In step 4 of LRA, we simply select the top num patterns most frequent patterns
and discard the remaining patterns. Perhaps a more sophisticated selection algorithm
would improve the performance of LRA. We have tried a variety of ways of selecting
patterns, but it seems that the method of selection has little impact on performance. We
hypothesize that the distributed vector representation is not sensitive to the selection
method, but it is possible that future work will find a method that yields significant
improvement in performance.
</bodyText>
<sectionHeader confidence="0.994904" genericHeader="conclusions">
9. Conclusion
</sectionHeader>
<bodyText confidence="0.999963">
This article has introduced a new method for calculating relational similarity, Latent
Relational Analysis. The experiments demonstrate that LRA performs better than the
VSM approach, when evaluated with SAT word analogy questions and with the task
of classifying noun-modifier expressions. The VSM approach represents the relation be-
tween a pair of words with a vector, in which the elements are based on the frequencies
of 64 hand-built patterns in a large corpus. LRA extends this approach in three ways:
(1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth
the data, and (3) a thesaurus is used to explore variations of the word pairs. With the
WMTS corpus (about 5 × 1010 English words), LRA achieves an F of 56.5%, whereas the
F of VSM is 40.3%.
We have presented several examples of the many potential applications for mea-
sures of relational similarity. Just as attributional similarity measures have proven
to have many practical uses, we expect that relational similarity measures will soon
become widely used. Gentner et al. (2001) argue that relational similarity is essential
to understanding novel metaphors (as opposed to conventional metaphors). Many
</bodyText>
<page confidence="0.995978">
412
</page>
<note confidence="0.925607">
Turney Similarity of Semantic Relations
</note>
<bodyText confidence="0.998917111111111">
researchers have argued that metaphor is the heart of human thinking (Lakoff and
Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner
et al. 2001; French 2002). We believe that relational similarity plays a fundamental role
in the mind and therefore relational similarity measures could be crucial for artificial
intelligence.
In future work, we plan to investigate some potential applications for LRA. It
is possible that the error rate of LRA is still too high for practical applications, but
the fact that LRA matches average human performance on SAT analogy questions is
encouraging.
</bodyText>
<sectionHeader confidence="0.998062" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.71160165">
Thanks to Michael Littman for sharing the
374 SAT analogy questions and for inspiring
me to tackle them. Thanks to Vivi Nastase
and Stan Szpakowicz for sharing their 600
classified noun-modifier phrases. Thanks to
Egidio Terra, Charlie Clarke, and the School
of Computer Science of the University of
Waterloo, for giving us a copy of the
Waterloo MultiText System and their
Terabyte Corpus. Thanks to Dekang Lin for
making his Dependency-Based Word
Similarity lexicon available online. Thanks to
Doug Rohde for SVDLIBC and Michael
Berry for SVDPACK. Thanks to Ted Pedersen
for making his WordNet::Similarity package
available. Thanks to Joel Martin for
comments on the article. Thanks to the
anonymous reviewers of Computational
Linguistics for their very helpful comments
and suggestions.
</bodyText>
<sectionHeader confidence="0.997801" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99936269117647">
Agresti, Alan. 1990. Categorical Data Analysis.
Wiley, Hoboken, NJ.
Ando, Rie Kubota. 2000. Latent semantic
space: Iterative scaling improves precision
of inter-document similarity measurement.
In Proceedings of the 23rd Annual ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR-2000),
pages 216–223, Athens, Greece.
Baeza-Yates, Ricardo A. and Berthier A.
Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press, New York, NY.
Banerjee, Satanjeev and Ted Pedersen. 2003.
Extended gloss overlaps as a measure of
semantic relatedness. In Proceedings of the
Eighteenth International Joint Conference on
Artificial Intelligence (IJCAI-03),
pages 805–810, Acapulco, Mexico.
Barker, Ken and Stan Szpakowicz. 1998.
Semi-automatic recognition of noun
modifier relationships. In Christian Boitet
and Pete Whitelock, editors, Proceedings of
the Thirty-Sixth Annual Meeting of the
Association for Computational Linguistics and
Seventeenth International Conference on
Computational Linguistics
(COLING-ACL’98), pages 96–102, San
Francisco, CA. Morgan Kaufmann
Publishers.
Berland, Matthew and Eugene Charniak.
1999. Finding parts in very large corpora.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics
(ACL ’99), pages 57–64, New Brunswick,
NJ.
Berry, Michael W. 1992. Large scale singular
value computations. International Journal of
Supercomputer Applications, 6(1):13–49.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in wordnet: An
experimental, application-oriented
evaluation of five measures. In Proceedings
of the Workshop on WordNet and Other
Lexical Resources, Second Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL-2001),
pages 29–34, Pittsburgh, PA.
Chiarello, Christine, Curt Burgess, Lorie
Richards, and Alma Pollock. 1990.
Semantic and associative priming in the
cerebral hemispheres: Some words do,
some words don’t ... sometimes, some
places. Brain and Language, 38:75–104.
Claman, Cathy. 2000. 10 Real SATs. College
Entrance Examination Board.
Clarke, Charles L. A., Gordon V. Cormack,
and Christopher R. Palmer. 1998. An
overview of multitext. ACM SIGIR Forum,
32(2):14–15.
Daganzo, Carlos F. 1994. The cell
transmission model: A dynamic
representation of highway traffic
consistent with the hydrodynamic theory.
Transportation Research Part B:
Methodological, 28(4):269–287.
Deerwester, Scott C., Susan T. Dumais,
Thomas K. Landauer, George W. Furnas,
and Richard A. Harshman.1990. Indexing
</reference>
<page confidence="0.996514">
413
</page>
<note confidence="0.519793">
Computational Linguistics Volume 32, Number 3
</note>
<reference confidence="0.997061144067797">
by latent semantic analysis. Journal of the
American Society for Information Science
(JASIS), 41(6):391–407.
Dolan, William B. 1995. Metaphor as an
emergent property of machine-readable
dictionaries. In Proceedings of the AAAI
1995 Spring Symposium Series: Representation
and Acquisition of Lexical Knowledge:
Polysemy, Ambiguity and Generativity,
pages 27–32, Menlo Park, CA.
Dumais, Susan T. 1990. Enhancing
performance in latent semantic indexing
(LSI) retrieval. Technical Report TM-ARH-
017527, Bellcore, Morristown, NJ.
Dumais, Susan T. 1993. Latent semantic
indexing (LSI) and TREC-2. In D. K.
Harman, editor, Proceedings of the
Second Text REtrieval Conference (TREC-2),
pages 105–115. National Institute
of Standards and Technology,
Gaithersburg, MD.
Falkenhainer, Brian. 1990. Analogical
interpretation in context. In Proceedings of
the Twelfth Annual Conference of the Cognitive
Science Society, pages 69–76. Lawrence
Erlbaum Associates, Mahwah, NJ.
Falkenhainer, Brian, Kenneth D. Forbus,
and Dedre Gentner. 1989. The
structure-mapping engine: Algorithm and
examples. Artificial Intelligence, 41(1):1–63.
Federici, Stefano, Simonetta Montemagni,
and Vito Pirrelli. 1997. Inferring semantic
similarity from distributional evidence: An
analogy-based approach to word sense
disambiguation. In Proceedings of the
ACL/EACL Workshop on Automatic
Information Extraction and Building of Lexical
Semantic Resources for NLP Applications,
pages 90–97, Madrid, Spain.
Feelders, Ad and William Verkooijen.1995.
Which method learns the most from
data? Methodological issues in the
analysis of comparative studies. In Fifth
International Workshop on Artificial
Intelligence and Statistics, pages 219–225,
Ft. Lauderdale, FL.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
French, Robert M. 2002. The computational
modeling of analogy-making. Trends in
Cognitive Sciences, 6(5):200–205.
Gentner, Dedre. 1983. Structure-mapping: A
theoretical framework for analogy.
Cognitive Science, 7(2):155–170.
Gentner, Dedre, Brian Bowdle, Phillip Wolff,
and Consuelo Boronat. 2001. Metaphor is
like analogy. In Dedre Gentner, Keith J.
Holyoak, and Boicho N. Kokinov, editors,
The Analogical Mind: Perspectives from
Cognitive Science. MIT Press, Cambridge,
MA, pages 199–253.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245–288.
Girju, Roxana, Adriana Badulescu, and
Dan I. Moldovan. 2003. Learning semantic
constraints for the automatic discovery of
part-whole relations. In Proceedings of the
Human Language Technology Conference of
the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL
2003), pages 80–87, Edmonton, Canada.
Goldenberg, David. 2005. The emperor’s
new clothes: Undressing the new and
unimproved SAT. Gelf Magazine, March.
http://www.gelf-magazine.com/mt/
archives/the emperors new clothes.html.
Golub, Gene H. and Charles F. Van Loan.
1996. Matrix Computations. 3rd ed.
Johns Hopkins University Press,
Baltimore, MD.
Harman, Donna. 1986. An experimental
study of factors important in document
ranking. In Proceedings of the Ninth Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval (SIGIR’86), pages 186–193, Pisa,
Italy.
Hearst, Marti A. 1992. Automatic acquisition
of hyponyms from large text corpora. In
Proceedings of the Fourteenth International
Conference on Computational Linguistics,
pages 539–545, Nantes, France.
Hirst, Graeme and David St-Onge.1998.
Lexical chains as representations of context
for the detection and correction of
malapropisms. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database, pages 305–332. MIT Press,
Cambridge, MA.
Hofmann, Thomas. 1999. Probabilistic latent
semantic indexing. In Proceedings of the
22nd Annual ACM Conference on Research
and Development in Information Retrieval
(SIGIR ’99), pages 50–57, Berkeley, CA,
August.
Hofstadter, Douglas and the Fluid Analogies
Research Group. 1995. Fluid Concepts and
Creative Analogies: Computer Models of the
Fundamental Mechanisms of Thought. Basic
Books, New York.
Jarmasz, Mario and Stan Szpakowicz.
2003. Roget’s thesaurus and semantic
similarity. In Proceedings of the International
Conference on Recent Advances in Natural
Language Processing (RANLP-03),
pages 212–219, Borovets, Bulgaria.
</reference>
<page confidence="0.990121">
414
</page>
<note confidence="0.808187">
Turney Similarity of Semantic Relations
</note>
<reference confidence="0.999494906779661">
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the International Conference on
Research in Computational Linguistics
(ROCLING X), pages 19–33, Tapei, Taiwan.
Kurtz, Stanley. 2002. Testing debate.
National Review Magazine, August.
http://www.nationalreview.com/
kurtz/kurtz082102.asp.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato’s problem: The
latent semantic analysis theory of the
acquisition, induction, and representation
of knowledge. Psychological Review,
104(2):211–240.
Lapata, Mirella and Frank Keller. 2004. The
web as a baseline: Evaluating the
performance of unsupervised web-based
models for a range of NLP tasks. In
Proceedings of the Human Language
Technology Conference of the North American
Chapter of the Association for Computational
Linguistics (HLT-NAACL 2004),
pages 121–128, Boston, MA.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on
Compound Nouns. Ph.D. thesis, Macquarie
University, Sydney.
Leacock, Claudia and Martin Chodorow.
1998. Combining local context and
WordNet similarity for word sense
identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database, pages 265–283. MIT Press,
Cambridge, MA.
Lee, Daniel D. and H. Sebastian Seung. 1999.
Learning the parts of objects by
nonnegative matrix factorization. Nature,
401:788–791.
Lesk, Michael E. 1969. Word-word
associations in document retrieval
systems. American Documentation,
20(1):27–38.
Lesk, Michael E. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of ACM
SIGDOC ’86, pages 24–26, New York, NY.
Lewis, David D. 1991. Evaluating text
categorization. In Proceedings of the
Speech and Natural Language Workshop,
pages 312–318, Asilomar, CA. Morgan
Kaufmann, San Francisco, CA.
Lin, Dekang. 1998a. Automatic retrieval and
clustering of similar words. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and the 17th
International Conference on Computational
Linguistics (COLING-ACL ’98),
pages 768–774, Montreal, Canada.
Lin, Dekang. 1998b. An information-theoretic
definition of similarity. In Proceedings of the
15th International Conference on Machine
Learning (ICML ’98), pages 296–304.
Morgan Kaufmann, San Francisco, CA.
Marx, Zvika, Ido Dagan, Joachim Buhmann,
and Eli Shamir. 2002. Coupled clustering:
A method for detecting structural
correspondence. Journal of Machine
Learning Research, 3:747–780.
Medin, Douglas L., Robert L. Goldstone, and
Dedre Gentner.1990. Similarity involving
attributes and relations: Judgments of
similarity and difference are not inverses.
Psychological Science, 1(1):64–69.
Moldovan, Dan, Adriana Badulescu, Marta
Tatu, Daniel Antohe, and Roxana Girju.
2004. Models for the semantic
classification of noun phrases. In
Proceedings of the Computational Lexical
Semantics Workshop at HLT-NAACL 2004,
pages 60–67, Boston, MA.
Morris, Jane and Graeme Hirst. 1991. Lexical
cohesion computed by thesaural relations
as an indicator of the structure of text.
Computational Linguistics, 17(1):21–48.
Nastase, Vivi and Stan Szpakowicz. 2003.
Exploring noun–modifier semantic
relations. In Fifth International Workshop on
Computational Semantics (IWCS-5),
pages 285–301, Tilburg, the Netherlands.
Pantel, Patrick and Dekang Lin. 2002.
Discovering word senses from text. In
Proceedings of ACM SIGKDD Conference on
Knowledge Discovery and Data Mining,
pages 613–619, New York, NY.
Rada, Roy, Hafedh Mili, Ellen Bicknell, and
Maria Blettner. 1989. Development and
application of a metric on semantic nets.
IEEE Transactions on Systems, Man, and
Cybernetics, 19(1):17–30.
Rehder, Bob, M. E. Schreiner, Michael B. W.
Wolfe, Darrell Laham, Thomas K.
Landauer, and Walter Kintsch. 1998. Using
latent semantic analysis to assess
knowledge: Some technical considerations.
Discourse Processes, 25:337–354.
Reitman, Walter R. 1965. Cognition and
Thought: An Information Processing
Approach. John Wiley and Sons,
New York, NY.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity
in a taxonomy. In Proceedings of the 14th
</reference>
<page confidence="0.972202">
415
</page>
<reference confidence="0.993409504273505">
Computational Linguistics Volume 32, Number 3
International Joint Conference on Artificial
Intelligence (IJCAI-95), pages 448–453, San
Mateo, CA. Morgan Kaufmann, San
Francisco, CA.
Riloff, Ellen and Rosie Jones. 1999. Learning
dictionaries for information extraction by
multi-level bootstrapping. In Proceedings of
the Sixteenth National Conference on Artificial
Intelligence (AAAI-99), pages 474–479,
Menlo Park, CA.
Rosario, Barbara and Marti Hearst. 2001.
Classifying the semantic relations in
noun-compounds via a domain-specific
lexical hierarchy. In Proceedings of the 2001
Conference on Empirical Methods in Natural
Language Processing (EMNLP-01),
pages 82–90, Pittsburgh, PA.
Rosario, Barbara, Marti Hearst, and Charles
Fillmore. 2002. The descent of hierarchy,
and selection in relational semantics.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL ’02), pages 417–424,
Philadelphia, PA.
Ruge, Gerda. 1992. Experiments on
linguistically-based term associations.
Information Processing and Management,
28(3):317–332.
Salton, Gerard. 1989. Automatic Text
Processing: The Transformation, Analysis, and
Retrieval of Information by Computer.
Addison-Wesley, Reading, MA.
Salton, Gerard and Chris Buckley. 1988.
Term-weighting approaches in automatic
text retrieval. Information Processing and
Management, 24(5):513–523.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York, NY.
Scholkopf, Bernhard, Alexander J. Smola,
and Klaus-Robert Muller. 1997. Kernel
principal component analysis. In
Proceedings of the International Conference on
Artificial Neural Networks (ICANN-1997),
pages 583–588, Berlin.
Terra, Egidio and Charles L. A. Clarke. 2003.
Frequency estimates for statistical word
similarity measures. In Proceedings of the
Human Language Technology and North
American Chapter of Association of
Computational Linguistics Conference 2003
(HLT/NAACL 2003), pages 244–251,
Edmonton, Canada.
Turney, Peter D. 2001. Mining the Web for
synonyms: PMI-IR versus LSA on TOEFL.
In Proceedings of the Twelfth European
Conference on Machine Learning,
pages 491–502, Springer, Berlin.
Turney, Peter D. 2002. Thumbs up or thumbs
down? Semantic orientation applied to
unsupervised classification of reviews.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL’02), pages 417–424,
Philadelphia, PA.
Turney, Peter D. 2005. Measuring semantic
similarity by latent relational analysis. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI-05), pages 1136–1141, Edinburgh,
Scotland.
Turney, Peter D. and Michael L. Littman.
2005. Corpus-based learning of analogies
and semantic relations. Machine Learning,
60(1–3):251–278.
Turney, Peter D., Michael L. Littman,
Jeffrey Bigham, and Victor Shnayder.
2003. Combining independent modules
to solve multiple-choice synonym and
analogy problems. In Proceedings of
the International Conference on Recent
Advances in Natural Language Processing
(RANLP-03), pages 482–489, Borovets,
Bulgaria.
Vanderwende, Lucy. 1994. Algorithm for
automatic interpretation of noun
sequences. In Proceedings of the Fifteenth
International Conference on Computational
Linguistics, pages 782–788, Kyoto,
Japan.
Veale, Tony. 2003. The analogical thesaurus.
In Proceedings of the 15th Innovative
Applications of Artificial Intelligence
Conference (IAAI2003), pages 137–142,
Acapulco, Mexico.
Veale, Tony. 2004. WordNet sits the SAT:
A knowledge-based approach to
lexical analogy. In Proceedings of the 16th
European Conference on Artificial Intelligence
(ECAI2004), pages 606–612, Valencia,
Spain.
Yangarber, Roman. 2003. Counter-training in
discovery of semantic patterns. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(ACL-2003), pages 343–350, Sapporo,
Japan.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 266–271, Princeton, NJ.
Zelenko, Dmitry, Chinatsu Aone,
and Anthony Richardella. 2003. Kernel
methods for relation extraction.
Journal of Machine Learning Research,
3:1083–1106.
</reference>
<page confidence="0.998585">
416
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.210397">
<title confidence="0.973842">Similarity of Semantic Relations</title>
<author confidence="0.223804">D</author>
<affiliation confidence="0.828859">National Research Council Canada</affiliation>
<abstract confidence="0.996466117647059">are at least two kinds of similarity. similarity correspondence between rein contrast with which is correspondence between attributes. two words have a high degree of attributional similarity, we call them When two pairs of words have a high degree of relational similarity, we say that their relations are For example, the word pair mason:stone is analogous to the pair carpenter:wood. This article introduces Latent Relational Analysis (LRA), a method for measuring relational similarity. LRA has potential applications in many areas, including information extraction, word sense disambiguation, and information retrieval. Recently the Vector Space Model (VSM) of information retrieval has been adapted to measuring relational similarity, achieving a score of 47% on a collection of 374 college-level multiple-choice word analogy questions. In the VSM approach, the relation between a pair of words is characterized by a vector offrequencies of predefined patterns in a large corpus. LRA extends the VSM approach in three ways: (1) The patterns are derived automatically from the corpus, (2) the Singular Value Decomposition (SVD) is used to smooth the frequency data, and (3) automatically generated synonyms are used to explore variations of the word pairs. LRA achieves 56% on the 374 analogy questions, statistically equivalent to the average human score of 57%. On the related problem of classifying semantic relations, LRA achieves similar gains over the VSM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan Agresti</author>
</authors>
<title>Categorical Data Analysis.</title>
<date>1990</date>
<publisher>Wiley,</publisher>
<location>Hoboken, NJ.</location>
<contexts>
<context position="17250" citStr="Agresti 1990" startWordPosition="2599" endWordPosition="2600">s were skipped. When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30). The first five algorithms in Table 4 are implemented in Pedersen’s WordNetSimilarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003). The difference between the lowest performance (Jiang and Conrath 1997) and random guessing is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). However, the difference between the highest performance (Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically significant with 95% confidence. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. 2 See http://www.d.umn.edu/∼tpederse/similarity.html. 384 Turney Similarity of Semantic Relations 3. Related Work This section is a brief survey of the many problems that involve </context>
<context position="65607" citStr="Agresti 1990" startWordPosition="10469" endWordPosition="10470">ystem Table 12 shows the performance of the baseline LRA system on the 374 SAT questions, using the parameter settings and configuration described in Section 5. LRA correctly answered 210 of the 374 questions; 160 questions were answered incorrectly and 4 questions were skipped, because the stem pair and its alternates were represented by zero vectors. The performance of LRA is significantly better than the lexicon-based approach of Veale (2004) (see Section 3.1) and the best performance using attributional similarity (see Section 2.3), with 95% confidence, according to the Fisher Exact Test (Agresti 1990). As another point of reference, consider the simple strategy of always guessing the choice with the highest co-occurrence frequency. The idea here is that the words in the solution pair may occur together frequently, because there is presumably a clear and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation. This strategy is signifcantly worse than random guessing. The opposite strategy, always guessing the choice pair with the lowest co-occurrence frequency, is also worse than random guessing (but not</context>
<context position="70205" citStr="Agresti 1990" startWordPosition="11243" endWordPosition="11244">3 from Turney and Littman (2005). As mentioned in Section 4.2, we estimate this corpus contained about 5 × 1011 English words at the time the VSM-AV experiments took place. VSM-WMTS refers to the VSM using the WMTS, which contains about 5 × 1010 English words. We generated the VSM-WMTS results by adapting the VSM to the WMTS. The algorithm is slightly different from Turney and Littman’s (2005), because we used passage frequencies instead of document frequencies. All three pairwise differences in recall in Table 14 are statistically significant with 95% confidence, using the Fisher Exact Test (Agresti 1990). The pairwise differences in precision between LRA and the two VSM variations are also significant, but the difference in precision between the two VSM variations (42.4% vs. 47.7%) is not significant. Although VSM-AV has a corpus 10 times larger than LRA’s, LRA still performs better than VSM-AV. Comparing VSM-AV to VSM-WMTS, the smaller corpus has reduced the score of the VSM, but much of the drop is due to the larger number of questions that were skipped (34 for VSM-WMTS versus 5 for VSM-AV). With the smaller corpus, many more of the input word pairs simply do not appear together in short ph</context>
<context position="72998" citStr="Agresti 1990" startWordPosition="11716" endWordPosition="11717">o college-bound senior high school students. Claman (2000) suggests that the analogy questions may be somewhat harder than other verbal SAT Table 14 LRA versus VSM with 374 SAT analogy questions. Algorithm Correct Incorrect Skipped Precision Recall F VSM-AV 176 193 5 47.7 47.1 47.4 VSM-WMTS 144 196 34 42.4 38.5 40.3 LRA 210 160 4 56.8 56.1 56.5 402 Turney Similarity of Semantic Relations questions, so we may be slightly overestimating the mean human score on the analogy questions. Table 15 gives the 95% confidence intervals for LRA, VSM-AV, and VSM-WMTS, calculated by the Binomial Exact Test (Agresti 1990). There is no significant difference between LRA and human performance, but VSM-AV and VSM-WMTS are significantly below human-level performance. 6.4 Varying the Parameters in LRA There are several parameters in the LRA algorithm (see Section 5.5). The parameter values were determined by trying a small number of possible values on a small set of questions that were set aside. Since LRA is intended to be an unsupervised learning algorithm, we did not attempt to tune the parameter values to maximize the precision and recall on the 374 SAT questions. We hypothesized that LRA is relatively insensit</context>
<context position="75246" citStr="Agresti 1990" startWordPosition="12076" endWordPosition="12077">by replacing words with synonyms (step 1), (2) automatically generating connecting patterns (step 4), and (3) smoothing the data with SVD (step 9). In this subsection, we ablate each of these three components to assess their contribution to the performance of LRA. Table 17 shows the results. Table 15 Comparison with human SAT performance. The last column in the table indicates whether (YES) or not (NO) the average human performance (57%) falls within the 95% confidence interval of the corresponding algorithm’s performance. The confidence intervals are calculated using the Binomial Exact Test (Agresti 1990). System Recall 95% confidence Human-level (% correct) interval for recall (57%) VSM-AV 47.1 42.2–52.5 NO VSM-WMTS 38.5 33.5–43.6 NO LRA 56.1 51.0–61.2 YES 403 Computational Linguistics Volume 32, Number 3 Table 16 Variation in performance with different parameter values. The Baseline column marks the baseline parameter values. The Step column gives the step number in Section 5.5 where each parameter is discussed. Parameter Baseline Value Step Precision Recall F num sim 5 1 54.2 53.5 53.8 num sim 10 1 56.8 56.1 56.5 num sim 15 1 54.1 53.5 53.8 max phrase 4 2 55.8 55.1 55.5 max phrase 5 2 56.8 </context>
<context position="76585" citStr="Agresti 1990" startWordPosition="12331" endWordPosition="12332">6.5 num filter 4 2 55.7 55.1 55.4 num filter 5 2 54.3 53.7 54.0 num patterns 1000 4 55.9 55.3 55.6 num patterns 2000 4 57.6 57.0 57.3 num patterns 3000 4 58.4 57.8 58.1 num patterns 4000 4 56.8 56.1 56.5 num patterns 5000 4 57.0 56.4 56.7 num patterns 6000 4 57.0 56.4 56.7 num patterns 7000 4 58.1 57.5 57.8 k 100 10 55.7 55.1 55.4 k 300 10 56.8 56.1 56.5 k 500 10 57.6 57.0 57.3 k 700 10 56.5 55.9 56.2 k 900 10 56.2 55.6 55.9 Without SVD (compare column 1 to 2 in Table 17), performance drops, but the drop is not statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). However, we hypothesize that the drop in performance would be significant with a larger set of word pairs. More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that SVD is making a significant contribution. Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage. For example, Landauer and Dumais (1997) apply SVD to a matrix of 30,473 columns by 60,768 rows, but our matrix Table 17 Results of ablation experiments. LRA LRA LRA LRA VSM-WMTS Baseline No SVD No synonyms No SVD, 5 system 2 </context>
<context position="78741" citStr="Agresti 1990" startWordPosition="12696" endWordPosition="12697">If we eliminate both synonyms and SVD from LRA, all that distinguishes LRA from VSM-WMTS is the patterns (step 4). The VSM approach uses a fixed list of 64 patterns to generate 128 dimensional vectors (Turney and Littman 2005), whereas LRA uses a dynamically generated set of 4,000 patterns, resulting in 8,000 dimensional vectors. We can see the value of the automatically generated patterns by comparing LRA without synonyms and SVD (column 4) to VSM-WMTS (column 5). The difference in both precision and recall is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). The ablation experiments support the value of the patterns (step 4) and synonyms (step 1) in LRA, but the contribution of SVD (step 9) has not been proven, although we believe more data will support its effectiveness. Nonetheless, the three components together result in a 16% increase in F (compare column 1 to 5). 6.6 Matrix Symmetry We know a priori that, if A:B::C:D, then B:A::D:C. For example, mason is to stone as carpenter is to wood implies stone is to mason as wood is to carpenter. Therefore, a good measure of relational similarity, simr, should obey the following equation: simr(A:B,C:</context>
<context position="82278" citStr="Agresti 1990" startWordPosition="13309" endWordPosition="13310">sfied. 6.7 All Alternates versus Better Alternates In step 12 of LRA, the relational similarity between A:B and C:D is the average of the cosines, among the (num filter + 1)2 cosines from step 11, that are greater than or equal to the cosine of the original pairs, A:B and C:D. That is, the average includes only those alternates that are “better” than the originals. Taking all alternates instead of the better alternates, recall drops from 56.1% to 40.4% and precision drops from 56.8% to 40.8%. Both decreases are statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). 6.8 Interpreting Vectors Suppose a word pair A:B corresponds to a vector r in the matrix X. It would be convenient if inspection of r gave us a simple explanation or description of the relation between A and B. For example, suppose the word pair ostrich:bird maps to the row vector r. It would be pleasing to look in r and find that the largest element corresponds to the pattern “is the largest” (i.e., “ostrich is the largest bird”). Unfortunately, inspection of r reveals no such convenient patterns. We hypothesize that the semantic content of a vector is distributed over the whole vector; it </context>
<context position="94992" citStr="Agresti 1990" startWordPosition="15476" endWordPosition="15477">ry both collapse to participant. On the 30 class problem, LRA with the single nearest neighbor algorithm achieves an accuracy of 39.8% (239/600) and a macroaveraged F of 36.6%. Always guessing the majority class would result in an accuracy of 8.2% (49/600). On the 5 class problem, the accuracy is 58.0% (348/600) and the macroaveraged F is 54.6%. Always guessing the majority class would give an accuracy of 43.3% (260/600). For both the 30 class and 5 class problems, LRA’s accuracy is significantly higher than guessing the majority class, with 95% confidence, according to the Fisher Exact Test (Agresti 1990). 7.3 LRA versus VSM Table 20 shows the performance of LRA and VSM on the 30 class problem. VSM-AV is VSM with the AltaVista corpus and VSM-WMTS is VSM with the WMTS corpus. The results for VSM-AV are taken from Turney and Littman (2005). All three pairwise differences in the three F measures are statistically significant at the 95% level, according to the Paired t-Test (Feelders and Verkooijen 1995). The accuracy of LRA is significantly higher than the accuracies of VSM-AV and VSM-WMTS, according to the Fisher Exact Test (Agresti 1990), but the difference between the two VSM accuracies is not</context>
</contexts>
<marker>Agresti, 1990</marker>
<rawString>Agresti, Alan. 1990. Categorical Data Analysis. Wiley, Hoboken, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
</authors>
<title>Latent semantic space: Iterative scaling improves precision of inter-document similarity measurement.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-2000),</booktitle>
<pages>216--223</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="99543" citStr="Ando 2000" startWordPosition="16216" endWordPosition="16217">d, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using the combination method of Turney et al. (2003). SVD is only one of many methods for handling sparse, noisy data. We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling (IS) (Ando 2000). We had some interesting results with small matrices (around 2,000 rows by 1,000 columns), but none of these methods seemed substantially better than SVD and none of them scaled up to the matrix sizes we are using here (e.g., 17,232 rows and 8,000 columns; see Section 6.1). In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns. Perhaps a more sophisticated selection algorithm would improve the performance of LRA. We have tried a variety of ways of selecting patterns, but it seems that the method of selection has little impact on perf</context>
</contexts>
<marker>Ando, 2000</marker>
<rawString>Ando, Rie Kubota. 2000. Latent semantic space: Iterative scaling improves precision of inter-document similarity measurement. In Proceedings of the 23rd Annual ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-2000), pages 216–223, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
<author>Berthier A Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>ACM Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="4871" citStr="Baeza-Yates and Ribeiro-Neto 1999" startWordPosition="696" endWordPosition="699">l known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3. This article builds on the Vector Space Model (VSM) of information retrieval. Given a query, a search engine produces a ranked list of documents. The documents are ranked in order of decreasing attributional similarity between the query and each document. Almost all modern search engines measure attributional similarity using the VSM (Baeza-Yates and Ribeiro-Neto 1999). Turney and Littman (2005) adapt the VSM approach to measuring relational similarity. They used a vector of frequencies of patterns in a corpus to represent the relation between a pair of words. Section 4 presents the VSM approach to measuring similarity. In Section 5, we present an algorithm for measuring relational similarity, which we call Latent Relational Analysis (LRA). The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. LRA extends the VSM approach of Turney and Littman (2005) in three ways: (1) The connecting patterns are derived automaticall</context>
<context position="36762" citStr="Baeza-Yates and Ribeiro-Neto 1999" startWordPosition="5607" endWordPosition="5610">es. Moldovan et al. (2004) argue that semantic roles are merely a special case of semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. 4. The Vector Space Model This section examines past work on measuring attributional and relational similarity using the VSM. 4.1 Measuring Attributional Similarity with the Vector Space Model The VSM was first developed for information retrieval (Salton and McGill 1983; Salton and Buckley 1988; Salton 1989) and it is at the core of most modern search engines (Baeza-Yates and Ribeiro-Neto 1999). In the VSM approach to information retrieval, queries and documents are represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. The VSM approa</context>
<context position="59007" citStr="Baeza-Yates and Ribeiro-Neto 1999" startWordPosition="9369" endWordPosition="9372">ight to columns that are uniform. Therefore we weight the cell xi,j by wj = 1 − Hj/ log(m), which varies from 0 when pi,j is uniform to 1 when entropy is minimal. We also apply the log transformation to frequencies, log(xi,j + 1). (Entropy is calculated with the original frequency values, before the log transformation is applied.) For all i and all j, replace the original value xi,j in X by the new value wj log(xi,j + 1). This is an instance of the Term Frequency-Inverse Document Frequency (TF-IDF) family of transformations, which is familiar in information retrieval (Salton and Buckley 1988; Baeza-Yates and Ribeiro-Neto 1999): log(xi,j + 1) is the TF term and wj is the IDF term. 9. Apply SVD: After the log and entropy transformations have been applied to the matrix X, run SVDLIBC. SVD decomposes a matrix X into a product of three matrices UEVT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length: UTU = VTV = I) and E is a diagonal matrix of singular values (hence SVD) (Golub and Van Loan 1996). If X is of rank r, then E is also of rank r. Let Ek, where k &lt; r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be the matrices produced by s</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Baeza-Yates, Ricardo A. and Berthier A. Ribeiro-Neto. 1999. Modern Information Retrieval. ACM Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03),</booktitle>
<pages>805--810</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="3644" citStr="Banerjee and Pedersen 2003" startWordPosition="524" endWordPosition="527">inguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known</context>
<context position="13352" citStr="Banerjee and Pedersen 2003" startWordPosition="1993" endWordPosition="1996">similarity and relational similarity, following Medin, Goldstone, and Gentner (1990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example</context>
<context position="28142" citStr="Banerjee and Pedersen 2003" startWordPosition="4239" endWordPosition="4242">then we can disambiguate the given word. Yarowsky’s (1993) observation that collocations are almost always monosemous is evidence for this view. Federici, Montemagni, and Pirrelli (1997) present an analogybased approach to word sense disambiguation. For example, consider the word plant. Out of context, plant could refer to an industrial plant or a living organism. Suppose plant appears in some text near food. A typical approach to disambiguating plant would compare the attributional similarity of food and industrial plant to the attributional similarity of food and living organism (Lesk 1986; Banerjee and Pedersen 2003). In this case, the decision may not be clear, since industrial plants often produce food and living organisms often serve as food. It would be very helpful to know the relation between food and plant in this example. In the phrase “food for the plant,” the relation between food and plant strongly suggests that the plant is a living organism, since industrial plants do not need food. In the text “food at the plant,” the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations. Thus, an algorithm for classifying semantic re</context>
<context position="49464" citStr="Banerjee and Pedersen 2003" startWordPosition="7731" endWordPosition="7734">l similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. WordNet’s sorting does not directly correspond to sorting by degree of attributional similarity, although various algorithms have been proposed for deriving attributional similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). 5.4 Singular Value Decomposition We use Rohde’s SVDLIBC implementation of the SVD, which is based on SVDPACKC (Berry 1992).6 In LRA, SVD is used to reduce noise and compensate for sparseness. 4 See http://multitext.uwaterloo.ca/. 5 The online demonstration is at http://www.cs.ualberta.ca/∼lindek/demos/depsim.htm and the downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz. 6 SVDLIBC is available at http://tedlab.mit.edu/∼dr/SVDLIBC/ and SVDPACKC is available at http://www.netlib.org/svdpack/. 394 Turney Similarity of Semantic Relations 5.5 The Algorithm We wil</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Banerjee, Satanjeev and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI-03), pages 805–810, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semi-automatic recognition of noun modifier relationships.</title>
<date>1998</date>
<booktitle>Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (COLING-ACL’98),</booktitle>
<pages>96--102</pages>
<editor>In Christian Boitet and Pete Whitelock, editors,</editor>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="26112" citStr="Barker and Szpakowicz (1998)" startWordPosition="3919" endWordPosition="3922">ase and Szpakowicz (2003) explore a similar approach to classifying general noun-modifier pairs (i.e., not restricted to a particular domain, such as medicine), using WordNet and Roget’s Thesaurus as lexical resources. Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to classify noun-modifier pairs. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used a measure of relational similarity, but their measure was based on literal matching, which limited its ability to generalize. Moldovan et al. (2004) also used a measure of relational similarity based on mapping each noun and modifier into semantic classes in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used the VSM (as a component in a single nearest n</context>
<context position="98532" citStr="Barker and Szpakowicz 1998" startWordPosition="16061" endWordPosition="16064"> yield performance improvements. With 600 noun-modifier pairs and 30 classes, the average class has only 411 Computational Linguistics Volume 32, Number 3 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Unfortunately, it is time consuming and expensive to acquire hand-labeled data. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. The 30 classes of Nastase and Szpakowicz (2003) might not be the best scheme. Other researchers have proposed different schemes (Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veal</context>
</contexts>
<marker>Barker, Szpakowicz, 1998</marker>
<rawString>Barker, Ken and Stan Szpakowicz. 1998. Semi-automatic recognition of noun modifier relationships. In Christian Boitet and Pete Whitelock, editors, Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (COLING-ACL’98), pages 96–102, San Francisco, CA. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL ’99),</booktitle>
<pages>57--64</pages>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="32418" citStr="Berland and Charniak (1999)" startWordPosition="4926" endWordPosition="4929">that relation in a corpus of semantically tagged text. They argue that the desired semantic relation can easily be inferred from the surface form of the question. A question of the form “Where... ?” is likely to be looking for entities with a location relation and a question of the form “What did ... make?” is likely to be looking for entities with a product relation. In Section 7, we show how LRA can recognize relations such as location and product (see Table 19). 3.8 Automatic Thesaurus Generation Hearst (1992) presents an algorithm for learning hyponym (type of) relations from a corpus and Berland and Charniak (1999) describe how to learn meronym (part of) relations from a corpus. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. WordNet distinguishes more than a dozen semantic relations between words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for noun-modifier pairs. Hearst and Berland and Charniak (1999) use manually generated rules to mine text for semantic relations. Turney and Littman (2005) also use a manually generated set of 64 patterns. LRA does not use a pred</context>
<context position="85585" citStr="Berland and Charniak (1999)" startWordPosition="13871" endWordPosition="13874"> of the 64 manual patterns can be found in the automatic patterns. If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns. This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns. Turney and Littman (2005) point out that some of their 64 patterns have been used by other researchers. For example, Hearst (1992) used the pattern such as to discover hyponyms and Berland and Charniak (1999) used the pattern of the to discover meronyms. Both of these patterns are included in the 4,000 patterns automatically generated by LRA. The novelty in Turney and Littman (2005) is that their patterns are not used to mine text for instances of word pairs that fit the patterns (Hearst 1992; Berland and Charniak 1999); instead, they are used to gather frequency data for building vectors that represent the relation between a given pair of words. The results in Section 6.8 show that a vector contains more information than any single pattern or small set of patterns; a vector is a distributed repre</context>
<context position="87006" citStr="Berland and Charniak (1999)" startWordPosition="14122" endWordPosition="14125">ney and Littman (2005) by finding patterns automatically. Riloff and Jones (1999) and Yangarber (2003) also find patterns automatically, but their goal is to mine text for instances of word pairs; the same goal as Hearst (1992) and Table 18 Performance as a function of N. N Correct Incorrect Skipped Precision Recall F 1 114 179 81 38.9 30.5 34.2 3 146 206 22 41.5 39.0 40.2 10 167 201 6 45.4 44.7 45.0 30 174 196 4 47.0 46.5 46.8 100 178 192 4 48.1 47.6 47.8 300 192 178 4 51.9 51.3 51.6 1000 198 172 4 53.5 52.9 53.2 3000 207 163 4 55.9 55.3 55.6 407 Computational Linguistics Volume 32, Number 3 Berland and Charniak (1999). Because LRA uses patterns to build distributed vector representations, it can exploit patterns that would be much too noisy and unreliable for the kind of text mining instance extraction that is the objective of Hearst (1992), Berland and Charniak (1999), Riloff and Jones (1999), and Yangarber (2003). Therefore LRA can simply select the highest frequency patterns (step 4 in Section 5.5); it does not need the more sophisticated selection algorithms of Riloff and Jones (1999) and Yangarber (2003). 7. Experiments with Noun-Modifier Relations This section describes experiments with 600 noun-modi</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Berland, Matthew and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL ’99), pages 57–64, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
</authors>
<title>Large scale singular value computations.</title>
<date>1992</date>
<journal>International Journal of Supercomputer Applications,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="49588" citStr="Berry 1992" startWordPosition="7752" endWordPosition="7753">re the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. WordNet’s sorting does not directly correspond to sorting by degree of attributional similarity, although various algorithms have been proposed for deriving attributional similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). 5.4 Singular Value Decomposition We use Rohde’s SVDLIBC implementation of the SVD, which is based on SVDPACKC (Berry 1992).6 In LRA, SVD is used to reduce noise and compensate for sparseness. 4 See http://multitext.uwaterloo.ca/. 5 The online demonstration is at http://www.cs.ualberta.ca/∼lindek/demos/depsim.htm and the downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz. 6 SVDLIBC is available at http://tedlab.mit.edu/∼dr/SVDLIBC/ and SVDPACKC is available at http://www.netlib.org/svdpack/. 394 Turney Similarity of Semantic Relations 5.5 The Algorithm We will go through each step of LRA, using an example to illustrate the steps. Assume that the input to LRA is the 374 multiple-ch</context>
</contexts>
<marker>Berry, 1992</marker>
<rawString>Berry, Michael W. 1992. Large scale singular value computations. International Journal of Supercomputer Applications, 6(1):13–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Semantic distance in wordnet: An experimental, application-oriented evaluation of five measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on WordNet and Other Lexical Resources, Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2001),</booktitle>
<pages>29--34</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3615" citStr="Budanitsky and Hirst 2001" startWordPosition="520" endWordPosition="523">Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational sim</context>
<context position="10768" citStr="Budanitsky and Hirst (2001)" startWordPosition="1617" endWordPosition="1620">elations of A:B and C:D, the greater their relational similarity. For example, dog:bark and cat:meow have a relatively high degree of relational similarity. Cognitive scientists distinguish words that are semantically associated (bee–honey) from words that are semantically similar (deer–pony), although they recognize that some words are both associated and similar (doctor–nurse) (Chiarello et al. 1990). Both of these are types of attributional similarity, since they are based on correspondence between attributes (e.g., bees and honey are both found in hives; deer and ponies are both mammals). Budanitsky and Hirst (2001) describe semantic relatedness as follows: Recent research on the topic in computational linguistics has emphasized the perspective of semantic relatedness of two lexemes in a lexical resource, or its inverse, semantic distance. It’s important to note that semantic relatedness is a more general concept than similarity; similar entities are usually assumed to be related by virtue of their likeness (bank–trust company), but dissimilar entities may also be semantically related by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold), or just by any kind of functional relation</context>
<context position="13323" citStr="Budanitsky and Hirst 2001" startWordPosition="1989" endWordPosition="1992">se the terms attributional similarity and relational similarity, following Medin, Goldstone, and Gentner (1990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign</context>
<context position="49435" citStr="Budanitsky and Hirst 2001" startWordPosition="7727" endWordPosition="7730"> of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. WordNet’s sorting does not directly correspond to sorting by degree of attributional similarity, although various algorithms have been proposed for deriving attributional similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). 5.4 Singular Value Decomposition We use Rohde’s SVDLIBC implementation of the SVD, which is based on SVDPACKC (Berry 1992).6 In LRA, SVD is used to reduce noise and compensate for sparseness. 4 See http://multitext.uwaterloo.ca/. 5 The online demonstration is at http://www.cs.ualberta.ca/∼lindek/demos/depsim.htm and the downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz. 6 SVDLIBC is available at http://tedlab.mit.edu/∼dr/SVDLIBC/ and SVDPACKC is available at http://www.netlib.org/svdpack/. 394 Turney Similarity of Semantic Relat</context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>Budanitsky, Alexander and Graeme Hirst. 2001. Semantic distance in wordnet: An experimental, application-oriented evaluation of five measures. In Proceedings of the Workshop on WordNet and Other Lexical Resources, Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2001), pages 29–34, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Chiarello</author>
<author>Curt Burgess</author>
<author>Lorie Richards</author>
<author>Alma Pollock</author>
</authors>
<title>Semantic and associative priming in the cerebral hemispheres: Some words do, some words don’t ... sometimes, some places. Brain and Language,</title>
<date>1990</date>
<pages>38--75</pages>
<contexts>
<context position="10546" citStr="Chiarello et al. 1990" startWordPosition="1582" endWordPosition="1585">between A and B and the relations between C and D. A measure of relational similarity is a function that maps two pairs, A:B and C:D, to a real number, simr(A:B,C:D) E R. The more correspondence there is between the relations of A:B and C:D, the greater their relational similarity. For example, dog:bark and cat:meow have a relatively high degree of relational similarity. Cognitive scientists distinguish words that are semantically associated (bee–honey) from words that are semantically similar (deer–pony), although they recognize that some words are both associated and similar (doctor–nurse) (Chiarello et al. 1990). Both of these are types of attributional similarity, since they are based on correspondence between attributes (e.g., bees and honey are both found in hives; deer and ponies are both mammals). Budanitsky and Hirst (2001) describe semantic relatedness as follows: Recent research on the topic in computational linguistics has emphasized the perspective of semantic relatedness of two lexemes in a lexical resource, or its inverse, semantic distance. It’s important to note that semantic relatedness is a more general concept than similarity; similar entities are usually assumed to be related by vir</context>
<context position="12903" citStr="Chiarello et al. 1990" startWordPosition="1930" endWordPosition="1933">lusion of other link types; that view will also be taken here, although admittedly it excludes some potentially useful information. Thus semantic similarity is a specific type of attributional similarity. The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet relational similarity is not any less semantic than attributional similarity. To avoid confusion, we will use the terms attributional similarity and relational similarity, following Medin, Goldstone, and Gentner (1990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al</context>
</contexts>
<marker>Chiarello, Burgess, Richards, Pollock, 1990</marker>
<rawString>Chiarello, Christine, Curt Burgess, Lorie Richards, and Alma Pollock. 1990. Semantic and associative priming in the cerebral hemispheres: Some words do, some words don’t ... sometimes, some places. Brain and Language, 38:75–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cathy Claman</author>
</authors>
<title>10 Real SATs. College Entrance Examination Board.</title>
<date>2000</date>
<contexts>
<context position="52625" citStr="Claman (2000)" startWordPosition="8259" endWordPosition="8260">17 alternates are dropped). This step tends to eliminate alternates that have no clear semantic relation. The third column in Table 7 shows the frequency with which each pair co-occurs in a window of max phrase words. The last column in Table 7 shows the pairs that are selected. 3. Find phrases: For each pair (originals and alternates), make a list of phrases in the corpus that contain the pair. Query the WMTS for all phrases that begin with one member of the pair and end with the other (in either order). We ignore suffixes when searching for phrases that match Table 6 This SAT question, from Claman (2000), is used to illustrate the steps in the LRA algorithm. Stem: quart:volume Choices: (a) day:night (b) mile:distance (c) decade:century (d) friction:heat (e) part:whole Solution: (b) mile:distance 395 Computational Linguistics Volume 32, Number 3 Table 7 Alternate forms of the original pair quart:volume. The first column shows the original pair and the alternate pairs. The second column shows Lin’s similarity score for the alternate word compared to the original word. For example, the similarity between quart and pint is 0.210. The third column shows the frequency of the pair in the WMTS corpus</context>
<context position="72109" citStr="Claman 2000" startWordPosition="11567" endWordPosition="11568">rcent correct) of about 57% (Turney and Littman 2005). The SAT I test consists of 78 verbal questions and 60 math questions (there is also an SAT II test, covering specific subjects, such as chemistry). Analogy questions are only a subset of the 78 verbal SAT questions. If we assume that the difficulty of our 374 analogy questions is comparable to the difficulty of the 78 verbal SAT I questions, then we can estimate that the average college-bound senior would correctly answer about 57% of the 374 analogy questions. Of our 374 SAT questions, 190 are from a collection of ten official SAT tests (Claman 2000). On this subset of the questions, LRA has a recall of 61.1%, compared to a recall of 51.1% on the other 184 questions. The 184 questions that are not from Claman (2000) seem to be more difficult. This indicates that we may be underestimating how well LRA performs, relative to college-bound senior high school students. Claman (2000) suggests that the analogy questions may be somewhat harder than other verbal SAT Table 14 LRA versus VSM with 374 SAT analogy questions. Algorithm Correct Incorrect Skipped Precision Recall F VSM-AV 176 193 5 47.7 47.1 47.4 VSM-WMTS 144 196 34 42.4 38.5 40.3 LRA 21</context>
</contexts>
<marker>Claman, 2000</marker>
<rawString>Claman, Cathy. 2000. 10 Real SATs. College Entrance Examination Board.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles L A Clarke</author>
<author>Gordon V Cormack</author>
<author>Christopher R Palmer</author>
</authors>
<title>An overview of multitext.</title>
<date>1998</date>
<journal>ACM SIGIR Forum,</journal>
<volume>32</volume>
<issue>2</issue>
<marker>Clarke, Cormack, Palmer, 1998</marker>
<rawString>Clarke, Charles L. A., Gordon V. Cormack, and Christopher R. Palmer. 1998. An overview of multitext. ACM SIGIR Forum, 32(2):14–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos F Daganzo</author>
</authors>
<title>The cell transmission model: A dynamic representation of highway traffic consistent with the hydrodynamic theory.</title>
<date>1994</date>
<journal>Transportation Research Part B: Methodological,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="2493" citStr="Daganzo 1994" startWordPosition="372" endWordPosition="373"> degree of attributional similarity, we call them synonyms. When two word pairs have a high degree of relational similarity, we say they are analogous. Verbal analogies are often written in the form A:B::C:D, meaning A is to B as C is to D; for example, traffic:street::water:riverbed. Traffic flows over a street; water flows over a riverbed. A street carries traffic; a riverbed carries water. There is a high degree of relational similarity between the word pair traffic:street and the word pair water:riverbed. In fact, this analogy is the basis of several mathematical theories of traffic flow (Daganzo 1994). In Section 2, we look more closely at the connections between attributional and relational similarity. In analogies such as mason:stone::carpenter:wood, it seems that * Institute for Information Technology, National Research Council Canada, M-50 Montreal Road, Ottawa, Ontario, Canada K1A 0R6. E-mail: peter.turney@nrc-cnrc.gc.ca. Submission received: 30 March 2005; revised submission received: 10 November 2005; accepted for publication: 27 February 2006. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attri</context>
</contexts>
<marker>Daganzo, 1994</marker>
<rawString>Daganzo, Carlos F. 1994. The cell transmission model: A dynamic representation of highway traffic consistent with the hydrodynamic theory. Transportation Research Part B: Methodological, 28(4):269–287.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman 1990</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<journal>Journal of the American Society for Information Science (JASIS),</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="3855" citStr="Deerwester et al. 1990" startWordPosition="552" endWordPosition="555">er the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and informa</context>
<context position="37959" citStr="Deerwester et al. 1990" startWordPosition="5793" endWordPosition="5796">ng cosine. The VSM approach has also been used to measure the attributional similarity of words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Latent Semantic Analysis enhances the VSM approach to information retrieval by using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 Turney Similarity of Semantic Relations to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993; Landauer and Dumais 1997). SVD improves both document-query attributional similarity measures (Deerwester et al. 1990; Dumais 1993) and word–word attributional similarity measures (Landauer and Dumais 1997). LRA also uses SVD to smooth vectors, as we discuss in Section 5. 4.2 Measuring Relational Similarity with the Vector Space Model Let R1 be the semantic relation (or set of relations) between a pair of words, A and B, and let R2 be the semantic relation (or set of relations) between another pair, C and D. We wish to measure the relational similarity between R1 and R2. The rel</context>
<context position="60656" citStr="Deerwester et al. 1990" startWordPosition="9689" endWordPosition="9692">he original matrix. In the subsequent steps, we will be calculating cosines for row vectors. For this purpose, we can simplify calculations by dropping V. The cosine of two vectors is their dot product, after they have been normalized to unit length. The matrix XXT contains the dot products of all of the row vectors. We can find the dot product of the ith and jth row vectors by looking at the cell in row i, column j of the matrix XXT. Since VTV = I, we have XXT = UEVT(UEVT)T = UEVTVETUT = UE(UE)T, which means that we can calculate cosines with the smaller matrix UE, instead of using X = UEVT (Deerwester et al. 1990). 10. Projection: Calculate UkEk (we use k = 300). This matrix has the same number of rows as X, but only k columns (instead of 2 x num patterns columns; in our experiments, that is 300 columns instead of 8,000). We can compare two word pairs by calculating the cosine of the corresponding row vectors in UkEk. The row vector for each word pair has been projected from the original 8,000 dimensional space into a new 300 dimensional space. The value k = 300 is recommended by Landauer and Dumais (1997) for measuring the attributional similarity between words. We investigate other values in Section </context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, 1990, </marker>
<rawString>Deerwester, Scott C., Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science (JASIS), 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
</authors>
<title>Metaphor as an emergent property of machine-readable dictionaries.</title>
<date>1995</date>
<booktitle>In Proceedings of the AAAI 1995 Spring Symposium Series: Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity and Generativity,</booktitle>
<pages>27--32</pages>
<location>Menlo Park, CA.</location>
<contexts>
<context position="23236" citStr="Dolan (1995)" startWordPosition="3513" endWordPosition="3514">nce of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. 3.3 Metaphor Metaphorical language is very common in our daily life, so common that we are usually unaware of it (Lakoff and Johnson 1980). Gentner et al. (2001) argue that novel metaphors are understood using analogy, but conventional metaphors are simply recalled from memory. A conventional metaphor is a metaphor that has become entrenched in our language (Lakoff and Johnson 1980). Dolan (1995) describes an algorithm that can recognize conventional metaphors, but is not suited to novel metaphors. This suggests that it may be fruitful to combine Dolan’s (1995) algorithm for handling conventional metaphorical language with LRA and SME for handling novel metaphors. Lakoff and Johnson (1980) give many examples of sentences in support of their claim that metaphorical language is ubiquitous. The metaphors in their sample sentences can be expressed using SAT-style verbal analogies of the form A:B::C:D. The first column in Table 5 is a list of sentences from Lakoff and Johnson (1980) and th</context>
</contexts>
<marker>Dolan, 1995</marker>
<rawString>Dolan, William B. 1995. Metaphor as an emergent property of machine-readable dictionaries. In Proceedings of the AAAI 1995 Spring Symposium Series: Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity and Generativity, pages 27–32, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
</authors>
<title>Enhancing performance in latent semantic indexing (LSI) retrieval.</title>
<date>1990</date>
<tech>Technical Report TM-ARH017527,</tech>
<location>Bellcore, Morristown, NJ.</location>
<contexts>
<context position="57089" citStr="Dumais 1990" startWordPosition="9000" endWordPosition="9001">columns in X. This duplication of columns is examined in Section 6.6. 7. Generate a sparse matrix: Generate a matrix X in sparse matrix format, suitable for input to SVDLIBC. The value for the cell in row i and column j is the frequency of the jth pattern (see step 6) in phrases that contain the ith word pair (see step 5). Table 9 gives some examples of pattern frequencies for quart:volume. 8. Calculate entropy: Apply log and entropy transformations to the sparse matrix (Landauer and Dumais 1997). These transformations have been found to be very helpful for information retrieval (Harman 1986; Dumais 1990). Let xi,j be the cell in row i and column j of the matrix X from step 7. Let m be the number of rows in X and let n be the number of columns. We wish to weight the cell xi,j by the entropy of the jth column. To calculate the entropy of the column, we need to convert the column into a vector of probabilities. Let pi,j be the probability of xi,j, calculated by normalizing the column vector so that the sum of the elements is one, pi j = xi j/Emk= 1 xk ,j The entropy of the jth column is then Hj = − Emk= 1 pk j log(pk j ). Entropy is at its maximum when pi,j is a uniform distribution, pi,j = 1/m,</context>
</contexts>
<marker>Dumais, 1990</marker>
<rawString>Dumais, Susan T. 1990. Enhancing performance in latent semantic indexing (LSI) retrieval. Technical Report TM-ARH017527, Bellcore, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
</authors>
<title>Latent semantic indexing (LSI) and TREC-2. In</title>
<date>1993</date>
<booktitle>Proceedings of the Second Text REtrieval Conference (TREC-2),</booktitle>
<pages>105--115</pages>
<editor>D. K. Harman, editor,</editor>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="37972" citStr="Dumais 1993" startWordPosition="5797" endWordPosition="5798">ach has also been used to measure the attributional similarity of words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Latent Semantic Analysis enhances the VSM approach to information retrieval by using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 Turney Similarity of Semantic Relations to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993; Landauer and Dumais 1997). SVD improves both document-query attributional similarity measures (Deerwester et al. 1990; Dumais 1993) and word–word attributional similarity measures (Landauer and Dumais 1997). LRA also uses SVD to smooth vectors, as we discuss in Section 5. 4.2 Measuring Relational Similarity with the Vector Space Model Let R1 be the semantic relation (or set of relations) between a pair of words, A and B, and let R2 be the semantic relation (or set of relations) between another pair, C and D. We wish to measure the relational similarity between R1 and R2. The relations R1 and</context>
</contexts>
<marker>Dumais, 1993</marker>
<rawString>Dumais, Susan T. 1993. Latent semantic indexing (LSI) and TREC-2. In D. K. Harman, editor, Proceedings of the Second Text REtrieval Conference (TREC-2), pages 105–115. National Institute of Standards and Technology, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Falkenhainer</author>
</authors>
<title>Analogical interpretation in context.</title>
<date>1990</date>
<booktitle>In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society,</booktitle>
<pages>69--76</pages>
<location>Mahwah, NJ.</location>
<contexts>
<context position="22238" citStr="Falkenhainer 1990" startWordPosition="3355" endWordPosition="3356">on) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nucleus) and attracts(sun, planet) to attracts(nucleus, electron). Each individual connection (e.g., from revolve(planet, sun) to revolve(electron, nucleus)) in an analogical mapping implies that the connected relations are similar; thus, SMT requires a measure of relational similarity in order to form maps. Early versions of SME only mapped identical relations, but later versions of SME allowed similar, nonidentical relations to match (Falkenhainer 1990). However, the focus of research in analogy making has been on the mapping process as a whole, rather than measuring the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary. We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. 3</context>
</contexts>
<marker>Falkenhainer, 1990</marker>
<rawString>Falkenhainer, Brian. 1990. Analogical interpretation in context. In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, pages 69–76. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Falkenhainer</author>
<author>Kenneth D Forbus</author>
<author>Dedre Gentner</author>
</authors>
<title>The structure-mapping engine: Algorithm and examples.</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<volume>41</volume>
<issue>1</issue>
<marker>Falkenhainer, Forbus, Gentner, 1989</marker>
<rawString>Falkenhainer, Brian, Kenneth D. Forbus, and Dedre Gentner. 1989. The structure-mapping engine: Algorithm and examples. Artificial Intelligence, 41(1):1–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Federici</author>
<author>Simonetta Montemagni</author>
<author>Vito Pirrelli</author>
</authors>
<title>Inferring semantic similarity from distributional evidence: An analogy-based approach to word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>90--97</pages>
<location>Madrid,</location>
<marker>Federici, Montemagni, Pirrelli, 1997</marker>
<rawString>Federici, Stefano, Simonetta Montemagni, and Vito Pirrelli. 1997. Inferring semantic similarity from distributional evidence: An analogy-based approach to word sense disambiguation. In Proceedings of the ACL/EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 90–97, Madrid, Spain.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ad Feelders</author>
<author>William Verkooijen 1995</author>
</authors>
<title>Which method learns the most from data? Methodological issues in the analysis of comparative studies.</title>
<booktitle>In Fifth International Workshop on Artificial Intelligence and Statistics,</booktitle>
<pages>219--225</pages>
<location>Ft. Lauderdale, FL.</location>
<marker>Feelders, 1995, </marker>
<rawString>Feelders, Ad and William Verkooijen.1995. Which method learns the most from data? Methodological issues in the analysis of comparative studies. In Fifth International Workshop on Artificial Intelligence and Statistics, pages 219–225, Ft. Lauderdale, FL.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="16564" citStr="(1998)" startWordPosition="2493" endWordPosition="2493">ion of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: number of correct guesses precision = (2) total number of guesses made recall = number of correct guesses (3) maximum possible number correct _ 2 × precision × recall ( ) F precision + recall 4 Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general). Table 4 shows the experimental results for our set of 374 analogy questions. For example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered correctly, 224 incorrectly, and 30 questions were skipped. When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30). The first five algorithms in Table 4 are implemented in Pedersen’s WordNetSimilarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003). The difference between the lowest performance (Jiang and Conrath 1997) and random guessing is statistica</context>
<context position="20182" citStr="(1998)" startWordPosition="3061" endWordPosition="3061">2) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, </context>
<context position="26112" citStr="(1998)" startWordPosition="3922" endWordPosition="3922">03) explore a similar approach to classifying general noun-modifier pairs (i.e., not restricted to a particular domain, such as medicine), using WordNet and Roget’s Thesaurus as lexical resources. Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to classify noun-modifier pairs. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used a measure of relational similarity, but their measure was based on literal matching, which limited its ability to generalize. Moldovan et al. (2004) also used a measure of relational similarity based on mapping each noun and modifier into semantic classes in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used the VSM (as a component in a single nearest n</context>
</contexts>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M French</author>
</authors>
<title>The computational modeling of analogy-making.</title>
<date>2002</date>
<journal>Trends in Cognitive Sciences,</journal>
<volume>6</volume>
<issue>5</issue>
<contexts>
<context position="19578" citStr="French (2002)" startWordPosition="2969" endWordPosition="2970">he same 374 SAT questions, attaining a score of 43%. Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths in WordNet, joining A to B and C to D. The quality measure was based on the similarity between the A:B paths and the C:D paths. Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version of the VSM approach, which reached 56% on the 374 SAT questions. Here we go beyond Turney (2005) by describing LRA in more detail, performing more extensive experiments, and analyzing the algorithm and related work in more depth. 3.2 Structure Mapping Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1</context>
<context position="101800" citStr="French 2002" startWordPosition="16577" endWordPosition="16578">ples of the many potential applications for measures of relational similarity. Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used. Gentner et al. (2001) argue that relational similarity is essential to understanding novel metaphors (as opposed to conventional metaphors). Many 412 Turney Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking (Lakoff and Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner et al. 2001; French 2002). We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence. In future work, we plan to investigate some potential applications for LRA. It is possible that the error rate of LRA is still too high for practical applications, but the fact that LRA matches average human performance on SAT analogy questions is encouraging. Acknowledgments Thanks to Michael Littman for sharing the 374 SAT analogy questions and for inspiring me to tackle them. Thanks to Vivi Nastase and Stan Szpakowicz for sha</context>
</contexts>
<marker>French, 2002</marker>
<rawString>French, Robert M. 2002. The computational modeling of analogy-making. Trends in Cognitive Sciences, 6(5):200–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
</authors>
<title>Structure-mapping: A theoretical framework for analogy.</title>
<date>1983</date>
<journal>Cognitive Science,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="9071" citStr="Gentner (1983)" startWordPosition="1345" endWordPosition="1346"> and future work in Section 8 and we conclude in Section 9. 2. Attributional and Relational Similarity In this section, we explore connections between attributional and relational similarity. 2.1 Types of Similarity Medin, Goldstone, and Gentner (1990) distinguish attributes and relations as follows: Attributes are predicates taking one argument (e.g., X is red, X is large), whereas relations are predicates taking two or more arguments (e.g., X collides with Y, X is larger than Y). Attributes are used to state properties of objects; relations express relations between objects or propositions. Gentner (1983) notes that what counts as an attribute or a relation can depend on the context. For example, large can be viewed as an attribute of X, LARGE(X), or a relation between X and some standard Y, LARGER THAN(X, Y). The amount of attributional similarity between two words, A and B, depends on the degree of correspondence between the properties of A and B. A measure of attributional similarity is a function that maps two words, A and B, to a real number, 381 Computational Linguistics Volume 32, Number 3 sima(A,B) E R. The more correspondence there is between the properties of A and B, the greater the</context>
<context position="14433" citStr="Gentner 1983" startWordPosition="2152" endWordPosition="2153"> been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best performance on the TOEFL questions for each type of attributional similarity algorithm. The results support the claim that lexicon-based algorithms have no advantage over corpus-based algorithms for recognizing synonymy. 2.3 Using Attributional Similarity to Solve Analogies We may distinguish near analogies (mason:stone::carpenter:wood) from far analogies (traffic:street::water:riverbed) (Gentner 1983; Medin, Goldstone, and Gentner 1990). In an analogy A:B::C:D, where there is a high degree of relational similarity between A:B and C:D, if there is also a high degree of attributional similarity between A and C, and between B and D, then A:B::C:D is a near analogy; otherwise, it is a far analogy. It seems possible that SAT analogy questions might consist largely of near analogies, in which case they can be solved using attributional similarity measures. We could score each candidate analogy by the average of the attributional similarity, sima, between A and C and between B and D: score(A:B::</context>
<context position="19630" citStr="Gentner 1983" startWordPosition="2976" endWordPosition="2977"> Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths in WordNet, joining A to B and C to D. The quality measure was based on the similarity between the A:B paths and the C:D paths. Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version of the VSM approach, which reached 56% on the 374 SAT questions. Here we go beyond Turney (2005) by describing LRA in more detail, performing more extensive experiments, and analyzing the algorithm and related work in more depth. 3.2 Structure Mapping Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath </context>
</contexts>
<marker>Gentner, 1983</marker>
<rawString>Gentner, Dedre. 1983. Structure-mapping: A theoretical framework for analogy. Cognitive Science, 7(2):155–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
<author>Brian Bowdle</author>
<author>Phillip Wolff</author>
<author>Consuelo Boronat</author>
</authors>
<title>Metaphor is like analogy.</title>
<date>2001</date>
<editor>In Dedre Gentner, Keith J. Holyoak, and Boicho N. Kokinov, editors,</editor>
<contexts>
<context position="22998" citStr="Gentner et al. (2001)" startWordPosition="3476" endWordPosition="3479"> any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary. We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. 3.3 Metaphor Metaphorical language is very common in our daily life, so common that we are usually unaware of it (Lakoff and Johnson 1980). Gentner et al. (2001) argue that novel metaphors are understood using analogy, but conventional metaphors are simply recalled from memory. A conventional metaphor is a metaphor that has become entrenched in our language (Lakoff and Johnson 1980). Dolan (1995) describes an algorithm that can recognize conventional metaphors, but is not suited to novel metaphors. This suggests that it may be fruitful to combine Dolan’s (1995) algorithm for handling conventional metaphorical language with LRA and SME for handling novel metaphors. Lakoff and Johnson (1980) give many examples of sentences in support of their claim that</context>
<context position="101447" citStr="Gentner et al. (2001)" startWordPosition="16524" endWordPosition="16527">large corpus. LRA extends this approach in three ways: (1) The patterns are generated dynamically from the corpus, (2) SVD is used to smooth the data, and (3) a thesaurus is used to explore variations of the word pairs. With the WMTS corpus (about 5 × 1010 English words), LRA achieves an F of 56.5%, whereas the F of VSM is 40.3%. We have presented several examples of the many potential applications for measures of relational similarity. Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used. Gentner et al. (2001) argue that relational similarity is essential to understanding novel metaphors (as opposed to conventional metaphors). Many 412 Turney Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking (Lakoff and Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner et al. 2001; French 2002). We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence. In future work, we plan to investigate some potential applications for LRA. It </context>
</contexts>
<marker>Gentner, Bowdle, Wolff, Boronat, 2001</marker>
<rawString>Gentner, Dedre, Brian Bowdle, Phillip Wolff, and Consuelo Boronat. 2001. Metaphor is like analogy. In Dedre Gentner, Keith J. Holyoak, and Boicho N. Kokinov, editors,</rawString>
</citation>
<citation valid="false">
<title>The Analogical Mind: Perspectives from Cognitive Science.</title>
<pages>199--253</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<marker></marker>
<rawString>The Analogical Mind: Perspectives from Cognitive Science. MIT Press, Cambridge, MA, pages 199–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="35822" citStr="Gildea and Jurafsky 2002" startWordPosition="5457" endWordPosition="5460">as {Catholic, Protestant, ...} from the Christian corpus. Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Christian schools and traditions. This is interesting work, but it is not directly applicable to SAT analogies, because it discovers analogies between clusters of words rather than individual words. 3.10 Identifying Semantic Roles A semantic frame for an event such as judgement contains semantic roles such as judge, evaluee, and reason, whereas an event such as statement contains roles such as speaker, addressee, and message (Gildea and Jurafsky 2002). The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. We believe that it may be helpful to view semantic frames and their semantic roles as sets of semantic relations; thus, a measure of relational similarity should help us to identify semantic roles. Moldovan et al. (2004) argue that semantic roles are merely a special case of semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. 4. The Vector Space Model This section examines past work</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan I Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<pages>80--87</pages>
<location>Edmonton, Canada.</location>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Girju, Roxana, Adriana Badulescu, and Dan I. Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2003), pages 80–87, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Goldenberg</author>
</authors>
<title>The emperor’s new clothes: Undressing the new and unimproved SAT. Gelf Magazine,</title>
<date>2005</date>
<note>http://www.gelf-magazine.com/mt/ archives/the emperors new clothes.html.</note>
<contexts>
<context position="6716" citStr="Goldenberg 2005" startWordPosition="988" endWordPosition="989">he analogy. The correct choice is called the solution and the incorrect choices are distractors. We evaluate LRA by testing its ability to select the solution and avoid the distractors. The average performance of collegebound senior high school students on verbal SAT questions corresponds to an accuracy of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the VSM attained 47%. 1 The College Board eliminated analogies from the SAT in 2005, apparently because it was believed that analogy questions discriminate against minorities, although it has been argued by liberals (Goldenberg 2005) that dropping analogy questions has increased discrimination against minorities and by conservatives (Kurtz 2002) that it has decreased academic standards. Analogy questions remain an important component in many other tests, such as the GRE. 380 Turney Similarity of Semantic Relations Table 1 An example of a typical SAT question, from the collection of 374 questions. Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood One application for relational similarity is classifying semantic relations in nounm</context>
</contexts>
<marker>Goldenberg, 2005</marker>
<rawString>Goldenberg, David. 2005. The emperor’s new clothes: Undressing the new and unimproved SAT. Gelf Magazine, March. http://www.gelf-magazine.com/mt/ archives/the emperors new clothes.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene H Golub</author>
<author>Charles F Van Loan</author>
</authors>
<date>1996</date>
<booktitle>Matrix Computations. 3rd ed. Johns Hopkins</booktitle>
<publisher>University Press,</publisher>
<location>Baltimore, MD.</location>
<marker>Golub, Van Loan, 1996</marker>
<rawString>Golub, Gene H. and Charles F. Van Loan. 1996. Matrix Computations. 3rd ed. Johns Hopkins University Press, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<title>An experimental study of factors important in document ranking.</title>
<date>1986</date>
<booktitle>In Proceedings of the Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’86),</booktitle>
<pages>186--193</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="57075" citStr="Harman 1986" startWordPosition="8998" endWordPosition="8999">num patterns columns in X. This duplication of columns is examined in Section 6.6. 7. Generate a sparse matrix: Generate a matrix X in sparse matrix format, suitable for input to SVDLIBC. The value for the cell in row i and column j is the frequency of the jth pattern (see step 6) in phrases that contain the ith word pair (see step 5). Table 9 gives some examples of pattern frequencies for quart:volume. 8. Calculate entropy: Apply log and entropy transformations to the sparse matrix (Landauer and Dumais 1997). These transformations have been found to be very helpful for information retrieval (Harman 1986; Dumais 1990). Let xi,j be the cell in row i and column j of the matrix X from step 7. Let m be the number of rows in X and let n be the number of columns. We wish to weight the cell xi,j by the entropy of the jth column. To calculate the entropy of the column, we need to convert the column into a vector of probabilities. Let pi,j be the probability of xi,j, calculated by normalizing the column vector so that the sum of the elements is one, pi j = xi j/Emk= 1 xk ,j The entropy of the jth column is then Hj = − Emk= 1 pk j log(pk j ). Entropy is at its maximum when pi,j is a uniform distributio</context>
</contexts>
<marker>Harman, 1986</marker>
<rawString>Harman, Donna. 1986. An experimental study of factors important in document ranking. In Proceedings of the Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’86), pages 186–193, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="32309" citStr="Hearst (1992)" startWordPosition="4911" endWordPosition="4912">dovan et al. (2004) propose to map a given question to a semantic relation and then search for that relation in a corpus of semantically tagged text. They argue that the desired semantic relation can easily be inferred from the surface form of the question. A question of the form “Where... ?” is likely to be looking for entities with a location relation and a question of the form “What did ... make?” is likely to be looking for entities with a product relation. In Section 7, we show how LRA can recognize relations such as location and product (see Table 19). 3.8 Automatic Thesaurus Generation Hearst (1992) presents an algorithm for learning hyponym (type of) relations from a corpus and Berland and Charniak (1999) describe how to learn meronym (part of) relations from a corpus. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. WordNet distinguishes more than a dozen semantic relations between words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for noun-modifier pairs. Hearst and Berland and Charniak (1999) use manually generated rules to mine text for semantic r</context>
<context position="33598" citStr="Hearst (1992)" startWordPosition="5109" endWordPosition="5110">patterns. LRA does not use a predefined set of patterns; it learns patterns from a large corpus. Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data. Girju, Badulescu, and Moldovan (2003) present an algorithm for learning meronym relations from a corpus. Like Hearst (1992) and Berland and Charniak (1999), they use manually generated rules to mine text for their desired relation. However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules. 3.9 Information Retrieval Veale (2003) has developed an algorithm for recognizing certain types of word analogies, based on information in WordNet. He proposes to use the algorithm for analogical information retrieval. For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas. The algorithm was designed with a focus</context>
<context position="85507" citStr="Hearst (1992)" startWordPosition="13859" endWordPosition="13860">tomatically generated patterns. If we require an exact match, 50 of the 64 manual patterns can be found in the automatic patterns. If we are lenient about wildcards, and count the pattern not the as matching * not the (for example), then 60 of the 64 manual patterns appear within the automatic patterns. This suggests that the improvement in performance with the automatic patterns is due to the increased quantity of patterns, rather than a qualitative difference in the patterns. Turney and Littman (2005) point out that some of their 64 patterns have been used by other researchers. For example, Hearst (1992) used the pattern such as to discover hyponyms and Berland and Charniak (1999) used the pattern of the to discover meronyms. Both of these patterns are included in the 4,000 patterns automatically generated by LRA. The novelty in Turney and Littman (2005) is that their patterns are not used to mine text for instances of word pairs that fit the patterns (Hearst 1992; Berland and Charniak 1999); instead, they are used to gather frequency data for building vectors that represent the relation between a given pair of words. The results in Section 6.8 show that a vector contains more information tha</context>
<context position="87233" citStr="Hearst (1992)" startWordPosition="14160" endWordPosition="14161">Performance as a function of N. N Correct Incorrect Skipped Precision Recall F 1 114 179 81 38.9 30.5 34.2 3 146 206 22 41.5 39.0 40.2 10 167 201 6 45.4 44.7 45.0 30 174 196 4 47.0 46.5 46.8 100 178 192 4 48.1 47.6 47.8 300 192 178 4 51.9 51.3 51.6 1000 198 172 4 53.5 52.9 53.2 3000 207 163 4 55.9 55.3 55.6 407 Computational Linguistics Volume 32, Number 3 Berland and Charniak (1999). Because LRA uses patterns to build distributed vector representations, it can exploit patterns that would be much too noisy and unreliable for the kind of text mining instance extraction that is the objective of Hearst (1992), Berland and Charniak (1999), Riloff and Jones (1999), and Yangarber (2003). Therefore LRA can simply select the highest frequency patterns (step 4 in Section 5.5); it does not need the more sophisticated selection algorithms of Riloff and Jones (1999) and Yangarber (2003). 7. Experiments with Noun-Modifier Relations This section describes experiments with 600 noun-modifier pairs, hand-labeled with 30 classes of semantic relations (Nastase and Szpakowicz 2003). In the following experiments, LRA is used with the baseline parameter values, exactly as described in Section 5.5. No adjustments wer</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, Marti A. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the Fourteenth International Conference on Computational Linguistics, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge 1998</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database,</booktitle>
<pages>305--332</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Hirst, 1998, </marker>
<rawString>Hirst, Graeme and David St-Onge.1998. Lexical chains as representations of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 305–332. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual ACM Conference on Research and Development in Information Retrieval (SIGIR ’99),</booktitle>
<pages>50--57</pages>
<location>Berkeley, CA,</location>
<contexts>
<context position="99422" citStr="Hofmann 1999" startWordPosition="16199" endWordPosition="16200">h to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using the combination method of Turney et al. (2003). SVD is only one of many methods for handling sparse, noisy data. We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling (IS) (Ando 2000). We had some interesting results with small matrices (around 2,000 rows by 1,000 columns), but none of these methods seemed substantially better than SVD and none of them scaled up to the matrix sizes we are using here (e.g., 17,232 rows and 8,000 columns; see Section 6.1). In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns. Perhaps a more sophisticated selection algorithm would improve the performance of LRA. W</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, Thomas. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual ACM Conference on Research and Development in Information Retrieval (SIGIR ’99), pages 50–57, Berkeley, CA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Hofstadter</author>
</authors>
<title>and the Fluid Analogies Research Group.</title>
<date>1995</date>
<publisher>Basic Books,</publisher>
<location>New York.</location>
<marker>Hofstadter, 1995</marker>
<rawString>Hofstadter, Douglas and the Fluid Analogies Research Group. 1995. Fluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought. Basic Books, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03),</booktitle>
<pages>212--219</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="15665" citStr="Jarmasz and Szpakowicz (2003)" startWordPosition="2345" endWordPosition="2348">) = 12(sima(A,C)+sima(B,D)) (1) This kind of approach was used in two of the thirteen modules in Turney et al. (2003) (see Section 3.1). Table 2 An example of a typical TOEFL question, from the collection of 80 questions. Stem: Levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed 383 Computational Linguistics Volume 32, Number 3 Table 3 Performance of attributional similarity measures on the 80 TOEFL questions. (The average non-English US college applicant’s performance is included in the bottom row, for comparison.) Reference Description Percent correct Jarmasz and Szpakowicz (2003) Best lexicon-based algorithm 78.75 Terra and Clarke (2003) Best corpus-based algorithm 81.25 Turney et al. (2003) Best hybrid algorithm 97.50 Landauer and Dumais (1997) Average human score 64.50 To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: number of correct guesses precision = (2) total number of guesses made recall = number of correct guesses (3) maximum possible number correct _ 2 × precision × recall ( ) F precision + </context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Jarmasz, Mario and Stan Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03), pages 212–219, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics (ROCLING X),</booktitle>
<pages>pages</pages>
<location>Tapei, Taiwan.</location>
<contexts>
<context position="3564" citStr="Jiang and Conrath 1997" startWordPosition="512" endWordPosition="515">uary 2006. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional simil</context>
<context position="13489" citStr="Jiang and Conrath 1997" startWordPosition="2016" endWordPosition="2019">ally similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best performance on the TOEFL questions for each type of attribut</context>
<context position="17130" citStr="Jiang and Conrath 1997" startWordPosition="2579" endWordPosition="2582"> example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered correctly, 224 incorrectly, and 30 questions were skipped. When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30). The first five algorithms in Table 4 are implemented in Pedersen’s WordNetSimilarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003). The difference between the lowest performance (Jiang and Conrath 1997) and random guessing is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). However, the difference between the highest performance (Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically significant with 95% confidence. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. 2 See http://www.d.umn.edu/∼tpederse/similarity.html. 384 T</context>
<context position="20236" citStr="Jiang and Conrath (1997)" startWordPosition="3066" endWordPosition="3069">MT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attr</context>
<context position="49408" citStr="Jiang and Conrath 1997" startWordPosition="7723" endWordPosition="7726">f words, sorted in order of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. WordNet’s sorting does not directly correspond to sorting by degree of attributional similarity, although various algorithms have been proposed for deriving attributional similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). 5.4 Singular Value Decomposition We use Rohde’s SVDLIBC implementation of the SVD, which is based on SVDPACKC (Berry 1992).6 In LRA, SVD is used to reduce noise and compensate for sparseness. 4 See http://multitext.uwaterloo.ca/. 5 The online demonstration is at http://www.cs.ualberta.ca/∼lindek/demos/depsim.htm and the downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz. 6 SVDLIBC is available at http://tedlab.mit.edu/∼dr/SVDLIBC/ and SVDPACKC is available at http://www.netlib.org/svdpack/. 394 Turney S</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jiang, Jay J. and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics (ROCLING X), pages 19–33, Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kurtz</author>
</authors>
<title>Testing debate. National Review Magazine,</title>
<date>2002</date>
<note>http://www.nationalreview.com/ kurtz/kurtz082102.asp.</note>
<contexts>
<context position="6830" citStr="Kurtz 2002" startWordPosition="1002" endWordPosition="1003">ting its ability to select the solution and avoid the distractors. The average performance of collegebound senior high school students on verbal SAT questions corresponds to an accuracy of about 57%. LRA achieves an accuracy of about 56%. On these same questions, the VSM attained 47%. 1 The College Board eliminated analogies from the SAT in 2005, apparently because it was believed that analogy questions discriminate against minorities, although it has been argued by liberals (Goldenberg 2005) that dropping analogy questions has increased discrimination against minorities and by conservatives (Kurtz 2002) that it has decreased academic standards. Analogy questions remain an important component in many other tests, such as the GRE. 380 Turney Similarity of Semantic Relations Table 1 An example of a typical SAT question, from the collection of 374 questions. Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood One application for relational similarity is classifying semantic relations in nounmodifier pairs (Turney and Littman 2005). In Section 7, we evaluate the performance of LRA with a set of 600 noun-m</context>
</contexts>
<marker>Kurtz, 2002</marker>
<rawString>Kurtz, Stanley. 2002. Testing debate. National Review Magazine, August. http://www.nationalreview.com/ kurtz/kurtz082102.asp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="22975" citStr="Lakoff and Johnson 1980" startWordPosition="3472" endWordPosition="3475">ing the similarity between any two particular relations; hence, the similarity measures used in SME at the level of individual connections are somewhat rudimentary. We believe that a more sophisticated measure of relational similarity, such as LRA, may enhance the performance of SME. Likewise, the focus of our work here is on the similarity between particular relations, and we ignore systematic mapping between sets of relations, so LRA may also be enhanced by integration with SME. 3.3 Metaphor Metaphorical language is very common in our daily life, so common that we are usually unaware of it (Lakoff and Johnson 1980). Gentner et al. (2001) argue that novel metaphors are understood using analogy, but conventional metaphors are simply recalled from memory. A conventional metaphor is a metaphor that has become entrenched in our language (Lakoff and Johnson 1980). Dolan (1995) describes an algorithm that can recognize conventional metaphors, but is not suited to novel metaphors. This suggests that it may be fruitful to combine Dolan’s (1995) algorithm for handling conventional metaphorical language with LRA and SME for handling novel metaphors. Lakoff and Johnson (1980) give many examples of sentences in supp</context>
<context position="24261" citStr="Lakoff and Johnson (1980)" startWordPosition="3673" endWordPosition="3676">The metaphors in their sample sentences can be expressed using SAT-style verbal analogies of the form A:B::C:D. The first column in Table 5 is a list of sentences from Lakoff and Johnson (1980) and the second column shows how the metaphor that is implicit in each sentence may be made explicit as a verbal analogy. 3.4 Classifying Semantic Relations The task of classifying semantic relations is to identify the relation between a pair of words. Often the pairs are restricted to noun-modifier pairs, but there are many 386 Turney Similarity of Semantic Relations Table 5 Metaphorical sentences from Lakoff and Johnson (1980), rendered as SAT-style verbal analogies. Metaphorical sentence SAT-style verbal analogy He shot down all of my arguments. aircraft:shoot down::argument:refute I demolished his argument. building:demolish::argument:refute You need to budget your time. money:budget::time:schedule I’ve invested a lot of time in her. money:invest::time:allocate My mind just isn’t operating today. machine:operate::mind:think Life has cheated me. charlatan:cheat::life:disappoint Inflation is eating up our profits. animal:eat::inflation:reduce interesting relations, such as antonymy, that do not occur in noun-modifi</context>
<context position="101709" citStr="Lakoff and Johnson 1980" startWordPosition="16561" endWordPosition="16564">lish words), LRA achieves an F of 56.5%, whereas the F of VSM is 40.3%. We have presented several examples of the many potential applications for measures of relational similarity. Just as attributional similarity measures have proven to have many practical uses, we expect that relational similarity measures will soon become widely used. Gentner et al. (2001) argue that relational similarity is essential to understanding novel metaphors (as opposed to conventional metaphors). Many 412 Turney Similarity of Semantic Relations researchers have argued that metaphor is the heart of human thinking (Lakoff and Johnson 1980; Hofstadter and the Fluid Analogies Research Group 1995; Gentner et al. 2001; French 2002). We believe that relational similarity plays a fundamental role in the mind and therefore relational similarity measures could be crucial for artificial intelligence. In future work, we plan to investigate some potential applications for LRA. It is possible that the error rate of LRA is still too high for practical applications, but the fact that LRA matches average human performance on SAT analogy questions is encouraging. Acknowledgments Thanks to Michael Littman for sharing the 374 SAT analogy questi</context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>Lakoff, George and Mark Johnson. 1980. Metaphors We Live By. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="3540" citStr="Landauer and Dumais 1997" startWordPosition="508" endWordPosition="511">d for publication: 27 February 2006. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measure</context>
<context position="13403" citStr="Landauer and Dumais 1997" startWordPosition="2000" endWordPosition="2003"> Goldstone, and Gentner (1990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table </context>
<context position="15834" citStr="Landauer and Dumais (1997)" startWordPosition="2369" endWordPosition="2372">FL question, from the collection of 80 questions. Stem: Levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed 383 Computational Linguistics Volume 32, Number 3 Table 3 Performance of attributional similarity measures on the 80 TOEFL questions. (The average non-English US college applicant’s performance is included in the bottom row, for comparison.) Reference Description Percent correct Jarmasz and Szpakowicz (2003) Best lexicon-based algorithm 78.75 Terra and Clarke (2003) Best corpus-based algorithm 81.25 Turney et al. (2003) Best hybrid algorithm 97.50 Landauer and Dumais (1997) Average human score 64.50 To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: number of correct guesses precision = (2) total number of guesses made recall = number of correct guesses (3) maximum possible number correct _ 2 × precision × recall ( ) F precision + recall 4 Note that recall is the same as percent correct (for multiple-choice questions, with only zero or one guesses allowed per question, but not in general). Table 4</context>
<context position="37999" citStr="Landauer and Dumais 1997" startWordPosition="5799" endWordPosition="5802">been used to measure the attributional similarity of words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Latent Semantic Analysis enhances the VSM approach to information retrieval by using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 Turney Similarity of Semantic Relations to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993; Landauer and Dumais 1997). SVD improves both document-query attributional similarity measures (Deerwester et al. 1990; Dumais 1993) and word–word attributional similarity measures (Landauer and Dumais 1997). LRA also uses SVD to smooth vectors, as we discuss in Section 5. 4.2 Measuring Relational Similarity with the Vector Space Model Let R1 be the semantic relation (or set of relations) between a pair of words, A and B, and let R2 be the semantic relation (or set of relations) between another pair, C and D. We wish to measure the relational similarity between R1 and R2. The relations R1 and R2 are not given to us; ou</context>
<context position="56978" citStr="Landauer and Dumais 1997" startWordPosition="8982" endWordPosition="8985"> pattern P, create a column for “word1 P word2” and another column for “word2 P word1.” Thus there will be 2 × num patterns columns in X. This duplication of columns is examined in Section 6.6. 7. Generate a sparse matrix: Generate a matrix X in sparse matrix format, suitable for input to SVDLIBC. The value for the cell in row i and column j is the frequency of the jth pattern (see step 6) in phrases that contain the ith word pair (see step 5). Table 9 gives some examples of pattern frequencies for quart:volume. 8. Calculate entropy: Apply log and entropy transformations to the sparse matrix (Landauer and Dumais 1997). These transformations have been found to be very helpful for information retrieval (Harman 1986; Dumais 1990). Let xi,j be the cell in row i and column j of the matrix X from step 7. Let m be the number of rows in X and let n be the number of columns. We wish to weight the cell xi,j by the entropy of the jth column. To calculate the entropy of the column, we need to convert the column into a vector of probabilities. Let pi,j be the probability of xi,j, calculated by normalizing the column vector so that the sum of the elements is one, pi j = xi j/Emk= 1 xk ,j The entropy of the jth column is</context>
<context position="61158" citStr="Landauer and Dumais (1997)" startWordPosition="9780" endWordPosition="9783">)T, which means that we can calculate cosines with the smaller matrix UE, instead of using X = UEVT (Deerwester et al. 1990). 10. Projection: Calculate UkEk (we use k = 300). This matrix has the same number of rows as X, but only k columns (instead of 2 x num patterns columns; in our experiments, that is 300 columns instead of 8,000). We can compare two word pairs by calculating the cosine of the corresponding row vectors in UkEk. The row vector for each word pair has been projected from the original 8,000 dimensional space into a new 300 dimensional space. The value k = 300 is recommended by Landauer and Dumais (1997) for measuring the attributional similarity between words. We investigate other values in Section 6.4. 11. Evaluate alternates: Let A:B and C:D be any two word pairs in the input set. From step 2, we have (num filter + 1) versions of A:B, the original and num filter alternates. Likewise, we have (num filter + 1) versions of C:D. 398 Turney Similarity of Semantic Relations Therefore we have (num filter + 1)2 ways to compare a version of A:B with a version of C:D. Look for the row vectors in UkEk that correspond to the versions of A:B and the versions of C:D and calculate the (num filter + 1)2 c</context>
<context position="76999" citStr="Landauer and Dumais (1997)" startWordPosition="12394" endWordPosition="12397">6.2 k 900 10 56.2 55.6 55.9 Without SVD (compare column 1 to 2 in Table 17), performance drops, but the drop is not statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). However, we hypothesize that the drop in performance would be significant with a larger set of word pairs. More word pairs would increase the sample size, which would decrease the 95% confidence interval, which would likely show that SVD is making a significant contribution. Furthermore, more word pairs would increase the matrix size, which would give SVD more leverage. For example, Landauer and Dumais (1997) apply SVD to a matrix of 30,473 columns by 60,768 rows, but our matrix Table 17 Results of ablation experiments. LRA LRA LRA LRA VSM-WMTS Baseline No SVD No synonyms No SVD, 5 system 2 3 no synonyms 1 4 Correct 210 198 185 178 144 Incorrect 160 172 167 173 196 Skipped 4 4 22 23 34 Precision 56.8 53.5 52.6 50.7 42.4 Recall 56.1 52.9 49.5 47.6 38.5 F 56.5 53.2 51.0 49.1 40.3 404 Turney Similarity of Semantic Relations here is 8,000 columns by 17,232 rows. We are currently gathering more SAT questions to test this hypothesis. Without synonyms (compare column 1 to 3 in Table 17), recall drops sig</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, Thomas K. and Susan T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of NLP tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004),</booktitle>
<pages>121--128</pages>
<location>Boston, MA.</location>
<contexts>
<context position="27107" citStr="Lapata and Keller (2004)" startWordPosition="4078" endWordPosition="4081">nd the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used the VSM (as a component in a single nearest neighbor learning algorithm) to measure relational similarity. We take the same approach here, substituting LRA for the VSM, in Section 7. Lauer (1995) used a corpus-based approach (using the BNC) to paraphrase noun– modifier pairs by inserting the prepositions of, for, in, at, on, from, with, and about. For example, reptile haven was paraphrased as haven for reptiles. Lapata and Keller (2004) achieved improved results on this task by using the database of AltaVista’s search engine as a corpus. 3.5 Word Sense Disambiguation We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text. If we can identify the semantic 387 Computational Linguistics Volume 32, Number 3 relations between the given word and its context, then we can disambiguate the given word. Yarowsky’s (1993) observation that collocations are almost always monosemous is evidence for this view. Federici, Montemagni, and Pirrelli (1997) prese</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Lapata, Mirella and Frank Keller. 2004. The web as a baseline: Evaluating the performance of unsupervised web-based models for a range of NLP tasks. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), pages 121–128, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Compound Nouns.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University, Sydney.</institution>
<contexts>
<context position="26862" citStr="Lauer (1995)" startWordPosition="4041" endWordPosition="4042">hich limited its ability to generalize. Moldovan et al. (2004) also used a measure of relational similarity based on mapping each noun and modifier into semantic classes in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used the VSM (as a component in a single nearest neighbor learning algorithm) to measure relational similarity. We take the same approach here, substituting LRA for the VSM, in Section 7. Lauer (1995) used a corpus-based approach (using the BNC) to paraphrase noun– modifier pairs by inserting the prepositions of, for, in, at, on, from, with, and about. For example, reptile haven was paraphrased as haven for reptiles. Lapata and Keller (2004) achieved improved results on this task by using the database of AltaVista’s search engine as a corpus. 3.5 Word Sense Disambiguation We believe that the intended sense of a polysemous word is determined by its semantic relations with the other words in the surrounding text. If we can identify the semantic 387 Computational Linguistics Volume 32, Number</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Lauer, Mark. 1995. Designing Statistical Language Learners: Experiments on Compound Nouns. Ph.D. thesis, Macquarie University, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database,</booktitle>
<pages>265--283</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="20286" citStr="Leacock and Chodorow (1998)" startWordPosition="3074" endWordPosition="3077">e Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (exp</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Leacock, Claudia and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database, pages 265–283. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Learning the parts of objects by nonnegative matrix factorization.</title>
<date>1999</date>
<journal>Nature,</journal>
<pages>401--788</pages>
<contexts>
<context position="99360" citStr="Lee and Seung 1999" startWordPosition="16190" endWordPosition="16193">s scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using the combination method of Turney et al. (2003). SVD is only one of many methods for handling sparse, noisy data. We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling (IS) (Ando 2000). We had some interesting results with small matrices (around 2,000 rows by 1,000 columns), but none of these methods seemed substantially better than SVD and none of them scaled up to the matrix sizes we are using here (e.g., 17,232 rows and 8,000 columns; see Section 6.1). In step 4 of LRA, we simply select the top num patterns most frequent patterns and discard the remaining patterns. Perhaps a more sophisticat</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>Lee, Daniel D. and H. Sebastian Seung. 1999. Learning the parts of objects by nonnegative matrix factorization. Nature, 401:788–791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Lesk</author>
</authors>
<title>Word-word associations in document retrieval systems.</title>
<date>1969</date>
<journal>American Documentation,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="3501" citStr="Lesk 1969" startWordPosition="504" endWordPosition="505">0 November 2005; accepted for publication: 27 February 2006. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similari</context>
<context position="13377" citStr="Lesk 1969" startWordPosition="1998" endWordPosition="1999">wing Medin, Goldstone, and Gentner (1990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL q</context>
<context position="37443" citStr="Lesk 1969" startWordPosition="5716" endWordPosition="5717">e represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. The VSM approach has also been used to measure the attributional similarity of words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Latent Semantic Analysis enhances the VSM approach to information retrieval by using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 Turney Similarity of Semantic Relations to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993; Landauer and Dumais 1997). SVD improves both document-query attributi</context>
</contexts>
<marker>Lesk, 1969</marker>
<rawString>Lesk, Michael E. 1969. Word-word associations in document retrieval systems. American Documentation, 20(1):27–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of ACM SIGDOC ’86,</booktitle>
<pages>24--26</pages>
<location>New York, NY.</location>
<contexts>
<context position="4043" citStr="Lesk 1986" startWordPosition="579" endWordPosition="580"> measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3. This article builds on the Vector Space Model (VSM) of information retrieval. Given a query, a search engine produces a ranked list of document</context>
<context position="13296" citStr="Lesk 1986" startWordPosition="1987" endWordPosition="1988">, we will use the terms attributional similarity and relational similarity, following Medin, Goldstone, and Gentner (1990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the T</context>
<context position="28113" citStr="Lesk 1986" startWordPosition="4237" endWordPosition="4238">s context, then we can disambiguate the given word. Yarowsky’s (1993) observation that collocations are almost always monosemous is evidence for this view. Federici, Montemagni, and Pirrelli (1997) present an analogybased approach to word sense disambiguation. For example, consider the word plant. Out of context, plant could refer to an industrial plant or a living organism. Suppose plant appears in some text near food. A typical approach to disambiguating plant would compare the attributional similarity of food and industrial plant to the attributional similarity of food and living organism (Lesk 1986; Banerjee and Pedersen 2003). In this case, the decision may not be clear, since industrial plants often produce food and living organisms often serve as food. It would be very helpful to know the relation between food and plant in this example. In the phrase “food for the plant,” the relation between food and plant strongly suggests that the plant is a living organism, since industrial plants do not need food. In the text “food at the plant,” the relation strongly suggests that the plant is an industrial plant, since living organisms are not usually considered as locations. Thus, an algorith</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, Michael E. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of ACM SIGDOC ’86, pages 24–26, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Evaluating text categorization.</title>
<date>1991</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>312--318</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>Asilomar, CA.</location>
<contexts>
<context position="93551" citStr="Lewis 1991" startWordPosition="15244" endWordPosition="15245">calculating only 359,400 + 288, 000 = 647,400 cosines. There are 600 word pairs in the input set for LRA. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 2,400 pairs. In step 5, for each pair A:B, we add B:A, yielding 4,800 pairs. However, some pairs are dropped because they correspond to zero vectors and a few words do not appear in Lin’s thesaurus. The sparse matrix (step 7) has 4,748 rows and 8,000 columns, with a density of 8.4%. Following Turney and Littman (2005), we evaluate the performance by accuracy and also by the macroaveraged F measure (Lewis 1991). Macroaveraging calculates the precision, recall, and F for each class separately, and then calculates the average across all classes. Microaveraging combines the true positive, false positive, and false negative counts for all of the classes, and then calculates precision, recall, and F from the combined counts. Macroaveraging gives equal weight to all classes, but microaveraging gives more weight to larger classes. We use macroaveraging (giving equal weight to all classes), because we have no reason to believe that the class sizes in the data set reflect the actual distribution of the class</context>
</contexts>
<marker>Lewis, 1991</marker>
<rawString>Lewis, David D. 1991. Evaluating text categorization. In Proceedings of the Speech and Natural Language Workshop, pages 312–318, Asilomar, CA. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL ’98),</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="3574" citStr="Lin 1998" startWordPosition="516" endWordPosition="517">ation for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the</context>
<context position="13413" citStr="Lin 1998" startWordPosition="2004" endWordPosition="2005">990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3</context>
<context position="20325" citStr="Lin (1998" startWordPosition="3082" endWordPosition="3083">nd Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). T</context>
<context position="39860" citStr="Lin 1998" startWordPosition="6150" endWordPosition="6151"> (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases that contain X and Y, such as X of Y, Y of X, X for Y, Y for X, X to Y, and Y to X. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Several authors report that the logarithmic transformation of frequencies improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin 1998b). Turney and Littman (2005) evaluated the VSM approach by its performance on 374 SAT analogy questions, achieving a score of 47%. Since there are five choices for each question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. 391 Computational Linguistics Volume 32, Numb</context>
<context position="48395" citStr="Lin 1998" startWordPosition="7561" endWordPosition="7562">hesaurus is available through an on-line interactive demonstration or it can be downloaded.5 We used the on-line demonstration, since the downloadable version seems to contain fewer words. For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms. As a courtesy to other users of Lin’s on-line system, we insert a 20-second delay between each two queries. Lin’s thesaurus was generated by parsing a corpus of about 5 x 107 English words, consisting of text from the Wall Street Journal, San Jose Mercury, and AP Newswire (Lin 1998a). The parser was used to extract pairs of words and their grammatical relations. Words were then clustered into synonym sets, based on the similarity of their grammatical relations. Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. Given a word and its part of speech, Lin’s thesaurus provides a list of words, sorted in order of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordN</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998a. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics (COLING-ACL ’98), pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th International Conference on Machine Learning (ICML ’98),</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="3574" citStr="Lin 1998" startWordPosition="516" endWordPosition="517">ation for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the</context>
<context position="13413" citStr="Lin 1998" startWordPosition="2004" endWordPosition="2005">990). Instead of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3</context>
<context position="20325" citStr="Lin (1998" startWordPosition="3082" endWordPosition="3083">nd Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). T</context>
<context position="39860" citStr="Lin 1998" startWordPosition="6150" endWordPosition="6151"> (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases that contain X and Y, such as X of Y, Y of X, X for Y, Y for X, X to Y, and Y to X. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Several authors report that the logarithmic transformation of frequencies improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin 1998b). Turney and Littman (2005) evaluated the VSM approach by its performance on 374 SAT analogy questions, achieving a score of 47%. Since there are five choices for each question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. 391 Computational Linguistics Volume 32, Numb</context>
<context position="48395" citStr="Lin 1998" startWordPosition="7561" endWordPosition="7562">hesaurus is available through an on-line interactive demonstration or it can be downloaded.5 We used the on-line demonstration, since the downloadable version seems to contain fewer words. For each word in the input set of word pairs, we automatically query the on-line demonstration and fetch the resulting list of synonyms. As a courtesy to other users of Lin’s on-line system, we insert a 20-second delay between each two queries. Lin’s thesaurus was generated by parsing a corpus of about 5 x 107 English words, consisting of text from the Wall Street Journal, San Jose Mercury, and AP Newswire (Lin 1998a). The parser was used to extract pairs of words and their grammatical relations. Words were then clustered into synonym sets, based on the similarity of their grammatical relations. Two words were judged to be highly similar when they tended to have the same kinds of grammatical relations with the same sets of words. Given a word and its part of speech, Lin’s thesaurus provides a list of words, sorted in order of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordN</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998b. An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning (ICML ’98), pages 296–304. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zvika Marx</author>
<author>Ido Dagan</author>
<author>Joachim Buhmann</author>
<author>Eli Shamir</author>
</authors>
<title>Coupled clustering: A method for detecting structural correspondence.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--747</pages>
<contexts>
<context position="34770" citStr="Marx et al. (2002)" startWordPosition="5295" endWordPosition="5298">Vedas. The algorithm was designed with a focus on analogies of the form adjective:noun::adjective:noun, such as Christian:church::Muslim:mosque. A measure of relational similarity is applicable to this task. Given a pair of words, A and B, the task is to return another pair of words, X and Y, such that there is high relational similarity between the pair A:X and the pair Y:B. For example, given 389 Computational Linguistics Volume 32, Number 3 A = Muslim and B = church, return X = mosque and Y = Christian. (The pair Muslim:mosque has a high relational similarity to the pair Christian:church.) Marx et al. (2002) developed an unsupervised algorithm for discovering analogies by clustering words from two different corpora. Each cluster of words in one corpus is coupled one-to-one with a cluster in the other corpus. For example, one experiment used a corpus of Buddhist documents and a corpus of Christian documents. A cluster of words such as {Hindu, Mahayana, Zen, ...} from the Buddhist corpus was coupled with a cluster of words such as {Catholic, Protestant, ...} from the Christian corpus. Thus the algorithm appears to have discovered an analogical mapping between Buddhist schools and traditions and Chr</context>
</contexts>
<marker>Marx, Dagan, Buhmann, Shamir, 2002</marker>
<rawString>Marx, Zvika, Ido Dagan, Joachim Buhmann, and Eli Shamir. 2002. Coupled clustering: A method for detecting structural correspondence. Journal of Machine Learning Research, 3:747–780.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Douglas L Medin</author>
<author>Robert L Goldstone</author>
</authors>
<title>and Dedre Gentner.1990. Similarity involving attributes and relations: Judgments of similarity and difference are not inverses.</title>
<journal>Psychological Science,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Medin, Goldstone, </marker>
<rawString>Medin, Douglas L., Robert L. Goldstone, and Dedre Gentner.1990. Similarity involving attributes and relations: Judgments of similarity and difference are not inverses. Psychological Science, 1(1):64–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
<author>Roxana Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>In Proceedings of the Computational Lexical Semantics Workshop at HLT-NAACL 2004,</booktitle>
<pages>60--67</pages>
<location>Boston, MA.</location>
<contexts>
<context position="26312" citStr="Moldovan et al. (2004)" startWordPosition="3949" endWordPosition="3952">l resources. Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to classify noun-modifier pairs. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used a measure of relational similarity, but their measure was based on literal matching, which limited its ability to generalize. Moldovan et al. (2004) also used a measure of relational similarity based on mapping each noun and modifier into semantic classes in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used the VSM (as a component in a single nearest neighbor learning algorithm) to measure relational similarity. We take the same approach here, substituting LRA for the VSM, in Section 7. Lauer (1995) used a corpus-based approach (using the BNC) to p</context>
<context position="31278" citStr="Moldovan et al. (2004)" startWordPosition="4734" endWordPosition="4737">wever, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval. A query to a search engine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector. There are well-known techniques in information retrieval for coping with this disparity, such as weighting schemes for query vectors that are different from the weighting schemes for document vectors (Salton and Buckley 1988). 388 Turney Similarity of Semantic Relations 3.7 Question Answering In their article on classifying semantic relations, Moldovan et al. (2004) suggest that an important application of their work is question answering (QA). As defined in the Text Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as “Where have nuclear incidents occurred?”, by retrieving a relevant document from a large corpus and then extracting a short string from the document, such as The Three Mile Island nuclear incident caused a DOE policy crisis. Moldovan et al. (2004) propose to map a given question to a semantic relation and then search for that relation in a corpus of semantically tagged text. They argue that the desired sema</context>
<context position="36154" citStr="Moldovan et al. (2004)" startWordPosition="5513" endWordPosition="5516">rds rather than individual words. 3.10 Identifying Semantic Roles A semantic frame for an event such as judgement contains semantic roles such as judge, evaluee, and reason, whereas an event such as statement contains roles such as speaker, addressee, and message (Gildea and Jurafsky 2002). The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. We believe that it may be helpful to view semantic frames and their semantic roles as sets of semantic relations; thus, a measure of relational similarity should help us to identify semantic roles. Moldovan et al. (2004) argue that semantic roles are merely a special case of semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. 4. The Vector Space Model This section examines past work on measuring attributional and relational similarity using the VSM. 4.1 Measuring Attributional Similarity with the Vector Space Model The VSM was first developed for information retrieval (Salton and McGill 1983; Salton and Buckley 1988; Salton 1989) and it is at the core of most modern search engines (Baeza-Yates and Ribeiro-Ne</context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Moldovan, Dan, Adriana Badulescu, Marta Tatu, Daniel Antohe, and Roxana Girju. 2004. Models for the semantic classification of noun phrases. In Proceedings of the Computational Lexical Semantics Workshop at HLT-NAACL 2004, pages 60–67, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="4000" citStr="Morris and Hirst 1991" startWordPosition="571" endWordPosition="574">ionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3. This article builds on the Vector Space Model (VSM) of information retrieval. Given a query, a searc</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring noun–modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Fifth International Workshop on Computational Semantics (IWCS-5),</booktitle>
<pages>285--301</pages>
<location>Tilburg, the Netherlands.</location>
<contexts>
<context position="7478" citStr="Nastase and Szpakowicz (2003)" startWordPosition="1095" endWordPosition="1098">sed academic standards. Analogy questions remain an important component in many other tests, such as the GRE. 380 Turney Similarity of Semantic Relations Table 1 An example of a typical SAT question, from the collection of 374 questions. Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood One application for relational similarity is classifying semantic relations in nounmodifier pairs (Turney and Littman 2005). In Section 7, we evaluate the performance of LRA with a set of 600 noun-modifier pairs from Nastase and Szpakowicz (2003). The problem is to classify a noun-modifier pair, such as “laser printer,” according to the semantic relation between the head noun (printer) and the modifier (laser). The 600 pairs have been manually labeled with 30 classes of semantic relations. For example, “laser printer” is classified as instrument; the printer uses the laser as an instrument for printing. We approach the task of classifying semantic relations in noun-modifier pairs as a supervised learning problem. The 600 pairs are divided into training and testing sets and a testing pair is classified according to the label of its sin</context>
<context position="25509" citStr="Nastase and Szpakowicz (2003)" startWordPosition="3835" endWordPosition="3838">noun-modifier pairs are interesting due to their high frequency in English. For instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms. Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, using Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS) as lexical resources for representing each noun-modifier pair with a feature vector. They trained a neural network to distinguish 13 classes of semantic relations. Nastase and Szpakowicz (2003) explore a similar approach to classifying general noun-modifier pairs (i.e., not restricted to a particular domain, such as medicine), using WordNet and Roget’s Thesaurus as lexical resources. Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to classify noun-modifier pairs. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. Barker and Szpakowicz (19</context>
<context position="32761" citStr="Nastase and Szpakowicz (2003)" startWordPosition="4978" endWordPosition="4981">ties with a product relation. In Section 7, we show how LRA can recognize relations such as location and product (see Table 19). 3.8 Automatic Thesaurus Generation Hearst (1992) presents an algorithm for learning hyponym (type of) relations from a corpus and Berland and Charniak (1999) describe how to learn meronym (part of) relations from a corpus. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. WordNet distinguishes more than a dozen semantic relations between words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for noun-modifier pairs. Hearst and Berland and Charniak (1999) use manually generated rules to mine text for semantic relations. Turney and Littman (2005) also use a manually generated set of 64 patterns. LRA does not use a predefined set of patterns; it learns patterns from a large corpus. Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measur</context>
<context position="40743" citStr="Nastase and Szpakowicz (2003)" startWordPosition="6290" endWordPosition="6293"> analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. 391 Computational Linguistics Volume 32, Number 3 The VSM was also evaluated by its performance as a distance (nearness) measure in a supervised nearest neighbor classifier for noun-modifier semantic relations (Turney and Littman 2005). The evaluation used 600 hand-labeled noun-modifier pairs from Nastase and Szpakowicz (2003). A testing pair is classified by searching for its single nearest neighbor in the labeled training data. The best guess is the label for the training pair with the highest cosine. LRA is evaluated with the same set of noun-modifier pairs in Section 7. Turney and Littman (2005) used the AltaVista search engine to obtain the frequency information required to build vectors for the VSM. Thus their corpus was the set of all Web pages indexed by AltaVista. At the time, the English subset of this corpus consisted of about 5 × 1011 words. Around April 2004, AltaVista made substantial changes to their</context>
<context position="87698" citStr="Nastase and Szpakowicz 2003" startWordPosition="14224" endWordPosition="14227">tations, it can exploit patterns that would be much too noisy and unreliable for the kind of text mining instance extraction that is the objective of Hearst (1992), Berland and Charniak (1999), Riloff and Jones (1999), and Yangarber (2003). Therefore LRA can simply select the highest frequency patterns (step 4 in Section 5.5); it does not need the more sophisticated selection algorithms of Riloff and Jones (1999) and Yangarber (2003). 7. Experiments with Noun-Modifier Relations This section describes experiments with 600 noun-modifier pairs, hand-labeled with 30 classes of semantic relations (Nastase and Szpakowicz 2003). In the following experiments, LRA is used with the baseline parameter values, exactly as described in Section 5.5. No adjustments were made to tune LRA to the noun-modifier pairs. LRA is used as a distance (nearness) measure in a single nearest neighbor supervised learning algorithm. 7.1 Classes of Relations The following experiments use the 600 labeled noun-modifier pairs of Nastase and Szpakowicz (2003). This data set includes information about the part of speech and WordNet synset (synonym set; i.e., word sense tag) of each word, but our algorithm does not use this information. Table 19 l</context>
<context position="89184" citStr="Nastase and Szpakowicz (2003)" startWordPosition="14469" endWordPosition="14472">ions that are typically expressed with longer phrases (three or more words), rather than noun-modifier word pairs. For clarity, we decided not to include these relations in Table 19. In this table, H represents the head noun and M represents the modifier. For example, in flu virus, the head noun (H) is virus and the modifier (M) is flu (*). In English, the modifier (typically a noun or adjective) usually precedes the head noun. In the description of purpose, V represents an arbitrary verb. In concert hall, the hall is for presenting concerts (V is present) or holding concerts (V is hold) (†). Nastase and Szpakowicz (2003) organized the relations into groups. The five capitalized terms in the Relation column of Table 19 are the names of five groups of semantic relations. (The original table had a sixth group, but there are no examples of this group in the data set.) We make use of this grouping in the following experiments. 7.2 Baseline LRA with Single Nearest Neighbor The following experiments use single nearest neighbor classification with leave-one-out cross-validation. For leave-one-out cross-validation, the testing set consists of a single noun-modifier pair and the training set consists of the 599 remaini</context>
<context position="98406" citStr="Nastase and Szpakowicz (2003)" startWordPosition="16043" endWordPosition="16046">ake 10 or even 100 terabytes affordable in the relatively near future. For noun-modifier classification, more labeled data should yield performance improvements. With 600 noun-modifier pairs and 30 classes, the average class has only 411 Computational Linguistics Volume 32, Number 3 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Unfortunately, it is time consuming and expensive to acquire hand-labeled data. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. The 30 classes of Nastase and Szpakowicz (2003) might not be the best scheme. Other researchers have proposed different schemes (Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. </context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Nastase, Vivi and Stan Szpakowicz. 2003. Exploring noun–modifier semantic relations. In Fifth International Workshop on Computational Semantics (IWCS-5), pages 285–301, Tilburg, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<location>New York, NY.</location>
<contexts>
<context position="37476" citStr="Pantel and Lin 2002" startWordPosition="5720" endWordPosition="5723">rs. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. The VSM approach has also been used to measure the attributional similarity of words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Latent Semantic Analysis enhances the VSM approach to information retrieval by using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 Turney Similarity of Semantic Relations to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993; Landauer and Dumais 1997). SVD improves both document-query attributional similarity measures (Deerwes</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 613–619, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Rada</author>
<author>Hafedh Mili</author>
<author>Ellen Bicknell</author>
<author>Maria Blettner</author>
</authors>
<title>Development and application of a metric on semantic nets.</title>
<date>1989</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="12139" citStr="Rada et al. (1989)" startWordPosition="1817" endWordPosition="1820">g., hot and cold are both kinds of temperature, pencil and paper are both used for writing). Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity. The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion. Resnik (1995) describes semantic similarity as follows: Semantic similarity represents a special case of semantic relatedness: for example, cars and gasoline would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar. Rada et al. (1989) suggest that the assessment of similarity in semantic networks can in fact be thought of as involving just taxonomic (IS-A) links, to the exclusion of other link types; that view will also be taken here, although admittedly it excludes some potentially useful information. Thus semantic similarity is a specific type of attributional similarity. The term semantic similarity is misleading, because it refers to a type of attributional similarity, yet relational similarity is not any less semantic than attributional similarity. To avoid confusion, we will use the terms attributional similarity and</context>
</contexts>
<marker>Rada, Mili, Bicknell, Blettner, 1989</marker>
<rawString>Rada, Roy, Hafedh Mili, Ellen Bicknell, and Maria Blettner. 1989. Development and application of a metric on semantic nets. IEEE Transactions on Systems, Man, and Cybernetics, 19(1):17–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Rehder</author>
<author>M E Schreiner</author>
<author>Michael B W Wolfe</author>
<author>Darrell Laham</author>
<author>Thomas K Landauer</author>
<author>Walter Kintsch</author>
</authors>
<title>Using latent semantic analysis to assess knowledge: Some technical considerations.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--337</pages>
<contexts>
<context position="3948" citStr="Rehder et al. 1998" startWordPosition="564" endWordPosition="567"> Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3. This article builds on the Vector Space Model (V</context>
</contexts>
<marker>Rehder, Schreiner, Wolfe, Laham, Landauer, Kintsch, 1998</marker>
<rawString>Rehder, Bob, M. E. Schreiner, Michael B. W. Wolfe, Darrell Laham, Thomas K. Landauer, and Walter Kintsch. 1998. Using latent semantic analysis to assess knowledge: Some technical considerations. Discourse Processes, 25:337–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter R Reitman</author>
</authors>
<title>Cognition and Thought: An Information Processing Approach.</title>
<date>1965</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="18222" citStr="Reitman 1965" startWordPosition="2744" endWordPosition="2745">ional similarity to perform as well as relational similarity. 2 See http://www.d.umn.edu/∼tpederse/similarity.html. 384 Turney Similarity of Semantic Relations 3. Related Work This section is a brief survey of the many problems that involve semantic relations and could potentially make use of an algorithm for measuring relational similarity. 3.1 Recognizing Word Analogies The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem. This problem was first attempted by a system called Argus (Reitman 1965), using a small hand-built semantic network. Argus could only solve the limited set of analogy questions that its programmer had anticipated. Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity. Turney et al. (2003) combined 13 independent modules to answer SAT questions. The final output of the system was based on a weighted combination of the outputs of each individual module. The best of the 13 modules was the VSM, which is described in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT questions, achiev</context>
</contexts>
<marker>Reitman, 1965</marker>
<rawString>Reitman, Walter R. 1965. Cognition and Thought: An Information Processing Approach. John Wiley and Sons, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th Computational Linguistics Volume 32, Number</booktitle>
<volume>3</volume>
<contexts>
<context position="3514" citStr="Resnik 1995" startWordPosition="506" endWordPosition="507">2005; accepted for publication: 27 February 2006. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as</context>
<context position="11860" citStr="Resnik (1995)" startWordPosition="1776" endWordPosition="1777">by lexical relationships such as meronymy (car–wheel) and antonymy (hot–cold), or just by any kind of functional relationship or frequent association (pencil–paper, penguin–Antarctica). As these examples show, semantic relatedness is the same as attributional similarity (e.g., hot and cold are both kinds of temperature, pencil and paper are both used for writing). Here we prefer to use the term attributional similarity because it emphasizes the contrast with relational similarity. The term semantic relatedness may lead to confusion when the term relational similarity is also under discussion. Resnik (1995) describes semantic similarity as follows: Semantic similarity represents a special case of semantic relatedness: for example, cars and gasoline would seem to be more closely related than, say, cars and bicycles, but the latter pair are certainly more similar. Rada et al. (1989) suggest that the assessment of similarity in semantic networks can in fact be thought of as involving just taxonomic (IS-A) links, to the exclusion of other link types; that view will also be taken here, although admittedly it excludes some potentially useful information. Thus semantic similarity is a specific type of </context>
<context position="13465" citStr="Resnik 1995" startWordPosition="2014" endWordPosition="2015">) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best performance on the TOEFL questions f</context>
<context position="20363" citStr="Resnik (1995)" startWordPosition="3088" endWordPosition="3089">ential work on modeling of analogy making. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source </context>
<context position="49384" citStr="Resnik 1995" startWordPosition="7721" endWordPosition="7722">ides a list of words, sorted in order of decreasing attributional similarity. This sorting is convenient for LRA, since it makes it possible to focus on words with higher attributional similarity and ignore the rest. WordNet, in contrast, given a word and its part of speech, provides a list of words grouped by the possible senses of the given word, with groups sorted by the frequencies of the senses. WordNet’s sorting does not directly correspond to sorting by degree of attributional similarity, although various algorithms have been proposed for deriving attributional similarity from WordNet (Resnik 1995; Jiang and Conrath 1997; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). 5.4 Singular Value Decomposition We use Rohde’s SVDLIBC implementation of the SVD, which is based on SVDPACKC (Berry 1992).6 In LRA, SVD is used to reduce noise and compensate for sparseness. 4 See http://multitext.uwaterloo.ca/. 5 The online demonstration is at http://www.cs.ualberta.ca/∼lindek/demos/depsim.htm and the downloadable version is at http://armena.cs.ualberta.ca/lindek/downloads/sims.lsp.gz. 6 SVDLIBC is available at http://tedlab.mit.edu/∼dr/SVDLIBC/ and SVDPACKC is available at http://www.netlib.or</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik, Philip. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th Computational Linguistics Volume 32, Number 3</rawString>
</citation>
<citation valid="false">
<booktitle>International Joint Conference on Artificial Intelligence (IJCAI-95),</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<marker></marker>
<rawString>International Joint Conference on Artificial Intelligence (IJCAI-95), pages 448–453, San Mateo, CA. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99),</booktitle>
<pages>474--479</pages>
<location>Menlo Park, CA.</location>
<contexts>
<context position="86460" citStr="Riloff and Jones (1999)" startWordPosition="14014" endWordPosition="14017">rs that fit the patterns (Hearst 1992; Berland and Charniak 1999); instead, they are used to gather frequency data for building vectors that represent the relation between a given pair of words. The results in Section 6.8 show that a vector contains more information than any single pattern or small set of patterns; a vector is a distributed representation. LRA is distinct from Hearst (1992) and Berland and Charniak (1999) in its focus on distributed representations, which it shares with Turney and Littman (2005), but LRA goes beyond Turney and Littman (2005) by finding patterns automatically. Riloff and Jones (1999) and Yangarber (2003) also find patterns automatically, but their goal is to mine text for instances of word pairs; the same goal as Hearst (1992) and Table 18 Performance as a function of N. N Correct Incorrect Skipped Precision Recall F 1 114 179 81 38.9 30.5 34.2 3 146 206 22 41.5 39.0 40.2 10 167 201 6 45.4 44.7 45.0 30 174 196 4 47.0 46.5 46.8 100 178 192 4 48.1 47.6 47.8 300 192 178 4 51.9 51.3 51.6 1000 198 172 4 53.5 52.9 53.2 3000 207 163 4 55.9 55.3 55.6 407 Computational Linguistics Volume 32, Number 3 Berland and Charniak (1999). Because LRA uses patterns to build distributed vecto</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Riloff, Ellen and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99), pages 474–479, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
</authors>
<title>Classifying the semantic relations in noun-compounds via a domain-specific lexical hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-01),</booktitle>
<pages>82--90</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="25138" citStr="Rosario and Hearst (2001)" startWordPosition="3782" endWordPosition="3785">money:budget::time:schedule I’ve invested a lot of time in her. money:invest::time:allocate My mind just isn’t operating today. machine:operate::mind:think Life has cheated me. charlatan:cheat::life:disappoint Inflation is eating up our profits. animal:eat::inflation:reduce interesting relations, such as antonymy, that do not occur in noun-modifier pairs. However, noun-modifier pairs are interesting due to their high frequency in English. For instance, WordNet 2.0 contains more than 26,000 noun-modifier pairs, although many common noun-modifiers are not in WordNet, especially technical terms. Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, using Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS) as lexical resources for representing each noun-modifier pair with a feature vector. They trained a neural network to distinguish 13 classes of semantic relations. Nastase and Szpakowicz (2003) explore a similar approach to classifying general noun-modifier pairs (i.e., not restricted to a particular domain, such as medicine), using WordNet and Roget’s Thesaurus as lexical resources. Vanderwende (1994) used hand-built </context>
<context position="98557" citStr="Rosario and Hearst 2001" startWordPosition="16065" endWordPosition="16068">nts. With 600 noun-modifier pairs and 30 classes, the average class has only 411 Computational Linguistics Volume 32, Number 3 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Unfortunately, it is time consuming and expensive to acquire hand-labeled data. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. The 30 classes of Nastase and Szpakowicz (2003) might not be the best scheme. Other researchers have proposed different schemes (Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using t</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Rosario, Barbara and Marti Hearst. 2001. Classifying the semantic relations in noun-compounds via a domain-specific lexical hierarchy. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP-01), pages 82–90, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
<author>Charles Fillmore</author>
</authors>
<title>The descent of hierarchy, and selection in relational semantics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL ’02),</booktitle>
<pages>417--424</pages>
<location>Philadelphia, PA.</location>
<marker>Rosario, Hearst, Fillmore, 2002</marker>
<rawString>Rosario, Barbara, Marti Hearst, and Charles Fillmore. 2002. The descent of hierarchy, and selection in relational semantics. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL ’02), pages 417–424, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerda Ruge</author>
</authors>
<title>Experiments on linguistically-based term associations.</title>
<date>1992</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>28--3</pages>
<contexts>
<context position="37454" citStr="Ruge 1992" startWordPosition="5718" endWordPosition="5719">ed by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the search engine sorts the matching documents in order of decreasing cosine. The VSM approach has also been used to measure the attributional similarity of words (Lesk 1969; Ruge 1992; Pantel and Lin 2002). Pantel and Lin (2002) clustered words according to their attributional similarity, as measured by a VSM. Their algorithm is able to discover the different senses of polysemous words, using unsupervised learning. Latent Semantic Analysis enhances the VSM approach to information retrieval by using the Singular Value Decomposition (SVD) to smooth the vectors, which helps 390 Turney Similarity of Semantic Relations to handle noise and sparseness in the data (Deerwester et al. 1990; Dumais 1993; Landauer and Dumais 1997). SVD improves both document-query attributional simila</context>
<context position="39850" citStr="Ruge 1992" startWordPosition="6148" endWordPosition="6149">and Littman (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases that contain X and Y, such as X of Y, Y of X, X for Y, Y for X, X to Y, and Y to X. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Several authors report that the logarithmic transformation of frequencies improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin 1998b). Turney and Littman (2005) evaluated the VSM approach by its performance on 374 SAT analogy questions, achieving a score of 47%. Since there are five choices for each question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. 391 Computational Linguistics Volum</context>
</contexts>
<marker>Ruge, 1992</marker>
<rawString>Ruge, Gerda. 1992. Experiments on linguistically-based term associations. Information Processing and Management, 28(3):317–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="36674" citStr="Salton 1989" startWordPosition="5594" endWordPosition="5595">e of relational similarity should help us to identify semantic roles. Moldovan et al. (2004) argue that semantic roles are merely a special case of semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. 4. The Vector Space Model This section examines past work on measuring attributional and relational similarity using the VSM. 4.1 Measuring Attributional Similarity with the Vector Space Model The VSM was first developed for information retrieval (Salton and McGill 1983; Salton and Buckley 1988; Salton 1989) and it is at the core of most modern search engines (Baeza-Yates and Ribeiro-Neto 1999). In the VSM approach to information retrieval, queries and documents are represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a given query, the </context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, Gerard. 1989. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--5</pages>
<contexts>
<context position="31135" citStr="Salton and Buckley 1988" startWordPosition="4714" endWordPosition="4717">d vector for John Smith and Hardcom Corporation would be very sparse, since these entities might be mentioned only once in the given document. However, this is not a new problem for the VSM; it is the standard situation when the VSM is used for information retrieval. A query to a search engine is represented by a very sparse vector, whereas a document is represented by a relatively dense vector. There are well-known techniques in information retrieval for coping with this disparity, such as weighting schemes for query vectors that are different from the weighting schemes for document vectors (Salton and Buckley 1988). 388 Turney Similarity of Semantic Relations 3.7 Question Answering In their article on classifying semantic relations, Moldovan et al. (2004) suggest that an important application of their work is question answering (QA). As defined in the Text Retrieval Conference (TREC) QA track, the task is to answer simple questions, such as “Where have nuclear incidents occurred?”, by retrieving a relevant document from a large corpus and then extracting a short string from the document, such as The Three Mile Island nuclear incident caused a DOE policy crisis. Moldovan et al. (2004) propose to map a gi</context>
<context position="36660" citStr="Salton and Buckley 1988" startWordPosition="5590" endWordPosition="5593">relations; thus, a measure of relational similarity should help us to identify semantic roles. Moldovan et al. (2004) argue that semantic roles are merely a special case of semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. 4. The Vector Space Model This section examines past work on measuring attributional and relational similarity using the VSM. 4.1 Measuring Attributional Similarity with the Vector Space Model The VSM was first developed for information retrieval (Salton and McGill 1983; Salton and Buckley 1988; Salton 1989) and it is at the core of most modern search engines (Baeza-Yates and Ribeiro-Neto 1999). In the VSM approach to information retrieval, queries and documents are represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their corresponding vectors. For a giv</context>
<context position="39839" citStr="Salton and Buckley 1988" startWordPosition="6144" endWordPosition="6147">ntaining X and Y. Turney and Littman (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases that contain X and Y, such as X of Y, Y of X, X for Y, Y for X, X to Y, and Y to X. These phrases are then used as queries for a search engine and the number of hits (matching documents) is recorded for each query. This process yields a vector of 128 numbers. If the number of hits for a query is x, then the corresponding element in the vector r is log(x + 1). Several authors report that the logarithmic transformation of frequencies improves cosine-based similarity measures (Salton and Buckley 1988; Ruge 1992; Lin 1998b). Turney and Littman (2005) evaluated the VSM approach by its performance on 374 SAT analogy questions, achieving a score of 47%. Since there are five choices for each question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. 391 Computational Lingui</context>
<context position="58971" citStr="Salton and Buckley 1988" startWordPosition="9365" endWordPosition="9368"> to the next, and less weight to columns that are uniform. Therefore we weight the cell xi,j by wj = 1 − Hj/ log(m), which varies from 0 when pi,j is uniform to 1 when entropy is minimal. We also apply the log transformation to frequencies, log(xi,j + 1). (Entropy is calculated with the original frequency values, before the log transformation is applied.) For all i and all j, replace the original value xi,j in X by the new value wj log(xi,j + 1). This is an instance of the Term Frequency-Inverse Document Frequency (TF-IDF) family of transformations, which is familiar in information retrieval (Salton and Buckley 1988; Baeza-Yates and Ribeiro-Neto 1999): log(xi,j + 1) is the TF term and wj is the IDF term. 9. Apply SVD: After the log and entropy transformations have been applied to the matrix X, run SVDLIBC. SVD decomposes a matrix X into a product of three matrices UEVT, where U and V are in column orthonormal form (i.e., the columns are orthogonal and have unit length: UTU = VTV = I) and E is a diagonal matrix of singular values (hence SVD) (Golub and Van Loan 1996). If X is of rank r, then E is also of rank r. Let Ek, where k &lt; r, be the diagonal matrix formed from the top k singular values, and let Uk </context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Salton, Gerard and Chris Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing and Management, 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="36635" citStr="Salton and McGill 1983" startWordPosition="5586" endWordPosition="5589">les as sets of semantic relations; thus, a measure of relational similarity should help us to identify semantic roles. Moldovan et al. (2004) argue that semantic roles are merely a special case of semantic relations (Section 3.4), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech. 4. The Vector Space Model This section examines past work on measuring attributional and relational similarity using the VSM. 4.1 Measuring Attributional Similarity with the Vector Space Model The VSM was first developed for information retrieval (Salton and McGill 1983; Salton and Buckley 1988; Salton 1989) and it is at the core of most modern search engines (Baeza-Yates and Ribeiro-Neto 1999). In the VSM approach to information retrieval, queries and documents are represented by vectors. Elements in these vectors are based on the frequencies of words in the corresponding queries and documents. The frequencies are usually transformed by various formulas and weights, tailored to improve the effectiveness of the search engine (Salton 1989). The attributional similarity between a query and a document is measured by the cosine of the angle between their corresp</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, Gerard and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Scholkopf</author>
<author>Alexander J Smola</author>
<author>Klaus-Robert Muller</author>
</authors>
<title>Kernel principal component analysis.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Artificial Neural Networks (ICANN-1997),</booktitle>
<pages>583--588</pages>
<location>Berlin.</location>
<marker>Scholkopf, Smola, Muller, 1997</marker>
<rawString>Scholkopf, Bernhard, Alexander J. Smola, and Klaus-Robert Muller. 1997. Kernel principal component analysis. In Proceedings of the International Conference on Artificial Neural Networks (ICANN-1997), pages 583–588, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egidio Terra</author>
<author>Charles L A Clarke</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference</booktitle>
<pages>244--251</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="15724" citStr="Terra and Clarke (2003)" startWordPosition="2353" endWordPosition="2356"> two of the thirteen modules in Turney et al. (2003) (see Section 3.1). Table 2 An example of a typical TOEFL question, from the collection of 80 questions. Stem: Levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed 383 Computational Linguistics Volume 32, Number 3 Table 3 Performance of attributional similarity measures on the 80 TOEFL questions. (The average non-English US college applicant’s performance is included in the bottom row, for comparison.) Reference Description Percent correct Jarmasz and Szpakowicz (2003) Best lexicon-based algorithm 78.75 Terra and Clarke (2003) Best corpus-based algorithm 81.25 Turney et al. (2003) Best hybrid algorithm 97.50 Landauer and Dumais (1997) Average human score 64.50 To evaluate this approach, we applied several measures of attributional similarity to our collection of 374 SAT questions. The performance of the algorithms was measured by precision, recall, and F, defined as follows: number of correct guesses precision = (2) total number of guesses made recall = number of correct guesses (3) maximum possible number correct _ 2 × precision × recall ( ) F precision + recall 4 Note that recall is the same as percent correct (f</context>
<context position="17058" citStr="Terra and Clarke (2003)" startWordPosition="2569" endWordPosition="2572">shows the experimental results for our set of 374 analogy questions. For example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered correctly, 224 incorrectly, and 30 questions were skipped. When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30). The first five algorithms in Table 4 are implemented in Pedersen’s WordNetSimilarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003). The difference between the lowest performance (Jiang and Conrath 1997) and random guessing is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). However, the difference between the highest performance (Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically significant with 95% confidence. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational</context>
<context position="41986" citStr="Terra and Clarke 2003" startWordPosition="6495" endWordPosition="6498">ing their advanced search operators. Their search engine no longer supports the asterisk operator, which was used by Turney and Littman (2005) for stemming and wild-card searching. AltaVista also changed their policy toward automated searching, which is now forbidden.3 Turney and Littman (2005) used AltaVista’s hit count, which is the number of documents (Web pages) matching a given query, but LRA uses the number of passages (strings) matching a query. In our experiments with LRA (Sections 6 and 7), we use a local copy of the Waterloo MultiText System (WMTS) (Clarke, Cormack, and Palmer 1998; Terra and Clarke 2003), running on a 16 CPU Beowulf Cluster, with a corpus of about 5 × 1010 English words. The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval). The text and index require approximately one terabyte of disk space. Although AltaVista only gives a rough estimate of the number of matching documents, the WMTS gives exact counts of the number of matching passages. Turney et al. (2003) combine 13 independent modules to answer SAT questions. The performance of LRA significantly s</context>
<context position="47195" citStr="Terra and Clarke 2003" startWordPosition="7358" endWordPosition="7361">244 word pairs. The output similarity measure is based on cosines, so the degree of similarity can range from −1 (dissimilar; 0 = 180°) to +1 (similar; 0 = 0°). Before applying SVD, the vectors are 393 Computational Linguistics Volume 32, Number 3 completely non-negative, which implies that the cosine can only range from 0 to +1, but SVD introduces negative values, so it is possible for the cosine to be negative, although we have never observed this in our experiments. 5.2 Search Engine and Corpus In the following experiments, we use a local copy of the WMTS (Clarke, Cormack, and Palmer 1998; Terra and Clarke 2003).4 The corpus consists of about 5 x 1010 English words, gathered by a Web crawler, mainly from US academic Web sites. The Web pages cover a very wide range of topics, styles, genres, quality, and writing skill. The WMTS is well suited to LRA, because the WMTS scales well to large corpora (one terabyte, in our case), it gives exact frequency counts (unlike most Web search engines), it is designed for passage retrieval (rather than document retrieval), and it has a powerful query syntax. 5.3 Thesaurus As a source of synonyms, we use Lin’s (1998a) automatically generated thesaurus. This thesaurus</context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>Terra, Egidio and Charles L. A. Clarke. 2003. Frequency estimates for statistical word similarity measures. In Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 (HLT/NAACL 2003), pages 244–251, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="3588" citStr="Turney 2001" startWordPosition="518" endWordPosition="519">omputational Linguistics Computational Linguistics Volume 32, Number 3 relational similarity can be reduced to attributional similarity, since mason and carpenter are attributionally similar, as are stone and wood. In general, this reduction fails. Consider the analogy traffic:street::water:riverbed. Traffic and water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential app</context>
<context position="13428" citStr="Turney 2001" startWordPosition="2006" endWordPosition="2007">ad of semantic similarity (Resnik 1995) or semantically similar (Chiarello et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best</context>
<context position="16975" citStr="Turney 2001" startWordPosition="2558" endWordPosition="2559"> zero or one guesses allowed per question, but not in general). Table 4 shows the experimental results for our set of 374 analogy questions. For example, using the algorithm of Hirst and St-Onge (1998), 120 questions were answered correctly, 224 incorrectly, and 30 questions were skipped. When the algorithm assigned the same similarity to all of the choices for a given question, that question was skipped. The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30). The first five algorithms in Table 4 are implemented in Pedersen’s WordNetSimilarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003). The difference between the lowest performance (Jiang and Conrath 1997) and random guessing is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). However, the difference between the highest performance (Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically significant with 95% confidence. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not </context>
<context position="20399" citStr="Turney (2001)" startWordPosition="3094" endWordPosition="3095">aking. The goal of computational modeling of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain re</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Turney, Peter D. 2001. Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning, pages 491–502, Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<pages>417--424</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3903" citStr="Turney 2002" startWordPosition="559" endWordPosition="560">water are not attributionally similar. Street and riverbed are only moderately attributionally similar. Many algorithms have been proposed for measuring the attributional similarity between two words (Lesk 1969; Resnik 1995; Landauer and Dumais 1997; Jiang and Conrath 1997; Lin 1998b; Turney 2001; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003). Measures of attributional similarity have been studied extensively, due to their applications in problems such as recognizing synonyms (Landauer and Dumais 1997), information retrieval (Deerwester et al. 1990), determining semantic orientation (Turney 2002), grading student essays (Rehder et al. 1998), measuring textual cohesion (Morris and Hirst 1991), and word sense disambiguation (Lesk 1986). On the other hand, since measures of relational similarity are not as well developed as measures of attributional similarity, the potential applications of relational similarity are not as well known. Many problems that involve semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3. Thi</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter D. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 417–424, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Measuring semantic similarity by latent relational analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI-05),</booktitle>
<pages>1136--1141</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="19240" citStr="Turney (2005)" startWordPosition="2917" endWordPosition="2918">e outputs of each individual module. The best of the 13 modules was the VSM, which is described in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%. In contrast with the corpus-based approach of Turney and Littman (2005), Veale (2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score of 43%. Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths in WordNet, joining A to B and C to D. The quality measure was based on the similarity between the A:B paths and the C:D paths. Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version of the VSM approach, which reached 56% on the 374 SAT questions. Here we go beyond Turney (2005) by describing LRA in more detail, performing more extensive experiments, and analyzing the algorithm and related work in more depth. 3.2 Structure Mapping Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner 1989) as the most influential work on modeling of analogy making. The goal of computational modeling of analogy m</context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Turney, Peter D. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI-05), pages 1136–1141, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Corpus-based learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="4898" citStr="Turney and Littman (2005)" startWordPosition="700" endWordPosition="703">semantic relations would benefit from an algorithm for measuring relational similarity. We discuss related problems in natural language processing, information retrieval, and information extraction in more detail in Section 3. This article builds on the Vector Space Model (VSM) of information retrieval. Given a query, a search engine produces a ranked list of documents. The documents are ranked in order of decreasing attributional similarity between the query and each document. Almost all modern search engines measure attributional similarity using the VSM (Baeza-Yates and Ribeiro-Neto 1999). Turney and Littman (2005) adapt the VSM approach to measuring relational similarity. They used a vector of frequencies of patterns in a corpus to represent the relation between a pair of words. Section 4 presents the VSM approach to measuring similarity. In Section 5, we present an algorithm for measuring relational similarity, which we call Latent Relational Analysis (LRA). The algorithm learns from a large corpus of unlabeled, unstructured text, without supervision. LRA extends the VSM approach of Turney and Littman (2005) in three ways: (1) The connecting patterns are derived automatically from the corpus, instead </context>
<context position="7355" citStr="Turney and Littman 2005" startWordPosition="1074" endWordPosition="1077">analogy questions has increased discrimination against minorities and by conservatives (Kurtz 2002) that it has decreased academic standards. Analogy questions remain an important component in many other tests, such as the GRE. 380 Turney Similarity of Semantic Relations Table 1 An example of a typical SAT question, from the collection of 374 questions. Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood One application for relational similarity is classifying semantic relations in nounmodifier pairs (Turney and Littman 2005). In Section 7, we evaluate the performance of LRA with a set of 600 noun-modifier pairs from Nastase and Szpakowicz (2003). The problem is to classify a noun-modifier pair, such as “laser printer,” according to the semantic relation between the head noun (printer) and the modifier (laser). The 600 pairs have been manually labeled with 30 classes of semantic relations. For example, “laser printer” is classified as instrument; the printer uses the laser as an instrument for printing. We approach the task of classifying semantic relations in noun-modifier pairs as a supervised learning problem. </context>
<context position="17368" citStr="Turney and Littman 2005" startWordPosition="2614" endWordPosition="2617"> that question was skipped. The precision was 120/(120 + 224) and the recall was 120/(120 + 224 + 30). The first five algorithms in Table 4 are implemented in Pedersen’s WordNetSimilarity package.2 The sixth algorithm (Turney 2001) used the Waterloo MultiText System (WMTS), as described in Terra and Clarke (2003). The difference between the lowest performance (Jiang and Conrath 1997) and random guessing is statistically significant with 95% confidence, according to the Fisher Exact Test (Agresti 1990). However, the difference between the highest performance (Turney 2001) and the VSM approach (Turney and Littman 2005) is also statistically significant with 95% confidence. We conclude that there are enough near analogies in the 374 SAT questions for attributional similarity to perform better than random guessing, but not enough near analogies for attributional similarity to perform as well as relational similarity. 2 See http://www.d.umn.edu/∼tpederse/similarity.html. 384 Turney Similarity of Semantic Relations 3. Related Work This section is a brief survey of the many problems that involve semantic relations and could potentially make use of an algorithm for measuring relational similarity. 3.1 Recognizing</context>
<context position="18761" citStr="Turney and Littman (2005)" startWordPosition="2830" endWordPosition="2833"> to the stem. This problem was first attempted by a system called Argus (Reitman 1965), using a small hand-built semantic network. Argus could only solve the limited set of analogy questions that its programmer had anticipated. Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity. Turney et al. (2003) combined 13 independent modules to answer SAT questions. The final output of the system was based on a weighted combination of the outputs of each individual module. The best of the 13 modules was the VSM, which is described in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%. In contrast with the corpus-based approach of Turney and Littman (2005), Veale (2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score of 43%. Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths in WordNet, joining A to B and C to D. The quality measure was based on the similarity between the A:B paths and the C:D paths. Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version of the VSM approach, which reached 56% on the 374 SAT q</context>
<context position="20453" citStr="Turney and Littman (2005)" startWordPosition="3100" endWordPosition="3103">of analogy making is to understand how people form complex, Table 4 Performance of attributional similarity measures on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom two rows are not attributional similarity measures. They are included for comparison.) Algorithm Type Precision Recall F Hirst and St-Onge (1998) Lexicon-based 34.9 32.1 33.4 Jiang and Conrath (1997) Hybrid 29.8 27.3 28.5 Leacock and Chodorow (1998) Lexicon-based 32.8 31.3 32.0 Lin (1998b) Hybrid 31.2 27.3 29.1 Resnik (1995) Hybrid 35.7 33.2 34.4 Turney (2001) Corpus-based 35.0 35.0 35.0 Turney and Littman (2005) Relational (VSM) 47.7 47.1 47.4 Random Random 20.0 20.0 20.0 385 Computational Linguistics Volume 32, Number 3 structured analogies. SME takes representations of a source domain and a target domain and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain relations. For example, there is an analogy between the </context>
<context position="26661" citStr="Turney and Littman (2005)" startWordPosition="4006" endWordPosition="4009">ss must be relationally similar to some extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used a measure of relational similarity, but their measure was based on literal matching, which limited its ability to generalize. Moldovan et al. (2004) also used a measure of relational similarity based on mapping each noun and modifier into semantic classes in WordNet. The noun-modifier pairs were taken from a corpus, and the surrounding context in the corpus was used in a word sense disambiguation algorithm to improve the mapping of the noun and modifier into WordNet. Turney and Littman (2005) used the VSM (as a component in a single nearest neighbor learning algorithm) to measure relational similarity. We take the same approach here, substituting LRA for the VSM, in Section 7. Lauer (1995) used a corpus-based approach (using the BNC) to paraphrase noun– modifier pairs by inserting the prepositions of, for, in, at, on, from, with, and about. For example, reptile haven was paraphrased as haven for reptiles. Lapata and Keller (2004) achieved improved results on this task by using the database of AltaVista’s search engine as a corpus. 3.5 Word Sense Disambiguation We believe that the </context>
<context position="29892" citStr="Turney and Littman 2005" startWordPosition="4513" endWordPosition="4516">ntence John Smith is the chief scientist of the Hardcom Corporation, there is a person–affiliation relation between John Smith and Hardcom Corporation (Zelenko, Aone, and Richardella 2003). This is similar to the problem of classifying semantic relations (Section 3.4), except that information extraction focuses on the relation between a specific pair of entities in a specific document, rather than a general pair of words in general text. Therefore an algorithm for classifying semantic relations should be useful for information extraction. In the VSM approach to classifying semantic relations (Turney and Littman 2005), we would have a training set of labeled examples of the relation person–affiliation, for instance. Each example would be represented by a vector of pattern frequencies. Given a specific document discussing John Smith and Hardcom Corporation, we could construct a vector representing the relation between these two entities and then measure the relational similarity between this unlabeled vector and each of our labeled training vectors. It would seem that there is a problem here because the training vectors would be relatively dense, since they would presumably be derived from a large corpus, b</context>
<context position="32944" citStr="Turney and Littman (2005)" startWordPosition="5005" endWordPosition="5008">an algorithm for learning hyponym (type of) relations from a corpus and Berland and Charniak (1999) describe how to learn meronym (part of) relations from a corpus. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. WordNet distinguishes more than a dozen semantic relations between words (Fellbaum 1998) and Nastase and Szpakowicz (2003) list 30 semantic relations for noun-modifier pairs. Hearst and Berland and Charniak (1999) use manually generated rules to mine text for semantic relations. Turney and Littman (2005) also use a manually generated set of 64 patterns. LRA does not use a predefined set of patterns; it learns patterns from a large corpus. Instead of manually generating new rules or patterns for each new semantic relation, it is possible to automatically learn a measure of relational similarity that can handle arbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data. Girju, Badulescu, and Moldovan (2003) present an algorithm for learni</context>
<context position="38743" citStr="Turney and Littman 2005" startWordPosition="5925" endWordPosition="5928"> attributional similarity measures (Landauer and Dumais 1997). LRA also uses SVD to smooth vectors, as we discuss in Section 5. 4.2 Measuring Relational Similarity with the Vector Space Model Let R1 be the semantic relation (or set of relations) between a pair of words, A and B, and let R2 be the semantic relation (or set of relations) between another pair, C and D. We wish to measure the relational similarity between R1 and R2. The relations R1 and R2 are not given to us; our task is to infer these hidden (latent) relations and then compare them. In the VSM approach to relational similarity (Turney and Littman 2005), we create vectors, r1 and r2, that represent features of R1 and R2, and then measure the similarity of R1 and R2 by the cosine of the angle 0 between r1 and r2: r1 = (r1,1, ... , r1,n) (5) r2 = (r2,1,...r2,n) (6) n r1,i · r2,i i=1 = � n i=1 i=1 n (r1,i)2 · (r2,i)2 cosine(0) = (7) 11r111 11r211 r1 · r2 = · r1 √r1· ·√r2· r2 r1 · r2 We create a vector, r, to characterize the relationship between two words, X and Y, by counting the frequencies of various short phrases containing X and Y. Turney and Littman (2005) use a list of 64 joining terms, such as of, for, and to, to form 128 phrases that c</context>
<context position="40650" citStr="Turney and Littman 2005" startWordPosition="6278" endWordPosition="6281">ach question, the expected score for random guessing is 20%. To answer a multiple-choice analogy question, vectors are created for the stem pair and each choice pair, and then cosines are calculated for the angles between the stem pair and each choice pair. The best guess is the choice pair with the highest cosine. We use the same set of analogy questions to evaluate LRA in Secti on 6. 391 Computational Linguistics Volume 32, Number 3 The VSM was also evaluated by its performance as a distance (nearness) measure in a supervised nearest neighbor classifier for noun-modifier semantic relations (Turney and Littman 2005). The evaluation used 600 hand-labeled noun-modifier pairs from Nastase and Szpakowicz (2003). A testing pair is classified by searching for its single nearest neighbor in the labeled training data. The best guess is the label for the training pair with the highest cosine. LRA is evaluated with the same set of noun-modifier pairs in Section 7. Turney and Littman (2005) used the AltaVista search engine to obtain the frequency information required to build vectors for the VSM. Thus their corpus was the set of all Web pages indexed by AltaVista. At the time, the English subset of this corpus cons</context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Turney, Peter D. and Michael L. Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning, 60(1–3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
<author>Jeffrey Bigham</author>
<author>Victor Shnayder</author>
</authors>
<title>Combining independent modules to solve multiple-choice synonym and analogy problems.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03),</booktitle>
<pages>482--489</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="13510" citStr="Turney et al. 2003" startWordPosition="2020" endWordPosition="2023">et al. 1990), we prefer the term taxonomical similarity, which we take to be a specific type of attributional similarity. We interpret synonymy as a high degree of attributional similarity. Analogy is a high degree of relational similarity. 382 Turney Similarity of Semantic Relations 2.2 Measuring Attributional Similarity Algorithms for measuring attributional similarity can be lexicon-based (Lesk 1986; Budanitsky and Hirst 2001; Banerjee and Pedersen 2003), corpus-based (Lesk 1969; Landauer and Dumais 1997; Lin 1998a; Turney 2001), or a hybrid of the two (Resnik 1995; Jiang and Conrath 1997; Turney et al. 2003). Intuitively, we might expect that lexicon-based algorithms would be better at capturing synonymy than corpusbased algorithms, since lexicons, such as WordNet, explicitly provide synonymy information that is only implicit in a corpus. However, experiments do not support this intuition. Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of one of the 80 TOEFL questions appears in Table 2. Table 3 shows the best performance on the TOEFL questions for each type of attributional similarity algo</context>
<context position="15153" citStr="Turney et al. (2003)" startWordPosition="2271" endWordPosition="2274">ional similarity between A:B and C:D, if there is also a high degree of attributional similarity between A and C, and between B and D, then A:B::C:D is a near analogy; otherwise, it is a far analogy. It seems possible that SAT analogy questions might consist largely of near analogies, in which case they can be solved using attributional similarity measures. We could score each candidate analogy by the average of the attributional similarity, sima, between A and C and between B and D: score(A:B::C:D) = 12(sima(A,C)+sima(B,D)) (1) This kind of approach was used in two of the thirteen modules in Turney et al. (2003) (see Section 3.1). Table 2 An example of a typical TOEFL question, from the collection of 80 questions. Stem: Levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed 383 Computational Linguistics Volume 32, Number 3 Table 3 Performance of attributional similarity measures on the 80 TOEFL questions. (The average non-English US college applicant’s performance is included in the bottom row, for comparison.) Reference Description Percent correct Jarmasz and Szpakowicz (2003) Best lexicon-based algorithm 78.75 Terra and Clarke (2003) Best corpus-based algorithm </context>
<context position="18497" citStr="Turney et al. (2003)" startWordPosition="2784" endWordPosition="2787">d potentially make use of an algorithm for measuring relational similarity. 3.1 Recognizing Word Analogies The problem of recognizing word analogies is, given a stem word pair and a finite list of choice word pairs, selecting the choice that is most analogous to the stem. This problem was first attempted by a system called Argus (Reitman 1965), using a small hand-built semantic network. Argus could only solve the limited set of analogy questions that its programmer had anticipated. Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity. Turney et al. (2003) combined 13 independent modules to answer SAT questions. The final output of the system was based on a weighted combination of the outputs of each individual module. The best of the 13 modules was the VSM, which is described in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%. In contrast with the corpus-based approach of Turney and Littman (2005), Veale (2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score of 43%. Veale evaluated the quality of a candidate analogy A:B::C:D by looking for path</context>
<context position="42491" citStr="Turney et al. (2003)" startWordPosition="6577" endWordPosition="6580"> use a local copy of the Waterloo MultiText System (WMTS) (Clarke, Cormack, and Palmer 1998; Terra and Clarke 2003), running on a 16 CPU Beowulf Cluster, with a corpus of about 5 × 1010 English words. The WMTS is a distributed (multiprocessor) search engine, designed primarily for passage retrieval (although document retrieval is possible, as a special case of passage retrieval). The text and index require approximately one terabyte of disk space. Although AltaVista only gives a rough estimate of the number of matching documents, the WMTS gives exact counts of the number of matching passages. Turney et al. (2003) combine 13 independent modules to answer SAT questions. The performance of LRA significantly surpasses this combined system, but there is no real contest between these approaches, because we can simply add LRA to the combination, as a fourteenth module. Since the VSM module had the best performance of the 13 modules (Turney et al. 2003), the following experiments focus on comparing VSM and LRA. 5. Latent Relational Analysis LRA takes as input a set of word pairs and produces as output a measure of the relational similarity between any two of the input pairs. LRA relies on three resources, a s</context>
<context position="99011" citStr="Turney et al. 2003" startWordPosition="16134" endWordPosition="16137">akowicz (2003) might not be the best scheme. Other researchers have proposed different schemes (Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using the combination method of Turney et al. (2003). SVD is only one of many methods for handling sparse, noisy data. We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling (IS) (Ando 2000). We had some interesting results with small matrices (around 2,000 </context>
</contexts>
<marker>Turney, Littman, Bigham, Shnayder, 2003</marker>
<rawString>Turney, Peter D., Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. 2003. Combining independent modules to solve multiple-choice synonym and analogy problems. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03), pages 482–489, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Algorithm for automatic interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics,</booktitle>
<pages>782--788</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="25721" citStr="Vanderwende (1994)" startWordPosition="3867" endWordPosition="3868">al terms. Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, using Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS) as lexical resources for representing each noun-modifier pair with a feature vector. They trained a neural network to distinguish 13 classes of semantic relations. Nastase and Szpakowicz (2003) explore a similar approach to classifying general noun-modifier pairs (i.e., not restricted to a particular domain, such as medicine), using WordNet and Roget’s Thesaurus as lexical resources. Vanderwende (1994) used hand-built rules, together with a lexical knowledge base, to classify noun-modifier pairs. None of these approaches explicitly involved measuring relational similarity, but any classification of semantic relations necessarily employs some implicit notion of relational similarity since members of the same class must be relationally similar to some extent. Barker and Szpakowicz (1998) tried a corpus-based approach that explicitly used a measure of relational similarity, but their measure was based on literal matching, which limited its ability to generalize. Moldovan et al. (2004) also use</context>
<context position="98504" citStr="Vanderwende 1994" startWordPosition="16059" endWordPosition="16060">abeled data should yield performance improvements. With 600 noun-modifier pairs and 30 classes, the average class has only 411 Computational Linguistics Volume 32, Number 3 20 examples. We expect that the accuracy would improve substantially with 5 or 10 times more examples. Unfortunately, it is time consuming and expensive to acquire hand-labeled data. Another issue with noun-modifier classification is the choice of classification scheme for the semantic relations. The 30 classes of Nastase and Szpakowicz (2003) might not be the best scheme. Other researchers have proposed different schemes (Vanderwende 1994; Barker and Szpakowicz 1998; Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the le</context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>Vanderwende, Lucy. 1994. Algorithm for automatic interpretation of noun sequences. In Proceedings of the Fifteenth International Conference on Computational Linguistics, pages 782–788, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
</authors>
<title>The analogical thesaurus.</title>
<date>2003</date>
<booktitle>In Proceedings of the 15th Innovative Applications of Artificial Intelligence Conference (IAAI2003),</booktitle>
<pages>137--142</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="33869" citStr="Veale (2003)" startWordPosition="5148" endWordPosition="5149">rbitrary semantic relations. A nearest neighbor algorithm can then use this relational similarity measure to learn to classify according to any set of classes of relations, given the appropriate labeled training data. Girju, Badulescu, and Moldovan (2003) present an algorithm for learning meronym relations from a corpus. Like Hearst (1992) and Berland and Charniak (1999), they use manually generated rules to mine text for their desired relation. However, they supplement their manual rules with automatically learned constraints, to increase the precision of the rules. 3.9 Information Retrieval Veale (2003) has developed an algorithm for recognizing certain types of word analogies, based on information in WordNet. He proposes to use the algorithm for analogical information retrieval. For example, the query Muslim church should return mosque and the query Hindu bible should return the Vedas. The algorithm was designed with a focus on analogies of the form adjective:noun::adjective:noun, such as Christian:church::Muslim:mosque. A measure of relational similarity is applicable to this task. Given a pair of words, A and B, the task is to return another pair of words, X and Y, such that there is high</context>
</contexts>
<marker>Veale, 2003</marker>
<rawString>Veale, Tony. 2003. The analogical thesaurus. In Proceedings of the 15th Innovative Applications of Artificial Intelligence Conference (IAAI2003), pages 137–142, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
</authors>
<title>WordNet sits the SAT: A knowledge-based approach to lexical analogy.</title>
<date>2004</date>
<booktitle>In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI2004),</booktitle>
<pages>606--612</pages>
<location>Valencia,</location>
<contexts>
<context position="18927" citStr="Veale (2004)" startWordPosition="2861" endWordPosition="2862">y questions that its programmer had anticipated. Argus was based on a spreading activation model and did not explicitly attempt to measure relational similarity. Turney et al. (2003) combined 13 independent modules to answer SAT questions. The final output of the system was based on a weighted combination of the outputs of each individual module. The best of the 13 modules was the VSM, which is described in detail in Turney and Littman (2005). The VSM was evaluated on a set of 374 SAT questions, achieving a score of 47%. In contrast with the corpus-based approach of Turney and Littman (2005), Veale (2004) applied a lexicon-based approach to the same 374 SAT questions, attaining a score of 43%. Veale evaluated the quality of a candidate analogy A:B::C:D by looking for paths in WordNet, joining A to B and C to D. The quality measure was based on the similarity between the A:B paths and the C:D paths. Turney (2005) introduced Latent Relational Analysis (LRA), an enhanced version of the VSM approach, which reached 56% on the 374 SAT questions. Here we go beyond Turney (2005) by describing LRA in more detail, performing more extensive experiments, and analyzing the algorithm and related work in mor</context>
<context position="65443" citStr="Veale (2004)" startWordPosition="10445" endWordPosition="10446">mn 3). 6. Experiments with Word Analogy Questions This section presents various experiments with 374 multiple-choice SAT word analogy questions. 6.1 Baseline LRA System Table 12 shows the performance of the baseline LRA system on the 374 SAT questions, using the parameter settings and configuration described in Section 5. LRA correctly answered 210 of the 374 questions; 160 questions were answered incorrectly and 4 questions were skipped, because the stem pair and its alternates were represented by zero vectors. The performance of LRA is significantly better than the lexicon-based approach of Veale (2004) (see Section 3.1) and the best performance using attributional similarity (see Section 2.3), with 95% confidence, according to the Fisher Exact Test (Agresti 1990). As another point of reference, consider the simple strategy of always guessing the choice with the highest co-occurrence frequency. The idea here is that the words in the solution pair may occur together frequently, because there is presumably a clear and meaningful relation between the solution words, whereas the distractors may only occur together rarely because they have no meaningful relation. This strategy is signifcantly wor</context>
<context position="67468" citStr="Veale (2004)" startWordPosition="10768" endWordPosition="10769">781; see choice (b) in column 3). Stem: quart:volume Average Original Highest cosines cosines cosines 1 2 3 Choices: day:night 0.374 0.327 0.443 mile:distance 0.677 0.525 0.781 decade:century 0.389 0.327 0.470 friction:heat 0.428 0.336 0.552 part:whole 0.370 0.330 0.408 Solution: (b) mile:distance 0.677 0.525 0.781 Gap: (b)−(d) 0.249 0.189 0.229 400 Turney Similarity of Semantic Relations Table 12 Performance of LRA on the 374 SAT questions. Precision, recall, and F are reported as percentages. (The bottom five rows are included for comparison.) Algorithm Precision Recall F LRA 56.8 56.1 56.5 Veale (2004) 42.8 42.8 42.8 Best attributional similarity 35.0 35.0 35.0 Random guessing 20.0 20.0 20.0 Lowest co-occurrence frequency 16.8 16.8 16.8 Highest co-occurrence frequency 11.8 11.8 11.8 With 374 questions and six word pairs per question (one stem and five choices), there are 2,244 pairs in the input set. In step 2, introducing alternate pairs multiplies the number of pairs by four, resulting in 8,976 pairs. In step 5, for each pair A:B, we add B:A, yielding 17,952 pairs. However, some pairs are dropped because they correspond to zero vectors (they do not appear together in a window of five word</context>
<context position="99140" citStr="Veale (2004)" startWordPosition="16157" endWordPosition="16158">1998; Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002). It seems likely that some schemes are easier for machine learning than others. For some applications, 30 classes may not be necessary; the 5 class scheme may be sufficient. LRA, like VSM, is a corpus-based approach to measuring relational similarity. Past work suggests that a hybrid approach, combining multiple modules, some corpusbased, some lexicon-based, will surpass any purebred approach (Turney et al. 2003). In future work, it would be natural to combine the corpus-based approach of LRA with the lexicon-based approach of Veale (2004), perhaps using the combination method of Turney et al. (2003). SVD is only one of many methods for handling sparse, noisy data. We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Components Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and Iterative Scaling (IS) (Ando 2000). We had some interesting results with small matrices (around 2,000 rows by 1,000 columns), but none of these methods seemed substantially better than SVD and none of them scaled up to the matrix s</context>
</contexts>
<marker>Veale, 2004</marker>
<rawString>Veale, Tony. 2004. WordNet sits the SAT: A knowledge-based approach to lexical analogy. In Proceedings of the 16th European Conference on Artificial Intelligence (ECAI2004), pages 606–612, Valencia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
</authors>
<title>Counter-training in discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>343--350</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="86481" citStr="Yangarber (2003)" startWordPosition="14019" endWordPosition="14020">arst 1992; Berland and Charniak 1999); instead, they are used to gather frequency data for building vectors that represent the relation between a given pair of words. The results in Section 6.8 show that a vector contains more information than any single pattern or small set of patterns; a vector is a distributed representation. LRA is distinct from Hearst (1992) and Berland and Charniak (1999) in its focus on distributed representations, which it shares with Turney and Littman (2005), but LRA goes beyond Turney and Littman (2005) by finding patterns automatically. Riloff and Jones (1999) and Yangarber (2003) also find patterns automatically, but their goal is to mine text for instances of word pairs; the same goal as Hearst (1992) and Table 18 Performance as a function of N. N Correct Incorrect Skipped Precision Recall F 1 114 179 81 38.9 30.5 34.2 3 146 206 22 41.5 39.0 40.2 10 167 201 6 45.4 44.7 45.0 30 174 196 4 47.0 46.5 46.8 100 178 192 4 48.1 47.6 47.8 300 192 178 4 51.9 51.3 51.6 1000 198 172 4 53.5 52.9 53.2 3000 207 163 4 55.9 55.3 55.6 407 Computational Linguistics Volume 32, Number 3 Berland and Charniak (1999). Because LRA uses patterns to build distributed vector representations, it</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>Yangarber, Roman. 2003. Counter-training in discovery of semantic patterns. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 343–350, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>266--271</pages>
<location>Princeton, NJ.</location>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, David. 1993. One sense per collocation. In Proceedings of the ARPA Human Language Technology Workshop, pages 266–271, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Zelenko, Dmitry, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>