<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000247">
<title confidence="0.9987975">
Learning Phrase-Based Spelling Error Models
from Clickthrough Data
</title>
<author confidence="0.991824">
Xu Sun∗
</author>
<affiliation confidence="0.9987735">
Dept. of Mathematical Informatics
University of Tokyo, Tokyo, Japan
</affiliation>
<email confidence="0.990786">
xusun@mist.i.u-tokyo.ac.jp
</email>
<author confidence="0.994862">
Daniel Micol
</author>
<affiliation confidence="0.8275385">
Microsoft Corporation
Munich, Germany
</affiliation>
<email confidence="0.977559">
danielmi@microsoft.com
</email>
<sectionHeader confidence="0.996467" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993464166666667">
This paper explores the use of clickthrough data
for query spelling correction. First, large amounts
of query-correction pairs are derived by analyzing
users&apos; query reformulation behavior encoded in
the clickthrough data. Then, a phrase-based error
model that accounts for the transformation
probability between multi-term phrases is trained
and integrated into a query speller system. Expe-
riments are carried out on a human-labeled data
set. Results show that the system using the
phrase-based error model outperforms signifi-
cantly its baseline systems.
</bodyText>
<sectionHeader confidence="0.998762" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999638235294118">
Search queries present a particular challenge for
traditional spelling correction methods for three
main reasons (Ahmad and Kondrak, 2004). First,
spelling errors are more common in search queries
than in regular written text: roughly 10-15% of
queries contain misspelled terms (Cucerzan and
Brill, 2004). Second, most search queries consist
of a few key words rather than grammatical sen-
tences, making a grammar-based approach inap-
propriate. Most importantly, many queries con-
tain search terms, such as proper nouns and names,
which are not well established in the language.
For example, Chen et al. (2007) reported that
16.5% of valid search terms do not occur in their
200K-entry spelling lexicon.
Therefore, recent research has focused on the
use of Web corpora and query logs, rather than
</bodyText>
<subsectionHeader confidence="0.2471635">
Jianfeng Gao
Microsoft Research
</subsectionHeader>
<address confidence="0.905406">
Redmond, WA, USA
</address>
<email confidence="0.975933">
jfgao@microsoft.com
</email>
<author confidence="0.724464">
Chris Quirk
</author>
<affiliation confidence="0.618361">
Microsoft Research
</affiliation>
<address confidence="0.89935">
Redmond, WA, USA
</address>
<email confidence="0.951699">
chrisq@microsoft.com
</email>
<bodyText confidence="0.992032571428571">
human-compiled lexicons, to infer knowledge
about misspellings and word usage in search
queries (e.g., Whitelaw et al., 2009). Another
important data source that would be useful for this
purpose is clickthrough data. Although it is
well-known that clickthrough data contain rich
information about users&apos; search behavior, e.g.,
how a user (re-) formulates a query in order to
find the relevant document, there has been little
research on exploiting the data for the develop-
ment of a query speller system.
In this paper we present a novel method of
extracting large amounts of query-correction pairs
from the clickthrough data. These pairs, impli-
citly judged by millions of users, are used to train
a set of spelling error models. Among these
models, the most effective one is a phrase-based
error model that captures the probability of
transforming one multi-term phrase into another
multi-term phrase. Comparing to traditional error
models that account for transformation probabili-
ties between single characters (Kernighan et al.,
1990) or sub-word strings (Brill and Moore,
2000), the phrase-based model is more powerful
in that it captures some contextual information by
retaining inter-term dependencies. We show that
this information is crucial to detect the correction
of a query term, because unlike in regular written
text, any query word can be a valid search term
and in many cases the only way for a speller
system to make the judgment is to explore its
usage according to the contextual information.
We conduct a set of experiments on a large
data set, consisting of human-labeled
∗ The work was done when Xu Sun was visiting Microsoft Research Redmond.
</bodyText>
<page confidence="0.968221">
266
</page>
<note confidence="0.943652">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 266–274,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999807235294118">
query-correction pairs. Results show that the error
models learned from clickthrough data lead to
significant improvements on the task of query
spelling correction. In particular, the speller sys-
tem incorporating a phrase-based error model
significantly outperforms its baseline systems.
To the best of our knowledge, this is the first
extensive study of learning phase-based error
models from clickthrough data for query spelling
correction. The rest of the paper is structured as
follows. Section 2 reviews related work. Section 3
presents the way query-correction pairs are ex-
tracted from the clickthrough data. Section 4
presents the baseline speller system used in this
study. Section 5 describes in detail the phrase-
based error model. Section 6 presents the expe-
riments. Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997008433735">
Spelling correction for regular written text is a
long standing research topic. Previous researches
can be roughly grouped into two categories:
correcting non-word errors and real-word errors.
In non-word error spelling correction, any
word that is not found in a pre-compiled lexicon is
considered to be misspelled. Then, a list of lexical
words that are similar to the misspelled word are
proposed as candidate spelling corrections. Most
traditional systems use a manually tuned similar-
ity function (e.g., edit distance function) to rank
the candidates, as reviewed by Kukich (1992).
During the last two decades, statistical error
models learned on training data (i.e.,
query-correction pairs) have become increasingly
popular, and have proven more effective (Ker-
nighan et al., 1990; Brill and Moore, 2000; Tou-
tanova and Moore, 2002; Okazaki et al., 2008).
Real-word spelling correction is also referred
to as context sensitive spelling correction (CSSC).
It tries to detect incorrect usages of a valid word
based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot;
in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in
CSSC is as follows. First, a pre-defined confusion
set is used to generate candidate corrections, then
a scoring model, such as a trigram language
model or naïve Bayes classifier, is used to rank the
candidates according to their context (e.g.,
Golding and Roth, 1996; Mangu and Brill, 1997;
Church et al., 2007).
When designed to handle regular written text,
both CSSC and non-word error speller systems
rely on a pre-defined vocabulary (i.e., either a
lexicon or a confusion set). However, in query
spelling correction, it is impossible to compile
such a vocabulary, and the boundary between the
non-word and real-word errors is quite vague.
Therefore, recent research on query spelling
correction has focused on exploiting noisy Web
data and query logs to infer knowledge about
misspellings and word usage in search queries.
Cucerzan and Brill (2004) discuss in detail the
challenges of query spelling correction, and
suggest the use of query logs. Ahmad and Kon-
drak (2005) propose a method of estimating an
error model from query logs using the EM algo-
rithm. Li et al. (2006) extend the error model by
capturing word-level similarities learned from
query logs. Chen et al. (2007) suggest using web
search results to improve spelling correction.
Whitelaw et al. (2009) present a query speller
system in which both the error model and the
language model are trained using Web data.
Compared to Web corpora and query logs,
clickthrough data contain much richer informa-
tion about users’ search behavior. Although there
has been a lot of research on using clickthrough
data to improve Web document retrieval (e.g.,
Joachims, 2002; Agichtein et al., 2006; Gao et al.,
2009), the data have not been fully explored for
query spelling correction. This study tries to learn
error models from clickthrough data. To our
knowledge, this is the first such attempt using
clickthrough data.
Most of the speller systems reviewed above are
based on the framework of the source channel
model. Typically, a language model (source
model) is used to capture contextual information,
while an error model (channel model) is consi-
dered to be context free in that it does not take into
account any contextual information in modeling
word transformation probabilities. In this study
we argue that it is beneficial to capture contextual
information in the error model. To this end, in-
spired by the phrase-based statistical machine
translation (SMT) systems (Koehn et al., 2003;
Och and Ney, 2004), we propose a phrase-based
error model where we assume that query spelling
correction is performed at the phrase level.
In what follows, before presenting the phrase-
based error model, we will first describe the
clickthrough data and the query speller system we
used in this study.
</bodyText>
<sectionHeader confidence="0.9612655" genericHeader="method">
3 Clickthrough Data and Spelling Cor-
rection
</sectionHeader>
<bodyText confidence="0.999655">
This section describes the way the
query-correction pairs are extracted from click-
</bodyText>
<page confidence="0.993421">
267
</page>
<bodyText confidence="0.999975436363637">
through data. Two types of clickthrough data are
explored in our experiment.
The clickthrough data of the first type has been
widely used in previous research and proved to be
useful for Web search (Joachims, 2002; Agichtein
et al., 2006; Gao et al., 2009) and query refor-
mulation (Wang and Zhai, 2008; Suzuki et al.,
2009). We start with this same data with the hope
of achieving similar improvements in our task.
The data consist of a set of query sessions that
were extracted from one year of log files from a
commercial Web search engine. A query session
contains a query issued by a user and a ranked list
of links (i.e., URLs) returned to that same user
along with records of which URLs were clicked.
Following Suzuki et al. (2009), we extract
query-correction pairs as follows. First, we extract
pairs of queries Q, and Q2 such that (1) they are
issued by the same user; (2) Q2 was issued within
3 minutes of Q,; and (3) Q2 contained at least one
clicked URL in the result page while Q, did not
result in any clicks. We then scored each query
pair (Q,, Q2) using the edit distance between Q,
and Q2, and retained those with an edit distance
score lower than a pre-set threshold as query
correction pairs.
Unfortunately, we found in our experiments
that the pairs extracted using the method are too
noisy for reliable error model training, even with a
very tight threshold, and we did not see any sig-
nificant improvement. Therefore, in Section 6 we
will not report results using this dataset.
The clickthrough data of the second type con-
sists of a set of query reformulation sessions
extracted from 3 months of log files from a
commercial Web browser. A query reformulation
session contains a list of URLs that record user
behaviors that relate to the query reformulation
functions, provided by a Web search engine. For
example, almost all commercial search engines
offer the &amp;quot;did you mean&amp;quot; function, suggesting a
possible alternate interpretation or spelling of a
user-issued query. Figure 1 shows a sample of the
query reformulation sessions that record the &amp;quot;did
you mean&amp;quot; sessions from three of the most pop-
ular search engines. These sessions encode the
same user behavior: A user first queries for
&amp;quot;harrypotter sheme park&amp;quot;, and then clicks on the
resulting spelling suggestion &amp;quot;harry potter theme
park&amp;quot;. In our experiments, we &amp;quot;reverse-engineer&amp;quot;
the parameters from the URLs of these sessions,
and deduce how each search engine encodes both
a query and the fact that a user arrived at a URL
by clicking on the spelling suggestion of the query
– an important indication that the spelling sug-
</bodyText>
<figure confidence="0.995511625">
Google:
http://www.google.com/search?
hl=en&amp;source=hp&amp;
q=harrypotter+sheme+park&amp;aq=f&amp;oq=&amp;aqi=
http://www.google.com/search?
hl=en&amp;ei=rnNAS8-oKsWe_AaB2eHlCA&amp;
sa=X&amp;oi=spell&amp;resnum=0&amp;ct=
result&amp;cd=1&amp;ved=0CA4QBSgA&amp;
q=harry+potter+theme+park&amp;spell=1
Yahoo:
http://search.yahoo.com/search;
_ylt=A0geu6ywckBL_XIBSDtXNyoA?e
p=harrypotter+shem+park&amp;
fr2=sb-top&amp;fr=yfp-t-701&amp;sao=1
http://search.yahoo.com/search?
ei=UTF-8&amp;fr=yfp-t-701&amp;
p=harry+potter+theme+park
&amp;SpellState=n-2672070758_q-tsI55N6srhZa.
qORA0MuawAAAA%40%40&amp;fr2=sp-top
Bing:
http://www.bing.com/search?
q=harrypotter+sheme+park&amp;form=QBRE&amp;qs=n
http://www.bing.com/search?
q=harry+potter+theme+park&amp;FORM=SSRE
</figure>
<figureCaption confidence="0.897705">
Figure 1. A sample of query reformulation sessions
from three popular search engines. These sessions
</figureCaption>
<bodyText confidence="0.995141777777778">
show that a user first issues the query &amp;quot;harrypotter
sheme park&amp;quot;, and then clicks on the resulting spell
suggestion &amp;quot;harry potter theme park&amp;quot;.
gestion is desired. From these three months of
query reformulation sessions, we extracted about
3 million query-correction pairs. Compared to the
pairs extracted from the clickthrough data of the
first type (query sessions), this data set is much
cleaner because all these spelling corrections are
actually clicked, and thus judged implicitly, by
many users.
In addition to the &amp;quot;did you mean&amp;quot; function,
recently some search engines have introduced two
new spelling suggestion functions. One is the
&amp;quot;auto-correction&amp;quot; function, where the search
engine is confident enough to automatically apply
the spelling correction to the query and execute it
to produce search results for the user. The other is
the &amp;quot;split pane&amp;quot; result page, where one half por-
tion of the search results are produced using the
original query, while the other half, usually vi-
sually separate portion of results are produced
using the auto-corrected query.
In neither of these functions does the user ever
receive an opportunity to approve or disapprove
of the correction. Since our extraction approach
focuses on user-approved spelling suggestions,
</bodyText>
<page confidence="0.984769">
268
</page>
<bodyText confidence="0.999972848484848">
we ignore the query reformulation sessions re-
cording either of the two functions. Although by
doing so we could miss some basic, obvious
spelling corrections, our experiments show that
the negative impact on error model training is
negligible. One possible reason is that our base-
line system, which does not use any error model
learned from the clickthrough data, is already able
to correct these basic, obvious spelling mistakes.
Thus, including these data for training is unlikely
to bring any further improvement.
We found that the error models trained using
the data directly extracted from the query refor-
mulation sessions suffer from the problem of
underestimating the self-transformation probabil-
ity of a query P(Q2=Q,|Q,), because we only
included in the training data the pairs where the
query is different from the correction. To deal
with this problem, we augmented the training data
by including correctly spelled queries, i.e., the
pairs (Q,, Q2) where Q, = Q2. First, we extracted a
set of queries from the sessions where no spell
suggestion is presented or clicked on. Second, we
removed from the set those queries that were
recognized as being auto-corrected by a search
engine. We do so by running a sanity check of the
queries against our baseline spelling correction
system, which will be described in Section 6. If
the system thinks an input query is misspelled, we
assumed it was an obvious misspelling, and re-
moved it. The remaining queries were assumed to
be correctly spelled and were added to the training
data.
</bodyText>
<sectionHeader confidence="0.958945" genericHeader="method">
4 The Baseline Speller System
</sectionHeader>
<bodyText confidence="0.966977666666667">
The spelling correction problem is typically
formulated under the framework of the source
channel model. Given an input query Q =
ql... qj, we want to find the best spelling correc-
tion C = cl... cj among all candidate spelling
corrections:
</bodyText>
<equation confidence="0.9917235">
C* = argmax P(C|Q) (1)
C
</equation>
<bodyText confidence="0.9992055">
Applying Bayes&apos; Rule and dropping the constant
denominator, we have
</bodyText>
<equation confidence="0.984168">
P(Q|C)P(C) (2)
</equation>
<bodyText confidence="0.999945942857143">
where the error model P(Q|C) models the trans-
formation probability from C to Q, and the lan-
guage model P(C) models how likely C is a
correctly spelled query.
The speller system used in our experiments is
based on a ranking model (or ranker), which can
be viewed as a generalization of the source
channel model. The system consists of two
components: (1) a candidate generator, and (2) a
ranker.
In candidate generation, an input query is first
tokenized into a sequence of terms. Then we scan
the query from left to right, and each query term q
is looked up in lexicon to generate a list of spel-
ling suggestions c whose edit distance from q is
lower than a preset threshold. The lexicon we
used contains around 430,000 entries; these are
high frequency query terms collected from one
year of search query logs. The lexicon is stored
using a trie-based data structure that allows effi-
cient search for all terms within a maximum edit
distance.
The set of all the generated spelling sugges-
tions is stored using a lattice data structure, which
is a compact representation of exponentially many
possible candidate spelling corrections. We then
use a decoder to identify the top twenty candi-
dates from the lattice according to the source
channel model of Equation (2). The language
model (the second factor) is a backoff bigram
model trained on the tokenized form of one year
of query logs, using maximum likelihood estima-
tion with absolute discounting smoothing. The
error model (the first factor) is approximated by
the edit distance function as
</bodyText>
<equation confidence="0.852808">
—logP(Q|C) a EditDist(Q, C) (3)
</equation>
<bodyText confidence="0.999516904761905">
The decoder uses a standard two-pass algorithm
to generate 20-best candidates. The first pass uses
the Viterbi algorithm to find the best C according
to the model of Equations (2) and (3). In the
second pass, the A-Star algorithm is used to find
the 20-best corrections, using the Viterbi scores
computed at each state in the first pass as heuris-
tics. Notice that we always include the input query
Q in the 20-best candidate list.
The core of the second component of the
speller system is a ranker, which re-ranks the
20-best candidate spelling corrections. If the top
C after re-ranking is different than the original
query Q, the system returns C as the correction.
Let f be a feature vector extracted from a query
and candidate spelling correction pair (Q, C). The
ranker maps f to a real value y that indicates how
likely C is a desired correction of Q. For example,
a linear ranker simply maps f to y with a learned
weight vector w such as y = w · f, where w is
optimized w.r.t. accuracy on a set of hu-
</bodyText>
<equation confidence="0.916584">
C* = argmax
C
</equation>
<page confidence="0.987583">
269
</page>
<table confidence="0.9117544">
C: “disney theme park” correct query
S: [“disney”, “theme park”] segmentation
T: [“disnee”, “theme part”] translation
M: (1 4 2, 24 1) permutation
Q: “theme part disnee” misspelled query
</table>
<figureCaption confidence="0.9876125">
Figure 2: Example demonstrating the generative
procedure behind the phrase-based error model.
</figureCaption>
<bodyText confidence="0.999691727272727">
man-labeled (Q, C) pairs. The features in f are
arbitrary functions that map (Q, C) to a real value.
Since we define the logarithm of the probabilities
of the language model and the error model (i.e.,
the edit distance function) as features, the ranker
can be viewed as a more general framework,
subsuming the source channel model as a special
case. In our experiments we used 96 features and a
non-linear model, implemented as a two-layer
neural net, though the details of the ranker and the
features are beyond the scope of this paper.
</bodyText>
<sectionHeader confidence="0.99295" genericHeader="method">
5 A Phrase-Based Error Model
</sectionHeader>
<bodyText confidence="0.999915607142857">
The goal of the phrase-based error model is to
transform a correctly spelled query C into a
misspelled query Q. Rather than replacing single
words in isolation, this model replaces sequences
of words with sequences of words, thus incorpo-
rating contextual information. For instance, we
might learn that “theme part” can be replaced by
“theme park” with relatively high probability,
even though “part” is not a misspelled word. We
assume the following generative story: first the
correctly spelled query C is broken into K
non-empty word sequences c1, ..., ck, then each is
replaced with a new non-empty word sequence q1,
..., qk, and finally these phrases are permuted and
concatenated to form the misspelled Q. Here, c
and q denote consecutive sequences of words.
To formalize this generative process, let S
denote the segmentation of C into K phrases c1...cK,
and let T denote the K replacement phrases
q1...qK – we refer to these (ci, qi) pairs as
bi-phrases. Finally, let M denote a permutation of
K elements representing the final reordering step.
Figure 2 demonstrates the generative procedure.
Next let us place a probability distribution over
rewrite pairs. Let B(C, Q) denote the set of S, T, M
triples that transform C into Q. If we assume a
uniform probability over segmentations, then the
phrase-based probability can be defined as:
</bodyText>
<equation confidence="0.988824">
ܲሺܳ|ܥሻ ן ෍ ܲሺܶ|ܥ, ܵሻ ڄ ܲሺܯ|ܥ, ܵ, ܶሻ (4)
ሺௌ,்,ெሻא
஻ሺ஼,ொሻ
</equation>
<bodyText confidence="0.9947215">
As is common practice in SMT, we use the
maximum approximation to the sum:
</bodyText>
<equation confidence="0.971782333333333">
ܲሺܳ|ܥሻ ൎ max
ሺௌ,்,ெሻא ܲሺܶ|ܥ,ܵሻ ڄ ܲሺܯ|ܥ,ܵ,ܶሻ (5)
஻ሺ஼,ொሻ
</equation>
<subsectionHeader confidence="0.941357">
5.1 Forced Alignments
</subsectionHeader>
<bodyText confidence="0.999984866666667">
Although we have defined a generative model for
transforming queries, our goal is not to propose
new queries, but rather to provide scores over
existing Q and C pairs which act as features for
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be iden-
tified with little ambiguity. Thus we restrict our
attention to those phrase transformations consis-
tent with a good word-level alignment.
Let J be the length of Q, L be the length of C,
and A = a1, ..., aJ be a hidden variable
representing the word alignment. Each ai takes on
a value ranging from 1 to L indicating its corres-
ponding word position in C, or 0 if the ith word in
Q is unaligned. The cost of assigning k to ai is
equal to the Levenshtein edit distance (Levensh-
tein, 1966) between the ith word in Q and the kth
word in C, and the cost of assigning 0 to ai is equal
to the length of the ith word in Q. We can deter-
mine the least cost alignment A* between Q and C
efficiently using the A-star algorithm.
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples
that are consistent with the word alignment, which
we denote as B(C, Q, A*). Here, consistency re-
quires that if two words are aligned in A*, then
they must appear in the same bi-phrase (ci, qi).
Once the word alignment is fixed, the final per-
mutation is uniquely determined, so we can safely
discard that factor. Thus we have:
</bodyText>
<equation confidence="0.979173">
ܲሺܳ|ܥሻ ൎ max
ሺௌ,்,ெሻא ܲሺܶ|ܥ, ܵሻ (6)
ࣜሺ஼,ொ,஺څሻ
</equation>
<bodyText confidence="0.99948475">
For the sole remaining factor P(T|C, S), we
make the assumption that a segmented query T =
q1... qK is generated from left to right by trans-
forming each phrase c1...cK independently:
</bodyText>
<page confidence="0.982835">
270
</page>
<bodyText confidence="0.922261">
Input: biPhraseLattice “PL” with length = K &amp; height
</bodyText>
<equation confidence="0.994385894736842">
= L;
Initialization: biPhrase.maxProb = 0;
for (x = 0; x &lt;= K – 1; x++)
for (y = 1; y &lt;= L; y++)
for (yPre = 1; yPre &lt;= L; yPre++)
{
xPre = x – y;
biPhrasePre = PL.get(xPre, yPre);
biPhrase = PL.get(x, y);
if (!biPhrasePre  ||!biPhrase)
continue;
probIncrs = PL.getProbIncrease(biPhrasePre,
biPhrase);
maxProbPre = biPhrasePre.maxProb;
totalProb = probIncrs + maxProbPre;
if (totalProb &gt; biPhrase.maxProb)
{
biPhrase.maxProb = totalProb;
biPhrase.yPre = yPre;
</equation>
<figure confidence="0.8346236">
}
}
Result: record at each bi-phrase boundary its maxi-
mum probability (biPhrase.maxProb) and optimal
back-tracking biPhrases (biPhrase.yPre).
</figure>
<figureCaption confidence="0.9593495">
Figure 3: The dynamic programming algorithm for
Viterbi bi-phrase segmentation.
</figureCaption>
<equation confidence="0.941391">
P(T|C. S) _ fIk=1 P (qk  |ck)�, (7)
</equation>
<bodyText confidence="0.997940733333333">
where P(qk|ck) is a phrase transformation
probability, the estimation of which will be de-
scribed in Section 5.2.
To find the maximum probability assignment
efficiently, we can use a dynamic programming
approach, somewhat similar to the monotone
decoding algorithm described in Och (2002).
Here, though, both the input and the output word
sequences are specified as the input to the algo-
rithm, as is the word alignment. We define the
quantity aj to be the probability of the most likely
sequence of bi-phrases that produce the first j
terms of Q and are consistent with the word
alignment and C. It can be calculated using the
following algorithm:
</bodyText>
<listItem confidence="0.913277">
1. Initialization:
</listItem>
<equation confidence="0.9324315">
ao = 1 (8)
2. Induction:
aj = max {aj′P(glcq)l (9)
j′&lt;j.9=q,′�� ...ql
3. Total:
P(Q|C) = aj (10)
</equation>
<figureCaption confidence="0.997968">
Figure 4: Toy example of (left) a word alignment
between two strings &amp;quot;adcf&amp;quot; and &amp;quot;ABCDEF&amp;quot;; and (right)
the bi-phrases containing up to four words that are
consistent with the word alignment.
</figureCaption>
<bodyText confidence="0.999978">
The pseudo-code of the above algorithm is
shown in Figure 3. After generating Q from left to
right according to Equations (8) to (10), we record
at each possible bi-phrase boundary its maximum
probability, and we obtain the total probability at
the end-position of Q. Then, by back-tracking the
most probable bi-phrase boundaries, we obtain B*.
The algorithm takes a complexity of O(KL2),
where K is the total number of word alignments in
A* which does not contain empty words, and L is
the maximum length of a bi-phrase, which is a
hyper-parameter of the algorithm. Notice that
when we set L=1, the phrase-based error model is
reduced to a word-based error model which as-
sumes that words are transformed independently
from C to Q, without taking into account any
contextual information.
</bodyText>
<subsectionHeader confidence="0.998876">
5.2 Model Estimation
</subsectionHeader>
<bodyText confidence="0.999734761904762">
We follow a method commonly used in SMT
(Koehn et al., 2003) to extract bi-phrases and
estimate their replacement probabilities. From
each query-correction pair with its word align-
ment (Q, C, A*), all bi-phrases consistent with the
word alignment are identified. Consistency here
implies two things. First, there must be at least
one aligned word pair in the bi-phrase. Second,
there must not be any word alignments from
words inside the bi-phrase to words outside the
bi-phrase. That is, we do not extract a phrase pair
if there is an alignment from within the phrase
pair to outside the phrase pair. The toy example
shown in Figure 4 illustrates the bilingual phrases
we can generate by this process.
After gathering all such bi-phrases from the
full training data, we can estimate conditional
relative frequency estimates without smoothing.
For example, the phrase transformation probabil-
ity P(g|c) in Equation (7) can be estimated ap-
proximately as
</bodyText>
<figure confidence="0.999618652173913">
A B
A
E F a
C D
a
d
c
c
C
F
f
#
#
adc
d
dc
#
ABCD
D
CD
# dcf
CDEF
f
</figure>
<page confidence="0.919245">
271
</page>
<equation confidence="0.6004105">
N(c, q) (11)
Zy, N(c, g&apos;)
</equation>
<bodyText confidence="0.9981438">
where N(c, q) is the number of times that c is
aligned to q in training data. These estimates are
useful for contextual lexical selection with suffi-
cient training data, but can be subject to data
sparsity issues.
An alternate translation probability estimate
not subject to data sparsity issues is the so-called
lexical weight estimate (Koehn et al., 2003).
Assume we have a word translation distribution
t(q|c) (defined over individual words, not
phrases), and a word alignment A between q and c;
here, the word alignment contains (i, j) pairs,
where i E L. |q |and j E 0.. |c|, with 0 indicat-
ing an inserted word. Then we can use the fol-
lowing estimate:
</bodyText>
<equation confidence="0.999709666666667">
Pw(g|c,A) _ F1 1
|{I|(1, ) E A} |1 t(gi|cj)
i=1 V(Q)EA
</equation>
<bodyText confidence="0.999992">
We assume that for every position in q, there is
either a single alignment to 0, or multiple align-
ments to non-zero positions in c. In effect, this
computes a product of per-word translation scores;
the per-word scores are averages of all the trans-
lations for the alignment links of that word. We
estimate the word translation probabilities using
counts from the word aligned corpus: t(g|c) _
</bodyText>
<equation confidence="0.768032">
N(c,q)Here N(c, is the number of times that
Zq′N(c,q). �l)
</equation>
<bodyText confidence="0.999917">
the words (not phrases as in Equation 11) c and q
are aligned in the training data. These word based
scores of bi-phrases, though not as effective in
contextual selection, are more robust to noise and
sparsity.
Throughout this section, we have approached
this model in a noisy channel approach, finding
probabilities of the misspelled query given the
corrected query. However, the method can be run
in both directions, and in practice SMT systems
benefit from also including the direct probability
of the corrected query given this misspelled query
(Och, 2002).
</bodyText>
<subsectionHeader confidence="0.995098">
5.3 Phrase-Based Error Model Features
</subsectionHeader>
<bodyText confidence="0.99991">
To use the phrase-based error model for spelling
correction, we derive five features and integrate
them into the ranker-based query speller system,
described in Section 4. These features are as
follows.
</bodyText>
<listItem confidence="0.771292">
• Two phrase transformation features:
</listItem>
<bodyText confidence="0.994304529411765">
These are the phrase transformation scores
based on relative frequency estimates in two
directions. In the correction-to-query direc-
tion, we define the feature as fPT(Q, C, A) _
log P(Q|C), where P(Q|C) is computed by
Equations (8) to (10), and P(gICJ is the rel-
ative frequency estimate of Equation (11).
• Two lexical weight features: These are the
phrase transformation scores based on the
lexical weighting models in two directions.
For example, in the correction-to-query di-
rection, we define the feature
as fiw(Q, C, A) _ log P(Q|C), where P(Q|C)
is computed by Equations (8) to (10), and the
phrase transformation probability is the
computed as lexical weight according to Eq-
uation (12).
</bodyText>
<listItem confidence="0.8312515">
• Unaligned word penalty feature: the feature
is defined as the ratio between the number of
unaligned query words and the total number
of query words.
</listItem>
<sectionHeader confidence="0.999505" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999820625">
We evaluate the spelling error models on a large
scale real world data set containing 24,172 queries
sampled from one year’s worth of query logs from
a commercial search engine. The spelling of each
query is judged and corrected by four annotators.
We divided the data set into training and test
data sets. The two data sets do not overlap. The
training data contains 8,515 query-correction
pairs, among which 1,743 queries are misspelled
(i.e., in these pairs, the corrections are different
from the queries). The test data contains 15,657
query-correction pairs, among which 2,960 que-
ries are misspelled. The average length of queries
in the training and test data is 2.7 words.
The speller systems we developed in this study
are evaluated using the following three metrics.
</bodyText>
<listItem confidence="0.962208090909091">
• Accuracy: The number of correct outputs
generated by the system divided by the total
number of queries in the test set.
• Precision: The number of correct spelling
corrections for misspelled queries generated
by the system divided by the total number of
corrections generated by the system.
• Recall: The number of correct spelling cor-
rections for misspelled queries generated by
the system divided by the total number of
misspelled queries in the test set.
</listItem>
<bodyText confidence="0.99986275">
We also perform a significance test, i.e., a t-test
with a significance level of 0.05. A significant
difference should be read as significant at the 95%
level.
</bodyText>
<equation confidence="0.901035333333333">
P(g|c) _
|q|
(12)
</equation>
<page confidence="0.989472">
272
</page>
<table confidence="0.9999136">
# System Accuracy Precision Recall
1 Source-channel 0.8526 0.7213 0.3586
2 Ranker-based 0.8904 0.7414 0.4964
3 Word model 0.8994 0.7709 0.5413
4 Phrase model (L=3) 0.9043 0.7814 0.5732
</table>
<tableCaption confidence="0.998562">
Table 1. Summary of spelling correction results.
</tableCaption>
<table confidence="0.999946666666667">
# System Accuracy Precision Recall
5 Phrase model (L=1) 0.8994 0.7709 0.5413
6 Phrase model (L=2) 0.9014 0.7795 0.5605
7 Phrase model (L=3) 0.9043 0.7814 0.5732
8 Phrase model (L=5) 0.9035 0.7834 0.5698
9 Phrase model (L=8) 0.9033 0.7821 0.5713
</table>
<tableCaption confidence="0.94786">
Table 2. Variations of spelling performance as a func-
tion of phrase length.
</tableCaption>
<table confidence="0.9999546">
# System Accuracy Precision Recall
10 L=3; 0 month data 0.8904 0.7414 0.4964
11 L=3; 0.5 month data 0.8959 0.7701 0.5234
12 L=3; 1.5 month data 0.9023 0.7787 0.5667
13 L=3; 3 month data 0.9043 0.7814 0.5732
</table>
<tableCaption confidence="0.9749365">
Table 3. Variations of spelling performance as a func-
tion of the size of clickthrough data used for training.
</tableCaption>
<bodyText confidence="0.999971294117647">
In our experiments, all the speller systems are
ranker-based. In most cases, other than the base-
line system (a linear neural net), the ranker is a
two-layer neural net with 5 hidden nodes. The free
parameters of the neural net are trained to optim-
ize accuracy on the training data using the back
propagation algorithm, running for 500 iterations
with a very small learning rate (0.1) to avoid
overfitting. We did not adjust the neural net
structure (e.g., the number of hidden nodes) or
any training parameters for different speller sys-
tems. Neither did we try to seek the best tradeoff
between precision and recall. Since all the sys-
tems are optimized for accuracy, we use accuracy
as the primary metric for comparison.
Table 1 summarizes the main spelling correc-
tion results. Row 1 is the baseline speller system
where the source-channel model of Equations (2)
and (3) is used. In our implementation, we use a
linear ranker with only two features, derived
respectively from the language model and the
error model models. The error model is based on
the edit distance function. Row 2 is the rank-
er-based spelling system that uses all 96 ranking
features, as described in Section 4. Note that the
system uses the features derived from two error
models. One is the edit distance model used for
candidate generation. The other is a phonetic
model that measures the edit distance between the
metaphones (Philips, 1990) of a query word and
its aligned correction word. Row 3 is the same
system as Row 2, with an additional set of features
derived from a word-based error model. This
model is a special case of the phrase-based error
model described in Section 5 with the maximum
phrase length set to one. Row 4 is the system that
uses the additional 5 features derived from the
phrase-based error models with a maximum
bi-phrase length of 3.
In phrase based error model, L is the maxi-
mum length of a bi-phrase (Figure 3). This value
is important for the spelling performance. We
perform experiments to study the impact of L;
the results are displayed in Table 2. Moreover,
since we proposed to use clickthrough data for
spelling correction, it is interesting to study the
impact on spelling performance from the size of
clickthrough data used for training. We varied
the size of clickthrough data and the experi-
mental results are presented in Table 3.
The results show first and foremost that the
ranker-based system significantly outperforms
the spelling system based solely on the
source-channel model, largely due to the richer
set of features used (Row 1 vs. Row 2). Second,
the error model learned from clickthrough data
leads to significant improvements (Rows 3 and 4
vs. Row 2). The phrase-based error model, due to
its capability of capturing contextual information,
outperforms the word-based model with a small
but statistically significant margin (Row 4 vs.
Row 3), though using phrases longer (L &gt; 3) does
not lead to further significant improvement (Rows
6 and 7 vs. Rows 8 and 9). Finally, using more
clickthrough data leads to significant improve-
ment (Row 13 vs. Rows 10 to 12). The benefit
does not appear to have peaked – further im-
provements are likely given a larger data set.
</bodyText>
<sectionHeader confidence="0.999347" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999548125">
Unlike conventional textual documents, most
search queries consist of a sequence of key words,
many of which are valid search terms but are not
stored in any compiled lexicon. This presents a
challenge to any speller system that is based on a
dictionary.
This paper extends the recent research on using
Web data and query logs for query spelling cor-
rection in two aspects. First, we show that a large
amount of training data (i.e. query-correction
pairs) can be extracted from clickthrough data,
focusing on query reformulation sessions. The
resulting data are very clean and effective for
error model training. Second, we argue that it is
critical to capture contextual information for
query spelling correction. To this end, we propose
</bodyText>
<page confidence="0.992481">
273
</page>
<bodyText confidence="0.9998353">
a new phrase-based error model, which leads to
significant improvement in our spelling correc-
tion experiments.
There is additional potentially useful informa-
tion that can be exploited in this type of model.
For example, in future work we plan to investigate
the combination of the clickthrough data collected
from a Web browser with the noisy but large
query sessions collected from a commercial
search engine.
</bodyText>
<sectionHeader confidence="0.998172" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997314666666667">
The authors would like to thank Andreas Bode,
Mei Li, Chenyu Yan and Galen Andrew for the
very helpful discussions and collaboration.
</bodyText>
<sectionHeader confidence="0.999074" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999811157142858">
Agichtein, E., Brill, E. and Dumais, S. 2006. Im-
proving web search ranking by incorporating user
behavior information. In SIGIR, pp. 19-26.
Ahmad, F., and Kondrak, G. 2005. Learning a
spelling error model from search query logs. In
HLT-EMNLP, pp 955-962.
Brill, E., and Moore, R. C. 2000. An improved error
model for noisy channel spelling correction. In
ACL, pp. 286-293.
Chen, Q., Li, M., and Zhou, M. 2007. Improving
query spelling correction using web search results.
In EMNLP-CoNLL, pp. 181-189.
Church, K., Hard, T., and Gao, J. 2007. Compress-
ing trigram language models with Golomb cod-
ing. In EMNLP-CoNLL, pp. 199-207.
Cucerzan, S., and Brill, E. 2004. Spelling correction
as an iterative process that exploits the collective
knowledge of web users. In EMNLP, pp. 293-300.
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y.
2009. Smoothing clickthrough data for web
search ranking. In SIGIR.
Golding, A. R., and Roth, D. 1996. Applying win-
now to context-sensitive spelling correction. In
ICML, pp. 182-190.
Joachims, T. 2002. Optimizing search engines using
clickthrough data. In SIGKDD, pp. 133-142.
Kernighan, M. D., Church, K. W., and Gale, W. A.
1990. A spelling correction program based on a
noisy channel model. In COLING, pp. 205-210.
Koehn, P., Och, F., and Marcu, D. 2003. Statistical
phrase-based translation. In HLT/NAACL, pp.
127-133.
Kukich, K. 1992. Techniques for automatically
correcting words in text. ACM Computing Sur-
veys. 24(4): 377-439.
Levenshtein, V. I. 1966. Binary codes capable of
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707-710.
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006.
Exploring distributional similarity based models
for query spelling correction. In ACL, pp.
1025-1032.
Mangu, L., and Brill, E. 1997. Automatic rule ac-
quisition for spelling correction. In ICML, pp.
187-194.
Och, F. 2002. Statistical machine translation: from
single-word models to alignment templates. PhD
thesis, RWTH Aachen.
Och, F., and Ney, H. 2004. The alignment template
approach to statistical machine translation.
Computational Linguistics, 30(4): 417-449.
Okazaki, N., Tsuruoka, Y., Ananiadou, S., and
Tsujii, J. 2008. A discriminative candidate gene-
rator for string transformations. In EMNLP, pp.
447-456.
Philips, L. 1990. Hanging on the metaphone.
Computer Language Magazine, 7(12):38-44.
Suzuki, H., Li, X., and Gao, J. 2009. Discovery of
term variation in Japanese web search queries. In
EMNLP.
Toutanova, K., and Moore, R. 2002. Pronunciation
modeling for improved spelling correction. In
ACL, pp. 144-151.
Wang, X., and Zhai, C. 2008. Mining term associa-
tion patterns from search logs for effective query
reformulation. In CIKM, pp. 479-488.
Whitelaw, C., Hutchinson, B., Chung, G. Y., and
Ellis, G. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In
EMNLP, pp. 890-899.
</reference>
<page confidence="0.998297">
274
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.717849">
<title confidence="0.999343">Learning Phrase-Based Spelling Error Models</title>
<author confidence="0.841767">from Clickthrough Data</author>
<affiliation confidence="0.966377">Dept. of Mathematical Informatics University of Tokyo, Tokyo, Japan</affiliation>
<email confidence="0.936064">xusun@mist.i.u-tokyo.ac.jp</email>
<author confidence="0.992593">Daniel Micol</author>
<affiliation confidence="0.999975">Microsoft Corporation</affiliation>
<address confidence="0.996959">Munich, Germany</address>
<email confidence="0.99998">danielmi@microsoft.com</email>
<abstract confidence="0.999217076923077">This paper explores the use of clickthrough data for query spelling correction. First, large amounts of query-correction pairs are derived by analyzing users&apos; query reformulation behavior encoded in the clickthrough data. Then, a phrase-based error model that accounts for the transformation probability between multi-term phrases is trained and integrated into a query speller system. Experiments are carried out on a human-labeled data set. Results show that the system using the phrase-based error model outperforms significantly its baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>E Brill</author>
<author>S Dumais</author>
</authors>
<title>Improving web search ranking by incorporating user behavior information.</title>
<date>2006</date>
<booktitle>In SIGIR,</booktitle>
<pages>pp.</pages>
<contexts>
<context position="7197" citStr="Agichtein et al., 2006" startWordPosition="1105" endWordPosition="1108">ery logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models from clickthrough data. To our knowledge, this is the first such attempt using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In thi</context>
<context position="8666" citStr="Agichtein et al., 2006" startWordPosition="1344" endWordPosition="1347"> error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of queries Q, and Q2 such th</context>
</contexts>
<marker>Agichtein, Brill, Dumais, 2006</marker>
<rawString>Agichtein, E., Brill, E. and Dumais, S. 2006. Improving web search ranking by incorporating user behavior information. In SIGIR, pp. 19-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ahmad</author>
<author>G Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP,</booktitle>
<pages>955--962</pages>
<contexts>
<context position="6521" citStr="Ahmad and Kondrak (2005)" startWordPosition="993" endWordPosition="997">n text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to </context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Ahmad, F., and Kondrak, G. 2005. Learning a spelling error model from search query logs. In HLT-EMNLP, pp 955-962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>R C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In ACL,</booktitle>
<pages>286--293</pages>
<contexts>
<context position="2835" citStr="Brill and Moore, 2000" startWordPosition="413" endWordPosition="416">or the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled ∗ The work was done when Xu Sun was visiting Microsoft Research Redmond. 266 Proceedings</context>
<context position="5241" citStr="Brill and Moore, 2000" startWordPosition="785" endWordPosition="788">rrors. In non-word error spelling correction, any word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Chur</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Brill, E., and Moore, R. C. 2000. An improved error model for noisy channel spelling correction. In ACL, pp. 286-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Chen</author>
<author>M Li</author>
<author>M Zhou</author>
</authors>
<title>Improving query spelling correction using web search results.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>181--189</pages>
<contexts>
<context position="1430" citStr="Chen et al. (2007)" startWordPosition="199" endWordPosition="202">troduction Search queries present a particular challenge for traditional spelling correction methods for three main reasons (Ahmad and Kondrak, 2004). First, spelling errors are more common in search queries than in regular written text: roughly 10-15% of queries contain misspelled terms (Cucerzan and Brill, 2004). Second, most search queries consist of a few key words rather than grammatical sentences, making a grammar-based approach inappropriate. Most importantly, many queries contain search terms, such as proper nouns and names, which are not well established in the language. For example, Chen et al. (2007) reported that 16.5% of valid search terms do not occur in their 200K-entry spelling lexicon. Therefore, recent research has focused on the use of Web corpora and query logs, rather than Jianfeng Gao Microsoft Research Redmond, WA, USA jfgao@microsoft.com Chris Quirk Microsoft Research Redmond, WA, USA chrisq@microsoft.com human-compiled lexicons, to infer knowledge about misspellings and word usage in search queries (e.g., Whitelaw et al., 2009). Another important data source that would be useful for this purpose is clickthrough data. Although it is well-known that clickthrough data contain r</context>
<context position="6728" citStr="Chen et al. (2007)" startWordPosition="1030" endWordPosition="1033">ry, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models fr</context>
</contexts>
<marker>Chen, Li, Zhou, 2007</marker>
<rawString>Chen, Q., Li, M., and Zhou, M. 2007. Improving query spelling correction using web search results. In EMNLP-CoNLL, pp. 181-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>T Hard</author>
<author>J Gao</author>
</authors>
<title>Compressing trigram language models with Golomb coding.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>199--207</pages>
<contexts>
<context position="5857" citStr="Church et al., 2007" startWordPosition="889" endWordPosition="892">2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correct</context>
</contexts>
<marker>Church, Hard, Gao, 2007</marker>
<rawString>Church, K., Hard, T., and Gao, J. 2007. Compressing trigram language models with Golomb coding. In EMNLP-CoNLL, pp. 199-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>E Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>293--300</pages>
<contexts>
<context position="1127" citStr="Cucerzan and Brill, 2004" startWordPosition="150" endWordPosition="153">model that accounts for the transformation probability between multi-term phrases is trained and integrated into a query speller system. Experiments are carried out on a human-labeled data set. Results show that the system using the phrase-based error model outperforms significantly its baseline systems. 1 Introduction Search queries present a particular challenge for traditional spelling correction methods for three main reasons (Ahmad and Kondrak, 2004). First, spelling errors are more common in search queries than in regular written text: roughly 10-15% of queries contain misspelled terms (Cucerzan and Brill, 2004). Second, most search queries consist of a few key words rather than grammatical sentences, making a grammar-based approach inappropriate. Most importantly, many queries contain search terms, such as proper nouns and names, which are not well established in the language. For example, Chen et al. (2007) reported that 16.5% of valid search terms do not occur in their 200K-entry spelling lexicon. Therefore, recent research has focused on the use of Web corpora and query logs, rather than Jianfeng Gao Microsoft Research Redmond, WA, USA jfgao@microsoft.com Chris Quirk Microsoft Research Redmond, W</context>
<context position="6398" citStr="Cucerzan and Brill (2004)" startWordPosition="973" endWordPosition="976">r context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain muc</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Cucerzan, S., and Brill, E. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In EMNLP, pp. 293-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>W Yuan</author>
<author>X Li</author>
<author>K Deng</author>
<author>J-Y Nie</author>
</authors>
<title>Smoothing clickthrough data for web search ranking.</title>
<date>2009</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="7216" citStr="Gao et al., 2009" startWordPosition="1109" endWordPosition="1112">gorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models from clickthrough data. To our knowledge, this is the first such attempt using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue th</context>
<context position="8685" citStr="Gao et al., 2009" startWordPosition="1348" endWordPosition="1351">sume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of queries Q, and Q2 such that (1) they are iss</context>
</contexts>
<marker>Gao, Yuan, Li, Deng, Nie, 2009</marker>
<rawString>Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 2009. Smoothing clickthrough data for web search ranking. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>Applying winnow to context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In ICML,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="5812" citStr="Golding and Roth, 1996" startWordPosition="881" endWordPosition="884">tive (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in de</context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>Golding, A. R., and Roth, D. 1996. Applying winnow to context-sensitive spelling correction. In ICML, pp. 182-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>SIGKDD,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="7173" citStr="Joachims, 2002" startWordPosition="1103" endWordPosition="1104">or model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models from clickthrough data. To our knowledge, this is the first such attempt using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformati</context>
<context position="8642" citStr="Joachims, 2002" startWordPosition="1342" endWordPosition="1343">e a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of q</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims, T. 2002. Optimizing search engines using clickthrough data. In SIGKDD, pp. 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Kernighan</author>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>In COLING,</booktitle>
<pages>205--210</pages>
<contexts>
<context position="2791" citStr="Kernighan et al., 1990" startWordPosition="406" endWordPosition="409">been little research on exploiting the data for the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled ∗ The work was done when Xu Sun was visiting</context>
<context position="5218" citStr="Kernighan et al., 1990" startWordPosition="780" endWordPosition="784">d errors and real-word errors. In non-word error spelling correction, any word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mang</context>
</contexts>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>Kernighan, M. D., Church, K. W., and Gale, W. A. 1990. A spelling correction program based on a noisy channel model. In COLING, pp. 205-210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT/NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="7997" citStr="Koehn et al., 2003" startWordPosition="1234" endWordPosition="1237"> first such attempt using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and prove</context>
<context position="24214" citStr="Koehn et al., 2003" startWordPosition="3900" endWordPosition="3903">nd-position of Q. Then, by back-tracking the most probable bi-phrase boundaries, we obtain B*. The algorithm takes a complexity of O(KL2), where K is the total number of word alignments in A* which does not contain empty words, and L is the maximum length of a bi-phrase, which is a hyper-parameter of the algorithm. Notice that when we set L=1, the phrase-based error model is reduced to a word-based error model which assumes that words are transformed independently from C to Q, without taking into account any contextual information. 5.2 Model Estimation We follow a method commonly used in SMT (Koehn et al., 2003) to extract bi-phrases and estimate their replacement probabilities. From each query-correction pair with its word alignment (Q, C, A*), all bi-phrases consistent with the word alignment are identified. Consistency here implies two things. First, there must be at least one aligned word pair in the bi-phrase. Second, there must not be any word alignments from words inside the bi-phrase to words outside the bi-phrase. That is, we do not extract a phrase pair if there is an alignment from within the phrase pair to outside the phrase pair. The toy example shown in Figure 4 illustrates the bilingua</context>
<context position="25560" citStr="Koehn et al., 2003" startWordPosition="4136" endWordPosition="4139">ditional relative frequency estimates without smoothing. For example, the phrase transformation probability P(g|c) in Equation (7) can be estimated approximately as A B A E F a C D a d c c C F f # # adc d dc # ABCD D CD # dcf CDEF f 271 N(c, q) (11) Zy, N(c, g&apos;) where N(c, q) is the number of times that c is aligned to q in training data. These estimates are useful for contextual lexical selection with sufficient training data, but can be subject to data sparsity issues. An alternate translation probability estimate not subject to data sparsity issues is the so-called lexical weight estimate (Koehn et al., 2003). Assume we have a word translation distribution t(q|c) (defined over individual words, not phrases), and a word alignment A between q and c; here, the word alignment contains (i, j) pairs, where i E L. |q |and j E 0.. |c|, with 0 indicating an inserted word. Then we can use the following estimate: Pw(g|c,A) _ F1 1 |{I|(1, ) E A} |1 t(gi|cj) i=1 V(Q)EA We assume that for every position in q, there is either a single alignment to 0, or multiple alignments to non-zero positions in c. In effect, this computes a product of per-word translation scores; the per-word scores are averages of all the tr</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F., and Marcu, D. 2003. Statistical phrase-based translation. In HLT/NAACL, pp. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys.</journal>
<volume>24</volume>
<issue>4</issue>
<pages>377--439</pages>
<contexts>
<context position="5018" citStr="Kukich (1992)" startWordPosition="754" endWordPosition="755">es the paper. 2 Related Work Spelling correction for regular written text is a long standing research topic. Previous researches can be roughly grouped into two categories: correcting non-word errors and real-word errors. In non-word error spelling correction, any word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generat</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Kukich, K. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys. 24(4): 377-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<pages>10--8</pages>
<contexts>
<context position="20737" citStr="Levenshtein, 1966" startWordPosition="3309" endWordPosition="3311">ing Q and C pairs which act as features for the ranker. Furthermore, the word-level alignments between Q and C can most often be identified with little ambiguity. Thus we restrict our attention to those phrase transformations consistent with a good word-level alignment. Let J be the length of Q, L be the length of C, and A = a1, ..., aJ be a hidden variable representing the word alignment. Each ai takes on a value ranging from 1 to L indicating its corresponding word position in C, or 0 if the ith word in Q is unaligned. The cost of assigning k to ai is equal to the Levenshtein edit distance (Levenshtein, 1966) between the ith word in Q and the kth word in C, and the cost of assigning 0 to ai is equal to the length of the ith word in Q. We can determine the least cost alignment A* between Q and C efficiently using the A-star algorithm. When scoring a given candidate pair, we further restrict our attention to those S, T, M triples that are consistent with the word alignment, which we denote as B(C, Q, A*). Here, consistency requires that if two words are aligned in A*, then they must appear in the same bi-phrase (ci, qi). Once the word alignment is fixed, the final permutation is uniquely determined,</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Levenshtein, V. I. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>M Zhu</author>
<author>Y Zhang</author>
<author>M Zhou</author>
</authors>
<title>Exploring distributional similarity based models for query spelling correction.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<pages>1025--1032</pages>
<contexts>
<context position="6624" citStr="Li et al. (2006)" startWordPosition="1014" endWordPosition="1017">r a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the da</context>
</contexts>
<marker>Li, Zhu, Zhang, Zhou, 2006</marker>
<rawString>Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. Exploring distributional similarity based models for query spelling correction. In ACL, pp. 1025-1032.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In ICML,</booktitle>
<pages>187--194</pages>
<contexts>
<context position="5835" citStr="Mangu and Brill, 1997" startWordPosition="885" endWordPosition="888">1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of </context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>Mangu, L., and Brill, E. 1997. Automatic rule acquisition for spelling correction. In ICML, pp. 187-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Statistical machine translation: from single-word models to alignment templates.</title>
<date>2002</date>
<tech>PhD thesis,</tech>
<institution>RWTH Aachen.</institution>
<contexts>
<context position="22679" citStr="Och (2002)" startWordPosition="3638" endWordPosition="3639">se.maxProb) { biPhrase.maxProb = totalProb; biPhrase.yPre = yPre; } } Result: record at each bi-phrase boundary its maximum probability (biPhrase.maxProb) and optimal back-tracking biPhrases (biPhrase.yPre). Figure 3: The dynamic programming algorithm for Viterbi bi-phrase segmentation. P(T|C. S) _ fIk=1 P (qk |ck)�, (7) where P(qk|ck) is a phrase transformation probability, the estimation of which will be described in Section 5.2. To find the maximum probability assignment efficiently, we can use a dynamic programming approach, somewhat similar to the monotone decoding algorithm described in Och (2002). Here, though, both the input and the output word sequences are specified as the input to the algorithm, as is the word alignment. We define the quantity aj to be the probability of the most likely sequence of bi-phrases that produce the first j terms of Q and are consistent with the word alignment and C. It can be calculated using the following algorithm: 1. Initialization: ao = 1 (8) 2. Induction: aj = max {aj′P(glcq)l (9) j′&lt;j.9=q,′�� ...ql 3. Total: P(Q|C) = aj (10) Figure 4: Toy example of (left) a word alignment between two strings &amp;quot;adcf&amp;quot; and &amp;quot;ABCDEF&amp;quot;; and (right) the bi-phrases contain</context>
<context position="26927" citStr="Och, 2002" startWordPosition="4374" endWordPosition="4375">e N(c, is the number of times that Zq′N(c,q). �l) the words (not phrases as in Equation 11) c and q are aligned in the training data. These word based scores of bi-phrases, though not as effective in contextual selection, are more robust to noise and sparsity. Throughout this section, we have approached this model in a noisy channel approach, finding probabilities of the misspelled query given the corrected query. However, the method can be run in both directions, and in practice SMT systems benefit from also including the direct probability of the corrected query given this misspelled query (Och, 2002). 5.3 Phrase-Based Error Model Features To use the phrase-based error model for spelling correction, we derive five features and integrate them into the ranker-based query speller system, described in Section 4. These features are as follows. • Two phrase transformation features: These are the phrase transformation scores based on relative frequency estimates in two directions. In the correction-to-query direction, we define the feature as fPT(Q, C, A) _ log P(Q|C), where P(Q|C) is computed by Equations (8) to (10), and P(gICJ is the relative frequency estimate of Equation (11). • Two lexical </context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Och, F. 2002. Statistical machine translation: from single-word models to alignment templates. PhD thesis, RWTH Aachen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>417--449</pages>
<contexts>
<context position="8017" citStr="Och and Ney, 2004" startWordPosition="1238" endWordPosition="1241">using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for W</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Och, F., and Ney, H. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4): 417-449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
<author>Y Tsuruoka</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>A discriminative candidate generator for string transformations.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>447--456</pages>
<contexts>
<context position="5291" citStr="Okazaki et al., 2008" startWordPosition="794" endWordPosition="797">word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular </context>
</contexts>
<marker>Okazaki, Tsuruoka, Ananiadou, Tsujii, 2008</marker>
<rawString>Okazaki, N., Tsuruoka, Y., Ananiadou, S., and Tsujii, J. 2008. A discriminative candidate generator for string transformations. In EMNLP, pp. 447-456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Philips</author>
</authors>
<title>Hanging on the metaphone.</title>
<date>1990</date>
<journal>Computer Language Magazine,</journal>
<pages>7--12</pages>
<contexts>
<context position="31773" citStr="Philips, 1990" startWordPosition="5175" endWordPosition="5176">peller system where the source-channel model of Equations (2) and (3) is used. In our implementation, we use a linear ranker with only two features, derived respectively from the language model and the error model models. The error model is based on the edit distance function. Row 2 is the ranker-based spelling system that uses all 96 ranking features, as described in Section 4. Note that the system uses the features derived from two error models. One is the edit distance model used for candidate generation. The other is a phonetic model that measures the edit distance between the metaphones (Philips, 1990) of a query word and its aligned correction word. Row 3 is the same system as Row 2, with an additional set of features derived from a word-based error model. This model is a special case of the phrase-based error model described in Section 5 with the maximum phrase length set to one. Row 4 is the system that uses the additional 5 features derived from the phrase-based error models with a maximum bi-phrase length of 3. In phrase based error model, L is the maximum length of a bi-phrase (Figure 3). This value is important for the spelling performance. We perform experiments to study the impact </context>
</contexts>
<marker>Philips, 1990</marker>
<rawString>Philips, L. 1990. Hanging on the metaphone. Computer Language Magazine, 7(12):38-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Suzuki</author>
<author>X Li</author>
<author>J Gao</author>
</authors>
<title>Discovery of term variation in Japanese web search queries.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8752" citStr="Suzuki et al., 2009" startWordPosition="1360" endWordPosition="1363">evel. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of queries Q, and Q2 such that (1) they are issued by the same user; (2) Q2 was issued within 3 minutes of Q,; and</context>
</contexts>
<marker>Suzuki, Li, Gao, 2009</marker>
<rawString>Suzuki, H., Li, X., and Gao, J. 2009. Discovery of term variation in Japanese web search queries. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>R Moore</author>
</authors>
<title>Pronunciation modeling for improved spelling correction.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="5268" citStr="Toutanova and Moore, 2002" startWordPosition="789" endWordPosition="793">r spelling correction, any word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as &amp;quot;peace&amp;quot; and &amp;quot;piece&amp;quot; in the context &amp;quot;a _ of cake&amp;quot;. A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When desi</context>
</contexts>
<marker>Toutanova, Moore, 2002</marker>
<rawString>Toutanova, K., and Moore, R. 2002. Pronunciation modeling for improved spelling correction. In ACL, pp. 144-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>C Zhai</author>
</authors>
<title>Mining term association patterns from search logs for effective query reformulation.</title>
<date>2008</date>
<booktitle>In CIKM,</booktitle>
<pages>479--488</pages>
<contexts>
<context position="8730" citStr="Wang and Zhai, 2008" startWordPosition="1356" endWordPosition="1359">ormed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of queries Q, and Q2 such that (1) they are issued by the same user; (2) Q2 was issued withi</context>
</contexts>
<marker>Wang, Zhai, 2008</marker>
<rawString>Wang, X., and Zhai, C. 2008. Mining term association patterns from search logs for effective query reformulation. In CIKM, pp. 479-488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>B Hutchinson</author>
<author>G Y Chung</author>
<author>G Ellis</author>
</authors>
<title>Using the web for language independent spellchecking and autocorrection.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>890--899</pages>
<contexts>
<context position="1880" citStr="Whitelaw et al., 2009" startWordPosition="264" endWordPosition="267">priate. Most importantly, many queries contain search terms, such as proper nouns and names, which are not well established in the language. For example, Chen et al. (2007) reported that 16.5% of valid search terms do not occur in their 200K-entry spelling lexicon. Therefore, recent research has focused on the use of Web corpora and query logs, rather than Jianfeng Gao Microsoft Research Redmond, WA, USA jfgao@microsoft.com Chris Quirk Microsoft Research Redmond, WA, USA chrisq@microsoft.com human-compiled lexicons, to infer knowledge about misspellings and word usage in search queries (e.g., Whitelaw et al., 2009). Another important data source that would be useful for this purpose is clickthrough data. Although it is well-known that clickthrough data contain rich information about users&apos; search behavior, e.g., how a user (re-) formulates a query in order to find the relevant document, there has been little research on exploiting the data for the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error mode</context>
<context position="6816" citStr="Whitelaw et al. (2009)" startWordPosition="1043" endWordPosition="1046">fore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models from clickthrough data. To our knowledge, this is the first such attempt using clickthroug</context>
</contexts>
<marker>Whitelaw, Hutchinson, Chung, Ellis, 2009</marker>
<rawString>Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis, G. 2009. Using the web for language independent spellchecking and autocorrection. In EMNLP, pp. 890-899.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>