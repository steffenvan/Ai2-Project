<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000495">
<title confidence="0.998389">
Multilingual Spectral Clustering
Using Document Similarity Propagation
</title>
<author confidence="0.986308">
Dani Yogatama and Kumiko Tanaka-Ishii
</author>
<affiliation confidence="0.952869">
Graduate School of Information Science and Technology, University of Tokyo
</affiliation>
<address confidence="0.836172">
13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo, Japan
</address>
<email confidence="0.998871">
yogatama@cl.ci.i.u-tokyo.ac.jp kumiko@i.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996784">
We present a novel approach for multilin-
gual document clustering using only com-
parable corpora to achieve cross-lingual
semantic interoperability. The method
models document collections as weighted
graph, and supervisory information is
given as sets of must-linked constraints for
documents in different languages. Recur-
sive k-nearest neighbor similarity propa-
gation is used to exploit the prior knowl-
edge and merge two language spaces.
Spectral method is applied to find the best
cuts of the graph. Experimental results
show that using limited supervisory in-
formation, our method achieves promis-
ing clustering results. Furthermore, since
the method does not need any language
dependent information in the process, our
algorithm can be applied to languages in
various alphabetical systems.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997307892857143">
Document clustering is unsupervised classifica-
tion of text collections into distinct groups of sim-
ilar documents. It has been used in many in-
formation retrieval tasks, including data organiza-
tion (Siersdorfer and Sizov, 2004), language mod-
eling (Liu and Croft, 2004), and improving per-
formances of text categorization system (Aggar-
wal et al., 1999). Advance in internet technology
has made the task of managing multilingual docu-
ments an intriguing research area. The growth of
internet leads to the necessity of organizing docu-
ments in various languages. There exist thousands
of languages, not to mention countless minor ones.
Creating document clustering model for each lan-
guage is simply unfeasible. We need methods to
deal with text collections in diverse languages si-
multaneously.
Multilingual document clustering (MLDC) in-
volves partitioning documents, written in more
than one languages, into sets of clusters. Simi-
lar documents, even if they are written in differ-
ent languages, should be grouped together into
one cluster. The major challenge of MLDC is
achieving cross-lingual semantic interoperability.
Most monolingual techniques will not work since
documents in different languages are mapped into
different spaces. Spectral method such as Latent
Semantic Analysis has been commonly applied
for MLDC task. However, current techniques
strongly rely on the presence of common words
between different languages. This method would
only work if the languages are highly related, i.e.,
languages that share the same root. Therefore, we
need another method to improve the robustness of
MLDC model.
In this paper, we focus on the problem of bridg-
ing multilingual space for document clustering.
We are given text documents in different lan-
guages and asked to group them into clusters such
that documents that belong to the same topic are
grouped together. Traditional monolingual ap-
proach is impracticable since it is unable to pre-
dict how similar two multilingual documents are.
They have two different spaces which make con-
ventional cosine similarity irrelevant. We try to
solve this problem utilizing prior knowledge in
the form of must-linked constraints, gathered from
comparable corpora. Propagation method is used
to guide the language-space merging process. Ex-
perimental results show that the approach gives
encouraging clustering results.
This paper is organized as follows. In section 2,
we review related work. In section 3, we propose
our algorithm for multilingual document cluster-
ing. The experimental results are shown in section
4. Section 5 concludes with a summary.
</bodyText>
<page confidence="0.974003">
871
</page>
<note confidence="0.997827">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 871–879,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.997129" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999912771428572">
Chen and Lin (2000) proposed methods to clus-
ter multilingual documents using translation tech-
nology, relying on cross-lingual dictionary and
machine-translation system. Multilingual ontol-
ogy, such as Eurovoc, is also popular for MLDC
(Pouliquen et al., 2004). However, such resources
are scarce and expensive to build. Several other
drawbacks of using this technique include dictio-
nary limitation and word ambiguity.
More recently, parallel texts have been used to
connect document collections from different lan-
guages (Wei et al., 2008). This is done by collaps-
ing columns in a term by document matrix that are
translations of each other. Nevertheless, building
parallel texts is also expensive and requires a lot of
works, hence shifting the paradigm of multilingual
works to comparable corpora.
Comparable corpora are collections of texts in
different languages regarding similar topics pro-
duced at the same time. The key difference be-
tween comparable corpora and parallel texts is that
documents in comparable corpora are not neces-
sarily translations of each other. They are easier
to be acquired, and do not need exhaustive works
to be prepared. News agencies often give informa-
tion in many different languages and can be good
sources for comparable corpora. Terms in com-
parable corpora, being about the same topic, up
to some point explain the same concepts in differ-
ent languages. Pairing comparable corpora with
spectral method such as Latent Semantic Analysis
has become prevalent, e.g. (Gliozzo and Strappar-
ava, 2005). They rely on the presence of common
words and proper nouns among various languages
to build a language-independent space. The per-
formance of such method is highly dependent on
the languages being used. Here, we present an-
other approach to exploit knowledge in compa-
rable corpora; using propagation method to aid
spreading similarity between collections of docu-
ments in different languages.
Spectral clustering is the task of finding good
clusters by using information contained in the
eigenvectors of a matrix derived from the data.
It has been successfully applied in many applica-
tions including information retrieval (Deerwester
et al., 2003) and computer vision (Meila and Shi,
2000). An in-depth analysis of spectral algo-
rithm for clustering problems is given in (Ng et
al., 2002). Zhang and Mao (2008) used a related
technique called Modularity Eigenmap to extract
community structure features from the document
network to solve hypertext classification problem.
Semi-supervised clustering enhances clustering
task by incorporating prior knowledge to aid clus-
tering process. It allows user to guide the cluster-
ing process by giving some feedback to the model.
In traditional clustering algorithm, only unlabeled
data is used to find assignments of data points
to clusters. In semi-supervised clustering, prior
knowledge is given to improve performance of the
system. The supervision is usually given as pair
of must-linked constraints and cannot link con-
straints, first introduced in (Wagstaff and Cardie,
2000). Kamvar et al. (2003) proposed spectral
learning algorithm that can take supervisory infor-
mation in the form of pairwise constraints or la-
beled data. Their algorithm is intended to be used
in monolingual context, while our algorithm is de-
signed to work in multilingual context.
</bodyText>
<sectionHeader confidence="0.96639" genericHeader="method">
3 Multilingual Spectral Clustering
</sectionHeader>
<bodyText confidence="0.999977615384615">
There have been several works on multilingual
document clustering as mention previously in Sec-
tion 2. Our key contribution here is the propaga-
tion method to make spectral clustering algorithm
works for multilingual problems. The clustering
model exploits the supervisory information by de-
tecting k nearest neighbors of the newly-linked
documents, and propagates document similarity to
these neighbors. The model can be applied to any
multilingual text collections regardless of the lan-
guages. Overall algorithm is given in Section 3.1
and the method to merge multilingual spaces by
similarity propagation is given in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.999742">
3.1 Spectral Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.999197857142857">
Spectral clustering tries to find good clusters by
using top eigenvectors of normalized data affin-
ity matrix. The document set is being modeled as
undirected graph G(V, E, W), where V , E, and
W denote the graph vertex set, edge set, and tran-
sition probability matrix, respectively. In graph
G, v ∈ V represents a document, and weight
wij ∈ W represents transition probability between
document vi to vj. The transition probabilities
can be interpreted as edge flows in Markov ran-
dom walk over graph vertices (documents in col-
lections).
Algorithm to perform spectral clustering is
given in Algorithm 1. Let A be affinity matrix
</bodyText>
<page confidence="0.996047">
872
</page>
<bodyText confidence="0.997893060606061">
where element Aij is cosine similarity between
document vi and vj (Algorithm 1, line 1). It is
straightforward that documents belonging to dif-
ferent languages will have similarity zero. Rare
exception occurs when they have common words
because the languages are related one another.
As a consequence, the similarity matrix will have
many zeros. Our model amplifies prior knowledge
in the form of comparable corpora by perform-
ing document similarity propagation, presented in
Section 3.2 (Algorithm 1, line 4; Algorithm 2, ex-
plained in Section 3.2). After propagation, the
affinity matrix is post-processed (Algorithm 1, line
6, explained in Section 3.2) before being trans-
formed into transition probability matrix.
The transformation can be done using any nor-
malization for spectral methods. Define N =
D−1A, as in (Meila and Shi, 2001), where D is the
diagonal matrix whose elements Dij = Ej Aij
(Algorithm 1, line 7). Alternatively, we can define
N = D−1/2AD−1/2 (Ng et al., 2002), or N =
(A + dmaxI − D)/dmax (Fiedler, 1975), where
dmax is the maximum rowsum of A. For our ex-
periment, we use the first normalization method,
though other methods can be applied as well.
Meila and Shi (2001) show that probability tran-
sition matrix N with t strong clusters will have t
piecewise constant eigenvectors. They also sug-
gest using these t eigenvectors in clustering pro-
cess. We use the information contains in t largest
eigenvectors of N (Algorithm 1, line 8-11) and
perform K-means clustering algorithm to find the
data clusters (Algorithm 1, line 12).
</bodyText>
<subsectionHeader confidence="0.999804">
3.2 Propagating Prior Knowledge
</subsectionHeader>
<bodyText confidence="0.938012736842105">
We use information obtained from comparable
corpora to merge multilingual language spaces.
Suppose we have text collections in L different
languages. We combine this collections with com-
parable corpora, also in L languages, that act as
our supervisory information. Comparable corpora
are used to gather prior knowledge by making
must-linked constraints for documents in different
languages that belong to the same topic in the cor-
pora, propagating similarity to other documents
while doing so.
Initially, our affinity matrix A represents cosine
similarity between all pairs of documents. Aij is
set to zero if j is not the top k nearest neighbors
of i and likewise. Next, set Aij and Aji to 1 if
document i and document j are different in lan-
Algorithm 1 Multilingual Spectral Clustering
Input: Term by document matrix M, pairwise
constraints
</bodyText>
<listItem confidence="0.979861652173913">
Output: Document clusters
1: Create graph affinity matrix A E Rn×n where
each element Aij represents the similarity be-
tween document vi and vj.
2: for all pairwise constraints in comparable cor-
pora do
3: Aij +- 1,Aji +- 1.
4: Recursive Propagation (A, S, Q, k, vi, vj).
5: end for
6: Post-process matrix A so that every value in
A is greater than δ and less than 1.
7: Form a diagonal matrix D, where Dii =
Ej Aij. Normalize N = D−1A.
8: Find x1, x2 , xt, the t largest eigenvectors
of N.
9: Form matrix X = [x1, x2, , xt] E Rn×t.
10: Normalize row X to be unit length.
11: Project each document into eigen-space
spanned by the above t eigenvectors (by treat-
ing each row of X as a point in Rt, row i rep-
resents document vi).
12: Apply K-means algorithm in this space to find
document clusters.
</listItem>
<bodyText confidence="0.999552043478261">
guage and belong to the same topic in our com-
parable corpora. This will incorporate the must-
linked constraint to our model. We can also give
supervisory information for pairs of document in
the same language, but this is optional. We also do
not use cannot-linked constraints since the main
goal is to merge multilingual spaces. In our exper-
iment we show that using only must-linked con-
straints with propagation is enough to achieve en-
couraging clustering results.
The supervisory information acquired from
comparable corpora only connects two nodes in
our graph. Therefore, the number of edges be-
tween documents in different languages is about
as many as the number of must-linked constraints
given. We argue that we need more edges between
pairs of documents in different languages to get
better results.
We try to build more edges by propagating sim-
ilarity to other documents that are most similar to
the newly-linked documents. Figure 1 gives an il-
lustration of edge-creation process when two mul-
tilingual documents (nodes) are connected. Sup-
</bodyText>
<page confidence="0.99415">
873
</page>
<figure confidence="0.985823">
(b) Effect on neighbor nodes
</figure>
<figureCaption confidence="0.720192">
Figure 1: Pairing two multilingual documents af-
fect their neighbors. vi and vj are documents in
two different languages. yx and zx are neighbors
of vi and vj respectively.
</figureCaption>
<bodyText confidence="0.999860604166667">
pose that we have six documents in two differ-
ent languages. Initially, documents are only con-
nected with other documents that belong to the
same language. The supervisory information tells
us that two multilingual documents vi and vj
should be connected (Figure 1(a)). We then build
an edge between these two documents. Further-
more, we also use this information to build edges
between vi and neighbors of vj and likewise (Fig-
ure 1(b)).
This follows from the hypothesis that bringing
together two documents should also bring other
documents that are similar to those two closer in
our clustering space. Klein et al. (2002) stated
that a good clustering algorithm, besides satisfy-
ing known constraints, should also be able to sat-
isfy the implications of those constraints. Here,
we allow not only instance-level inductive impli-
cations, but utilize it to get higher-level inductive
implications. In other words, we alter similarity
space so that it can detect other clusters by chang-
ing the topology of the original space.
The process is analogous to shortening the dis-
tance between sets of documents in Euclidean
space. In vector space model, two documents that
are close to each other have high similarity, and
thus will belong to the same cluster. Pairing two
documents can be seen as setting the distance in
this space to 0, thus raising their similarity to 1.
While doing so, each document would also draw
sets of documents connected to it closer to the cen-
tre of the merge, which is equivalent to increasing
their similarities.
Suppose we have document vi and vj, and y and
z are sets of their respective k nearest neighbors,
where |y |= |z |= k. The propagation method
is a recursive algorithm with base 5, the num-
ber of desired level of propagation. Recursive k-
nearest neighbor makes decision to give high sim-
ilarity between multilingual documents not only
determined by their similarity to the newly-linked
documents, but also their similarity to the k near-
est neighbors of the respective document. Several
documents are affected by a single supervisory in-
formation. This will prove useful when only lim-
ited amount of supervisory information given. It
uses document similarity matrix A, as defined in
the previous section.
</bodyText>
<listItem confidence="0.979510333333333">
1. For yx E y we propagate QAviyx to Avjyx.
Set Ayxvj = Avjyx (Algorithm 2, line 5-6).
In other words, we propagate the similarity
between document vi and y nearest neighbors
of vi to document vj.
2. Similarly, for zx E z we propagate QAvjzx
to Avizx. Set Azxvi = Avizx (Algorithm 2,
line 10-11). In other words, we propagate the
similarity between document vj and z nearest
neighbors of vj to document vi.
3. Propagate higher order similarity to k nearest
neighbors of y and z, discounting the similar-
</listItem>
<bodyText confidence="0.981950470588235">
ity quadratically, until required level of prop-
agation 5 is reached (Algorithm 2, line 7 and
12).
The coefficient Q represents the degree of en-
forcement that the documents similar to a docu-
ment in one language, will also have high simi-
larity with other document in other language that
is paired up with its ancestor. On the other hand,
k represents the number of documents that are af-
fected by pairing up two multilingual documents.
After propagation, similarity of documents that
falls below some threshold δ is set to zero (Al-
gorithm 1, line 6). This post-processing step is
performed to nullify insignificant similarity values
propagated to a document. Additionally, if there
exists similarity of documents that is higher than
one, it is set to one.
</bodyText>
<figure confidence="0.719574153846154">
zx1 vj zx2
yx1
zx1 vj zx2
yx1 vi yx2
(a) Connect two nodes
vi yx2
874
Algorithm 2 Recursive Propagation
Input: Affinity matrix A, level of propagation 5,
Q, number of nearest neighbors k, document vi
and vj
Output: Propagated affinity matrix
1: if 5 = 0 then
</figure>
<listItem confidence="0.968466333333333">
2: return
3: else
4: for all yx E k-NN document vi do
5: Avjyx Avjyx + QAviyx
6: Ayxvj Avjyx
7: Recursive Propagation (A, 5 − 1,
Q2, k, yx, vj)
8: end for
9: for all zx E k-NN document vj do
10: Set Avizx Avizx + QAvjzx
11: Set Azxvi Avizx
12: Recursive Propagation (A, 5 − 1,
</listItem>
<equation confidence="0.6282405">
2
Q , k, vi, zx)
</equation>
<bodyText confidence="0.2697215">
13: end for
14: end if
</bodyText>
<sectionHeader confidence="0.938373" genericHeader="evaluation">
4 Performance Evaluation
</sectionHeader>
<bodyText confidence="0.998003">
The goals of empirical evaluation include (1) test-
ing whether the propagation method can merge
multilingual space and produce acceptable clus-
tering results; (2) comparing the performance to
spectral clustering method without propagation.
</bodyText>
<subsectionHeader confidence="0.987487">
4.1 Data Description
</subsectionHeader>
<bodyText confidence="0.999979142857143">
We tested our model using Reuters Corpus Vol-
ume 2 (RCV2), a multilingual corpus contain-
ing news in thirteen different languages. For our
experiment, three different languages: English,
French, and Spanish; in six different topics: sci-
ence, sports, disasters accidents, religion, health,
and economy are used. We discarded documents
with multiple category labels.
We do not apply any language specific pre-
processing method to the raw text data. Mono-
lingual TFIDF is used for feature weighting. All
document vectors are then converted into unit vec-
tor by dividing by its length. Table 1 shows the
average length of documents in our corpus.
</bodyText>
<subsectionHeader confidence="0.971432">
4.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.975999666666667">
For our experiment, we used Rand Index (RI)
which is a common evaluation technique for clus-
tering task where the true class of unlabeled data
</bodyText>
<table confidence="0.999454625">
English French Spanish Total
Science 290.10 165.10 213.45 222.88
Sports 182.55 156.83 189.75 176.37
Disasters 154.29 175.89 165.31 165.16
Religion 317.77 177.91 242.67 246.11
Health 251.19 233.70 227.25 237.38
Economy 266.89 192.55 306.11 255.08
Total 243.79 183.61 224.09 217.16
</table>
<tableCaption confidence="0.999002">
Table 1: Average number of words of documents
</tableCaption>
<bodyText confidence="0.988691333333333">
in the corpus. Each language consists of 600 doc-
uments, and each topic consists of 100 documents
(per language).
is known. Rand Index measures the percentage of
decisions that are correct, or simply the accuracy
of the model. Rand Index is defined as:
</bodyText>
<equation confidence="0.999426">
TP + TN
RI = TP + FP + TN + FN
</equation>
<bodyText confidence="0.999646866666667">
Rand Index penalizes false positive and false neg-
ative decisions during clustering. It takes into ac-
count decision that assign two similar documents
to one cluster (TP), two dissimilar documents to
different clusters (TN), two similar documents to
different clusters (FN), and two dissimilar docu-
ments to one cluster (FP). We do not include links
created by supervisory information when calculat-
ing true positive decisions and only consider the
number of free decisions made.
We also used Fα-measure, the weighted har-
monic mean of precision (P) and recall (R). Fα-
measure is defined as:
Last, we used purity to evaluate the accuracy of
assignments. Purity is defined as:
</bodyText>
<equation confidence="0.997545">
1 �
P urity = N
t
</equation>
<bodyText confidence="0.99987375">
where N is the number of documents, t is the num-
ber of clusters, j is the number of classes, wt and
cj are sets of documents in cluster t and class j
respectively.
</bodyText>
<equation confidence="0.960160125">
Fα =α2P + R
= TP
P TP+FP
= TP
R TP+FN
(α2 + 1)PR
max |wt n cj|
j
</equation>
<page confidence="0.974845">
875
</page>
<figure confidence="0.999871708333333">
0 0.2 0.4 0.6 0.8 1
Proportion of supervisory information
(a) Rand Index for 6 topics
0 0.2 0.4 0.6 0.8 1
Proportion of supervisory information
(b) Rand Index for 4 topics
With propagation
Without propagation
LSA
With propagation
Without propagation
LSA
Rand Index 1
0.8
0.6
0.4
0.2
0
Rand Index 1
0.8
0.6
0.4
0.2
0
</figure>
<figureCaption confidence="0.983026666666667">
Figure 2: Rand Index on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4
topics as the proportion of supervisory information increases. k = 30, 6 = 0.03, Q = 0.5, t = number of
topics, and S = 2.
</figureCaption>
<figure confidence="0.999743384615385">
0 0.2 0.4 0.6 0.8 1
Proportion of supervisory information
(a) Purity for 6 topics
0 0.2 0.4 0.6 0.8 1
Proportion of supervisory information
(b) Purity for 4 topics
Purity
0.8
0.6
0.4
0.2
0
1
With propagation
Without propagation
LSA
Purity
0.8
0.6
0.4
0.2
0
1
With propagation
Without propagation
LSA
</figure>
<figureCaption confidence="0.960384666666667">
Figure 3: Purity on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4 topics
as the proportion of supervisory information increases. k = 30, 6 = 0.03, Q = 0.5, t = number of topics,
and S = 2.
</figureCaption>
<subsectionHeader confidence="0.999468">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999853741935484">
To prove the effectiveness of our clustering algo-
rithm, we performed the following experiments on
our data set. We first tested our algorithm on four
topics, science, sports, religion, and economy. We
then tested our algorithm using all six topics to
get an understanding of the performance of our
model in larger collections with more topics. We
used subset of our data as supervisory informa-
tion and built must-linked constraints from it. The
proportion of supervisory information provided to
the system is given in x-axis (Figure 2 - Figure
4.3). 0.2 here means 20% of documents in each
language are taken to be used as prior knowledge.
Since the number of documents in each language
for our experiment is the same, we have the same
numbers of documents in subset of English col-
lection, subset of French collection, and subset of
Spanish collection. We also ensure there are same
numbers of documents for a particular topic in all
three languages. We can build must-linked con-
straints as follows. For each document in the sub-
set of English collection, we create must-linked
constraints with one randomly selected document
from the subset of French collection and one ran-
domly selected document from the subset of Span-
ish collection that belong to the same topic with it.
We then create must-linked constraint between the
respective French and Spanish documents. The
constraints given to the algorithm are chosen so
that there are several links that connect every topic
in every language. Note that the class label in-
</bodyText>
<page confidence="0.995779">
876
</page>
<figure confidence="0.99974525">
0 0.2 0.4 0.6 0.8 1
Proportion of supervisory information
(a) F2-measure for 6 topics
0 0.2 0.4 0.6 0.8 1
Proportion of supervisory information
(b) F2-measure for 4 topics
With propagation
Without propagation
LSA
With propagation
Without propagation
LSA
F2-measure 1
0.8
0.6
0.4
0.2
0
F2-measure 1
0.8
0.6
0.4
0.2
0
</figure>
<figureCaption confidence="0.67067">
Figure 4: F2-measure on the RCV2 task with (a) 1800 documents, 6 topics; and (b) 1200 documents, 4
topics as the proportion of supervisory information increases. k = 30, δ = 0.03, Q = 0.5, t = number of
topics, and S = 2.
</figureCaption>
<bodyText confidence="0.999883985294118">
formation is only used to build must-linked con-
straints between documents, and we do not assign
the documents to a particular cluster.
Figure 2 shows the Rand Index as proportion
of supervisory information increases. Figure 3
and Figure 4.3 give purity and F2-measure for
the algorithm respectively. To show the impor-
tance of the propagation in multilingual space, we
give comparison with spectral clustering model
without propagation. Three lines in Figure 2 to
Figure 4.3 indicate: (1) results with propagation
(solid line); (2) results without propagation (long-
dashed line); and (3) results using Latent Se-
mantic Analysis(LSA)-based method by exploit-
ing common words between languages (short-
dashed line). For each figure, 6 plots are taken
starting from 0 in 0.2-point-increments. We con-
ducted the experiments three times for each pro-
portion of supervisory information and use the av-
erage values. As we can see from Figure 2, Fig-
ure 3, and Figure 4.3, the propagation method can
significantly improve the performance of spectral
clustering algorithm. For 1800 documents in 6
topics, we manage to achieve RI = 0.91, purity
= 0.84, and F2-measure = 0.76 with only 20% of
documents (360 documents) used as supervisory
information. Spectral clustering algorithm with-
out propagation can only achieve 0.69, 0.30, 0.28
for RI, purity, and F2-measure respectively. The
propagation method is highly effective when only
small amount of supervisory information given to
the algorithm. Obviously, the more supervisory in-
formation given, the better the performance is. As
the number of supervisory information increases,
the difference of the model performance with and
without propagation becomes smaller. This is
because there are already enough links between
multilingual documents, so we do not necessar-
ily build more links through similarity propagation
anymore. However, even when there are already
many links, our model with propagation still out-
performs the model without propagation.
We compare the performance of our algorithm
to LSA-based multilingual document clustering
model. We performed LSA to the multilingual
term by document matrix. We do not use paral-
lel texts and only rely on common words across
languages as well as must-linked constraints to
build multilingual space. The results show that ex-
ploiting common words between languages alone
is not enough to build a good multilingual se-
mantic space, justifying the usage of supervisory
information in multilingual document clustering
task. When supervisory information is introduced,
our method achieves better results than LSA-based
method. In general, the LSA-based method per-
forms better than the model without propagation.
We assess the sensitivity of our algorithm to
parameter Q, the penalty for similarity propaga-
tion. We assess the sensitivity of our algorithm
to parameter Q, the penalty for similarity prop-
agation. We tested our algorithm using various
Q, starting from 0 to 1 in 0.2-point-increments,
while other parameters being held constant. Fig-
ure 5(a) shows that changing Q to some extent af-
fects the performance of the algorithm. However,
after some value of reasonable Q is found, increas-
ing Q does not have significant impact on the per-
</bodyText>
<page confidence="0.970459">
877
</page>
<figure confidence="0.991568083333333">
Rand Index 1 Rand Index 1
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0 0
0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 100
β k
(a) Changing ,3, k = 30, t = 6 (b) Changing k, ,3 = 0.5, t = 6
0 5 10 15 20
t
(c) Changing t, ,3 = 0.5, k = 30
</figure>
<figureCaption confidence="0.758302333333333">
Figure 5: Rand Index on the RCV2 task with 1800 documents and 6 topics as (a) Q increases; (b)
k increases; and (c) t increases. δ = 0.03, S = 2, and 20% of documents are used as supervisory
information.
</figureCaption>
<figure confidence="0.999304333333333">
Rand Index 1
0.8
0.6
0.4
0.2
0
</figure>
<bodyText confidence="0.999833714285714">
formance of the algorithm. We also tested our al-
gorithm using various k, starting from 0 to 100
in 20-point-increments. Figure 5(b) reveals that
the performances of the model with different k are
comparable, as long as k is not too small. How-
ever, using too large k will slightly decrease the
performance of the model. Too many propaga-
tions make several dissimilar documents receive
high similarity value that cannot be nullified by
the post-processing step. Last, we experimented
using various t ranging from 2 to 20. Figure 5(c)
shows that the method performs best when t = 10,
and for reasonable value of t the method achieves
comparable performance.
</bodyText>
<sectionHeader confidence="0.999462" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999569">
We present here a multilingual spectral cluster-
ing model that is able to work irrespective of the
languages being used. The key component of
our model is the propagation algorithm to merge
multilingual spaces. We tested our algorithm
on Reuters RCV2 Corpus and compared the per-
formance with spectral clustering model without
propagation. Experimental results reveal that us-
ing limited supervisory information, the algorithm
achieves encouraging clustering results.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997955">
Charu C. Aggarwal, Stephen C. Gates and Philip S.
Yu. 1999. On The Merits of Building Catego-
rization Systems by Supervised Clustering. In Pro-
ceedings of Conference on Knowledge Discovery in
Databases:352-356.
Hsin-Hsi Chen and Chuan-Jie Lin. 2000. A Mul-
tilingual News Summarizer. In Proceedings of
18th International Conference on Computational
Linguistics:159-165.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harsh-
man. 1990. Indexing by Latent Semantic Analy-
sis. Journal of the American Society of Information
Science:41(6):391-407.
Miroslav Fiedler. 1975. A Property of Eigenvectors of
Nonnegative Symmetric Matrices and its Applica-
tions to Graph Theory. Czechoslovak Mathematical
Journal, 25:619-672.
</reference>
<page confidence="0.982368">
878
</page>
<reference confidence="0.999532538461539">
Alfio Gliozzo and Carlo Strapparava. 2005. Cross lan-
guage Text Categorization by acquiring Multilingual
Domain Models from Comparable Corpora. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts:9-16.
Sepandar D. Kamvar, Dan Klein, and Christopher D.
Manning. 2003. Spectral Learning. In Proceed-
ings of the International Joint Conference on Artifi-
cial Intelligence (IJCAI).
Dan Klein, Sepandar D. Kamvar, and Christopher D.
Manning. 2002. From instance-level constraints to
space-level constraints: Making the most of prior
knowledge in data clustering. In The Nineteenth In-
ternational Conference on Machine Learning.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-
based Retrieval using Language Models. In Pro-
ceedings of the 27th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval:186-193.
Marinla Meil˘a and Jianbo Shi. 2000. Learning seg-
mentation by random walks. In Advances in Neural
Information Processing Systems:873-879.
Marinla Meil˘a and Jianbo Shi. 2001. A Random Walks
View of Spectral Segmentation. In AI and Statistics
(AISTATS).
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss.
2002. On Spectral Clustering: Analysis and an al-
gorithm. In Proceedings of Advances in Neural In-
formation Processing Systems (NIPS 14).
Bruno Pouliquen, Ralf Steinberger, Camelia Ignat,
Emilia K¨asper, and Irina Temnikova. 2004. Mul-
tilingual and Cross-lingual News Topic Tracking. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Stefan Siersdorfer and Sergej Sizov. 2004. Restrictive
Clustering and Metaclustering for Self-Organizing
Document. In Proceedings of the 27th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval.
Kiri Wagstaff and Claire Cardie 2000. Clustering
with Instance-level Constraints. In Proceedings
of the 17th International Conference on Machine
Learning:1103-1110.
Chih-Ping Wei, Christopher C. Yang, and Chia-Min
Lin. 2008. A Latent Semantic Indexing Based Ap-
proach to Multilingual Document Clustering. In De-
cision Support Systems, 45(3):606-620
Dell Zhang and Robert Mao. 2008. Extracting Com-
munity Structure Features for Hypertext Classifi-
cation. In Proceedings of the 3rd IEEE Interna-
tional Conference on Digital Information Manage-
ment (ICDIM).
</reference>
<page confidence="0.998947">
879
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805857">
<title confidence="0.9991245">Multilingual Spectral Using Document Similarity Propagation</title>
<author confidence="0.921813">Yogatama</author>
<affiliation confidence="0.990637">Graduate School of Information Science and Technology, University of</affiliation>
<address confidence="0.910289">13F Akihabara Daibiru, 1-18-13 Kanda Chiyoda-ku, Tokyo,</address>
<email confidence="0.948266">yogatama@cl.ci.i.u-tokyo.ac.jpkumiko@i.u-tokyo.ac.jp</email>
<abstract confidence="0.999591809523809">We present a novel approach for multilingual document clustering using only comparable corpora to achieve cross-lingual semantic interoperability. The method models document collections as weighted graph, and supervisory information is given as sets of must-linked constraints for documents in different languages. Recurneighbor similarity propagation is used to exploit the prior knowledge and merge two language spaces. Spectral method is applied to find the best cuts of the graph. Experimental results show that using limited supervisory information, our method achieves promising clustering results. Furthermore, since the method does not need any language dependent information in the process, our algorithm can be applied to languages in various alphabetical systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charu C Aggarwal</author>
<author>Stephen C Gates</author>
<author>Philip S Yu</author>
</authors>
<title>On The Merits of Building Categorization Systems by Supervised Clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of Conference on Knowledge Discovery in Databases:352-356.</booktitle>
<contexts>
<context position="1462" citStr="Aggarwal et al., 1999" startWordPosition="200" endWordPosition="204">using limited supervisory information, our method achieves promising clustering results. Furthermore, since the method does not need any language dependent information in the process, our algorithm can be applied to languages in various alphabetical systems. 1 Introduction Document clustering is unsupervised classification of text collections into distinct groups of similar documents. It has been used in many information retrieval tasks, including data organization (Siersdorfer and Sizov, 2004), language modeling (Liu and Croft, 2004), and improving performances of text categorization system (Aggarwal et al., 1999). Advance in internet technology has made the task of managing multilingual documents an intriguing research area. The growth of internet leads to the necessity of organizing documents in various languages. There exist thousands of languages, not to mention countless minor ones. Creating document clustering model for each language is simply unfeasible. We need methods to deal with text collections in diverse languages simultaneously. Multilingual document clustering (MLDC) involves partitioning documents, written in more than one languages, into sets of clusters. Similar documents, even if the</context>
</contexts>
<marker>Aggarwal, Gates, Yu, 1999</marker>
<rawString>Charu C. Aggarwal, Stephen C. Gates and Philip S. Yu. 1999. On The Merits of Building Categorization Systems by Supervised Clustering. In Proceedings of Conference on Knowledge Discovery in Databases:352-356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>Chuan-Jie Lin</author>
</authors>
<title>A Multilingual News Summarizer.</title>
<date>2000</date>
<booktitle>In Proceedings of 18th International Conference on Computational Linguistics:159-165.</booktitle>
<contexts>
<context position="3891" citStr="Chen and Lin (2000)" startWordPosition="573" endWordPosition="576">nts, gathered from comparable corpora. Propagation method is used to guide the language-space merging process. Experimental results show that the approach gives encouraging clustering results. This paper is organized as follows. In section 2, we review related work. In section 3, we propose our algorithm for multilingual document clustering. The experimental results are shown in section 4. Section 5 concludes with a summary. 871 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 871–879, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP 2 Related Work Chen and Lin (2000) proposed methods to cluster multilingual documents using translation technology, relying on cross-lingual dictionary and machine-translation system. Multilingual ontology, such as Eurovoc, is also popular for MLDC (Pouliquen et al., 2004). However, such resources are scarce and expensive to build. Several other drawbacks of using this technique include dictionary limitation and word ambiguity. More recently, parallel texts have been used to connect document collections from different languages (Wei et al., 2008). This is done by collapsing columns in a term by document matrix that are transla</context>
</contexts>
<marker>Chen, Lin, 2000</marker>
<rawString>Hsin-Hsi Chen and Chuan-Jie Lin. 2000. A Multilingual News Summarizer. In Proceedings of 18th International Conference on Computational Linguistics:159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science:41(6):391-407.</journal>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science:41(6):391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miroslav Fiedler</author>
</authors>
<title>A Property of Eigenvectors of Nonnegative Symmetric Matrices and its Applications to Graph Theory.</title>
<date>1975</date>
<journal>Czechoslovak Mathematical Journal,</journal>
<pages>25--619</pages>
<contexts>
<context position="9553" citStr="Fiedler, 1975" startWordPosition="1463" endWordPosition="1464">a by performing document similarity propagation, presented in Section 3.2 (Algorithm 1, line 4; Algorithm 2, explained in Section 3.2). After propagation, the affinity matrix is post-processed (Algorithm 1, line 6, explained in Section 3.2) before being transformed into transition probability matrix. The transformation can be done using any normalization for spectral methods. Define N = D−1A, as in (Meila and Shi, 2001), where D is the diagonal matrix whose elements Dij = Ej Aij (Algorithm 1, line 7). Alternatively, we can define N = D−1/2AD−1/2 (Ng et al., 2002), or N = (A + dmaxI − D)/dmax (Fiedler, 1975), where dmax is the maximum rowsum of A. For our experiment, we use the first normalization method, though other methods can be applied as well. Meila and Shi (2001) show that probability transition matrix N with t strong clusters will have t piecewise constant eigenvectors. They also suggest using these t eigenvectors in clustering process. We use the information contains in t largest eigenvectors of N (Algorithm 1, line 8-11) and perform K-means clustering algorithm to find the data clusters (Algorithm 1, line 12). 3.2 Propagating Prior Knowledge We use information obtained from comparable c</context>
</contexts>
<marker>Fiedler, 1975</marker>
<rawString>Miroslav Fiedler. 1975. A Property of Eigenvectors of Nonnegative Symmetric Matrices and its Applications to Graph Theory. Czechoslovak Mathematical Journal, 25:619-672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Cross language Text Categorization by acquiring Multilingual Domain Models from Comparable Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts:9-16.</booktitle>
<contexts>
<context position="5399" citStr="Gliozzo and Strapparava, 2005" startWordPosition="806" endWordPosition="810">d at the same time. The key difference between comparable corpora and parallel texts is that documents in comparable corpora are not necessarily translations of each other. They are easier to be acquired, and do not need exhaustive works to be prepared. News agencies often give information in many different languages and can be good sources for comparable corpora. Terms in comparable corpora, being about the same topic, up to some point explain the same concepts in different languages. Pairing comparable corpora with spectral method such as Latent Semantic Analysis has become prevalent, e.g. (Gliozzo and Strapparava, 2005). They rely on the presence of common words and proper nouns among various languages to build a language-independent space. The performance of such method is highly dependent on the languages being used. Here, we present another approach to exploit knowledge in comparable corpora; using propagation method to aid spreading similarity between collections of documents in different languages. Spectral clustering is the task of finding good clusters by using information contained in the eigenvectors of a matrix derived from the data. It has been successfully applied in many applications including i</context>
</contexts>
<marker>Gliozzo, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo and Carlo Strapparava. 2005. Cross language Text Categorization by acquiring Multilingual Domain Models from Comparable Corpora. In Proceedings of the ACL Workshop on Building and Using Parallel Texts:9-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepandar D Kamvar</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Spectral Learning.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="6939" citStr="Kamvar et al. (2003)" startWordPosition="1042" endWordPosition="1045">etwork to solve hypertext classification problem. Semi-supervised clustering enhances clustering task by incorporating prior knowledge to aid clustering process. It allows user to guide the clustering process by giving some feedback to the model. In traditional clustering algorithm, only unlabeled data is used to find assignments of data points to clusters. In semi-supervised clustering, prior knowledge is given to improve performance of the system. The supervision is usually given as pair of must-linked constraints and cannot link constraints, first introduced in (Wagstaff and Cardie, 2000). Kamvar et al. (2003) proposed spectral learning algorithm that can take supervisory information in the form of pairwise constraints or labeled data. Their algorithm is intended to be used in monolingual context, while our algorithm is designed to work in multilingual context. 3 Multilingual Spectral Clustering There have been several works on multilingual document clustering as mention previously in Section 2. Our key contribution here is the propagation method to make spectral clustering algorithm works for multilingual problems. The clustering model exploits the supervisory information by detecting k nearest ne</context>
</contexts>
<marker>Kamvar, Klein, Manning, 2003</marker>
<rawString>Sepandar D. Kamvar, Dan Klein, and Christopher D. Manning. 2003. Spectral Learning. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Sepandar D Kamvar</author>
<author>Christopher D Manning</author>
</authors>
<title>From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering.</title>
<date>2002</date>
<booktitle>In The Nineteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="13616" citStr="Klein et al. (2002)" startWordPosition="2154" endWordPosition="2157">se that we have six documents in two different languages. Initially, documents are only connected with other documents that belong to the same language. The supervisory information tells us that two multilingual documents vi and vj should be connected (Figure 1(a)). We then build an edge between these two documents. Furthermore, we also use this information to build edges between vi and neighbors of vj and likewise (Figure 1(b)). This follows from the hypothesis that bringing together two documents should also bring other documents that are similar to those two closer in our clustering space. Klein et al. (2002) stated that a good clustering algorithm, besides satisfying known constraints, should also be able to satisfy the implications of those constraints. Here, we allow not only instance-level inductive implications, but utilize it to get higher-level inductive implications. In other words, we alter similarity space so that it can detect other clusters by changing the topology of the original space. The process is analogous to shortening the distance between sets of documents in Euclidean space. In vector space model, two documents that are close to each other have high similarity, and thus will b</context>
</contexts>
<marker>Klein, Kamvar, Manning, 2002</marker>
<rawString>Dan Klein, Sepandar D. Kamvar, and Christopher D. Manning. 2002. From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering. In The Nineteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyong Liu</author>
<author>W Bruce Croft</author>
</authors>
<title>Clusterbased Retrieval using Language Models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information</booktitle>
<pages>186--193</pages>
<contexts>
<context position="1380" citStr="Liu and Croft, 2004" startWordPosition="188" endWordPosition="191">d is applied to find the best cuts of the graph. Experimental results show that using limited supervisory information, our method achieves promising clustering results. Furthermore, since the method does not need any language dependent information in the process, our algorithm can be applied to languages in various alphabetical systems. 1 Introduction Document clustering is unsupervised classification of text collections into distinct groups of similar documents. It has been used in many information retrieval tasks, including data organization (Siersdorfer and Sizov, 2004), language modeling (Liu and Croft, 2004), and improving performances of text categorization system (Aggarwal et al., 1999). Advance in internet technology has made the task of managing multilingual documents an intriguing research area. The growth of internet leads to the necessity of organizing documents in various languages. There exist thousands of languages, not to mention countless minor ones. Creating document clustering model for each language is simply unfeasible. We need methods to deal with text collections in diverse languages simultaneously. Multilingual document clustering (MLDC) involves partitioning documents, written</context>
</contexts>
<marker>Liu, Croft, 2004</marker>
<rawString>Xiaoyong Liu and W. Bruce Croft. 2004. Clusterbased Retrieval using Language Models. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval:186-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marinla Meil˘a</author>
<author>Jianbo Shi</author>
</authors>
<title>Learning segmentation by random walks.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems:873-879.</booktitle>
<marker>Meil˘a, Shi, 2000</marker>
<rawString>Marinla Meil˘a and Jianbo Shi. 2000. Learning segmentation by random walks. In Advances in Neural Information Processing Systems:873-879.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marinla Meil˘a</author>
<author>Jianbo Shi</author>
</authors>
<title>A Random Walks View of Spectral Segmentation.</title>
<date>2001</date>
<booktitle>In AI and Statistics (AISTATS).</booktitle>
<marker>Meil˘a, Shi, 2001</marker>
<rawString>Marinla Meil˘a and Jianbo Shi. 2001. A Random Walks View of Spectral Segmentation. In AI and Statistics (AISTATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On Spectral Clustering: Analysis and an algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems (NIPS 14).</booktitle>
<contexts>
<context position="6185" citStr="Ng et al., 2002" startWordPosition="931" endWordPosition="934">dent on the languages being used. Here, we present another approach to exploit knowledge in comparable corpora; using propagation method to aid spreading similarity between collections of documents in different languages. Spectral clustering is the task of finding good clusters by using information contained in the eigenvectors of a matrix derived from the data. It has been successfully applied in many applications including information retrieval (Deerwester et al., 2003) and computer vision (Meila and Shi, 2000). An in-depth analysis of spectral algorithm for clustering problems is given in (Ng et al., 2002). Zhang and Mao (2008) used a related technique called Modularity Eigenmap to extract community structure features from the document network to solve hypertext classification problem. Semi-supervised clustering enhances clustering task by incorporating prior knowledge to aid clustering process. It allows user to guide the clustering process by giving some feedback to the model. In traditional clustering algorithm, only unlabeled data is used to find assignments of data points to clusters. In semi-supervised clustering, prior knowledge is given to improve performance of the system. The supervis</context>
<context position="9508" citStr="Ng et al., 2002" startWordPosition="1451" endWordPosition="1454">rior knowledge in the form of comparable corpora by performing document similarity propagation, presented in Section 3.2 (Algorithm 1, line 4; Algorithm 2, explained in Section 3.2). After propagation, the affinity matrix is post-processed (Algorithm 1, line 6, explained in Section 3.2) before being transformed into transition probability matrix. The transformation can be done using any normalization for spectral methods. Define N = D−1A, as in (Meila and Shi, 2001), where D is the diagonal matrix whose elements Dij = Ej Aij (Algorithm 1, line 7). Alternatively, we can define N = D−1/2AD−1/2 (Ng et al., 2002), or N = (A + dmaxI − D)/dmax (Fiedler, 1975), where dmax is the maximum rowsum of A. For our experiment, we use the first normalization method, though other methods can be applied as well. Meila and Shi (2001) show that probability transition matrix N with t strong clusters will have t piecewise constant eigenvectors. They also suggest using these t eigenvectors in clustering process. We use the information contains in t largest eigenvectors of N (Algorithm 1, line 8-11) and perform K-means clustering algorithm to find the data clusters (Algorithm 1, line 12). 3.2 Propagating Prior Knowledge </context>
</contexts>
<marker>Ng, Jordan, Weiss, 2002</marker>
<rawString>Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2002. On Spectral Clustering: Analysis and an algorithm. In Proceedings of Advances in Neural Information Processing Systems (NIPS 14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Ralf Steinberger</author>
<author>Camelia Ignat</author>
<author>Emilia K¨asper</author>
<author>Irina Temnikova</author>
</authors>
<title>Multilingual and Cross-lingual News Topic Tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<marker>Pouliquen, Steinberger, Ignat, K¨asper, Temnikova, 2004</marker>
<rawString>Bruno Pouliquen, Ralf Steinberger, Camelia Ignat, Emilia K¨asper, and Irina Temnikova. 2004. Multilingual and Cross-lingual News Topic Tracking. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Siersdorfer</author>
<author>Sergej Sizov</author>
</authors>
<title>Restrictive Clustering and Metaclustering for Self-Organizing Document.</title>
<date>2004</date>
<booktitle>In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="1339" citStr="Siersdorfer and Sizov, 2004" startWordPosition="181" endWordPosition="184">dge and merge two language spaces. Spectral method is applied to find the best cuts of the graph. Experimental results show that using limited supervisory information, our method achieves promising clustering results. Furthermore, since the method does not need any language dependent information in the process, our algorithm can be applied to languages in various alphabetical systems. 1 Introduction Document clustering is unsupervised classification of text collections into distinct groups of similar documents. It has been used in many information retrieval tasks, including data organization (Siersdorfer and Sizov, 2004), language modeling (Liu and Croft, 2004), and improving performances of text categorization system (Aggarwal et al., 1999). Advance in internet technology has made the task of managing multilingual documents an intriguing research area. The growth of internet leads to the necessity of organizing documents in various languages. There exist thousands of languages, not to mention countless minor ones. Creating document clustering model for each language is simply unfeasible. We need methods to deal with text collections in diverse languages simultaneously. Multilingual document clustering (MLDC)</context>
</contexts>
<marker>Siersdorfer, Sizov, 2004</marker>
<rawString>Stefan Siersdorfer and Sergej Sizov. 2004. Restrictive Clustering and Metaclustering for Self-Organizing Document. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiri Wagstaff</author>
<author>Claire Cardie</author>
</authors>
<title>Clustering with Instance-level Constraints.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Conference on Machine Learning:1103-1110.</booktitle>
<contexts>
<context position="6917" citStr="Wagstaff and Cardie, 2000" startWordPosition="1038" endWordPosition="1041">features from the document network to solve hypertext classification problem. Semi-supervised clustering enhances clustering task by incorporating prior knowledge to aid clustering process. It allows user to guide the clustering process by giving some feedback to the model. In traditional clustering algorithm, only unlabeled data is used to find assignments of data points to clusters. In semi-supervised clustering, prior knowledge is given to improve performance of the system. The supervision is usually given as pair of must-linked constraints and cannot link constraints, first introduced in (Wagstaff and Cardie, 2000). Kamvar et al. (2003) proposed spectral learning algorithm that can take supervisory information in the form of pairwise constraints or labeled data. Their algorithm is intended to be used in monolingual context, while our algorithm is designed to work in multilingual context. 3 Multilingual Spectral Clustering There have been several works on multilingual document clustering as mention previously in Section 2. Our key contribution here is the propagation method to make spectral clustering algorithm works for multilingual problems. The clustering model exploits the supervisory information by </context>
</contexts>
<marker>Wagstaff, Cardie, 2000</marker>
<rawString>Kiri Wagstaff and Claire Cardie 2000. Clustering with Instance-level Constraints. In Proceedings of the 17th International Conference on Machine Learning:1103-1110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Ping Wei</author>
<author>Christopher C Yang</author>
<author>Chia-Min Lin</author>
</authors>
<title>A Latent Semantic Indexing Based Approach to Multilingual Document Clustering. In Decision Support Systems,</title>
<date>2008</date>
<pages>45--3</pages>
<contexts>
<context position="4409" citStr="Wei et al., 2008" startWordPosition="648" endWordPosition="651"> pages 871–879, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP 2 Related Work Chen and Lin (2000) proposed methods to cluster multilingual documents using translation technology, relying on cross-lingual dictionary and machine-translation system. Multilingual ontology, such as Eurovoc, is also popular for MLDC (Pouliquen et al., 2004). However, such resources are scarce and expensive to build. Several other drawbacks of using this technique include dictionary limitation and word ambiguity. More recently, parallel texts have been used to connect document collections from different languages (Wei et al., 2008). This is done by collapsing columns in a term by document matrix that are translations of each other. Nevertheless, building parallel texts is also expensive and requires a lot of works, hence shifting the paradigm of multilingual works to comparable corpora. Comparable corpora are collections of texts in different languages regarding similar topics produced at the same time. The key difference between comparable corpora and parallel texts is that documents in comparable corpora are not necessarily translations of each other. They are easier to be acquired, and do not need exhaustive works to</context>
</contexts>
<marker>Wei, Yang, Lin, 2008</marker>
<rawString>Chih-Ping Wei, Christopher C. Yang, and Chia-Min Lin. 2008. A Latent Semantic Indexing Based Approach to Multilingual Document Clustering. In Decision Support Systems, 45(3):606-620</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Robert Mao</author>
</authors>
<title>Extracting Community Structure Features for Hypertext Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd IEEE International Conference on Digital Information Management (ICDIM).</booktitle>
<contexts>
<context position="6207" citStr="Zhang and Mao (2008)" startWordPosition="935" endWordPosition="938">ges being used. Here, we present another approach to exploit knowledge in comparable corpora; using propagation method to aid spreading similarity between collections of documents in different languages. Spectral clustering is the task of finding good clusters by using information contained in the eigenvectors of a matrix derived from the data. It has been successfully applied in many applications including information retrieval (Deerwester et al., 2003) and computer vision (Meila and Shi, 2000). An in-depth analysis of spectral algorithm for clustering problems is given in (Ng et al., 2002). Zhang and Mao (2008) used a related technique called Modularity Eigenmap to extract community structure features from the document network to solve hypertext classification problem. Semi-supervised clustering enhances clustering task by incorporating prior knowledge to aid clustering process. It allows user to guide the clustering process by giving some feedback to the model. In traditional clustering algorithm, only unlabeled data is used to find assignments of data points to clusters. In semi-supervised clustering, prior knowledge is given to improve performance of the system. The supervision is usually given a</context>
</contexts>
<marker>Zhang, Mao, 2008</marker>
<rawString>Dell Zhang and Robert Mao. 2008. Extracting Community Structure Features for Hypertext Classification. In Proceedings of the 3rd IEEE International Conference on Digital Information Management (ICDIM).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>