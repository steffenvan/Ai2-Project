<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000596">
<title confidence="0.996206">
Exploiting Comparable Corpora and Bilingual Dictionaries
for Cross-Language Text Categorization
</title>
<note confidence="0.646253333333333">
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
</note>
<email confidence="0.994724">
{gliozzo,strappa}@itc.it
</email>
<sectionHeader confidence="0.99457" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998962571428572">
Cross-language Text Categorization is the
task of assigning semantic classes to docu-
ments written in a target language (e.g. En-
glish) while the system is trained using la-
beled documents in a source language (e.g.
Italian).
In this work we present many solutions ac-
cording to the availability of bilingual re-
sources, and we show that it is possible
to deal with the problem even when no
such resources are accessible. The core
technique relies on the automatic acquisi-
tion of Multilingual Domain Models from
comparable corpora.
Experiments show the effectiveness of our
approach, providing a low cost solution for
the Cross Language Text Categorization
task. In particular, when bilingual dictio-
naries are available the performance of the
categorization gets close to that of mono-
lingual text categorization.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998827">
In the worldwide scenario of the Web age, mul-
tilinguality is a crucial issue to deal with and
to investigate, leading us to reformulate most of
the classical Natural Language Processing (NLP)
problems into a multilingual setting. For in-
stance the classical monolingual Text Categoriza-
tion (TC) problem can be reformulated as a Cross
Language Text Categorization (CLTC) task, in
which the system is trained using labeled exam-
ples in a source language (e.g. English), and it
classifies documents in a different target language
(e.g. Italian).
The applicative interest for the CLTC is im-
mediately clear in the globalized Web scenario.
For example, in the community based trade (e.g.
eBay) it is often necessary to archive texts in dif-
ferent languages by adopting common merceolog-
ical categories, very often defined by collections
of documents in a source language (e.g. English).
Another application along this direction is Cross
Lingual Question Answering, in which it would
be very useful to filter out the candidate answers
according to their topics.
In the literature, this task has been proposed
quite recently (Bel et al., 2003; Gliozzo and Strap-
parava, 2005). In those works, authors exploited
comparable corpora showing promising results. A
more recent work (Rigutini et al., 2005) proposed
the use of Machine Translation techniques to ap-
proach the same task.
Classical approaches for multilingual problems
have been conceived by following two main direc-
tions: (i) knowledge based approaches, mostly im-
plemented by rule based systems and (ii) empirical
approaches, in general relying on statistical learn-
ing from parallel corpora. Knowledge based ap-
proaches are often affected by low accuracy. Such
limitation is mainly due to the problem of tun-
ing large scale multilingual lexical resources (e.g.
MultiWordNet, EuroWordNet) for the specific ap-
plication task (e.g. discarding irrelevant senses,
extending the lexicon with domain specific terms
and their translations). On the other hand, em-
pirical approaches are in general more accurate,
because they can be trained from domain specific
collections of parallel text to represent the appli-
cation needs. There exist many interesting works
about using parallel corpora for multilingual appli-
cations (Melamed, 2001), such as Machine Trans-
lation (Callison-Burch et al., 2004), Cross Lingual
</bodyText>
<page confidence="0.983374">
553
</page>
<note confidence="0.667263">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553–560,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.988670849056604">
Information Retrieval (Littman et al., 1998), and
so on.
However it is not always easy to find or build
parallel corpora. This is the main reason why
the “weaker” notion of comparable corpora is a
matter of recent interest in the field of Computa-
tional Linguistics (Gaussier et al., 2004). In fact,
comparable corpora are easier to collect for most
languages (e.g. collections of international news
agencies), providing a low cost knowledge source
for multilingual applications.
The main problem of adopting comparable cor-
pora for multilingual knowledge acquisition is that
only weaker statistical evidence can be captured.
In fact, while parallel corpora provide stronger
(text-based) statistical evidence to detect transla-
tion pairs by analyzing term co-occurrences in
translated documents, comparable corpora pro-
vides weaker (term-based) evidence, because text
alignments are not available.
In this paper we present some solutions to deal
with CLTC according to the availability of bilin-
gual resources, and we show that it is possible
to deal with the problem even when no such re-
sources are accessible. The core technique relies
on the automatic acquisition of Multilingual Do-
main Models (MDMs) from comparable corpora.
This allows us to define a kernel function (i.e. a
similarity function among documents in different
languages) that is then exploited inside a Support
Vector Machines classification framework. We
also investigate this problem exploiting synset-
aligned multilingual WordNets and standard bilin-
gual dictionaries (e.g. Collins).
Experiments show the effectiveness of our ap-
proach, providing a simple and low cost solu-
tion for the Cross-Language Text Categorization
task. In particular, when bilingual dictionar-
ies/repositories are available, the performance of
the categorization gets close to that of monolin-
gual TC.
The paper is structured as follows. Section 2
briefly discusses the notion of comparable cor-
pora. Section 3 shows how to perform cross-
lingual TC when no bilingual dictionaries are
available and it is possible to rely on a compa-
rability assumption. Section 4 present a more
elaborated technique to acquire MDMs exploiting
bilingual resources, such as MultiWordNet (i.e.
a synset-aligned WordNet) and Collins bilingual
dictionary. Section 5 evaluates our methodolo-
gies and Section 6 concludes the paper suggesting
some future developments.
</bodyText>
<sectionHeader confidence="0.935882" genericHeader="method">
2 Comparable Corpora
</sectionHeader>
<bodyText confidence="0.997530425531915">
Comparable corpora are collections of texts in dif-
ferent languages regarding similar topics (e.g. a
collection of news published by agencies in the
same period). More restrictive requirements are
expected for parallel corpora (i.e. corpora com-
posed of texts which are mutual translations),
while the class of the multilingual corpora (i.e.
collection of texts expressed in different languages
without any additional requirement) is the more
general. Obviously parallel corpora are also com-
parable, while comparable corpora are also multi-
lingual.
In a more precise way, let L =
{L1, L2,... ,Ll} be a set of languages, let
Ti = {ti1, ti2, ... , tin} be a collection of texts ex-
pressed in the language Li E L, and let ψ(th, ti�)
be a function that returns 1 if ti� is the translation
of t�h and 0 otherwise. A multilingual corpus is
the collection of texts defined by T* = Ui Ti. If
the function ψ exists for every text ti� E T* and
for every language L�, and is known, then the
corpus is parallel and aligned at document level.
For the purpose of this paper it is enough to as-
sume that two corpora are comparable, i.e. they
are composed of documents about the same top-
ics and produced in the same period (e.g. possibly
from different news agencies), and it is not known
if a function ψ exists, even if in principle it could
exist and return 1 for a strict subset of document
pairs.
The texts inside comparable corpora, being
about the same topics, should refer to the same
concepts by using various expressions in different
languages. On the other hand, most of the proper
nouns, relevant entities and words that are not yet
lexicalized in the language, are expressed by using
their original terms. As a consequence the same
entities will be denoted with the same words in
different languages, allowing us to automatically
detect couples of translation pairs just by look-
ing at the word shape (Koehn and Knight, 2002).
Our hypothesis is that comparable corpora contain
a large amount of such words, just because texts,
referring to the same topics in different languages,
will often adopt the same terms to denote the same
entities1.
&apos;According to our assumption, a possible additional cri-
</bodyText>
<page confidence="0.991967">
554
</page>
<bodyText confidence="0.999977">
However, the simple presence of these shared
words is not enough to get significant results in
CLTC tasks. As we will see, we need to exploit
these common words to induce a second-order
similarity for the other words in the lexicons.
</bodyText>
<subsectionHeader confidence="0.996861">
2.1 The Multilingual Vector Space Model
</subsectionHeader>
<bodyText confidence="0.999953464285714">
Let T = {t1, t2, ... , tn} be a corpus, and V =
{w1, w2, ... , wk} be its vocabulary. In the mono-
lingual settings, the Vector Space Model (VSM)
is a k-dimensional space Rk, in which the text
tj E T is represented by means of the vector t�
such that the zth component of �tj is the frequency
of wz in tj. The similarity among two texts in the
VSM is then estimated by computing the cosine of
their vectors in the VSM.
Unfortunately, such a model cannot be adopted
in the multilingual settings, because the VSMs of
different languages are mainly disjoint, and the
similarity between two texts in different languages
would always turn out to be zero. This situation
is represented in Figure 1, in which both the left-
bottom and the rigth-upper regions of the matrix
are totally filled by zeros.
On the other hand, the assumption of corpora
comparability seen in Section 2, implies the pres-
ence of a number of common words, represented
by the central rows of the matrix in Figure 1.
As we will show in Section 5, this model is
rather poor because of its sparseness. In the next
section, we will show how to use such words as
seeds to induce a Multilingual Domain VSM, in
which second order relations among terms and
documents in different languages are considered
to improve the similarity estimation.
</bodyText>
<sectionHeader confidence="0.97228" genericHeader="method">
3 Exploiting Comparable Corpora
</sectionHeader>
<bodyText confidence="0.999539357142857">
Looking at the multilingual term-by-document
matrix in Figure 1, a first attempt to merge the
subspaces associated to each language is to exploit
the information provided by external knowledge
sources, such as bilingual dictionaries, e.g. col-
lapsing all the rows representing translation pairs.
In this setting, the similarity among texts in dif-
ferent languages could be estimated by exploit-
ing the classical VSM just described. However,
the main disadvantage of this approach to esti-
mate inter-lingual text similarity is that it strongly
terion to decide whether two corpora are comparable is to
estimate the percentage of terms in the intersection of their
vocabularies.
relies on the availability of a multilingual lexical
resource. For languages with scarce resources a
bilingual dictionary could be not easily available.
Secondly, an important requirement of such a re-
source is its coverage (i.e. the amount of possible
translation pairs that are actually contained in it).
Finally, another problem is that ambiguous terms
could be translated in different ways, leading us to
collapse together rows describing terms with very
different meanings. In Section 4 we will see how
the availability of bilingual dictionaries influences
the techniques and the performance. In the present
Section we want to explore the case in which such
resources are supposed not available.
</bodyText>
<subsectionHeader confidence="0.993704">
3.1 Multilingual Domain Model
</subsectionHeader>
<bodyText confidence="0.99798175">
A MDM is a multilingual extension of the concept
of Domain Model. In the literature, Domain Mod-
els have been introduced to represent ambiguity
and variability (Gliozzo et al., 2004) and success-
fully exploited in many NLP applications, such as
Word Sense Disambiguation (Strapparava et al.,
2004), Text Categorization and Term Categoriza-
tion.
A Domain Model is composed of soft clusters
of terms. Each cluster represents a semantic do-
main, i.e. a set of terms that often co-occur in
texts having similar topics. Such clusters iden-
tify groups of words belonging to the same seman-
tic field, and thus highly paradigmatically related.
MDMs are Domain Models containing terms in
more than one language.
A MDM is represented by a matrix D, contain-
ing the degree of association among terms in all
the languages and domains, as illustrated in Table
1. For example the term virus is associated to both
</bodyText>
<table confidence="0.98436025">
MEDICINE COMPUTER SCIENCE
HIVe/i 1 0
AIDSe/i 1 0
viruse/i 0.5 0.5
hospitale 1 0
laptope 0 1
Microsofte/i 0 1
clinicai 1 0
</table>
<tableCaption confidence="0.999714">
Table 1: Example of Domain Matrix. we denotes
</tableCaption>
<bodyText confidence="0.9947748">
English terms, wi Italian terms and we/i the com-
mon terms to both languages.
the domain COMPUTER SCIENCE and the domain
MEDICINE while the domain MEDICINE is associ-
ated to both the terms AIDS and HIV. Inter-lingual
</bodyText>
<page confidence="0.986027">
555
</page>
<table confidence="0.999238">
� English texts Italian texts �
� te1 te2 · · · te�−1 te� ti1 ti2 · · · ti,,,−1 ti,,, �
� we1 0 1 ··· 0 1 0 0 ··· �
� � � we2 1 1 ··· 1 0 0 ... � � �
English
� � Lexicon � �
� � � ... ... 0 ... � � �
� � � we 0 1 ··· 0 0 ... 0 � � �
p−1
� � wep 0 1 ··· 0 0 ··· 0 0 � �
� � common wi e/i 0 1 ··· 0 0 0 0 ··· 1 0 � �
w 1
� � � ... � � �
� � wi1 0 0 ··· 0 1 ··· 1 1 � �
� � wi2 0 ... 1 1 ··· 0 1 � �
Italian
� � Lexicon � �
� � � ... ... 0 ... � � �
� � wi�−1 ... 0 0 1 ··· 0 1 � �
� �
wi� ··· 0 0 0 1 ··· 1 0
</table>
<figureCaption confidence="0.99738">
Figure 1: Multilingual term-by-document matrix
</figureCaption>
<bodyText confidence="0.999688476190476">
domain relations are captured by placing differ-
ent terms of different languages in the same se-
mantic field (as for example HIV e/i, AIDSe/i,
hospitale, and clinicai). Most of the named enti-
ties, such as Microsoft and HIV are expressed us-
ing the same string in both languages.
Formally, let V i = {wi 1, wi2, ... , wiki} be the
vocabulary of the corpus Ti composed of doc-
ument expressed in the language Li, let V ∗ =
Ui V i be the set of all the terms in all the lan-
guages, and let k∗ = |V ∗ |be the cardinality of
this set. Let D = {D1, D2,..., Dd} be a set of do-
mains. A DM is fully defined by a k∗ x d domain
matrix D representing in each cell di,z the domain
relevance of the ith term of V ∗ with respect to the
domain Dz. The domain matrix D is used to de-
fine a function D : Rk* —* Rd, that maps the doc-
ument vectors &apos;tj expressed into the multilingual
classical VSM (see Section 2.1), into the vectors
�t0j in the multilingual domain VSM. The function
D is defined bye
</bodyText>
<equation confidence="0.77137175">
D(�tj) = �tj(IIDFD) = �t0 (1)
j
where IIDF is a diagonal matrix such that iIDF
i,l =
</equation>
<bodyText confidence="0.8052145">
IDF(wli), t� is represented as a row vector, and
IDF(wli) is the Inverse Document Frequency of
</bodyText>
<footnote confidence="0.936384">
2In (Wong et al., 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM
is a particular instance.
</footnote>
<equation confidence="0.53575">
wli evaluated in the corpus Tl.
</equation>
<bodyText confidence="0.995808034482759">
In this work we exploit Latent Semantic Anal-
ysis (LSA) (Deerwester et al., 1990) to automat-
ically acquire a MDM from comparable corpora.
LSA is an unsupervised technique for estimating
the similarity among texts and terms in a large
corpus. In the monolingual settings LSA is per-
formed by means of a Singular Value Decom-
position (SVD) of the term-by-document matrix
T describing the corpus. SVD decomposes the
term-by-document matrix T into three matrixes
T ^_ VEk&apos;UT where Ek&apos; is the diagonal k x k
matrix containing the highest k0 « k eigenval-
ues of T, and all the remaining elements are set
to 0. The parameter k0 is the dimensionality of
the Domain VSM and can be fixed in advance (i.e.
k0 = d).
In the literature (Littman et al., 1998) LSA
has been used in multilingual settings to define
a multilingual space in which texts in different
languages can be represented and compared. In
that work LSA strongly relied on the availability
of aligned parallel corpora: documents in all the
languages are represented in a term-by-document
matrix (see Figure 1) and then the columns corre-
sponding to sets of translated documents are col-
lapsed (i.e. they are substituted by their sum) be-
fore starting the LSA process. The effect of this
step is to merge the subspaces (i.e. the right and
the left sectors of the matrix in Figure 1) in which
</bodyText>
<page confidence="0.996413">
556
</page>
<bodyText confidence="0.999355851851852">
the documents have been originally represented.
In this paper we propose a variation of this strat-
egy, performing a multilingual LSA in the case in
which an aligned parallel corpus is not available.
It exploits the presence of common words among
different languages in the term-by-document ma-
trix. The SVD process has the effect of creating a
LSA space in which documents in both languages
are represented. Of course, the higher the number
of common words, the more information will be
provided to the SVD algorithm to find common
LSA dimension for the two languages. The re-
sulting LSA dimensions can be perceived as mul-
tilingual clusters of terms and document. LSA can
then be used to define a Multilingual Domain Ma-
trix DLSA. For further details see (Gliozzo and
Strapparava, 2005).
As Kernel Methods are the state-of-the-art su-
pervised framework for learning and they have
been successfully adopted to approach the TC task
(Joachims, 2002), we chose this framework to per-
form all our experiments, in particular Support
Vector Machines3. Taking into account the exter-
nal knowledge provided by a MDM it is possible
estimate the topic similarity among two texts ex-
pressed in different languages, with the following
kernel:
</bodyText>
<equation confidence="0.89805">
KD(ti, tj) =
�(D(tj), D(tj))(D(ti), D(ti))
(2)
</equation>
<bodyText confidence="0.999837125">
where D is defined as in equation 1.
Note that when we want to estimate the similar-
ity in the standard Multilingual VSM, as described
in Section 2.1, we can use a simple bag of words
kernel. The BoW kernel is a particular case of the
Domain Kernel, in which D = I, and I is the iden-
tity matrix. In the evaluation typically we consider
the BoW Kernel as a baseline.
</bodyText>
<sectionHeader confidence="0.987547" genericHeader="method">
4 Exploiting Bilingual Dictionaries
</sectionHeader>
<bodyText confidence="0.9997155">
When bilingual resources are available it is possi-
ble to augment the the “common” portion of the
matrix in Figure 1. In our experiments we ex-
ploit two alternative multilingual resources: Mul-
tiWordNet and the Collins English-Italian bilin-
gual dictionary.
</bodyText>
<footnote confidence="0.904334">
3We adopted the efficient implementation freely available
at http://svmlight.joachims.org/.
</footnote>
<bodyText confidence="0.999232425531915">
MultiWordNet4. It is a multilingual computa-
tional lexicon, conceived to be strictly aligned
with the Princeton WordNet. The available lan-
guages are Italian, Spanish, Hebrew and Roma-
nian. In our experiment we used the English and
the Italian components. The last version of the
Italian WordNet contains around 58,000 Italian
word senses and 41,500 lemmas organized into
32,700 synsets aligned whenever possible with
WordNet English synsets. The Italian synsets
are created in correspondence with the Princeton
WordNet synsets, whenever possible, and seman-
tic relations are imported from the corresponding
English synsets. This implies that the synset index
structure is the same for the two languages.
Thus for the all the monosemic words, we aug-
ment each text in the dataset with the correspond-
ing synset-id, which act as an expansion of the
“common” terms of the matrix in Figure 1. Adopt-
ing the methodology described in Section 3.1, we
exploit these common sense-indexing to induce
a second-order similarity for the other terms in
the lexicons. We evaluate the performance of the
cross-lingual text categorization, using both the
BoW Kernel and the Multilingual Domain Kernel,
observing that also in this case the leverage of the
external knowledge brought by the MDM is effec-
tive.
It is also possible to augment each text with all
the synset-ids of all the words (i.e. monosemic and
polysemic) present in the dataset, hoping that the
SVM machine learning device cut off the noise
due to the inevitable spurious senses introduced in
the training examples. Obviously in this case, dif-
ferently from the “monosemic” enrichment seen
above, it does not make sense to apply any dimen-
sionality reduction supplied by the Multilingual
Domain Model (i.e. the resulting second-order re-
lations among terms and documents produced on
a such “extended” corpus should not be meaning-
ful)5.
Collins. The Collins machine-readable bilingual
dictionary is a medium size dictionary includ-
ing 37,727 headwords in the English Section and
32,602 headwords in the Italian Section.
This is a traditional dictionary, without sense in-
dexing like the WordNet repository. In this case
</bodyText>
<footnote confidence="0.9809632">
4Available at http://multiwordnet.itc.it.
5The use of a WSD system would help in this issue. How-
ever the rationale of this paper is to see how far it is possible
to go with very few resources. And we suppose that a multi-
lingual all-words WSD system is not easily available.
</footnote>
<equation confidence="0.204769">
(D(ti), D(tj))
</equation>
<page confidence="0.810315">
557
</page>
<table confidence="0.999582428571429">
Categories English Total Italian Total
Training Test Training Test
Quality of Life 5759 1989 7748 5781 1901 7682
Made in Italy 5711 1864 7575 6111 2068 8179
Tourism 5731 1857 7588 6090 2015 8105
Culture and School 3665 1245 4910 6284 2104 8388
Total 20866 6955 27821 24266 8088 32354
</table>
<tableCaption confidence="0.999003">
Table 2: Number of documents in the data set partitions
</tableCaption>
<bodyText confidence="0.9996784">
we follow the way, for each text of one language,
to augment all the present words with the transla-
tion words found in the dictionary. For the same
reason, we chose not to exploit the MDM, while
experimenting along this way.
</bodyText>
<sectionHeader confidence="0.996831" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999812514285714">
The CLTC task has been rarely attempted in the
literature, and standard evaluation benchmark are
not available. For this reason, we developed
an evaluation task by adopting a news corpus
kindly put at our disposal by AdnKronos, an im-
portant Italian news provider. The corpus con-
sists of 32,354 Italian and 27,821 English news
partitioned by AdnKronos into four fixed cat-
egories: QUALITY OF LIFE, MADE IN ITALY,
TOURISM, CULTURE AND SCHOOL. The En-
glish and the Italian corpora are comparable, in
the sense stated in Section 2, i.e. they cover the
same topics and the same period of time. Some
news stories are translated in the other language
(but no alignment indication is given), some oth-
ers are present only in the English set, and some
others only in the Italian. The average length of
the news stories is about 300 words. We randomly
split both the English and Italian part into 75%
training and 25% test (see Table 2). We processed
the corpus with PoS taggers, keeping only nouns,
verbs, adjectives and adverbs.
Table 3 reports the vocabulary dimensions of
the English and Italian training partitions, the vo-
cabulary of the merged training, and how many
common lemmata are present (about 14% of the
total). Among the common lemmata, 97% are
nouns and most of them are proper nouns. Thus
the initial term-by-document matrix is a 43,384 ×
45,132 matrix, while the DLSA was acquired us-
ing 400 dimensions.
As far as the CLTC task is concerned, we tried
the many possible options. In all the cases we
trained on the English part and we classified the
Italian part, and we trained on the Italian and clas-
</bodyText>
<table confidence="0.9980798">
# lemmata
English training 22,704
Italian training 26,404
English + Italian 43,384
common lemmata 5,724
</table>
<tableCaption confidence="0.821978">
Table 3: Number of lemmata in the training parts
of the corpus
</tableCaption>
<bodyText confidence="0.997735333333333">
sified on the English part. When used, the MDM
was acquired running the SVD only on the joint
(English and Italian) training parts.
Using only comparable corpora. Figure 2 re-
ports the performance without any use of bilingual
dictionaries. Each graph show the learning curves
respectively using a BoW kernel (that is consid-
ered here as a baseline) and the multilingual do-
main kernel. We can observe that the latter largely
outperform a standard BoW approach. Analyzing
the learning curves, it is worth noting that when
the quantity of training increases, the performance
becomes better and better for the Multilingual Do-
main Kernel, suggesting that with more available
training it could be possible to improve the results.
Using bilingual dictionaries. Figure 3 reports
the learning curves exploiting the addition of the
synset-ids of the monosemic words in the corpus.
As expected the use of a multilingual repository
improves the classification results. Note that the
MDM outperforms the BoW kernel.
Figure 4 shows the results adding in the English
and Italian parts of the corpus all the synset-ids
(i.e. monosemic and polisemic) and all the transla-
tions found in the Collins dictionary respectively.
These are the best results we get in our experi-
ments. In these figures we report also the perfor-
mance of the corresponding monolingual TC (we
used the SVM with the BoW kernel), which can
be considered as an upper bound. We can observe
that the CLTC results are quite close to the perfor-
mance obtained in the monolingual classification
tasks.
</bodyText>
<page confidence="0.976779">
558
</page>
<figure confidence="0.999252318181818">
F1 measure
F1 measure
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data (train on English, test on Italian)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data (train on Italian, test on English)
0.7
0.6
0.5
0.4
0.3
0.2
Multilingual Domain Kernel
Bow Kernel
0.7
0.6
0.5
0.4
0.3
0.2
Multilingual Domain Kernel
Bow Kernel
</figure>
<figureCaption confidence="0.954645">
Figure 2: Cross-language learning curves: no use of bilingual dictionaries
</figureCaption>
<figure confidence="0.998855727272727">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data (train on English, test on Italian)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data (train on Italian, test on English)
F1 measure
0.7
0.6
0.5
0.4
0.3
0.2
Multilingual Domain Kernel
Bow Kernel
F1 measure
0.7
0.6
0.5
0.4
0.3
0.2
Multilingual Domain Kernel
Bow Kernel
</figure>
<figureCaption confidence="0.994457">
Figure 3: Cross-language learning curves: monosemic synsets from MultiWordNet
</figureCaption>
<figure confidence="0.999383448275862">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data (train on English, test on Italian)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data (train on Italian, test on English)
Monolingual (Italian) TC
Collins
MultiWordNet
F1 measure
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1
Monolingual (English) TC
Collins
MultiWordNet
F1 measure 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
</figure>
<figureCaption confidence="0.999551">
Figure 4: Cross-language learning curves: all synsets from MultiWordNet // All translations from Collins
</figureCaption>
<page confidence="0.996489">
559
</page>
<sectionHeader confidence="0.994462" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.998646125">
In this paper we have shown that the problem of
cross-language text categorization on comparable
corpora is a feasible task. In particular, it is pos-
sible to deal with it even when no bilingual re-
sources are available. On the other hand when it is
possible to exploit bilingual repositories, such as a
synset-aligned WordNet or a bilingual dictionary,
the obtained performance is close to that achieved
for the monolingual task. In any case we think
that our methodology is low-cost and simple, and
it can represent a technologically viable solution
for multilingual problems. For the future we try to
explore also the use of a word sense disambigua-
tion all-words system. We are confident that even
with the actual state-of-the-art WSD performance,
we can improve the actual results.
</bodyText>
<sectionHeader confidence="0.999026" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9944775">
This work has been partially supported by the ON-
TOTEXT (From Text to Knowledge for the Se-
mantic Web) project, funded by the Autonomous
Province of Trento under the FUP-2004 program.
</bodyText>
<sectionHeader confidence="0.998287" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805807692308">
N. Bel, C. Koster, and M. Villegas. 2003. Cross-
lingual text categorization. In Proceedings of Eu-
ropean Conference on Digital Libraries (ECDL),
Trondheim, August.
C. Callison-Burch, D. Talbot, and M. Osborne.
2004. Statistical machine translation with word-and
sentence-aligned parallel corpora. In Proceedings of
ACL-04, Barcelona, Spain, July.
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society
for Information Science, 41(6):391–407.
E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and
H. Dejean. 2004. A geometric view on bilingual
lexicon extraction from comparable corpora. In Pro-
ceedings of ACL-04, Barcelona, Spain, July.
A. Gliozzo and C. Strapparava. 2005. Cross language
text categorization by acquiring multilingual domain
models from comparable corpora. In Proc. of the
ACL Workshop on Building and Using Parallel Texts
(in conjunction ofACL-05), University of Michigan,
Ann Arbor, June.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275–299.
T. Joachims. 2002. Learning to Classify Text using
Support Vector Machines. Kluwer Academic Pub-
lishers.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings
ofACL Workshop on Unsupervised Lexical Acquisi-
tion, Philadelphia, July.
M. Littman, S. Dumais, and T. Landauer. 1998. Auto-
matic cross-language information retrieval using la-
tent semantic indexing. In G. Grefenstette, editor,
Cross Language Information Retrieval, pages 51–
62. Kluwer Academic Publishers.
D. Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
L. Rigutini, M. Maggini, and B. Liu. 2005. An EM
based training algorithm for cross-language text cat-
egorizaton. In Proceedings of Web Intelligence Con-
ference (WI-2005), Compi`egne, France, September.
C. Strapparava, A. Gliozzo, and C. Giuliano.
2004. Pattern abstraction and term similarity for
word sense disambiguation. In Proceedings of
SENSEVAL-3, Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985.
Generalized vector space model in information re-
trieval. In Proceedings of the 81h ACM SIGIR Con-
ference.
</reference>
<page confidence="0.996964">
560
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343309">
<title confidence="0.9997595">Exploiting Comparable Corpora and Bilingual Dictionaries for Cross-Language Text Categorization</title>
<author confidence="0.997917">Alfio Gliozzo</author>
<author confidence="0.997917">Carlo Strapparava</author>
<affiliation confidence="0.379111">ITC-Irst</affiliation>
<address confidence="0.628274">via Sommarive, I-38050, Trento, ITALY</address>
<abstract confidence="0.998057454545454">Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language (e.g. English) while the system is trained using labeled documents in a source language (e.g. Italian). In this work we present many solutions according to the availability of bilingual resources, and we show that it is possible to deal with the problem even when no such resources are accessible. The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora. Experiments show the effectiveness of our approach, providing a low cost solution for the Cross Language Text Categorization task. In particular, when bilingual dictionaries are available the performance of the categorization gets close to that of monolingual text categorization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bel</author>
<author>C Koster</author>
<author>M Villegas</author>
</authors>
<title>Crosslingual text categorization.</title>
<date>2003</date>
<booktitle>In Proceedings of European Conference on Digital Libraries (ECDL),</booktitle>
<location>Trondheim,</location>
<contexts>
<context position="2166" citStr="Bel et al., 2003" startWordPosition="330" endWordPosition="333">t language (e.g. Italian). The applicative interest for the CLTC is immediately clear in the globalized Web scenario. For example, in the community based trade (e.g. eBay) it is often necessary to archive texts in different languages by adopting common merceological categories, very often defined by collections of documents in a source language (e.g. English). Another application along this direction is Cross Lingual Question Answering, in which it would be very useful to filter out the candidate answers according to their topics. In the literature, this task has been proposed quite recently (Bel et al., 2003; Gliozzo and Strapparava, 2005). In those works, authors exploited comparable corpora showing promising results. A more recent work (Rigutini et al., 2005) proposed the use of Machine Translation techniques to approach the same task. Classical approaches for multilingual problems have been conceived by following two main directions: (i) knowledge based approaches, mostly implemented by rule based systems and (ii) empirical approaches, in general relying on statistical learning from parallel corpora. Knowledge based approaches are often affected by low accuracy. Such limitation is mainly due t</context>
</contexts>
<marker>Bel, Koster, Villegas, 2003</marker>
<rawString>N. Bel, C. Koster, and M. Villegas. 2003. Crosslingual text categorization. In Proceedings of European Conference on Digital Libraries (ECDL), Trondheim, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>D Talbot</author>
<author>M Osborne</author>
</authors>
<title>Statistical machine translation with word-and sentence-aligned parallel corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3362" citStr="Callison-Burch et al., 2004" startWordPosition="508" endWordPosition="511"> Such limitation is mainly due to the problem of tuning large scale multilingual lexical resources (e.g. MultiWordNet, EuroWordNet) for the specific application task (e.g. discarding irrelevant senses, extending the lexicon with domain specific terms and their translations). On the other hand, empirical approaches are in general more accurate, because they can be trained from domain specific collections of parallel text to represent the application needs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation (Callison-Burch et al., 2004), Cross Lingual 553 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553–560, Sydney, July 2006. c�2006 Association for Computational Linguistics Information Retrieval (Littman et al., 1998), and so on. However it is not always easy to find or build parallel corpora. This is the main reason why the “weaker” notion of comparable corpora is a matter of recent interest in the field of Computational Linguistics (Gaussier et al., 2004). In fact, comparable corpora are easier to collect for most languages (e.g. collections of int</context>
</contexts>
<marker>Callison-Burch, Talbot, Osborne, 2004</marker>
<rawString>C. Callison-Burch, D. Talbot, and M. Osborne. 2004. Statistical machine translation with word-and sentence-aligned parallel corpora. In Proceedings of ACL-04, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="14412" citStr="Deerwester et al., 1990" startWordPosition="2464" endWordPosition="2467"> —* Rd, that maps the document vectors &apos;tj expressed into the multilingual classical VSM (see Section 2.1), into the vectors �t0j in the multilingual domain VSM. The function D is defined bye D(�tj) = �tj(IIDFD) = �t0 (1) j where IIDF is a diagonal matrix such that iIDF i,l = IDF(wli), t� is represented as a row vector, and IDF(wli) is the Inverse Document Frequency of 2In (Wong et al., 1985) the formula 1 is used to define a Generalized Vector Space Model, of which the Domain VSM is a particular instance. wli evaluated in the corpus Tl. In this work we exploit Latent Semantic Analysis (LSA) (Deerwester et al., 1990) to automatically acquire a MDM from comparable corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a large corpus. In the monolingual settings LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. SVD decomposes the term-by-document matrix T into three matrixes T ^_ VEk&apos;UT where Ek&apos; is the diagonal k x k matrix containing the highest k0 « k eigenvalues of T, and all the remaining elements are set to 0. The parameter k0 is the dimensionality of the Domain VSM and can be fixed in adv</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gaussier</author>
<author>J M Renders</author>
<author>I Matveeva</author>
<author>C Goutte</author>
<author>H Dejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3867" citStr="Gaussier et al., 2004" startWordPosition="587" endWordPosition="590">llel corpora for multilingual applications (Melamed, 2001), such as Machine Translation (Callison-Burch et al., 2004), Cross Lingual 553 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553–560, Sydney, July 2006. c�2006 Association for Computational Linguistics Information Retrieval (Littman et al., 1998), and so on. However it is not always easy to find or build parallel corpora. This is the main reason why the “weaker” notion of comparable corpora is a matter of recent interest in the field of Computational Linguistics (Gaussier et al., 2004). In fact, comparable corpora are easier to collect for most languages (e.g. collections of international news agencies), providing a low cost knowledge source for multilingual applications. The main problem of adopting comparable corpora for multilingual knowledge acquisition is that only weaker statistical evidence can be captured. In fact, while parallel corpora provide stronger (text-based) statistical evidence to detect translation pairs by analyzing term co-occurrences in translated documents, comparable corpora provides weaker (term-based) evidence, because text alignments are not avail</context>
</contexts>
<marker>Gaussier, Renders, Matveeva, Goutte, Dejean, 2004</marker>
<rawString>E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and H. Dejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings of ACL-04, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Strapparava</author>
</authors>
<title>Cross language text categorization by acquiring multilingual domain models from comparable corpora.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Building and Using Parallel Texts (in conjunction ofACL-05),</booktitle>
<institution>University of Michigan,</institution>
<location>Ann Arbor,</location>
<contexts>
<context position="2198" citStr="Gliozzo and Strapparava, 2005" startWordPosition="334" endWordPosition="338">talian). The applicative interest for the CLTC is immediately clear in the globalized Web scenario. For example, in the community based trade (e.g. eBay) it is often necessary to archive texts in different languages by adopting common merceological categories, very often defined by collections of documents in a source language (e.g. English). Another application along this direction is Cross Lingual Question Answering, in which it would be very useful to filter out the candidate answers according to their topics. In the literature, this task has been proposed quite recently (Bel et al., 2003; Gliozzo and Strapparava, 2005). In those works, authors exploited comparable corpora showing promising results. A more recent work (Rigutini et al., 2005) proposed the use of Machine Translation techniques to approach the same task. Classical approaches for multilingual problems have been conceived by following two main directions: (i) knowledge based approaches, mostly implemented by rule based systems and (ii) empirical approaches, in general relying on statistical learning from parallel corpora. Knowledge based approaches are often affected by low accuracy. Such limitation is mainly due to the problem of tuning large sc</context>
<context position="16456" citStr="Gliozzo and Strapparava, 2005" startWordPosition="2817" endWordPosition="2820"> an aligned parallel corpus is not available. It exploits the presence of common words among different languages in the term-by-document matrix. The SVD process has the effect of creating a LSA space in which documents in both languages are represented. Of course, the higher the number of common words, the more information will be provided to the SVD algorithm to find common LSA dimension for the two languages. The resulting LSA dimensions can be perceived as multilingual clusters of terms and document. LSA can then be used to define a Multilingual Domain Matrix DLSA. For further details see (Gliozzo and Strapparava, 2005). As Kernel Methods are the state-of-the-art supervised framework for learning and they have been successfully adopted to approach the TC task (Joachims, 2002), we chose this framework to perform all our experiments, in particular Support Vector Machines3. Taking into account the external knowledge provided by a MDM it is possible estimate the topic similarity among two texts expressed in different languages, with the following kernel: KD(ti, tj) = �(D(tj), D(tj))(D(ti), D(ti)) (2) where D is defined as in equation 1. Note that when we want to estimate the similarity in the standard Multilingu</context>
</contexts>
<marker>Gliozzo, Strapparava, 2005</marker>
<rawString>A. Gliozzo and C. Strapparava. 2005. Cross language text categorization by acquiring multilingual domain models from comparable corpora. In Proc. of the ACL Workshop on Building and Using Parallel Texts (in conjunction ofACL-05), University of Michigan, Ann Arbor, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Strapparava</author>
<author>I Dagan</author>
</authors>
<title>Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language,</title>
<date>2004</date>
<pages>18--275</pages>
<contexts>
<context position="11354" citStr="Gliozzo et al., 2004" startWordPosition="1810" endWordPosition="1813">ntained in it). Finally, another problem is that ambiguous terms could be translated in different ways, leading us to collapse together rows describing terms with very different meanings. In Section 4 we will see how the availability of bilingual dictionaries influences the techniques and the performance. In the present Section we want to explore the case in which such resources are supposed not available. 3.1 Multilingual Domain Model A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such as Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. A Domain Model is composed of soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identify groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among terms</context>
</contexts>
<marker>Gliozzo, Strapparava, Dagan, 2004</marker>
<rawString>A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language, 18:275–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning to Classify Text using Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="16615" citStr="Joachims, 2002" startWordPosition="2843" endWordPosition="2844">t of creating a LSA space in which documents in both languages are represented. Of course, the higher the number of common words, the more information will be provided to the SVD algorithm to find common LSA dimension for the two languages. The resulting LSA dimensions can be perceived as multilingual clusters of terms and document. LSA can then be used to define a Multilingual Domain Matrix DLSA. For further details see (Gliozzo and Strapparava, 2005). As Kernel Methods are the state-of-the-art supervised framework for learning and they have been successfully adopted to approach the TC task (Joachims, 2002), we chose this framework to perform all our experiments, in particular Support Vector Machines3. Taking into account the external knowledge provided by a MDM it is possible estimate the topic similarity among two texts expressed in different languages, with the following kernel: KD(ti, tj) = �(D(tj), D(tj))(D(ti), D(ti)) (2) where D is defined as in equation 1. Note that when we want to estimate the similarity in the standard Multilingual VSM, as described in Section 2.1, we can use a simple bag of words kernel. The BoW kernel is a particular case of the Domain Kernel, in which D = I, and I i</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Learning to Classify Text using Support Vector Machines. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<location>Philadelphia,</location>
<contexts>
<context position="7891" citStr="Koehn and Knight, 2002" startWordPosition="1233" endWordPosition="1236">even if in principle it could exist and return 1 for a strict subset of document pairs. The texts inside comparable corpora, being about the same topics, should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing us to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that comparable corpora contain a large amount of such words, just because texts, referring to the same topics in different languages, will often adopt the same terms to denote the same entities1. &apos;According to our assumption, a possible additional cri554 However, the simple presence of these shared words is not enough to get significant results in CLTC tasks. As we will see, we need to exploit these common words to induce a second-order similarity for the other words in the lexicons. 2.1 The Multilingual Vector Space Model Let T = {t1, t2, ... , tn} be a corpus, and V = {w</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>P. Koehn and K. Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings ofACL Workshop on Unsupervised Lexical Acquisition, Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Littman</author>
<author>S Dumais</author>
<author>T Landauer</author>
</authors>
<title>Automatic cross-language information retrieval using latent semantic indexing.</title>
<date>1998</date>
<booktitle>Cross Language Information Retrieval,</booktitle>
<pages>51--62</pages>
<editor>In G. Grefenstette, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3623" citStr="Littman et al., 1998" startWordPosition="543" endWordPosition="546">tions). On the other hand, empirical approaches are in general more accurate, because they can be trained from domain specific collections of parallel text to represent the application needs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation (Callison-Burch et al., 2004), Cross Lingual 553 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553–560, Sydney, July 2006. c�2006 Association for Computational Linguistics Information Retrieval (Littman et al., 1998), and so on. However it is not always easy to find or build parallel corpora. This is the main reason why the “weaker” notion of comparable corpora is a matter of recent interest in the field of Computational Linguistics (Gaussier et al., 2004). In fact, comparable corpora are easier to collect for most languages (e.g. collections of international news agencies), providing a low cost knowledge source for multilingual applications. The main problem of adopting comparable corpora for multilingual knowledge acquisition is that only weaker statistical evidence can be captured. In fact, while paral</context>
<context position="15072" citStr="Littman et al., 1998" startWordPosition="2582" endWordPosition="2585">mparable corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a large corpus. In the monolingual settings LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. SVD decomposes the term-by-document matrix T into three matrixes T ^_ VEk&apos;UT where Ek&apos; is the diagonal k x k matrix containing the highest k0 « k eigenvalues of T, and all the remaining elements are set to 0. The parameter k0 is the dimensionality of the Domain VSM and can be fixed in advance (i.e. k0 = d). In the literature (Littman et al., 1998) LSA has been used in multilingual settings to define a multilingual space in which texts in different languages can be represented and compared. In that work LSA strongly relied on the availability of aligned parallel corpora: documents in all the languages are represented in a term-by-document matrix (see Figure 1) and then the columns corresponding to sets of translated documents are collapsed (i.e. they are substituted by their sum) before starting the LSA process. The effect of this step is to merge the subspaces (i.e. the right and the left sectors of the matrix in Figure 1) in which 556</context>
</contexts>
<marker>Littman, Dumais, Landauer, 1998</marker>
<rawString>M. Littman, S. Dumais, and T. Landauer. 1998. Automatic cross-language information retrieval using latent semantic indexing. In G. Grefenstette, editor, Cross Language Information Retrieval, pages 51– 62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Melamed</author>
</authors>
<title>Empirical Methods for Exploiting Parallel Texts.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="3303" citStr="Melamed, 2001" startWordPosition="501" endWordPosition="502">pproaches are often affected by low accuracy. Such limitation is mainly due to the problem of tuning large scale multilingual lexical resources (e.g. MultiWordNet, EuroWordNet) for the specific application task (e.g. discarding irrelevant senses, extending the lexicon with domain specific terms and their translations). On the other hand, empirical approaches are in general more accurate, because they can be trained from domain specific collections of parallel text to represent the application needs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation (Callison-Burch et al., 2004), Cross Lingual 553 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553–560, Sydney, July 2006. c�2006 Association for Computational Linguistics Information Retrieval (Littman et al., 1998), and so on. However it is not always easy to find or build parallel corpora. This is the main reason why the “weaker” notion of comparable corpora is a matter of recent interest in the field of Computational Linguistics (Gaussier et al., 2004). In fact, comparable corpora are ea</context>
</contexts>
<marker>Melamed, 2001</marker>
<rawString>D. Melamed. 2001. Empirical Methods for Exploiting Parallel Texts. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rigutini</author>
<author>M Maggini</author>
<author>B Liu</author>
</authors>
<title>An EM based training algorithm for cross-language text categorizaton.</title>
<date>2005</date>
<booktitle>In Proceedings of Web Intelligence Conference (WI-2005),</booktitle>
<location>Compi`egne, France,</location>
<contexts>
<context position="2322" citStr="Rigutini et al., 2005" startWordPosition="353" endWordPosition="356">sed trade (e.g. eBay) it is often necessary to archive texts in different languages by adopting common merceological categories, very often defined by collections of documents in a source language (e.g. English). Another application along this direction is Cross Lingual Question Answering, in which it would be very useful to filter out the candidate answers according to their topics. In the literature, this task has been proposed quite recently (Bel et al., 2003; Gliozzo and Strapparava, 2005). In those works, authors exploited comparable corpora showing promising results. A more recent work (Rigutini et al., 2005) proposed the use of Machine Translation techniques to approach the same task. Classical approaches for multilingual problems have been conceived by following two main directions: (i) knowledge based approaches, mostly implemented by rule based systems and (ii) empirical approaches, in general relying on statistical learning from parallel corpora. Knowledge based approaches are often affected by low accuracy. Such limitation is mainly due to the problem of tuning large scale multilingual lexical resources (e.g. MultiWordNet, EuroWordNet) for the specific application task (e.g. discarding irrel</context>
</contexts>
<marker>Rigutini, Maggini, Liu, 2005</marker>
<rawString>L. Rigutini, M. Maggini, and B. Liu. 2005. An EM based training algorithm for cross-language text categorizaton. In Proceedings of Web Intelligence Conference (WI-2005), Compi`egne, France, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Gliozzo</author>
<author>C Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of SENSEVAL-3,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="11468" citStr="Strapparava et al., 2004" startWordPosition="1827" endWordPosition="1830">g us to collapse together rows describing terms with very different meanings. In Section 4 we will see how the availability of bilingual dictionaries influences the techniques and the performance. In the present Section we want to explore the case in which such resources are supposed not available. 3.1 Multilingual Domain Model A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such as Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. A Domain Model is composed of soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identify groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among terms in all the languages and domains, as illustrated in Table 1. For example the term virus is associated to both MED</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation. In Proceedings of SENSEVAL-3, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>W Ziarko</author>
<author>P C N Wong</author>
</authors>
<title>Generalized vector space model in information retrieval.</title>
<date>1985</date>
<booktitle>In Proceedings of the 81h ACM SIGIR Conference.</booktitle>
<contexts>
<context position="14183" citStr="Wong et al., 1985" startWordPosition="2423" endWordPosition="2426">f domains. A DM is fully defined by a k∗ x d domain matrix D representing in each cell di,z the domain relevance of the ith term of V ∗ with respect to the domain Dz. The domain matrix D is used to define a function D : Rk* —* Rd, that maps the document vectors &apos;tj expressed into the multilingual classical VSM (see Section 2.1), into the vectors �t0j in the multilingual domain VSM. The function D is defined bye D(�tj) = �tj(IIDFD) = �t0 (1) j where IIDF is a diagonal matrix such that iIDF i,l = IDF(wli), t� is represented as a row vector, and IDF(wli) is the Inverse Document Frequency of 2In (Wong et al., 1985) the formula 1 is used to define a Generalized Vector Space Model, of which the Domain VSM is a particular instance. wli evaluated in the corpus Tl. In this work we exploit Latent Semantic Analysis (LSA) (Deerwester et al., 1990) to automatically acquire a MDM from comparable corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a large corpus. In the monolingual settings LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. SVD decomposes the term-by-document matrix T into three matr</context>
</contexts>
<marker>Wong, Ziarko, Wong, 1985</marker>
<rawString>S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 81h ACM SIGIR Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>