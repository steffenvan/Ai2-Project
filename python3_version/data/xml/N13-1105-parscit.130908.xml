<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998815">
A Quantum-Theoretic Approach to Distributional Semantics
</title>
<author confidence="0.999852">
William Blacoe1, Elham Kashefi2, Mirella Lapata1
</author>
<affiliation confidence="0.99969">
1Institute for Language, Cognition and Computation,
2Laboratory for Foundations of Computer Science
School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB
</affiliation>
<email confidence="0.976737">
w.b.blacoe@sms.ed.ac.uk, ekashefi@inf.ed.ac.uk, mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.995046" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9947905">
In this paper we explore the potential of
quantum theory as a formal framework for
capturing lexical meaning. We present a
novel semantic space model that is syntacti-
cally aware, takes word order into account,
and features key quantum aspects such as
superposition and entanglement. We define
a dependency-based Hilbert space and show
how to represent the meaning of words by den-
sity matrices that encode dependency neigh-
borhoods. Experiments on word similarity
and association reveal that our model achieves
results competitive with a variety of classical
models.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999812321428572">
The fields of cognitive science and natural language
processing have recently produced an ensemble of
semantic models which have an impressive track
record of replicating human behavior and enabling
real-world applications. Examples include simula-
tions of word association (Denhi`ere and Lemaire,
2004; Griffiths et al., 2007), semantic priming (Lund
and Burgess, 1996; Landauer and Dumais, 1997;
Griffiths et al., 2007), categorization (Laham, 2000),
numerous studies of lexicon acquisition (Grefen-
stette, 1994; Lin, 1998), word sense discrimination
(Sch¨utze, 1998), and paraphrase recognition (Socher
et al., 2011). The term “semantic” derives from the
intuition that words seen in the context of a given
word contribute to its meaning (Firth, 1957). Al-
though the specific details of the individual models
differ, they all process a corpus of text as input and
represent words (or concepts) in a (reduced) high-
dimensional space.
In this paper, we explore the potential of quan-
tum theory as a formal framework for capturing lex-
ical meaning and modeling semantic processes such
as word similarity and association (see Section 6
for an overview of related research in this area).
We use the term quantum theory to refer to the ab-
stract mathematical foundation of quantum mechan-
ics which is not specifically tied to physics (Hughes,
1989; Isham, 1989). Quantum theory is in prin-
ciple applicable in any discipline where there is a
need to formalize uncertainty. Indeed, researchers
have been pursuing applications in areas as diverse
as economics (Baaquie, 2004), information theory
(Nielsen and Chuang, 2010), psychology (Khren-
nikov, 2010; Pothos and Busemeyer, 2012), and cog-
nitive science (Busemeyer and Bruza, 2012; Aerts,
2009; Bruza et al., 2008). But what are the features
of quantum theory which make it a promising frame-
work for modeling meaning?
Superposition, entanglement, incompatibility,
and interference are all related aspects of quantum
theory, which endow it with a unique character.1 Su-
perposition is a way of modeling uncertainty, more
so than in classical probability theory. It contains in-
formation about the potentialities of a system’s state.
An electron whose location in an atom is uncertain
can be modeled as being in a superposition of loca-
tions. Analogously, words in natural language can
have multiple meanings. In isolation, the word pen
may refer to a writing implement, an enclosure for
confining livestock, a playpen, a penitentiary or a fe-
male swan. However, when observed in the context
of the word ink the ambiguity resolves into the sense
of the word dealing with writing. The meanings of
words in a semantic space are superposed in a way
which is intuitively similar to the atom’s electron.
Entanglement concerns the relationship between
</bodyText>
<footnote confidence="0.872381">
1It is outside the scope of the current paper to give a detailed
introduction on the history of quantum mechanics. We refer
the interested reader to Vedral (2006) and Kleppner and Jackiw
(2000) for comprehensive overviews.
</footnote>
<page confidence="0.934027">
847
</page>
<note confidence="0.4762985">
Proceedings of NAACL-HLT 2013, pages 847–857,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999947840909091">
systems for which it is impossible to specify a joint
probability distribution from the probability distri-
butions of their constituent parts. With regard to
word meanings, entanglement encodes (hidden) re-
lationships between concepts. The different senses
of a word “exist in parallel” until it is observed
in some context. This reduction of ambiguity has
effects on other concepts connected via entangle-
ment. The notion of incompatibility is fundamen-
tal to quantum systems. In classical systems, it is
assumed by default that measurements are compati-
ble, that is, independent, and as a result the order in
which these take place does not matter. By contrast
in quantum theory, measurements may share (hid-
den) order-sensitive inter-dependencies and the out-
come of the first measurement can change the out-
come of the second measurement.
Interference is a feature of quantum probability
that can cause classical assumptions such as the law
of total probability to be violated. When concepts
interact their joint representation can exhibit non-
classical behavior, e.g., with regard to conjunction
and disjunction (Aerts, 2009). An often cited ex-
ample is the “guppy effect”. Although guppy is an
example of a pet-fish it is neither a very typical pet
nor fish (Osherson and Smith, 1981).
In the following we use the rich mathematical
framework of quantum theory to model semantic in-
formation. Specifically, we show how word mean-
ings can be expressed as quantum states. A word
brings with it its own subspace which is spanned by
vectors representing its potential usages. We present
a specific implementation of a semantic space that is
syntactically aware, takes word order into account,
and features key aspects of quantum theory. We em-
pirically evaluate our model on word similarity and
association and show that it achieves results com-
petitive with a variety of classical models. We be-
gin by introducing some of the mathematical back-
ground needed for describing our approach (Sec-
tion 2). Next, we present our semantic space model
(Section 3) and our evaluation experiments (Sec-
tions 4 and 5). We conclude by discussing related
work (Section 6).
</bodyText>
<sectionHeader confidence="0.995136" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.99904175">
Let c = reiθ be a complex number, expressed in po-
lar form, with absolute value r = |c |and phase θ. Its
complex conjugate c∗ = re−iθ has the inverse phase.
Thus, their product cc∗ = (reiθ)(re−iθ) = r2 is real.
</bodyText>
<subsectionHeader confidence="0.970181">
2.1 Vectors
</subsectionHeader>
<bodyText confidence="0.914313153846154">
We are interested in finite-dimensional, complex-
valued vector spaces Cn with an inner product, oth-
erwise known as Hilbert space. A column vector
→−ψ ∈ Cn can be written as an ordered vertical array
of its n complex-valued components, or alternatively
as a weighted sum of base vectors:
Whereas Equation (1) uses base vectors from the
−→ bn}, any other set of n
orthonormal vectors serves just as well as a base for
the same space. Dirac (1939) introduced the so-
called bra-ket notation which is equally expressive
but notationally more convenient. A column vector
becomes a ket:
</bodyText>
<equation confidence="0.936725">
→−ψ ≡ |ψi = ψ1|b1i+ψ2|b2i+...+ψn|bni (2)
</equation>
<bodyText confidence="0.98441625">
and a row vector becomes a bra hψ|. Transposing
a complex-valued vector or matrix (via the super-
script “†”) involves complex-conjugating all compo-
nents:
</bodyText>
<equation confidence="0.998473">
|ψi† = hψ |= ψ∗1hb1 |+ψ∗2hb2|+...+ψ∗nhbn |(3)
</equation>
<bodyText confidence="0.999571">
The Dirac notation for the inner product h·|·i il-
lustrates the origin of the terminology “bra-ket”.
Since Bstd’s elements are normalised and pairwise
orthogonal their inner product is:
</bodyText>
<equation confidence="0.900263857142857">
� 1, if i = j
hbi|bji = (4)
0, otherwise
The inner product is also applicable to pairs of non-
base kets:
(ψ∗ 1 ψ∗ 2 ··· ψ∗ n)
= (∑i ψ∗i hbi|)(∑j φj|bji� (5)
</equation>
<bodyText confidence="0.7801705">
= ∑i,j ψ∗i φjhbi|bji = ∑iψ∗i φihbi|bii
= ∑iψ∗ i φi
</bodyText>
<figure confidence="0.993828666666667">
0
..
.
+...+ψn
=ψ1
1
0
..
→− ψ=
(1)
0
.
1
0
1
�
� � �
�
� � �
�
� � �
�
� � �
�
� � �
ψ1
ψ2
...
ψn
standard base Bstd = { b1,...,
�
� � �
φ1
�
� � �
≡ hψ|φi
φ2
...
φn
</figure>
<page confidence="0.959429">
848
</page>
<bodyText confidence="0.9358558">
Reversing the order of an inner product complex-
conjugates it:
the same subscript as the corresponding subspace.
Thus, the order no longer matters, as in the follow-
ing inner product of composed kets:
</bodyText>
<equation confidence="0.9998985">
(hψ|φi)∗ = hφ|ψi (6)
(ha|1hb|2hc|3)(|ei3|di1|fi2) = ha|dihb|fihc|ei (11)
</equation>
<subsectionHeader confidence="0.991907">
2.2 Matrices
</subsectionHeader>
<bodyText confidence="0.999889">
Matrices are sums of outer products |·ih·|. For ex-
ample, the matrix (Mi,j)i, j can be thought of as
the weighted sum of “base-matrices” Bi,j ≡ |biihbj|,
whose components are all 0 except for a 1 in the i-th
row and j-th column. The outer product extends lin-
early to non-base kets in the following manor:
</bodyText>
<equation confidence="0.994261181818182">
� �
|ψihφ |= (∑iψi|bii) ∑j φ∗ jhbj|
= ∑i,j ψiφ∗j|biihbj|
This is analogous to the conventional multiplication:
� � �
ψ1φ∗1 ··· ψ1φ∗n
� �
�(φ∗ 1 ··· φ∗ ..
n)= � . ..�
.. . .� (8)
ψnφ∗1 ··· ψnφ∗n
</equation>
<bodyText confidence="0.998178">
We will also make use of the tensor product. Its ap-
plication to kets, bras and outer products is linear:
</bodyText>
<equation confidence="0.999598">
(|ai+|bi)⊗|ci = |ai ⊗|ci+ |bi ⊗|ci
(ha |+ hb|) ⊗ hc |= ha |⊗ hc |+ hb |⊗ hc |(9)
(|aihb |+ |cihd|) ⊗ |eihf |=
(|ai ⊗ |ei)(hb |⊗ hf|) + (|ci ⊗ |ei)(hd |⊗ hf|)
</equation>
<bodyText confidence="0.9972455">
For convenience we omit “⊗” where no confusion
arises, e.g., |ai ⊗ |bi = |ai|bi. When applied to
Hilbert spaces, the tensor product creates the com-
posed Hilbert space H = H1 ⊗ ... ⊗ Hn whose base
kets are simply induced by the tensor product of its
subspaces’ base kets:
</bodyText>
<equation confidence="0.99845925">
base(H1 ⊗... ⊗Hn) =
� n (10)
® |bii : |bii ∈ base(Hi), 1 ≤ i ≤ n
i=1
</equation>
<bodyText confidence="0.999352333333333">
Whereas the order of composed kets |ai|bi|ci usu-
ally suffices to identify which subket lives in which
subspace, we make this explicit by giving subkets
</bodyText>
<figure confidence="0.9620573">
Definition 1. Self-adjoint Matrix
A matrix M is self-adjoint iff Mi, j = M∗j,i for all i, j.
Consequently, all diagonal elements are real-valued,
and M = M† is its own transpose conjugate.
(7) Definition 2. Density Matrix
�
� �
ψ1
...
ψn
</figure>
<bodyText confidence="0.9703833">
A self-adjoint matrix M is a density matrix iff
it is positive semi-definite, i.e., hφ|M|φi ≥ 0 for
all |φi ∈ Cn, and it has unit trace, i.e., Tr(M) =
∑|bi∈B hb|M|bi = 1.
The term “density matrix” is synonymous with
“density operator”. Any density matrix ρ can
be decomposed arbitrarily as ρ = ∑i pi|siihsi|, the
weighted sum of sub-matrices |siihsi |with pi ∈ R&gt;0
and hsi|sii = 1. The pi need not sum to 1. In fact the
decomposition where the pi sum to 1 and the |sii are
mutually orthogonal is unique and is called the eigen
decomposition. Consequently Beig = {|sii}i consti-
tutes an orthonormal base, ρ’s so-called eigen base.
Density operators are used in quantum theory to de-
scribe the state of some system. If the system’s state
ρ is certain we call it a pure state and write ρ = |sihs|
for some unit ket |si. Systems whose state is uncer-
tain are described by a mixed state ρ = ∑i pi|siihsi|
which represents an ensemble of substates or pure
states {(pi,si)}i where the system is in substate si
with probability pi. Hence, the term “density” as in
probability density.
It is possible to normalize a density matrix
without committing to any particular decomposi-
tion. Only the trace function is required, because
norm(ρ) = ρ/Tr(ρ). Definition 2 mentions what the
trace function does. However, notice that the same
result is produced for any orthonormal base B, in-
cluding ρ’s eigen base Beig = {|eii}i. Even though
we do not know the content of Beig, we know that it
</bodyText>
<page confidence="0.982853">
849
</page>
<bodyText confidence="0.670572">
exists. So we use it to show that dividing p by:
</bodyText>
<equation confidence="0.933680545454545">
Tr(p) = Tr(Ei pi|eiihei|)
= Ejhej|(Ei pi|eiihei|)|eji (12)
= Ei, j pihej|eiihei|eji
= Ei pihei|eiihei|eii = Ei pi
normalizes its probability distribution over eigen
kets:
Tr(p) — =
p _ Ei pi|eiihei|
Ej pj (13)
Ei EP&apos;pj
 |eii hei
</equation>
<sectionHeader confidence="0.994237" genericHeader="method">
3 Semantic Space Model
</sectionHeader>
<bodyText confidence="0.99998075">
We represent the meaning of words by density ma-
trices. Specifically, a lexical item w is modeled as
an ensemble Uw = {(pi,ui)}i of usages ui and the
corresponding probabilities pi that w gets used “in
the i-th manor”. A word’s usage is comprised of
distributional information about its syntactic and se-
mantic preferences, in the form of a ket |uii. The
density matrix pw = Ei pi|uiihui |represents the en-
semble Uw. This section explains our method of ex-
tracting lexical density matrices from a dependency-
parsed corpus. Once density matrices have been
learned, we can predict the expected usage similar-
ity of two words as a simple function of their density
matrices. Our explication will be formally precise,
but at the same time illustrate each principle through
a toy example.
</bodyText>
<subsectionHeader confidence="0.997849">
3.1 Dependency Hilbert Space
</subsectionHeader>
<bodyText confidence="0.996060083333333">
Our model learns the meaning of words from a
dependency-parsed corpus. Our experiments have
used the Stanford parser (de Marneffe and Man-
ning, 2008), however any other dependency parser
with broadly similar output could be used instead.
A word’s usage is learned from the type of depen-
dency relations it has with its immediate neighbors
in dependency graphs. Its semantic content is thus
approximated by its “neighborhood”, i.e., its co-
occurrence frequency with neighboring words.
Neighborhoods are defined by a vocabu-
lary V = {w1,...,wnV} of the nV most fre-
</bodyText>
<equation confidence="0.52806975">
quent (non-stop) words in the corpus. Let
Rel = {sub−1,dobj−1,amod,num,poss,...} denote
Document 1:
Document 2:
</equation>
<figureCaption confidence="0.9978926">
Figure 1: Example dependency trees in a toy corpus. Dot-
ted arcs are ignored because they are either not connected
to the target words jaguar and elephant or because their
relation is not taken into account in constructing the se-
mantic space. Words are shown as lemmas.
</figureCaption>
<bodyText confidence="0.94450648">
a subset of all dependency relations provided by
the parser and their inverses. The choice of Rel is a
model parameter. We considered only the most fre-
quently occuring relations above a certain threshold,
which turned out to be about half of the full inven-
tory. Relation symbols with the superscript “−1”
indicate the inversion of the dependency direction
(dependent to head). All other relation symbols
have the conventional direction (head to dependent).
xyz xyz−1
Hence, w → v is equivalent to v → w. We then
partition Rel into disjoint clusters of syntactically
similar relations Part = {RC1,...,RCnPart}. For
example, we consider syntactically similar relations
which connect target words with neighbors with
the same part of speech. Each relation cluster RCk
is assigned a Hilbert space Hk whose base kets
{|w(k)
j i}j correspond to the words in V = {wj}j.
Figure 1 shows the dependency parses for a
toy corpus consisting of two documents and five
sentences. To create a density matrix for the target
words jaguar and elephant, let us assume that we
the man see two angry jaguar
we see two angry elephant
</bodyText>
<figure confidence="0.99022055">
det subj
subj
two elephant run
num nsubj
dobj
num
amod
dobj
num
anod
she buy a nice new jaguar
subj
I like my jaguar
poss
subj
dobj
dobj
det
amod
amod
</figure>
<page confidence="0.9671">
850
</page>
<bodyText confidence="0.970341">
will consider the following relation clusters:
</bodyText>
<equation confidence="0.956126">
RC1 = {dobj−&apos;,iobj−&apos;,agent−&apos;,nsubj−&apos;,...},
</equation>
<bodyText confidence="0.6658945">
RC2 = {advmod,amod,tmod,...} and RC3 = {nn,
appos,num,poss,...}.
</bodyText>
<subsectionHeader confidence="0.999649">
3.2 Mapping from Dependency Graphs to Kets
</subsectionHeader>
<bodyText confidence="0.999991454545454">
Next, we create kets which encode syntactic and se-
mantic relations as follows. For each occurrence of
the target word w in a dependency graph, we only
consider the subtree made up of w and the immedi-
ate neighbors connected to it via a relation in Rel.
In Figure 1, arcs from the dependency parse that we
ignore are shown as dotted. Let the subtree of in-
terest be st = {(RC1,v1),...,(RCnPart,vnPart)}, that is,
w is connected to vk via some relation in RCk, for
k ∈ {1,...,nPart}. For any relation cluster RCk that
does not feature in the subtree, let RCk be paired with
the abstract symbol w/0 in st. This symbol represents
uncertainty about a potential RCk-neighbor.
We convert all subtrees st in the corpus for the tar-
get word w into kets |ψsti ∈ 711 ⊗ ... ⊗71nPart. These
in turn make up the word’s density matrix ρw. Be-
fore we do so, we assign each relation cluster RCk
a complex value αk = eiθk. The idea behind these
values is to control for how much each subtree con-
tributes to the overall density matrix. This becomes
more apparent after we formulate our method of in-
ducing usage kets and density matrices.
</bodyText>
<equation confidence="0.9145315">
�|ψsti = αst |vik, (14)
(RCk,v)∈st
</equation>
<bodyText confidence="0.999922888888889">
where αst = ∑(RCk,v)∈st,v6=w/0 αk. Every RCk paired
with some neighbor v ∈ V induces a basic subket
|vik ∈ base(71k), i.e., a base ket of the k-th sub-
space or subsystem. All other subkets |w/0ik =
∑v∈V |V|− 12 |vik are in a uniformly weighted super-
position of all base kets. The factor |V|− 12 ensures
that hw/0|w/0i = 1. The composed ket for the sub-
tree st is again weighted by the complex-valued αst.
αst is the sum of complex values αk = eiθk, each
with absolute value 1. Therefore, its own abso-
lute value depends highly on the relative orienta-
tion θk among its summands: equal phases reinforce
absolute value, but the more phases are opposed
(i.e., their difference approaches π), the more they
cancel out the sum’s absolute value. Only those αk
contribute to this sum whose relation cluster is not
paired with w/0. The choice of the parameters θk al-
lows us to put more weight on some combinations
</bodyText>
<equation confidence="0.7825845">
hψst1 |hψst2 |hψst3|
=6 0 =6 0 =6 0
=6 0 =6 0 =6 0
=6 0 =6 0 =6 0
hψst1 |hψst2 |hψst3|
=60 0 0
0 =60 0
0 0 =60
</equation>
<figureCaption confidence="0.997496125">
Figure 2: Excerpts of density matrices that result from
the dependency subtrees st1,st2,st3. Element mi,j in row i
and column j is mi,j|ψstiihψstj |in Dirac notation. (a) All
three subtrees are in the same document. Thus their kets
contribute to diagonal and off-diagonal matrix elements.
(b) Each subtree is in a separate document. Therefore
their kets do not group, affecting only diagonal matrix
elements.
</figureCaption>
<bodyText confidence="0.977466454545455">
of dependency relations than others.
Arbitrarily choosing θ1 = π4, θ2 = 7π 4
,
and θ3 = 3π4 renders the subtrees in Fig-
ure 1 as |ψst1ai = eiπ/4|seei1|angryi2|twoi3,
√
√ |ψst2ai = 2|buyi1(|nicei2 +|newi2)|w/0i3, |ψst2bi =
2eiπ/2|likei1|w/0i2|myi3, which are relevant for
jaguar, and |ψst1bi = eiπ/4|seei1|angryi2|twoi3,
√
|ψst1ci = 2eiπ/2|runi1|w/0i2|twoi3, which are
relevant for elephant. The subscripts outside of the
subkets correspond to those of the relation clusters
RC1,RC2,RC3 chosen in Section 3.1.
In sentence 2a, jaguar has two neighbors under
RC2. Therefore the subket from 712 is a superpo-
sition of the base kets |nicei2 and |newi2. This is
a more intuitive formulation of the equivalent ap-
proach which first splits the subtree for buy nice new
jaguar into two similar subtrees for buy nice jaguar
and for buy new jaguar, and then processes them as
seperate subtrees within the same document.
</bodyText>
<subsectionHeader confidence="0.999226">
3.3 Creating Lexical Density Matrices
</subsectionHeader>
<bodyText confidence="0.996895222222222">
We assume that a word’s usage is uniform through-
out the same document. In our toy corpus in Fig-
ure 1, jaguar is always the direct object of the main
verb. However, in Document 1 it is used in the an-
imal sense, whereas in Document 2 it is used in the
car sense. Even though the usage of jaguar in sen-
tence (2b) is ambiguous, we group it with that of
sentence (2a).
These considerations can all be comfortably en-
</bodyText>
<figure confidence="0.875437666666667">
|ψst1i
|ψst2i
|ψst3i
|ψst1i
|ψst2i
|ψst3i
</figure>
<page confidence="0.957993">
851
</page>
<equation confidence="0.999631846153846">
ρjaguar = (|ψst1a)(ψst1a|+(|ψst2a)+|ψst2b))((ψst2a|+(ψst2b|))/7 =
0.14|see)1|angry)2|two)3(see|1(angry|2(two|3+ 0.29|buy)1|nice)2|w/0)3(buy|1(nice|2(w/0|3+
0.29|buy)1|nice)2|w/0)3(buy|1(new|2(w/0|3+ 0.29|buy)1|new)2|w/0)3(buy|1(nice|2(w/0|3+
0.29|buy)1|new)2|w/0)3(buy|1(new|2(w/0|3+ 0.29eπ/2|like)1|w/0)2|my)3(buy|1(nice|2(w/0|3+
0.29eπ/2|like)1|w/0)2|my)3(buy|1(new|2(w/0|3+ 0.29e−π/2|buy)1|nice)2|w/0)3(like|1(w/0|2(my|3+
0.29e−π/2|buy)1|new)2|w/0)3(like|1(w/0|2(my|3+ 0.29|like)1|w/0)2|my)3(like|1(w/0|2(my|3
ρelephant = ((|ψst1b)+|ψst1c))((ψst1b |+(ψst1c|))/3 =
0.33|see)1|angry)2|two)3(see|1(angry|2(two|3+ 0.47eπ/4|run)1|w/0)2|two)3(see|1(angry|2(two|3+
0.47e−π/4|see)1|angry)2|two)3(run|1(w/0|2(two|3+ 0.67|run)1|w/0)2|two)3(run|1(w/0|2(two|3
Tr(ρjaguarρelephant)=Tr(0.05|ψst1a)(ψst1b|+0.05eiπ/4|ψst1a)(ψst1c|)=Tr(0.05|see)1|angry)2|two)3(see|1(angry|2(two|3+
0.07e−π/4|see)1|angry)2|two)3(run|1(w/0|2(two|3) = ∑ (b|(0.05|see)1|angry)2|two)3(see|1(angry|2(two|3+
|b)Ebase(H1(&amp;H2(&amp;H3)
0.07e−π/4|see)1|angry)2|two)3(run|1(w/0|2(two|3)|b) = 0.05
</equation>
<figureCaption confidence="0.996472">
Figure 3: Lexical density matrices for the words jaguar and elephant and their similarity.
</figureCaption>
<bodyText confidence="0.999656818181818">
coded in a density matrix. This is simply gener-
ated via the outer product of our subtree kets |ψst).
For example, ρD1,jaguar = |ψst1a)(ψst1a |represents
the contribution that document D1 makes to ρjaguar.
Document D2, however, has more than one ket
relevant to ρjaguar. Due to our assumption of
document-internal uniformity of word usage, we
group D2’s subtree-kets additively: ρD2, jaguar =
(|ψst2a)+|ψst2b))((ψst2a|+(ψst2b|). The target word’s
density matrix ρw is the normalized sum of all den-
sity matrices ρD,w obtained from each D:
</bodyText>
<equation confidence="0.950676">
ρD,w = ∑|ψst) ∑(ψst |(15)
(stESTD,w )(stESTD,w
</equation>
<bodyText confidence="0.99965305882353">
where STD,w is the set of all subtrees for target
word w in document D. To illustrate the differ-
ence that this grouping makes, consider the den-
sity matrices in Figure 2. Whereas in (a) the sub-
trees st1,st2,st3 share a document, in (b) they are
from distinct documents. This grouping causes them
to not only contribute to diagonal matrix elements,
e.g., |ψst2)(ψst2|, as in (b), but also to off diagonal
ones, e.g., |ψst2)(ψst1|, as in (a).
Over the course of many documents the summa-
tion of all contributions, no matter how small or
large the groups are, causes “clusters of weight”
to form, which hopefully coincide with word us-
ages. As mentioned in Section 3.2, adding complex-
valued matrix elements increases or decreases the
sum’s absolute value depending on relative phase
orientation. This makes it possible for interference
</bodyText>
<figureCaption confidence="0.763971666666667">
to occur. Since the same word appears in varying
contexts, the corresponding complex-valued outer
products interact upon summation. Finally, the den-
sity matrix gets normalized, i.e., divided by its trace.
This leaves the distributional information intact and
merely normalizes the probabilities. Figure 3 illus-
trates the estimation of the density matrices for the
words jaguar and elephant from the toy corpus in
Figure 1.
</figureCaption>
<subsectionHeader confidence="0.996783">
3.4 Usage Similarity
</subsectionHeader>
<bodyText confidence="0.980752692307692">
Decomposing the density matrix of the target
word w, ρw = ∑i pi|ui)(ui |recovers the usage ensem-
ble Uw = I(pi,ui)Ji. However, in general there are
infinitely many possible ensembles which ρw might
represent. This subsection explains our metric for
estimating the usage similarity of two words. The
math involved shows that we can avoid the question
of how to best decompose ρw.
We compute the usage similarity of two words w
and v by comparing each usage of w with each us-
age of v and weighting these similarity values with
the corresponding usage probabilities. Let ρw =
∑i p(w)
</bodyText>
<equation confidence="0.869249666666667">
i  |u�w)) (u�w)  |and ρv = ∑i p(v)
i |u(v)
i )(u(v) i|. The
</equation>
<bodyText confidence="0.916714555555555">
similarity of some usage kets |u(w)
i ) and |u(v)
j ) is ob-
tained, as is common in the literature, by their in-
ner product (u(w)
i |u(v)
j ). However, as this is a com-
plex value, we multiply it with its complex conju-
gate, rendering the real value (u(v)
</bodyText>
<equation confidence="0.99514225">
j  |u�w)) (u�w) |u(v)
j ) =
|(u(w)
i |u(v)
</equation>
<bodyText confidence="0.9818225">
j )|2. Therefore, in total the expected simi-
larity of w and v is:
</bodyText>
<page confidence="0.959786">
852
</page>
<equation confidence="0.953104277777778">
Tr (∑i
pi |u(w)
(w) i ihu(w)
i |)(∑
j
∑= Tr
i,j
�
(w) (v) pi pj |u(w)
i ihu(w)
i |u(v)
j ihu(v)
j  |=
�
(v)
pj |u(v)
j ihu(v)
j |)
</equation>
<bodyText confidence="0.9995635">
ucts. Usage pairs with a high distributional simi-
(16) larity roughly “align” and then get weighted by the
probabilities of those usages. Two words are similar
if they are substitutable, that is, if they can be used
in the same syntactic environment and have the same
meaning. Hopefully, this leads to more accurate es-
timation of distributional similarity and can be used
to compute word meaning in context.
</bodyText>
<equation confidence="0.989863666666667">
sim(w,v) = ∑ (w) pi pj hu(v)
i,j (v) j |u(w)
i ihu(w)
i |u(v)
j i
= Tr(ρwρv)
</equation>
<bodyText confidence="0.999941583333333">
We see that the similarity function simply reduces to
multiplying ρw with ρv and applying the trace func-
tion. The so-called cyclic property of the trace func-
tion (i.e., Tr(M1M2) = Tr(M2M1) for any two matri-
ces M1,M2) gives us the corollary that this particular
similarity function is symmetric.
Figure 3 (bottom) shows how to calculate the sim-
ilarity of jaguar and elephant. Only the coefficient
of the first outer product survives the tracing pro-
cess because its ket and bra are equal modulo trans-
pose conjugate. As for the second outer product,
0.05eiπ/4hb|ψst1aihψst1c|bi is 0 for all base kets |bi.
</bodyText>
<subsectionHeader confidence="0.886165">
3.5 What Does This Achieve?
</subsectionHeader>
<bodyText confidence="0.999961692307692">
We represent word meanings as described above for
several reasons. The density matrix decomposes into
usages each of which are a superposition of combi-
nations of dependents. Internally, these usages are
established automatically by way of “clustering”.
Our model is parameterized with regard to the
phases of sub-systems (i.e., clusters of syntactic re-
lations) which allows us to make optimal use of in-
terference, as this plays a large role in the over-
all quality of representation. It is possible for a
combination of (groups of) dependents to get en-
tangled if they repeatedly appear together under the
same word, and only in that combination. If the
co-occurence of (groups of) dependents is uncorre-
lated, though, they remain unentangled. Quantum
entanglement gives our semantic structures the po-
tential for long-distance effects, once quantum mea-
surement becomes involved. This is in analogy to
the nonlocal correlation between properties of sub-
atomic particles, such as the magnetic spin of elec-
trons or the polarization of photons. Such an exten-
sion to our implementation will also uncover which
sets of measurements are order-sensitive, i.e., in-
compatible.
Our similarity metric allows two words to “select”
each other’s usages via their pairwise inner prod-
</bodyText>
<sectionHeader confidence="0.998006" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.998174384615385">
Data All our experiments used a dependency
parsed and lemmatized version of the British Na-
tional Corpus (BNC). As mentioned in Section 3, we
obtained dependencies from the output of the Stan-
ford parser (de Marneffe and Manning, 2008). The
BNC comprises 4,049 texts totalling approximately
100 million words.
Evaluation Tasks We evaluated our model on
word similarity and association. Both tasks are em-
ployed routinely to assess how well semantic models
predict human judgments of word relatedness. We
used the WordSim353 test collection (Finkelstein et
al., 2002) which consists of similarity judgments for
word pairs. Participants gave each pair a similar-
ity rating using a 0 to 10 scale (e.g., tiger–cat are
very similar, whereas delay–racism are not). The
average rating for each pair represents an estimate of
the perceived similarity of the two words. The col-
lection contains ratings for 437 unique words (353
pairs) all of which appeared in our corpus. Word as-
sociation is a slightly different task: Participants are
given a cue word (e.g., rice) and asked to name an
associate in response (e.g., Chinese, wedding, food,
white). We used the norms collected by Nelson et
al. (1998). We estimated the strength of association
between a cue and its associate, as the relative fre-
quency with which it was named. The norms con-
tain 9,968 unique words (70,739 pairs) out of which
9,862 were found in our corpus, excluding multi-
word expressions.
For both tasks, we used correlation analysis to ex-
amine the degree of linear relationship between hu-
man ratings and model similarity values. We report
correlation coefficients using Spearman’s rank cor-
relation coefficient.
Quantum Model Parameters The quantum
framework presented in Section 3 is quite flexible.
Depending on the choice of dependency rela-
tions Rel, dependency clusters RCj, and complex
</bodyText>
<page confidence="0.997867">
853
</page>
<bodyText confidence="0.99993234">
values aj = eiqj, different classes of models can be
derived. To explore these parameters, we partitioned
the WordSim353 dataset and Nelson et al.’s (1998)
norms into a development and test set following
a 70–30 split. We tested 9 different intuitively
chosen relation partitions {RC1,...,RCnPart}, cre-
ating models that considered only neighboring
heads, models that considered only neighboring
dependents, and models that considered both. For
the latter two we experimented with partitions of
one, two or three clusters. In addition to these more
coarse grained clusters, for models that included
both heads and dependents we explored a partition
with twelve clusters broadly corresponding to
objects, subjects, modifiers, auxiliaries, determiners
and so on. In all cases stopwords were not taken
into account in the construction of the semantic
space.
For each model variant we performed a grid
search over the possible phases qj = kp with range
k = 0 4, 1 4,..., 74 for the complex-valued aj assigned
to the respective relation cluster RCj (see Section
3.2 for details). In general, we observed that the
choice of dependency relations and their clustering
as well as the phases assigned to each cluster greatly
influenced the semantic space. On both tasks, the
best performing model had the relation partition de-
scribed in Section 3.1. Section 5 reports our results
on the test set using this model.
Comparison Models We compared our quantum
space against three classical distributional models.
These include a simple semantic space, where a
word’s meaning is a vector of co-occurrences with
neighboring words (Mitchell and Lapata, 2010), a
syntax-aware space based on weighted distributional
triples that encode typed co-occurrence relations
among words (Baroni and Lenci, 2010) and word
embeddings computed with a neural language model
(Bengio, 2001; Collobert and Weston, 2008) For all
three models we used parameters that have been re-
ported in the literature as optimal.
Specifically, for the simple co-occurrence-based
space we follow the settings of Mitchell and Lapata
(2010): a context window of five words on either
side of the target word and 2,000 vector dimensions
(i.e., the 2000 most common context words in the
BNC). Vector components were set to the ratio of
the probability of the context word given the target
word to the probability of the context word overall.
For the neural language model, we adopted the best
</bodyText>
<table confidence="0.9814126">
Models WordSim353 Nelson Norms
SDS 0.433 0.151
DM 0.318 0.123
NLM 0.196 0.091
QM 0.535 0.185
</table>
<tableCaption confidence="0.99512825">
Table 1: Performance of distributional models on Word-
Sim353 dataset and Nelson et al.’s (1998) norms (test
set). Correlation coefficients are all statistically signifi-
cant (p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.999929666666667">
performing parameters from our earlier comparison
of different vector sources for distributional seman-
tics (Blacoe and Lapata, 2012) where we also used
the BNC for training. There we obtained best results
with 50 dimensions, a context window of size 4,
and an embedding learning rate of 10−9. Our third
comparison model uses Baroni and Lenci’s (2010)
third-order tensor2 which they obtained from a very
large dependency-parsed corpus containing approxi-
mately 2.3 billion words. Their tensor assigns a mu-
tual information score to instances of word pairs w,v
and a linking word l. We obtained vectors w from
the tensor following the methodology proposed in
Blacoe and Lapata (2012) using 100 (l,v) contexts
as dimensions.
</bodyText>
<sectionHeader confidence="0.999805" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999987">
Our results are summarized in Table 1. As can
be seen, the quantum model (QM) obtains perfor-
mance superior to other better-known models such
as Mitchell and Lapata’s (2010) simple semantic
space (SDS), Baroni and Lenci’s (2010) distribu-
tional memory tensor (DM), and Collobert and We-
ston’s (2008) neural language model (NLM). Our
results on the association norms are comparable to
the state of the art (Silberer and Lapata, 2012; Grif-
fiths et al., 2007). With regard to WordSim353,
Huang et al. (2012) report correlations in the range
of 0.713–0.769, however they use Wikipedia as a
training corpus and a more sophisticated version of
the NLM presented here, that takes into account
global context and performs word sense discrimi-
nation. In the future, we also plan to evaluate our
model on larger Wikipedia-scale corpora. We would
also like to model semantic composition as our ap-
proach can do this easily by taking advantage of the
notion of quantum measurement. Specifically, we
</bodyText>
<footnote confidence="0.998654">
2Available at http://clic.cimec.unitn.it/dm/.
</footnote>
<page confidence="0.992901">
854
</page>
<tableCaption confidence="0.81455325">
Models bar order
Table 2: Associates for bar and order ranked according to
similarity. Underlined associates overlap with the human
responses (HS).
</tableCaption>
<bodyText confidence="0.999927666666667">
can work out the meaning of a dependency tree by
measuring the meaning of its heads in the context of
their dependents.
Table 2 shows the five most similar associates (or-
dered from high to low) for the cues bar and order
for the quantum model and the comparison models.
We also show the human responses (HS) according
to Nelson et al.’s (1998) norms. The associates gen-
erated by the quantum model correspond to several
different meanings correlated with the target. For
example, prison refers to the “behind bars” sense
of bar, liquor and beer refer to what is consumed
or served in bars, club refers to the entertainment
function of bars, whereas graph refers to how data
is displayed in a chart.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="conclusions">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999966428571429">
Within cognitive science the formal apparatus of
quantum theory has been used to formulate models
of cognition that are superior to those based on tra-
ditional probability theory. For example, conjunc-
tion fallacies3 (Tversky and Kahneman, 1983) have
been explained by making reference to quantum the-
ory’s context dependence of the probability assess-
ment. Violations of the sure-thing principle4 (Tver-
sky and Shafir, 1992) have been modeled in terms of
a quantum interference effect. And the asymmetry
of similarity relations has been explained by pos-
tulating that different concepts correspond to sub-
spaces of different dimensionality (Pothos and Buse-
meyer, 2012). Several approaches have drawn on
</bodyText>
<footnote confidence="0.983156">
3A conjunction fallacy occurs when it is assumed that spe-
cific conditions are more probable than a single general one.
4The principle is the expectation that human behavior ought
to conform to the law of total probability
</footnote>
<bodyText confidence="0.999609387755102">
quantum theory in order to model semantic phe-
nomena such as concept combination (Bruza and
Cole, 2005), the emergence of new concepts (Aerts
and Gabora, 2005), and the human mental lexicon
(Bruza et al., 2009). Chen (2002) captures syllo-
gisms in a quantum theoretic framework; the model
takes statements like All whales are mammals and
all mammals are animals as input and outputs con-
clusions like All whales are animals.
The first attempts to connect the mathematical
basis of semantic space models with quantum the-
ory are due to Aerts and Czachor (2004) and Bruza
and Cole (2005). They respectively demonstrate
that Latent Semantic Analysis (Landauer and Du-
mais, 1997) and the Hyperspace Analog to Lan-
guage model (Lund and Burgess, 1996) are essen-
tially Hilbert space formalisms, without, however,
providing concrete ways of building these models
beyond a few hand-picked examples. Interestingly,
Bruza and Cole (2005) show how lexical operators
may be contrived from corpus co-occurrence counts,
albeit admitting to the fact that their operators do not
provide sensical eigenkets, most likely because of
the simplified method of populating the matrix from
corpus statistics. Grefenstette et al. (2011) present a
model for capturing semantic composition in a quan-
tum theoretical context, although it appears to be
reducible to the classical probabilistic paradigm. It
does not make use of the unique aspects of quantum
theory (e.g., entanglement, interference, or quantum
collapse).
Our own work follows Aerts and Czachor (2004)
and Bruza and Cole (2005) in formulating a model
that exhibits important aspects of quantum theory.
Contrary to them, we present a fully-fledged seman-
tic space rather than a proof-of-concept. We obtain
quantum states (i.e., lexical representations) for each
word by taking its syntactic context into account.
Quantum states are expressed as density operators
rather than kets. While a ket can only capture one
pure state of a system, a density operator contains
an ensemble of pure states which we argue is advan-
tageous from a modeling perspective. Within this
framework, not only can we compute the meaning of
individual words but also phrases or sentences, with-
out postulating any additional operations. Compo-
sitional meaning reduces to quantum measurement
at each inner node of the (dependency) parse of the
structure in question.
</bodyText>
<table confidence="0.992153142857143">
SDS pub, snack, restau-
rant, grill, coctail
DM counter, rack, strip,
pipe, code
NLM room, pole, drink,
rail, coctail
QM prison, liquor, beer,
club, graph
HS drink, beer, stool, al-
cohol, grill
form, direct, proce-
dure, plan, request
court, demand, form,
law, list
direct, command,
plan, court, demand
organization, food,
law, structure,
regulation
food, form, law, heat,
court
</table>
<page confidence="0.996715">
855
</page>
<sectionHeader confidence="0.983289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999825923809524">
Diederik Aerts and Marek Czachor. 2004. Quantum as-
pects of semantic analysis and symbolic artificial intel-
ligence. Journal of PhysicsA: Mathematical and Gen-
eral, 37:123–132.
Diederik Aerts and Liane Gabora. 2005. A state-context-
property model of concepts and their combinations
ii: A hilbert space representation. Kybernetes, 1–
2(34):192–221.
Diederik Aerts. 2009. Quantum structure in cognition.
Journal of Mathematical Psychology, 53:314–348.
Belal E. Baaquie. 2004. Quantum Finance: Path In-
tegrals and Hamiltonians for Oprions and Interest
Rates. Cambridge University Press, Cambridge.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673–
721.
Yoshua Bengio. 2001. Neural net language models.
Scholarpedia, 3(1):3881.
William Blacoe and Mirella Lapata. 2012. A compari-
son of vector-based representations for semantic com-
position. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 546–556, Jeju Island, Korea, July. Association
for Computational Linguistics.
Peter D. Bruza and Richard J. Cole. 2005. Quantum
logic of semantic space: An exploratory investigation
of context effects in practical reasoning. In S. Arte-
mov, H. Barringer, S. A. d’Avila Garcez, L. C. Lamb,
and J. Woods, editors, We Will Show Them: Essays
in Honour of Dov Gabbay, volume 1, pages 339–361.
London: College Publications.
Peter D. Bruza, Kirsty Kitto, Douglas McEnvoy, and
Cathy McEnvoy. 2008. Entangling words and mean-
ing. In Second Quantum Interaction Symposium. Ox-
ford University.
Peter D. Bruza, Kirsty Kitto, Douglas Nelson, and Cathy
McEvoy. 2009. Is there something quantum-like in
the human mental lexicon? Journal of Mathematical
Psychology, 53(5):362–377.
Jerome R. Busemeyer and Peter D. Bruza. 2012. Quan-
tum Models of Cognition and Decision. Cambridge
University Press, Cambridge.
Joseph C. H. Chen. 2002. Quantum computation and
natural language processing. Ph.D. thesis, Universit¨at
Hamburg.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neural
networks with multita sk learning. In Proceedings of
the 25th International Conference on Machine Learn-
ing, pages 160–167, New York, NY. ACM.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Coling 2008: Proceedings of the work-
shop on Cross-Framework and Cross-Domain Parser
Evaluation, pages 1–8, Manchester, UK.
Guy Denhi`ere and Benoit Lemaire. 2004. A compu-
tational model of children’s semantic memory. In
Proceedings of the 26th Annual Meeting of the Cog-
nitive Science Society, pages 297–302, Mahwah, NJ.
Lawrence Erlbaum Associates.
Paul A. M. Dirac. 1939. A new notation for quantum me-
chanics. Mathematical Proceedings of the Cambridge
Philosophical Society, 35:416–418.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing Search in Context: The Con-
cept Revisited. ACM Transactions on Information Sys-
tems, 20(1):116–131, January.
John R. Firth. 1957. A synopsis of linguistic theory
1930-1955. In Studies in Linguistic Analysis, pages
1–32. Philological Society, Oxford.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distribu-
tional models of meaning. Proceedings of the 9th In-
ternational Conference on Computational Semantics
(IWCS11), pages 125–134.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representation.
Psychological Review, 114(2):211–244.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 873–882, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
R. I. G. Hughes. 1989. The Structure and Interpreta-
tion of Quantum Mechnics. Harvard University Press,
Cambridge, MA.
Chris J. Isham. 1989. Lectures on Quantum Theory. Sin-
gapore: World Scientific.
Andrei Y. Khrennikov. 2010. Ubiquitous Quantum
Structure: From Psychology to Finance. Springer.
Daniel Kleppner and Roman Jackiw. 2000. One hundred
years of quantum physics. Science, 289(5481):893–
898.
Darrell R. Laham. 2000. Automated Content Assess-
ment of Text Using Latent Semantic Analysis to Sim-
ulate Human Cognition. Ph.D. thesis, University of
Colorado at Boulder.
</reference>
<page confidence="0.988686">
856
</page>
<reference confidence="0.999490836363636">
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to Plato’s problem: the latent semantic analysis
theory of acquisition, induction and representation of
knowledge. Psychological Review, 104(2):211–240.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the joint Annual
Meeting of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics, pages 768–774, Montr´eal, Canada.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments
&amp; Computers, 28:203–208.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
38(8):1388–1429.
Douglas L. Nelson, Cathy L. McEvoy, and Thomas A.
Schreiber. 1998. The University of South Florida
Word Association, Rhyme, and Word Fragment
Norms.
Michael A. Nielsen and Isaac L. Chuang. 2010. Quan-
tum Computation and Information Theory. Cambridge
University Press, Cambridge.
Daniel Osherson and Edward E. Smith. 1981. On the
adequacy of prototype theory as a theory of concepts.
Cognition, 9:35–38.
Emmanuel M. Pothos and Jerome R. Busemeyer. 2012.
Can quantum probability provide a new direction for
cognitive modeling? Behavioral and Brain Sciences.
to appear.
Hinrich Sch¨utze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97–124.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1423–1433, Jeju
Island, Korea. Association for Computational Linguis-
tics.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In J. Shawe-Taylor, R.S.
Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Wein-
berger, editors, Advances in Neural Information Pro-
cessing Systems 24, pages 801–809.
Amos Tversky and Daniel Kahneman. 1983. Exten-
sional versus intuitive reasoning: The conjuctive fal-
lacy in probability judgment. Psychological Review,
4(90):293–315.
Amos Tversky and Eldar Shafir. 1992. The disjunction
effect in choice under uncertainty. Psychological Sci-
ence, 3(5):305–309.
Vlatko Vedral. 2006. Introduction to Quantum Informa-
tion Science. Oxford University Press, New York.
</reference>
<page confidence="0.998208">
857
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.273023">
<title confidence="0.983144">A Quantum-Theoretic Approach to Distributional Semantics</title>
<author confidence="0.723956">Elham Mirella</author>
<note confidence="0.447589333333333">for Language, Cognition and for Foundations of Computer School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh EH8</note>
<email confidence="0.992554">w.b.blacoe@sms.ed.ac.uk,ekashefi@inf.ed.ac.uk,mlap@inf.ed.ac.uk</email>
<abstract confidence="0.998439933333334">In this paper we explore the potential of quantum theory as a formal framework for capturing lexical meaning. We present a novel semantic space model that is syntactically aware, takes word order into account, and features key quantum aspects such as superposition and entanglement. We define a dependency-based Hilbert space and show how to represent the meaning of words by density matrices that encode dependency neighborhoods. Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Diederik Aerts</author>
<author>Marek Czachor</author>
</authors>
<title>Quantum aspects of semantic analysis and symbolic artificial intelligence.</title>
<date>2004</date>
<journal>Journal of PhysicsA: Mathematical and General,</journal>
<pages>37--123</pages>
<contexts>
<context position="33759" citStr="Aerts and Czachor (2004)" startWordPosition="5530" endWordPosition="5533">that human behavior ought to conform to the law of total probability quantum theory in order to model semantic phenomena such as concept combination (Bruza and Cole, 2005), the emergence of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mammals are animals as input and outputs conclusions like All whales are animals. The first attempts to connect the mathematical basis of semantic space models with quantum theory are due to Aerts and Czachor (2004) and Bruza and Cole (2005). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hilbert space formalisms, without, however, providing concrete ways of building these models beyond a few hand-picked examples. Interestingly, Bruza and Cole (2005) show how lexical operators may be contrived from corpus co-occurrence counts, albeit admitting to the fact that their operators do not provide sensical eigenkets, most likely because of the simplified method of populating the matrix f</context>
</contexts>
<marker>Aerts, Czachor, 2004</marker>
<rawString>Diederik Aerts and Marek Czachor. 2004. Quantum aspects of semantic analysis and symbolic artificial intelligence. Journal of PhysicsA: Mathematical and General, 37:123–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diederik Aerts</author>
<author>Liane Gabora</author>
</authors>
<title>A state-contextproperty model of concepts and their combinations ii: A hilbert space representation.</title>
<date>2005</date>
<journal>Kybernetes,</journal>
<volume>1</volume>
<pages>2--34</pages>
<contexts>
<context position="33362" citStr="Aerts and Gabora, 2005" startWordPosition="5463" endWordPosition="5466">uantum interference effect. And the asymmetry of similarity relations has been explained by postulating that different concepts correspond to subspaces of different dimensionality (Pothos and Busemeyer, 2012). Several approaches have drawn on 3A conjunction fallacy occurs when it is assumed that specific conditions are more probable than a single general one. 4The principle is the expectation that human behavior ought to conform to the law of total probability quantum theory in order to model semantic phenomena such as concept combination (Bruza and Cole, 2005), the emergence of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mammals are animals as input and outputs conclusions like All whales are animals. The first attempts to connect the mathematical basis of semantic space models with quantum theory are due to Aerts and Czachor (2004) and Bruza and Cole (2005). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hi</context>
</contexts>
<marker>Aerts, Gabora, 2005</marker>
<rawString>Diederik Aerts and Liane Gabora. 2005. A state-contextproperty model of concepts and their combinations ii: A hilbert space representation. Kybernetes, 1– 2(34):192–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diederik Aerts</author>
</authors>
<title>Quantum structure in cognition.</title>
<date>2009</date>
<journal>Journal of Mathematical Psychology,</journal>
<pages>53--314</pages>
<contexts>
<context position="2676" citStr="Aerts, 2009" startWordPosition="393" endWordPosition="394"> Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantum theory, which endow it with a unique character.1 Superposition is a way of modeling uncertainty, more so than in classical probability theory. It contains information about the potentialities of a system’s state. An electron whose location in an atom is uncertain can be modeled as being in a superposition of locations. Analogously, words in natural language can have multiple</context>
<context position="5199" citStr="Aerts, 2009" startWordPosition="788" endWordPosition="789"> by default that measurements are compatible, that is, independent, and as a result the order in which these take place does not matter. By contrast in quantum theory, measurements may share (hidden) order-sensitive inter-dependencies and the outcome of the first measurement can change the outcome of the second measurement. Interference is a feature of quantum probability that can cause classical assumptions such as the law of total probability to be violated. When concepts interact their joint representation can exhibit nonclassical behavior, e.g., with regard to conjunction and disjunction (Aerts, 2009). An often cited example is the “guppy effect”. Although guppy is an example of a pet-fish it is neither a very typical pet nor fish (Osherson and Smith, 1981). In the following we use the rich mathematical framework of quantum theory to model semantic information. Specifically, we show how word meanings can be expressed as quantum states. A word brings with it its own subspace which is spanned by vectors representing its potential usages. We present a specific implementation of a semantic space that is syntactically aware, takes word order into account, and features key aspects of quantum the</context>
</contexts>
<marker>Aerts, 2009</marker>
<rawString>Diederik Aerts. 2009. Quantum structure in cognition. Journal of Mathematical Psychology, 53:314–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Belal E Baaquie</author>
</authors>
<title>Quantum Finance: Path Integrals and Hamiltonians for Oprions and Interest Rates.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2507" citStr="Baaquie, 2004" startWordPosition="369" endWordPosition="370"> we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantum theory, which endow it with a unique character.1 Superposition is a way of modeling uncertainty, more so than in classical probability theory. It contains information about the potentialities of a system’s st</context>
</contexts>
<marker>Baaquie, 2004</marker>
<rawString>Belal E. Baaquie. 2004. Quantum Finance: Path Integrals and Hamiltonians for Oprions and Interest Rates. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>721</pages>
<contexts>
<context position="28732" citStr="Baroni and Lenci, 2010" startWordPosition="4712" endWordPosition="4715">as well as the phases assigned to each cluster greatly influenced the semantic space. On both tasks, the best performing model had the relation partition described in Section 3.1. Section 5 reports our results on the test set using this model. Comparison Models We compared our quantum space against three classical distributional models. These include a simple semantic space, where a word’s meaning is a vector of co-occurrences with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional triples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010) and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008) For all three models we used parameters that have been reported in the literature as optimal. Specifically, for the simple co-occurrence-based space we follow the settings of Mitchell and Lapata (2010): a context window of five words on either side of the target word and 2,000 vector dimensions (i.e., the 2000 most common context words in the BNC). Vector components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673– 721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Neural net language models.</title>
<date>2001</date>
<journal>Scholarpedia,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="28804" citStr="Bengio, 2001" startWordPosition="4725" endWordPosition="4726">e. On both tasks, the best performing model had the relation partition described in Section 3.1. Section 5 reports our results on the test set using this model. Comparison Models We compared our quantum space against three classical distributional models. These include a simple semantic space, where a word’s meaning is a vector of co-occurrences with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional triples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010) and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008) For all three models we used parameters that have been reported in the literature as optimal. Specifically, for the simple co-occurrence-based space we follow the settings of Mitchell and Lapata (2010): a context window of five words on either side of the target word and 2,000 vector dimensions (i.e., the 2000 most common context words in the BNC). Vector components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall. For the neural language model, we adopted the best Models WordSim353 N</context>
</contexts>
<marker>Bengio, 2001</marker>
<rawString>Yoshua Bengio. 2001. Neural net language models. Scholarpedia, 3(1):3881.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="29794" citStr="Blacoe and Lapata, 2012" startWordPosition="4883" endWordPosition="4886">s in the BNC). Vector components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall. For the neural language model, we adopted the best Models WordSim353 Nelson Norms SDS 0.433 0.151 DM 0.318 0.123 NLM 0.196 0.091 QM 0.535 0.185 Table 1: Performance of distributional models on WordSim353 dataset and Nelson et al.’s (1998) norms (test set). Correlation coefficients are all statistically significant (p &lt; 0.01). performing parameters from our earlier comparison of different vector sources for distributional semantics (Blacoe and Lapata, 2012) where we also used the BNC for training. There we obtained best results with 50 dimensions, a context window of size 4, and an embedding learning rate of 10−9. Our third comparison model uses Baroni and Lenci’s (2010) third-order tensor2 which they obtained from a very large dependency-parsed corpus containing approximately 2.3 billion words. Their tensor assigns a mutual information score to instances of word pairs w,v and a linking word l. We obtained vectors w from the tensor following the methodology proposed in Blacoe and Lapata (2012) using 100 (l,v) contexts as dimensions. 5 Results Ou</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Bruza</author>
<author>Richard J Cole</author>
</authors>
<title>Quantum logic of semantic space: An exploratory investigation of context effects in practical reasoning.</title>
<date>2005</date>
<booktitle>We Will Show Them: Essays in Honour of Dov Gabbay,</booktitle>
<volume>1</volume>
<pages>339--361</pages>
<editor>In S. Artemov, H. Barringer, S. A. d’Avila Garcez, L. C. Lamb, and J. Woods, editors,</editor>
<publisher>College Publications.</publisher>
<location>London:</location>
<contexts>
<context position="33306" citStr="Bruza and Cole, 2005" startWordPosition="5454" endWordPosition="5457">ky and Shafir, 1992) have been modeled in terms of a quantum interference effect. And the asymmetry of similarity relations has been explained by postulating that different concepts correspond to subspaces of different dimensionality (Pothos and Busemeyer, 2012). Several approaches have drawn on 3A conjunction fallacy occurs when it is assumed that specific conditions are more probable than a single general one. 4The principle is the expectation that human behavior ought to conform to the law of total probability quantum theory in order to model semantic phenomena such as concept combination (Bruza and Cole, 2005), the emergence of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mammals are animals as input and outputs conclusions like All whales are animals. The first attempts to connect the mathematical basis of semantic space models with quantum theory are due to Aerts and Czachor (2004) and Bruza and Cole (2005). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to La</context>
<context position="34760" citStr="Bruza and Cole (2005)" startWordPosition="5681" endWordPosition="5684">al operators may be contrived from corpus co-occurrence counts, albeit admitting to the fact that their operators do not provide sensical eigenkets, most likely because of the simplified method of populating the matrix from corpus statistics. Grefenstette et al. (2011) present a model for capturing semantic composition in a quantum theoretical context, although it appears to be reducible to the classical probabilistic paradigm. It does not make use of the unique aspects of quantum theory (e.g., entanglement, interference, or quantum collapse). Our own work follows Aerts and Czachor (2004) and Bruza and Cole (2005) in formulating a model that exhibits important aspects of quantum theory. Contrary to them, we present a fully-fledged semantic space rather than a proof-of-concept. We obtain quantum states (i.e., lexical representations) for each word by taking its syntactic context into account. Quantum states are expressed as density operators rather than kets. While a ket can only capture one pure state of a system, a density operator contains an ensemble of pure states which we argue is advantageous from a modeling perspective. Within this framework, not only can we compute the meaning of individual wor</context>
</contexts>
<marker>Bruza, Cole, 2005</marker>
<rawString>Peter D. Bruza and Richard J. Cole. 2005. Quantum logic of semantic space: An exploratory investigation of context effects in practical reasoning. In S. Artemov, H. Barringer, S. A. d’Avila Garcez, L. C. Lamb, and J. Woods, editors, We Will Show Them: Essays in Honour of Dov Gabbay, volume 1, pages 339–361. London: College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Bruza</author>
<author>Kirsty Kitto</author>
<author>Douglas McEnvoy</author>
<author>Cathy McEnvoy</author>
</authors>
<title>Entangling words and meaning.</title>
<date>2008</date>
<booktitle>In Second Quantum Interaction Symposium.</booktitle>
<location>Oxford University.</location>
<contexts>
<context position="2697" citStr="Bruza et al., 2008" startWordPosition="395" endWordPosition="398">r an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantum theory, which endow it with a unique character.1 Superposition is a way of modeling uncertainty, more so than in classical probability theory. It contains information about the potentialities of a system’s state. An electron whose location in an atom is uncertain can be modeled as being in a superposition of locations. Analogously, words in natural language can have multiple meanings. In isolati</context>
</contexts>
<marker>Bruza, Kitto, McEnvoy, McEnvoy, 2008</marker>
<rawString>Peter D. Bruza, Kirsty Kitto, Douglas McEnvoy, and Cathy McEnvoy. 2008. Entangling words and meaning. In Second Quantum Interaction Symposium. Oxford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Bruza</author>
<author>Kirsty Kitto</author>
<author>Douglas Nelson</author>
<author>Cathy McEvoy</author>
</authors>
<title>Is there something quantum-like in the human mental lexicon?</title>
<date>2009</date>
<journal>Journal of Mathematical Psychology,</journal>
<volume>53</volume>
<issue>5</issue>
<contexts>
<context position="33413" citStr="Bruza et al., 2009" startWordPosition="5472" endWordPosition="5475">rity relations has been explained by postulating that different concepts correspond to subspaces of different dimensionality (Pothos and Busemeyer, 2012). Several approaches have drawn on 3A conjunction fallacy occurs when it is assumed that specific conditions are more probable than a single general one. 4The principle is the expectation that human behavior ought to conform to the law of total probability quantum theory in order to model semantic phenomena such as concept combination (Bruza and Cole, 2005), the emergence of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mammals are animals as input and outputs conclusions like All whales are animals. The first attempts to connect the mathematical basis of semantic space models with quantum theory are due to Aerts and Czachor (2004) and Bruza and Cole (2005). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hilbert space formalisms, without, however, providing</context>
</contexts>
<marker>Bruza, Kitto, Nelson, McEvoy, 2009</marker>
<rawString>Peter D. Bruza, Kirsty Kitto, Douglas Nelson, and Cathy McEvoy. 2009. Is there something quantum-like in the human mental lexicon? Journal of Mathematical Psychology, 53(5):362–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome R Busemeyer</author>
<author>Peter D Bruza</author>
</authors>
<title>Quantum Models of Cognition and Decision.</title>
<date>2012</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2663" citStr="Busemeyer and Bruza, 2012" startWordPosition="389" endWordPosition="392">larity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantum theory, which endow it with a unique character.1 Superposition is a way of modeling uncertainty, more so than in classical probability theory. It contains information about the potentialities of a system’s state. An electron whose location in an atom is uncertain can be modeled as being in a superposition of locations. Analogously, words in natural language can </context>
</contexts>
<marker>Busemeyer, Bruza, 2012</marker>
<rawString>Jerome R. Busemeyer and Peter D. Bruza. 2012. Quantum Models of Cognition and Decision. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph C H Chen</author>
</authors>
<title>Quantum computation and natural language processing.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at Hamburg.</institution>
<contexts>
<context position="33426" citStr="Chen (2002)" startWordPosition="5476" endWordPosition="5477">en explained by postulating that different concepts correspond to subspaces of different dimensionality (Pothos and Busemeyer, 2012). Several approaches have drawn on 3A conjunction fallacy occurs when it is assumed that specific conditions are more probable than a single general one. 4The principle is the expectation that human behavior ought to conform to the law of total probability quantum theory in order to model semantic phenomena such as concept combination (Bruza and Cole, 2005), the emergence of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mammals are animals as input and outputs conclusions like All whales are animals. The first attempts to connect the mathematical basis of semantic space models with quantum theory are due to Aerts and Czachor (2004) and Bruza and Cole (2005). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hilbert space formalisms, without, however, providing concrete way</context>
</contexts>
<marker>Chen, 2002</marker>
<rawString>Joseph C. H. Chen. 2002. Quantum computation and natural language processing. Ph.D. thesis, Universit¨at Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multita sk learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY.</location>
<contexts>
<context position="28833" citStr="Collobert and Weston, 2008" startWordPosition="4727" endWordPosition="4730">ks, the best performing model had the relation partition described in Section 3.1. Section 5 reports our results on the test set using this model. Comparison Models We compared our quantum space against three classical distributional models. These include a simple semantic space, where a word’s meaning is a vector of co-occurrences with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional triples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010) and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008) For all three models we used parameters that have been reported in the literature as optimal. Specifically, for the simple co-occurrence-based space we follow the settings of Mitchell and Lapata (2010): a context window of five words on either side of the target word and 2,000 vector dimensions (i.e., the 2000 most common context words in the BNC). Vector components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall. For the neural language model, we adopted the best Models WordSim353 Nelson Norms SDS 0.433 0.151 D</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multita sk learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167, New York, NY. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<location>Manchester, UK.</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Denhi`ere</author>
<author>Benoit Lemaire</author>
</authors>
<title>A computational model of children’s semantic memory.</title>
<date>2004</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>297--302</pages>
<location>Mahwah, NJ. Lawrence Erlbaum Associates.</location>
<marker>Denhi`ere, Lemaire, 2004</marker>
<rawString>Guy Denhi`ere and Benoit Lemaire. 2004. A computational model of children’s semantic memory. In Proceedings of the 26th Annual Meeting of the Cognitive Science Society, pages 297–302, Mahwah, NJ. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul A M Dirac</author>
</authors>
<title>A new notation for quantum mechanics.</title>
<date>1939</date>
<booktitle>Mathematical Proceedings of the Cambridge Philosophical Society,</booktitle>
<pages>35--416</pages>
<contexts>
<context position="6901" citStr="Dirac (1939)" startWordPosition="1084" endWordPosition="1085">form, with absolute value r = |c |and phase θ. Its complex conjugate c∗ = re−iθ has the inverse phase. Thus, their product cc∗ = (reiθ)(re−iθ) = r2 is real. 2.1 Vectors We are interested in finite-dimensional, complexvalued vector spaces Cn with an inner product, otherwise known as Hilbert space. A column vector →−ψ ∈ Cn can be written as an ordered vertical array of its n complex-valued components, or alternatively as a weighted sum of base vectors: Whereas Equation (1) uses base vectors from the −→ bn}, any other set of n orthonormal vectors serves just as well as a base for the same space. Dirac (1939) introduced the socalled bra-ket notation which is equally expressive but notationally more convenient. A column vector becomes a ket: →−ψ ≡ |ψi = ψ1|b1i+ψ2|b2i+...+ψn|bni (2) and a row vector becomes a bra hψ|. Transposing a complex-valued vector or matrix (via the superscript “†”) involves complex-conjugating all components: |ψi† = hψ |= ψ∗1hb1 |+ψ∗2hb2|+...+ψ∗nhbn |(3) The Dirac notation for the inner product h·|·i illustrates the origin of the terminology “bra-ket”. Since Bstd’s elements are normalised and pairwise orthogonal their inner product is: � 1, if i = j hbi|bji = (4) 0, otherwise</context>
</contexts>
<marker>Dirac, 1939</marker>
<rawString>Paul A. M. Dirac. 1939. A new notation for quantum mechanics. Mathematical Proceedings of the Cambridge Philosophical Society, 35:416–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="25664" citStr="Finkelstein et al., 2002" startWordPosition="4225" endWordPosition="4228">er’s usages via their pairwise inner prod4 Experimental Setup Data All our experiments used a dependency parsed and lemmatized version of the British National Corpus (BNC). As mentioned in Section 3, we obtained dependencies from the output of the Stanford parser (de Marneffe and Manning, 2008). The BNC comprises 4,049 texts totalling approximately 100 million words. Evaluation Tasks We evaluated our model on word similarity and association. Both tasks are employed routinely to assess how well semantic models predict human judgments of word relatedness. We used the WordSim353 test collection (Finkelstein et al., 2002) which consists of similarity judgments for word pairs. Participants gave each pair a similarity rating using a 0 to 10 scale (e.g., tiger–cat are very similar, whereas delay–racism are not). The average rating for each pair represents an estimate of the perceived similarity of the two words. The collection contains ratings for 437 unique words (353 pairs) all of which appeared in our corpus. Word association is a slightly different task: Participants are given a cue word (e.g., rice) and asked to name an associate in response (e.g., Chinese, wedding, food, white). We used the norms collected </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116–131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<publisher>Philological Society,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1699" citStr="Firth, 1957" startWordPosition="236" endWordPosition="237">ve track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A synopsis of linguistic theory 1930-1955. In Studies in Linguistic Analysis, pages 1–32. Philological Society, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
<author>Bob Coecke</author>
<author>Stephen Pulman</author>
</authors>
<title>Concrete sentence spaces for compositional distributional models of meaning.</title>
<date>2011</date>
<booktitle>Proceedings of the 9th International Conference on Computational Semantics (IWCS11),</booktitle>
<pages>125--134</pages>
<contexts>
<context position="34408" citStr="Grefenstette et al. (2011)" startWordPosition="5626" endWordPosition="5629">05). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hilbert space formalisms, without, however, providing concrete ways of building these models beyond a few hand-picked examples. Interestingly, Bruza and Cole (2005) show how lexical operators may be contrived from corpus co-occurrence counts, albeit admitting to the fact that their operators do not provide sensical eigenkets, most likely because of the simplified method of populating the matrix from corpus statistics. Grefenstette et al. (2011) present a model for capturing semantic composition in a quantum theoretical context, although it appears to be reducible to the classical probabilistic paradigm. It does not make use of the unique aspects of quantum theory (e.g., entanglement, interference, or quantum collapse). Our own work follows Aerts and Czachor (2004) and Bruza and Cole (2005) in formulating a model that exhibits important aspects of quantum theory. Contrary to them, we present a fully-fledged semantic space rather than a proof-of-concept. We obtain quantum states (i.e., lexical representations) for each word by taking </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, Clark, Coecke, Pulman, 2011</marker>
<rawString>Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Conference on Computational Semantics (IWCS11), pages 125–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1458" citStr="Grefenstette, 1994" startWordPosition="198" endWordPosition="200">tion reveal that our model achieves results competitive with a variety of classical models. 1 Introduction The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and associati</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation.</title>
<date>2007</date>
<journal>Psychological Review,</journal>
<volume>114</volume>
<issue>2</issue>
<contexts>
<context position="1273" citStr="Griffiths et al., 2007" startWordPosition="172" endWordPosition="175">We define a dependency-based Hilbert space and show how to represent the meaning of words by density matrices that encode dependency neighborhoods. Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models. 1 Introduction The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional s</context>
<context position="30845" citStr="Griffiths et al., 2007" startWordPosition="5055" endWordPosition="5059"> a linking word l. We obtained vectors w from the tensor following the methodology proposed in Blacoe and Lapata (2012) using 100 (l,v) contexts as dimensions. 5 Results Our results are summarized in Table 1. As can be seen, the quantum model (QM) obtains performance superior to other better-known models such as Mitchell and Lapata’s (2010) simple semantic space (SDS), Baroni and Lenci’s (2010) distributional memory tensor (DM), and Collobert and Weston’s (2008) neural language model (NLM). Our results on the association norms are comparable to the state of the art (Silberer and Lapata, 2012; Griffiths et al., 2007). With regard to WordSim353, Huang et al. (2012) report correlations in the range of 0.713–0.769, however they use Wikipedia as a training corpus and a more sophisticated version of the NLM presented here, that takes into account global context and performs word sense discrimination. In the future, we also plan to evaluate our model on larger Wikipedia-scale corpora. We would also like to model semantic composition as our approach can do this easily by taking advantage of the notion of quantum measurement. Specifically, we 2Available at http://clic.cimec.unitn.it/dm/. 854 Models bar order Tabl</context>
</contexts>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psychological Review, 114(2):211–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Huang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>873--882</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="30893" citStr="Huang et al. (2012)" startWordPosition="5064" endWordPosition="5067">nsor following the methodology proposed in Blacoe and Lapata (2012) using 100 (l,v) contexts as dimensions. 5 Results Our results are summarized in Table 1. As can be seen, the quantum model (QM) obtains performance superior to other better-known models such as Mitchell and Lapata’s (2010) simple semantic space (SDS), Baroni and Lenci’s (2010) distributional memory tensor (DM), and Collobert and Weston’s (2008) neural language model (NLM). Our results on the association norms are comparable to the state of the art (Silberer and Lapata, 2012; Griffiths et al., 2007). With regard to WordSim353, Huang et al. (2012) report correlations in the range of 0.713–0.769, however they use Wikipedia as a training corpus and a more sophisticated version of the NLM presented here, that takes into account global context and performs word sense discrimination. In the future, we also plan to evaluate our model on larger Wikipedia-scale corpora. We would also like to model semantic composition as our approach can do this easily by taking advantage of the notion of quantum measurement. Specifically, we 2Available at http://clic.cimec.unitn.it/dm/. 854 Models bar order Table 2: Associates for bar and order ranked accordi</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric Huang, Richard Socher, Christopher Manning, and Andrew Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 873–882, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R I G Hughes</author>
</authors>
<title>The Structure and Interpretation of Quantum Mechnics.</title>
<date>1989</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2283" citStr="Hughes, 1989" startWordPosition="335" endWordPosition="336">to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related asp</context>
</contexts>
<marker>Hughes, 1989</marker>
<rawString>R. I. G. Hughes. 1989. The Structure and Interpretation of Quantum Mechnics. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris J Isham</author>
</authors>
<date>1989</date>
<booktitle>Lectures on Quantum Theory. Singapore: World Scientific.</booktitle>
<contexts>
<context position="2297" citStr="Isham, 1989" startWordPosition="337" endWordPosition="338"> (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantu</context>
</contexts>
<marker>Isham, 1989</marker>
<rawString>Chris J. Isham. 1989. Lectures on Quantum Theory. Singapore: World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Y Khrennikov</author>
</authors>
<title>Ubiquitous Quantum Structure: From Psychology to Finance.</title>
<date>2010</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2584" citStr="Khrennikov, 2010" startWordPosition="378" endWordPosition="380">ring lexical meaning and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantum theory, which endow it with a unique character.1 Superposition is a way of modeling uncertainty, more so than in classical probability theory. It contains information about the potentialities of a system’s state. An electron whose location in an atom is uncertain can be modeled as bei</context>
</contexts>
<marker>Khrennikov, 2010</marker>
<rawString>Andrei Y. Khrennikov. 2010. Ubiquitous Quantum Structure: From Psychology to Finance. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Kleppner</author>
<author>Roman Jackiw</author>
</authors>
<title>One hundred years of quantum physics.</title>
<date>2000</date>
<journal>Science,</journal>
<volume>289</volume>
<issue>5481</issue>
<pages>898</pages>
<contexts>
<context position="3915" citStr="Kleppner and Jackiw (2000)" startWordPosition="593" endWordPosition="596"> isolation, the word pen may refer to a writing implement, an enclosure for confining livestock, a playpen, a penitentiary or a female swan. However, when observed in the context of the word ink the ambiguity resolves into the sense of the word dealing with writing. The meanings of words in a semantic space are superposed in a way which is intuitively similar to the atom’s electron. Entanglement concerns the relationship between 1It is outside the scope of the current paper to give a detailed introduction on the history of quantum mechanics. We refer the interested reader to Vedral (2006) and Kleppner and Jackiw (2000) for comprehensive overviews. 847 Proceedings of NAACL-HLT 2013, pages 847–857, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics systems for which it is impossible to specify a joint probability distribution from the probability distributions of their constituent parts. With regard to word meanings, entanglement encodes (hidden) relationships between concepts. The different senses of a word “exist in parallel” until it is observed in some context. This reduction of ambiguity has effects on other concepts connected via entanglement. The notion of incompatibilit</context>
</contexts>
<marker>Kleppner, Jackiw, 2000</marker>
<rawString>Daniel Kleppner and Roman Jackiw. 2000. One hundred years of quantum physics. Science, 289(5481):893– 898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darrell R Laham</author>
</authors>
<title>Automated Content Assessment of Text Using Latent Semantic Analysis to Simulate Human Cognition.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Colorado at Boulder.</institution>
<contexts>
<context position="1397" citStr="Laham, 2000" startWordPosition="191" endWordPosition="192">ighborhoods. Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models. 1 Introduction The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and mode</context>
</contexts>
<marker>Laham, 2000</marker>
<rawString>Darrell R. Laham. 2000. Automated Content Assessment of Text Using Latent Semantic Analysis to Simulate Human Cognition. Ph.D. thesis, University of Colorado at Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1342" citStr="Landauer and Dumais, 1997" startWordPosition="182" endWordPosition="185">t the meaning of words by density matrices that encode dependency neighborhoods. Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models. 1 Introduction The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a </context>
<context position="33874" citStr="Landauer and Dumais, 1997" startWordPosition="5546" endWordPosition="5550">enomena such as concept combination (Bruza and Cole, 2005), the emergence of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mammals are animals as input and outputs conclusions like All whales are animals. The first attempts to connect the mathematical basis of semantic space models with quantum theory are due to Aerts and Czachor (2004) and Bruza and Cole (2005). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hilbert space formalisms, without, however, providing concrete ways of building these models beyond a few hand-picked examples. Interestingly, Bruza and Cole (2005) show how lexical operators may be contrived from corpus co-occurrence counts, albeit admitting to the fact that their operators do not provide sensical eigenkets, most likely because of the simplified method of populating the matrix from corpus statistics. Grefenstette et al. (2011) present a model for capturing semantic composition in a quantum t</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1470" citStr="Lin, 1998" startWordPosition="201" endWordPosition="202"> model achieves results competitive with a variety of classical models. 1 Introduction The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Sect</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the joint Annual Meeting of the Association for Computational Linguistics and International Conference on Computational Linguistics, pages 768–774, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments &amp; Computers,</journal>
<pages>28--203</pages>
<contexts>
<context position="1315" citStr="Lund and Burgess, 1996" startWordPosition="178" endWordPosition="181">and show how to represent the meaning of words by density matrices that encode dependency neighborhoods. Experiments on word similarity and association reveal that our model achieves results competitive with a variety of classical models. 1 Introduction The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potent</context>
<context position="33943" citStr="Lund and Burgess, 1996" startWordPosition="5559" endWordPosition="5562">e of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mammals are animals as input and outputs conclusions like All whales are animals. The first attempts to connect the mathematical basis of semantic space models with quantum theory are due to Aerts and Czachor (2004) and Bruza and Cole (2005). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hilbert space formalisms, without, however, providing concrete ways of building these models beyond a few hand-picked examples. Interestingly, Bruza and Cole (2005) show how lexical operators may be contrived from corpus co-occurrence counts, albeit admitting to the fact that their operators do not provide sensical eigenkets, most likely because of the simplified method of populating the matrix from corpus statistics. Grefenstette et al. (2011) present a model for capturing semantic composition in a quantum theoretical context, although it appears to be reducible to the classi</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments &amp; Computers, 28:203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>38</volume>
<issue>8</issue>
<contexts>
<context position="28590" citStr="Mitchell and Lapata, 2010" startWordPosition="4693" endWordPosition="4696">pective relation cluster RCj (see Section 3.2 for details). In general, we observed that the choice of dependency relations and their clustering as well as the phases assigned to each cluster greatly influenced the semantic space. On both tasks, the best performing model had the relation partition described in Section 3.1. Section 5 reports our results on the test set using this model. Comparison Models We compared our quantum space against three classical distributional models. These include a simple semantic space, where a word’s meaning is a vector of co-occurrences with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional triples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010) and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008) For all three models we used parameters that have been reported in the literature as optimal. Specifically, for the simple co-occurrence-based space we follow the settings of Mitchell and Lapata (2010): a context window of five words on either side of the target word and 2,000 vector dimensions (i.e., the 2000 most common context words in the BNC). Vecto</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 38(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas L Nelson</author>
<author>Cathy L McEvoy</author>
<author>Thomas A Schreiber</author>
</authors>
<date>1998</date>
<institution>The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</institution>
<contexts>
<context position="26287" citStr="Nelson et al. (1998)" startWordPosition="4330" endWordPosition="4333">ich consists of similarity judgments for word pairs. Participants gave each pair a similarity rating using a 0 to 10 scale (e.g., tiger–cat are very similar, whereas delay–racism are not). The average rating for each pair represents an estimate of the perceived similarity of the two words. The collection contains ratings for 437 unique words (353 pairs) all of which appeared in our corpus. Word association is a slightly different task: Participants are given a cue word (e.g., rice) and asked to name an associate in response (e.g., Chinese, wedding, food, white). We used the norms collected by Nelson et al. (1998). We estimated the strength of association between a cue and its associate, as the relative frequency with which it was named. The norms contain 9,968 unique words (70,739 pairs) out of which 9,862 were found in our corpus, excluding multiword expressions. For both tasks, we used correlation analysis to examine the degree of linear relationship between human ratings and model similarity values. We report correlation coefficients using Spearman’s rank correlation coefficient. Quantum Model Parameters The quantum framework presented in Section 3 is quite flexible. Depending on the choice of depe</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>Douglas L. Nelson, Cathy L. McEvoy, and Thomas A. Schreiber. 1998. The University of South Florida Word Association, Rhyme, and Word Fragment Norms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Nielsen</author>
<author>Isaac L Chuang</author>
</authors>
<title>Quantum Computation and Information Theory.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2554" citStr="Nielsen and Chuang, 2010" startWordPosition="373" endWordPosition="376"> theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantum theory, which endow it with a unique character.1 Superposition is a way of modeling uncertainty, more so than in classical probability theory. It contains information about the potentialities of a system’s state. An electron whose location in an atom is u</context>
</contexts>
<marker>Nielsen, Chuang, 2010</marker>
<rawString>Michael A. Nielsen and Isaac L. Chuang. 2010. Quantum Computation and Information Theory. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Osherson</author>
<author>Edward E Smith</author>
</authors>
<title>On the adequacy of prototype theory as a theory of concepts.</title>
<date>1981</date>
<journal>Cognition,</journal>
<pages>9--35</pages>
<contexts>
<context position="5358" citStr="Osherson and Smith, 1981" startWordPosition="816" endWordPosition="819">st in quantum theory, measurements may share (hidden) order-sensitive inter-dependencies and the outcome of the first measurement can change the outcome of the second measurement. Interference is a feature of quantum probability that can cause classical assumptions such as the law of total probability to be violated. When concepts interact their joint representation can exhibit nonclassical behavior, e.g., with regard to conjunction and disjunction (Aerts, 2009). An often cited example is the “guppy effect”. Although guppy is an example of a pet-fish it is neither a very typical pet nor fish (Osherson and Smith, 1981). In the following we use the rich mathematical framework of quantum theory to model semantic information. Specifically, we show how word meanings can be expressed as quantum states. A word brings with it its own subspace which is spanned by vectors representing its potential usages. We present a specific implementation of a semantic space that is syntactically aware, takes word order into account, and features key aspects of quantum theory. We empirically evaluate our model on word similarity and association and show that it achieves results competitive with a variety of classical models. We </context>
</contexts>
<marker>Osherson, Smith, 1981</marker>
<rawString>Daniel Osherson and Edward E. Smith. 1981. On the adequacy of prototype theory as a theory of concepts. Cognition, 9:35–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel M Pothos</author>
<author>Jerome R Busemeyer</author>
</authors>
<title>Can quantum probability provide a new direction for cognitive modeling? Behavioral and Brain Sciences.</title>
<date>2012</date>
<note>to appear.</note>
<contexts>
<context position="2613" citStr="Pothos and Busemeyer, 2012" startWordPosition="381" endWordPosition="384">ng and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to refer to the abstract mathematical foundation of quantum mechanics which is not specifically tied to physics (Hughes, 1989; Isham, 1989). Quantum theory is in principle applicable in any discipline where there is a need to formalize uncertainty. Indeed, researchers have been pursuing applications in areas as diverse as economics (Baaquie, 2004), information theory (Nielsen and Chuang, 2010), psychology (Khrennikov, 2010; Pothos and Busemeyer, 2012), and cognitive science (Busemeyer and Bruza, 2012; Aerts, 2009; Bruza et al., 2008). But what are the features of quantum theory which make it a promising framework for modeling meaning? Superposition, entanglement, incompatibility, and interference are all related aspects of quantum theory, which endow it with a unique character.1 Superposition is a way of modeling uncertainty, more so than in classical probability theory. It contains information about the potentialities of a system’s state. An electron whose location in an atom is uncertain can be modeled as being in a superposition of loca</context>
<context position="32947" citStr="Pothos and Busemeyer, 2012" startWordPosition="5394" endWordPosition="5398"> apparatus of quantum theory has been used to formulate models of cognition that are superior to those based on traditional probability theory. For example, conjunction fallacies3 (Tversky and Kahneman, 1983) have been explained by making reference to quantum theory’s context dependence of the probability assessment. Violations of the sure-thing principle4 (Tversky and Shafir, 1992) have been modeled in terms of a quantum interference effect. And the asymmetry of similarity relations has been explained by postulating that different concepts correspond to subspaces of different dimensionality (Pothos and Busemeyer, 2012). Several approaches have drawn on 3A conjunction fallacy occurs when it is assumed that specific conditions are more probable than a single general one. 4The principle is the expectation that human behavior ought to conform to the law of total probability quantum theory in order to model semantic phenomena such as concept combination (Bruza and Cole, 2005), the emergence of new concepts (Aerts and Gabora, 2005), and the human mental lexicon (Bruza et al., 2009). Chen (2002) captures syllogisms in a quantum theoretic framework; the model takes statements like All whales are mammals and all mam</context>
</contexts>
<marker>Pothos, Busemeyer, 2012</marker>
<rawString>Emmanuel M. Pothos and Jerome R. Busemeyer. 2012. Can quantum probability provide a new direction for cognitive modeling? Behavioral and Brain Sciences. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1423--1433</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="30820" citStr="Silberer and Lapata, 2012" startWordPosition="5051" endWordPosition="5054">ances of word pairs w,v and a linking word l. We obtained vectors w from the tensor following the methodology proposed in Blacoe and Lapata (2012) using 100 (l,v) contexts as dimensions. 5 Results Our results are summarized in Table 1. As can be seen, the quantum model (QM) obtains performance superior to other better-known models such as Mitchell and Lapata’s (2010) simple semantic space (SDS), Baroni and Lenci’s (2010) distributional memory tensor (DM), and Collobert and Weston’s (2008) neural language model (NLM). Our results on the association norms are comparable to the state of the art (Silberer and Lapata, 2012; Griffiths et al., 2007). With regard to WordSim353, Huang et al. (2012) report correlations in the range of 0.713–0.769, however they use Wikipedia as a training corpus and a more sophisticated version of the NLM presented here, that takes into account global context and performs word sense discrimination. In the future, we also plan to evaluate our model on larger Wikipedia-scale corpora. We would also like to model semantic composition as our approach can do this easily by taking advantage of the notion of quantum measurement. Specifically, we 2Available at http://clic.cimec.unitn.it/dm/. </context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1423–1433, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems 24,</booktitle>
<pages>801--809</pages>
<editor>In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors,</editor>
<contexts>
<context position="1564" citStr="Socher et al., 2011" startWordPosition="211" endWordPosition="214">on The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Section 6 for an overview of related research in this area). We use the term quantum theory to ref</context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
<author>Daniel Kahneman</author>
</authors>
<title>Extensional versus intuitive reasoning: The conjuctive fallacy in probability judgment.</title>
<date>1983</date>
<journal>Psychological Review,</journal>
<volume>4</volume>
<issue>90</issue>
<contexts>
<context position="32528" citStr="Tversky and Kahneman, 1983" startWordPosition="5331" endWordPosition="5334">t al.’s (1998) norms. The associates generated by the quantum model correspond to several different meanings correlated with the target. For example, prison refers to the “behind bars” sense of bar, liquor and beer refer to what is consumed or served in bars, club refers to the entertainment function of bars, whereas graph refers to how data is displayed in a chart. 6 Related Work Within cognitive science the formal apparatus of quantum theory has been used to formulate models of cognition that are superior to those based on traditional probability theory. For example, conjunction fallacies3 (Tversky and Kahneman, 1983) have been explained by making reference to quantum theory’s context dependence of the probability assessment. Violations of the sure-thing principle4 (Tversky and Shafir, 1992) have been modeled in terms of a quantum interference effect. And the asymmetry of similarity relations has been explained by postulating that different concepts correspond to subspaces of different dimensionality (Pothos and Busemeyer, 2012). Several approaches have drawn on 3A conjunction fallacy occurs when it is assumed that specific conditions are more probable than a single general one. 4The principle is the expec</context>
</contexts>
<marker>Tversky, Kahneman, 1983</marker>
<rawString>Amos Tversky and Daniel Kahneman. 1983. Extensional versus intuitive reasoning: The conjuctive fallacy in probability judgment. Psychological Review, 4(90):293–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amos Tversky</author>
<author>Eldar Shafir</author>
</authors>
<title>The disjunction effect in choice under uncertainty.</title>
<date>1992</date>
<journal>Psychological Science,</journal>
<volume>3</volume>
<issue>5</issue>
<contexts>
<context position="32705" citStr="Tversky and Shafir, 1992" startWordPosition="5357" endWordPosition="5361">bars” sense of bar, liquor and beer refer to what is consumed or served in bars, club refers to the entertainment function of bars, whereas graph refers to how data is displayed in a chart. 6 Related Work Within cognitive science the formal apparatus of quantum theory has been used to formulate models of cognition that are superior to those based on traditional probability theory. For example, conjunction fallacies3 (Tversky and Kahneman, 1983) have been explained by making reference to quantum theory’s context dependence of the probability assessment. Violations of the sure-thing principle4 (Tversky and Shafir, 1992) have been modeled in terms of a quantum interference effect. And the asymmetry of similarity relations has been explained by postulating that different concepts correspond to subspaces of different dimensionality (Pothos and Busemeyer, 2012). Several approaches have drawn on 3A conjunction fallacy occurs when it is assumed that specific conditions are more probable than a single general one. 4The principle is the expectation that human behavior ought to conform to the law of total probability quantum theory in order to model semantic phenomena such as concept combination (Bruza and Cole, 2005</context>
</contexts>
<marker>Tversky, Shafir, 1992</marker>
<rawString>Amos Tversky and Eldar Shafir. 1992. The disjunction effect in choice under uncertainty. Psychological Science, 3(5):305–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlatko Vedral</author>
</authors>
<title>Introduction to Quantum Information Science.</title>
<date>2006</date>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="3884" citStr="Vedral (2006)" startWordPosition="590" endWordPosition="591">tiple meanings. In isolation, the word pen may refer to a writing implement, an enclosure for confining livestock, a playpen, a penitentiary or a female swan. However, when observed in the context of the word ink the ambiguity resolves into the sense of the word dealing with writing. The meanings of words in a semantic space are superposed in a way which is intuitively similar to the atom’s electron. Entanglement concerns the relationship between 1It is outside the scope of the current paper to give a detailed introduction on the history of quantum mechanics. We refer the interested reader to Vedral (2006) and Kleppner and Jackiw (2000) for comprehensive overviews. 847 Proceedings of NAACL-HLT 2013, pages 847–857, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics systems for which it is impossible to specify a joint probability distribution from the probability distributions of their constituent parts. With regard to word meanings, entanglement encodes (hidden) relationships between concepts. The different senses of a word “exist in parallel” until it is observed in some context. This reduction of ambiguity has effects on other concepts connected via entanglemen</context>
</contexts>
<marker>Vedral, 2006</marker>
<rawString>Vlatko Vedral. 2006. Introduction to Quantum Information Science. Oxford University Press, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>