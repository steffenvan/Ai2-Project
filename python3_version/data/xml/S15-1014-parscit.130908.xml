<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9978415">
Towards Semantic Language Classification: Inducing and Clustering
Semantic Association Networks from Europarl
</title>
<author confidence="0.996677">
Steffen Eger&apos;, Niko Schenk&apos; and Alexander Mehler&apos;
</author>
<affiliation confidence="0.928756333333333">
&apos;Text Technology Lab
&apos;Applied Computational Linguistics Lab
Goethe University Frankfurt am Main
</affiliation>
<email confidence="0.995041">
{steeger,nschenk,amehler}@em.uni-frankfurt.de
</email>
<sectionHeader confidence="0.995561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999220625">
We induce semantic association networks
from translation relations in parallel corpora.
The resulting semantic spaces are encoded in
a single reference language, which ensures
cross-language comparability. As our main
contribution, we cluster the obtained (cross-
lingually comparable) lexical semantic spaces.
We find that, in our sample of languages,
lexical semantic spaces largely coincide with
genealogical relations. To our knowledge,
this constitutes the first large-scale quantita-
tive lexical semantic typology that is com-
pletely unsupervised, bottom-up, and data-
driven. Our results may be important for the
decision which multilingual resources to inte-
grate in a semantic evaluation task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998482">
There has been a recent surge of interest in integrat-
ing multilingual resources in natural language pro-
cessing (NLP). For example, Snyder et al. (2008)
show that jointly considering morphological seg-
mentations across languages improves performance
compared to the monolingual baseline. Bhargava
and Kondrak (2011) and Bhargava and Kondrak
(2012) demonstrate that string transduction can ben-
efit from supplemental information provided in other
languages. Analogously, in lexical semantics, Nav-
igli and Ponzetto (2012) explore semantic relations
from Wikipedia in different languages to induce a
huge integrated lexical semantic network.
In this paper, we also focus on multilingual re-
sources in lexical semantics. But rather than inte-
grating them, we investigate their (dis-)similarities.
More precisely, we cluster (classify) languages
based on their semantic relations between lexical
units. The outcome of our classification may have
direct consequences for approaches that integrate di-
verse multilingual resources. For example, from a
linguistic point of view, it might be argued that in-
tegrating very heterogeneous/dissimilar semantic re-
sources is harmful, e.g., in a monolingual semantic
similarity task, because semantically unrelated lan-
guages might contribute semantic relations unavail-
able in the language for which semantic similarity is
computed. Alternatively, from a statistical point of
view, it might be argued that integrating heteroge-
neous/dissimilar resources is beneficial due to their
higher degree of uncorrelatedness. In any case, ei-
ther of these implications necessitates knowledge of
a typology of lexical semantics.
In order to address this question, we provide a
translation-based model of lexical semantic spaces.
Our approach is to generate association networks in
which the weight of a link between two words de-
pends on their degree of partial synonymy. To mea-
sure synonymy, we rely on translation data that is
input to a statistical alignment toolkit. We define the
degree of synonymy of two words to be proportional
to the number of common translations in a reference
language, weighted by the probability of translation.
By pivoting on the reference language, we represent
semantic associations among words in different lan-
guages by means of the synonymy relations of their
translations in the same target language. This ap-
proach ensures cross-language comparability of se-
mantic spaces: Greek and Bulgarian are compared,
for example, by means of the synonymy relations
</bodyText>
<page confidence="0.961954">
127
</page>
<note confidence="0.953666">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 127–136,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.999043666666667">
that are retained when translating them into the same
pivot language (e.g., English).
This approach does not only address proximities
of pairs of words shared among languages (e.g.,
MEAT and BEEF, MOUTH and DOOR, CHILD and
FRUIT – cf. Vanhove et al. (2008)). By averaging
over word pairs, it also allows for calculating seman-
tic distances between pairs of languages.
The Sapir-Whorf Hypothesis (SWH) (Whorf,
1956) already predicts that semantic relations are
not universal. Though we are agnostic about
the assumptions underlying the SWH, it neverthe-
less gives an evaluation criterion for our experi-
ment: if the SWH is true, we expect a clustering
of translation-based semantic spaces along the ge-
nealogical relationships of the languages involved.
However, genealogy is certainly not the sole prin-
ciple potentially underlying a typology of lexical
semantics. For example, Cooper (2008) finds that
French is semantically closer to Basque, a putatively
non-Indoeuropean language, than to German. To
the best of our knowledge, a large-scale quantitative
typological analysis of lexical semantics is lacking
thus far and we intend to make first steps towards
this target.
The paper is structured as follows. Section 2 out-
lines related work. Section 3 presents our formal
model and Section 4 details our experiments on clus-
tering semantic spaces across selected languages of
the European Union. We conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99994180952381">
A field related to our research is semantic related-
ness, in which the task is to determine the degree
of semantic similarity between pairs of words, such
as tiger and cat, sex and love, etc. Classically, se-
mantic word networks such as WordNet (Fellbaum,
1998) or EuroWordNet (Vossen, 1998) have been
used to address this problem (Jiang and Conrath,
1997), and, more recently, taxonomies and knowl-
edge bases such as Wikipedia (Strube and Ponzetto,
2006). Hassan and Mihalcea (2009) define the
task of cross-lingual semantic relatedness, in which
the goal is to determine the semantic similarity be-
tween words from different languages, and Navigli
and Ponzetto (2012) have combined WordNet with
Wikipedia to construct a multi-layer semantic net-
work in which computation of cross-lingual seman-
tic relatedness may be performed. Most recently,
neural network-based distributed semantic represen-
tations focusing on cross-language similarities be-
tween words and larger textual units have become
popular (Chandar A P et al. (2014), Hermann and
Blunsom (2014), Mikolov et al. (2013)).
There have been (a) few different computa-
tional approaches to semantic language classifica-
tion. Mehler et al. (2011) test whether languages
are genealogically separable via topological prop-
erties of semantic (concept) graphs derived from
Wikipedia. This approach is top-down in that it as-
sumes that the genealogical tree is the desired out-
put of the classification. Cooper (2008) computes
semantic distances between languages based on the
curvature of translation histograms in bilingual dic-
tionaries. While this results in some interesting find-
ings as indicated, the approach is not applied to lan-
guage classification, but focuses on computing se-
mantically similar languages for a given query lan-
guage. Vanhove et al. (2008) construct so-called
semantic proximity networks based on monolingual
dictionaries, and envision to use them for semantic
typologies. They do not apply their methodology to
the multilingual setup, however, which a typology
necessitates.
Orthographic, phonetic and syntactic similar-
ity of languages have received considerably more
attention than semantic similarity, as we focus
on. Classical approaches in determining ortho-
graphic/phonetic relatedness of languages are based
on lexico-statistical comparisons of items in stan-
dardized word lists (Campbell, 2003; Rama and
Borin, 2015), such as the Swadesh lists (Swadesh,
1955). Rama and Borin (2015) study the impact of
different string similarity measures on orthographic
language classification. Ciobanu and Dinu (2014)
measure orthographic similarity between Romanian
and related languages. They also indicate applica-
tions of (knowledge of) similarity values between
languages, such as serving as a guide for machine
translation (Scannell, 2006). Koehn (2005) pro-
duces a genealogical clustering of the languages in
Europarl based on ease of translation, as measured in
BLEU scores, between any two languages (which,
putatively, yields a syntactic similarity indication).
This results in an imperfect reproduction of the ge-
</bodyText>
<page confidence="0.997249">
128
</page>
<figureCaption confidence="0.997631">
Figure 1: Excerpts of bilingual dictionaries as bi-
</figureCaption>
<bodyText confidence="0.6199525">
partite graphs with links between words if and only
if one is a translation of the other. Data from
www.latin-dictionary.net and dict.leo.org.
nealogical language tree for the languages involved.
</bodyText>
<sectionHeader confidence="0.99753" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.99372794">
We start with motivating our approach by example
of bilingual dictionaries before we formally gener-
alize it in terms of probabilistic translation relations.
Bilingual dictionaries, or the bipartite graphs that
represent them (cf. Figure 1), induce lexical seman-
tic association networks in any of the languages in-
volved by placing a link between two words of the
same language if and only if they share a common
translation in the other language (cf. Figure 2).
Since translations provide partially synonymous
expression in the target language, the latter links can
be seen to denote semantic relatedness (in terms of
synonymy) of the interlinked words. Further, the
more distant two words in such a lexical semantic
association network, the lower the degree of their
partial synonymy: the longer the path from one word
to another, the higher the loss of relatedness among
them (cf. Eger and Sejane (2010)).
Note that association networks derived from bilin-
gual dictionaries represent semantic similarities of
words of the source language R subject to semantic
relations of their translations in the target language
L. The reason is that whether or not a link is es-
tablished between two words α and Q in R depends
on associations of their translations present in L. To
illustrate this, consider the association networks out-
lined in Figure 2, induced from the bilingual dictio-
naries outlined in Figure 1, which match between
R = English and L = Latin and L = German, re-
spectively. When L is classical Latin, the semantic
field centered around (the English word) MAN is par-
tially different from the semantic field around MAN
when L is German. For example, under L = Latin,
MAN is directly linked with HERO and WARRIOR (in-
directly with DEMIGOD) – these semantic associa-
tions are not present when German is the language
L.
By fixing R and varying L, we can create different
lexical semantic association networks, each encoded
in language R, and each representing the semantic
relations of L.1 Analyzing and contrasting such net-
works may then allow for clustering languages due
to shared lexical semantic associations.
As mentioned above, we generalize the model
outlined so far to the situation of probabilistic trans-
lation relationships derived from corpus data, rather
than from bilingual dictionaries. Working on corpus
data has both advantages and disadvantages com-
pared to using human compiled and edited dictio-
naries. On the one hand,
</bodyText>
<listItem confidence="0.965277681818182">
• the translation relations induced from corpus
data are noisy since their estimation is par-
tially inaccurate due to limitations of alignment
toolkits such as GIZA++ (Och and Ney, 2003)
as employed by us. Implications of this inaccu-
racy are outlined below.
• By using unannotated corpora, we cannot
straightforwardly distinguish between cases
of polysemy and homonymy. The problem
is that homonymy should (ideally) not con-
tribute to generating lexical semantic associa-
tion networks as considered here. However,
homonymy is apparently a rather rare phe-
nomenon, while polysemy, which we expect to
underlie the structure of our networks, is abun-
dant (cf. L¨obner (2002)).
On the other hand,
• classical dictionaries can be very heteroge-
neous in their scope and denomination of trans-
lation links between words (see, e.g., Cooper
(2008)), making the respective editors of the
bilingual dictionaries distorting variables.
</listItem>
<footnote confidence="0.539893666666667">
1Each network represents the semantic relations of both lan-
guages R and L, but since we keep R fixed and vary L, each
association network inherits the same properties from R.
</footnote>
<figure confidence="0.998630764705882">
MAN
vir
PERSON MAN mann
HUMAN PERSON
homo
DEMIGOD
(a) English-Latin
MALE
(b) English-German
HUSBAND
HERO
heros
WARRIOR
HUMAN
HUSBAND
mensch
GUY
</figure>
<page confidence="0.956022">
129
</page>
<listItem confidence="0.8904452">
• Corpus data allows for inducing probabilities of
translation relations of words, which indicate
weighted links more accurately than ranked as-
signments provided by classical dictionaries.
• Corpus data allows for dealing with real lan-
</listItem>
<bodyText confidence="0.967571318181818">
guage use by means of comparable excerpts of
natural language data.
Network generation Assume that we are given
different natural languages L1, ... , LM, R and
bilingual translation relations that map from lan-
guage Lk to language R, for all 1 &lt; k &lt; M. We
call the language R reference language.2 In our
work, we assume that the translation relations are
probabilistic. That is, we assume that there exist
probabilistic ‘operators’ Pk that indicate the prob-
abilities – denoted by Pk[α|z] – by which a word
z of language Lk translates into a word α of lan-
guage R. Our motivation is to induce M differ-
ent lexical semantic networks that represent the lex-
ical semantic spaces of the languages L1, ... , LM,
each encoded in language R, which finally allows
for comparing the semantic spaces of the M differ-
ent source languages. To this end, we define the
weighted graphs Gk = (Vk, Wk), where the nodes
Vk of Gk are given by the vocabulary Rvoc of lan-
guage R, i.e. Vk = Rvoc. We define the weight of an
edge (α, Q) E (Rvoc)2 as
</bodyText>
<equation confidence="0.851607">
Pk[α|z]Pk[Q|z]p[z], (1)
</equation>
<bodyText confidence="0.948055857142857">
where p[z] denotes the (corpus) probability of word
z E Lvoc
k . Since each Gk is spanned using the same
subset of the vocabulary of the reference language
R, we call it the Lk(-based) network version of R.
Eq. (1) can be motivated by postulating that Wk
is a joint probability. In this case we can write
</bodyText>
<equation confidence="0.437163875">
� �Wk(α, Q, z) = Wk(α, Q|z)Wk(z)
Wk(α, Q) = z∈L-
z∈Lvoc k
k
E≈ Wk(α|z)Wk(Q|z)Wk(z),
z∈Lvoc
k
(2)
</equation>
<bodyText confidence="0.976267473684211">
where the first equality is marginalization (‘sum-
ming out over the possible states of the world’),
and the third step is an approximation which would
2Alternative names for the concept we have in mind might,
e.g., be pivot language, tertium comparationis or interlingua.
be accurate if α and Q were conditionally indepen-
dent given z. By inserting the conditional probabil-
ities Pk[α|z], Pk[Q|z] (whose existence we assumed
above) and the corpus probability p[z] into Eq. (2),
we obtain Eq. (1). Note that in the special case of
a bilingual dictionary of Lk and R, where Pk[α|z]
can be defined as 1 or 0 depending on whether α is
a translation of z or not,3 Wk(α, Q) is proportional
to the number of words z (in language Lk) whose
translation is both α and Q; i.e., assuming that p[z]
is a constant in this setup, Eq. (1) simplifies to:
Wk(α, Q) a � 1.
zELvoc k:z translates into α and β
Clearly, the more common translations two words
have in the target language, the closer their seman-
tic similarity should be, all else being equal.4 Eq.
(1) generalizes this interpretation by non-uniformly
‘prioritizing’ the translations of z.
Network analysis In order to compare the net-
work versions G1, ... , GM of language R that
are output by network generation, we first de-
fine the vector representation of node vk in graph
Gk = (Vk, Wk) as the probability vector of end-
ing up in any of the nodes of Gk when a ran-
dom surfer starts from vk and surfs on the graph
Gk according to the normalized weight matrix
Wk = [Wk(α, Q)](α,β)EVk×Vk. Note that the higher
Wk(α, Q), the higher the likelihood that the surfer
takes the transition from α to Q. More precisely, we
let the meaning [vk] of node vk in graph Gk be the
vector vk that results as the limit of the iterative pro-
cess (see, e.g., Brin and Page (1998), Gaume and
Mathieu (2008), Kok and Brockett (2010)),
</bodyText>
<equation confidence="0.813889">
vkN+1 = dvk NA(k) + (1 − d)vk0,
</equation>
<bodyText confidence="0.9999524">
where each vkN, for N ≥ 0, is a 1 x |Rvoc |vector,
A(k) is obtained from Wk by normalizing all rows
such that A(k) is row-stochastic, and d is a damping
factor that describes preference for the starting vec-
tor vk0, which is a vector of zeros except for index
</bodyText>
<footnote confidence="0.771537">
3More correctly, one could define Pk[α|z] = fz1 , whenever
α is a translation of z, and Pk[α|z] = 0, otherwise, where fz
is the number of translations of word z. This would lead to an
analogous interpretation as the given one.
4This reasoning ignores cases of homonymy, which weaken
the semantic argument. See our discussion above.
</footnote>
<figure confidence="0.929993055555556">
�
Wk(α, Q) =
zELvoc
k
130
DEMIGOD
HERO
MAN
WARRIOR
HUSBAND
HUMAN
PERSON
GUY
MAN
MALE
HUSBAND
HUMAN
PERSON
</figure>
<figureCaption confidence="0.8623745">
Figure 2: Lexical semantic association networks derived from bilingual dictionaries, given in Figure 1, by
linking two English words if and only if they have a common translation in Latin (left) or German (right).
</figureCaption>
<bodyText confidence="0.959145875">
The node for MAN is highlighted in both networks.
position of word vk, where vk0 has value 1.5 Subse-
quently, we can contrast words v and w (or, rather,
their meanings) in the same network version of ref-
erence language R, by considering, for instance, the
cosine similarity or vector distance of their associ-
ated vectors. More generally, we can contrast the
lexical semantic meanings vk and wj of any two
language R words v and w, across two languages
Lk and Lj, by, e.g., evaluating,
vk· wj (scalar product, cosine similarity)
or
||vk − wj ||(vector distance).
Finally, the lexical semantic distance or similarity
between two languages Lk and Lj can be deter-
mined by simple averaging,
</bodyText>
<equation confidence="0.979099333333333">
1 � 5(vk, vj), (3)
D(Lk,Lj) = |Rvoc|
v∈Rvoc
</equation>
<bodyText confidence="0.999735727272727">
where 5 is a distance or similarity function.
Discussion We mentioned above that toolkits
like GIZA++ cannot perfectly estimate transla-
tion relationships between words in different lan-
guages. Thus, we have to face situations of ‘noisily’
weighted links between words in the same network
version of reference language R. Typically, a higher
chance of mismatch occurs in the case of bigrams.
To illustrate, consider the French phrase ˆetres chers
(‘beings loved’/‘loved ones’). Here, GIZA++ typi-
cally assigns positive weight mass to Pfr[LOVE|ˆetre]
</bodyText>
<footnote confidence="0.504549">
5We always set d to 0.8 in our experiments.
</footnote>
<bodyText confidence="0.999813444444445">
although, from a point of view of a classical dic-
tionary, translating ˆetre into love is clearly problem-
atic. Since it is likely that, e.g., Pfr[HUMAN|ˆetre] and
Pfr[BEING|ˆetre] will also be positive, we can expect
weighted links in the French network version of En-
glish between HUMAN and LOVE as well as between
BEING and LOVE. Thus, besides ‘true’ semantic re-
lations, our approach also captures, though uninten-
tionally, co-occurrence relations.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999953388888889">
We evaluate our method by means of the Europarl
corpus (Koehn, 2005). Europarl documents the pro-
ceedings of the European parliament in the 21 offi-
cial languages of the European Union. This provides
us with sentence-aligned multi-texts in which each
tuple of sentences expresses the same underlying
meaning.6 Using GIZA++, this allows us to estimate
the conditional translation probabilities P[A|B] for
any two words A, B from any two languages in the
Europarl corpus. In our experiment, we focus on the
approx. 400,000 sentences for which translations in
all 21 languages are available. To process this data,
we set all words of all sentences to lower-case. Ide-
ally, we would have lemmatized all texts, but did not
do so because of the unavailability of lemmatizers
for some of the languages. Therefore, we decided to
lemmatize only words in the reference language and
kept full-forms for all source languages.7 We choose
</bodyText>
<footnote confidence="0.9962094">
6In a tuple of sentences, one sentence is the source of which
all the other sentences are translations.
7Lemmatization tools and models are taken from
the TreeTagger (Schmid, 1994) home page www.cis.
uni-muenchen.de/˜schmid/tools/TreeTagger
</footnote>
<page confidence="0.995738">
131
</page>
<bodyText confidence="0.999861780487805">
English as the reference language.8 In all languages,
we omitted all words whose corpus frequency is less
than 50 and excluded the 100 most frequent (mostly
function) words.9 In the reference language, we also
ignored all words whose characters do not belong to
the standard English character set.
Figure 3 shows subgraphs centered around the
seed word WOMAN in five network versions of En-
glish. All subgraphs are constructed using the Eu-
roparl data. Apparently, the network versions of En-
glish diverge from each other. For instance, the se-
mantic association between WOMAN and WIFE ap-
pears to be strongest in the French and in the Spanish
version of English, while in the Finnish version there
does not even exist a link between these nodes. In
contrast, the weight of the link between WOMAN and
LESBIAN is highest in the Czech version of English,
while that between WOMAN and GIRL is strongest
in the Finnish version. All in all, the wiring and the
thickness of links clearly differ across language net-
works, indicating that the languages differ in terms
of semantic relations of their translations.
Table 1 shows network statistics of the graphs Gk.
All network versions of English consist of exactly
5,021 English (lemmatized) words. The networks
show a high cluster value, indicating that neighbors
of a word are probably interlinked (i.e., semantically
related) (cf. Watts and Strogatz (1998)). Average
path lengths and diameters are low, that is, distances
between words are short, as is typically observed
for semantic networks (cf. Steyvers and Tenenbaum
(2005)). The density of the networks (measured by
the ratio of existing links and the upper bound of the-
oretically possible links) varies substantially for the
language networks. For instance, in the Hungarian
network version of English, only 2.56% of the pos-
sible links are realized, while in the Dutch version,
8.45% are present. This observation may hint at the
‘degree of analyticity’ of a language: the more word
forms per lemma there are in a language, the less
likely they are linked by means of Eq. (1).
</bodyText>
<footnote confidence="0.993775142857143">
8Due to the limited availability of lemmatizers, not all lan-
guages could have served as a reference language. Although we
posit that the choice of reference language has no (or minimal)
impact upon the resulting language classification as outlined be-
low, this would need to be experimentally verified in follow-up
work.
9The threshold of 50 serves to reduce computational effort.
</footnote>
<table confidence="0.9944081">
# nodes CV GD D density (%)
cs 5,021 0.39 1.96 4 4.51
da 5,021 0.43 1.95 5 5.35
nl 5,021 0.50 1.85 4 8.45 (9.22)
et 5,021 0.37 1.98 5 3.81 (4.57)
fi 5,021 0.35 1.99 4 3.28 (6.63)
fr 5,021 0.44 1.91 4 6.37 (8.23)
de 5,021 0.43 1.96 5 5.03 (5.81)
el 5,021 0.36 2.00 5 3.79
hu 5,021 0.33 2.07 5 2.56
it 5,021 0.45 1.87 4 7.41 (9.53)
lv 5,021 0.41 1.94 4 5.29
lt 5,021 0.41 1.94 4 5.08
pl 5,021 0.39 1.94 4 4.84 (6.56)
pt 5,021 0.40 1.97 4 4.74
ro 5,021 0.39 2.00 5 4.22
sk 5,021 0.36 1.99 5 3.73 (5.23)
sl 5,021 0.38 1.97 4 4.13
es 5,021 0.40 1.98 5 4.67 (5.80)
sv 5,021 0.43 1.94 5 5.69
</table>
<tableCaption confidence="0.9528618">
Table 1: Number of nodes, cluster value (CV), geodesic
distance (GD), diameter (D) and density of different net-
work versions of English. Links are binarized depending
on whether their weights are positive or not. In brackets:
values of lemmatized versions of Lk.
</tableCaption>
<bodyText confidence="0.999962666666667">
Note that since the density of a network may have
substantial impact on random surfer processes as ap-
plied by us, and since analyticity is a morphologi-
cal rather than a semantic phenomenon, it may be
possible that the classification results reported below
are in fact due to syntagmatic relations – in contrast
to our hypothesis about their semantic, paradigmatic
nature. We address this issue below.
Semantic similarity Before proceeding to our
main task, the clustering of semantic spaces, we
measure how strongly our semantic association net-
works capture semantics. To this end, we com-
pute the correlation coefficient between the se-
mantic similarity scores of the word pairs in the
WordSimilarity-353 (Finkelstein et al., 2001) En-
glish word relatedness dataset and the similarity
scores, for the same word pairs, obtained by our
method. The WordSimilarity-353 dataset consists
of 353 word pairs annotated by the average of 13
human experts, each on a scale from 0 (unrelated) to
10 (very closely related or identical). We evaluated
only on those word pairs for which each word in the
pair is contained in our set of 5,021 English words,
which amounted to 172 word pairs. To be more
</bodyText>
<page confidence="0.975567">
132
</page>
<figure confidence="0.99986032">
mother
woman
spouse
lesbian
gender
female
girl
wife
pregnan
man
mother
woman
spouse
lesbian
gender
female
girl
wife
pregnant
man
mother
woman
spouse
lesbian
gender
female
girl
wife
pregnan
man
mother
woman
spouse
lesbian
gender
female
girl
wife
pregnant
man
mother
woman
spouse
lesbian
gender
female
wife
girl
pregnan
man
</figure>
<figureCaption confidence="0.996889">
Figure 3: From left to right: Czech, Finnish, French, German, and Spanish networks. Thickness of edges indicates
weights of links. Links with weights below a fixed threshold are ignored for better graphical presentation.
</figureCaption>
<bodyText confidence="0.934195955223881">
precise on the computation of semantic relatedness,
for each word pair (u, v) in the WordSimilarity-353
dataset, we computed the semantic similarity of the
word pair in the language Lk version of English by
considering the cosine similarity of uk and vk, that
is, by means of the semantic meanings of u and v
generated by the random surfer process on network
Gk. Doing so for each language Lk gives 20 dif-
ferent correlation coefficients, one for each network
version of English, shown in Table 2.
it 0.34678 ... ...
pt 0.32249 sl 0.25720
es 0.31990 bg 0.25372
ro 0.31204 hu 0.24910
nl 0.30885 et 0.24212
da 0.30715 lt 0.24207
Table 2: Sample Pearson correlation coefficients be-
tween human gold standard and our approach for
different network versions of English.
We first note that the correlation coefficients dif-
fer between network versions of English, where the
Italian version exhibits the highest correlation with
the (English) human reference, and the Lithuanian
version the lowest. Note that Hassan and Mihal-
cea (2009) obtain a correlation coefficient of 0.55 on
the whole WordSimilarity-353 dataset, which is con-
siderably higher than our best score of 0.34. How-
ever, first note that our networks, which consist of
5,021 lexical units, are quite small compared to the
data sizes that other studies rely on, which makes
a comparison highly unfair. Secondly, one has to
see that we compute the semantic relatedness of En-
glish words from the semantic point of view of two
languages: the reference language and the respec-
tive source language (e.g., the Italian version of En-
glish), which, by our very postulate, differs from the
semantics of the reference language. According to
Table 2, the semantics of English is apparently better
represented by the semantics of Italian, Portuguese,
Spanish, Romanian, and Dutch, than, e.g., by the
one of Bulgarian, Hungarian, Estonian, and Lithua-
nian – at least subject to the translations provided by
the Europarl corpus.10
Clustering of semantic spaces Finally, we clus-
ter semantic spaces by comparing the network ver-
sions of the English reference language. To deter-
mine the semantic distance between two languages
Lk and Lj, we plug in each pair of languages in Eq.
(3) – with S(vk, vj) as vector distance – thus ob-
taining a symmetric 20 × 20 distance matrix. Fig-
ures 4 and 5 show the results when feeding this
distance matrix as input to k-means clustering (a
centroid based clustering approach) and to hierar-
chical clustering using default parameters. As can
be seen, both clustering methods arrange the lan-
guages on the basis of their semantic spaces along
genealogical relationships. For instance, both clus-
tering algorithms group Danish, Swedish, Dutch and
German (Germanic), Portuguese, Spanish, French,
Italian, Romanian (Romance), Bulgarian, Czech,
Polish, Slovak, Slovene (Slavic), Finnish, Hungar-
ian, Estonian (Finno-Ugric), and Latvian, Lithua-
nian (Baltic). Greek, which is genealogically iso-
lated in our selection of languages, is in our classi-
fication associated with the Romance languages, but
constitutes an outlier in this group. All in all, the
clustering appears highly non-random and almost a
</bodyText>
<footnote confidence="0.99007725">
10Table 2 also suggests that the Romance languages are se-
mantically closer to English in our data than, e.g., the Germanic,
which may be considered a deviation from, e.g., genealogical
language similarity.
</footnote>
<page confidence="0.997003">
133
</page>
<figure confidence="0.8574">
Component 1
</figure>
<figureCaption confidence="0.9993578">
Figure 4: k-means cluster analysis of the 20 Eu-
roparl languages. Optimal number of clusters k = 5
determined by sum of squared error analysis.
Figure 5: Dendrogram of hierarchical clustering of
the 20 non-lemmatized Europarl languages.
</figureCaption>
<bodyText confidence="0.96417346">
perfect match of what is genealogically expected.
To address the question of whether morphological
principles are the driving force behind the clustering
of the semantic spaces generated here, we lemma-
tized the reference language English and all source
languages Lk for which lemmatizers were freely
available in order to conduct the same classification
procedure. This included 10 languages: Bulgarian,
Dutch, Estonian, Finnish, French, German, Italian,
Polish, Slovak, and Spanish. This procedure leads to
an assimilation of density values in the graphs Gk as
shown in Table 1: for the 10 languages, the relative
standard deviation in network density decreases by
about 23%. However, the optimal groupings of the
languages do not change in that k-means clustering
determines the five groups Spanish, French, Italian;
Bulgarian, Slovak, Polish; German, Dutch; Finnish;
Estonian, irrespective of whether the named ten lan-
guages are lemmatized or not.11
Integrated networks Lastly, we address the
derivative question raised in the introduction, viz.,
11The clustering based on 10 languages slightly differs in that
Finnish and Estonian are assigned to distinct clusters.
whether the integration of heterogeneous/dissimilar
multilingual resources may be harmful or beneficial.
To this end, we consider integrated networks G(S) in
which the weight of a link (α, Q) ∈ E(S) is given as
the average (arithmetic mean) link weight of all link
weights in the networks for a selection of languages
S. Using our optimal number of k = 5 clusters (and
the clusters themselves) derived above, we thus let S
range over the union of all the languages in the 2k−1
possible subsets of clusters.12 For each so resulting
network G(S), we determine semantic similarity be-
tween any pair of words exactly as above and then
compute correlation with the WordSimilarity-353
dataset. Results are given in Table 3. The numbers
appear to support the hypothesis that, in the given
monolingual semantic similarity task for English,
integrating semantically similar languages (and, pu-
tatively, languages whose semantic similarity to En-
glish itself is closer) leads to better results than in-
tegrating heterogeneous languages. For example,
the average network consisting of the Romance lan-
guages has a roughly 2% higher correlation than
the network consisting of all languages. Interest-
ingly, however, the very best combination result is
achieved when we integrate the Romance, Germanic
and the three non-Indoeuropean languages Finnish,
Hungarian and Estonian.
</bodyText>
<table confidence="0.998858166666667">
R+G+F 0.34402 ... ...
R+G 0.34376 S+B 0.27496
R+F 0.33743 S 0.27462
R 0.33719 B+F 0.27424
... ... F 0.26074
R+G+F+B+S 0.31670 B 0.25904
</table>
<tableCaption confidence="0.992031">
Table 3: Sample Pearson correlation coefficients be-
</tableCaption>
<bodyText confidence="0.697895625">
tween human gold standard and our approach for
different integrated network versions. Language
cluster abbreviations: Romance (it, fr, pt, es, ro, el),
Germanic (sv, nl, de, da), Slavic (bg, cz, pl, sk, sl),
Baltic (lv, lt), Finno-Ugric (fi, hu, et).
12Ideally, we would have let S range over all possible 2&amp;quot; − 1
nonempty subsets of n languages, but this would have required
220 − 1 &gt; 1 million comparisons.
</bodyText>
<figure confidence="0.997873720930233">
−3 −2 −1 0 1 2 3
pt
it
ro
esfr
I
nl
et
hu
de
Component 2
−2 0 1 2
sv
da
el
cs
pl
lv
lt
sk
sl
bg
0.10 0.16 0.22
hu
et
fi
pt
es
fr
it
sl
bg
pl
da
sv
nl
de
lv
lt
cs
sk
el
ro
</figure>
<page confidence="0.992042">
134
</page>
<sectionHeader confidence="0.997819" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999962333333333">
We have encoded lexical semantic spaces of differ-
ent languages by means of the same pivot language
in order to make the languages comparable. To this
end, we introduced association networks in which
links between words in the reference language de-
pend on translations from the respective source lan-
guage, weighted by probability of translation. Our
methodology is closely related to analogous ap-
proaches in the paraphrasing community which in-
terlink paraphrases by means of their translations in
other languages (e.g., Bannard and Callison-Burch
(2005), Kok and Brockett (2010)), but our appli-
cation scenario is different and we also describe a
principled manner to generate weighted links be-
tween lexical units from multilingual data. Using
random walks to represent similarities among words
in the association networks, we finally derived sim-
ilarity values for pairs of languages. This allowed
us to perform several cluster analyses to group the
20 source languages. Interestingly, in our data sam-
ple, semantic language classification appears to be
almost perfectly correlated with genealogical rela-
tionships between languages. To the best of our
knowledge, our translation-based lexical semantic
classification is the first large-scale quantitative ap-
proach to establishing a lexical semantic typology
that is completely unsupervised, ‘bottom-up’, and
data-driven.13
In future work, we intend to delineate specific lex-
ical semantic fields in which particular languages
differ, which can easily be accomplished within our
approach. Also, it must be investigated whether our
association networks can capture semantic similar-
ity in a competitive manner once they are scaled up
appropriately. Finally, applying our methodology to
a much larger set of languages is highly desirable.
</bodyText>
<sectionHeader confidence="0.998493" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99036837704918">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ’05, pages 597–604,
13But see also the first author’s preliminary investigations
on semantic language classification in Sejane and Eger (2013),
based on freely available (low-quality) bilingual dictionaries,
and Eger (2012).
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Aditya Bhargava and Grzegorz Kondrak. 2011. How
Do You Pronounce Your Name?: Improving G2P with
Transliterations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, HLT
’11, pages 399–408, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Aditya Bhargava and Grzegorz Kondrak. 2012. Lever-
aging Supplemental Representations for Sequential
Transduction. In HLT-NAACL, pages 396–406. The
Association for Computational Linguistics.
Sergey Brin and Lawrence Page. 1998. The Anatomy of
a Large-scale Hypertextual Web Search Engine. Com-
put. Netw. ISDNSyst., 30(1-7):107–117, April.
Lyle Campbell. 2003. How to show Languages are re-
lated: Methods for Distant Genetic Relationship. In
The Handbook of Historical Linguistics. Blackwell.
Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An Autoencoder
Approach to Learning Bilingual Word Representa-
tions. In Z. Ghahramani, M. Welling, C. Cortes, N.D.
Lawrence, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
1853–1861. Curran Associates, Inc.
Alina Maria Ciobanu and Liviu P. Dinu. 2014. An Ety-
mological Approach to Cross-Language Orthographic
Similarity. Application on Romanian. In Proceedings
of the 2014 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2014, October 25-
29, 2014, Doha, Qatar, A meeting of SIGDAT, a Spe-
cial Interest Group of the ACL, pages 1047–1058.
Martin C. Cooper. 2008. Measuring the Semantic Dis-
tance between Languages from a Statistical Analysis
of Bilingual Dictionaries. Journal of Quantitative Lin-
guistics, 15(1):1–33.
Steffen Eger and Ineta Sejane. 2010. Computing Seman-
tic Similarity from Bilingual Dictionaries. In Proceed-
ings of the 10th International Conference on the Sta-
tistical Analysis of Textual Data (JADT-2010), pages
1217–1225, Rome, Italy, June. JADT-2010.
Steffen Eger. 2012. Lexical Semantic Typologies from
Bilingual Corpora — A Framework. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 90–94. As-
sociation for Computational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
</reference>
<page confidence="0.985015">
135
</page>
<reference confidence="0.999867175824176">
Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi,
Rivlin Ehud, Solan Zach, Wolfman Gadi, and Ruppin
Eytan. 2001. Placing Search in Context: The Concept
Revisited. In Proceedings of the Tenth International
World Wide Web Conference.
Bruno Gaume and Fabien Mathieu. 2008. PageRank In-
duced Topology for Real-World Networks. Complex
Systems, page (on line).
Samer Hassan and Rada Mihalcea. 2009. Cross-lingual
Semantic Relatedness Using Encyclopedic Knowl-
edge. In EMNLP, pages 1192–1201. ACL.
Karl Moritz Hermann and Phil Blunsom. 2014. Multilin-
gual Models for Compositional Distributed Semantics.
CoRR, abs/1404.4641.
Jay J. Jiang and David .W. Conrath. 1997. Semantic Sim-
ilarity Based on Corpus Statistics and Lexical Taxon-
omy. In Proc. of the Int’l. Conf. on Research in Com-
putational Linguistics, pages 19–33.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: The Tenth Machine Translation Summit,
pages 79–86, Phuket, Thailand. AAMT, AAMT.
Stanley Kok and Chris Brockett. 2010. Hitting the Right
Paraphrases in Good Time. In HLT-NAACL, pages
145–153. The Association for Computational Linguis-
tics.
Sebastian L¨obner. 2002. Understanding Semantics. Ox-
ford University Press, New York.
Alexander Mehler, Olga Pustylnikov, and Nils Diewald.
2011. Geography of social ontologies: Testing a
variant of the Sapir-Whorf Hypothesis in the con-
text of Wikipedia. Computer Speech &amp; Language,
25(3):716–740.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The Automatic Construction, Evaluation and
Application of a Wide-coverage Multilingual Seman-
tic Network. Artificial Intelligence, 193(0):217 – 250.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Comput. Linguist., 29(1):19–51, March.
Taraka Rama and Lars Borin. 2015. Comparative eval-
uation of string similarity measures for automatic lan-
guage classification. In Sequences in Language and
Text. De Gruyter Mouton.
Kevin Scannell. 2006. Machine translation for closely
related languages. In Proceedings of the Workshop on
Strategies for Developing Machine Translation for Mi-
nority Languages, pages 103–107.
Helmut Schmid. 1994. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. In International Confer-
ence on New Methods in Language Processing, pages
44–49, Manchester, UK.
Ineta Sejane and Steffen Eger. 2013. Semantic typolo-
gies by means of network analysis of bilingual dictio-
naries. In Lars Borin and Anju Saxena, editors, Ap-
proaches to Measuring Linguistic Differences, pages
447–474. De Gruyter.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised Multilingual
Learning for POS Tagging. In EMNLP, pages 1041–
1050. ACL.
Mark Steyvers and Josh Tenenbaum. 2005. The Large-
Scale Structure of Semantic Networks: Statistical
Analyses and a Model of Semantic Growth. Cognitive
Science, 29(1):41–78.
Michael Strube and Simone P. Ponzetto. 2006.
WikiRelate! Computing Semantic Relatedness using
Wikipedia. In Proceedings of the National Confer-
ence on Artificial Intelligence, volume 21, page 1419.
Menlo Park, CA; Cambridge, MA; London; AAAI
Press; MIT Press; 1999.
Morris Swadesh. 1955. Towards Greater Accuracy in
Lexicostatistic Dating. International Journal ofAmer-
ican Linguistics, 21:121–137.
Martine Vanhove, Bruno Gaume, and Karine Duvig-
nau. 2008. Semantic Associations and Confluences in
Paradigmatic Networks. In From Polysemy to Seman-
tic Change: Towards a Typology of Lexical Semantic
Associations, pages 233–264. John Benjamins.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers, Norwell, MA, USA.
Duncan. J. Watts and Steven H. Strogatz. 1998. Col-
lective dynamics of ’small-world’ networks. Nature,
393(6684):409–10.
Benjamin Whorf. 1956. Language, Thought, and Real-
ity: Selected Writings of Benjamin Lee Whorf. MIT
Press, Cambridge.
</reference>
<page confidence="0.998796">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.627848">
<title confidence="0.9995305">Towards Semantic Language Classification: Inducing and Semantic Association Networks from Europarl</title>
<author confidence="0.999703">Niko Alexander</author>
<affiliation confidence="0.872832333333333">Technology Computational Linguistics Goethe University Frankfurt am</affiliation>
<abstract confidence="0.996023529411765">We induce semantic association networks from translation relations in parallel corpora. The resulting semantic spaces are encoded in a single reference language, which ensures cross-language comparability. As our main contribution, we cluster the obtained (crosslingually comparable) lexical semantic spaces. We find that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with Bilingual Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="32046" citStr="Bannard and Callison-Burch (2005)" startWordPosition="5184" endWordPosition="5187">.22 hu et fi pt es fr it sl bg pl da sv nl de lv lt cs sk el ro 134 5 Conclusion We have encoded lexical semantic spaces of different languages by means of the same pivot language in order to make the languages comparable. To this end, we introduced association networks in which links between words in the reference language depend on translations from the respective source language, weighted by probability of translation. Our methodology is closely related to analogous approaches in the paraphrasing community which interlink paraphrases by means of their translations in other languages (e.g., Bannard and Callison-Burch (2005), Kok and Brockett (2010)), but our application scenario is different and we also describe a principled manner to generate weighted links between lexical units from multilingual data. Using random walks to represent similarities among words in the association networks, we finally derived similarity values for pairs of languages. This allowed us to perform several cluster analyses to group the 20 source languages. Interestingly, in our data sample, semantic language classification appears to be almost perfectly correlated with genealogical relationships between languages. To the best of our kno</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 597–604,</rawString>
</citation>
<citation valid="true">
<title>13But see also the first author’s preliminary investigations on semantic language classification in Sejane and Eger (2013), based on freely available (low-quality) bilingual dictionaries, and Eger</title>
<date>2012</date>
<contexts>
<context position="1367" citStr="(2012)" startWordPosition="181" endWordPosition="181">tutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. More precisely, we cluster (classify) languages based on their semantic relations between lexical units. The outcome of our classification may have direct co</context>
<context position="5768" citStr="(2012)" startWordPosition="854" endWordPosition="854"> which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological prop</context>
</contexts>
<marker>2012</marker>
<rawString>13But see also the first author’s preliminary investigations on semantic language classification in Sejane and Eger (2013), based on freely available (low-quality) bilingual dictionaries, and Eger (2012).</rawString>
</citation>
<citation valid="false">
<authors>
<author>PA Stroudsburg</author>
</authors>
<institution>USA. Association for Computational Linguistics.</institution>
<marker>Stroudsburg, </marker>
<rawString>Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Bhargava</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>How Do You Pronounce Your Name?: Improving G2P with Transliterations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>399--408</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1335" citStr="Bhargava and Kondrak (2011)" startWordPosition="173" endWordPosition="176">genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. More precisely, we cluster (classify) languages based on their semantic relations between lexical units. The outcome of our c</context>
</contexts>
<marker>Bhargava, Kondrak, 2011</marker>
<rawString>Aditya Bhargava and Grzegorz Kondrak. 2011. How Do You Pronounce Your Name?: Improving G2P with Transliterations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 399–408, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Bhargava</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Leveraging Supplemental Representations for Sequential Transduction. In</title>
<date>2012</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>396--406</pages>
<contexts>
<context position="1367" citStr="Bhargava and Kondrak (2012)" startWordPosition="178" endWordPosition="181">nowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. More precisely, we cluster (classify) languages based on their semantic relations between lexical units. The outcome of our classification may have direct co</context>
</contexts>
<marker>Bhargava, Kondrak, 2012</marker>
<rawString>Aditya Bhargava and Grzegorz Kondrak. 2012. Leveraging Supplemental Representations for Sequential Transduction. In HLT-NAACL, pages 396–406. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The Anatomy of a Large-scale Hypertextual Web Search Engine.</title>
<date>1998</date>
<journal>Comput. Netw. ISDNSyst.,</journal>
<pages>30--1</pages>
<contexts>
<context position="15588" citStr="Brin and Page (1998)" startWordPosition="2465" endWordPosition="2468">k versions G1, ... , GM of language R that are output by network generation, we first define the vector representation of node vk in graph Gk = (Vk, Wk) as the probability vector of ending up in any of the nodes of Gk when a random surfer starts from vk and surfs on the graph Gk according to the normalized weight matrix Wk = [Wk(α, Q)](α,β)EVk×Vk. Note that the higher Wk(α, Q), the higher the likelihood that the surfer takes the transition from α to Q. More precisely, we let the meaning [vk] of node vk in graph Gk be the vector vk that results as the limit of the iterative process (see, e.g., Brin and Page (1998), Gaume and Mathieu (2008), Kok and Brockett (2010)), vkN+1 = dvk NA(k) + (1 − d)vk0, where each vkN, for N ≥ 0, is a 1 x |Rvoc |vector, A(k) is obtained from Wk by normalizing all rows such that A(k) is row-stochastic, and d is a damping factor that describes preference for the starting vector vk0, which is a vector of zeros except for index 3More correctly, one could define Pk[α|z] = fz1 , whenever α is a translation of z, and Pk[α|z] = 0, otherwise, where fz is the number of translations of word z. This would lead to an analogous interpretation as the given one. 4This reasoning ignores case</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-scale Hypertextual Web Search Engine. Comput. Netw. ISDNSyst., 30(1-7):107–117, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyle Campbell</author>
</authors>
<title>How to show Languages are related: Methods for Distant Genetic Relationship.</title>
<date>2003</date>
<booktitle>In The Handbook of Historical Linguistics.</booktitle>
<publisher>Blackwell.</publisher>
<contexts>
<context position="7464" citStr="Campbell, 2003" startWordPosition="1098" endWordPosition="1099">ages for a given query language. Vanhove et al. (2008) construct so-called semantic proximity networks based on monolingual dictionaries, and envision to use them for semantic typologies. They do not apply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, phonetic and syntactic similarity of languages have received considerably more attention than semantic similarity, as we focus on. Classical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages</context>
</contexts>
<marker>Campbell, 2003</marker>
<rawString>Lyle Campbell. 2003. How to show Languages are related: Methods for Distant Genetic Relationship. In The Handbook of Historical Linguistics. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarath Chandar A P</author>
<author>Stanislas</author>
</authors>
<title>Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha.</title>
<date>2014</date>
<booktitle>Advances in Neural Information Processing Systems 27,</booktitle>
<pages>1853--1861</pages>
<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<marker>P, Stanislas, 2014</marker>
<rawString>Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. 2014. An Autoencoder Approach to Learning Bilingual Word Representations. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1853–1861. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Maria Ciobanu</author>
<author>Liviu P Dinu</author>
</authors>
<title>An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>1047--1058</pages>
<location>Doha,</location>
<contexts>
<context position="7675" citStr="Ciobanu and Dinu (2014)" startWordPosition="1127" endWordPosition="1130">ply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, phonetic and syntactic similarity of languages have received considerably more attention than semantic similarity, as we focus on. Classical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). This results in an imperfect reproduction of the ge128 Figure 1: Excerpts of bilingual dictionaries as bipartite graphs with links between words if</context>
</contexts>
<marker>Ciobanu, Dinu, 2014</marker>
<rawString>Alina Maria Ciobanu and Liviu P. Dinu. 2014. An Etymological Approach to Cross-Language Orthographic Similarity. Application on Romanian. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 1047–1058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin C Cooper</author>
</authors>
<title>Measuring the Semantic Distance between Languages from a Statistical Analysis of Bilingual Dictionaries.</title>
<date>2008</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="4557" citStr="Cooper (2008)" startWordPosition="659" endWordPosition="660">ver word pairs, it also allows for calculating semantic distances between pairs of languages. The Sapir-Whorf Hypothesis (SWH) (Whorf, 1956) already predicts that semantic relations are not universal. Though we are agnostic about the assumptions underlying the SWH, it nevertheless gives an evaluation criterion for our experiment: if the SWH is true, we expect a clustering of translation-based semantic spaces along the genealogical relationships of the languages involved. However, genealogy is certainly not the sole principle potentially underlying a typology of lexical semantics. For example, Cooper (2008) finds that French is semantically closer to Basque, a putatively non-Indoeuropean language, than to German. To the best of our knowledge, a large-scale quantitative typological analysis of lexical semantics is lacking thus far and we intend to make first steps towards this target. The paper is structured as follows. Section 2 outlines related work. Section 3 presents our formal model and Section 4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatednes</context>
<context position="6558" citStr="Cooper (2008)" startWordPosition="970" endWordPosition="971">ral network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological properties of semantic (concept) graphs derived from Wikipedia. This approach is top-down in that it assumes that the genealogical tree is the desired output of the classification. Cooper (2008) computes semantic distances between languages based on the curvature of translation histograms in bilingual dictionaries. While this results in some interesting findings as indicated, the approach is not applied to language classification, but focuses on computing semantically similar languages for a given query language. Vanhove et al. (2008) construct so-called semantic proximity networks based on monolingual dictionaries, and envision to use them for semantic typologies. They do not apply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, pho</context>
<context position="11704" citStr="Cooper (2008)" startWordPosition="1772" endWordPosition="1773">of this inaccuracy are outlined below. • By using unannotated corpora, we cannot straightforwardly distinguish between cases of polysemy and homonymy. The problem is that homonymy should (ideally) not contribute to generating lexical semantic association networks as considered here. However, homonymy is apparently a rather rare phenomenon, while polysemy, which we expect to underlie the structure of our networks, is abundant (cf. L¨obner (2002)). On the other hand, • classical dictionaries can be very heterogeneous in their scope and denomination of translation links between words (see, e.g., Cooper (2008)), making the respective editors of the bilingual dictionaries distorting variables. 1Each network represents the semantic relations of both languages R and L, but since we keep R fixed and vary L, each association network inherits the same properties from R. MAN vir PERSON MAN mann HUMAN PERSON homo DEMIGOD (a) English-Latin MALE (b) English-German HUSBAND HERO heros WARRIOR HUMAN HUSBAND mensch GUY 129 • Corpus data allows for inducing probabilities of translation relations of words, which indicate weighted links more accurately than ranked assignments provided by classical dictionaries. • C</context>
</contexts>
<marker>Cooper, 2008</marker>
<rawString>Martin C. Cooper. 2008. Measuring the Semantic Distance between Languages from a Statistical Analysis of Bilingual Dictionaries. Journal of Quantitative Linguistics, 15(1):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Eger</author>
<author>Ineta Sejane</author>
</authors>
<title>Computing Semantic Similarity from Bilingual Dictionaries.</title>
<date>2010</date>
<booktitle>In Proceedings of the 10th International Conference on the Statistical Analysis of Textual Data (JADT-2010),</booktitle>
<pages>1217--1225</pages>
<location>Rome, Italy,</location>
<contexts>
<context position="9341" citStr="Eger and Sejane (2010)" startWordPosition="1386" endWordPosition="1389">rks in any of the languages involved by placing a link between two words of the same language if and only if they share a common translation in the other language (cf. Figure 2). Since translations provide partially synonymous expression in the target language, the latter links can be seen to denote semantic relatedness (in terms of synonymy) of the interlinked words. Further, the more distant two words in such a lexical semantic association network, the lower the degree of their partial synonymy: the longer the path from one word to another, the higher the loss of relatedness among them (cf. Eger and Sejane (2010)). Note that association networks derived from bilingual dictionaries represent semantic similarities of words of the source language R subject to semantic relations of their translations in the target language L. The reason is that whether or not a link is established between two words α and Q in R depends on associations of their translations present in L. To illustrate this, consider the association networks outlined in Figure 2, induced from the bilingual dictionaries outlined in Figure 1, which match between R = English and L = Latin and L = German, respectively. When L is classical Latin</context>
</contexts>
<marker>Eger, Sejane, 2010</marker>
<rawString>Steffen Eger and Ineta Sejane. 2010. Computing Semantic Similarity from Bilingual Dictionaries. In Proceedings of the 10th International Conference on the Statistical Analysis of Textual Data (JADT-2010), pages 1217–1225, Rome, Italy, June. JADT-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Eger</author>
</authors>
<title>Lexical Semantic Typologies from Bilingual Corpora — A Framework. In</title>
<date>2012</date>
<booktitle>SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>90--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Eger, 2012</marker>
<rawString>Steffen Eger. 2012. Lexical Semantic Typologies from Bilingual Corpora — A Framework. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 90–94. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="15588" citStr="(1998)" startWordPosition="2468" endWordPosition="2468"> ... , GM of language R that are output by network generation, we first define the vector representation of node vk in graph Gk = (Vk, Wk) as the probability vector of ending up in any of the nodes of Gk when a random surfer starts from vk and surfs on the graph Gk according to the normalized weight matrix Wk = [Wk(α, Q)](α,β)EVk×Vk. Note that the higher Wk(α, Q), the higher the likelihood that the surfer takes the transition from α to Q. More precisely, we let the meaning [vk] of node vk in graph Gk be the vector vk that results as the limit of the iterative process (see, e.g., Brin and Page (1998), Gaume and Mathieu (2008), Kok and Brockett (2010)), vkN+1 = dvk NA(k) + (1 − d)vk0, where each vkN, for N ≥ 0, is a 1 x |Rvoc |vector, A(k) is obtained from Wk by normalizing all rows such that A(k) is row-stochastic, and d is a damping factor that describes preference for the starting vector vk0, which is a vector of zeros except for index 3More correctly, one could define Pk[α|z] = fz1 , whenever α is a translation of z, and Pk[α|z] = 0, otherwise, where fz is the number of translations of word z. This would lead to an analogous interpretation as the given one. 4This reasoning ignores case</context>
<context position="20931" citStr="(1998)" startWordPosition="3351" endWordPosition="3351"> and LESBIAN is highest in the Czech version of English, while that between WOMAN and GIRL is strongest in the Finnish version. All in all, the wiring and the thickness of links clearly differ across language networks, indicating that the languages differ in terms of semantic relations of their translations. Table 1 shows network statistics of the graphs Gk. All network versions of English consist of exactly 5,021 English (lemmatized) words. The networks show a high cluster value, indicating that neighbors of a word are probably interlinked (i.e., semantically related) (cf. Watts and Strogatz (1998)). Average path lengths and diameters are low, that is, distances between words are short, as is typically observed for semantic networks (cf. Steyvers and Tenenbaum (2005)). The density of the networks (measured by the ratio of existing links and the upper bound of theoretically possible links) varies substantially for the language networks. For instance, in the Hungarian network version of English, only 2.56% of the possible links are realized, while in the Dutch version, 8.45% are present. This observation may hint at the ‘degree of analyticity’ of a language: the more word forms per lemma </context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Gabrilovich Evgenly</author>
<author>Matias Yossi</author>
<author>Rivlin Ehud</author>
<author>Solan Zach</author>
<author>Wolfman Gadi</author>
<author>Ruppin Eytan</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth International World Wide Web Conference.</booktitle>
<contexts>
<context position="23567" citStr="Finkelstein et al., 2001" startWordPosition="3809" endWordPosition="3812">d by us, and since analyticity is a morphological rather than a semantic phenomenon, it may be possible that the classification results reported below are in fact due to syntagmatic relations – in contrast to our hypothesis about their semantic, paradigmatic nature. We address this issue below. Semantic similarity Before proceeding to our main task, the clustering of semantic spaces, we measure how strongly our semantic association networks capture semantics. To this end, we compute the correlation coefficient between the semantic similarity scores of the word pairs in the WordSimilarity-353 (Finkelstein et al., 2001) English word relatedness dataset and the similarity scores, for the same word pairs, obtained by our method. The WordSimilarity-353 dataset consists of 353 word pairs annotated by the average of 13 human experts, each on a scale from 0 (unrelated) to 10 (very closely related or identical). We evaluated only on those word pairs for which each word in the pair is contained in our set of 5,021 English words, which amounted to 172 word pairs. To be more 132 mother woman spouse lesbian gender female girl wife pregnan man mother woman spouse lesbian gender female girl wife pregnant man mother woman</context>
</contexts>
<marker>Finkelstein, Evgenly, Yossi, Ehud, Zach, Gadi, Eytan, 2001</marker>
<rawString>Lev Finkelstein, Gabrilovich Evgenly, Matias Yossi, Rivlin Ehud, Solan Zach, Wolfman Gadi, and Ruppin Eytan. 2001. Placing Search in Context: The Concept Revisited. In Proceedings of the Tenth International World Wide Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Gaume</author>
<author>Fabien Mathieu</author>
</authors>
<title>PageRank Induced Topology for Real-World Networks. Complex Systems,</title>
<date>2008</date>
<note>page (on line).</note>
<contexts>
<context position="15614" citStr="Gaume and Mathieu (2008)" startWordPosition="2469" endWordPosition="2472">M of language R that are output by network generation, we first define the vector representation of node vk in graph Gk = (Vk, Wk) as the probability vector of ending up in any of the nodes of Gk when a random surfer starts from vk and surfs on the graph Gk according to the normalized weight matrix Wk = [Wk(α, Q)](α,β)EVk×Vk. Note that the higher Wk(α, Q), the higher the likelihood that the surfer takes the transition from α to Q. More precisely, we let the meaning [vk] of node vk in graph Gk be the vector vk that results as the limit of the iterative process (see, e.g., Brin and Page (1998), Gaume and Mathieu (2008), Kok and Brockett (2010)), vkN+1 = dvk NA(k) + (1 − d)vk0, where each vkN, for N ≥ 0, is a 1 x |Rvoc |vector, A(k) is obtained from Wk by normalizing all rows such that A(k) is row-stochastic, and d is a damping factor that describes preference for the starting vector vk0, which is a vector of zeros except for index 3More correctly, one could define Pk[α|z] = fz1 , whenever α is a translation of z, and Pk[α|z] = 0, otherwise, where fz is the number of translations of word z. This would lead to an analogous interpretation as the given one. 4This reasoning ignores cases of homonymy, which weake</context>
</contexts>
<marker>Gaume, Mathieu, 2008</marker>
<rawString>Bruno Gaume and Fabien Mathieu. 2008. PageRank Induced Topology for Real-World Networks. Complex Systems, page (on line).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samer Hassan</author>
<author>Rada Mihalcea</author>
</authors>
<title>Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>1192--1201</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="5583" citStr="Hassan and Mihalcea (2009)" startWordPosition="823" endWordPosition="826">4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). Ther</context>
<context position="25594" citStr="Hassan and Mihalcea (2009)" startWordPosition="4142" endWordPosition="4146">20 different correlation coefficients, one for each network version of English, shown in Table 2. it 0.34678 ... ... pt 0.32249 sl 0.25720 es 0.31990 bg 0.25372 ro 0.31204 hu 0.24910 nl 0.30885 et 0.24212 da 0.30715 lt 0.24207 Table 2: Sample Pearson correlation coefficients between human gold standard and our approach for different network versions of English. We first note that the correlation coefficients differ between network versions of English, where the Italian version exhibits the highest correlation with the (English) human reference, and the Lithuanian version the lowest. Note that Hassan and Mihalcea (2009) obtain a correlation coefficient of 0.55 on the whole WordSimilarity-353 dataset, which is considerably higher than our best score of 0.34. However, first note that our networks, which consist of 5,021 lexical units, are quite small compared to the data sizes that other studies rely on, which makes a comparison highly unfair. Secondly, one has to see that we compute the semantic relatedness of English words from the semantic point of view of two languages: the reference language and the respective source language (e.g., the Italian version of English), which, by our very postulate, differs fr</context>
</contexts>
<marker>Hassan, Mihalcea, 2009</marker>
<rawString>Samer Hassan and Rada Mihalcea. 2009. Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge. In EMNLP, pages 1192–1201. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual Models for Compositional Distributed Semantics.</title>
<date>2014</date>
<location>CoRR, abs/1404.4641.</location>
<contexts>
<context position="6153" citStr="Hermann and Blunsom (2014)" startWordPosition="906" endWordPosition="909"> (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological properties of semantic (concept) graphs derived from Wikipedia. This approach is top-down in that it assumes that the genealogical tree is the desired output of the classification. Cooper (2008) computes semantic distances between languages based on the curvature of translation histograms in bilingual dictionaries. While this results in some interesting findings as indicated, the approa</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual Models for Compositional Distributed Semantics. CoRR, abs/1404.4641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the Int’l. Conf. on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="5457" citStr="Jiang and Conrath, 1997" startWordPosition="804" endWordPosition="807">arget. The paper is structured as follows. Section 2 outlines related work. Section 3 presents our formal model and Section 4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David .W. Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In Proc. of the Int’l. Conf. on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: The Tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand. AAMT, AAMT.</location>
<contexts>
<context position="7916" citStr="Koehn (2005)" startWordPosition="1162" endWordPosition="1163">es in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). This results in an imperfect reproduction of the ge128 Figure 1: Excerpts of bilingual dictionaries as bipartite graphs with links between words if and only if one is a translation of the other. Data from www.latin-dictionary.net and dict.leo.org. nealogical language tree for the languages involved. 3 Model We start with motivating our approach by example of bilingual dictionaries befo</context>
<context position="18435" citStr="Koehn, 2005" startWordPosition="2948" endWordPosition="2949">sitive weight mass to Pfr[LOVE|ˆetre] 5We always set d to 0.8 in our experiments. although, from a point of view of a classical dictionary, translating ˆetre into love is clearly problematic. Since it is likely that, e.g., Pfr[HUMAN|ˆetre] and Pfr[BEING|ˆetre] will also be positive, we can expect weighted links in the French network version of English between HUMAN and LOVE as well as between BEING and LOVE. Thus, besides ‘true’ semantic relations, our approach also captures, though unintentionally, co-occurrence relations. 4 Experiments We evaluate our method by means of the Europarl corpus (Koehn, 2005). Europarl documents the proceedings of the European parliament in the 21 official languages of the European Union. This provides us with sentence-aligned multi-texts in which each tuple of sentences expresses the same underlying meaning.6 Using GIZA++, this allows us to estimate the conditional translation probabilities P[A|B] for any two words A, B from any two languages in the Europarl corpus. In our experiment, we focus on the approx. 400,000 sentences for which translations in all 21 languages are available. To process this data, we set all words of all sentences to lower-case. Ideally, w</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: The Tenth Machine Translation Summit, pages 79–86, Phuket, Thailand. AAMT, AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Chris Brockett</author>
</authors>
<title>Hitting the Right Paraphrases in Good Time.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>145--153</pages>
<contexts>
<context position="15639" citStr="Kok and Brockett (2010)" startWordPosition="2473" endWordPosition="2476">utput by network generation, we first define the vector representation of node vk in graph Gk = (Vk, Wk) as the probability vector of ending up in any of the nodes of Gk when a random surfer starts from vk and surfs on the graph Gk according to the normalized weight matrix Wk = [Wk(α, Q)](α,β)EVk×Vk. Note that the higher Wk(α, Q), the higher the likelihood that the surfer takes the transition from α to Q. More precisely, we let the meaning [vk] of node vk in graph Gk be the vector vk that results as the limit of the iterative process (see, e.g., Brin and Page (1998), Gaume and Mathieu (2008), Kok and Brockett (2010)), vkN+1 = dvk NA(k) + (1 − d)vk0, where each vkN, for N ≥ 0, is a 1 x |Rvoc |vector, A(k) is obtained from Wk by normalizing all rows such that A(k) is row-stochastic, and d is a damping factor that describes preference for the starting vector vk0, which is a vector of zeros except for index 3More correctly, one could define Pk[α|z] = fz1 , whenever α is a translation of z, and Pk[α|z] = 0, otherwise, where fz is the number of translations of word z. This would lead to an analogous interpretation as the given one. 4This reasoning ignores cases of homonymy, which weaken the semantic argument. </context>
<context position="32071" citStr="Kok and Brockett (2010)" startWordPosition="5188" endWordPosition="5191">a sv nl de lv lt cs sk el ro 134 5 Conclusion We have encoded lexical semantic spaces of different languages by means of the same pivot language in order to make the languages comparable. To this end, we introduced association networks in which links between words in the reference language depend on translations from the respective source language, weighted by probability of translation. Our methodology is closely related to analogous approaches in the paraphrasing community which interlink paraphrases by means of their translations in other languages (e.g., Bannard and Callison-Burch (2005), Kok and Brockett (2010)), but our application scenario is different and we also describe a principled manner to generate weighted links between lexical units from multilingual data. Using random walks to represent similarities among words in the association networks, we finally derived similarity values for pairs of languages. This allowed us to perform several cluster analyses to group the 20 source languages. Interestingly, in our data sample, semantic language classification appears to be almost perfectly correlated with genealogical relationships between languages. To the best of our knowledge, our translation-b</context>
</contexts>
<marker>Kok, Brockett, 2010</marker>
<rawString>Stanley Kok and Chris Brockett. 2010. Hitting the Right Paraphrases in Good Time. In HLT-NAACL, pages 145–153. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian L¨obner</author>
</authors>
<title>Understanding Semantics.</title>
<date>2002</date>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<marker>L¨obner, 2002</marker>
<rawString>Sebastian L¨obner. 2002. Understanding Semantics. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Mehler</author>
<author>Olga Pustylnikov</author>
<author>Nils Diewald</author>
</authors>
<title>Geography of social ontologies: Testing a variant of the Sapir-Whorf Hypothesis in the context of Wikipedia.</title>
<date>2011</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="6295" citStr="Mehler et al. (2011)" startWordPosition="928" endWordPosition="931">e semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological properties of semantic (concept) graphs derived from Wikipedia. This approach is top-down in that it assumes that the genealogical tree is the desired output of the classification. Cooper (2008) computes semantic distances between languages based on the curvature of translation histograms in bilingual dictionaries. While this results in some interesting findings as indicated, the approach is not applied to language classification, but focuses on computing semantically similar languages for a given query language. Vanhove et a</context>
</contexts>
<marker>Mehler, Pustylnikov, Diewald, 2011</marker>
<rawString>Alexander Mehler, Olga Pustylnikov, and Nils Diewald. 2011. Geography of social ontologies: Testing a variant of the Sapir-Whorf Hypothesis in the context of Wikipedia. Computer Speech &amp; Language, 25(3):716–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="6176" citStr="Mikolov et al. (2013)" startWordPosition="910" endWordPosition="913">. Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological properties of semantic (concept) graphs derived from Wikipedia. This approach is top-down in that it assumes that the genealogical tree is the desired output of the classification. Cooper (2008) computes semantic distances between languages based on the curvature of translation histograms in bilingual dictionaries. While this results in some interesting findings as indicated, the approach is not applied to la</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The Automatic Construction, Evaluation and Application of a Wide-coverage Multilingual Semantic Network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<volume>193</volume>
<issue>0</issue>
<pages>250</pages>
<contexts>
<context position="1538" citStr="Navigli and Ponzetto (2012)" startWordPosition="200" endWordPosition="204">rtant for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we investigate their (dis-)similarities. More precisely, we cluster (classify) languages based on their semantic relations between lexical units. The outcome of our classification may have direct consequences for approaches that integrate diverse multilingual resources. For example, from a linguistic point of view, it might be argued that integrating very heterogeneo</context>
<context position="5768" citStr="Navigli and Ponzetto (2012)" startWordPosition="851" endWordPosition="854">antic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), Mikolov et al. (2013)). There have been (a) few different computational approaches to semantic language classification. Mehler et al. (2011) test whether languages are genealogically separable via topological prop</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The Automatic Construction, Evaluation and Application of a Wide-coverage Multilingual Semantic Network. Artificial Intelligence, 193(0):217 – 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11058" citStr="Och and Ney, 2003" startWordPosition="1669" endWordPosition="1672">ontrasting such networks may then allow for clustering languages due to shared lexical semantic associations. As mentioned above, we generalize the model outlined so far to the situation of probabilistic translation relationships derived from corpus data, rather than from bilingual dictionaries. Working on corpus data has both advantages and disadvantages compared to using human compiled and edited dictionaries. On the one hand, • the translation relations induced from corpus data are noisy since their estimation is partially inaccurate due to limitations of alignment toolkits such as GIZA++ (Och and Ney, 2003) as employed by us. Implications of this inaccuracy are outlined below. • By using unannotated corpora, we cannot straightforwardly distinguish between cases of polysemy and homonymy. The problem is that homonymy should (ideally) not contribute to generating lexical semantic association networks as considered here. However, homonymy is apparently a rather rare phenomenon, while polysemy, which we expect to underlie the structure of our networks, is abundant (cf. L¨obner (2002)). On the other hand, • classical dictionaries can be very heterogeneous in their scope and denomination of translation</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taraka Rama</author>
<author>Lars Borin</author>
</authors>
<title>Comparative evaluation of string similarity measures for automatic language classification.</title>
<date>2015</date>
<booktitle>In Sequences in Language and Text. De Gruyter Mouton.</booktitle>
<contexts>
<context position="7487" citStr="Rama and Borin, 2015" startWordPosition="1100" endWordPosition="1103"> query language. Vanhove et al. (2008) construct so-called semantic proximity networks based on monolingual dictionaries, and envision to use them for semantic typologies. They do not apply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, phonetic and syntactic similarity of languages have received considerably more attention than semantic similarity, as we focus on. Classical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yi</context>
</contexts>
<marker>Rama, Borin, 2015</marker>
<rawString>Taraka Rama and Lars Borin. 2015. Comparative evaluation of string similarity measures for automatic language classification. In Sequences in Language and Text. De Gruyter Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Scannell</author>
</authors>
<title>Machine translation for closely related languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Strategies for Developing Machine Translation for Minority Languages,</booktitle>
<pages>103--107</pages>
<contexts>
<context position="7902" citStr="Scannell, 2006" startWordPosition="1160" endWordPosition="1161">lassical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). This results in an imperfect reproduction of the ge128 Figure 1: Excerpts of bilingual dictionaries as bipartite graphs with links between words if and only if one is a translation of the other. Data from www.latin-dictionary.net and dict.leo.org. nealogical language tree for the languages involved. 3 Model We start with motivating our approach by example of bilingual dic</context>
</contexts>
<marker>Scannell, 2006</marker>
<rawString>Kevin Scannell. 2006. Machine translation for closely related languages. In Proceedings of the Workshop on Strategies for Developing Machine Translation for Minority Languages, pages 103–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="19469" citStr="Schmid, 1994" startWordPosition="3115" endWordPosition="3116"> focus on the approx. 400,000 sentences for which translations in all 21 languages are available. To process this data, we set all words of all sentences to lower-case. Ideally, we would have lemmatized all texts, but did not do so because of the unavailability of lemmatizers for some of the languages. Therefore, we decided to lemmatize only words in the reference language and kept full-forms for all source languages.7 We choose 6In a tuple of sentences, one sentence is the source of which all the other sentences are translations. 7Lemmatization tools and models are taken from the TreeTagger (Schmid, 1994) home page www.cis. uni-muenchen.de/˜schmid/tools/TreeTagger 131 English as the reference language.8 In all languages, we omitted all words whose corpus frequency is less than 50 and excluded the 100 most frequent (mostly function) words.9 In the reference language, we also ignored all words whose characters do not belong to the standard English character set. Figure 3 shows subgraphs centered around the seed word WOMAN in five network versions of English. All subgraphs are constructed using the Europarl data. Apparently, the network versions of English diverge from each other. For instance, t</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ineta Sejane</author>
<author>Steffen Eger</author>
</authors>
<title>Semantic typologies by means of network analysis of bilingual dictionaries.</title>
<date>2013</date>
<booktitle>In Lars Borin and Anju Saxena, editors, Approaches to Measuring Linguistic Differences,</booktitle>
<pages>447--474</pages>
<note>De Gruyter.</note>
<marker>Sejane, Eger, 2013</marker>
<rawString>Ineta Sejane and Steffen Eger. 2013. Semantic typologies by means of network analysis of bilingual dictionaries. In Lars Borin and Anju Saxena, editors, Approaches to Measuring Linguistic Differences, pages 447–474. De Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised Multilingual Learning for POS Tagging. In</title>
<date>2008</date>
<booktitle>EMNLP,</booktitle>
<pages>1041--1050</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1173" citStr="Snyder et al. (2008)" startWordPosition="153" endWordPosition="156"> the obtained (crosslingually comparable) lexical semantic spaces. We find that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task. 1 Introduction There has been a recent surge of interest in integrating multilingual resources in natural language processing (NLP). For example, Snyder et al. (2008) show that jointly considering morphological segmentations across languages improves performance compared to the monolingual baseline. Bhargava and Kondrak (2011) and Bhargava and Kondrak (2012) demonstrate that string transduction can benefit from supplemental information provided in other languages. Analogously, in lexical semantics, Navigli and Ponzetto (2012) explore semantic relations from Wikipedia in different languages to induce a huge integrated lexical semantic network. In this paper, we also focus on multilingual resources in lexical semantics. But rather than integrating them, we i</context>
</contexts>
<marker>Snyder, Naseem, Eisenstein, Barzilay, 2008</marker>
<rawString>Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay. 2008. Unsupervised Multilingual Learning for POS Tagging. In EMNLP, pages 1041– 1050. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Josh Tenenbaum</author>
</authors>
<title>The LargeScale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth.</title>
<date>2005</date>
<journal>Cognitive Science,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="21103" citStr="Steyvers and Tenenbaum (2005)" startWordPosition="3374" endWordPosition="3377">g and the thickness of links clearly differ across language networks, indicating that the languages differ in terms of semantic relations of their translations. Table 1 shows network statistics of the graphs Gk. All network versions of English consist of exactly 5,021 English (lemmatized) words. The networks show a high cluster value, indicating that neighbors of a word are probably interlinked (i.e., semantically related) (cf. Watts and Strogatz (1998)). Average path lengths and diameters are low, that is, distances between words are short, as is typically observed for semantic networks (cf. Steyvers and Tenenbaum (2005)). The density of the networks (measured by the ratio of existing links and the upper bound of theoretically possible links) varies substantially for the language networks. For instance, in the Hungarian network version of English, only 2.56% of the possible links are realized, while in the Dutch version, 8.45% are present. This observation may hint at the ‘degree of analyticity’ of a language: the more word forms per lemma there are in a language, the less likely they are linked by means of Eq. (1). 8Due to the limited availability of lemmatizers, not all languages could have served as a refe</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Mark Steyvers and Josh Tenenbaum. 2005. The LargeScale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth. Cognitive Science, 29(1):41–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
<author>Simone P Ponzetto</author>
</authors>
<title>WikiRelate! Computing Semantic Relatedness using Wikipedia.</title>
<date>2006</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<volume>21</volume>
<pages>1419</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="5555" citStr="Strube and Ponzetto, 2006" startWordPosition="819" endWordPosition="822">ur formal model and Section 4 details our experiments on clustering semantic spaces across selected languages of the European Union. We conclude in Section 5. 2 Related work A field related to our research is semantic relatedness, in which the task is to determine the degree of semantic similarity between pairs of words, such as tiger and cat, sex and love, etc. Classically, semantic word networks such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998) have been used to address this problem (Jiang and Conrath, 1997), and, more recently, taxonomies and knowledge bases such as Wikipedia (Strube and Ponzetto, 2006). Hassan and Mihalcea (2009) define the task of cross-lingual semantic relatedness, in which the goal is to determine the semantic similarity between words from different languages, and Navigli and Ponzetto (2012) have combined WordNet with Wikipedia to construct a multi-layer semantic network in which computation of cross-lingual semantic relatedness may be performed. Most recently, neural network-based distributed semantic representations focusing on cross-language similarities between words and larger textual units have become popular (Chandar A P et al. (2014), Hermann and Blunsom (2014), </context>
</contexts>
<marker>Strube, Ponzetto, 2006</marker>
<rawString>Michael Strube and Simone P. Ponzetto. 2006. WikiRelate! Computing Semantic Relatedness using Wikipedia. In Proceedings of the National Conference on Artificial Intelligence, volume 21, page 1419. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morris Swadesh</author>
</authors>
<title>Towards Greater Accuracy in Lexicostatistic Dating.</title>
<date>1955</date>
<journal>International Journal ofAmerican Linguistics,</journal>
<pages>21--121</pages>
<contexts>
<context position="7530" citStr="Swadesh, 1955" startWordPosition="1109" endWordPosition="1110">o-called semantic proximity networks based on monolingual dictionaries, and envision to use them for semantic typologies. They do not apply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, phonetic and syntactic similarity of languages have received considerably more attention than semantic similarity, as we focus on. Classical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swadesh lists (Swadesh, 1955). Rama and Borin (2015) study the impact of different string similarity measures on orthographic language classification. Ciobanu and Dinu (2014) measure orthographic similarity between Romanian and related languages. They also indicate applications of (knowledge of) similarity values between languages, such as serving as a guide for machine translation (Scannell, 2006). Koehn (2005) produces a genealogical clustering of the languages in Europarl based on ease of translation, as measured in BLEU scores, between any two languages (which, putatively, yields a syntactic similarity indication). Th</context>
</contexts>
<marker>Swadesh, 1955</marker>
<rawString>Morris Swadesh. 1955. Towards Greater Accuracy in Lexicostatistic Dating. International Journal ofAmerican Linguistics, 21:121–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martine Vanhove</author>
<author>Bruno Gaume</author>
<author>Karine Duvignau</author>
</authors>
<title>Semantic Associations and Confluences in Paradigmatic Networks.</title>
<date>2008</date>
<booktitle>In From Polysemy to Semantic Change: Towards a Typology of Lexical Semantic Associations,</booktitle>
<pages>233--264</pages>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="3927" citStr="Vanhove et al. (2008)" startWordPosition="563" endWordPosition="566">my relations of their translations in the same target language. This approach ensures cross-language comparability of semantic spaces: Greek and Bulgarian are compared, for example, by means of the synonymy relations 127 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 127–136, Denver, Colorado, June 4–5, 2015. that are retained when translating them into the same pivot language (e.g., English). This approach does not only address proximities of pairs of words shared among languages (e.g., MEAT and BEEF, MOUTH and DOOR, CHILD and FRUIT – cf. Vanhove et al. (2008)). By averaging over word pairs, it also allows for calculating semantic distances between pairs of languages. The Sapir-Whorf Hypothesis (SWH) (Whorf, 1956) already predicts that semantic relations are not universal. Though we are agnostic about the assumptions underlying the SWH, it nevertheless gives an evaluation criterion for our experiment: if the SWH is true, we expect a clustering of translation-based semantic spaces along the genealogical relationships of the languages involved. However, genealogy is certainly not the sole principle potentially underlying a typology of lexical semanti</context>
<context position="6904" citStr="Vanhove et al. (2008)" startWordPosition="1021" endWordPosition="1024">t al. (2011) test whether languages are genealogically separable via topological properties of semantic (concept) graphs derived from Wikipedia. This approach is top-down in that it assumes that the genealogical tree is the desired output of the classification. Cooper (2008) computes semantic distances between languages based on the curvature of translation histograms in bilingual dictionaries. While this results in some interesting findings as indicated, the approach is not applied to language classification, but focuses on computing semantically similar languages for a given query language. Vanhove et al. (2008) construct so-called semantic proximity networks based on monolingual dictionaries, and envision to use them for semantic typologies. They do not apply their methodology to the multilingual setup, however, which a typology necessitates. Orthographic, phonetic and syntactic similarity of languages have received considerably more attention than semantic similarity, as we focus on. Classical approaches in determining orthographic/phonetic relatedness of languages are based on lexico-statistical comparisons of items in standardized word lists (Campbell, 2003; Rama and Borin, 2015), such as the Swa</context>
</contexts>
<marker>Vanhove, Gaume, Duvignau, 2008</marker>
<rawString>Martine Vanhove, Bruno Gaume, and Karine Duvignau. 2008. Semantic Associations and Confluences in Paradigmatic Networks. In From Polysemy to Semantic Change: Towards a Typology of Lexical Semantic Associations, pages 233–264. John Benjamins.</rawString>
</citation>
<citation valid="true">
<title>EuroWordNet: A Multilingual Database with Lexical Semantic Networks.</title>
<date>1998</date>
<editor>Piek Vossen, editor.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<contexts>
<context position="15588" citStr="(1998)" startWordPosition="2468" endWordPosition="2468"> ... , GM of language R that are output by network generation, we first define the vector representation of node vk in graph Gk = (Vk, Wk) as the probability vector of ending up in any of the nodes of Gk when a random surfer starts from vk and surfs on the graph Gk according to the normalized weight matrix Wk = [Wk(α, Q)](α,β)EVk×Vk. Note that the higher Wk(α, Q), the higher the likelihood that the surfer takes the transition from α to Q. More precisely, we let the meaning [vk] of node vk in graph Gk be the vector vk that results as the limit of the iterative process (see, e.g., Brin and Page (1998), Gaume and Mathieu (2008), Kok and Brockett (2010)), vkN+1 = dvk NA(k) + (1 − d)vk0, where each vkN, for N ≥ 0, is a 1 x |Rvoc |vector, A(k) is obtained from Wk by normalizing all rows such that A(k) is row-stochastic, and d is a damping factor that describes preference for the starting vector vk0, which is a vector of zeros except for index 3More correctly, one could define Pk[α|z] = fz1 , whenever α is a translation of z, and Pk[α|z] = 0, otherwise, where fz is the number of translations of word z. This would lead to an analogous interpretation as the given one. 4This reasoning ignores case</context>
<context position="20931" citStr="(1998)" startWordPosition="3351" endWordPosition="3351"> and LESBIAN is highest in the Czech version of English, while that between WOMAN and GIRL is strongest in the Finnish version. All in all, the wiring and the thickness of links clearly differ across language networks, indicating that the languages differ in terms of semantic relations of their translations. Table 1 shows network statistics of the graphs Gk. All network versions of English consist of exactly 5,021 English (lemmatized) words. The networks show a high cluster value, indicating that neighbors of a word are probably interlinked (i.e., semantically related) (cf. Watts and Strogatz (1998)). Average path lengths and diameters are low, that is, distances between words are short, as is typically observed for semantic networks (cf. Steyvers and Tenenbaum (2005)). The density of the networks (measured by the ratio of existing links and the upper bound of theoretically possible links) varies substantially for the language networks. For instance, in the Hungarian network version of English, only 2.56% of the possible links are realized, while in the Dutch version, 8.45% are present. This observation may hint at the ‘degree of analyticity’ of a language: the more word forms per lemma </context>
</contexts>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Watts</author>
<author>Steven H Strogatz</author>
</authors>
<title>Collective dynamics of ’small-world’ networks.</title>
<date>1998</date>
<journal>Nature,</journal>
<volume>393</volume>
<issue>6684</issue>
<contexts>
<context position="20931" citStr="Watts and Strogatz (1998)" startWordPosition="3348" endWordPosition="3351"> link between WOMAN and LESBIAN is highest in the Czech version of English, while that between WOMAN and GIRL is strongest in the Finnish version. All in all, the wiring and the thickness of links clearly differ across language networks, indicating that the languages differ in terms of semantic relations of their translations. Table 1 shows network statistics of the graphs Gk. All network versions of English consist of exactly 5,021 English (lemmatized) words. The networks show a high cluster value, indicating that neighbors of a word are probably interlinked (i.e., semantically related) (cf. Watts and Strogatz (1998)). Average path lengths and diameters are low, that is, distances between words are short, as is typically observed for semantic networks (cf. Steyvers and Tenenbaum (2005)). The density of the networks (measured by the ratio of existing links and the upper bound of theoretically possible links) varies substantially for the language networks. For instance, in the Hungarian network version of English, only 2.56% of the possible links are realized, while in the Dutch version, 8.45% are present. This observation may hint at the ‘degree of analyticity’ of a language: the more word forms per lemma </context>
</contexts>
<marker>Watts, Strogatz, 1998</marker>
<rawString>Duncan. J. Watts and Steven H. Strogatz. 1998. Collective dynamics of ’small-world’ networks. Nature, 393(6684):409–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Whorf</author>
</authors>
<title>Language, Thought, and Reality: Selected Writings of Benjamin Lee Whorf.</title>
<date>1956</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4084" citStr="Whorf, 1956" startWordPosition="588" endWordPosition="589">d, for example, by means of the synonymy relations 127 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 127–136, Denver, Colorado, June 4–5, 2015. that are retained when translating them into the same pivot language (e.g., English). This approach does not only address proximities of pairs of words shared among languages (e.g., MEAT and BEEF, MOUTH and DOOR, CHILD and FRUIT – cf. Vanhove et al. (2008)). By averaging over word pairs, it also allows for calculating semantic distances between pairs of languages. The Sapir-Whorf Hypothesis (SWH) (Whorf, 1956) already predicts that semantic relations are not universal. Though we are agnostic about the assumptions underlying the SWH, it nevertheless gives an evaluation criterion for our experiment: if the SWH is true, we expect a clustering of translation-based semantic spaces along the genealogical relationships of the languages involved. However, genealogy is certainly not the sole principle potentially underlying a typology of lexical semantics. For example, Cooper (2008) finds that French is semantically closer to Basque, a putatively non-Indoeuropean language, than to German. To the best of our</context>
</contexts>
<marker>Whorf, 1956</marker>
<rawString>Benjamin Whorf. 1956. Language, Thought, and Reality: Selected Writings of Benjamin Lee Whorf. MIT Press, Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>