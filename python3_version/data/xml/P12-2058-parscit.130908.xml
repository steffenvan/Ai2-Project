<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002729">
<title confidence="0.985739">
Heuristic Cube Pruning in Linear Time
</title>
<author confidence="0.997188">
Andrea Gesmundo
</author>
<affiliation confidence="0.998308666666667">
Department of
Computer Science
University of Geneva
</affiliation>
<email confidence="0.990946">
andrea.gesmundo@unige.ch
</email>
<author confidence="0.993596">
Giorgio Satta
</author>
<affiliation confidence="0.973377">
Department of
Information Engineering
University of Padua
</affiliation>
<email confidence="0.991966">
satta@dei.unipd.it
</email>
<author confidence="0.989668">
James Henderson
</author>
<affiliation confidence="0.998069">
Department of
Computer Science
University of Geneva
</affiliation>
<email confidence="0.993446">
james.henderson@unige.ch
</email>
<sectionHeader confidence="0.995575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998152">
We propose a novel heuristic algorithm for
Cube Pruning running in linear time in the
beam size. Empirically, we show a gain in
running time of a standard machine translation
system, at a small loss in accuracy.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999809307692308">
Since its first appearance in (Huang and Chiang,
2005), the Cube Pruning (CP) algorithm has quickly
gained popularity in statistical natural language pro-
cessing. Informally, this algorithm applies to sce-
narios in which we have the k-best solutions for two
input sub-problems, and we need to compute the k-
best solutions for the new problem representing the
combination of the two sub-problems.
CP has applications in tree and phrase based ma-
chine translation (Chiang, 2007; Huang and Chi-
ang, 2007; Pust and Knight, 2009), parsing (Huang
and Chiang, 2005), sentence alignment (Riesa and
Marcu, 2010), and in general in all systems combin-
ing inexact beam decoding with dynamic program-
ming under certain monotonic conditions on the def-
inition of the scores in the search space.
Standard implementations of CP run in time
O(k log(k)), with k being the size of the in-
put/output beams (Huang and Chiang, 2005). Ges-
mundo and Henderson (2010) propose Faster CP
(FCP) which optimizes the algorithm but keeps the
O(k log(k)) time complexity. Here, we propose a
novel heuristic algorithm for CP running in time
O(k) and evaluate its impact on the efficiency and
performance of a real-world machine translation
system.
</bodyText>
<sectionHeader confidence="0.992559" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.999397888888889">
Let G = (x0, ... , xk−1) be a list over R, that is,
an ordered sequence of real numbers, possibly with
repetitions. We write |G |= k to denote the length of
G. We say that G is descending if xi &gt; xj for every
i, j with 0 &lt; i &lt; j &lt; k. Let G1 = (x0, ... , xk−1)
and G2 = (y0, ... , yk′−1) be two descending lists
over R. We write G1 ® G2 to denote the descending
list with elements xi + yj for every i, j with 0 &lt; i &lt;
k and 0 &lt; j &lt; k′.
In cube pruning (CP) we are given as input two
descending lists G1, G2 over R with |G1 |= |G2 |=
k, and we are asked to compute the descending list
consisting of the first k elements of G1 ® G2.
A problem related to CP is the k-way merge
problem (Horowitz and Sahni, 1983). Given de-
scending lists Gi for every i with 0 &lt; i &lt; k, we
write mergek−1
i=0 Gi to denote the “merge” of all the
lists Gi, that is, the descending list with all elements
from the lists Gi, including repetitions.
For A E R we define shift(G, A) = G ® (A). In
words, shift(G, A) is the descending list whose ele-
ments are obtained by “shifting” the elements of G
by A, preserving the order. Let G1, G2 be descend-
ing lists of length k, with G2 = (y0, . . . , yk−1).
Then we can express the output of CP on G1, G2 as
the list
</bodyText>
<equation confidence="0.9786995">
mergek−1
i=0 shift(G1, yi) (1)
</equation>
<bodyText confidence="0.919945">
truncated after the first k elements. This shows that
the CP problem is a particular instance of the k-way
merge problem, in which all input lists are related by
k independent shifts.
</bodyText>
<page confidence="0.977649">
296
</page>
<note confidence="0.683232">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 296–300,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999870733333333">
Computation of the solution of the k-way merge
problem takes time O(q log(k)), where q is the
size of the output list. In case each input list has
length k this becomes O(k2 log(k)), and by restrict-
ing the computation to the first k elements, as re-
quired by the CP problem, we can further reduce to
O(k log(k)). This is the already known upper bound
on the CP problem (Huang and Chiang, 2005; Ges-
mundo and Henderson, 2010). Unfortunately, there
seems to be no way to achieve an asymptotically
faster algorithm by exploiting the restriction that the
input lists are all related by some shifts. Nonethe-
less, in the next sections we use the above ideas to
develop a heuristic algorithm running in time linear
in k.
</bodyText>
<sectionHeader confidence="0.986804" genericHeader="method">
3 Cube Pruning With Constant Slope
</sectionHeader>
<bodyText confidence="0.998764">
Consider lists L1, L2 defined as in section 2. We say
that L2 has constant slope if yi−1 − yi = 0 &gt; 0 for
every i with 0 &lt; i &lt; k. Throughout this section we
assume that L2 has constant slope, and we develop
an (exact) linear time algorithm for solving the CP
problem under this assumption.
</bodyText>
<equation confidence="0.9657518">
For each i ≥ 0, let Ii be the left-open interval
(x0 − (i + 1) · 0, x0 − i · 0] of R. Let also s =
⌊(x0 − xk−1)/0⌋ + 1. We split L1 into (possibly
empty) sublists Qi, 0 &lt; i &lt; s, called segments, such
that each Qi is the descending sublist consisting of
</equation>
<bodyText confidence="0.997583958333333">
all elements from L1 that belong to Ii. Thus, moving
down one segment in L1 is the closest equivalent to
moving down one element in L2.
Let t = min{k, s}; we define descending lists
Mi, 0 &lt; i &lt; t, as follows. We set M0 =
shift(Q0, y0), and for 1 &lt; i &lt; t we let
as the descending sublist consisting of all elements
of that column that belong to shift(Ii, yj). Then we
have Qi,j = shift(Qi, yj).
For any d with 0 &lt; d &lt; t, consider now all
segments Qi,j with i + j = d, forming a sub-
antidiagonal in L. We observe that these segments
contain all and only those elements of L that belong
to the interval Id. It is not difficult to show by in-
duction that these elements are exactly the elements
that appear in descending order in the list Mi defined
in (2).
We can then directly use relation (2) to iteratively
compute CP on two lists of length k, under our as-
sumption that one of the two lists has constant slope.
Using the fact that the merge of two lists as in (2) can
be computed in time linear in the size of the output
list, it is not difficult to implement the above algo-
rithm to run in time O(k).
</bodyText>
<sectionHeader confidence="0.975983" genericHeader="method">
4 Linear Time Heuristic Solution
</sectionHeader>
<bodyText confidence="0.9999244">
In this section we further elaborate on the exact al-
gorithm of section 3 for the constant slope case, and
develop a heuristic solution for the general CP prob-
lem. Let L1, L2, L and k be defined as in sections 2
and 3. Despite the fact that L2 does not have a con-
stant slope, we can still split each column of L into
segments, as follows.
Let eIi, 0 &lt; i &lt; k − 1, be the left-open interval
(x0 + yi+1, x0 + yi] of R. Note that, unlike the case
of section 3, intervals eIi’s are not all of the same size
now. Let also eIk−1 = [xk−1 + yk−1, x0 + yk−1].
For each i, j with 0 &lt; j &lt; k and 0 &lt; i &lt; k −
j, we define segment eQi,j as the descending sublist
consisting of all elements of the j-th column of L
that belong to eIi+j. In this way, the j-th column
</bodyText>
<equation confidence="0.597819">
Mi = merge{shift(Qi, y0), shift(Mi−1, −0)} (2) of L is split into segments eIj, eIj+1, ... , eIk−1, and
</equation>
<bodyText confidence="0.977859944444444">
We claim that the ordered concatenation of M0,
M1, ... , Mt−1 truncated after the first k elements
is exactly the output of CP on input L1, L2.
To prove our claim, it helps to visualize the de-
scending list L1 ⊕ L2 (of size k2) as a k x k matrix
L whose j-th column is shift(L1, yj), 0 &lt; j &lt; k.
For an interval I = (x, x′], we define shift(I, y) =
(x + y, x′ + y]. Similarly to what we have done with
L1, we can split each column of L into s segments.
For each i, j with 0 &lt; i &lt; s and 0 &lt; j &lt; k, we de-
fine the i-th segment of the j-th column, written Qi,j,
we have a variable number of segments per column.
Note that segments eQi,j with a constant value of i+j
contain all and only those elements of L that belong
to the left-open interval eIi+j.
Similarly to section 3, we define descending lists
fMi, 0 &lt; i &lt; k, by setting fM0 = eQ0,0 and, for
1 &lt; i &lt; k, by letting
</bodyText>
<equation confidence="0.95199">
Mi = merge{eQi,0 , path( fMi−1,L)} (3)
</equation>
<bodyText confidence="0.995542">
Note that the function path( Mi−1, L) should not re-
turn shift(fMi−1, −0), for some value 0, as in the
</bodyText>
<page confidence="0.972409">
297
</page>
<listItem confidence="0.995547315789474">
1: Algorithm 1 (L1, L2) : eL⋆
2: eL⋆.insert(L[0,0]);
3: referColumn ← 0;
4: xfollow ← L[0, 1];
5: xdeviate ← L[1, 0];
6: C ← CircularList([0,1]);
7: C-iteratoer ← C.begin();
8: while |L ⋆ |&lt; k do
9: if xfollow &gt; xdeviate then
10: eL⋆.insert(xfollow);
11: if C-iterator.current()=[0,1] then
12: referColumn++;
13: [i, j] ← C-iterator.next();
14: xfollow ← L[i,referColumn+j];
15: elsee
16: L⋆.insert(xdeviate);
17: i ← xdeviate.row();
18: C-iterator.insert([i, −referColumn]);
19: xdeviate ← L[i + 1, 0];
</listItem>
<bodyText confidence="0.985703857142857">
case of (2). This is because input list L2 does not
have constant slope in general. In an exact algo-
rithm, path(fMi−1, L) should return the descending
list L⋆ i−1 = mergeij=1 e�i−j,j: Unfortunately, we do
not know how to compute such a i-way merge with-
out introducing a logarithmic factor.
Our solution is to define path(fMi−1, L) in such a
</bodyText>
<subsectionHeader confidence="0.434597">
e
</subsectionHeader>
<bodyText confidence="0.955688523809524">
way that it computes a list Li−1 which is a permu-
tation of the correct solution L⋆
i−1. To do this, we
consider the “relative” path starting at x0 +yi−1 that
we need to follow in L in order to collect all the el-
ements of fMi−1 in the given order. We then apply
such a path starting at x0 + yi and return the list of
collected elements. Finally, we compute the output
list eL⋆ as the concatenation of all lists fMi up to the
first k elements.
It is not difficult to see that when L2 has constant
slope we ehave Mi = Mi for all i with 0 ≤ i &lt; k,
and list L⋆ is the exact solution to the CP prob-
lem. When L2 does not have a constant slope, list
eL⋆ might depart from the exact solution in two re-
spects: it might not be a descending list, because
of local variations in the ordering of the elements;
and it might not be a permutation of the exact so-
lution, because of local variations at the end of the
list. In the next section we evaluate the impact that
referColumn =0
</bodyText>
<figureCaption confidence="0.975080777777778">
Figure 1: A running example for Algorithm 1.
our heuristic solution has on the performance of a
real-world machine translation system.
Algorithm 1 implements the idea presented in (3).
The algorithm takes as input two descending lists
L1, L2 of length k and outputs the list eL⋆ which
approximates the desired solution. Element L[i, j]
denotes the combined value xi + yj, and is always
computed on demand.
</figureCaption>
<bodyText confidence="0.9991122">
We encode a relative path (mentioned above) as
a sequence of elements, called displacements, each
of the form [i, S]. Here i is the index of the next row,
and S represents the relative displacement needed to
reach the next column, to be summed to a variable
called referColumn denoting the index of the col-
umn of the first element of the path. The reason
why only the second coordinate is a relative value
is that we shift paths only horizontally (row indices
are preserved). The relative path is stored in a circu-
lar list C, with displacement [0, 1] marking the start-
ing point (paths are always shifted one element to
the right). When merging the list obtained through
the path for Mi−1 with segment e�i,0, as specified
in (3), we update C accordingly, so that the new rel-
ative path can be used at the next round for fMi. The
merge operator is implemented by the while cycle
at lines 8 to 19 of algorithm 1. The if statement at
line 9 tests whether the next step should follow the
relative path for fMi−1 stored in C (lines 10 to 14) or
</bodyText>
<figure confidence="0.991551946428571">
12
21
C)
a)
_ [0,1]
-iterator
=1 d) =2
21 18
15
21
18 15
16
14
14
16 13
[0,1]
[1,-1]
[0,1]
[1,-1]
21
18 15
=2
e)
f)
[0,1]
[1,-1]
[2,-2]
=2
21
12
18 15
16 13
14
9
[0,1]
[2,-2]
[1,-1]
14
16 13
9
18
b)
21
=1
9 6 3 0
18 15
16
16
7
5
[0,1]
0
298
score loss (%)
1 10 100 1000
beam size
</figure>
<figureCaption confidence="0.999066">
Figure 2: Search-score loss relative to standard CP.
Figure 3: Linear CP relative speed gain.
</figureCaption>
<figure confidence="0.964867130434783">
45
40
35
30
25
20
15
10
-5
5
0
Baseline score loss over CP
LCP score loss over CP
FCP score loss over CP
1 10 100 1000
beam size
speed gain (%)
25
20
15
10
5
0
</figure>
<bodyText confidence="0.724207375">
LCP speed gain over CP
LCP speed gain over FCP
else depart visiting an element from Qi,0 in the first
column of L (lines 16 to 19). In the latter case, we
update C with the new displacement (line 18), where
the function insert() inserts a new element before
the one currently pointed to. The function next() at
line 13 moves the iterator to the next element and
then returns its value.
A running example of algorithm 1 is reported in
Figure 1. The input lists are L1 = (12, 7, 5, 0),
L2 = (9, 6, 3, 0). Each of the picture in the sequence
represents the state of the algorithm when the test at
line 9 is executed. The value in the shaded cell in the
first column is xdeviate, while the value in the other
shaded cell is xfollow.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="conclusions">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999954306122449">
We implement Linear CP (LCP) on top of Cdec
(Dyer et al., 2010), a widely-used hierarchical MT
system that includes implementations of standard
CP and FCP algorithms. The experiments were ex-
ecuted on the NIST 2003 Chinese-English parallel
corpus. The training corpus contains 239k sentence
pairs. A binary translation grammar was extracted
using a suffix array rule extractor (Lopez, 2007).
The model was tuned using MERT (Och, 2003).
The algorithms are compared on the NIST-03 test
set, which contains 919 sentence pairs. The features
used are basic lexical features, word penalty and a
3-gram Language Model (Heafield, 2011).
Since we compare decoding algorithms on the
same search space, the accuracy comparison is done
in terms of search score. For each algorithm we
compute the average score of the best translation
found for the test sentences. In Figure 2 we plot
the score-loss relative to standard CP average score.
Note that the FCP loss is always &lt; 3%, and the LCP
loss is always &lt; 7%. The dotted line plots the loss
of a baseline linear time heuristic algorithm which
assumes that both input lists have constant slope,
and that scans L along parallel lines whose steep
is the ratio of the average slope of each input list.
The baseline greatly deteriorates the accuracy: this
shows that finding a reasonable linear time heuristic
algorithm is not trivial. We can assume a bounded
loss in accuracy, because for larger beam size all the
algorithms tend to converge to exhaustive search.
We found that these differences in search score
resulted in no significant variations in BLEU score
(e.g. with k = 30, CP reaches 32.2 while LCP 32.3).
The speed comparison is done in terms of algo-
rithm run-time. Figure 3 plots the relative speed gain
of LCP over standard CP and over FCP. Given the
log-scale used for the beam size k, the linear shape
of the speed gain over FCP (and CP) in Figure 3 em-
pirically confirms that LCP has a log(k) asymptotic
advantage over FCP and CP.
In addition to Chinese-English, we ran experi-
ments on translating English to French (from Eu-
roparl corpus (Koehn, 2005)), and find that the LCP
score-loss relative to CP is &lt; 9% while the speed
relative advantage of LCP over CP increases in aver-
age by 11.4% every time the beam size is multiplied
by 10 (e.g. with k = 1000 the speed advantage is
34.3%). These results confirm the bounded accu-
racy loss and log(k) speed advantage of LCP.
</bodyText>
<page confidence="0.997277">
299
</page>
<sectionHeader confidence="0.995893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999780333333333">
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL ’10: Proceedings of the ACL 2010 System
Demonstrations, Uppsala, Sweden.
Andrea Gesmundo and James Henderson. 2010. Faster
Cube Pruning. In IWSLT ’10: Proceedings of the 7th
International Workshop on Spoken Language Transla-
tion, Paris, France.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In WMT ’11: Proceedings of
the 6th Workshop on Statistical Machine Translation,
Edinburgh, Scotland, UK.
E. Horowitz and S. Sahni. 1983. Fundamentals of
data structures. Computer software engineering se-
ries. Computer Science Press.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT ’05: Proceedings of the 9th Interna-
tional Workshop on Parsing Technology, Vancouver,
British Columbia, Canada.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In ACL ’07: Proceedings of the 45th Confer-
ence of the Association for Computational Linguistics,
Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit, Phuket, Thailand.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In EMNLP-CoNLL ’07: Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putationalNatural Language Learning, Prague, Czech
Republic.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ’03: Pro-
ceedings of the 41st Conference of the Association for
Computational Linguistics, Sapporo, Japan.
Michael Pust and Kevin Knight. 2009. Faster MT decod-
ing through pervasive laziness. In NAACL ’09: Pro-
ceedings ofthe 10th Conference ofthe North American
Chapter ofthe Association for Computational Linguis-
tics, Boulder, CO, USA.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In ACL ’10: Proceedings
of the 48th Conference of the Association for Compu-
tational Linguistics, Uppsala, Sweden.
</reference>
<page confidence="0.997987">
300
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672875">
<title confidence="0.999802">Heuristic Cube Pruning in Linear Time</title>
<author confidence="0.999309">Andrea</author>
<affiliation confidence="0.988214">Department Computer University of Geneva</affiliation>
<email confidence="0.785637">andrea.gesmundo@unige.ch</email>
<author confidence="0.99039">Giorgio</author>
<affiliation confidence="0.996668">Department Information University of Padua</affiliation>
<email confidence="0.98525">satta@dei.unipd.it</email>
<author confidence="0.970144">James</author>
<affiliation confidence="0.994854333333333">Department Computer University of Geneva</affiliation>
<email confidence="0.974331">james.henderson@unige.ch</email>
<abstract confidence="0.997611666666667">We propose a novel heuristic algorithm for Cube Pruning running in linear time in the beam size. Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1022" citStr="Chiang, 2007" startWordPosition="149" endWordPosition="150">beam size. Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algor</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Hendra Setiawan</author>
<author>Ferhan Ture</author>
<author>Vladimir Eidelman</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL ’10: Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="12300" citStr="Dyer et al., 2010" startWordPosition="2377" endWordPosition="2380">t (line 18), where the function insert() inserts a new element before the one currently pointed to. The function next() at line 13 moves the iterator to the next element and then returns its value. A running example of algorithm 1 is reported in Figure 1. The input lists are L1 = (12, 7, 5, 0), L2 = (9, 6, 3, 0). Each of the picture in the sequence represents the state of the algorithm when the test at line 9 is executed. The value in the shaded cell in the first column is xdeviate, while the value in the other shaded cell is xfollow. 5 Experiments We implement Linear CP (LCP) on top of Cdec (Dyer et al., 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. The experiments were executed on the NIST 2003 Chinese-English parallel corpus. The training corpus contains 239k sentence pairs. A binary translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). The model was tuned using MERT (Och, 2003). The algorithms are compared on the NIST-03 test set, which contains 919 sentence pairs. The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011). Since we compare decoding algorithm</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Setiawan, Ture, Eidelman, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Hendra Setiawan, Ferhan Ture, Vladimir Eidelman, Phil Blunsom, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL ’10: Proceedings of the ACL 2010 System Demonstrations, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Gesmundo</author>
<author>James Henderson</author>
</authors>
<title>Faster Cube Pruning.</title>
<date>2010</date>
<booktitle>In IWSLT ’10: Proceedings of the 7th International Workshop on Spoken Language Translation,</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="1484" citStr="Gesmundo and Henderson (2010)" startWordPosition="224" endWordPosition="228">best solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and performance of a real-world machine translation system. 2 Preliminaries Let G = (x0, ... , xk−1) be a list over R, that is, an ordered sequence of real numbers, possibly with repetitions. We write |G |= k to denote the length of G. We say that G is descending if xi &gt; xj for every i, j with 0 &lt; i &lt; j &lt; k. Let G1 = (x0, ... , xk−1) and G2 = (y0, ... , yk′−1) be two descending lists </context>
<context position="3836" citStr="Gesmundo and Henderson, 2010" startWordPosition="685" endWordPosition="689"> shifts. 296 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 296–300, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Computation of the solution of the k-way merge problem takes time O(q log(k)), where q is the size of the output list. In case each input list has length k this becomes O(k2 log(k)), and by restricting the computation to the first k elements, as required by the CP problem, we can further reduce to O(k log(k)). This is the already known upper bound on the CP problem (Huang and Chiang, 2005; Gesmundo and Henderson, 2010). Unfortunately, there seems to be no way to achieve an asymptotically faster algorithm by exploiting the restriction that the input lists are all related by some shifts. Nonetheless, in the next sections we use the above ideas to develop a heuristic algorithm running in time linear in k. 3 Cube Pruning With Constant Slope Consider lists L1, L2 defined as in section 2. We say that L2 has constant slope if yi−1 − yi = 0 &gt; 0 for every i with 0 &lt; i &lt; k. Throughout this section we assume that L2 has constant slope, and we develop an (exact) linear time algorithm for solving the CP problem under th</context>
</contexts>
<marker>Gesmundo, Henderson, 2010</marker>
<rawString>Andrea Gesmundo and James Henderson. 2010. Faster Cube Pruning. In IWSLT ’10: Proceedings of the 7th International Workshop on Spoken Language Translation, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In WMT ’11: Proceedings of the 6th Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="12863" citStr="Heafield, 2011" startWordPosition="2464" endWordPosition="2465">t Linear CP (LCP) on top of Cdec (Dyer et al., 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. The experiments were executed on the NIST 2003 Chinese-English parallel corpus. The training corpus contains 239k sentence pairs. A binary translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). The model was tuned using MERT (Och, 2003). The algorithms are compared on the NIST-03 test set, which contains 919 sentence pairs. The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011). Since we compare decoding algorithms on the same search space, the accuracy comparison is done in terms of search score. For each algorithm we compute the average score of the best translation found for the test sentences. In Figure 2 we plot the score-loss relative to standard CP average score. Note that the FCP loss is always &lt; 3%, and the LCP loss is always &lt; 7%. The dotted line plots the loss of a baseline linear time heuristic algorithm which assumes that both input lists have constant slope, and that scans L along parallel lines whose steep is the ratio of the average slope of each inp</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In WMT ’11: Proceedings of the 6th Workshop on Statistical Machine Translation, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Horowitz</author>
<author>S Sahni</author>
</authors>
<title>Fundamentals of data structures. Computer software engineering series.</title>
<date>1983</date>
<publisher>Computer Science Press.</publisher>
<contexts>
<context position="2479" citStr="Horowitz and Sahni, 1983" startWordPosition="432" endWordPosition="435">umbers, possibly with repetitions. We write |G |= k to denote the length of G. We say that G is descending if xi &gt; xj for every i, j with 0 &lt; i &lt; j &lt; k. Let G1 = (x0, ... , xk−1) and G2 = (y0, ... , yk′−1) be two descending lists over R. We write G1 ® G2 to denote the descending list with elements xi + yj for every i, j with 0 &lt; i &lt; k and 0 &lt; j &lt; k′. In cube pruning (CP) we are given as input two descending lists G1, G2 over R with |G1 |= |G2 |= k, and we are asked to compute the descending list consisting of the first k elements of G1 ® G2. A problem related to CP is the k-way merge problem (Horowitz and Sahni, 1983). Given descending lists Gi for every i with 0 &lt; i &lt; k, we write mergek−1 i=0 Gi to denote the “merge” of all the lists Gi, that is, the descending list with all elements from the lists Gi, including repetitions. For A E R we define shift(G, A) = G ® (A). In words, shift(G, A) is the descending list whose elements are obtained by “shifting” the elements of G by A, preserving the order. Let G1, G2 be descending lists of length k, with G2 = (y0, . . . , yk−1). Then we can express the output of CP on G1, G2 as the list mergek−1 i=0 shift(G1, yi) (1) truncated after the first k elements. This show</context>
</contexts>
<marker>Horowitz, Sahni, 1983</marker>
<rawString>E. Horowitz and S. Sahni. 1983. Fundamentals of data structures. Computer software engineering series. Computer Science Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In IWPT ’05: Proceedings of the 9th International Workshop on Parsing Technology,</booktitle>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1104" citStr="Huang and Chiang, 2005" startWordPosition="161" endWordPosition="164">chine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and per</context>
<context position="3805" citStr="Huang and Chiang, 2005" startWordPosition="681" endWordPosition="684">related by k independent shifts. 296 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 296–300, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics Computation of the solution of the k-way merge problem takes time O(q log(k)), where q is the size of the output list. In case each input list has length k this becomes O(k2 log(k)), and by restricting the computation to the first k elements, as required by the CP problem, we can further reduce to O(k log(k)). This is the already known upper bound on the CP problem (Huang and Chiang, 2005; Gesmundo and Henderson, 2010). Unfortunately, there seems to be no way to achieve an asymptotically faster algorithm by exploiting the restriction that the input lists are all related by some shifts. Nonetheless, in the next sections we use the above ideas to develop a heuristic algorithm running in time linear in k. 3 Cube Pruning With Constant Slope Consider lists L1, L2 defined as in section 2. We say that L2 has constant slope if yi−1 − yi = 0 &gt; 0 for every i with 0 &lt; i &lt; k. Throughout this section we assume that L2 has constant slope, and we develop an (exact) linear time algorithm for </context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In IWPT ’05: Proceedings of the 9th International Workshop on Parsing Technology, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1046" citStr="Huang and Chiang, 2007" startWordPosition="151" endWordPosition="155">irically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in t</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In ACL ’07: Proceedings of the 45th Conference of the Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation</booktitle>
<location>Summit, Phuket, Thailand.</location>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL ’07: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12626" citStr="Lopez, 2007" startWordPosition="2426" endWordPosition="2427">re in the sequence represents the state of the algorithm when the test at line 9 is executed. The value in the shaded cell in the first column is xdeviate, while the value in the other shaded cell is xfollow. 5 Experiments We implement Linear CP (LCP) on top of Cdec (Dyer et al., 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. The experiments were executed on the NIST 2003 Chinese-English parallel corpus. The training corpus contains 239k sentence pairs. A binary translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). The model was tuned using MERT (Och, 2003). The algorithms are compared on the NIST-03 test set, which contains 919 sentence pairs. The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011). Since we compare decoding algorithms on the same search space, the accuracy comparison is done in terms of search score. For each algorithm we compute the average score of the best translation found for the test sentences. In Figure 2 we plot the score-loss relative to standard CP average score. Note that the FCP loss is always &lt; 3%, and the LCP loss is alway</context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>Adam Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In EMNLP-CoNLL ’07: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Conference of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="12670" citStr="Och, 2003" startWordPosition="2434" endWordPosition="2435"> algorithm when the test at line 9 is executed. The value in the shaded cell in the first column is xdeviate, while the value in the other shaded cell is xfollow. 5 Experiments We implement Linear CP (LCP) on top of Cdec (Dyer et al., 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. The experiments were executed on the NIST 2003 Chinese-English parallel corpus. The training corpus contains 239k sentence pairs. A binary translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). The model was tuned using MERT (Och, 2003). The algorithms are compared on the NIST-03 test set, which contains 919 sentence pairs. The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011). Since we compare decoding algorithms on the same search space, the accuracy comparison is done in terms of search score. For each algorithm we compute the average score of the best translation found for the test sentences. In Figure 2 we plot the score-loss relative to standard CP average score. Note that the FCP loss is always &lt; 3%, and the LCP loss is always &lt; 7%. The dotted line plots the loss of a </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Conference of the Association for Computational Linguistics, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pust</author>
<author>Kevin Knight</author>
</authors>
<title>Faster MT decoding through pervasive laziness.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings ofthe 10th Conference ofthe North American Chapter ofthe Association for Computational Linguistics,</booktitle>
<location>Boulder, CO, USA.</location>
<contexts>
<context position="1070" citStr="Pust and Knight, 2009" startWordPosition="156" endWordPosition="159"> in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate it</context>
</contexts>
<marker>Pust, Knight, 2009</marker>
<rawString>Michael Pust and Kevin Knight. 2009. Faster MT decoding through pervasive laziness. In NAACL ’09: Proceedings ofthe 10th Conference ofthe North American Chapter ofthe Association for Computational Linguistics, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Daniel Marcu</author>
</authors>
<title>Hierarchical search for word alignment.</title>
<date>2010</date>
<booktitle>In ACL ’10: Proceedings of the 48th Conference of the Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="1148" citStr="Riesa and Marcu, 2010" startWordPosition="167" endWordPosition="170">accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and performance of a real-world machine translation</context>
</contexts>
<marker>Riesa, Marcu, 2010</marker>
<rawString>Jason Riesa and Daniel Marcu. 2010. Hierarchical search for word alignment. In ACL ’10: Proceedings of the 48th Conference of the Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>