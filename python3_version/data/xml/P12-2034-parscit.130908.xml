<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035884">
<title confidence="0.991879">
Syntactic Stylometry for Deception Detection
</title>
<author confidence="0.999368">
Song Feng Ritwik Banerjee Yejin Choi
</author>
<affiliation confidence="0.9623115">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.950512">
Stony Brook, NY 11794-4400
</address>
<email confidence="0.995986">
songfeng, rbanerjee, ychoi@cs.stonybrook.edu
</email>
<sectionHeader confidence="0.983182" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999686">
Most previous studies in computerized de-
ception detection have relied only on shal-
low lexico-syntactic patterns. This pa-
per investigates syntactic stylometry for
deception detection, adding a somewhat
unconventional angle to prior literature.
Over four different datasets spanning from
the product review to the essay domain,
we demonstrate that features driven from
Context Free Grammar (CFG) parse trees
consistently improve the detection perfor-
mance over several baselines that are based
only on shallow lexico-syntactic features.
Our results improve the best published re-
sult on the hotel review data (Ott et al.,
2011) reaching 91.2% accuracy with 14%
error reduction.
</bodyText>
<sectionHeader confidence="0.996295" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954108108108">
Previous studies in computerized deception de-
tection have relied only on shallow lexico-
syntactic cues. Most are based on dictionary-
based word counting using LIWC (Pennebaker
et al., 2007) (e.g., Hancock et al. (2007), Vrij et
al. (2007)), while some recent ones explored the
use of machine learning techniques using sim-
ple lexico-syntactic patterns, such as n-grams
and part-of-speech (POS) tags (Mihalcea and
Strapparava (2009), Ott et al. (2011)). These
previous studies unveil interesting correlations
between certain lexical items or categories with
deception that may not be readily apparent to
human judges. For instance, the work of Ott
et al. (2011) in the hotel review domain results
in very insightful observations that deceptive re-
viewers tend to use verbs and personal pronouns
(e.g., “I&amp;quot;, “my&amp;quot;) more often, while truthful re-
viewers tend to use more of nouns, adjectives,
prepositions. In parallel to these shallow lexical
patterns, might there be deep syntactic struc-
tures that are lurking in deceptive writing?
This paper investigates syntactic stylometry
for deception detection, adding a somewhat un-
conventional angle to prior literature. Over four
different datasets spanning from the product re-
view domain to the essay domain, we find that
features driven from Context Free Grammar
(CFG) parse trees consistently improve the de-
tection performance over several baselines that
are based only on shallow lexico-syntactic fea-
tures. Our results improve the best published re-
sult on the hotel review data of Ott et al. (2011)
reaching 91.2% accuracy with 14% error reduc-
tion. We also achieve substantial improvement
over the essay data of Mihalcea and Strapparava
(2009), obtaining upto 85.0% accuracy.
</bodyText>
<sectionHeader confidence="0.988802" genericHeader="method">
2 Four Datasets
</sectionHeader>
<bodyText confidence="0.995160333333333">
To explore different types of deceptive writing,
we consider the following four datasets spanning
from the product review to the essay domain:
</bodyText>
<footnote confidence="0.643859166666667">
I. TripAdvisor—Gold: Introduced in Ott et
al. (2011), this dataset contains 400 truthful re-
views obtained from www.tripadviser.com and
400 deceptive reviews gathered using Amazon
Mechanical Turk, evenly distributed across 20
Chicago hotels.
</footnote>
<page confidence="0.947018">
171
</page>
<note confidence="0.8374685">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 171–175,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<table confidence="0.999458666666667">
TRIPADVISOR–GOLD TRIPADVISOR–HEURISTIC
DECEPTIVE TRUTHFUL DECEPTIVE TRUTHFUL
NP&amp;quot;PP —4 DT NNP NNP NNP S&amp;quot;ROOT —4 VP . NP&amp;quot;S —4 PRP VP&amp;quot;S —4 VBZ NP
SBAR&amp;quot;NP —4 S NP&amp;quot;NP —4 $ CD SBAR&amp;quot;S —4 WHADVP S NP&amp;quot;NP —4 NNS
NP&amp;quot;VP —4 NP SBAR PRN&amp;quot;NP —4 LRB NP RRB VP&amp;quot;S —4 VBD PP WHNP&amp;quot;SBAR —4 WDT
NP&amp;quot;NP —4 PRP$ NN NP&amp;quot;NP —4 NNS S&amp;quot;SBAR —4 NP VP NP&amp;quot;NP —4 NP PP PP
NP&amp;quot;S —4 DT NNP NNP NNP NP&amp;quot;S —4 NN S&amp;quot;ROOT —4 PP NP VP . NP&amp;quot;S —4 EX
VP&amp;quot;S —4 VBG PP NP&amp;quot;PP —4 DT NNP VP&amp;quot;S —4 VBD S NX&amp;quot;NX —4 JJ NN
NP&amp;quot;PP —4 PRP$ NN NP&amp;quot;PP —4 CD NNS NP&amp;quot;S —4 NP CC NP NP&amp;quot;NP —4 NP PP
VP&amp;quot;S —4 MD ADVP VP NP&amp;quot;NP —4 NP PRN NP&amp;quot;S —4 PRP$ NN VP&amp;quot;S —4 VBZ RB NP
VP&amp;quot;S —4 TO VP PRN&amp;quot;NP —4 LRB PP RRB NP&amp;quot;PP —4 DT NNP PP&amp;quot;NP —4 IN NP
ADJP&amp;quot;NP —4 RBS JJ NP&amp;quot;NP —4 CD NNS NP&amp;quot;PP —4 PRP$ NN PP&amp;quot;ADJP —4 TO NP
</table>
<tableCaption confidence="0.996103">
Table 1: Most discriminative rewrite rules (r): hotel review datasets
</tableCaption>
<figureCaption confidence="0.972293">
Figure 1: Parsed trees
</figureCaption>
<listItem confidence="0.995634764705883">
II. TripAdvisor—Heuristic: This dataset
contains 400 truthful and 400 deceptive reviews
harvested from www.tripadviser.com, based
on fake review detection heuristics introduced
in Feng et al. (2012).1
III. Yelp: This dataset is our own creation
using www.yelp.com. We collect 400 filtered re-
views and 400 displayed reviews for 35 Italian
restaurants with average ratings in the range of
[3.5, 4.0]. Class labels are based on the meta
data, which tells us whether each review is fil-
tered by Yelp’s automated review filtering sys-
tem or not. We expect that filtered reviews
roughly correspond to deceptive reviews, and
displayed reviews to truthful ones, but not with-
out considerable noise. We only collect 5-star
reviews to avoid unwanted noise from varying
</listItem>
<bodyText confidence="0.901485181818181">
&apos;Specifically, using the notation of Feng et al. (2012),
we use data created by STRATEGY-distd) heuristic, with
HS, S as deceptive and H&apos;S, T as truthful.
degree of sentiment.
IV. Essays: Introduced in Mihalcea and
Strapparava (2009), this corpus contains truth-
ful and deceptive essays collected using Amazon
Mechanic Turk for the following three topics:
“Abortion” (100 essays per class), “Best Friend”
(98 essays per class), and “Death Penalty” (98
essays per class).
</bodyText>
<sectionHeader confidence="0.97319" genericHeader="method">
3 Feature Encoding
</sectionHeader>
<bodyText confidence="0.998793555555555">
Words Previous work has shown that bag-of-
words are effective in detecting domain-specific
deception (Ott et al., 2011; Mihalcea and Strap-
parava, 2009). We consider unigram, bigram,
and the union of the two as features.
Shallow Syntax As has been used in many
previous studies in stylometry (e.g., Argamon-
Engelson et al. (1998), Zhao and Zobel (2007)),
we utilize part-of-speech (POS) tags to encode
shallow syntactic information. Note that Ott
et al. (2011) found that even though POS tags
are effective in detecting fake product reviews,
they are not as effective as words. Therefore, we
strengthen POS features with unigram features.
Deep syntax We experiment with four differ-
ent encodings of production rules based on the
Probabilistic Context Free Grammar (PCFG)
parse trees as follows:
</bodyText>
<listItem confidence="0.951623142857143">
• r: unlexicalized production rules (i.e., all
production rules except for those with ter-
minal nodes), e.g., NP2 --+ NP3 SBAR.
• r*: lexicalized production rules (i.e., all
production rules), e.g., PRP --+ “you&amp;quot;.
• r: unlexicalized production rules combined
with the grandparent node, e.g., NP2 - VP
</listItem>
<page confidence="0.986282">
172
</page>
<table confidence="0.9995648125">
TRIPADVISOR YELP ABORT ESSAY DEATH
GOLD HEUR BSTFR
unigram 88.4 74.4 59.9 70.0 77.0 67.4
words bigram 85.8 71.5 60.7 71.5 79.5 55.5
uni + bigram 89.6 73.8 60.1 72.0 81.5 65.5
pos(n=1) + unigram 87.4 74.0 62.0 70.0 80.0 66.5
shallow syntax pos(n=2) + unigram 88.6 74.6 59.0 67.0 82.0 66.5
+words pos(n=3) + unigram 88.6 74.6 59.3 67.0 82.0 66.5
r 78.5 65.3 56.9 62 67.5 55.5
deep syntax r� 74.8 65.3 56.5 58.5 65.5 56.0
r* 89.4 74.0 64.0 70.1 77.5 66.0
r* 90.4 75 63.5 71.0 78 67.5
r + unigram 89.0 74.3 62.3 76.5 82.0 69.0
deep syntax r� + unigram 88.5 74.3 62.5 77.0 81.5 70.5
+words r* + unigram 90.3 75.4 64.3 74.0 85.0 71.5
P* + unigram 91.2 76.6 62.1 76.0 84.5 71.0
</table>
<tableCaption confidence="0.805406">
Table 2: Deception Detection Accuracy (%).
</tableCaption>
<table confidence="0.534249">
1 --+ NP3 SBAR.
</table>
<listItem confidence="0.990019">
• r*: lexicalized production rules (i.e., all
production rules) combined with the grand-
parent node, e.g., PRP&amp;quot;NP 4 --+ “you&amp;quot;.
</listItem>
<sectionHeader confidence="0.987477" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999884625">
For all classification tasks, we use SVM classi-
fier, 80% of data for training and 20% for test-
ing, with 5-fold cross validation.2 All features
are encoded as tf-idf values. We use Berkeley
PCFG parser (Petrov and Klein, 2007) to parse
sentences. Table 2 presents the classification
performance using various features across four
different datasets introduced earlier.3
</bodyText>
<subsectionHeader confidence="0.988268">
4.1 TripAdvisor–Gold
</subsectionHeader>
<bodyText confidence="0.999685666666667">
We first discuss the results for the TripAdvisor-
Gold dataset shown in Table 2. As reported in
Ott et al. (2011), bag-of-words features achieve
surprisingly high performance, reaching upto
89.6% accuracy. Deep syntactic features, en-
coded as r* slightly improves this performance,
achieving 90.4% accuracy. When these syntactic
features are combined with unigram features, we
attain the best performance of 91.2% accuracy,
</bodyText>
<footnote confidence="0.901882">
2 W use LIBLINEAR (Fan et al., 2008) with L2-
regulization, parameter optimized over the 80% training
data (3 folds for training, 1 fold for testing).
3Numbers in italic are classification results reported
in Ott et al. (2011) and Mihalcea and Strapparava (2009).
</footnote>
<bodyText confidence="0.999318136363636">
yielding 14% error reduction over the word-only
features.
Given the power of word-based features, one
might wonder, whether the PCFG driven fea-
tures are being useful only due to their lexi-
cal production rules. To address such doubts,
we include experiments with unlexicalized rules,
r and r. These features achieve 78.5% and
74.8% accuracy respectively, which are signifi-
cantly higher than that of a random baseline
(-50.0%), confirming statistical differences in
deep syntactic structures. See Section 4.4 for
concrete exemplary rules.
Another question one might have is whether
the performance gain of PCFG features are
mostly from local sequences of POS tags, indi-
rectly encoded in the production rules. Compar-
ing the performance of [shallow syntax+words]
and [deep syntax+words] in Table 2, we find sta-
tistical evidence that deep syntax based features
offer information that are not available in simple
POS sequences.
</bodyText>
<subsectionHeader confidence="0.898572">
4.2 TripAdvisor–Heuristic &amp; Yelp
</subsectionHeader>
<bodyText confidence="0.999961833333333">
The performance is generally lower than that of
the previous dataset, due to the noisy nature
of these datasets. Nevertheless, we find similar
trends as those seen in the TripAdvisor-Gold
dataset, with respect to the relative performance
differences across different approaches. The sig-
</bodyText>
<page confidence="0.997445">
173
</page>
<table confidence="0.999532">
TRIPADVISOR–GOLD TRIPADVISOR–HEUR
DECEP TRUTH DECEP TRUTH
VP PRN VP PRN
SBAR QP WHADVP NX
WHADVP S SBAR WHNP
ADVP PRT WHADJP ADJP
CONJP UCP INTJ WHPP
</table>
<tableCaption confidence="0.984935">
Table 3: Most discriminative phrasal tags in PCFG
parse trees: TripAdvisor data.
</tableCaption>
<bodyText confidence="0.999945444444444">
nificance of these results comes from the fact
that these two datasets consists of real (fake)
reviews in the wild, rather than manufactured
ones that might invite unwanted signals that
can unexpectedly help with classification accu-
racy. In sum, these results indicate the exis-
tence of the statistical signals hidden in deep
syntax even in real product reviews with noisy
gold standards.
</bodyText>
<subsectionHeader confidence="0.98149">
4.3 Essay
</subsectionHeader>
<bodyText confidence="0.999995">
Finally in Table 2, the last dataset Essay con-
firms the similar trends again, that the deep syn-
tactic features consistently improve the perfor-
mance over several baselines based only on shal-
low lexico-syntactic features. The final results,
reaching accuracy as high as 85%, substantially
outperform what has been previously reported
in Mihalcea and Strapparava (2009). How ro-
bust are the syntactic cues in the cross topic set-
ting? Table 4 compares the results of Mihalcea
and Strapparava (2009) and ours, demonstrat-
ing that syntactic features achieve substantially
and surprisingly more robust results.
</bodyText>
<subsectionHeader confidence="0.992588">
4.4 Discriminative Production Rules
</subsectionHeader>
<bodyText confidence="0.999915909090909">
To give more concrete insights, we provide
10 most discriminative unlexicalized production
rules (augmented with the grand parent node)
for each class in Table 1. We order the rules
based on the feature weights assigned by LIB-
LINEAR classifier. Notice that the two produc-
tion rules in bolds — [SBAR&amp;quot;NP —* S] and [NP
&amp;quot;VP —* NP SBAR] — are parts of the parse tree
shown in Figure 1, whose sentence is taken from
an actual fake review. Table 3 shows the most
discriminative phrasal tags in the PCFG parse
</bodyText>
<table confidence="0.99657525">
training: A &amp; B A &amp; D B &amp; D
testing: DeathPen BestFrn Abortion
M&amp;S 2009 58.7 58.7 62.0
r* 66.8 70.9 69.0
</table>
<tableCaption confidence="0.966765">
Table 4: Cross topic deception detection accuracy:
Essay data
</tableCaption>
<bodyText confidence="0.99818125">
trees for each class. Interestingly, we find more
frequent use of VP, SBAR (clause introduced
by subordinating conjunction), and WHADVP
in deceptive reviews than truthful reviews.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999978913043478">
Much of the previous work for detecting de-
ceptive product reviews focused on related, but
slightly different problems, e.g., detecting dupli-
cate reviews or review spams (e.g., Jindal and
Liu (2008), Lim et al. (2010), Mukherjee et al.
(2011), Jindal et al. (2010)) due to notable dif-
ficulty in obtaining gold standard labels.4 The
Yelp data we explored in this work shares a sim-
ilar spirit in that gold standard labels are har-
vested from existing meta data, which are not
guaranteed to align well with true hidden la-
bels as to deceptive v.s. truthful reviews. Two
previous work obtained more precise gold stan-
dard labels by hiring Amazon turkers to write
deceptive articles (e.g., Mihalcea and Strappa-
rava (2009), Ott et al. (2011)), both of which
have been examined in this study with respect
to their syntactic characteristics. Although we
are not aware of any prior work that dealt
with syntactic cues in deceptive writing directly,
prior work on hedge detection (e.g., Greene and
Resnik (2009), Li et al. (2010)) relates to our
findings.
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999965">
We investigated syntactic stylometry for decep-
tion detection, adding a somewhat unconven-
tional angle to previous studies. Experimental
results consistently find statistical evidence of
deep syntactic patterns that are helpful in dis-
criminating deceptive writing.
</bodyText>
<footnote confidence="0.9996205">
4It is not possible for a human judge to tell with full
confidence whether a given review is a fake or not.
</footnote>
<page confidence="0.996535">
174
</page>
<sectionHeader confidence="0.978878" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999751202380953">
S. Argamon-Engelson, M. Koppel, and G. Avneri.
1998. Style-based text categorization: What
newspaper am i reading. In Proc. of the AAAI
Workshop on Text Categorization, pages 1–4.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–
1874.
S. Feng, L. Xing, Gogar A., and Y. Choi. 2012.
Distributional footprints of deceptive product re-
views. In Proceedings of the 2012 International
AAAI Conference on WebBlogs and Social Media,
June.
S. Greene and P. Resnik. 2009. More than
words: Syntactic packaging and implicit senti-
ment. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 503–511. Asso-
ciation for Computational Linguistics.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Wood-
worth. 2007. On lying and being lied to: A lin-
guistic analysis of deception in computer-mediated
communication. Discourse Processes, 45(1):1–23.
Nitin Jindal and Bing Liu. 2008. Opinion spam
and analysis. In Proceedings of the international
conference on Web search and web data mining,
WSDM &apos;08, pages 219–230, New York, NY, USA.
ACM.
Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010.
Finding unusual review patterns using unexpected
rules. In Proceedings of the 19th ACM Confer-
ence on Information and Knowledge Management,
pages 1549–1552.
X. Li, J. Shen, X. Gao, and X. Wang. 2010. Ex-
ploiting rich features for detecting hedges and
their scope. In Proceedings of the Fourteenth
Conference on Computational Natural Language
Learning—Shared Task, pages 78–83. Association
for Computational Linguistics.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing
Liu, and Hady Wirawan Lauw. 2010. Detecting
product review spammers using rating behaviors.
In Proceedings of the 19th ACM international con-
ference on Information and knowledge manage-
ment, CIKM &apos;10, pages 939–948, New York, NY,
USA. ACM.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages
309–312. Association for Computational Linguis-
tics.
Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie S.
Glance, and Nitin Jindal. 2011. Detecting group
review spam. In Proceedings of the 20th Interna-
tional Conference on World Wide Web (Compan-
ion Volume), pages 93–94.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, pages 309–319, Portland, Ore-
gon, USA, June. Association for Computational
Linguistics.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development
and psychometric properties of liwc2007. Austin,
TX, LIWC. Net.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404–411.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007.
Cues to deception and ability to detect lies as a
function of police interview styles. Law and hu-
man behavior, 31(5):499–518.
Ying Zhao and Justin Zobel. 2007. Searching with
style: authorship attribution in classic literature.
In Proceedings of the thirtieth Australasian confer-
ence on Computer science - Volume 62, ACSC &apos;07,
pages 59–68, Darlinghurst, Australia, Australia.
Australian Computer Society, Inc.
</reference>
<page confidence="0.99872">
175
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.791043">
<title confidence="0.999861">Syntactic Stylometry for Deception Detection</title>
<author confidence="0.999021">Song Feng Ritwik Banerjee Yejin Choi</author>
<affiliation confidence="0.997929">Department of Computer</affiliation>
<author confidence="0.930854">Stony Brook Stony Brook</author>
<author confidence="0.930854">NY</author>
<email confidence="0.999467">songfeng,rbanerjee,ychoi@cs.stonybrook.edu</email>
<abstract confidence="0.993735111111111">Most previous studies in computerized deception detection have relied only on shallow lexico-syntactic patterns. This paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. Over four different datasets spanning from the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon-Engelson</author>
<author>M Koppel</author>
<author>G Avneri</author>
</authors>
<title>Style-based text categorization: What newspaper am i reading.</title>
<date>1998</date>
<booktitle>In Proc. of the AAAI Workshop on Text Categorization,</booktitle>
<pages>1--4</pages>
<marker>Argamon-Engelson, Koppel, Avneri, 1998</marker>
<rawString>S. Argamon-Engelson, M. Koppel, and G. Avneri. 1998. Style-based text categorization: What newspaper am i reading. In Proc. of the AAAI Workshop on Text Categorization, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1874</pages>
<contexts>
<context position="8108" citStr="Fan et al., 2008" startWordPosition="1319" endWordPosition="1322">arse sentences. Table 2 presents the classification performance using various features across four different datasets introduced earlier.3 4.1 TripAdvisor–Gold We first discuss the results for the TripAdvisorGold dataset shown in Table 2. As reported in Ott et al. (2011), bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, encoded as r* slightly improves this performance, achieving 90.4% accuracy. When these syntactic features are combined with unigram features, we attain the best performance of 91.2% accuracy, 2 W use LIBLINEAR (Fan et al., 2008) with L2- regulization, parameter optimized over the 80% training data (3 folds for training, 1 fold for testing). 3Numbers in italic are classification results reported in Ott et al. (2011) and Mihalcea and Strapparava (2009). yielding 14% error reduction over the word-only features. Given the power of word-based features, one might wonder, whether the PCFG driven features are being useful only due to their lexical production rules. To address such doubts, we include experiments with unlexicalized rules, r and r. These features achieve 78.5% and 74.8% accuracy respectively, which are signific</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871– 1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feng</author>
<author>L Xing</author>
<author>A Gogar</author>
<author>Y Choi</author>
</authors>
<title>Distributional footprints of deceptive product reviews.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International AAAI Conference on WebBlogs and Social</booktitle>
<location>Media,</location>
<contexts>
<context position="4240" citStr="Feng et al. (2012)" startWordPosition="684" endWordPosition="687">P&amp;quot;S —4 VBG PP NP&amp;quot;PP —4 DT NNP VP&amp;quot;S —4 VBD S NX&amp;quot;NX —4 JJ NN NP&amp;quot;PP —4 PRP$ NN NP&amp;quot;PP —4 CD NNS NP&amp;quot;S —4 NP CC NP NP&amp;quot;NP —4 NP PP VP&amp;quot;S —4 MD ADVP VP NP&amp;quot;NP —4 NP PRN NP&amp;quot;S —4 PRP$ NN VP&amp;quot;S —4 VBZ RB NP VP&amp;quot;S —4 TO VP PRN&amp;quot;NP —4 LRB PP RRB NP&amp;quot;PP —4 DT NNP PP&amp;quot;NP —4 IN NP ADJP&amp;quot;NP —4 RBS JJ NP&amp;quot;NP —4 CD NNS NP&amp;quot;PP —4 PRP$ NN PP&amp;quot;ADJP —4 TO NP Table 1: Most discriminative rewrite rules (r): hotel review datasets Figure 1: Parsed trees II. TripAdvisor—Heuristic: This dataset contains 400 truthful and 400 deceptive reviews harvested from www.tripadviser.com, based on fake review detection heuristics introduced in Feng et al. (2012).1 III. Yelp: This dataset is our own creation using www.yelp.com. We collect 400 filtered reviews and 400 displayed reviews for 35 Italian restaurants with average ratings in the range of [3.5, 4.0]. Class labels are based on the meta data, which tells us whether each review is filtered by Yelp’s automated review filtering system or not. We expect that filtered reviews roughly correspond to deceptive reviews, and displayed reviews to truthful ones, but not without considerable noise. We only collect 5-star reviews to avoid unwanted noise from varying &apos;Specifically, using the notation of Feng </context>
</contexts>
<marker>Feng, Xing, Gogar, Choi, 2012</marker>
<rawString>S. Feng, L. Xing, Gogar A., and Y. Choi. 2012. Distributional footprints of deceptive product reviews. In Proceedings of the 2012 International AAAI Conference on WebBlogs and Social Media, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greene</author>
<author>P Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Greene, Resnik, 2009</marker>
<rawString>S. Greene and P. Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 503–511. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hancock</author>
<author>L E Curry</author>
<author>S Goorha</author>
<author>M Woodworth</author>
</authors>
<title>On lying and being lied to: A linguistic analysis of deception in computer-mediated communication.</title>
<date>2007</date>
<booktitle>Discourse Processes,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="1122" citStr="Hancock et al. (2007)" startWordPosition="157" endWordPosition="160">m the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction. 1 Introduction Previous studies in computerized deception detection have relied only on shallow lexicosyntactic cues. Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007) (e.g., Hancock et al. (2007), Vrij et al. (2007)), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009), Ott et al. (2011)). These previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of Ott et al. (2011) in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns (e.g., “I&amp;quot;, “my&amp;quot;) more oft</context>
</contexts>
<marker>Hancock, Curry, Goorha, Woodworth, 2007</marker>
<rawString>J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth. 2007. On lying and being lied to: A linguistic analysis of deception in computer-mediated communication. Discourse Processes, 45(1):1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the international conference on Web search and web data mining, WSDM &apos;08,</booktitle>
<pages>219--230</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11909" citStr="Jindal and Liu (2008)" startWordPosition="1925" endWordPosition="1928"> shows the most discriminative phrasal tags in the PCFG parse training: A &amp; B A &amp; D B &amp; D testing: DeathPen BestFrn Abortion M&amp;S 2009 58.7 58.7 62.0 r* 66.8 70.9 69.0 Table 4: Cross topic deception detection accuracy: Essay data trees for each class. Interestingly, we find more frequent use of VP, SBAR (clause introduced by subordinating conjunction), and WHADVP in deceptive reviews than truthful reviews. 5 Related Work Much of the previous work for detecting deceptive product reviews focused on related, but slightly different problems, e.g., detecting duplicate reviews or review spams (e.g., Jindal and Liu (2008), Lim et al. (2010), Mukherjee et al. (2011), Jindal et al. (2010)) due to notable difficulty in obtaining gold standard labels.4 The Yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011)), both of which have been examined in this study with respect to </context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of the international conference on Web search and web data mining, WSDM &apos;08, pages 219–230, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
<author>Ee-Peng Lim</author>
</authors>
<title>Finding unusual review patterns using unexpected rules.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>1549--1552</pages>
<contexts>
<context position="11975" citStr="Jindal et al. (2010)" startWordPosition="1937" endWordPosition="1940">ing: A &amp; B A &amp; D B &amp; D testing: DeathPen BestFrn Abortion M&amp;S 2009 58.7 58.7 62.0 r* 66.8 70.9 69.0 Table 4: Cross topic deception detection accuracy: Essay data trees for each class. Interestingly, we find more frequent use of VP, SBAR (clause introduced by subordinating conjunction), and WHADVP in deceptive reviews than truthful reviews. 5 Related Work Much of the previous work for detecting deceptive product reviews focused on related, but slightly different problems, e.g., detecting duplicate reviews or review spams (e.g., Jindal and Liu (2008), Lim et al. (2010), Mukherjee et al. (2011), Jindal et al. (2010)) due to notable difficulty in obtaining gold standard labels.4 The Yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011)), both of which have been examined in this study with respect to their syntactic characteristics. Although we are not aware of any </context>
</contexts>
<marker>Jindal, Liu, Lim, 2010</marker>
<rawString>Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010. Finding unusual review patterns using unexpected rules. In Proceedings of the 19th ACM Conference on Information and Knowledge Management, pages 1549–1552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>J Shen</author>
<author>X Gao</author>
<author>X Wang</author>
</authors>
<title>Exploiting rich features for detecting hedges and their scope.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning—Shared Task,</booktitle>
<pages>78--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Li, Shen, Gao, Wang, 2010</marker>
<rawString>X. Li, J. Shen, X. Gao, and X. Wang. 2010. Exploiting rich features for detecting hedges and their scope. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning—Shared Task, pages 78–83. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ee-Peng Lim</author>
<author>Viet-An Nguyen</author>
<author>Nitin Jindal</author>
<author>Bing Liu</author>
<author>Hady Wirawan Lauw</author>
</authors>
<title>Detecting product review spammers using rating behaviors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM &apos;10,</booktitle>
<pages>939--948</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11928" citStr="Lim et al. (2010)" startWordPosition="1929" endWordPosition="1932">inative phrasal tags in the PCFG parse training: A &amp; B A &amp; D B &amp; D testing: DeathPen BestFrn Abortion M&amp;S 2009 58.7 58.7 62.0 r* 66.8 70.9 69.0 Table 4: Cross topic deception detection accuracy: Essay data trees for each class. Interestingly, we find more frequent use of VP, SBAR (clause introduced by subordinating conjunction), and WHADVP in deceptive reviews than truthful reviews. 5 Related Work Much of the previous work for detecting deceptive product reviews focused on related, but slightly different problems, e.g., detecting duplicate reviews or review spams (e.g., Jindal and Liu (2008), Lim et al. (2010), Mukherjee et al. (2011), Jindal et al. (2010)) due to notable difficulty in obtaining gold standard labels.4 The Yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011)), both of which have been examined in this study with respect to their syntactic cha</context>
</contexts>
<marker>Lim, Nguyen, Jindal, Liu, Lauw, 2010</marker>
<rawString>Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu, and Hady Wirawan Lauw. 2010. Detecting product review spammers using rating behaviors. In Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM &apos;10, pages 939–948, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Strapparava</author>
</authors>
<title>The lie detector: Explorations in the automatic recognition of deceptive language.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference Short Papers,</booktitle>
<pages>309--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1334" citStr="Mihalcea and Strapparava (2009)" startWordPosition="188" endWordPosition="191">ased only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction. 1 Introduction Previous studies in computerized deception detection have relied only on shallow lexicosyntactic cues. Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007) (e.g., Hancock et al. (2007), Vrij et al. (2007)), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009), Ott et al. (2011)). These previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of Ott et al. (2011) in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns (e.g., “I&amp;quot;, “my&amp;quot;) more often, while truthful reviewers tend to use more of nouns, adjectives, prepositions. In parallel to these shallow lexical patterns, might there be deep syntactic structures that are lurking in deceptive writing? Thi</context>
<context position="2588" citStr="Mihalcea and Strapparava (2009)" startWordPosition="384" endWordPosition="387">ntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. Over four different datasets spanning from the product review domain to the essay domain, we find that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data of Ott et al. (2011) reaching 91.2% accuracy with 14% error reduction. We also achieve substantial improvement over the essay data of Mihalcea and Strapparava (2009), obtaining upto 85.0% accuracy. 2 Four Datasets To explore different types of deceptive writing, we consider the following four datasets spanning from the product review to the essay domain: I. TripAdvisor—Gold: Introduced in Ott et al. (2011), this dataset contains 400 truthful reviews obtained from www.tripadviser.com and 400 deceptive reviews gathered using Amazon Mechanical Turk, evenly distributed across 20 Chicago hotels. 171 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 171–175, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association</context>
<context position="5031" citStr="Mihalcea and Strapparava (2009)" startWordPosition="813" endWordPosition="816">rage ratings in the range of [3.5, 4.0]. Class labels are based on the meta data, which tells us whether each review is filtered by Yelp’s automated review filtering system or not. We expect that filtered reviews roughly correspond to deceptive reviews, and displayed reviews to truthful ones, but not without considerable noise. We only collect 5-star reviews to avoid unwanted noise from varying &apos;Specifically, using the notation of Feng et al. (2012), we use data created by STRATEGY-distd) heuristic, with HS, S as deceptive and H&apos;S, T as truthful. degree of sentiment. IV. Essays: Introduced in Mihalcea and Strapparava (2009), this corpus contains truthful and deceptive essays collected using Amazon Mechanic Turk for the following three topics: “Abortion” (100 essays per class), “Best Friend” (98 essays per class), and “Death Penalty” (98 essays per class). 3 Feature Encoding Words Previous work has shown that bag-ofwords are effective in detecting domain-specific deception (Ott et al., 2011; Mihalcea and Strapparava, 2009). We consider unigram, bigram, and the union of the two as features. Shallow Syntax As has been used in many previous studies in stylometry (e.g., ArgamonEngelson et al. (1998), Zhao and Zobel (</context>
<context position="8334" citStr="Mihalcea and Strapparava (2009)" startWordPosition="1354" endWordPosition="1357"> dataset shown in Table 2. As reported in Ott et al. (2011), bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, encoded as r* slightly improves this performance, achieving 90.4% accuracy. When these syntactic features are combined with unigram features, we attain the best performance of 91.2% accuracy, 2 W use LIBLINEAR (Fan et al., 2008) with L2- regulization, parameter optimized over the 80% training data (3 folds for training, 1 fold for testing). 3Numbers in italic are classification results reported in Ott et al. (2011) and Mihalcea and Strapparava (2009). yielding 14% error reduction over the word-only features. Given the power of word-based features, one might wonder, whether the PCFG driven features are being useful only due to their lexical production rules. To address such doubts, we include experiments with unlexicalized rules, r and r. These features achieve 78.5% and 74.8% accuracy respectively, which are significantly higher than that of a random baseline (-50.0%), confirming statistical differences in deep syntactic structures. See Section 4.4 for concrete exemplary rules. Another question one might have is whether the performance ga</context>
<context position="10576" citStr="Mihalcea and Strapparava (2009)" startWordPosition="1702" endWordPosition="1705">actured ones that might invite unwanted signals that can unexpectedly help with classification accuracy. In sum, these results indicate the existence of the statistical signals hidden in deep syntax even in real product reviews with noisy gold standards. 4.3 Essay Finally in Table 2, the last dataset Essay confirms the similar trends again, that the deep syntactic features consistently improve the performance over several baselines based only on shallow lexico-syntactic features. The final results, reaching accuracy as high as 85%, substantially outperform what has been previously reported in Mihalcea and Strapparava (2009). How robust are the syntactic cues in the cross topic setting? Table 4 compares the results of Mihalcea and Strapparava (2009) and ours, demonstrating that syntactic features achieve substantially and surprisingly more robust results. 4.4 Discriminative Production Rules To give more concrete insights, we provide 10 most discriminative unlexicalized production rules (augmented with the grand parent node) for each class in Table 1. We order the rules based on the feature weights assigned by LIBLINEAR classifier. Notice that the two production rules in bolds — [SBAR&amp;quot;NP —* S] and [NP &amp;quot;VP —* NP SB</context>
<context position="12424" citStr="Mihalcea and Strapparava (2009)" startWordPosition="2013" endWordPosition="2017">ted, but slightly different problems, e.g., detecting duplicate reviews or review spams (e.g., Jindal and Liu (2008), Lim et al. (2010), Mukherjee et al. (2011), Jindal et al. (2010)) due to notable difficulty in obtaining gold standard labels.4 The Yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011)), both of which have been examined in this study with respect to their syntactic characteristics. Although we are not aware of any prior work that dealt with syntactic cues in deceptive writing directly, prior work on hedge detection (e.g., Greene and Resnik (2009), Li et al. (2010)) relates to our findings. 6 Conclusion We investigated syntactic stylometry for deception detection, adding a somewhat unconventional angle to previous studies. Experimental results consistently find statistical evidence of deep syntactic patterns that are helpful in discriminating deceptive wri</context>
</contexts>
<marker>Mihalcea, Strapparava, 2009</marker>
<rawString>R. Mihalcea and C. Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 309–312. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
<author>Junhui Wang</author>
<author>Natalie S Glance</author>
<author>Nitin Jindal</author>
</authors>
<title>Detecting group review spam.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web (Companion Volume),</booktitle>
<pages>93--94</pages>
<contexts>
<context position="11953" citStr="Mukherjee et al. (2011)" startWordPosition="1933" endWordPosition="1936">s in the PCFG parse training: A &amp; B A &amp; D B &amp; D testing: DeathPen BestFrn Abortion M&amp;S 2009 58.7 58.7 62.0 r* 66.8 70.9 69.0 Table 4: Cross topic deception detection accuracy: Essay data trees for each class. Interestingly, we find more frequent use of VP, SBAR (clause introduced by subordinating conjunction), and WHADVP in deceptive reviews than truthful reviews. 5 Related Work Much of the previous work for detecting deceptive product reviews focused on related, but slightly different problems, e.g., detecting duplicate reviews or review spams (e.g., Jindal and Liu (2008), Lim et al. (2010), Mukherjee et al. (2011), Jindal et al. (2010)) due to notable difficulty in obtaining gold standard labels.4 The Yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011)), both of which have been examined in this study with respect to their syntactic characteristics. Although we</context>
</contexts>
<marker>Mukherjee, Liu, Wang, Glance, Jindal, 2011</marker>
<rawString>Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie S. Glance, and Nitin Jindal. 2011. Detecting group review spam. In Proceedings of the 20th International Conference on World Wide Web (Companion Volume), pages 93–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myle Ott</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Jeffrey T Hancock</author>
</authors>
<title>Finding deceptive opinion spam by any stretch of the imagination.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>309--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="840" citStr="Ott et al., 2011" startWordPosition="114" endWordPosition="117">revious studies in computerized deception detection have relied only on shallow lexico-syntactic patterns. This paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. Over four different datasets spanning from the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction. 1 Introduction Previous studies in computerized deception detection have relied only on shallow lexicosyntactic cues. Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007) (e.g., Hancock et al. (2007), Vrij et al. (2007)), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009), Ott et al. (2011)). These previous studies unveil interesting correlations between certain lexical items</context>
<context position="2443" citStr="Ott et al. (2011)" startWordPosition="362" endWordPosition="365">hallow lexical patterns, might there be deep syntactic structures that are lurking in deceptive writing? This paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. Over four different datasets spanning from the product review domain to the essay domain, we find that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data of Ott et al. (2011) reaching 91.2% accuracy with 14% error reduction. We also achieve substantial improvement over the essay data of Mihalcea and Strapparava (2009), obtaining upto 85.0% accuracy. 2 Four Datasets To explore different types of deceptive writing, we consider the following four datasets spanning from the product review to the essay domain: I. TripAdvisor—Gold: Introduced in Ott et al. (2011), this dataset contains 400 truthful reviews obtained from www.tripadviser.com and 400 deceptive reviews gathered using Amazon Mechanical Turk, evenly distributed across 20 Chicago hotels. 171 Proceedings of the</context>
<context position="5404" citStr="Ott et al., 2011" startWordPosition="870" endWordPosition="873">m varying &apos;Specifically, using the notation of Feng et al. (2012), we use data created by STRATEGY-distd) heuristic, with HS, S as deceptive and H&apos;S, T as truthful. degree of sentiment. IV. Essays: Introduced in Mihalcea and Strapparava (2009), this corpus contains truthful and deceptive essays collected using Amazon Mechanic Turk for the following three topics: “Abortion” (100 essays per class), “Best Friend” (98 essays per class), and “Death Penalty” (98 essays per class). 3 Feature Encoding Words Previous work has shown that bag-ofwords are effective in detecting domain-specific deception (Ott et al., 2011; Mihalcea and Strapparava, 2009). We consider unigram, bigram, and the union of the two as features. Shallow Syntax As has been used in many previous studies in stylometry (e.g., ArgamonEngelson et al. (1998), Zhao and Zobel (2007)), we utilize part-of-speech (POS) tags to encode shallow syntactic information. Note that Ott et al. (2011) found that even though POS tags are effective in detecting fake product reviews, they are not as effective as words. Therefore, we strengthen POS features with unigram features. Deep syntax We experiment with four different encodings of production rules based</context>
<context position="7762" citStr="Ott et al. (2011)" startWordPosition="1270" endWordPosition="1273">s (i.e., all production rules) combined with the grandparent node, e.g., PRP&amp;quot;NP 4 --+ “you&amp;quot;. 4 Experimental Results For all classification tasks, we use SVM classifier, 80% of data for training and 20% for testing, with 5-fold cross validation.2 All features are encoded as tf-idf values. We use Berkeley PCFG parser (Petrov and Klein, 2007) to parse sentences. Table 2 presents the classification performance using various features across four different datasets introduced earlier.3 4.1 TripAdvisor–Gold We first discuss the results for the TripAdvisorGold dataset shown in Table 2. As reported in Ott et al. (2011), bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, encoded as r* slightly improves this performance, achieving 90.4% accuracy. When these syntactic features are combined with unigram features, we attain the best performance of 91.2% accuracy, 2 W use LIBLINEAR (Fan et al., 2008) with L2- regulization, parameter optimized over the 80% training data (3 folds for training, 1 fold for testing). 3Numbers in italic are classification results reported in Ott et al. (2011) and Mihalcea and Strapparava (2009). yielding 14% error reducti</context>
<context position="12443" citStr="Ott et al. (2011)" startWordPosition="2018" endWordPosition="2021">ems, e.g., detecting duplicate reviews or review spams (e.g., Jindal and Liu (2008), Lim et al. (2010), Mukherjee et al. (2011), Jindal et al. (2010)) due to notable difficulty in obtaining gold standard labels.4 The Yelp data we explored in this work shares a similar spirit in that gold standard labels are harvested from existing meta data, which are not guaranteed to align well with true hidden labels as to deceptive v.s. truthful reviews. Two previous work obtained more precise gold standard labels by hiring Amazon turkers to write deceptive articles (e.g., Mihalcea and Strapparava (2009), Ott et al. (2011)), both of which have been examined in this study with respect to their syntactic characteristics. Although we are not aware of any prior work that dealt with syntactic cues in deceptive writing directly, prior work on hedge detection (e.g., Greene and Resnik (2009), Li et al. (2010)) relates to our findings. 6 Conclusion We investigated syntactic stylometry for deception detection, adding a somewhat unconventional angle to previous studies. Experimental results consistently find statistical evidence of deep syntactic patterns that are helpful in discriminating deceptive writing. 4It is not po</context>
</contexts>
<marker>Ott, Choi, Cardie, Hancock, 2011</marker>
<rawString>Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Hancock. 2011. Finding deceptive opinion spam by any stretch of the imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 309–319, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>C K Chung</author>
<author>M Ireland</author>
<author>A Gonzales</author>
<author>R J Booth</author>
</authors>
<title>The development and psychometric properties of liwc2007.</title>
<date>2007</date>
<location>Austin, TX, LIWC. Net.</location>
<contexts>
<context position="1093" citStr="Pennebaker et al., 2007" startWordPosition="152" endWordPosition="155"> different datasets spanning from the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction. 1 Introduction Previous studies in computerized deception detection have relied only on shallow lexicosyntactic cues. Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007) (e.g., Hancock et al. (2007), Vrij et al. (2007)), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009), Ott et al. (2011)). These previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of Ott et al. (2011) in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronou</context>
</contexts>
<marker>Pennebaker, Chung, Ireland, Gonzales, Booth, 2007</marker>
<rawString>J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales, and R.J. Booth. 2007. The development and psychometric properties of liwc2007. Austin, TX, LIWC. Net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>404--411</pages>
<contexts>
<context position="7486" citStr="Petrov and Klein, 2007" startWordPosition="1229" endWordPosition="1232">r + unigram 89.0 74.3 62.3 76.5 82.0 69.0 deep syntax r� + unigram 88.5 74.3 62.5 77.0 81.5 70.5 +words r* + unigram 90.3 75.4 64.3 74.0 85.0 71.5 P* + unigram 91.2 76.6 62.1 76.0 84.5 71.0 Table 2: Deception Detection Accuracy (%). 1 --+ NP3 SBAR. • r*: lexicalized production rules (i.e., all production rules) combined with the grandparent node, e.g., PRP&amp;quot;NP 4 --+ “you&amp;quot;. 4 Experimental Results For all classification tasks, we use SVM classifier, 80% of data for training and 20% for testing, with 5-fold cross validation.2 All features are encoded as tf-idf values. We use Berkeley PCFG parser (Petrov and Klein, 2007) to parse sentences. Table 2 presents the classification performance using various features across four different datasets introduced earlier.3 4.1 TripAdvisor–Gold We first discuss the results for the TripAdvisorGold dataset shown in Table 2. As reported in Ott et al. (2011), bag-of-words features achieve surprisingly high performance, reaching upto 89.6% accuracy. Deep syntactic features, encoded as r* slightly improves this performance, achieving 90.4% accuracy. When these syntactic features are combined with unigram features, we attain the best performance of 91.2% accuracy, 2 W use LIBLIN</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL HLT 2007, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vrij</author>
<author>S Mann</author>
<author>S Kristen</author>
<author>R P Fisher</author>
</authors>
<title>Cues to deception and ability to detect lies as a function of police interview styles. Law and human behavior,</title>
<date>2007</date>
<pages>31--5</pages>
<contexts>
<context position="1142" citStr="Vrij et al. (2007)" startWordPosition="161" endWordPosition="164"> the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (Ott et al., 2011) reaching 91.2% accuracy with 14% error reduction. 1 Introduction Previous studies in computerized deception detection have relied only on shallow lexicosyntactic cues. Most are based on dictionarybased word counting using LIWC (Pennebaker et al., 2007) (e.g., Hancock et al. (2007), Vrij et al. (2007)), while some recent ones explored the use of machine learning techniques using simple lexico-syntactic patterns, such as n-grams and part-of-speech (POS) tags (Mihalcea and Strapparava (2009), Ott et al. (2011)). These previous studies unveil interesting correlations between certain lexical items or categories with deception that may not be readily apparent to human judges. For instance, the work of Ott et al. (2011) in the hotel review domain results in very insightful observations that deceptive reviewers tend to use verbs and personal pronouns (e.g., “I&amp;quot;, “my&amp;quot;) more often, while truthful r</context>
</contexts>
<marker>Vrij, Mann, Kristen, Fisher, 2007</marker>
<rawString>A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007. Cues to deception and ability to detect lies as a function of police interview styles. Law and human behavior, 31(5):499–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>Justin Zobel</author>
</authors>
<title>Searching with style: authorship attribution in classic literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the thirtieth Australasian conference on Computer science - Volume 62, ACSC &apos;07,</booktitle>
<pages>59--68</pages>
<publisher>Australian Computer Society, Inc.</publisher>
<location>Darlinghurst, Australia, Australia.</location>
<contexts>
<context position="5636" citStr="Zhao and Zobel (2007)" startWordPosition="909" endWordPosition="912">rapparava (2009), this corpus contains truthful and deceptive essays collected using Amazon Mechanic Turk for the following three topics: “Abortion” (100 essays per class), “Best Friend” (98 essays per class), and “Death Penalty” (98 essays per class). 3 Feature Encoding Words Previous work has shown that bag-ofwords are effective in detecting domain-specific deception (Ott et al., 2011; Mihalcea and Strapparava, 2009). We consider unigram, bigram, and the union of the two as features. Shallow Syntax As has been used in many previous studies in stylometry (e.g., ArgamonEngelson et al. (1998), Zhao and Zobel (2007)), we utilize part-of-speech (POS) tags to encode shallow syntactic information. Note that Ott et al. (2011) found that even though POS tags are effective in detecting fake product reviews, they are not as effective as words. Therefore, we strengthen POS features with unigram features. Deep syntax We experiment with four different encodings of production rules based on the Probabilistic Context Free Grammar (PCFG) parse trees as follows: • r: unlexicalized production rules (i.e., all production rules except for those with terminal nodes), e.g., NP2 --+ NP3 SBAR. • r*: lexicalized production ru</context>
</contexts>
<marker>Zhao, Zobel, 2007</marker>
<rawString>Ying Zhao and Justin Zobel. 2007. Searching with style: authorship attribution in classic literature. In Proceedings of the thirtieth Australasian conference on Computer science - Volume 62, ACSC &apos;07, pages 59–68, Darlinghurst, Australia, Australia. Australian Computer Society, Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>