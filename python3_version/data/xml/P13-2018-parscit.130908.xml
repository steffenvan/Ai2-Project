<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000203">
<title confidence="0.996492">
An Empirical Examination of Challenges in Chinese Parsing
</title>
<author confidence="0.999169">
Jonathan K. Kummerfeld† Daniel Tse‡ James R. Curran‡ Dan Klein†
</author>
<affiliation confidence="0.882147">
†Computer Science Division ‡School of Information Technology
University of California, Berkeley University of Sydney
Berkeley, CA 94720, USA Sydney, NSW 2006, Australia
</affiliation>
<email confidence="0.997186">
{jkk,klein}@cs.berkeley.edu {dtse6695,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.994969" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871133333333">
Aspects of Chinese syntax result in a dis-
tinctive mix of parsing challenges. How-
ever, the contribution of individual sources
of error to overall difficulty is not well un-
derstood. We conduct a comprehensive
automatic analysis of error types made by
Chinese parsers, covering a broad range of
error types for large sets of sentences, en-
abling the first empirical ranking of Chi-
nese error types by their performance im-
pact. We also investigate which error types
are resolved by using gold part-of-speech
tags, showing that improving Chinese tag-
ging only addresses certain error types,
leaving substantial outstanding challenges.
</bodyText>
<sectionHeader confidence="0.998128" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968291666667">
A decade of Chinese parsing research, enabled
by the Penn Chinese Treebank (PCTB; Xue et al.,
2005), has seen Chinese parsing performance im-
prove from 76.7 F1 (Bikel and Chiang, 2000) to
84.1 F1 (Qian and Liu, 2012). While recent ad-
vances have focused on understanding and reduc-
ing the errors that occur in segmentation and part-
of-speech tagging (Qian and Liu, 2012; Jiang et al.,
2009; Forst and Fang, 2009), a range of substantial
issues remain that are purely syntactic.
Early work by Levy and Manning (2003) pre-
sented modifications to a parser motivated by a
manual investigation of parsing errors. They noted
substantial differences between Chinese and En-
glish parsing, attributing some of the differences to
treebank annotation decisions and others to mean-
ingful differences in syntax. Based on this analysis
they considered how to modify their parser to cap-
ture the information necessary to model the syn-
tax within the PCTB. However, their manual ana-
lysis was limited in scope, covering only part of
the parser output, and was unable to characterize
the relative impact of the issues they uncovered.
This paper presents a more comprehensive ana-
lysis of errors in Chinese parsing, building on the
technique presented in Kummerfeld et al. (2012),
which characterized the error behavior of English
parsers by quantifying how often they make er-
rors such as PP attachment and coordination scope.
To accommodate error classes that are absent in
English, we augment the system to recognize
Chinese-specific parse errors.1 We use the modi-
fied system to show the relative impact of different
error types across a range of Chinese parsers.
To understand the impact of tagging errors on
different error types, we performed a part-of-
speech ablation experiment, in which particular
confusions are introduced in isolation. By analyz-
ing the distribution of errors in the system output
with and without gold part-of-speech tags, we are
able to isolate and quantify the error types that can
be resolved by improvements in tagging accuracy.
Our analysis shows that improvements in tag-
ging accuracy can only address a subset of the chal-
lenges of Chinese syntax. Further improvement in
Chinese parsing performance will require research
addressing other challenges, in particular, deter-
mining coordination scope.
</bodyText>
<sectionHeader confidence="0.981123" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999923">
The closest previous work is the detailed manual
analysis performed by Levy and Manning (2003).
While their focus was on issues faced by their fac-
tored PCFG parser (Klein and Manning, 2003b),
the error types they identified are general issues
presented by Chinese syntax in the PCTB. They
presented several Chinese error types that are rare
or absent in English, including noun/verb ambigu-
ity, NP-internal structure and coordination ambi-
guity due to pro-drop, suggesting that closing the
English-Chinese parsing gap demands techniques
</bodyText>
<footnote confidence="0.9885175">
1The system described in this paper is available from
http://code.google.com/p/berkeley-parser-analyser/
</footnote>
<note confidence="0.850779">
98
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98–103,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999851166666667">
beyond those currently used for English. How-
ever, as noted in their final section, their manual
analysis of parse errors in 100 sentences only cov-
ered a portion of a single parser’s output, limiting
the conclusions they could reach regarding the dis-
tribution of errors in Chinese parsing.
</bodyText>
<subsectionHeader confidence="0.998759">
2.1 Automatic Error Analysis
</subsectionHeader>
<bodyText confidence="0.999969">
Our analysis builds on Kummerfeld et al.
(2012), which presented a system that automati-
cally classifies English parse errors using a two
stage process. First, the system finds the shortest
path from the system output to the gold annota-
tions, where each step in the path is a tree transfor-
mation, fixing at least one bracket error. Second,
each transformation step is classified into one of
several error types.
When directly applied to Chinese parser output,
the system placed over 27% of the errors in the
catch-all ‘Other’ type. Many of these errors clearly
fall into one of a small set of error types, motivat-
ing an adaptation to Chinese syntax.
</bodyText>
<sectionHeader confidence="0.69509" genericHeader="method">
3 Adapting error analysis to Chinese
</sectionHeader>
<bodyText confidence="0.999975318181818">
To adapt the Kummerfeld et al. (2012) system to
Chinese, we developed a new version of the second
stage of the system, which assigns an error cate-
gory to each tree transformation step.
To characterize the errors the original system
placed in the ‘Other’ category, we looked through
one hundred sentences, identifying error types
generated by Chinese syntax that the existing sys-
tem did not account for. With these observations
we were able to implement new rules to catch the
previously missed cases, leading to the set shown
in Table 1. To ensure the accuracy of our classifica-
tions, we alternated between refining the classifica-
tion code and looking at affected classifications to
identify issues. We also periodically changed the
sentences from the development set we manually
checked, to avoid over-fitting.
Where necessary, we also expanded the infor-
mation available during classification. For exam-
ple, we use the structure of the final gold standard
tree when classifying errors that are a byproduct of
sense disambiguation errors.
</bodyText>
<sectionHeader confidence="0.948229" genericHeader="method">
4 Chinese parsing errors
</sectionHeader>
<bodyText confidence="0.7902385">
Table 1 presents the errors made by the Berkeley
parser. Below we describe the error types that are
</bodyText>
<table confidence="0.977388777777778">
Error Type Brackets % of total
NP-internal* 6019 22.70%
Coordination 2781 10.49%
Verb taking wrong args* 2310 8.71%
Unary 2262 8.53%
Modifier Attachment 1900 7.17%
One Word Span 1560 5.88%
Different label 1418 5.35%
Unary A-over-A 1208 4.56%
Wrong sense/bad attach* 1018 3.84%
Noun boundary error* 685 2.58%
VP Attachment 626 2.36%
Clause Attachment 542 2.04%
PP Attachment 514 1.94%
Split Verb Compound* 232 0.88%
Scope error* 143 0.54%
NP Attachment 109 0.41%
Other 3186 12.02%
</table>
<tableCaption confidence="0.999001">
Table 1: Errors made when parsing Chinese. Values are the
</tableCaption>
<bodyText confidence="0.98415964516129">
number of bracket errors attributed to that error type. The
values shown are for the Berkeley parser, evaluated on the
development set. * indicates error types that were added or
substantially changed as part of this work.
either new in this analysis, have had their definition
altered, or have an interesting distribution.2
In all of our results we follow Kummerfeld et al.
(2012), presenting the number of bracket errors
(missing or extra) attributed to each error type.
Bracket counts are more informative than a direct
count of each error type, because the impact on
EVALB F-score varies between errors, e.g. a sin-
gle attachment error can cause 20 bracket errors,
while a unary error causes only one.
NP-internal. (Figure 1a). Unlike the Penn
Treebank (Marcus et al., 1993), the PCTB anno-
tates some NP-internal structure. We assign this
error type when a transformation involves words
whose parts of speech in the gold tree are one of:
CC, CD, DEG, ETC, JJ, NN, NR, NT and OD.
We investigated the errors that fall into the NP-
internal category and found that 49% of the errors
involved the creation or deletion of a single pre-
termianl phrasal bracket. These errors arise when
a parser proposes a tree in which POS tags (for in-
stance, JJ or NN) occur as siblings of phrasal tags
(such as NP), a configuration used by the PCTB
bracketing guidelines to indicate complementation
as opposed to adjunction (Xue et al., 2005).
2For an explanation of the English error types, see Kum-
merfeld et al. (2012).
</bodyText>
<equation confidence="0.8613635">
99
N
N
NP
</equation>
<bodyText confidence="0.999962764705882">
Verb taking wrong args. (Figure 1b). This
error type arises when a verb (e.g. M reverse)
is hypothesized to take an incorrect argument
(wry ({ Bush instead of tthft position). Note that
this also covers some of the errors that Kummer-
feld et al. (2012) classified as NP Attachment,
changing the distribution for that type.
Unary. For mis-application of unary rules we
separate out instances in which the two brackets in
the production have the the same label (A-over-A).
This cases is created when traces are eliminated, a
standard step in evaluation. More than a third of
unary errors made by the Berkeley parser are of the
A-over-A type. This can be attributed to two fac-
tors: (i) the PCTB annotates non-local dependen-
cies using traces, and (ii) Chinese syntax generates
more traces than English syntax (Guo et al., 2007).
However, for parsers that do not return traces they
are a benign error.
Modifier attachment. (Figure 1c). Incorrect
modifier scope caused by modifier phrase attach-
ment level. This is less frequent in Chinese than
in English: while English VP modifiers occur in
pre- and post-verbal positions, Chinese only al-
lows pre-verbal modification.
Wrong sense/bad attach. (Figure 1d). This ap-
plies when the head word of a phrase receives the
wrong POS, leading to an attachment error. This
error type is common in Chinese because of POS
fluidity, e.g. the well-known Chinese verb/noun
ambiguity often causes mis-attachments that are
classified as this error type.
In Figure 1d, the word J�k5� invest has
both noun and verb senses. While the gold
standard interpretation is the relative clause
firms that Macau invests in, the parser returned an
NP interpretation Macau investment firms.
Noun boundary error. In this error type, a span
is moved to a position where the POS tags of its
new siblings all belong to the list of NP-internal
structure tags which we identified above, reflecting
the inclusion of additional material into an NP.
Split verb compound. The PCTB annota-
tions recognize several Chinese verb compound-
ing strategies, such as the serial verb construc-
tion (9j 11J iR plan [and] build) and the resulta-
tive construction ( 3,A cook [until] done), which
join a bare verb to another lexical item. We in-
troduce an error type specific to Chinese, in which
such verb compounds are split, with the two halves
of the compound placed in different phrases.
</bodyText>
<equation confidence="0.643821807692308">
g or
N
NP
教
练a) Na-NNter)aa sNNuctNre NPNPNP
N
N
ros r
er
PNN (a)N -inter al st ucture er
VP
N
PNP Moposit otDNPDEGnN ambushVVy
ADVP is the VPAD PQPQPVP(c) Mod fier a ta hme t ambi uity
ScIPV 投rror. ThNP 澳 are cNPNP PN NP
转
rev
s
eCP IPVPV NPDECNP(b
)Verb taking wron argu entsV VP Chinese paQPQPg. The left
CP
NP is
.
conf sion (d) e se
parsi g.The
ig re 1: eft P om nent error t pes n Chinese
</equation>
<bodyText confidence="0.91904152173913">
thesi gold s. structure; tr e t i e rig the t is th par er hypo
new cope er or. hes are cases in whic a
difier pan ust be adde to more losely bi d a mo
hrase (A VP, ADJP, and PP)
t.This error type is rare in Chi- PP att chmen es ,a s adjunct
PP are pre-ver al It does oc -c r near coordin
ated VP s, w here a mbi uity ri es about which
of co e the conjuncts he PPhas s
ment o er.Wheth r his part cular c se is PPattach
er or oor ination is ebatabl ; we fol ow Kumm
feld et al.( 012)and la el t PP attachmen .
nes -English co pa ison 4.1Chi
d rectly co pa e e It ror i anal difficult sis to
nese nd English parsing be aus results or hi
es in he cla sifi at on method, of subs an ial hang
n t eeba k nno ations. nd ifferences i
us sec ion, the set of As escr bd e previo
or Chin se is very di rro
- c tegores onsidered f
fernt ies to for the English. st of cate Even
for some of the categories that were not substan-
tially changed, errors may be classified differently
because of cross-over between categories between
</bodyText>
<table confidence="0.923813666666667">
100
NP Verb Mod. 1-Word Diff Wrong Noun VP Clause PP
System Fl Int. Coord Args Unary Attach Span Label Sense Edge Attach Attach Attach Other
</table>
<tableCaption confidence="0.9321175">
Table 2: Error breakdown for the development set of PCTB 6. The area filled in for each bar indicates the average number of
bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in
</tableCaption>
<table confidence="0.527919666666667">
the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with
two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel),
the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P).
Best 1.54 1.25 1.01 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87
Berk-G 86.8
Berk-2 81.8
Berk-1 81.1
ZPAR 78.1
Bikel 76.1
Stan-F 76.0
Stan-P 70.0
Worst 3.94 1.75 1.73 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11
</table>
<bodyText confidence="0.991327235294118">
two categories (e.g. between Verb taking wrong
args and NP Attachment).
Differences in treebank annotations also present
a challenge for cross-language error comparison.
The most common error type in Chinese, NP-
internal structure, is rare in the results of Kummer-
feld et al. (2012), but the datasets are not compara-
ble because the PTB has very limited NP-internal
structure annotated. Further characterization of the
impact of annotation differences on errors is be-
yond the scope of this paper.
Three conclusions that can be made are that (i)
coordination is a major issue in both languages,
(ii) PP attachment is a much greater problem in
English, and (iii) a higher frequency of trace-
generating syntax in Chinese compared to English
poses substantial challenges.
</bodyText>
<sectionHeader confidence="0.982303" genericHeader="method">
5 Cross-parser analysis
</sectionHeader>
<bodyText confidence="0.996651515151515">
The previous section described the error types
and their distribution for a single Chinese parser.
Here we confirm that these are general trends, by
showing that the same pattern is observed for sev-
eral different parsers on the PCTB 6 dev set.3
We include results for a transition-based parser
(ZPAR; Zhang and Clark, 2009), a split-merge
PCFG parser (Petrov et al., 2006; Petrov and Klein,
2007; Petrov, 2010), a lexicalized parser (Bikel
and Chiang, 2000), and a factored PCFG and de-
pendency parser (Levy and Manning, 2003; Klein
and Manning, 2003a,b). 4
Comparing the two Stanford parsers in Table 2,
the factored model provides clear improvements
3We use the standard data split suggested by the PCTB 6
file manifest. As a result, our results differ from those previ-
ously reported on other splits. All analysis is on the dev set,
to avoid revealing specific information about the test set.
4These parsers represent a variety of parsing methods,
though exclude some recently developed parsers that are not
publicly available (Qian and Liu, 2012; Xiong et al., 2005).
on sense disambiguation, but performs slightly
worse on coordination.
The Berkeley product parser we include uses
only two grammars because we found, in contrast
to the English results (Petrov, 2010), that further
grammars provided limited benefits. Comparing
the performance with the standard Berkeley parser
it seems that the diversity in the grammars only as-
sists certain error types, with most of the improve-
ment occurring in four of the categories, while
there is no improvement, or a slight decrease, in
five categories.
</bodyText>
<sectionHeader confidence="0.853092" genericHeader="method">
6 Tagging Error Impact
</sectionHeader>
<bodyText confidence="0.999860315789474">
The challenge of accurate POS tagging in Chi-
nese has been a major part of several recent papers
(Qian and Liu, 2012; Jiang et al., 2009; Forst and
Fang, 2009). The Berk-G row of Table 2 shows
the performance of the Berkeley parser when given
gold POS tags.5 While the F1 improvement is un-
surprising, for the first time we can clearly show
that the gains are only in a subset of the error types.
In particular, tagging improvement will not help
for two of the most significant challenges: coordi-
nation scope errors, and verb argument selection.
To see which tagging confusions contribute to
which error reductions, we adapt the POS ablation
approach of Tse and Curran (2012). We consider
the POS tag pairs shown in Table 3. To isolate the
effects of each confusion we start from the gold
tags and introduce the output of the Stanford tag-
ger whenever it returns one of the two tags being
considered.6 We then feed these “semi-gold” tags
</bodyText>
<footnote confidence="0.968296">
5We used the Berkeley parser as it was the best of the
parsers we considered. Note that the Berkeley parser occa-
sionally prunes all of the parses that use the gold POS tags,
and so returns the best available alternative. This leads to a
POS accuracy of 99.35%, which is still well above the parser’s
standard POS accuracy of 93.66%.
6We introduce errors to gold tags, rather than removing er-
</footnote>
<page confidence="0.570838">
101
</page>
<table confidence="0.9994152">
Confused tags Errors 0 Fl
VV NN 1055 -2.72
DEC DEG 526 -1.72
JJ NN 297 -0.57
NR NN 320 -0.05
</table>
<tableCaption confidence="0.913803">
Table 3: The most frequently confused POS tag pairs. Each
0 Fl is relative to Berk-G.
</tableCaption>
<bodyText confidence="0.99986835">
to the Berkeley parser, and run the fine-grained er-
ror analysis on its output.
VV/NN. This confusion has been consistently
shown to be a major contributor to parsing errors
(Levy and Manning, 2003; Tse and Curran, 2012;
Qian and Liu, 2012), and we find a drop of over 2.7
F1 when the output of the tagger is introduced. We
found that while most error types have contribu-
tions from a range of POS confusions, verb/noun
confusion was responsible for virtually all of the
noun boundary errors corrected by using gold tags.
DEG/DEC. This confusion between the rela-
tivizer and subordinator senses of the particle 的
de is the primary source of improvements on mod-
ifier attachment when using gold tags.
NR/NN and JJ/NN. Despite their frequency,
these confusions have little effect on parsing per-
formance. Even within the NP-internal error type
their impact is limited, and almost all of the errors
do not change the logical form.
</bodyText>
<sectionHeader confidence="0.99844" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999996692307692">
We have quantified the relative impacts of a
comprehensive set of error types in Chinese pars-
ing. Our analysis has also shown that while im-
provements in Chinese POS tagging can make a
substantial difference for some error types, it will
not address two high-frequency error types: in-
correct verb argument attachment and coordina-
tion scope. The frequency of these two error types
is also unimproved by the use of products of la-
tent variable grammars. These observations sug-
gest that resolving the core challenges of Chinese
parsing will require new developments that suit the
distinctive properties of Chinese syntax.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988979">
We extend our thanks to Yue Zhang for helping
us train new ZPAR models. We would also like
to thank the anonymous reviewers for their help-
ful suggestions. This research was supported by
a General Sir John Monash Fellowship to the first
rors from automatic tags, isolating the effect of a single con-
fusion by eliminating interaction between tagging decisions.
author, the Capital Markets CRC under ARC Dis-
covery grant DP1097291, and the NSF under grant
0643742.
</bodyText>
<sectionHeader confidence="0.995966" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996232613636364">
Daniel M. Bikel and David Chiang. 2000. Two
Statistical Parsing Models Applied to the Chi-
nese Treebank. In Proceedings of the Second
Chinese Language Processing Workshop, pages
1–6. Hong Kong, China.
Martin Forst and Ji Fang. 2009. TBL-improved
non-deterministic segmentation and POS tag-
ging for a Chinese parser. In Proceedings of the
12th Conference of the European Chapter of the
ACL, pages 264–272. Athens, Greece.
Yuqing Guo, Haifeng Wang, and Josef van Gen-
abith. 2007. Recovering Non-Local Dependen-
cies for Chinese. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 257–266. Prague, Czech Republic.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009.
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging
– A Case Study. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
volume 1, pages 522–530. Suntec, Singapore.
Dan Klein and Christopher D. Manning. 2003a.
Accurate Unlexicalized Parsing. In Proceedings
of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 423–430.
Sapporo,Japan.
Dan Klein and Christopher D. Manning. 2003b.
Fast Exact Inference with a Factored Model for
Natural Language Parsing. In Advances in Neu-
ral Information Processing Systems 15, pages
3–10. MIT Press, Cambridge, MA.
Jonathan K. Kummerfeld, David Hall, James R.
Curran, and Dan Klein. 2012. Parser Show-
down at the Wall Street Corral: An Empirical
Investigation of Error Types in Parser Output.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 1048–1059. Jeju Island, South
Korea.
</reference>
<page confidence="0.521086">
102
</page>
<reference confidence="0.995072833333334">
Roger Levy and Christopher Manning. 2003. Is
it harder to parse Chinese, or the Chinese Tree-
bank? In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguis-
tics, pages 439–446. Sapporo, Japan.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313–330.
Slav Petrov. 2010. Products of Random Latent
Variable Grammars. In Human Language Tech-
nologies: The 2010 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 19–27. Los
Angeles, California.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning Accurate, Com-
pact, and Interpretable Tree Annotation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and the 44th Annual
Meeting of the Association for Computational
Linguistics, pages 433–440. Sydney, Australia.
Slav Petrov and Dan Klein. 2007. Improved In-
ference for Unlexicalized Parsing. In Human
Language Technologies 2007: The Conference
of the North American Chapter of the Associ-
ation for Computational Linguistics; Proceed-
ings of the Main Conference, pages 404–411.
Rochester, New York, USA.
Xian Qian and Yang Liu. 2012. Joint Chinese
word segmentation, POS tagging and parsing.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language
Learning, pages 501–511. Jeju Island, Korea.
Daniel Tse and James R. Curran. 2012. The
Challenges of Parsing Chinese with Combina-
tory Categorial Grammar. In Proceedings of the
2012 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics: Human Language Technologies, pages
295–304. Montreal, Canada.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun
Lin, and Yueliang Qian. 2005. Parsing the Penn
Chinese Treebank with semantic knowledge. In
Proceedings of the Second international joint
conference on Natural Language Processing,
pages 70–81. Jeju Island, Korea.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and
Martha Palmer. 2005. The Penn Chinese
TreeBank: Phrase structure annotation of a
large corpus. Natural Language Engineering,
11(2):207–238.
Yue Zhang and Stephen Clark. 2009. Transition-
Based Parsing of the Chinese Treebank using a
Global Discriminative Model. In Proceedings
of the 11th International Conference on Parsing
Technologies (IWPT’09), pages 162–171. Paris,
France.
</reference>
<page confidence="0.968304">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975040">
<title confidence="0.999968">An Empirical Examination of Challenges in Chinese Parsing</title>
<author confidence="0.997449">K R</author>
<affiliation confidence="0.9986715">Science Division of Information Technology University of California, Berkeley University of Sydney</affiliation>
<address confidence="0.998272">Berkeley, CA 94720, USA Sydney, NSW 2006, Australia</address>
<abstract confidence="0.9988470625">Aspects of Chinese syntax result in a distinctive mix of parsing challenges. However, the contribution of individual sources of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>David Chiang</author>
</authors>
<title>Two Statistical Parsing Models Applied to the Chinese Treebank.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Chinese Language Processing Workshop,</booktitle>
<pages>1--6</pages>
<publisher>Hong Kong,</publisher>
<contexts>
<context position="1185" citStr="Bikel and Chiang, 2000" startWordPosition="171" endWordPosition="174">sive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modifications to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in</context>
<context position="14349" citStr="Bikel and Chiang, 2000" startWordPosition="2410" endWordPosition="2413">eater problem in English, and (iii) a higher frequency of tracegenerating syntax in Chinese compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers that are not publicly available (Qian and Liu, 2012; Xiong et</context>
</contexts>
<marker>Bikel, Chiang, 2000</marker>
<rawString>Daniel M. Bikel and David Chiang. 2000. Two Statistical Parsing Models Applied to the Chinese Treebank. In Proceedings of the Second Chinese Language Processing Workshop, pages 1–6. Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
<author>Ji Fang</author>
</authors>
<title>TBL-improved non-deterministic segmentation and POS tagging for a Chinese parser.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL,</booktitle>
<pages>264--272</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1410" citStr="Forst and Fang, 2009" startWordPosition="211" endWordPosition="214"> investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modifications to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of t</context>
<context position="15670" citStr="Forst and Fang, 2009" startWordPosition="2627" endWordPosition="2630">uct parser we include uses only two grammars because we found, in contrast to the English results (Petrov, 2010), that further grammars provided limited benefits. Comparing the performance with the standard Berkeley parser it seems that the diversity in the grammars only assists certain error types, with most of the improvement occurring in four of the categories, while there is no improvement, or a slight decrease, in five categories. 6 Tagging Error Impact The challenge of accurate POS tagging in Chinese has been a major part of several recent papers (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009). The Berk-G row of Table 2 shows the performance of the Berkeley parser when given gold POS tags.5 While the F1 improvement is unsurprising, for the first time we can clearly show that the gains are only in a subset of the error types. In particular, tagging improvement will not help for two of the most significant challenges: coordination scope errors, and verb argument selection. To see which tagging confusions contribute to which error reductions, we adapt the POS ablation approach of Tse and Curran (2012). We consider the POS tag pairs shown in Table 3. To isolate the effects of each conf</context>
</contexts>
<marker>Forst, Fang, 2009</marker>
<rawString>Martin Forst and Ji Fang. 2009. TBL-improved non-deterministic segmentation and POS tagging for a Chinese parser. In Proceedings of the 12th Conference of the European Chapter of the ACL, pages 264–272. Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Haifeng Wang</author>
<author>Josef van Genabith</author>
</authors>
<title>Recovering Non-Local Dependencies for Chinese.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>257--266</pages>
<location>Prague, Czech Republic.</location>
<marker>Guo, Wang, van Genabith, 2007</marker>
<rawString>Yuqing Guo, Haifeng Wang, and Josef van Genabith. 2007. Recovering Non-Local Dependencies for Chinese. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 257–266. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<volume>1</volume>
<pages>522--530</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="1387" citStr="Jiang et al., 2009" startWordPosition="207" endWordPosition="210">ance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modifications to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, </context>
<context position="15647" citStr="Jiang et al., 2009" startWordPosition="2623" endWordPosition="2626">n. The Berkeley product parser we include uses only two grammars because we found, in contrast to the English results (Petrov, 2010), that further grammars provided limited benefits. Comparing the performance with the standard Berkeley parser it seems that the diversity in the grammars only assists certain error types, with most of the improvement occurring in four of the categories, while there is no improvement, or a slight decrease, in five categories. 6 Tagging Error Impact The challenge of accurate POS tagging in Chinese has been a major part of several recent papers (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009). The Berk-G row of Table 2 shows the performance of the Berkeley parser when given gold POS tags.5 While the F1 improvement is unsurprising, for the first time we can clearly show that the gains are only in a subset of the error types. In particular, tagging improvement will not help for two of the most significant challenges: coordination scope errors, and verb argument selection. To see which tagging confusions contribute to which error reductions, we adapt the POS ablation approach of Tse and Curran (2012). We consider the POS tag pairs shown in Table 3. To isolate t</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, volume 1, pages 522–530. Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<publisher>Sapporo,Japan.</publisher>
<contexts>
<context position="3502" citStr="Klein and Manning, 2003" startWordPosition="543" endWordPosition="546">with and without gold part-of-speech tags, we are able to isolate and quantify the error types that can be resolved by improvements in tagging accuracy. Our analysis shows that improvements in tagging accuracy can only address a subset of the challenges of Chinese syntax. Further improvement in Chinese parsing performance will require research addressing other challenges, in particular, determining coordination scope. 2 Background The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). While their focus was on issues faced by their factored PCFG parser (Klein and Manning, 2003b), the error types they identified are general issues presented by Chinese syntax in the PCTB. They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to pro-drop, suggesting that closing the English-Chinese parsing gap demands techniques 1The system described in this paper is available from http://code.google.com/p/berkeley-parser-analyser/ 98 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98–103, Sofia, Bulgaria, August 4-9 2013. c�2013 A</context>
<context position="14441" citStr="Klein and Manning, 2003" startWordPosition="2426" endWordPosition="2429">compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers that are not publicly available (Qian and Liu, 2012; Xiong et al., 2005). on sense disambiguation, but performs slightly worse on coordination. The Berke</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. Accurate Unlexicalized Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430. Sapporo,Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15,</booktitle>
<pages>3--10</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3502" citStr="Klein and Manning, 2003" startWordPosition="543" endWordPosition="546">with and without gold part-of-speech tags, we are able to isolate and quantify the error types that can be resolved by improvements in tagging accuracy. Our analysis shows that improvements in tagging accuracy can only address a subset of the challenges of Chinese syntax. Further improvement in Chinese parsing performance will require research addressing other challenges, in particular, determining coordination scope. 2 Background The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). While their focus was on issues faced by their factored PCFG parser (Klein and Manning, 2003b), the error types they identified are general issues presented by Chinese syntax in the PCTB. They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to pro-drop, suggesting that closing the English-Chinese parsing gap demands techniques 1The system described in this paper is available from http://code.google.com/p/berkeley-parser-analyser/ 98 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98–103, Sofia, Bulgaria, August 4-9 2013. c�2013 A</context>
<context position="14441" citStr="Klein and Manning, 2003" startWordPosition="2426" endWordPosition="2429">compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers that are not publicly available (Qian and Liu, 2012; Xiong et al., 2005). on sense disambiguation, but performs slightly worse on coordination. The Berke</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing Systems 15, pages 3–10. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>David Hall</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1048--1059</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="2252" citStr="Kummerfeld et al. (2012)" startWordPosition="346" endWordPosition="349">differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. This paper presents a more comprehensive analysis of errors in Chinese parsing, building on the technique presented in Kummerfeld et al. (2012), which characterized the error behavior of English parsers by quantifying how often they make errors such as PP attachment and coordination scope. To accommodate error classes that are absent in English, we augment the system to recognize Chinese-specific parse errors.1 We use the modified system to show the relative impact of different error types across a range of Chinese parsers. To understand the impact of tagging errors on different error types, we performed a part-ofspeech ablation experiment, in which particular confusions are introduced in isolation. By analyzing the distribution of e</context>
<context position="4508" citStr="Kummerfeld et al. (2012)" startWordPosition="688" endWordPosition="691"> is available from http://code.google.com/p/berkeley-parser-analyser/ 98 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 98–103, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics beyond those currently used for English. However, as noted in their final section, their manual analysis of parse errors in 100 sentences only covered a portion of a single parser’s output, limiting the conclusions they could reach regarding the distribution of errors in Chinese parsing. 2.1 Automatic Error Analysis Our analysis builds on Kummerfeld et al. (2012), which presented a system that automatically classifies English parse errors using a two stage process. First, the system finds the shortest path from the system output to the gold annotations, where each step in the path is a tree transformation, fixing at least one bracket error. Second, each transformation step is classified into one of several error types. When directly applied to Chinese parser output, the system placed over 27% of the errors in the catch-all ‘Other’ type. Many of these errors clearly fall into one of a small set of error types, motivating an adaptation to Chinese syntax</context>
<context position="7229" citStr="Kummerfeld et al. (2012)" startWordPosition="1134" endWordPosition="1137">r* 685 2.58% VP Attachment 626 2.36% Clause Attachment 542 2.04% PP Attachment 514 1.94% Split Verb Compound* 232 0.88% Scope error* 143 0.54% NP Attachment 109 0.41% Other 3186 12.02% Table 1: Errors made when parsing Chinese. Values are the number of bracket errors attributed to that error type. The values shown are for the Berkeley parser, evaluated on the development set. * indicates error types that were added or substantially changed as part of this work. either new in this analysis, have had their definition altered, or have an interesting distribution.2 In all of our results we follow Kummerfeld et al. (2012), presenting the number of bracket errors (missing or extra) attributed to each error type. Bracket counts are more informative than a direct count of each error type, because the impact on EVALB F-score varies between errors, e.g. a single attachment error can cause 20 bracket errors, while a unary error causes only one. NP-internal. (Figure 1a). Unlike the Penn Treebank (Marcus et al., 1993), the PCTB annotates some NP-internal structure. We assign this error type when a transformation involves words whose parts of speech in the gold tree are one of: CC, CD, DEG, ETC, JJ, NN, NR, NT and OD. </context>
<context position="8610" citStr="Kummerfeld et al. (2012)" startWordPosition="1376" endWordPosition="1380">phrasal bracket. These errors arise when a parser proposes a tree in which POS tags (for instance, JJ or NN) occur as siblings of phrasal tags (such as NP), a configuration used by the PCTB bracketing guidelines to indicate complementation as opposed to adjunction (Xue et al., 2005). 2For an explanation of the English error types, see Kummerfeld et al. (2012). 99 N N NP Verb taking wrong args. (Figure 1b). This error type arises when a verb (e.g. M reverse) is hypothesized to take an incorrect argument (wry ({ Bush instead of tthft position). Note that this also covers some of the errors that Kummerfeld et al. (2012) classified as NP Attachment, changing the distribution for that type. Unary. For mis-application of unary rules we separate out instances in which the two brackets in the production have the the same label (A-over-A). This cases is created when traces are eliminated, a standard step in evaluation. More than a third of unary errors made by the Berkeley parser are of the A-over-A type. This can be attributed to two factors: (i) the PCTB annotates non-local dependencies using traces, and (ii) Chinese syntax generates more traces than English syntax (Guo et al., 2007). However, for parsers that d</context>
<context position="13384" citStr="Kummerfeld et al. (2012)" startWordPosition="2253" endWordPosition="2257">), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). Best 1.54 1.25 1.01 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87 Berk-G 86.8 Berk-2 81.8 Berk-1 81.1 ZPAR 78.1 Bikel 76.1 Stan-F 76.0 Stan-P 70.0 Worst 3.94 1.75 1.73 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11 two categories (e.g. between Verb taking wrong args and NP Attachment). Differences in treebank annotations also present a challenge for cross-language error comparison. The most common error type in Chinese, NPinternal structure, is rare in the results of Kummerfeld et al. (2012), but the datasets are not comparable because the PTB has very limited NP-internal structure annotated. Further characterization of the impact of annotation differences on errors is beyond the scope of this paper. Three conclusions that can be made are that (i) coordination is a major issue in both languages, (ii) PP attachment is a much greater problem in English, and (iii) a higher frequency of tracegenerating syntax in Chinese compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese p</context>
</contexts>
<marker>Kummerfeld, Hall, Curran, Klein, 2012</marker>
<rawString>Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. 2012. Parser Showdown at the Wall Street Corral: An Empirical Investigation of Error Types in Parser Output. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1048–1059. Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Christopher Manning</author>
</authors>
<title>Is it harder to parse Chinese, or the Chinese Treebank?</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>439--446</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1513" citStr="Levy and Manning (2003)" startWordPosition="228" endWordPosition="231"> Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modifications to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis they considered how to modify their parser to capture the information necessary to model the syntax within the PCTB. However, their manual analysis was limited in scope, covering only part of the parser output, and was unable to characterize the relative impact of the issues they uncovered. This</context>
<context position="3408" citStr="Levy and Manning (2003)" startWordPosition="526" endWordPosition="529">ons are introduced in isolation. By analyzing the distribution of errors in the system output with and without gold part-of-speech tags, we are able to isolate and quantify the error types that can be resolved by improvements in tagging accuracy. Our analysis shows that improvements in tagging accuracy can only address a subset of the challenges of Chinese syntax. Further improvement in Chinese parsing performance will require research addressing other challenges, in particular, determining coordination scope. 2 Background The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). While their focus was on issues faced by their factored PCFG parser (Klein and Manning, 2003b), the error types they identified are general issues presented by Chinese syntax in the PCTB. They presented several Chinese error types that are rare or absent in English, including noun/verb ambiguity, NP-internal structure and coordination ambiguity due to pro-drop, suggesting that closing the English-Chinese parsing gap demands techniques 1The system described in this paper is available from http://code.google.com/p/berkeley-parser-analyser/ 98 Proceedings of the 51st Annual Meeting of the Assoc</context>
<context position="14416" citStr="Levy and Manning, 2003" startWordPosition="2422" endWordPosition="2425">ating syntax in Chinese compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers that are not publicly available (Qian and Liu, 2012; Xiong et al., 2005). on sense disambiguation, but performs slightly worse o</context>
<context position="17217" citStr="Levy and Manning, 2003" startWordPosition="2904" endWordPosition="2907">at use the gold POS tags, and so returns the best available alternative. This leads to a POS accuracy of 99.35%, which is still well above the parser’s standard POS accuracy of 93.66%. 6We introduce errors to gold tags, rather than removing er101 Confused tags Errors 0 Fl VV NN 1055 -2.72 DEC DEG 526 -1.72 JJ NN 297 -0.57 NR NN 320 -0.05 Table 3: The most frequently confused POS tag pairs. Each 0 Fl is relative to Berk-G. to the Berkeley parser, and run the fine-grained error analysis on its output. VV/NN. This confusion has been consistently shown to be a major contributor to parsing errors (Levy and Manning, 2003; Tse and Curran, 2012; Qian and Liu, 2012), and we find a drop of over 2.7 F1 when the output of the tagger is introduced. We found that while most error types have contributions from a range of POS confusions, verb/noun confusion was responsible for virtually all of the noun boundary errors corrected by using gold tags. DEG/DEC. This confusion between the relativizer and subordinator senses of the particle 的 de is the primary source of improvements on modifier attachment when using gold tags. NR/NN and JJ/NN. Despite their frequency, these confusions have little effect on parsing performance</context>
</contexts>
<marker>Levy, Manning, 2003</marker>
<rawString>Roger Levy and Christopher Manning. 2003. Is it harder to parse Chinese, or the Chinese Treebank? In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 439–446. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7625" citStr="Marcus et al., 1993" startWordPosition="1199" endWordPosition="1202">es that were added or substantially changed as part of this work. either new in this analysis, have had their definition altered, or have an interesting distribution.2 In all of our results we follow Kummerfeld et al. (2012), presenting the number of bracket errors (missing or extra) attributed to each error type. Bracket counts are more informative than a direct count of each error type, because the impact on EVALB F-score varies between errors, e.g. a single attachment error can cause 20 bracket errors, while a unary error causes only one. NP-internal. (Figure 1a). Unlike the Penn Treebank (Marcus et al., 1993), the PCTB annotates some NP-internal structure. We assign this error type when a transformation involves words whose parts of speech in the gold tree are one of: CC, CD, DEG, ETC, JJ, NN, NR, NT and OD. We investigated the errors that fall into the NPinternal category and found that 49% of the errors involved the creation or deletion of a single pretermianl phrasal bracket. These errors arise when a parser proposes a tree in which POS tags (for instance, JJ or NN) occur as siblings of phrasal tags (such as NP), a configuration used by the PCTB bracketing guidelines to indicate complementation</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Products of Random Latent Variable Grammars. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="14302" citStr="Petrov, 2010" startWordPosition="2405" endWordPosition="2406">ages, (ii) PP attachment is a much greater problem in English, and (iii) a higher frequency of tracegenerating syntax in Chinese compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers that are not p</context>
</contexts>
<marker>Petrov, 2010</marker>
<rawString>Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="14263" citStr="Petrov et al., 2006" startWordPosition="2397" endWordPosition="2400">) coordination is a major issue in both languages, (ii) PP attachment is a much greater problem in English, and (iii) a higher frequency of tracegenerating syntax in Chinese compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some re</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440. Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York, USA.</location>
<contexts>
<context position="14287" citStr="Petrov and Klein, 2007" startWordPosition="2401" endWordPosition="2404">ajor issue in both languages, (ii) PP attachment is a much greater problem in English, and (iii) a higher frequency of tracegenerating syntax in Chinese compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411. Rochester, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Joint Chinese word segmentation, POS tagging and parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>501--511</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1217" citStr="Qian and Liu, 2012" startWordPosition="178" endWordPosition="181">es made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modifications to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the differences to treebank annotation decisions and others to meaningful differences in syntax. Based on this analysis </context>
<context position="14939" citStr="Qian and Liu, 2012" startWordPosition="2507" endWordPosition="2510">ser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers that are not publicly available (Qian and Liu, 2012; Xiong et al., 2005). on sense disambiguation, but performs slightly worse on coordination. The Berkeley product parser we include uses only two grammars because we found, in contrast to the English results (Petrov, 2010), that further grammars provided limited benefits. Comparing the performance with the standard Berkeley parser it seems that the diversity in the grammars only assists certain error types, with most of the improvement occurring in four of the categories, while there is no improvement, or a slight decrease, in five categories. 6 Tagging Error Impact The challenge of accurate P</context>
<context position="17260" citStr="Qian and Liu, 2012" startWordPosition="2912" endWordPosition="2915">est available alternative. This leads to a POS accuracy of 99.35%, which is still well above the parser’s standard POS accuracy of 93.66%. 6We introduce errors to gold tags, rather than removing er101 Confused tags Errors 0 Fl VV NN 1055 -2.72 DEC DEG 526 -1.72 JJ NN 297 -0.57 NR NN 320 -0.05 Table 3: The most frequently confused POS tag pairs. Each 0 Fl is relative to Berk-G. to the Berkeley parser, and run the fine-grained error analysis on its output. VV/NN. This confusion has been consistently shown to be a major contributor to parsing errors (Levy and Manning, 2003; Tse and Curran, 2012; Qian and Liu, 2012), and we find a drop of over 2.7 F1 when the output of the tagger is introduced. We found that while most error types have contributions from a range of POS confusions, verb/noun confusion was responsible for virtually all of the noun boundary errors corrected by using gold tags. DEG/DEC. This confusion between the relativizer and subordinator senses of the particle 的 de is the primary source of improvements on modifier attachment when using gold tags. NR/NN and JJ/NN. Despite their frequency, these confusions have little effect on parsing performance. Even within the NP-internal error type th</context>
</contexts>
<marker>Qian, Liu, 2012</marker>
<rawString>Xian Qian and Yang Liu. 2012. Joint Chinese word segmentation, POS tagging and parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 501–511. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Tse</author>
<author>James R Curran</author>
</authors>
<title>The Challenges of Parsing Chinese with Combinatory Categorial Grammar.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>295--304</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="16185" citStr="Tse and Curran (2012)" startWordPosition="2715" endWordPosition="2718"> been a major part of several recent papers (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009). The Berk-G row of Table 2 shows the performance of the Berkeley parser when given gold POS tags.5 While the F1 improvement is unsurprising, for the first time we can clearly show that the gains are only in a subset of the error types. In particular, tagging improvement will not help for two of the most significant challenges: coordination scope errors, and verb argument selection. To see which tagging confusions contribute to which error reductions, we adapt the POS ablation approach of Tse and Curran (2012). We consider the POS tag pairs shown in Table 3. To isolate the effects of each confusion we start from the gold tags and introduce the output of the Stanford tagger whenever it returns one of the two tags being considered.6 We then feed these “semi-gold” tags 5We used the Berkeley parser as it was the best of the parsers we considered. Note that the Berkeley parser occasionally prunes all of the parses that use the gold POS tags, and so returns the best available alternative. This leads to a POS accuracy of 99.35%, which is still well above the parser’s standard POS accuracy of 93.66%. 6We i</context>
</contexts>
<marker>Tse, Curran, 2012</marker>
<rawString>Daniel Tse and James R. Curran. 2012. The Challenges of Parsing Chinese with Combinatory Categorial Grammar. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 295–304. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
<author>Yueliang Qian</author>
</authors>
<title>Parsing the Penn Chinese Treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second international joint conference on Natural Language Processing,</booktitle>
<pages>70--81</pages>
<location>Jeju Island,</location>
<contexts>
<context position="14960" citStr="Xiong et al., 2005" startWordPosition="2511" endWordPosition="2514">g, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a variety of parsing methods, though exclude some recently developed parsers that are not publicly available (Qian and Liu, 2012; Xiong et al., 2005). on sense disambiguation, but performs slightly worse on coordination. The Berkeley product parser we include uses only two grammars because we found, in contrast to the English results (Petrov, 2010), that further grammars provided limited benefits. Comparing the performance with the standard Berkeley parser it seems that the diversity in the grammars only assists certain error types, with most of the improvement occurring in four of the categories, while there is no improvement, or a slight decrease, in five categories. 6 Tagging Error Impact The challenge of accurate POS tagging in Chinese</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese Treebank with semantic knowledge. In Proceedings of the Second international joint conference on Natural Language Processing, pages 70–81. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="1101" citStr="Xue et al., 2005" startWordPosition="157" endWordPosition="160"> of error to overall difficulty is not well understood. We conduct a comprehensive automatic analysis of error types made by Chinese parsers, covering a broad range of error types for large sets of sentences, enabling the first empirical ranking of Chinese error types by their performance impact. We also investigate which error types are resolved by using gold part-of-speech tags, showing that improving Chinese tagging only addresses certain error types, leaving substantial outstanding challenges. 1 Introduction A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al., 2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). While recent advances have focused on understanding and reducing the errors that occur in segmentation and partof-speech tagging (Qian and Liu, 2012; Jiang et al., 2009; Forst and Fang, 2009), a range of substantial issues remain that are purely syntactic. Early work by Levy and Manning (2003) presented modifications to a parser motivated by a manual investigation of parsing errors. They noted substantial differences between Chinese and English parsing, attributing some of the </context>
<context position="8269" citStr="Xue et al., 2005" startWordPosition="1314" endWordPosition="1317">P-internal structure. We assign this error type when a transformation involves words whose parts of speech in the gold tree are one of: CC, CD, DEG, ETC, JJ, NN, NR, NT and OD. We investigated the errors that fall into the NPinternal category and found that 49% of the errors involved the creation or deletion of a single pretermianl phrasal bracket. These errors arise when a parser proposes a tree in which POS tags (for instance, JJ or NN) occur as siblings of phrasal tags (such as NP), a configuration used by the PCTB bracketing guidelines to indicate complementation as opposed to adjunction (Xue et al., 2005). 2For an explanation of the English error types, see Kummerfeld et al. (2012). 99 N N NP Verb taking wrong args. (Figure 1b). This error type arises when a verb (e.g. M reverse) is hypothesized to take an incorrect argument (wry ({ Bush instead of tthft position). Note that this also covers some of the errors that Kummerfeld et al. (2012) classified as NP Attachment, changing the distribution for that type. Unary. For mis-application of unary rules we separate out instances in which the two brackets in the production have the the same label (A-over-A). This cases is created when traces are el</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>TransitionBased Parsing of the Chinese Treebank using a Global Discriminative Model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>162--171</pages>
<location>Paris, France.</location>
<contexts>
<context position="12754" citStr="Zhang and Clark (2009)" startWordPosition="2151" endWordPosition="2154">es between 100 NP Verb Mod. 1-Word Diff Wrong Noun VP Clause PP System Fl Int. Coord Args Unary Attach Span Label Sense Edge Attach Attach Attach Other Table 2: Error breakdown for the development set of PCTB 6. The area filled in for each bar indicates the average number of bracket errors per sentence attributed to that error type, where an empty bar is no errors and a full bar has the value indicated in the bottom row. The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). Best 1.54 1.25 1.01 0.76 0.72 0.21 0.30 0.05 0.21 0.26 0.22 0.18 1.87 Berk-G 86.8 Berk-2 81.8 Berk-1 81.1 ZPAR 78.1 Bikel 76.1 Stan-F 76.0 Stan-P 70.0 Worst 3.94 1.75 1.73 1.48 1.68 1.06 1.02 0.88 0.55 0.50 0.44 0.44 4.11 two categories (e.g. between Verb taking wrong args and NP Attachment). Differences in treebank annotations also present a challenge for cross-language error comparison. The most common error type in Chinese, NPinternal structure, is rare in the resul</context>
<context position="14215" citStr="Zhang and Clark, 2009" startWordPosition="2389" endWordPosition="2392">per. Three conclusions that can be made are that (i) coordination is a major issue in both languages, (ii) PP attachment is a much greater problem in English, and (iii) a higher frequency of tracegenerating syntax in Chinese compared to English poses substantial challenges. 5 Cross-parser analysis The previous section described the error types and their distribution for a single Chinese parser. Here we confirm that these are general trends, by showing that the same pattern is observed for several different parsers on the PCTB 6 dev set.3 We include results for a transition-based parser (ZPAR; Zhang and Clark, 2009), a split-merge PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007; Petrov, 2010), a lexicalized parser (Bikel and Chiang, 2000), and a factored PCFG and dependency parser (Levy and Manning, 2003; Klein and Manning, 2003a,b). 4 Comparing the two Stanford parsers in Table 2, the factored model provides clear improvements 3We use the standard data split suggested by the PCTB 6 file manifest. As a result, our results differ from those previously reported on other splits. All analysis is on the dev set, to avoid revealing specific information about the test set. 4These parsers represent a va</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. TransitionBased Parsing of the Chinese Treebank using a Global Discriminative Model. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 162–171. Paris, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>