<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007283">
<title confidence="0.977997">
Ranking-based readability assessment for early primary children’s literature
</title>
<author confidence="0.934325">
Yi Ma, Eric Fosler-Lussier Robert Lofthus
</author>
<affiliation confidence="0.8946515">
Dept. of Computer Science &amp; Engineering Xerox Corporation
The Ohio State University Rochester, NY 14604, USA
</affiliation>
<address confidence="0.461047">
Columbus, OH 43210, USA Robert.Lofthus@xerox.com
</address>
<email confidence="0.979658">
may,fosler@cse.ohio-state.edu
</email>
<sectionHeader confidence="0.995423" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877105263158">
Determining the reading level of children’s lit-
erature is an important task for providing edu-
cators and parents with an appropriate reading
trajectory through a curriculum. Automating
this process has been a challenge addressed
before in the computational linguistics litera-
ture, with most studies attempting to predict
the particular grade level of a text. However,
guided reading levels developed by educators
operate at a more fine-grained level, with mul-
tiple levels corresponding to each grade. We
find that ranking performs much better than
classification at the fine-grained leveling task,
and that features derived from the visual lay-
out of a book are just as predictive as standard
text features of level; including both sets of
features, we find that we can predict the read-
ing level up to 83% of the time on a small cor-
pus of children’s books.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999619418604651">
Determining the reading level of a text has received
significant attention in the literature, dating back to
simple arithmetic metrics to assess the reading level
based on syllable counts (Flesch, 1948). In the com-
putational linguistics community, several projects
have attempted to determine the grade level of a text
(2nd/3rd/4th/etc). However, the education commu-
nity typically makes finer distinctions in reading lev-
els, with each grade being covered by multiple lev-
els. Moreover, there are multiple scales within the
educational community; for example 1st grade is ap-
proximately covered by levels 3–14 on the Reading
Recovery scale,1 or levels C to H in the Fountas and
Pinnell leveling system.2
For grade-level assessment, classification and
regression approaches have been very promising.
However, it is not clear that an increased number of
classes will allow classification techniques to suc-
ceed with a more fine-grained leveling system. Sim-
ilarly, regression techniques may have problems if
the reading levels are not linearly distributed. In this
work, we investigate a ranking approach to book lev-
eling, and apply this to a fine-grained leveling prob-
lem for Kindergarten through 2nd grade books. The
ranking approach also allows us to be more agnostic
to the particular leveling system: for the vast ma-
jority of pairs of books, different systems will rank
the levels of the books the same way, even if the
exact differences in levels are not the same. Since
most previous work uses classification techniques,
we compare against an SVM multi-class classifier
as well as an SVM regression approach.
What has not received much attention in recent
research is the visual layout of the page. Yet, if one
walks into a bookstore and rummages through the
children’s section, it is very easy to tell the reading
level of a book just by thumbing through the pages.
Visual clues such as the number of text lines per
page, or the area of text boxes relative to the illustra-
tions, or the font size, give instant information to the
reader about the reading level of the book. What is
not clear is if this information is sensitive enough to
deliver a fine-grained assessment of the book. While
</bodyText>
<footnote confidence="0.9999425">
1http://www.readingrecovery.org
2http://www.fountasandpinnellleveledbooks.com
</footnote>
<page confidence="0.856548">
548
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 548–552,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.998891">
publishers may have standard guidelines for content
providers on visual layout, these guidelines likely
differ from publisher to publisher and are not avail-
able for the general public. Moreover, in the digi-
tal age teachers are also content providers who do
not have access to these guidelines, so our proposed
ranking system would be very helpful as they cre-
ate reading materials such as worksheets, web pages,
etc.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999987576923077">
Due to the limitations of traditional approaches,
more advanced methods which use statistical lan-
guage processing techniques have been introduced
by recent work in this area (Collins-Thompson and
Callan, 2004; Schwarm and Ostendorf, 2005; Feng
et al., 2010). Collins-Thompson and Callan (2004)
used a smoothed unigram language model to pre-
dict the grade reading levels of web page documents
and short passages. Heilman et al. (2007) com-
bined a language modeling approach with grammar-
based features to improve readability assessment for
first and second language texts. Schwarm/Petersen
and Ostendorf (2005; 2009) used a support vector
machine to combine surface features with language
models and parsed features. The datasets used in
these previous related works mostly consist of web
page documents and short passages, or articles from
educational newspapers. Since the datasets used are
text-intensive, many efforts have been made to in-
vestigate text properties at a higher linguistic level,
such as discourse analysis, language modeling, part-
of-speech and parsed-based features. However, to
the best of our knowledge, no prior work attempts to
rank scanned children’s books (in fine-grained read-
ing levels) directly by analyzing the visual layout of
the page.
</bodyText>
<sectionHeader confidence="0.974634" genericHeader="method">
3 Ranking Book Leveling Algorithm
</sectionHeader>
<bodyText confidence="0.999992826086957">
Our proposed method can be regarded as a modi-
fied version of a standard ranking algorithm, where
we develop a leveling classification by first rank-
ing books, and then assigning the level based on
the ranking output. Given a set of leveled books,
the process to generate a prediction for a new target
book involves the following two steps.
In the first step, we extract features from each
book, and train an off-the-shelf ranking model to
minimize the pairwise error of books. During the
test phase (second step), we rank all of the leveled
training books as well as the new target (test) book
using the trained ranking model. The predicted read-
ing level of the target book then can be inferred from
the reading levels of neighboring leveled books in
the rank-ordered list of books (in our experiment, we
take into account a window of three books above and
below the target book with reading levels weighted
by distance). Intuitively, we can imagine a book-
shelf in which books are sorted by their reading lev-
els. The ranker’s prediction of the reading level of a
target book corresponds to inserting the target book
into the sorted bookshelf.
</bodyText>
<sectionHeader confidence="0.998261" genericHeader="method">
4 Data Preparation
</sectionHeader>
<subsectionHeader confidence="0.999269">
4.1 Book Selection, Scanning and Markup
</subsectionHeader>
<bodyText confidence="0.99991152">
We have processed 36 children’s books which range
from reading level A to L (3 books each level). The
golden standard key reading levels of those books
are obtained from Fountas and Pinnell leveled book
list (Fountas and Pinnell, 1996) in which letter A in-
dicates the easiest books to read and letter L iden-
tifies more challenging books; this range covers
roughly Kindergarten through Second Grade. The
set of children’s books covers a large variety of gen-
res, series and publishers.
After seeking permission from the publishers,3
all of the books are scanned and OCRed (Optical
Character Recognized) to create PDF versions of
the book. In order to facilitate the feature extrac-
tion process, we manually annotate each book using
Adobe Acrobat markup drawing tools before con-
verting them into corresponding XML files. The
annotation process consists of two straightforward
steps: first, draw surrounding rectangles around the
location of text content; second, find where the pri-
mary illustration images are and mark them using
rectangle markups. Then the corresponding XML
can be generated directly from Adobe Acrobat with
one click on a customized menu item, which is im-
plemented by using Adobe Acrobat JavaScript API.
</bodyText>
<footnote confidence="0.992999">
3This is perhaps the most time-consuming part of the pro-
cess.
</footnote>
<page confidence="0.97548">
549
</page>
<table confidence="0.999965692307693">
# of partitions 1 2 3 4
f1 Accuracy %
SVM Ranker 72.2 69.4 80.6 83.3
SVM Classifier 47.2 61.1 55.6 63.9
SVM Regression 72.2 61.1 58.3 58.3
Flesch-Kincaid 30.6 30.6 30.6 19.4
Spache 27.8 13.9 13.9 11.1
Average leveling error f standard deviation
SVM Ranker 1.00 f 0.99 1.03 f 0.91 0.94 f 0.83 0.92 f 0.73
SVM Classifier 2.00 f 1.60 1.86 f 1.69 1.78 f 1.57 1.44 f 1.23
SVM Regression 1.14 f 1.13 1.25 f 1.11 1.33 f 1.22 1.36 f 1.22
Flesch-Kincaid 3.03 f 2.21 3.03 f 2.29 3.08 f 2.31 3.31 f 2.28
Spache 4.06 f 3.33 4.72 f 3.27 4.83 f 3.34 5.19 f 3.21
</table>
<tableCaption confidence="0.997629">
Table 1: Per-book (averaged) results for ranking versus classification, reporting accuracy within one level and average
error for different numbers of partitions
</tableCaption>
<subsectionHeader confidence="0.615883">
4.2 Feature Design
</subsectionHeader>
<subsubsectionHeader confidence="0.680821">
4.2.1 Surface-level Features
</subsubsectionHeader>
<bodyText confidence="0.997761">
We extract a number of purely text-based features
that have typically been used in the education litera-
ture (e.g., (Flesch, 1948)), including:
</bodyText>
<figureCaption confidence="0.643068666666667">
1. Number of words; 2. Number of letters per
word; 3. Number of sentences; 4. Average sentence
length; 5. Type-token ratio of the text content.
</figureCaption>
<subsubsectionHeader confidence="0.88109">
4.2.2 Visually-oriented Features
</subsubsectionHeader>
<bodyText confidence="0.99980425">
In this feature set, we include a number of features
that would not be available without looking at the
physical layout of the page; with the annotated PDF
versions of the book we are able to extract:
</bodyText>
<listItem confidence="0.701674">
1. Page count; 2. Number of words per page; 3.
Number of sentences per page; 4. Number of text
lines per page; 5. Number of words per text line;
6. Number of words per annotated text rectangle;
7. Number of text lines per annotated text rectan-
gle; 8. Average ratio of annotated text rectangle area
to page area; 9. Average ratio of annotated image
rectangle area to page area; 10. Average ratio of an-
notated text rectangle area to annotated image rect-
angle area; 11. Average font size.
</listItem>
<bodyText confidence="0.999906">
The OCR process provides some of this informa-
tion automatically; while we have manually anno-
tated rectangles for this study one could theoreti-
cally use the OCR information and vision process-
ing techniques to extract rectangles automatically.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999597">
5.1 Ranking vs. Classification/Regression
</subsectionHeader>
<bodyText confidence="0.999968629629629">
In this experiment, we look at whether treating book
leveling as a ranking problem is promising com-
pared to using classification/regression techniques.
Besides taking a whole book as input, we also exper-
iment with partitioning each book uniformly into 2,
3, or 4 parts, treating each sub-book as an indepen-
dent entity. We use a leave-n-out paradigm – dur-
ing each iteration of the training (iterated through all
books), the system leaves out all n partitions corre-
sponding to one book and then tests on all partitions
corresponding to the held-out book. By averaging
the results for the partitions of the held-out book, we
can obtain its predicted reading level.
For ranking, we use the SVMr,k ranker
(Joachims, 2006), which learns a (sparse) weight
vector that minimizes the number of swapped pairs
in the training set. The test book is inserted into the
ordering of the training books by the ranking algo-
rithm, and the level is assigned by averaging the lev-
els of the books above and below the order. To com-
pare the performance of our method with classifiers,
we use both SVM&amp;quot;lt&amp;quot;l&amp;quot;&apos; classifier (Tsochantaridis
et al., 2004) and SVMl��ht (with regression learning
option) (Joachims, 1999) to determine the level of
the book directly. All systems are given the same
set of surface text-based and visual-based features
(Sections 4.2.1 and 4.2.2) as input.
</bodyText>
<page confidence="0.990106">
550
</page>
<table confidence="0.999920555555556">
# of partitions 1 2 3 4
±1 Accuracy %
All Features 72.2 69.4 80.6 83.3
Surface Features 61.1 63.9 58.3 61.1
Visual Features 72.2 72.2 72.2 83.3
Average leveling error ± standard deviation
All Features 1.00 ± 0.99 1.03 ± 0.91 0.94 ± 0.83 0.92 ± 0.73
Surface Features 1.42 ± 1.18 1.28 ± 1.00 1.44 ± 0.91 1.28 ± 1.11
Visual Features 1.03 ± 0.88 0.94 ± 0.86 1.03 ± 0.81 0.89 ± 0.82
</table>
<tableCaption confidence="0.8217475">
Table 2: Per-book (averaged) results for all, surface-only, and visual-only features, reporting accuracy within one level
and average error for different numbers of partitions
</tableCaption>
<bodyText confidence="0.976082685714286">
We score the systems in two ways: first, we com-
pute the accuracy of the system by claiming it is cor-
rect if the book level is within ±1 of the true level.4
The second scoring method is the absolute error of
number of levels away from the true value, averaged
over all of the books.
As we can observe from Table 1, our ranking
system constantly beats the other two approaches
(the ranker is statistically significantly better than
the classifier at p &lt; 0.05 level – figures in bold).
One bit of interesting discovery is that SVM regres-
sion needs more data in order to have reliable results,
as the performance is downgraded when the number
of partitions goes up; the ranking approach benefits
from averaging the increasing number of partitions.5
All three methods have the same style of learner
(support vector learning), which suggests that the
performance gain is due to using a ranking crite-
rion in our method. Therefore we believe ranking
is likely a more effective and accurate method than
classification for this task.
One might also wonder how a traditional measure
of reading level (in this case, the Flesch-Kincaid
(Flesch, 1948) and Spache (Spache, 1953) Grade
Level) would hold up for this data. Flesch-Kincaid
and Spache predictions are linearly converted from
calculated grade levels to Fountas-Pinnell levels; all
of the systems utilizing our full feature set outper-
form these two baselines by a significant amount on
both ±1 accuracy and average leveling error.
4Note that this is still rather fine-grained as there are multi-
ple book levels per grade level.
5We only partition the books up to 4 sub-books because the
shortest book we have only contains 4 PDF pages (8 “book”
pages) and further partitioning the book will lead to sparse data.
</bodyText>
<subsectionHeader confidence="0.993727">
5.2 Visual vs. Surface Features
</subsectionHeader>
<bodyText confidence="0.9999604375">
In order to evaluate the benefits of using visual cues
to assess reading levels, we repeat the experiments
using SVMrank based on our proposed ranking book
leveling algorithm with only the visual features or
only surface features.
Table 2 shows that the visual features surprisingly
outperform the surface features (statistically signif-
icant at p &lt; 0.05 level – figures in bold) and on
some partition levels, visual cues even beat the com-
bination of all features. We note, however, that for
early children’s books, pictures and textual layout
dominate the book content over text. Visual features
can be as useful as traditional surface text-based fea-
tures, but as one moves out of primary literature, we
suspect text features will likely be more effective for
leveling as content becomes more complex.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999936333333333">
In this paper, we proposed a ranking-based book lev-
eling algorithm to assess reading level for children’s
literature. Our experimental results showed that the
ranking-based approach performs significantly bet-
ter than classification approaches as used in current
literature. The increased number of classes deterio-
rates the performance of classifiers in a fine-grained
leveling system. We also introduced visual features
into readability assessment and have seen consider-
able benefits of using visual cues. Since our target
data are children’s books that contain many illustra-
tions and pictures, it is quite reasonable to utilize vi-
sual content to help predict a more accurate reading
level. Future studies in early childhood readability
need to take visual content into account.
</bodyText>
<page confidence="0.996923">
551
</page>
<sectionHeader confidence="0.990146" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999684543478261">
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT / NAACL 2004, volume 4, pages
193–200, Boston, USA.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A comparison of features for automatic read-
ability assessment. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 276–284, Beijing, China. As-
sociation for Computational Linguistics.
R. Flesch. 1948. A new readability yardstick. Journal of
applied psychology, 32(3):221–233.
I. Fountas and G. Pinnell. 1996. Guided Reading:
Good First Teaching for All Children. Heinemann,
Portsmouth, NH.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of NAACL
HLT, pages 460–467.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vec-
tor Learning, chapter 11, pages 169–184. MIT Press,
Cambridge, MA.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217–226. ACM.
S. Petersen and M. Ostendorf. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech &amp; Language, 23(1):89–106.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 523–530. Association for Computational
Linguistics.
G. Spache. 1953. A new readability formula for primary-
grade reading materials. The Elementary School Jour-
nal, 53(7):410–413.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the twenty-first international conference on Machine
learning, page 104. ACM.
</reference>
<page confidence="0.997641">
552
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.644670">
<title confidence="0.995225">Ranking-based readability assessment for early primary children’s literature</title>
<author confidence="0.999885">Yi Ma</author>
<author confidence="0.999885">Eric Robert</author>
<affiliation confidence="0.8341765">Dept. of Computer Science &amp; Xerox The Ohio State Rochester, NY 14604,</affiliation>
<address confidence="0.998157">Columbus, OH 43210, Robert.Lofthus@xerox.com</address>
<email confidence="0.999379">may,fosler@cse.ohio-state.edu</email>
<abstract confidence="0.99837595">Determining the reading level of children’s literature is an important task for providing educators and parents with an appropriate reading trajectory through a curriculum. Automating this process has been a challenge addressed before in the computational linguistics literature, with most studies attempting to predict the particular grade level of a text. However, guided reading levels developed by educators operate at a more fine-grained level, with multiple levels corresponding to each grade. We find that ranking performs much better than classification at the fine-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level; including both sets of features, we find that we can predict the reading level up to 83% of the time on a small corpus of children’s books.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT / NAACL</booktitle>
<volume>4</volume>
<pages>193--200</pages>
<location>Boston, USA.</location>
<contexts>
<context position="4322" citStr="Collins-Thompson and Callan, 2004" startWordPosition="667" endWordPosition="670">hers may have standard guidelines for content providers on visual layout, these guidelines likely differ from publisher to publisher and are not available for the general public. Moreover, in the digital age teachers are also content providers who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and shor</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>K. Collins-Thompson and J. Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings of HLT / NAACL 2004, volume 4, pages 193–200, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Feng</author>
<author>M Jansche</author>
<author>M Huenerfauth</author>
<author>N Elhadad</author>
</authors>
<title>A comparison of features for automatic readability assessment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010),</booktitle>
<pages>276--284</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China.</location>
<contexts>
<context position="4371" citStr="Feng et al., 2010" startWordPosition="675" endWordPosition="678">l layout, these guidelines likely differ from publisher to publisher and are not available for the general public. Moreover, in the digital age teachers are also content providers who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and short passages, or articles from educational newspape</context>
</contexts>
<marker>Feng, Jansche, Huenerfauth, Elhadad, 2010</marker>
<rawString>L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad. 2010. A comparison of features for automatic readability assessment. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 276–284, Beijing, China. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Flesch</author>
</authors>
<title>A new readability yardstick.</title>
<date>1948</date>
<journal>Journal of applied psychology,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1388" citStr="Flesch, 1948" startWordPosition="208" endWordPosition="209">iple levels corresponding to each grade. We find that ranking performs much better than classification at the fine-grained leveling task, and that features derived from the visual layout of a book are just as predictive as standard text features of level; including both sets of features, we find that we can predict the reading level up to 83% of the time on a small corpus of children’s books. 1 Introduction Determining the reading level of a text has received significant attention in the literature, dating back to simple arithmetic metrics to assess the reading level based on syllable counts (Flesch, 1948). In the computational linguistics community, several projects have attempted to determine the grade level of a text (2nd/3rd/4th/etc). However, the education community typically makes finer distinctions in reading levels, with each grade being covered by multiple levels. Moreover, there are multiple scales within the educational community; for example 1st grade is approximately covered by levels 3–14 on the Reading Recovery scale,1 or levels C to H in the Fountas and Pinnell leveling system.2 For grade-level assessment, classification and regression approaches have been very promising. Howeve</context>
<context position="8784" citStr="Flesch, 1948" startWordPosition="1410" endWordPosition="1411"> 0.91 0.94 f 0.83 0.92 f 0.73 SVM Classifier 2.00 f 1.60 1.86 f 1.69 1.78 f 1.57 1.44 f 1.23 SVM Regression 1.14 f 1.13 1.25 f 1.11 1.33 f 1.22 1.36 f 1.22 Flesch-Kincaid 3.03 f 2.21 3.03 f 2.29 3.08 f 2.31 3.31 f 2.28 Spache 4.06 f 3.33 4.72 f 3.27 4.83 f 3.34 5.19 f 3.21 Table 1: Per-book (averaged) results for ranking versus classification, reporting accuracy within one level and average error for different numbers of partitions 4.2 Feature Design 4.2.1 Surface-level Features We extract a number of purely text-based features that have typically been used in the education literature (e.g., (Flesch, 1948)), including: 1. Number of words; 2. Number of letters per word; 3. Number of sentences; 4. Average sentence length; 5. Type-token ratio of the text content. 4.2.2 Visually-oriented Features In this feature set, we include a number of features that would not be available without looking at the physical layout of the page; with the annotated PDF versions of the book we are able to extract: 1. Page count; 2. Number of words per page; 3. Number of sentences per page; 4. Number of text lines per page; 5. Number of words per text line; 6. Number of words per annotated text rectangle; 7. Number of t</context>
<context position="13009" citStr="Flesch, 1948" startWordPosition="2129" endWordPosition="2130">SVM regression needs more data in order to have reliable results, as the performance is downgraded when the number of partitions goes up; the ranking approach benefits from averaging the increasing number of partitions.5 All three methods have the same style of learner (support vector learning), which suggests that the performance gain is due to using a ranking criterion in our method. Therefore we believe ranking is likely a more effective and accurate method than classification for this task. One might also wonder how a traditional measure of reading level (in this case, the Flesch-Kincaid (Flesch, 1948) and Spache (Spache, 1953) Grade Level) would hold up for this data. Flesch-Kincaid and Spache predictions are linearly converted from calculated grade levels to Fountas-Pinnell levels; all of the systems utilizing our full feature set outperform these two baselines by a significant amount on both ±1 accuracy and average leveling error. 4Note that this is still rather fine-grained as there are multiple book levels per grade level. 5We only partition the books up to 4 sub-books because the shortest book we have only contains 4 PDF pages (8 “book” pages) and further partitioning the book will le</context>
</contexts>
<marker>Flesch, 1948</marker>
<rawString>R. Flesch. 1948. A new readability yardstick. Journal of applied psychology, 32(3):221–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Fountas</author>
<author>G Pinnell</author>
</authors>
<title>Guided Reading: Good First Teaching for All Children.</title>
<date>1996</date>
<location>Heinemann, Portsmouth, NH.</location>
<contexts>
<context position="6847" citStr="Fountas and Pinnell, 1996" startWordPosition="1079" endWordPosition="1082">nto account a window of three books above and below the target book with reading levels weighted by distance). Intuitively, we can imagine a bookshelf in which books are sorted by their reading levels. The ranker’s prediction of the reading level of a target book corresponds to inserting the target book into the sorted bookshelf. 4 Data Preparation 4.1 Book Selection, Scanning and Markup We have processed 36 children’s books which range from reading level A to L (3 books each level). The golden standard key reading levels of those books are obtained from Fountas and Pinnell leveled book list (Fountas and Pinnell, 1996) in which letter A indicates the easiest books to read and letter L identifies more challenging books; this range covers roughly Kindergarten through Second Grade. The set of children’s books covers a large variety of genres, series and publishers. After seeking permission from the publishers,3 all of the books are scanned and OCRed (Optical Character Recognized) to create PDF versions of the book. In order to facilitate the feature extraction process, we manually annotate each book using Adobe Acrobat markup drawing tools before converting them into corresponding XML files. The annotation pro</context>
</contexts>
<marker>Fountas, Pinnell, 1996</marker>
<rawString>I. Fountas and G. Pinnell. 1996. Guided Reading: Good First Teaching for All Children. Heinemann, Portsmouth, NH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>M Eskenazi</author>
</authors>
<title>Combining lexical and grammatical features to improve readability measures for first and second language texts.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<pages>460--467</pages>
<contexts>
<context position="4546" citStr="Heilman et al. (2007)" startWordPosition="703" endWordPosition="706">oviders who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and short passages, or articles from educational newspapers. Since the datasets used are text-intensive, many efforts have been made to investigate text properties at a higher linguistic level, such as discourse analysis, language m</context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>M. Heilman, K. Collins-Thompson, J. Callan, and M. Eskenazi. 2007. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of NAACL HLT, pages 460–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning, chapter 11,</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11148" citStr="Joachims, 1999" startWordPosition="1805" endWordPosition="1806">ults for the partitions of the held-out book, we can obtain its predicted reading level. For ranking, we use the SVMr,k ranker (Joachims, 2006), which learns a (sparse) weight vector that minimizes the number of swapped pairs in the training set. The test book is inserted into the ordering of the training books by the ranking algorithm, and the level is assigned by averaging the levels of the books above and below the order. To compare the performance of our method with classifiers, we use both SVM&amp;quot;lt&amp;quot;l&amp;quot;&apos; classifier (Tsochantaridis et al., 2004) and SVMl��ht (with regression learning option) (Joachims, 1999) to determine the level of the book directly. All systems are given the same set of surface text-based and visual-based features (Sections 4.2.1 and 4.2.2) as input. 550 # of partitions 1 2 3 4 ±1 Accuracy % All Features 72.2 69.4 80.6 83.3 Surface Features 61.1 63.9 58.3 61.1 Visual Features 72.2 72.2 72.2 83.3 Average leveling error ± standard deviation All Features 1.00 ± 0.99 1.03 ± 0.91 0.94 ± 0.83 0.92 ± 0.73 Surface Features 1.42 ± 1.18 1.28 ± 1.00 1.44 ± 0.91 1.28 ± 1.11 Visual Features 1.03 ± 0.88 0.94 ± 0.86 1.03 ± 0.81 0.89 ± 0.82 Table 2: Per-book (averaged) results for all, surfac</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, chapter 11, pages 169–184. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear SVMs in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>217--226</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10676" citStr="Joachims, 2006" startWordPosition="1726" endWordPosition="1727">red to using classification/regression techniques. Besides taking a whole book as input, we also experiment with partitioning each book uniformly into 2, 3, or 4 parts, treating each sub-book as an independent entity. We use a leave-n-out paradigm – during each iteration of the training (iterated through all books), the system leaves out all n partitions corresponding to one book and then tests on all partitions corresponding to the held-out book. By averaging the results for the partitions of the held-out book, we can obtain its predicted reading level. For ranking, we use the SVMr,k ranker (Joachims, 2006), which learns a (sparse) weight vector that minimizes the number of swapped pairs in the training set. The test book is inserted into the ordering of the training books by the ranking algorithm, and the level is assigned by averaging the levels of the books above and below the order. To compare the performance of our method with classifiers, we use both SVM&amp;quot;lt&amp;quot;l&amp;quot;&apos; classifier (Tsochantaridis et al., 2004) and SVMl��ht (with regression learning option) (Joachims, 1999) to determine the level of the book directly. All systems are given the same set of surface text-based and visual-based features</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217–226. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petersen</author>
<author>M Ostendorf</author>
</authors>
<title>A machine learning approach to reading level assessment.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>1</issue>
<marker>Petersen, Ostendorf, 2009</marker>
<rawString>S. Petersen and M. Ostendorf. 2009. A machine learning approach to reading level assessment. Computer Speech &amp; Language, 23(1):89–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4351" citStr="Schwarm and Ostendorf, 2005" startWordPosition="671" endWordPosition="674">or content providers on visual layout, these guidelines likely differ from publisher to publisher and are not available for the general public. Moreover, in the digital age teachers are also content providers who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and short passages, or articles from </context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>S. Schwarm and M. Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 523–530. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Spache</author>
</authors>
<title>A new readability formula for primarygrade reading materials.</title>
<date>1953</date>
<journal>The Elementary School Journal,</journal>
<volume>53</volume>
<issue>7</issue>
<contexts>
<context position="13035" citStr="Spache, 1953" startWordPosition="2133" endWordPosition="2134">data in order to have reliable results, as the performance is downgraded when the number of partitions goes up; the ranking approach benefits from averaging the increasing number of partitions.5 All three methods have the same style of learner (support vector learning), which suggests that the performance gain is due to using a ranking criterion in our method. Therefore we believe ranking is likely a more effective and accurate method than classification for this task. One might also wonder how a traditional measure of reading level (in this case, the Flesch-Kincaid (Flesch, 1948) and Spache (Spache, 1953) Grade Level) would hold up for this data. Flesch-Kincaid and Spache predictions are linearly converted from calculated grade levels to Fountas-Pinnell levels; all of the systems utilizing our full feature set outperform these two baselines by a significant amount on both ±1 accuracy and average leveling error. 4Note that this is still rather fine-grained as there are multiple book levels per grade level. 5We only partition the books up to 4 sub-books because the shortest book we have only contains 4 PDF pages (8 “book” pages) and further partitioning the book will lead to sparse data. 5.2 Vis</context>
</contexts>
<marker>Spache, 1953</marker>
<rawString>G. Spache. 1953. A new readability formula for primarygrade reading materials. The Elementary School Journal, 53(7):410–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning,</booktitle>
<pages>104</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11084" citStr="Tsochantaridis et al., 2004" startWordPosition="1795" endWordPosition="1798">ts on all partitions corresponding to the held-out book. By averaging the results for the partitions of the held-out book, we can obtain its predicted reading level. For ranking, we use the SVMr,k ranker (Joachims, 2006), which learns a (sparse) weight vector that minimizes the number of swapped pairs in the training set. The test book is inserted into the ordering of the training books by the ranking algorithm, and the level is assigned by averaging the levels of the books above and below the order. To compare the performance of our method with classifiers, we use both SVM&amp;quot;lt&amp;quot;l&amp;quot;&apos; classifier (Tsochantaridis et al., 2004) and SVMl��ht (with regression learning option) (Joachims, 1999) to determine the level of the book directly. All systems are given the same set of surface text-based and visual-based features (Sections 4.2.1 and 4.2.2) as input. 550 # of partitions 1 2 3 4 ±1 Accuracy % All Features 72.2 69.4 80.6 83.3 Surface Features 61.1 63.9 58.3 61.1 Visual Features 72.2 72.2 72.2 83.3 Average leveling error ± standard deviation All Features 1.00 ± 0.99 1.03 ± 0.91 0.94 ± 0.83 0.92 ± 0.73 Surface Features 1.42 ± 1.18 1.28 ± 1.00 1.44 ± 0.91 1.28 ± 1.11 Visual Features 1.03 ± 0.88 0.94 ± 0.86 1.03 ± 0.81 </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international conference on Machine learning, page 104. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>