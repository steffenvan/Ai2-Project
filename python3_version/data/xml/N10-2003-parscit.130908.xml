<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009294">
<title confidence="0.9996465">
Phrasal: A Toolkit for Statistical Machine Translation
with Facilities for Extraction and Incorporation of Arbitrary Model Features
</title>
<author confidence="0.999379">
Daniel Cer, Michel Galley, Daniel Jurafsky and Christopher D. Manning
</author>
<affiliation confidence="0.8086565">
Stanford University
Stanford, CA 94305, USA
</affiliation>
<sectionHeader confidence="0.964762" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999236421052632">
We present a new Java-based open source
toolkit for phrase-based machine translation.
The key innovation provided by the toolkit
is to use APIs for integrating new fea-
tures (/knowledge sources) into the decod-
ing model and for extracting feature statis-
tics from aligned bitexts. The package in-
cludes a number of useful features written to
these APIs including features for hierarchi-
cal reordering, discriminatively trained linear
distortion, and syntax based language models.
Other useful utilities packaged with the toolkit
include: a conditional phrase extraction sys-
tem that builds a phrase table just for a spe-
cific dataset; and an implementation of MERT
that allows for pluggable evaluation metrics
for both training and evaluation with built in
support for a variety of metrics (e.g., TERp,
BLEU, METEOR).
</bodyText>
<sectionHeader confidence="0.990057" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999904697674418">
Progress in machine translation (MT) depends crit-
ically on the development of new and better model
features that allow translation systems to better iden-
tify and construct high quality machine translations.
The popular Moses decoder (Koehn et al., 2007)
was designed to allow new features to be defined us-
ing factored translation models. In such models, the
individual phrases being translated can be factored
into two or more abstract phrases (e.g., lemma, POS-
tags) that can be translated individually and then
combined in a seperate generation stage to arrive at
the final target translation. While greatly enriching
the space of models that can be used for phrase-
based machine translation, Moses only allows fea-
tures that can be defined at the level of individual
words and phrases.
The Phrasal toolkit provides easy-to-use APIs
for the development of arbitrary new model fea-
tures. It includes an API for extracting feature
statistics from aligned bitexts and for incor-
porating the new features into the decoding
model. The system has already been used to
develop a number of innovative new features
(Chang et al., 2009; Galley and Manning, 2008;
Galley and Manning, 2009; Green et al., 2010) and
to build translation systems that have placed well
at recent competitive evaluations, achieving second
place for Arabic to English translation on the NIST
2009 constrained data track.1
We implemented the toolkit in Java because it of-
fers a good balance between performance and de-
veloper productivity. Compared to C++, develop-
ers using Java are 30 to 200% faster, produce fewer
defects, and correct defects up to 6 times faster
(Phipps, 1999). While Java programs were histori-
cally much slower than similar programs written in
C or C++, modern Java virtual machines (JVMs) re-
sult in Java programs being nearly as fast as C++
programs (Bruckschlegel, 2005). Java also allows
for trivial code portability across different platforms.
In the remainder of the paper, we will highlight
various useful capabilities, components and model-
ing features included in the toolkit.
</bodyText>
<sectionHeader confidence="0.987391" genericHeader="introduction">
2 Toolkit
</sectionHeader>
<bodyText confidence="0.999857">
The toolkit provides end-to-end support for the cre-
ation and evaluation of machine translation models.
Given sentence-aligned parallel text, a new transla-
tion system can be built using a single command:
</bodyText>
<equation confidence="0.994035333333333">
java edu.stanford.nlp.mt.CreateModel \
(source.txt) (target.txt) \
(dev.source.txt) (dev.ref) (model_name)
</equation>
<bodyText confidence="0.991112333333333">
Running this command will first create word
level alignments for the sentences in source.txt
and target.txt using the Berkeley cross-EM aligner
</bodyText>
<footnote confidence="0.9931725">
1http://www.itl.nist.gov/iad/mig/tests
/mt/2009/ResultsRelease/currentArabic.html
</footnote>
<page confidence="0.895432">
9
</page>
<note confidence="0.4450865">
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998769">
Figure 1: Chinese-to-English translation using discontinuous phrases.
</figureCaption>
<bodyText confidence="0.994023117647059">
(Liang et al., 2006).2 From the word-to-word
alignments, the system extracts a phrase ta-
ble (Koehn et al., 2003) and hierarchical reorder-
ing model (Galley and Manning, 2008). Two n-
gram language models are trained on the tar-
get.txt sentences: one over lowercased target sen-
tences that will be used by the Phrasal decoder
and one over the original source sentences that
will be used for truecasing the MT output. Fi-
nally, the system trains the feature weights for the
decoding model using minimum error rate train-
ing (Och, 2003) to maximize the system’s BLEU
score (Papineni et al., 2002) on the development
data given by dev.source.txt and dev.ref. The toolkit
is distributed under the GNU general public license
(GPL) and can be downloaded from http://
nlp.stanford.edu/software/phrasal.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="method">
3 Decoder
</sectionHeader>
<bodyText confidence="0.995715217391304">
Decoding Engines The package includes two de-
coding engines, one that implements the left-to-
right beam search algorithm that was first intro-
duced with the Pharaoh machine translation system
(Koehn, 2004), and another that provides a recently
developed decoding algorithm for translating with
discontinuous phrases (Galley and Manning, 2010).
Both engines use features written to a common but
extensible feature API, which allows features to be
written once and then loaded into either engine.
Discontinuous phrases provide a mechanism for
systematically translating grammatical construc-
tions. As seen in Fig. 1, using discontinuous phrases
allows us to successfully capture that the Chinese
construction 当 X 的 can be translated as when X.
Multithreading The decoder has robust support
for multithreading, allowing it to take full advantage
of modern hardware that provides multiple CPU
cores. As shown in Fig. 2, decoding speed scales
well when the number of threads being used is in-
creased from one to four. However, increasing the
2Optionally, GIZA++ (Och and Ney, 2003) can also be used
to create the word-to-word alignments.
</bodyText>
<figure confidence="0.866432">
tranlations per minute 15 25 35
1 2 3 4 5 6 7 8
Cores
</figure>
<figureCaption confidence="0.987155333333333">
Figure 2: Multicore translations per minute on a sys-
tem with two Intel Xeon L5530 processors running at
2.40GHz.
</figureCaption>
<bodyText confidence="0.999967230769231">
threads past four results in only marginal additional
gains as the cost of managing the resources shared
between the threads is starting to overwhelm the
value provided by each additional thread. Moses
also does not run faster with more than 4-5 threads.3
Feature API The feature API was designed to
abstract away complex implementation details of
the underlying decoding engine and provide a sim-
ple consistent framework for creating new decoding
model features. During decoding, as each phrase
that is translated, the system constructs a Featuriz-
able object. As seen in Table 1, Featurizable objects
specify what phrase was just translated and an over-
all summary of the translation being built. Code that
implements a feature inspects the Featurizable and
returns one or more named feature values. Prior to
translating a new sentence, the sentence is passed to
the active features for a decoding model, so that they
can perform any necessary preliminary analysis.
Comparison with Moses Credible research into
new features requires baseline system performance
that is on par with existing state-of-the-art systems.
Seen in Table 2, Phrasal meets the performance of
Moses when using the exact same decoding model
feature set as Moses and outperforms Moses signifi-
cantly when using its own default feature set.4
</bodyText>
<footnote confidence="0.992078">
3http://statmt.org/moses
/?n=Moses.AdvancedFeatures (April 6, 2010)
4Phrasal was originally written to replicate Moses as it was
implemented in 2007 (release 2007-05-29), and the current ver-
</footnote>
<page confidence="0.966909">
10
</page>
<table confidence="0.972924428571429">
Featurizable
Last Translated Phrase Pair
Source and Target Alignments
Partial Translation
Source Sentence
Current Source Coverage
Pointer to Prior Featurizable
</table>
<tableCaption confidence="0.9993855">
Table 1: Information passed to features in the form of a
Featurizable object for each translated phrase.
</tableCaption>
<table confidence="0.9998995">
System Features MT06 (tune) MT03 MT05
Moses Moses 34.23 33.72 32.51
Phrasal Moses 34.25 33.72 32.49
Phrasal Default 35.02 34.98 33.21
</table>
<tableCaption confidence="0.991072">
Table 2: Comparison of two configurations of Phrasal
</tableCaption>
<figureCaption confidence="0.652757571428571">
to Moses on Chinese-to-English. One Phrasal configura-
tion uses the standard Moses feature set for single factor
phrase-based translation with distance and phrase level
msd-bidirectional-fe reordering features. The other uses
the default configuration of Phrasal, which replaces the
phrase level msd-bidirectional-fe feature with a heirarchi-
cal reordering feature.
</figureCaption>
<sectionHeader confidence="0.998382" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.997739179104478">
The toolkit includes the basic eight phrase-based
translation features available in Moses as well as
Moses’ implementation of lexical reordering fea-
tures. In addition to the common Moses features, we
also include innovative new features that improve
translation quality. One of these features is a hier-
archical generalization of the Moses lexical reorder-
ing model. Instead of just looking at the reorder-
ing relationship between individual phrases, the new
feature examines the reordering of blocks of ad-
jacent phrases (Galley and Manning, 2008) and im-
proves translation quality when the material being
reordered cannot be captured by single phrase. This
hierarchical lexicalized reordering model is used by
default in Phrasal and is responsible for the gains
shown in Table 2 using the default features.
To illustrate how Phrasal can effectively be used
to design rich feature sets, we present an overview
of various extensions that have been built upon the
sion still almost exactly replicates this implementation when
using only the baseline Moses features. To ensure this con-
figuration of the decoder is still competitive, we compared it
against the current Moses implementation (release 2009-04-
13) and found that the performance of the two systems is still
close. Tthe current Moses implementation obtains slightly
lower BLEU scores, respectively 33.98 and 32.39 on MT06 and
MT05.
Phrasal feature API. These extensions are currently
not included in the release:
Target Side Dependency Language Model The
n-gram language models that are traditionally used
to capture the syntax of the target language do a
poor job of modeling long distance syntactic rela-
tionships. For example, if there are a number of
intervening words between a verb and its subject,
n-gram language models will often not be of much
help in selecting the verb form that agrees with the
subject. The target side dependency language model
feature captures these long distance relationships by
providing a dependency score for the target transla-
tions produced by the decoder. This is done using
an efficient quadratic time algorithm that operates
within the main decoding loop rather than in a sepa-
rate reranking stage (Galley and Manning, 2009).
Discriminative Distortion The standard distor-
tion cost model used in phrase-based MT systems
such as Moses has two problems. First, it does not
estimate the future cost of known required moves,
thus increasing search errors. Second, the model pe-
nalizes distortion linearly, even when appropriate re-
orderings are performed. To address these problems,
we used the Phrasal feature API to design a new
discriminative distortion model that predicts word
movement during translation and that estimates fu-
ture cost. These extensions allow us to triple the
distortion limit and provide a statistically significant
improvement over the baseline (Green et al., 2010).
Discriminative Reordering with Chinese Gram-
matical Relations During translation, a source
sentence can be more accurately reordered if the
system knows something about the syntactic rela-
tionship between the words in the phrases being re-
ordered. The discriminative reordering with Chinese
grammatical relations feature examines the path be-
tween words in a source-side dependency tree and
uses it to evaluate the appropriateness of candidate
phrase reorderings (Chang et al., 2009).
</bodyText>
<sectionHeader confidence="0.982584" genericHeader="method">
5 Other components
</sectionHeader>
<bodyText confidence="0.998235166666667">
Training Decoding Models The package includes
a comprehensive toolset for training decoding mod-
els. It supports MERT training using coordinate de-
scent, Powell’s method, line search along random
search directions, and downhill Simplex. In addi-
tion to the BLEU metric, models can be trained
</bodyText>
<page confidence="0.998189">
11
</page>
<bodyText confidence="0.991932809523809">
to optimize other popular evaluation metrics such
as METEOR (Lavie and Denkowski, 2009), TERp
(Snover et al., 2009), mWER (Nießen et al., 2000),
and PER (Tillmann et al., 1997). It is also possible
to plug in other new user-created evaluation metrics.
Conditional Phrase Table Extraction Rather
than first building a massive phrase table from a par-
allel corpus and then filtering it down to just what
is needed for a specific data set, our toolkit sup-
ports the extraction of just those phrases that might
be used on a given evaluation set. In doing so, it
dramatically reduces the time required to build the
phrase table and related data structures such as re-
ordering models.
Feature Extraction API In order to assist in the
development of new features, the toolkit provides
an API for extracting feature statistics from a word-
aligned parallel corpus. This API ties into the condi-
tional phrase table extraction utility, and thus allows
for the extraction of just those feature statistics that
are relevant to a given data set.
</bodyText>
<sectionHeader confidence="0.999231" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999794076923077">
Phrasal is an open source state-of-the-art Java-
based machine translation system that was designed
specifically for research into new decoding model
features. The system supports traditional phrase-
based translation as well as translation using discon-
tinuous phrases. It includes a number of new and
innovative model features in addition to those typi-
cally found in phrase-based translation systems. It is
also packaged with other useful components such as
tools for extracting feature statistics, building phrase
tables for specific data sets, and MERT training rou-
tines that support a number of optimization tech-
niques and evaluation metrics.
</bodyText>
<sectionHeader confidence="0.994949" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999927333333333">
The Phrasal decoder has benefited from the help-
ful comments and code contributions of Pi-Chuan
Chang, Spence Green, Karthik Raghunathan,
Ankush Singla, and Huihsin Tseng. The software
presented in this paper is based on work work was
funded by the Defense Advanced Research Projects
Agency through IBM. The content does not neces-
sarily reflect the views of the U.S. Government, and
no official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850314814815">
Thomas Bruckschlegel. 2005. Microbenchmarking C++,
C#, and Java. C/C++ Users Journal.
P. Chang, H. Tseng, D. Jurafsky, and C.D. Manning.
2009. Discriminative reordering with Chinese gram-
matical relations features. In SSST Workshop at
NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In EMNLP.
Michel Galley and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine trans-
lation. In ACL.
Michel Galley and Christopher Manning. 2010. Improv-
ing phrase-based machine translation with discontigu-
ous phrases. In NAACL.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In In NAACL.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
Alon Lavie and Michael J. Denkowski. 2009. The
METEOR metric for automatic evaluation of machine
translation. Machine Translation, 23.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Sonja Nießen, Franz Josef Och, and Hermann Ney. 2000.
An evaluation tool for machine translation: Fast eval-
uation for MT research. In LREC.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Geoffrey Phipps. 1999. Comparing observed bug and
productivity rates for java and C++. Softw. Pract. Ex-
per., 29(4):345–358.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER?: exploring dif-
ferent human judgments with a tunable MT metric. In
SMT workshop at EACL.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In In Eurospeech.
</reference>
<page confidence="0.998461">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.999167">Phrasal: A Toolkit for Statistical Machine with Facilities for Extraction and Incorporation of Arbitrary Model Features</title>
<author confidence="0.984661">Daniel Cer</author>
<author confidence="0.984661">Michel Galley</author>
<author confidence="0.984661">Daniel Jurafsky</author>
<author confidence="0.984661">D Christopher</author>
<affiliation confidence="0.999989">Stanford University</affiliation>
<address confidence="0.999743">Stanford, CA 94305, USA</address>
<abstract confidence="0.988795628205127">We present a new Java-based open source toolkit for phrase-based machine translation. The key innovation provided by the toolkit is to use APIs for integrating new features (/knowledge sources) into the decoding model and for extracting feature statistics from aligned bitexts. The package includes a number of useful features written to these APIs including features for hierarchical reordering, discriminatively trained linear distortion, and syntax based language models. Other useful utilities packaged with the toolkit include: a conditional phrase extraction system that builds a phrase table just for a specific dataset; and an implementation of MERT that allows for pluggable evaluation metrics for both training and evaluation with built in support for a variety of metrics (e.g., TERp, BLEU, METEOR). 1 Motivation Progress in machine translation (MT) depends critically on the development of new and better model features that allow translation systems to better identify and construct high quality machine translations. The popular Moses decoder (Koehn et al., 2007) was designed to allow new features to be defined using factored translation models. In such models, the individual phrases being translated can be factored into two or more abstract phrases (e.g., lemma, POStags) that can be translated individually and then combined in a seperate generation stage to arrive at the final target translation. While greatly enriching the space of models that can be used for phrasebased machine translation, Moses only allows features that can be defined at the level of individual words and phrases. The Phrasal toolkit provides easy-to-use APIs for the development of arbitrary new model features. It includes an API for extracting feature statistics from aligned bitexts and for incorporating the new features into the decoding model. The system has already been used to develop a number of innovative new features (Chang et al., 2009; Galley and Manning, Galley and Manning, 2009; Green et al., 2010) and to build translation systems that have placed well at recent competitive evaluations, achieving second place for Arabic to English translation on the NIST constrained data We implemented the toolkit in Java because it offers a good balance between performance and developer productivity. Compared to C++, developers using Java are 30 to 200% faster, produce fewer defects, and correct defects up to 6 times faster (Phipps, 1999). While Java programs were historically much slower than similar programs written in C or C++, modern Java virtual machines (JVMs) result in Java programs being nearly as fast as C++ programs (Bruckschlegel, 2005). Java also allows for trivial code portability across different platforms. In the remainder of the paper, we will highlight various useful capabilities, components and modeling features included in the toolkit. 2 Toolkit The toolkit provides end-to-end support for the creation and evaluation of machine translation models. Given sentence-aligned parallel text, a new translation system can be built using a single command: java edu.stanford.nlp.mt.CreateModel \ (source.txt) (target.txt) \ (dev.source.txt) (dev.ref) (model_name) Running this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner /mt/2009/ResultsRelease/currentArabic.html 9 of the NAACL HLT 2010: Demonstration pages 9–12, Angeles, California, June 2010. Association for Computational Linguistics Figure 1: Chinese-to-English translation using discontinuous phrases. et al., From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license and can be downloaded from 3 Decoder Engines package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common but extensible feature API, which allows features to be written once and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese be translated as decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. tranlations per minute 15 25 35 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared between the threads is starting to overwhelm the value provided by each additional thread. Moses does not run faster with more than 4-5 API feature API was designed to abstract away complex implementation details of the underlying decoding engine and provide a simple consistent framework for creating new decoding model features. During decoding, as each phrase is translated, the system constructs a Featuriz- As seen in Table 1, specify what phrase was just translated and an overall summary of the translation being built. Code that a feature inspects the returns one or more named feature values. Prior to translating a new sentence, the sentence is passed to the active features for a decoding model, so that they can perform any necessary preliminary analysis. with Moses research into new features requires baseline system performance that is on par with existing state-of-the-art systems. Seen in Table 2, Phrasal meets the performance of Moses when using the exact same decoding model feature set as Moses and outperforms Moses signifiwhen using its own default feature /?n=Moses.AdvancedFeatures (April 6, 2010) was originally written to replicate Moses as it was in 2007 (release 2007-05-29), and the current ver- 10</abstract>
<title confidence="0.9319825">Featurizable Last Translated Phrase Pair Source and Target Alignments Partial Translation Source Sentence Current Source Coverage</title>
<author confidence="0.317073">Pointer to Prior Featurizable</author>
<abstract confidence="0.929535370370371">Table 1: Information passed to features in the form of a for each translated phrase. System Features MT06 (tune) MT03 MT05 Moses Moses 34.23 33.72 32.51 Phrasal Moses 34.25 33.72 32.49 Phrasal Default 35.02 34.98 33.21 Table 2: Comparison of two configurations of Phrasal to Moses on Chinese-to-English. One Phrasal configuration uses the standard Moses feature set for single factor phrase-based translation with distance and phrase level features. The other uses the default configuration of Phrasal, which replaces the level with a heirarchical reordering feature. 4 Features The toolkit includes the basic eight phrase-based translation features available in Moses as well as Moses’ implementation of lexical reordering features. In addition to the common Moses features, we also include innovative new features that improve translation quality. One of these features is a hierarchical generalization of the Moses lexical reordering model. Instead of just looking at the reordering relationship between individual phrases, the new feature examines the reordering of blocks of adjacent phrases (Galley and Manning, 2008) and improves translation quality when the material being reordered cannot be captured by single phrase. This hierarchical lexicalized reordering model is used by default in Phrasal and is responsible for the gains shown in Table 2 using the default features. To illustrate how Phrasal can effectively be used to design rich feature sets, we present an overview of various extensions that have been built upon the sion still almost exactly replicates this implementation when using only the baseline Moses features. To ensure this configuration of the decoder is still competitive, we compared it against the current Moses implementation (release 2009-04- 13) and found that the performance of the two systems is still close. Tthe current Moses implementation obtains slightly lower BLEU scores, respectively 33.98 and 32.39 on MT06 and MT05. Phrasal feature API. These extensions are currently not included in the release: Side Dependency Language Model n-gram language models that are traditionally used to capture the syntax of the target language do a poor job of modeling long distance syntactic relationships. For example, if there are a number of intervening words between a verb and its subject, n-gram language models will often not be of much help in selecting the verb form that agrees with the subject. The target side dependency language model feature captures these long distance relationships by providing a dependency score for the target translations produced by the decoder. This is done using an efficient quadratic time algorithm that operates within the main decoding loop rather than in a separate reranking stage (Galley and Manning, 2009). Distortion standard distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Gram- Relations translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Decoding Models package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained 11 to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Phrase Table Extraction than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Extraction API order to assist in the development of new features, the toolkit provides an API for extracting feature statistics from a wordaligned parallel corpus. This API ties into the conditional phrase table extraction utility, and thus allows for the extraction of just those feature statistics that are relevant to a given data set. 6 Conclusion Phrasal is an open source state-of-the-art Javabased machine translation system that was designed specifically for research into new decoding model features. The system supports traditional phrasebased translation as well as translation using discontinuous phrases. It includes a number of new and innovative model features in addition to those typically found in phrase-based translation systems. It is also packaged with other useful components such as tools for extracting feature statistics, building phrase tables for specific data sets, and MERT training routines that support a number of optimization techniques and evaluation metrics. Acknowledgements The Phrasal decoder has benefited from the helpful comments and code contributions of Pi-Chuan Chang, Spence Green, Karthik Raghunathan, Ankush Singla, and Huihsin Tseng. The software presented in this paper is based on work work was funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.</abstract>
<title confidence="0.866643">References</title>
<author confidence="0.745733">C Microbenchmarking</author>
<author confidence="0.745733">Java Users</author>
<abstract confidence="0.845698235294118">P. Chang, H. Tseng, D. Jurafsky, and C.D. Manning. 2009. Discriminative reordering with Chinese gramrelations features. In Workshop at Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering In Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine trans- In Michel Galley and Christopher Manning. 2010. Improving phrase-based machine translation with discontiguphrases. In Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for machine translation. In Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Staphrase-based translation. In</abstract>
<author confidence="0.8563725">Moses Open source toolkit for</author>
<abstract confidence="0.664564366666667">machine translation. In Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation mod- In Alon Lavie and Michael J. Denkowski. 2009. The METEOR metric for automatic evaluation of machine 23. Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignby agreement. In Sonja Nießen, Franz Josef Och, and Hermann Ney. 2000. An evaluation tool for machine translation: Fast evalfor MT research. In Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment mod- 29(1):19–51. Franz Josef Och. 2003. Minimum error rate training in machine translation. In Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. BLEU: a method for automatic evalof machine translation. In Geoffrey Phipps. 1999. Comparing observed bug and rates for java and C++. Pract. Ex- 29(4):345–358. M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz. 2009. Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric. In workshop at C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based search for statistical In</abstract>
<intro confidence="0.360402">12</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas Bruckschlegel</author>
</authors>
<date>2005</date>
<journal>Microbenchmarking C++, C#, and Java. C/C++ Users Journal.</journal>
<contexts>
<context position="2933" citStr="Bruckschlegel, 2005" startWordPosition="461" endWordPosition="462">have placed well at recent competitive evaluations, achieving second place for Arabic to English translation on the NIST 2009 constrained data track.1 We implemented the toolkit in Java because it offers a good balance between performance and developer productivity. Compared to C++, developers using Java are 30 to 200% faster, produce fewer defects, and correct defects up to 6 times faster (Phipps, 1999). While Java programs were historically much slower than similar programs written in C or C++, modern Java virtual machines (JVMs) result in Java programs being nearly as fast as C++ programs (Bruckschlegel, 2005). Java also allows for trivial code portability across different platforms. In the remainder of the paper, we will highlight various useful capabilities, components and modeling features included in the toolkit. 2 Toolkit The toolkit provides end-to-end support for the creation and evaluation of machine translation models. Given sentence-aligned parallel text, a new translation system can be built using a single command: java edu.stanford.nlp.mt.CreateModel \ (source.txt) (target.txt) \ (dev.source.txt) (dev.ref) (model_name) Running this command will first create word level alignments for the</context>
</contexts>
<marker>Bruckschlegel, 2005</marker>
<rawString>Thomas Bruckschlegel. 2005. Microbenchmarking C++, C#, and Java. C/C++ Users Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chang</author>
<author>H Tseng</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Discriminative reordering with Chinese grammatical relations features.</title>
<date>2009</date>
<booktitle>In SSST Workshop at NAACL.</booktitle>
<contexts>
<context position="2201" citStr="Chang et al., 2009" startWordPosition="340" endWordPosition="343">nd then combined in a seperate generation stage to arrive at the final target translation. While greatly enriching the space of models that can be used for phrasebased machine translation, Moses only allows features that can be defined at the level of individual words and phrases. The Phrasal toolkit provides easy-to-use APIs for the development of arbitrary new model features. It includes an API for extracting feature statistics from aligned bitexts and for incorporating the new features into the decoding model. The system has already been used to develop a number of innovative new features (Chang et al., 2009; Galley and Manning, 2008; Galley and Manning, 2009; Green et al., 2010) and to build translation systems that have placed well at recent competitive evaluations, achieving second place for Arabic to English translation on the NIST 2009 constrained data track.1 We implemented the toolkit in Java because it offers a good balance between performance and developer productivity. Compared to C++, developers using Java are 30 to 200% faster, produce fewer defects, and correct defects up to 6 times faster (Phipps, 1999). While Java programs were historically much slower than similar programs written</context>
<context position="11697" citStr="Chang et al., 2009" startWordPosition="1787" endWordPosition="1790">ensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained 11 to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction</context>
</contexts>
<marker>Chang, Tseng, Jurafsky, Manning, 2009</marker>
<rawString>P. Chang, H. Tseng, D. Jurafsky, and C.D. Manning. 2009. Discriminative reordering with Chinese grammatical relations features. In SSST Workshop at NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2227" citStr="Galley and Manning, 2008" startWordPosition="344" endWordPosition="347">a seperate generation stage to arrive at the final target translation. While greatly enriching the space of models that can be used for phrasebased machine translation, Moses only allows features that can be defined at the level of individual words and phrases. The Phrasal toolkit provides easy-to-use APIs for the development of arbitrary new model features. It includes an API for extracting feature statistics from aligned bitexts and for incorporating the new features into the decoding model. The system has already been used to develop a number of innovative new features (Chang et al., 2009; Galley and Manning, 2008; Galley and Manning, 2009; Green et al., 2010) and to build translation systems that have placed well at recent competitive evaluations, achieving second place for Arabic to English translation on the NIST 2009 constrained data track.1 We implemented the toolkit in Java because it offers a good balance between performance and developer productivity. Compared to C++, developers using Java are 30 to 200% faster, produce fewer defects, and correct defects up to 6 times faster (Phipps, 1999). While Java programs were historically much slower than similar programs written in C or C++, modern Java </context>
<context position="4091" citStr="Galley and Manning, 2008" startWordPosition="611" endWordPosition="614">unning this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner 1http://www.itl.nist.gov/iad/mig/tests /mt/2009/ResultsRelease/currentArabic.html 9 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 1: Chinese-to-English translation using discontinuous phrases. (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/sof</context>
<context position="8885" citStr="Galley and Manning, 2008" startWordPosition="1351" endWordPosition="1354"> msd-bidirectional-fe feature with a heirarchical reordering feature. 4 Features The toolkit includes the basic eight phrase-based translation features available in Moses as well as Moses’ implementation of lexical reordering features. In addition to the common Moses features, we also include innovative new features that improve translation quality. One of these features is a hierarchical generalization of the Moses lexical reordering model. Instead of just looking at the reordering relationship between individual phrases, the new feature examines the reordering of blocks of adjacent phrases (Galley and Manning, 2008) and improves translation quality when the material being reordered cannot be captured by single phrase. This hierarchical lexicalized reordering model is used by default in Phrasal and is responsible for the gains shown in Table 2 using the default features. To illustrate how Phrasal can effectively be used to design rich feature sets, we present an overview of various extensions that have been built upon the sion still almost exactly replicates this implementation when using only the baseline Moses features. To ensure this configuration of the decoder is still competitive, we compared it aga</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Quadratic-time dependency parsing for machine translation.</title>
<date>2009</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2253" citStr="Galley and Manning, 2009" startWordPosition="348" endWordPosition="351">e to arrive at the final target translation. While greatly enriching the space of models that can be used for phrasebased machine translation, Moses only allows features that can be defined at the level of individual words and phrases. The Phrasal toolkit provides easy-to-use APIs for the development of arbitrary new model features. It includes an API for extracting feature statistics from aligned bitexts and for incorporating the new features into the decoding model. The system has already been used to develop a number of innovative new features (Chang et al., 2009; Galley and Manning, 2008; Galley and Manning, 2009; Green et al., 2010) and to build translation systems that have placed well at recent competitive evaluations, achieving second place for Arabic to English translation on the NIST 2009 constrained data track.1 We implemented the toolkit in Java because it offers a good balance between performance and developer productivity. Compared to C++, developers using Java are 30 to 200% faster, produce fewer defects, and correct defects up to 6 times faster (Phipps, 1999). While Java programs were historically much slower than similar programs written in C or C++, modern Java virtual machines (JVMs) re</context>
<context position="10558" citStr="Galley and Manning, 2009" startWordPosition="1617" endWordPosition="1620">rget language do a poor job of modeling long distance syntactic relationships. For example, if there are a number of intervening words between a verb and its subject, n-gram language models will often not be of much help in selecting the verb form that agrees with the subject. The target side dependency language model feature captures these long distance relationships by providing a dependency score for the target translations produced by the decoder. This is done using an efficient quadratic time algorithm that operates within the main decoding loop rather than in a separate reranking stage (Galley and Manning, 2009). Discriminative Distortion The standard distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically sign</context>
</contexts>
<marker>Galley, Manning, 2009</marker>
<rawString>Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher Manning</author>
</authors>
<title>Improving phrase-based machine translation with discontiguous phrases.</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="5055" citStr="Galley and Manning, 2010" startWordPosition="761" endWordPosition="764">ng (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common but extensible feature API, which allows features to be written once and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well </context>
</contexts>
<marker>Galley, Manning, 2010</marker>
<rawString>Michel Galley and Christopher Manning. 2010. Improving phrase-based machine translation with discontiguous phrases. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved models of distortion cost for statistical machine translation. In</title>
<date>2010</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="2274" citStr="Green et al., 2010" startWordPosition="352" endWordPosition="355">arget translation. While greatly enriching the space of models that can be used for phrasebased machine translation, Moses only allows features that can be defined at the level of individual words and phrases. The Phrasal toolkit provides easy-to-use APIs for the development of arbitrary new model features. It includes an API for extracting feature statistics from aligned bitexts and for incorporating the new features into the decoding model. The system has already been used to develop a number of innovative new features (Chang et al., 2009; Galley and Manning, 2008; Galley and Manning, 2009; Green et al., 2010) and to build translation systems that have placed well at recent competitive evaluations, achieving second place for Arabic to English translation on the NIST 2009 constrained data track.1 We implemented the toolkit in Java because it offers a good balance between performance and developer productivity. Compared to C++, developers using Java are 30 to 200% faster, produce fewer defects, and correct defects up to 6 times faster (Phipps, 1999). While Java programs were historically much slower than similar programs written in C or C++, modern Java virtual machines (JVMs) result in Java programs</context>
<context position="11216" citStr="Green et al., 2010" startWordPosition="1717" endWordPosition="1720">d distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models</context>
</contexts>
<marker>Green, Galley, Manning, 2010</marker>
<rawString>Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statistical machine translation. In In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="4030" citStr="Koehn et al., 2003" startWordPosition="602" endWordPosition="605">target.txt) \ (dev.source.txt) (dev.ref) (model_name) Running this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner 1http://www.itl.nist.gov/iad/mig/tests /mt/2009/ResultsRelease/currentArabic.html 9 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 1: Chinese-to-English translation using discontinuous phrases. (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1332" citStr="Koehn et al., 2007" startWordPosition="197" endWordPosition="200">language models. Other useful utilities packaged with the toolkit include: a conditional phrase extraction system that builds a phrase table just for a specific dataset; and an implementation of MERT that allows for pluggable evaluation metrics for both training and evaluation with built in support for a variety of metrics (e.g., TERp, BLEU, METEOR). 1 Motivation Progress in machine translation (MT) depends critically on the development of new and better model features that allow translation systems to better identify and construct high quality machine translations. The popular Moses decoder (Koehn et al., 2007) was designed to allow new features to be defined using factored translation models. In such models, the individual phrases being translated can be factored into two or more abstract phrases (e.g., lemma, POStags) that can be translated individually and then combined in a seperate generation stage to arrive at the final target translation. While greatly enriching the space of models that can be used for phrasebased machine translation, Moses only allows features that can be defined at the level of individual words and phrases. The Phrasal toolkit provides easy-to-use APIs for the development o</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="4918" citStr="Koehn, 2004" startWordPosition="745" endWordPosition="746">ecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common but extensible feature API, which allows features to be written once and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The METEOR metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<contexts>
<context position="12097" citStr="Lavie and Denkowski, 2009" startWordPosition="1848" endWordPosition="1851">riminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained 11 to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Ext</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The METEOR metric for automatic evaluation of machine translation. Machine Translation, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="3938" citStr="Liang et al., 2006" startWordPosition="587" endWordPosition="590">m can be built using a single command: java edu.stanford.nlp.mt.CreateModel \ (source.txt) (target.txt) \ (dev.source.txt) (dev.ref) (model_name) Running this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner 1http://www.itl.nist.gov/iad/mig/tests /mt/2009/ResultsRelease/currentArabic.html 9 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Figure 1: Chinese-to-English translation using discontinuous phrases. (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="12153" citStr="Nießen et al., 2000" startWordPosition="1858" endWordPosition="1861">re examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained 11 to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assist in the development of new</context>
</contexts>
<marker>Nießen, Och, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, and Hermann Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5787" citStr="Och and Ney, 2003" startWordPosition="877" endWordPosition="880">ce and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. tranlations per minute 15 25 35 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared between the threads is starting to overwhelm the value provided by each additional thread. Moses also does not run faster with more than 4-5 threads.3 Feature API The feature API was designed to abstract away complex implementation details of the underlying decoding engi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4444" citStr="Och, 2003" startWordPosition="675" endWordPosition="676">l Linguistics Figure 1: Chinese-to-English translation using discontinuous phrases. (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Man</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4504" citStr="Papineni et al., 2002" startWordPosition="683" endWordPosition="686">lation using discontinuous phrases. (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common b</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Phipps</author>
</authors>
<title>Comparing observed bug and productivity rates for java and C++.</title>
<date>1999</date>
<journal>Softw. Pract. Exper.,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2720" citStr="Phipps, 1999" startWordPosition="426" endWordPosition="427">ystem has already been used to develop a number of innovative new features (Chang et al., 2009; Galley and Manning, 2008; Galley and Manning, 2009; Green et al., 2010) and to build translation systems that have placed well at recent competitive evaluations, achieving second place for Arabic to English translation on the NIST 2009 constrained data track.1 We implemented the toolkit in Java because it offers a good balance between performance and developer productivity. Compared to C++, developers using Java are 30 to 200% faster, produce fewer defects, and correct defects up to 6 times faster (Phipps, 1999). While Java programs were historically much slower than similar programs written in C or C++, modern Java virtual machines (JVMs) result in Java programs being nearly as fast as C++ programs (Bruckschlegel, 2005). Java also allows for trivial code portability across different platforms. In the remainder of the paper, we will highlight various useful capabilities, components and modeling features included in the toolkit. 2 Toolkit The toolkit provides end-to-end support for the creation and evaluation of machine translation models. Given sentence-aligned parallel text, a new translation system</context>
</contexts>
<marker>Phipps, 1999</marker>
<rawString>Geoffrey Phipps. 1999. Comparing observed bug and productivity rates for java and C++. Softw. Pract. Exper., 29(4):345–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B J Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In SMT workshop at EACL.</booktitle>
<contexts>
<context position="12125" citStr="Snover et al., 2009" startWordPosition="1853" endWordPosition="1856"> grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained 11 to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assi</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz. 2009. Fluency, adequacy, or HTER?: exploring different human judgments with a tunable MT metric. In SMT workshop at EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based search for statistical translation. In</title>
<date>1997</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="12186" citStr="Tillmann et al., 1997" startWordPosition="1864" endWordPosition="1867">rds in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained 11 to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assist in the development of new features, the toolkit provides a</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based search for statistical translation. In In Eurospeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>