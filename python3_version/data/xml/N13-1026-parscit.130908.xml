<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985103">
Measuring Term Informativeness in Context
</title>
<author confidence="0.99899">
Zhaohui Wu
</author>
<affiliation confidence="0.9916275">
Computer Science and Engineering
The Pennsylvania State University
</affiliation>
<address confidence="0.66863">
University Park, PA 16802, USA
</address>
<email confidence="0.982746">
zzw109@psu.edu
</email>
<author confidence="0.959566">
C. Lee Giles
</author>
<affiliation confidence="0.981061333333333">
Information Sciences and Technology
Computer Science and Engineering
The Pennsylvania State University
</affiliation>
<address confidence="0.671146">
University Park, PA 16802, USA
</address>
<email confidence="0.994603">
giles@ist.psu.edu
</email>
<sectionHeader confidence="0.982796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999842315789474">
Measuring term informativeness is a funda-
mental NLP task. Existing methods, mostly
based on statistical information in corpora, do
not actually measure informativeness of a term
with regard to its semantic context. This pa-
per proposes a new lightweight feature-free
approach to encode term informativeness in
context by leveraging web knowledge. Given
a term and its context, we model context-
aware term informativeness based on semantic
similarity between the context and the term’s
most featured context in a knowledge base,
Wikipedia. We apply our method to three ap-
plications: core term extraction from snippets
(text segment), scientific keywords extraction
(paper), and back-of-the-book index genera-
tion (book). The performance is state-of-the-
art or close to it for each application, demon-
strating its effectiveness and generality.
</bodyText>
<sectionHeader confidence="0.995146" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966957446809">
Computationally measuring importance of a word
in text, or “term informativeness” (Kireyev, 2009;
Rennie and Jaakkola, 2005), is fundamental to many
NLP tasks such as keyword extraction, text catego-
rization, clustering, and summarization, etc. Various
features derived from statistical and linguistic infor-
mation can be helpful in encoding term informative-
ness, whereas practical feature definition and selec-
tion are usually ad hoc, data-driven and application
dependent. Statistical information based on term
frequency (TF) and document frequency (DF) tend
to be more effective in finding keywords in large
corpora, but can have issues with small amounts of
text or small corpora. Linguistic information such
as POS tag patterns often require manual selection
based on prior applications. We contend that few
methods actually measure the informativeness of a
term to the discourse unit it contains. For example,
given a context such as “A graph comprises nodes
(also called vertices) connected by links (also known
as edges or arcs)”, it is difficult to measure the
term informativeness of “graph”, “nodes”, or “links”
based on any statistical or linguistic information.
This raises many issues. Is there a fundamental
and less ad hoc way to measure the term informa-
tiveness of a word within a discourse unit? Can we
actually find a general approach based on compre-
hensive and high-level “knowledge” and not have
to nitpick over features? Can this new metric be
effectively applied to real world applications? To
answer these questions, we develop a new term in-
formativeness metric, motivated by query-document
relevance in information retrieval. The higher the
relevance score a query-document pair is, the more
informative the query is to the document. If a sim-
ilar principle also exists between word and con-
text and there is an effective search engine return-
ing ranked contexts for a given word, then we con-
tend that word is more informative in the higher rank
contexts. To see the term informativeness of three
words “graph”, “nodes” and “links” in context, we
manually check the search results from Wikipedia,
Google, and Bing. We found that very similar con-
texts are among the top 5 ranked results of “graph”
while no such contexts appear in that of the other
two words. Thus, we define a context-aware term
informativeness based on the semantic relatedness
</bodyText>
<note confidence="0.727630666666667">
259
Proceedings of NAACL-HLT 2013, pages 259–269,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999825444444444">
between the context and the term’s featured contexts
(or the top important contexts that cover most of a
term’s semantics).
We apply the context-aware term informativeness
(CTI) to three typical NLP applications: core term
extraction in snippets, keyword extraction and back-
of-the-book index generation. Experiments show
that the method is effective and efficient. Moreover,
the metric can be easily combined with other meth-
ods, or as a feature for learning algorithms.
The remainder of this paper is organized as fol-
lows. Section 2 reviews the literature of term infor-
mativeness measurements. Section 3 proposes the
formal definition of the context-aware term informa-
tiveness as well as its practical implementation using
Web knowledge. Section 4 studies the three appli-
cations. Finally, we conclude with discussion and
future work.
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999561666666666">
Most known approaches to measure term informa-
tiveness fall into basically two categories: statistics-
based and semantic-based.
Statistics-based methods, such as TFIDF (Salton
and Buckley, 1988), ResidualIDF(RIDF), Variance,
Burstiness and Gain, are based on derivations from
term frequency (TF) and document frequency (DF).
Sprck Jones defines IDF or inverse document fre-
quency as:
</bodyText>
<equation confidence="0.999413">
IDF(w) = −log2(dfw/D) (1)
</equation>
<bodyText confidence="0.99400275">
where D is the size of the corpus (Jones, 1972;
Jones, 1973). Based on a finding that informative
words tend to have large deviation between IDF
and collection frequency fw(the total number of oc-
currence of a word), many other informativeness
scores have been proposed. Bookstein and Swan-
son (Bookstein and Swanson, 1974) introduced the
xI as:
</bodyText>
<equation confidence="0.8080765">
XI = fw − dfw
Church and Gale (1995) introduced
</equation>
<bodyText confidence="0.947767">
where tdw denotes w’s TF in d and ¯tw = fw/D indi-
cates its mean expected word rate. Another measure
</bodyText>
<equation confidence="0.95265">
suggested by them is
burstiness(w) = fw (3)
dfw
</equation>
<bodyText confidence="0.999953833333333">
which tends to compare collection frequency and
document frequency directly. Informative words
were found to have IDF scores that are larger than
what would be expected according to the Poisson
model; residual IDF (RIDF) was introduced to mea-
sure this deviation
</bodyText>
<equation confidence="0.993876142857143">
RIDF(w) = IDF(w) − IDF(w) (4)
where �
IDF(w) = −log2(1 − e−�t-). In addition,
Papineni (2001) introduced the notion of gain as
dfw
(dfw D D f 1
gain(w) = — 1 — log() J (5)
</equation>
<bodyText confidence="0.9999114">
More recently, Rennie and Jaakkola (2005) intro-
duced an informativeness score based on the fit of
a word’s frequency to a mixture of 2 Unigram dis-
tribution and applied it to named entity detection. It
is worth noting that term necessity, which measures
the probability that a term occurs in documents rel-
evant to a given query, has been well studied in In-
formation Retrieval community (Zhao and Callan,
2010; Yang and Callan, 2010). Though our CIT is
not designed for probabilistic retrieval models, we
may apply it to measure the term necessity in a query
by considering it as a context.
Despite extensive research on semantic analysis
and understanding of word and text (Deerwester et
al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and
Vitanyi, 2007; Gabrilovich and Markovitch, 2007;
Agirre et al., 2009; Yazdani and Popescu-Belis,
2012), little work studied the measurement of the
semantics of term informativeness. An exception
is the LSAspec from Kireyev (2009), based on la-
tent semantic analysis (Deerwester et al., 1990),
which is defined as the ratio of a term’s LSA vec-
tor length to its document frequency and thus can
be interpreted as the rate of vector length growth.
However, latent semantic models such as LSA are
notoriously hard to interpret since the “latent con-
cepts” cannot be readily mapped to human knowl-
edge (Gabrilovich and Markovitch, 2007). Our ap-
proach explicitly leverages the semantics of word
and text using existing knowledge bases.
</bodyText>
<equation confidence="0.887724666666667">
variance(w) = 1 D (tdw − ¯tw) (2)
D − 1d_1
260
</equation>
<bodyText confidence="0.99285325">
Previous methods, all corpus-based, might be ef-
fective in identifying informative words at the doc-
ument or corpus level, but do not the ability to cap-
ture term informativeness in a particular context due
to their absence of semantics and obliviousness of
context. Our method measures the term informative-
ness within a context in a semantic-based approach,
regardless of the absence of statistical information.
</bodyText>
<sectionHeader confidence="0.812888" genericHeader="method">
3 Context-aware Term Informativeness
</sectionHeader>
<subsectionHeader confidence="0.998004">
3.1 Context
</subsectionHeader>
<bodyText confidence="0.999867866666667">
A context of a word or phrase may refer to a few
words nearby (He et al., 2010), a sentence or para-
graph (Soricut and Marcu, 2003), or even a set
of documents containing it (Cilibrasi and Vitanyi,
2007). Here we define context as a syntactic unit
of discourse such as a sentence or paragraph, for ex-
ample, “PL/SQL is one of three key programming
languages embedded in the Oracle Database”, or
“There are two types of functions in PL/SQL”. The
universal context set U(t) of a word t is defined as
all the contexts containing it in the web. Different
contexts vary in their authority just like web pages
vary. For the two examples, we could argue that
the first context is much more “authoritative” than
the second. This can be verified by their popular-
ity on Google; (all results from actual search en-
gines were at the time of this publication) the first
retrieves approximately 302,000 exact matching re-
sults while the second retrieves only one. We con-
sider this as the number of citations of a context,
which, to some extent, indicates its “authority”. We
define the source of a context as the set of all docu-
ments citing it. Here “citing” instead of “containing”
is used because some documents may not literally
contain an exact copy of the context.
Given a term t, define its universal context set
U(t) = {ci} and the source of ci is S(ci) = {dij}.
Ideally, the authority of a context will be contributed
by every document citing it. Therefore, we define
the authority score of a context as
</bodyText>
<equation confidence="0.9894995">
CA(ci) = � DA(dij) (6)
j
</equation>
<bodyText confidence="0.999966846153846">
where DA(dij) denotes the authority contributed by
dij. It is very difficult to acquire the universal con-
text set of a term. Considering that usually we only
care about the top few results of a query returned by
search engines and ignore a large faction of less im-
portant ones, it is reasonable to assume that a term’s
semantics will be well covered by a few important
contexts. We therefore define the featured context
set of term t, or Uf(t), as the top k contexts with
the highest authority scores, where k is an applica-
tion dependent parameter. In our experiments, the
default k for the Wikipedia based implementation is
20.
</bodyText>
<subsectionHeader confidence="0.998648">
3.2 Term Informativeness
</subsectionHeader>
<bodyText confidence="0.988595333333333">
We now consider how to measure the term informa-
tiveness in context. Using the context “PL/SQL is
one of three key programming languages embedded
in the Oracle Database” (denoted by Cp) as an ex-
ample, for its term “PL/SQL”, the top three contexts
returned by Google are
</bodyText>
<listItem confidence="0.991800375">
1. PL/SQL (Procedural Language/Structured Query Language) is
Oracle Corporation’s procedural extension language for SQL and
the Oracle relational database.
2. PL/SQL is Oracle’s procedural extension to industry-standard
SQL. PL/SQL naturally, efficiently, and safely extends SQL.
3. This Oracle PL SQL tutorial teaches you the basics of program-
ming in PL/SQL like cursors, stored procedures, PlSQL func-
tions.
</listItem>
<bodyText confidence="0.999428642857143">
Those contexts, though being diverse in actual
meaning, all have semantic relatedness to Cp. Even
someone who does not completely understand them
can gain some meaning by observing common
words such as “Oracle”, “database” and “program-
ming”. However, checking the Google results for
“Oracle Database” or “programming languages”, we
will find little relatedness between them and Cp.
This suggests that if term ta in context c is more in-
formative than tb, then most likely the contexts from
ta’s featured context set will be more related to c
than will tb. Thus, given a term t and its featured
context set Uf(t) = {c1, ..., ck}, we define the term
informativeness of t in context ci as
</bodyText>
<equation confidence="0.9989845">
I(t, ci) = � l,(ci,cj) · CA(cj) (7)
cj∈Uf (t)
</equation>
<bodyText confidence="0.99991575">
where l,(ci,cj) is the semantic relatedness of
ci and cj, which can be computed by various
semantic relatedness metrics such as Wikipedia
based (Gabrilovich and Markovitch, 2007; Yazdani
</bodyText>
<page confidence="0.50464">
261
</page>
<bodyText confidence="0.999514818181818">
and Popescu-Belis, 2012), Wordnet based (Agirre et
al., 2009; Budanitsky and Hirst, 2006), or simple co-
sine similarity and Jaccard similarity based (Zobel
and Moffat, 1998).
The context-aware term informativeness (CTI) in-
troduced above is a formal and general definition.
As such the definition in Equation (7) includes sev-
eral features such as context authority score, fea-
tured context set, semantic relatedness, and knowl-
edge base, any or all of which could be flexible for
different applications.
</bodyText>
<subsectionHeader confidence="0.998716">
3.3 Implementation
</subsectionHeader>
<bodyText confidence="0.999946166666667">
Here, we present a simple practical implementation
using Wikipedia as the knowledge base and the con-
text authority estimated by the discounted rank of
the Wikipedia document. Note that the problem is
how to compute CA(cj) for each context in Uf. We
rewrite Equation (6) as
</bodyText>
<equation confidence="0.997426">
CA(ci) = DA(di0) + � DA(dij) (8)
j̸�0
</equation>
<bodyText confidence="0.999987222222222">
where di0 is the original document of ci and all the
others are further derivatives of “citing” ci. For ex-
ample, the Wikipedia page of “PL/SQL” will be con-
sidered as the original document of Cp while all
other documents citing Cp are its derivatives. Intu-
itively, the authority of a context will mainly rely on
the authority of its original document. Here, we sim-
ply assume that the context authority depends only
on its original document, or
</bodyText>
<equation confidence="0.993282">
CA(ci) Pt� DA(di0) (9)
</equation>
<bodyText confidence="0.999965384615385">
We then take the top ranked document returned by
the web knowledge base as the original document.
We present a practical implementation of CTI in
Algorithm 1. The discounted rank is used to rep-
resent the relative context authority score of each
context in Uf. We use Wikipedia as our knowl-
edge base to implement the metric since it is cur-
rently one of the largest and most readily available
knowledge repositories and, more importantly, pro-
vides free, unlimited and fast query APIs1. Given
any keyword, the Wikipedia query API will return
the ranked Wikipedia entries along with the contexts
containing the keyword. We set the default value 20
</bodyText>
<footnote confidence="0.405729">
1http://www.mediawiki.org/wiki/API:Query
</footnote>
<bodyText confidence="0.999917434782609">
for k, or len(Uf). Note that there could be other
variations of this implementation. For example, we
could rule out duplicate or very similar results in the
Uf. Search engines such as Google and Bing are
also potential sources since they return high qual-
ity web pages along with the contexts containing the
query keyword.
In terms of scalability, the proposed method is
inherently parallelizable, not only at the document
level, but also a the context level, since computing
CTI does not depend on any other context in the doc-
ument. In addition, we do not need to issue the same
query more than once. Our strategy is to locally
cache the returned results of every seen query. For a
new term seen in a previous query, we can directly
access the local cached file. If we have built a large
local pool, the queries will rarely go to a search en-
gine or other source. Given a corpus size N (words
in total), the number of actual issued queries will
be at most the number of unique terms, which is far
less than O(N). Of course, new terms never seen will
have to be processed, but there will be fewer of these
over time.
</bodyText>
<listItem confidence="0.68002975">
Algorithm 1: Wikipedia-based I(t, ci)
1 Input: t, ci
2 Output: I(t, ci)
3 begin
</listItem>
<equation confidence="0.988238125">
I S 0;
Uf ( queryWikipedia(t);
for j C range(len(Uf)) do
s ( r(ci, Uf[jD;
if j &gt; 0 then
I ( I + s/ log(j + 1);
else I ( I + s
return I;
</equation>
<sectionHeader confidence="0.997008" genericHeader="method">
4 Applications
</sectionHeader>
<subsectionHeader confidence="0.999903">
4.1 Core Terms Extraction from Snippets
</subsectionHeader>
<bodyText confidence="0.9989655">
We first investigate CTI in a well defined setting.
That is, if we have a collection of terms such that
its most important context is a ”definition,” e.g.
“database” and “A database is a structured collec-
tion of data, which are typically organized to model
relevant aspects of reality, in a way that supports
</bodyText>
<figure confidence="0.933577666666667">
4
5
6
7
8
9
10
11
262
</figure>
<table confidence="0.99193325">
Exemplary snippets of computer science terms Top 5 terms ranked by CTI
Acrobat, a document exchange software from Adobe Systems, provides a platform-independent means of Acrobat:3.19
creating, viewing, and printing documents. Acrobat can convert a DOS, Windows, UNIX or Macintosh Acrobat reader:2.94
documents into a Portable Document Format (PDF) which can be displayed on any computer with an Portable Document Format:2.08
Acrobat reader. The Acrobat reader can be downloaded free from the Adobe website. Adobe website:2.03
Adobe Systems:1.82
Data mining (DM), also known as Knowledge-Discovery in Databases (KDD) or Knowledge-Discovery data mining:3.77
and Data Mining (KDD), is the process of automatically searching large volumes of data for patterns. Data data mining techniques:3.64
mining uses automated data analysis techniques to uncover previously undetected relationships among KDD:1.79
data items. Data mining often involves the analysis of data stored in a data warehouse. Three of the major Knowledge-Discovery:1.66
data mining techniques are regression, classification and clustering. data analysis techniques:1.20
Firefox, also known as Mozilla Firefox, is a free, open source, cross-platform, graphical web browser Mozilla Firefox:3.89
developed by the Mozilla Corporation and hundreds of volunteers. Firefox includes an integrated pop-up firefox:3.13
blocker, tabbed browsing, live bookmarks, support for open standards, and an extension mechanism for web browser:2.44
adding functionality. Although other browsers have some of these features, Firefox became the first such browser:2.39
browser to include them all and achieve wide adoption. graphical web browser:2.35
</table>
<tableCaption confidence="0.999886">
Table 1: Term ranked by CTI from exemplary snippets
</tableCaption>
<bodyText confidence="0.999968523809524">
processes requiring this information”, can CTI iden-
tify “database” as the most informative term in this
context? To construct the term-context pairs, we
could use the Wikipedia title and the top ranked
context returned by searching the title using the
Wikipedia API. Then we could test our metric based
on other search engines such as Google or Bing.
Testing manually, we found the results compare well
to the search engine results, since both Google and
Bing give top ranks to Wikipedia pages if the query
keyword is a Wikipedia title. For further analy-
sis, we need a collection of term-context pairs from
other sources different from Wikipedia. Fortunately,
we found a list of 1255 computer science terms
with its definition snippets manually created by Web
users 2. The snippets are literally different from
those contexts in Wikipedia and some of the terms
are even not Wikipedia titles, e.g. bBlog, BetBug,
etc. These can be part of an “initial” evaluation. The
core term extraction algorithm works in the follow-
ing steps for each term-context pair:
</bodyText>
<listItem confidence="0.9977066">
1. Extract all n-grams (1 &lt; n &lt; 4) in the context
as candidates
2. For each candidate, calculate its CTI using
Wikipedia based implementation
3. Return the top K highest CTI as core terms
</listItem>
<bodyText confidence="0.751414666666667">
We used the top 20 returned Wikipedia contexts
as a featured context set U f and apply the cosine
similarity for n. We show some exemplary snippets
</bodyText>
<table confidence="0.999809571428571">
K Precision (%) Recall (%) F1(%)
1 37.5 37.5 37.5
2 35.1 55.2 42.9
3 32.3 64.7 43.1
4 31.3 72.2 43.7
5 27.6 76.3 40.5
10 20.0 88.1 32.6
</table>
<tableCaption confidence="0.795575">
Table 2: Results on computer science term extrac-
tion from descriptive snippets
</tableCaption>
<bodyText confidence="0.99977505882353">
with its top 5 core terms and their CTI scores in Ta-
ble 1. The overall performance is shown in Table 2,
in terms of precision, recall and F1 scores based on
the only one titled term of each snippet as the ground
truth. CTI can correctly find the core term for 37.5%
snippets. If we take the top 5 results, then the recall
increase to 76.3%.
Though the algorithm can be easily parallelized,
sequentially runtime on all snippets took only
slightly more than a minute on a 2.35GHz Intel(R)
Xeon(R) 4 processors, 23GB of RAM, and Red Hat
Enterprise Linux Server(5.7) machine. However, the
time could vary due to network conditions.
Though these results look promising, but it could
be due to the high lexical similarity between this
dataset and Wikipedia content. To test on a more
general corpora, we explore more real world tasks.
</bodyText>
<subsectionHeader confidence="0.991117">
4.2 Keyword Extraction
</subsectionHeader>
<bodyText confidence="0.992209">
There is a rich literature on keyword extraction prob-
lem (Frank et al., 1999; Witten et al., 1999; Turney,
2000; Hulth et al., 2003; Tomokiyo and Hurst, 2003;
</bodyText>
<table confidence="0.770863">
2http://www.labautopedia.org/mw/index.php/List of
programming and computer science terms
263
Wiki20 citeulike180
Method P R F P R F
TFIDF 13.7 17.8 15.5 14.4 16.0 15.2
KEA 18.4 21.5 19.8 20.4 22.3 21.3
CTI 19.6 22.7 21.0 18.5 21.4 19.8
</table>
<tableCaption confidence="0.990447">
Table 3: Results on Wiki20 and citeulike180
</tableCaption>
<table confidence="0.9999628">
Method Precision (%) Recall (%) F1(%)
TFIDF 14.9 15.3 15.1
HUMB 27.2 27.8 27.5
CTI 19.3 20.1 19.7
CTI+ 25.3 26.2 25.7
</table>
<tableCaption confidence="0.999767">
Table 4: Results on SemEval2010
</tableCaption>
<bodyText confidence="0.998764111111111">
Mihalcea and Tarau, 2004; Medelyan and Witten,
2008; Liu, 2010), most of which is treated as a clas-
sification or ranking problem with corresponding
machine learning algorithms that use statistical and
linguistic features in a corpus. Here, we consider
the task as finding the most informative keywords in
a document. Given a document d = {ci}, our key-
word extraction algorithm based on CTI works as
follows.
</bodyText>
<listItem confidence="0.963560833333333">
1. For each context ci in a document, compute the
semantic relatedness s(ci, d) between ci and d
2. For each n-gram (1 &lt; n &lt; 4) t in ci, calculate
I(t, ci) using Wikipedia based implementation
3. Select the top keywords with the highest
Ei I(t, ci) * s(ci, d)
</listItem>
<bodyText confidence="0.999725857142857">
Note that for the last step keywords are selected
based on a summarized weighted informativeness
score over a document. Obviously, the pure co-
sine or Jaccard similarity is not a good choice
to measure semantic relatedness between two text
segments of very low lexical similarity. We thus
use the Wikipedia based ESA (Gabrilovich and
Markovitch, 2007) to compute the semantic relat-
edness s(ci, d) and n(ci, cj). To make the cal-
culation more efficient, only the Wikipedia pages
whose title is contained in the dataset are used to
build the concept space. We ran the algorithm on
several datasets including Wiki20 (Medelyan et al.,
2008), citeulike180 (Medelyan et al., 2009) and Se-
mEval2010 (Kim et al., 2010) 3.
Though keyword extraction as a research topic
has a rich literature, to the best of our knowledge
there is no large scale datasets publicly available.
The Wiki20 dataset contains 20 computer science
articles each with around 5 terms labeled by 15
different teams. Every term is a Wikipedia title.
</bodyText>
<subsectionHeader confidence="0.271388">
3http://code.google.com/p/maui-indexer/downloads/list
</subsectionHeader>
<bodyText confidence="0.999959820512821">
The citeulike180 contains a set of 180 papers each
tagged with around three tags by 332 users. For each
dataset, the collection of all labeled keywords by dif-
ferent taggers are considered as the gold standard
for a document. We use the set of all keywords for
evaluation; otherwise a more complicated evaluation
metrics for each dataset will be needed. It would
be better to investigate other weighting schemes.
However, the datasets here are relatively small and
the number of tags on which at least two annota-
tors agreed is significantly small; weighting the key-
words might not make too much difference. KEA 4
builds a Naive Bayes model using features TFIDF,
first occurrence, length of a phrase, and node de-
gree (number of candidates that are semantically re-
lated to this phrase) (Witten et al., 1999). First oc-
currence is computed as the percentage of the doc-
ument preceding the first occurrence of the term in
the document. We compute the node degree as the
textrank (Mihalcea and Tarau, 2004) degree in a doc-
ument by simply relating two candidate terms with
each other if they are in the same context. KEA
uses 5 fold cross validation. All precision P, re-
call R and F1 F results are over the top 10 candi-
date keywords and the micro-averaged results of the
first two datasets are shown in Table 3. The CTI-
based algorithm works better than KEA on Wiki20
but slightly worse on citeulike180. We argue that
the reason might be two-fold. First, CTI does not
use any inter-document or corpus information while
KEA learns from the corpus. As such, CTI might not
perform as well as supervised learning methods for a
domain dependent large corpus. Second, the labeled
keywords in Wiki20 are all Wikipedia titles while
those in citeulike are general tags labeled by volun-
tary web users. CTI would give more preference to
Wikipedia titles since their featured context set re-
turned from Wikipedia is more semantically repre-
sentative than other non-Wikipedia title words.
</bodyText>
<footnote confidence="0.768198">
4http://www.nzdl.org/Kea/
</footnote>
<page confidence="0.486579">
264
</page>
<table confidence="0.993558">
Dataset #Books #Words #Contexts Main domains
Gutenberg 55 7,164,463 301,581 History, Art, Psychology, Philosophy, Literature, Zoology
Open Book 213 22,279,530 1,135,919 Computer Science, Engineering, Information Science
</table>
<tableCaption confidence="0.987994">
Table 5: Datasets for book index generation evaluation
</tableCaption>
<bodyText confidence="0.999935">
The SemEval2010 dataset contains a set of 284
scientific papers with 15 keyphrases assigned by
readers and authors. 144 of them are selected as
training set while the other 100 are for testing. A
comparison of CTI to the results from TFIDF and
the best reported results HUMB (Lopez and Romary,
2010) is shown in Table 4. It achieves 19.8% by
micro-averaged F1 score, ranking 11th out of the 19
systems submitted to the competition (Kim et al.,
2010). However, by adding the structural features
used by HUMB into CTI, we can improve the per-
formance by around 6%, making our results close
to that of HUMB. The structural information is en-
coded as weights for context that is located in ti-
tle, abstract, section titles and general content. Each
weight can be regarded as the prior probability that a
keyword will appear in the corresponding location,
whose value can be set according to the fraction of
the number of keyword occurrences of this type of
location with respect to the number of all keyword
occurrences in the entire training set. Here they are
set to be 0.3, 0.4, 0.25, and 0.05.
</bodyText>
<subsectionHeader confidence="0.998986">
4.3 Back-of-the-book Index Generation
</subsectionHeader>
<bodyText confidence="0.999944625">
A back-of-the-book index (or book index) is a col-
lection of words or phrases, often alphabetically ar-
ranged as an index, created to give readers impor-
tant location of important information in a given
book. Usually indexing is done by freelancers hired
by authors or publishers, namely professional in-
dexers 5. Csomai and Mihalcea first evaluated the
performance of different informativeness measure-
ments for selecting book index terms (2007) and
then investigated automatic book index generation
in a supervised learning framework (2008) using
syntactic features, linguistical features, encyclope-
dic features, etc., as a keyword extraction problem
rather than building a actual book index.
A set of keywords is not a back-of-the-book in-
dex. What really matters for such an index is that
</bodyText>
<footnote confidence="0.391234">
5http://www.asindexing.org/
</footnote>
<bodyText confidence="0.9999816">
an index term or phrase points to its proper loca-
tion in the text. For example, in “pattern recognition
and machine learning” by Bishop, “hidden Markov
model” appears in more than 20 pages while the
actual index entry has only 2 pages as its locators.
Thus the actual problem is to identify a index term
with its context. As such, learning a robust and ef-
ficient model for real book indexes is challenging.
First, books from different domains vary in vocabu-
lary composition and structure style, requiring vari-
ous indexing specialties. There are different index-
ing guides for medicine (Wyman, 1999), psychol-
ogy (Hornyak, 2002), and law (Kendrick and Zafran,
2001). Second, book indexing is a highly subjec-
tive work and indexes of different books are always
created by different professional indexers who have
their own preferences and background (Diodato and
Gandt, 1991; Diodato, 1994). Third, the training
set is extremely unbalanced. As we found in our
dataset, the index size is only 0.42% of the length of
book on average. All these motivate us to explore the
automatic creation of index terms that are aware of
the context at the term’s locations (locators). To do
so we propose the following efficient training-free
and domain independent approach:
</bodyText>
<listItem confidence="0.984384333333333">
1. For each context ci in a book, compute its
weight wi based on structural features
2. For each candidate term t in ci, calculate
I(t, ci) using Wikipedia based implementation
3. Select term-context pairs with the highest wi �
I(t, ci) as index entries
</listItem>
<bodyText confidence="0.8870449">
The weight in step 1 represents the relative im-
portance of a context in a book. w(c) = 1 −
cid (c)−cid(title,)
Ntitlec measures the weight based on the
le�
normalized distance from the context to its direct
chapter or sub-chapter title, where cid(c) denotes the
id of context c, titlec the title of context c and Ntitlec
the number of contexts under titlec. To select candi-
date terms, we first filter the improbable index terms
</bodyText>
<page confidence="0.773746">
265
</page>
<bodyText confidence="0.999984133333334">
based on POS patterns using the Standard POS Tag-
ger (Toutanova et al., 2003). We then select multi-
word keyphrases based on Pointwise Mutual Infor-
mation (PMI) (Church and Hanks, 1990), which was
shown to be the best metric to measure word associ-
ations (Terra and Clarke, 2003).
To evaluate our back-of-the-book index gener-
ation method, we conduct extensive experiments
on books in various domains, from the Gutenberg
dataset and the open book dataset described in Ta-
ble 5. The first one was created by (Csomai and
Mihalcea, 2006), containing 55 free books collected
from Gutenburg6. Since the dataset does not pro-
vide the locators of index terms, we can only serve
the evaluation as a keyword extraction task. The sec-
ond dataset was collected from CiteSeer repository,
most of which are in computer science and engineer-
ing. We extracted the paged body text and the back
index using Pdfbox7. Having each index term asso-
ciated with its locators (page numbers), we can per-
form an evaluation for different methods, not based
solely on keyword extraction.
We first compare CTI with other metrics on both
datasets for keywords extraction since all other met-
rics are context-oblivious. CTI selects index terms
based on the sum of a term’s CTI scores over all its
contexts, the same as the algorithm used in Section
4.2. The results are shown in Table 6, where the in-
dex size = n indicates the number of output terms is
n times of the true book index size for each book.
The scores are the average recall over a dataset.
The CTI outperforms all other 7 metrics in the two
datasets as the output index size increases. More-
over, results show that TF and TFIDF are better than
RIDF in identifying book index terms, which seems
contradictory to previous findings (Church and Gale,
1995). A possible reason is that a book is much
longer than a regular document thus enhancing TF
as a better indicator of keywords but weakening the
role of IDF. We believe this is why Variance, Gain,
and Burstiness, which relies on DF, are less effec-
tive here. Wikipedia keyphraseness (Csomai and Mi-
halcea, 2008) can only find a small fraction of index
terms because it emphasizes Wikipedia titles that
have high in-degree in hyper-link network formed
</bodyText>
<figure confidence="0.835877333333333">
6www.gutenberg.org/
7pdfbox.apache.org/
(c) SLD (d) size of featured context set
</figure>
<figureCaption confidence="0.99979">
Figure 1: Results for book index generation
</figureCaption>
<bodyText confidence="0.9985667">
by Wikipedia terms. However, a book index covers
much broader terms not titled in Wikipedia.
We then compare with three baselines TFIDF,
KEA, and SLD (supervised learning using decision
tree in Csomai’s (2008)) on the second dataset. For
SLD, we use all the features except the discourse
comprehension based ones which were too com-
plicate to implement. We choose a decision tree
because its training is much faster than the other
two models while its performance is quite close to
the best. We follow Csomai’s setting to choose
90%(192) books for training and the other 10%(21)
for test. We set two strategies to make the baselines
context-aware. First, we select the page of a term’s
first occurrence as its locator, denoted by “+FO” in
Figure 1. Second, we apply the context weight to
them, denoted by “+CW”. “CI-Indexer” denotes our
method. The results are shown in Figure 1a, 1b and
1c respectively. For all the three baselines, adding
context weight gives better performance than us-
ing the simple first occurrence guess, especially for
TFIDF. KEA benefits least from the context weights,
suggesting its first occurrence and node degree fea-
tures play a similar role as the context weight fea-
tures. SLD outperforms TFIDF and KEA under
both strategies probably because of the new fea-
tures of POS pattern and Wikipedia keyphraseness.
“SLD+CW” is the closest to ours. Finally, we show
in Figure 1d that increasing the size of featured con-
text set for CTI from 5 to 20 can slightly improve
</bodyText>
<figure confidence="0.9636858">
(a) TFIDF (b) KEA
�
�
�
0
</figure>
<page confidence="0.618558">
12345
</page>
<table confidence="0.896098625">
� size
Average Recall M)
Average Recall ���
Average Recall ���
Average ���������
266
Dataset Open book dataset Gutenberg dataset
Index size 1 2 3 4 5 1 2 3 4 5
Variance 2.4 4.8 7.5 10.4 13.4 1.1 2.9 5.3 8.0 11.0
Gain 2.9 6.4 10.2 14.3 18.2 4.9 9.0 14 18.6 23.0
Wikipedia keyphraseness 5.3 9.5 13.5 16.4 20.5 9.2 14.1 18.5 21.4 24.3
Burstiness 6.0 11.4 16.6 21.4 25.8 10.0 15.8 20.2 23.1 26.2
RIDF 8.6 14.5 19.5 23.9 28.0 10.4 15.9 20.1 23.2 26.3
TF 9.8 16.9 23.3 29.0 31.7 10.4 17.6 23.5 28.1 30.7
TFIDF 10.3 17.3 23.8 29.3 33.6 11.8 19.9 24.7 28.9 32.9
CTI 12.4 19.2 25.1 31.5 35.5 14.9 22.3 26.9 29.3 34.5
</table>
<tableCaption confidence="0.993528">
Table 6: Average recall(%) comparisons as the output index size increases
</tableCaption>
<bodyText confidence="0.815784">
performance in different index size settings.
</bodyText>
<subsectionHeader confidence="0.904259">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.99999064516129">
The three applications are (incrementally) designed
for different goals. The first is a toy applica-
tion to show the potential capability of this ap-
proach, regardless of syntactic or statistical informa-
tion. Clearly, there are simple heuristics that can
work very well for this task, e.g. the first term
of the context. TF or TFIDF also performs quite
well. We can rewrite each context (by reordering the
terms, changing sentence structures, or substituting
the core terms with pronouns) to make them inef-
fective. However, this will not effect our method,
because what it essentially measures is a term’s in-
formativeness among a list of terms appearing in the
same context. However, for keyword extraction, a
topic with a rich literature, to the best of our knowl-
edge, has no publicly available large scale datasets,
which makes SemEval2010 the best available. We
believe our application on back-of-the-book index
generation showed how CTI can scale real world
large corpora and will scale to millions of books
since each book can be processed separately.
Based on the applications we explored, we can
see that the practical utility of CTI used alone could
be limited, especially for context-oblivious tasks.
It seems reasonable that this method does not out-
perform supervised learning methods designed for
keyword extraction. However, our method shows
what simple but elegant methods can achieve with-
out the overhead of machine learning, especially for
context-aware scenarios such as finding book index
terms.
</bodyText>
<sectionHeader confidence="0.976861" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999935">
We developed a new web knowledge based method
for encoding informativeness of terms within a unit
of discourse. It is totally feature-free, corpus-free,
easy to implement, and inherently parallelizable.
Three typical applications on text snippets, scien-
tific papers and non-fiction books show its effec-
tiveness. The segmentation of context, the size of
featured context set, the semantic relatedness met-
ric n, and the knowledge base might more or less
affect the final performance of CTI in terms of ac-
curacy or efficiency. For all applications, we treat a
paragraph as an individual context, which is not nec-
essary a complete discourse unit. However, it may
not be fair to set the same number for all context
terms. In addition, selection of semantic relatedness
and knowledge bases need further investigation. The
Wikipedia-based implementation might be a good
choice for the definitional snippets, scientific arti-
cles and text books since they are all “educational”
resources sharing a similar concept space. However,
it is an open question as whether it works for corpora
such as tweets, online reviews, and forum posts.
Based on the proposed methods and encouraging
results, it would be interesting to build an online in-
dexing tool which automatically finds informative
terms in generic text and generates a back-of-the-
book index for a sets of papers, books, theses and
other collections.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.995483666666667">
We gratefully acknowledge partial support from the
National Science Foundation and useful comments
from referees.
</bodyText>
<page confidence="0.709058">
267
</page>
<sectionHeader confidence="0.985099" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996008881944445">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pasca,
and A. Soroa. 2009. A study on similarity and
relatedness using distributional and WordNet-based
approaches. In Proceedings of NAACL-HLT 2009,
pp. 19–27.
A. Bookstein and Don R. Swanson. 1974. Probabilistic
models for automatic indexing. Journal of the Ameri-
can Society for Information Science, 25(5):312–316.
A. Budanitsky and G. Hirst. 2012. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics 32:13–47.
K. W. Church and W. A. Gale. 1995. Inverse document
frequency(IDF): A measure of deviation from poisson.
In Proceedings of the Third Workshop on Very Large
Corpora 1995, pp. 121–130.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information and lexicography. Compu-
tational Linguistics, 16:22–29.
Rudi L. Cilibrasi and Paul M.B. Vitanyi. 2007. The
Google similarity distance. IEEE Transaction on
Knowledge and Data Engineering, 19(3):370–383.
A. Csomai and R. Mihalcea. 2006. Creating a testbed for
the evaluation of automatically generated back-of-the-
book indexes. In Proceedings of the 7th international
conference on Computational Linguistics and Intelli-
gent Text Processing 2006, pp. 429–440.
A. Csomai and R. Mihalcea. 2007. Investigations in
unsupervised back-of-the-book indexing. In Proceed-
ings of the Florida Artificial Intelligence Research So-
ciety 2007, pp. 211–216.
A. Csomai and R. Mihalcea. 2008. Linguistically Moti-
vated Features for Enhanced Back-of-the-Book Index-
ing. In Proceedings ofACL 2008, pp. 932–940.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-
dauer, and R. Harshman. 1990. Indexing by Latent
Semantic Analysis. Journal of the American Society
for Information Science 41:391-407.
V. Diodato and G. Gandt. 1991. Back of book indexes
and the characteristics of author and nonauthor index-
ing: Report of an exploratory study. Journal of the
American Society for Information Science, 42:341–
350.
V. Diodato. 1994. User preferences for features in back
of book indexes. Journal of the American Society for
Information Science, 45:529–536.
E. Frank, G. W. Paynter, I. H. Witten, and C. Gutwin.
1999 Domainspecic keyphrase extraction. In Pro-
ceedings IJCAI 1999, pp. 668-673.
E. Gabrilovich and S. Markovitch. 2007. Computing se-
mantic relatedness using Wikipedia-based explicit se-
mantic analysis. In Proceedings of IJCAI 2007, pp. 6–
12.
Q. He, J. Pei, D. Kifer, P. Mitra, and C. L. Giles. 2010.
Context-aware citation recommendation. In Proceed-
ings of WWW 2010, pp. 421–430.
B. Hornyak. 2002. Indexing Specialties: Psychology.
Medford, NJ : Information Today, Inc.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP 2003, pp. 216-223.
Karen Sprck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28(1):11–21.
Karen Sprck Jones. 1973. Index term weighting. Infor-
mation Storage and Retrieval, 9(11):619–633.
P. Kendrick and E. L. Zafran. 2001. Indexing Specialties:
Law. Medford, NJ : Information Today, Inc.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Tim-
othy Baldwin. 2010. Semeval-2010 task 5 : Auto-
matic keyphrase extraction from scientic articles. In
Proceedings of the 5th SIGLEX Workshop on Seman-
tic Evaluation. 2010, pp. 21–26.
K. Kireyev. 2009. Semantic-based Estimation of
Term Informativeness. In Proceedings of NAACL-HLT
2009, pp. 530–538.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP 2010, pp. 366–376.
P. Lopez and L. Romary. 2010. HUMB: Automatic Key
Term Extraction from Scientic Articles in GROBID.
In Proceedings of the 5th SIGLEX Workshop on Se-
mantic Evaluation. 2010, pp. 248-251.
O. Medelyan and I. H. Witten. 2008. Domain-
independent automatic keyphrase indexing with small
training sets. J. Am. Soc. Inf. Sci. Technol. 59:1026-
1040.
O. Medelyan, I. H. Witten, and D. Milne. 2008. Topic
indexing with Wikipedia. In Proceedings of AAAI08
Workshop on Wikipedia and Artificial Intelligence: an
Evolving Synergy 2008, pp. 19–24.
O. Medelyan, E. Frank, and I. H. Witten. 2009. Human-
competitive tagging using automatic keyphrase extrac-
tion. In Proceedings of EMNLP 2009, pp. 1318–1327.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP 2004,
pp. 404-411.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The PageRank Citation Ranking: Bringing Order to
the Web. Technical Report. Stanford InfoLab.
K. Papineni. 2001. Why inverse document frequency?
In Proceedings of NAACL-HLT 2001, pp. 1–8.
J. D. M. Rennie, and T. Jaakkola. 2005. Using Term
Informativeness for Named Entity Detection. In Pro-
ceedings of SIGIR 2005, pp. 353–360.
268
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information Pro-
cessing &amp; Management 24(5):513–523.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of NAACL-HLT 2003, pp. 149–156.
E. Terra and C. L. Clarke. 2003. Frequency estimates for
statistical word similarity measures. In Proceedings of
NAACL-HLT 2003, pp. 165–172.
T. Tomokiyo and M. Hurst. 2003. A language model ap-
proach to keyphrase extraction. In Proceedings of the
ACL 2003 workshop on Multiword expressions: anal-
ysis, acquisition and treatment, 2003, pp. 3340.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-Rich Part-of-Speech Tagging with a
Cyclic Dependency Network. In Proceedings of
NAACL-HLT 2003, pp. 252-259.
P. D. Turney. 2000. Learning Algorithms for Keyphrae
Extraction. Information Retrieval 2:303-336.
I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and
C. G. Nevill-Manning. 1999. Kea: practical automatic
keyphrase extraction. In Proceedings of the fourth
ACM conference on Digital libraries, 1999, pp. 254–
255.
L. P. Wyman. 1999. Indexing Specialities: Medicine.
Medford, NJ : Information Today, Inc.
M. Yazdani and A. Popescu-Belis. 2012. Computing
text semantic relatedness using the contents and links
of a hypertext encyclopedia. Artificial Intelligence
194:176–202.
J. Zobel and A. Moffat. 1998. Exploring the similarity
space. ACM SIGIR Forum 32(1):18–34.
Le Zhao and Jamie Callan. 2010. Term necessity predic-
tion. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment, 2010, pp. 259–268.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In Pro-
ceedings of the ACL, 2009, pp. 271–279 .
</reference>
<page confidence="0.940272">
269
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527628">
<title confidence="0.999733">Measuring Term Informativeness in Context</title>
<author confidence="0.962005">Zhaohui</author>
<affiliation confidence="0.908141">Computer Science and The Pennsylvania State</affiliation>
<address confidence="0.930859">University Park, PA 16802,</address>
<email confidence="0.997509">zzw109@psu.edu</email>
<author confidence="0.993217">C Lee</author>
<affiliation confidence="0.935050333333333">Information Sciences and Computer Science and The Pennsylvania State</affiliation>
<address confidence="0.977213">University Park, PA 16802,</address>
<email confidence="0.999779">giles@ist.psu.edu</email>
<abstract confidence="0.9959837">Measuring term informativeness is a fundamental NLP task. Existing methods, mostly based on statistical information in corpora, do not actually measure informativeness of a term with regard to its semantic context. This paper proposes a new lightweight feature-free approach to encode term informativeness in context by leveraging web knowledge. Given a term and its context, we model contextaware term informativeness based on semantic similarity between the context and the term’s most featured context in a knowledge base, Wikipedia. We apply our method to three applications: core term extraction from snippets (text segment), scientific keywords extraction (paper), and back-of-the-book index generation (book). The performance is state-of-theart or close to it for each application, demonstrating its effectiveness and generality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pasca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>pp.</pages>
<contexts>
<context position="6728" citStr="Agirre et al., 2009" startWordPosition="1054" endWordPosition="1057">tection. It is worth noting that term necessity, which measures the probability that a term occurs in documents relevant to a given query, has been well studied in Information Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to measure the term necessity in a query by considering it as a context. Despite extensive research on semantic analysis and understanding of word and text (Deerwester et al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and Vitanyi, 2007; Gabrilovich and Markovitch, 2007; Agirre et al., 2009; Yazdani and Popescu-Belis, 2012), little work studied the measurement of the semantics of term informativeness. An exception is the LSAspec from Kireyev (2009), based on latent semantic analysis (Deerwester et al., 1990), which is defined as the ratio of a term’s LSA vector length to its document frequency and thus can be interpreted as the rate of vector length growth. However, latent semantic models such as LSA are notoriously hard to interpret since the “latent concepts” cannot be readily mapped to human knowledge (Gabrilovich and Markovitch, 2007). Our approach explicitly leverages the s</context>
<context position="11725" citStr="Agirre et al., 2009" startWordPosition="1899" endWordPosition="1902">tween them and Cp. This suggests that if term ta in context c is more informative than tb, then most likely the contexts from ta’s featured context set will be more related to c than will tb. Thus, given a term t and its featured context set Uf(t) = {c1, ..., ck}, we define the term informativeness of t in context ci as I(t, ci) = � l,(ci,cj) · CA(cj) (7) cj∈Uf (t) where l,(ci,cj) is the semantic relatedness of ci and cj, which can be computed by various semantic relatedness metrics such as Wikipedia based (Gabrilovich and Markovitch, 2007; Yazdani 261 and Popescu-Belis, 2012), Wordnet based (Agirre et al., 2009; Budanitsky and Hirst, 2006), or simple cosine similarity and Jaccard similarity based (Zobel and Moffat, 1998). The context-aware term informativeness (CTI) introduced above is a formal and general definition. As such the definition in Equation (7) includes several features such as context authority score, featured context set, semantic relatedness, and knowledge base, any or all of which could be flexible for different applications. 3.3 Implementation Here, we present a simple practical implementation using Wikipedia as the knowledge base and the context authority estimated by the discounte</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pasca, and A. Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In Proceedings of NAACL-HLT 2009, pp. 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bookstein</author>
<author>Don R Swanson</author>
</authors>
<title>Probabilistic models for automatic indexing.</title>
<date>1974</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>25</volume>
<issue>5</issue>
<contexts>
<context position="5267" citStr="Bookstein and Swanson, 1974" startWordPosition="799" endWordPosition="802">based and semantic-based. Statistics-based methods, such as TFIDF (Salton and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). Sprck Jones defines IDF or inverse document frequency as: IDF(w) = −log2(dfw/D) (1) where D is the size of the corpus (Jones, 1972; Jones, 1973). Based on a finding that informative words tend to have large deviation between IDF and collection frequency fw(the total number of occurrence of a word), many other informativeness scores have been proposed. Bookstein and Swanson (Bookstein and Swanson, 1974) introduced the xI as: XI = fw − dfw Church and Gale (1995) introduced where tdw denotes w’s TF in d and ¯tw = fw/D indicates its mean expected word rate. Another measure suggested by them is burstiness(w) = fw (3) dfw which tends to compare collection frequency and document frequency directly. Informative words were found to have IDF scores that are larger than what would be expected according to the Poisson model; residual IDF (RIDF) was introduced to measure this deviation RIDF(w) = IDF(w) − IDF(w) (4) where � IDF(w) = −log2(1 − e−�t-). In addition, Papineni (2001) introduced the notion of </context>
</contexts>
<marker>Bookstein, Swanson, 1974</marker>
<rawString>A. Bookstein and Don R. Swanson. 1974. Probabilistic models for automatic indexing. Journal of the American Society for Information Science, 25(5):312–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of semantic distance.</title>
<date>2012</date>
<journal>Computational Linguistics</journal>
<pages>32--13</pages>
<marker>Budanitsky, Hirst, 2012</marker>
<rawString>A. Budanitsky and G. Hirst. 2012. Evaluating WordNetbased measures of semantic distance. Computational Linguistics 32:13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Inverse document frequency(IDF): A measure of deviation from poisson.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora</booktitle>
<pages>121--130</pages>
<contexts>
<context position="5326" citStr="Church and Gale (1995)" startWordPosition="812" endWordPosition="815"> (Salton and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). Sprck Jones defines IDF or inverse document frequency as: IDF(w) = −log2(dfw/D) (1) where D is the size of the corpus (Jones, 1972; Jones, 1973). Based on a finding that informative words tend to have large deviation between IDF and collection frequency fw(the total number of occurrence of a word), many other informativeness scores have been proposed. Bookstein and Swanson (Bookstein and Swanson, 1974) introduced the xI as: XI = fw − dfw Church and Gale (1995) introduced where tdw denotes w’s TF in d and ¯tw = fw/D indicates its mean expected word rate. Another measure suggested by them is burstiness(w) = fw (3) dfw which tends to compare collection frequency and document frequency directly. Informative words were found to have IDF scores that are larger than what would be expected according to the Poisson model; residual IDF (RIDF) was introduced to measure this deviation RIDF(w) = IDF(w) − IDF(w) (4) where � IDF(w) = −log2(1 − e−�t-). In addition, Papineni (2001) introduced the notion of gain as dfw (dfw D D f 1 gain(w) = — 1 — log() J (5) More r</context>
<context position="29807" citStr="Church and Gale, 1995" startWordPosition="4924" endWordPosition="4927">ics are context-oblivious. CTI selects index terms based on the sum of a term’s CTI scores over all its contexts, the same as the algorithm used in Section 4.2. The results are shown in Table 6, where the index size = n indicates the number of output terms is n times of the true book index size for each book. The scores are the average recall over a dataset. The CTI outperforms all other 7 metrics in the two datasets as the output index size increases. Moreover, results show that TF and TFIDF are better than RIDF in identifying book index terms, which seems contradictory to previous findings (Church and Gale, 1995). A possible reason is that a book is much longer than a regular document thus enhancing TF as a better indicator of keywords but weakening the role of IDF. We believe this is why Variance, Gain, and Burstiness, which relies on DF, are less effective here. Wikipedia keyphraseness (Csomai and Mihalcea, 2008) can only find a small fraction of index terms because it emphasizes Wikipedia titles that have high in-degree in hyper-link network formed 6www.gutenberg.org/ 7pdfbox.apache.org/ (c) SLD (d) size of featured context set Figure 1: Results for book index generation by Wikipedia terms. However</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>K. W. Church and W. A. Gale. 1995. Inverse document frequency(IDF): A measure of deviation from poisson. In Proceedings of the Third Workshop on Very Large Corpora 1995, pp. 121–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--22</pages>
<contexts>
<context position="28216" citStr="Church and Hanks, 1990" startWordPosition="4648" endWordPosition="4651"> entries The weight in step 1 represents the relative importance of a context in a book. w(c) = 1 − cid (c)−cid(title,) Ntitlec measures the weight based on the le� normalized distance from the context to its direct chapter or sub-chapter title, where cid(c) denotes the id of context c, titlec the title of context c and Ntitlec the number of contexts under titlec. To select candidate terms, we first filter the improbable index terms 265 based on POS patterns using the Standard POS Tagger (Toutanova et al., 2003). We then select multiword keyphrases based on Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which was shown to be the best metric to measure word associations (Terra and Clarke, 2003). To evaluate our back-of-the-book index generation method, we conduct extensive experiments on books in various domains, from the Gutenberg dataset and the open book dataset described in Table 5. The first one was created by (Csomai and Mihalcea, 2006), containing 55 free books collected from Gutenburg6. Since the dataset does not provide the locators of index terms, we can only serve the evaluation as a keyword extraction task. The second dataset was collected from CiteSeer repository, most of which </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. W. Church and P. Hanks. 1990. Word association norms, mutual information and lexicography. Computational Linguistics, 16:22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi L Cilibrasi</author>
<author>Paul M B Vitanyi</author>
</authors>
<title>The Google similarity distance.</title>
<date>2007</date>
<journal>IEEE Transaction on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="6673" citStr="Cilibrasi and Vitanyi, 2007" startWordPosition="1046" endWordPosition="1049">ure of 2 Unigram distribution and applied it to named entity detection. It is worth noting that term necessity, which measures the probability that a term occurs in documents relevant to a given query, has been well studied in Information Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to measure the term necessity in a query by considering it as a context. Despite extensive research on semantic analysis and understanding of word and text (Deerwester et al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and Vitanyi, 2007; Gabrilovich and Markovitch, 2007; Agirre et al., 2009; Yazdani and Popescu-Belis, 2012), little work studied the measurement of the semantics of term informativeness. An exception is the LSAspec from Kireyev (2009), based on latent semantic analysis (Deerwester et al., 1990), which is defined as the ratio of a term’s LSA vector length to its document frequency and thus can be interpreted as the rate of vector length growth. However, latent semantic models such as LSA are notoriously hard to interpret since the “latent concepts” cannot be readily mapped to human knowledge (Gabrilovich and Mar</context>
<context position="8094" citStr="Cilibrasi and Vitanyi, 2007" startWordPosition="1281" endWordPosition="1284">d, might be effective in identifying informative words at the document or corpus level, but do not the ability to capture term informativeness in a particular context due to their absence of semantics and obliviousness of context. Our method measures the term informativeness within a context in a semantic-based approach, regardless of the absence of statistical information. 3 Context-aware Term Informativeness 3.1 Context A context of a word or phrase may refer to a few words nearby (He et al., 2010), a sentence or paragraph (Soricut and Marcu, 2003), or even a set of documents containing it (Cilibrasi and Vitanyi, 2007). Here we define context as a syntactic unit of discourse such as a sentence or paragraph, for example, “PL/SQL is one of three key programming languages embedded in the Oracle Database”, or “There are two types of functions in PL/SQL”. The universal context set U(t) of a word t is defined as all the contexts containing it in the web. Different contexts vary in their authority just like web pages vary. For the two examples, we could argue that the first context is much more “authoritative” than the second. This can be verified by their popularity on Google; (all results from actual search engi</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2007</marker>
<rawString>Rudi L. Cilibrasi and Paul M.B. Vitanyi. 2007. The Google similarity distance. IEEE Transaction on Knowledge and Data Engineering, 19(3):370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Csomai</author>
<author>R Mihalcea</author>
</authors>
<title>Creating a testbed for the evaluation of automatically generated back-of-thebook indexes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th international conference on Computational Linguistics and Intelligent Text Processing</booktitle>
<pages>429--440</pages>
<contexts>
<context position="28562" citStr="Csomai and Mihalcea, 2006" startWordPosition="4706" endWordPosition="4709"> contexts under titlec. To select candidate terms, we first filter the improbable index terms 265 based on POS patterns using the Standard POS Tagger (Toutanova et al., 2003). We then select multiword keyphrases based on Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which was shown to be the best metric to measure word associations (Terra and Clarke, 2003). To evaluate our back-of-the-book index generation method, we conduct extensive experiments on books in various domains, from the Gutenberg dataset and the open book dataset described in Table 5. The first one was created by (Csomai and Mihalcea, 2006), containing 55 free books collected from Gutenburg6. Since the dataset does not provide the locators of index terms, we can only serve the evaluation as a keyword extraction task. The second dataset was collected from CiteSeer repository, most of which are in computer science and engineering. We extracted the paged body text and the back index using Pdfbox7. Having each index term associated with its locators (page numbers), we can perform an evaluation for different methods, not based solely on keyword extraction. We first compare CTI with other metrics on both datasets for keywords extracti</context>
</contexts>
<marker>Csomai, Mihalcea, 2006</marker>
<rawString>A. Csomai and R. Mihalcea. 2006. Creating a testbed for the evaluation of automatically generated back-of-thebook indexes. In Proceedings of the 7th international conference on Computational Linguistics and Intelligent Text Processing 2006, pp. 429–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Csomai</author>
<author>R Mihalcea</author>
</authors>
<title>Investigations in unsupervised back-of-the-book indexing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Florida Artificial Intelligence Research Society</booktitle>
<pages>211--216</pages>
<marker>Csomai, Mihalcea, 2007</marker>
<rawString>A. Csomai and R. Mihalcea. 2007. Investigations in unsupervised back-of-the-book indexing. In Proceedings of the Florida Artificial Intelligence Research Society 2007, pp. 211–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Csomai</author>
<author>R Mihalcea</author>
</authors>
<title>Linguistically Motivated Features for Enhanced Back-of-the-Book Indexing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>932--940</pages>
<contexts>
<context position="30115" citStr="Csomai and Mihalcea, 2008" startWordPosition="4977" endWordPosition="4981"> book. The scores are the average recall over a dataset. The CTI outperforms all other 7 metrics in the two datasets as the output index size increases. Moreover, results show that TF and TFIDF are better than RIDF in identifying book index terms, which seems contradictory to previous findings (Church and Gale, 1995). A possible reason is that a book is much longer than a regular document thus enhancing TF as a better indicator of keywords but weakening the role of IDF. We believe this is why Variance, Gain, and Burstiness, which relies on DF, are less effective here. Wikipedia keyphraseness (Csomai and Mihalcea, 2008) can only find a small fraction of index terms because it emphasizes Wikipedia titles that have high in-degree in hyper-link network formed 6www.gutenberg.org/ 7pdfbox.apache.org/ (c) SLD (d) size of featured context set Figure 1: Results for book index generation by Wikipedia terms. However, a book index covers much broader terms not titled in Wikipedia. We then compare with three baselines TFIDF, KEA, and SLD (supervised learning using decision tree in Csomai’s (2008)) on the second dataset. For SLD, we use all the features except the discourse comprehension based ones which were too complic</context>
</contexts>
<marker>Csomai, Mihalcea, 2008</marker>
<rawString>A. Csomai and R. Mihalcea. 2008. Linguistically Motivated Features for Enhanced Back-of-the-Book Indexing. In Proceedings ofACL 2008, pp. 932–940.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science</journal>
<pages>41--391</pages>
<contexts>
<context position="6616" citStr="Deerwester et al., 1990" startWordPosition="1038" endWordPosition="1041">core based on the fit of a word’s frequency to a mixture of 2 Unigram distribution and applied it to named entity detection. It is worth noting that term necessity, which measures the probability that a term occurs in documents relevant to a given query, has been well studied in Information Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to measure the term necessity in a query by considering it as a context. Despite extensive research on semantic analysis and understanding of word and text (Deerwester et al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and Vitanyi, 2007; Gabrilovich and Markovitch, 2007; Agirre et al., 2009; Yazdani and Popescu-Belis, 2012), little work studied the measurement of the semantics of term informativeness. An exception is the LSAspec from Kireyev (2009), based on latent semantic analysis (Deerwester et al., 1990), which is defined as the ratio of a term’s LSA vector length to its document frequency and thus can be interpreted as the rate of vector length growth. However, latent semantic models such as LSA are notoriously hard to interpret since the “latent concepts” cannot </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science 41:391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Diodato</author>
<author>G Gandt</author>
</authors>
<title>Back of book indexes and the characteristics of author and nonauthor indexing: Report of an exploratory study.</title>
<date>1991</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>42</volume>
<pages>350</pages>
<contexts>
<context position="26962" citStr="Diodato and Gandt, 1991" startWordPosition="4431" endWordPosition="4434">tors. Thus the actual problem is to identify a index term with its context. As such, learning a robust and efficient model for real book indexes is challenging. First, books from different domains vary in vocabulary composition and structure style, requiring various indexing specialties. There are different indexing guides for medicine (Wyman, 1999), psychology (Hornyak, 2002), and law (Kendrick and Zafran, 2001). Second, book indexing is a highly subjective work and indexes of different books are always created by different professional indexers who have their own preferences and background (Diodato and Gandt, 1991; Diodato, 1994). Third, the training set is extremely unbalanced. As we found in our dataset, the index size is only 0.42% of the length of book on average. All these motivate us to explore the automatic creation of index terms that are aware of the context at the term’s locations (locators). To do so we propose the following efficient training-free and domain independent approach: 1. For each context ci in a book, compute its weight wi based on structural features 2. For each candidate term t in ci, calculate I(t, ci) using Wikipedia based implementation 3. Select term-context pairs with the</context>
</contexts>
<marker>Diodato, Gandt, 1991</marker>
<rawString>V. Diodato and G. Gandt. 1991. Back of book indexes and the characteristics of author and nonauthor indexing: Report of an exploratory study. Journal of the American Society for Information Science, 42:341– 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Diodato</author>
</authors>
<title>User preferences for features in back of book indexes.</title>
<date>1994</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>45--529</pages>
<contexts>
<context position="26978" citStr="Diodato, 1994" startWordPosition="4435" endWordPosition="4436">blem is to identify a index term with its context. As such, learning a robust and efficient model for real book indexes is challenging. First, books from different domains vary in vocabulary composition and structure style, requiring various indexing specialties. There are different indexing guides for medicine (Wyman, 1999), psychology (Hornyak, 2002), and law (Kendrick and Zafran, 2001). Second, book indexing is a highly subjective work and indexes of different books are always created by different professional indexers who have their own preferences and background (Diodato and Gandt, 1991; Diodato, 1994). Third, the training set is extremely unbalanced. As we found in our dataset, the index size is only 0.42% of the length of book on average. All these motivate us to explore the automatic creation of index terms that are aware of the context at the term’s locations (locators). To do so we propose the following efficient training-free and domain independent approach: 1. For each context ci in a book, compute its weight wi based on structural features 2. For each candidate term t in ci, calculate I(t, ci) using Wikipedia based implementation 3. Select term-context pairs with the highest wi � I(</context>
</contexts>
<marker>Diodato, 1994</marker>
<rawString>V. Diodato. 1994. User preferences for features in back of book indexes. Journal of the American Society for Information Science, 45:529–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>G W Paynter</author>
<author>I H Witten</author>
<author>C Gutwin</author>
</authors>
<title>Domainspecic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings IJCAI</booktitle>
<pages>668--673</pages>
<contexts>
<context position="19637" citStr="Frank et al., 1999" startWordPosition="3224" endWordPosition="3227"> the recall increase to 76.3%. Though the algorithm can be easily parallelized, sequentially runtime on all snippets took only slightly more than a minute on a 2.35GHz Intel(R) Xeon(R) 4 processors, 23GB of RAM, and Red Hat Enterprise Linux Server(5.7) machine. However, the time could vary due to network conditions. Though these results look promising, but it could be due to the high lexical similarity between this dataset and Wikipedia content. To test on a more general corpora, we explore more real world tasks. 4.2 Keyword Extraction There is a rich literature on keyword extraction problem (Frank et al., 1999; Witten et al., 1999; Turney, 2000; Hulth et al., 2003; Tomokiyo and Hurst, 2003; 2http://www.labautopedia.org/mw/index.php/List of programming and computer science terms 263 Wiki20 citeulike180 Method P R F P R F TFIDF 13.7 17.8 15.5 14.4 16.0 15.2 KEA 18.4 21.5 19.8 20.4 22.3 21.3 CTI 19.6 22.7 21.0 18.5 21.4 19.8 Table 3: Results on Wiki20 and citeulike180 Method Precision (%) Recall (%) F1(%) TFIDF 14.9 15.3 15.1 HUMB 27.2 27.8 27.5 CTI 19.3 20.1 19.7 CTI+ 25.3 26.2 25.7 Table 4: Results on SemEval2010 Mihalcea and Tarau, 2004; Medelyan and Witten, 2008; Liu, 2010), most of which is treat</context>
</contexts>
<marker>Frank, Paynter, Witten, Gutwin, 1999</marker>
<rawString>E. Frank, G. W. Paynter, I. H. Witten, and C. Gutwin. 1999 Domainspecic keyphrase extraction. In Proceedings IJCAI 1999, pp. 668-673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI</booktitle>
<pages>6--12</pages>
<contexts>
<context position="6707" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1050" endWordPosition="1053"> and applied it to named entity detection. It is worth noting that term necessity, which measures the probability that a term occurs in documents relevant to a given query, has been well studied in Information Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to measure the term necessity in a query by considering it as a context. Despite extensive research on semantic analysis and understanding of word and text (Deerwester et al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and Vitanyi, 2007; Gabrilovich and Markovitch, 2007; Agirre et al., 2009; Yazdani and Popescu-Belis, 2012), little work studied the measurement of the semantics of term informativeness. An exception is the LSAspec from Kireyev (2009), based on latent semantic analysis (Deerwester et al., 1990), which is defined as the ratio of a term’s LSA vector length to its document frequency and thus can be interpreted as the rate of vector length growth. However, latent semantic models such as LSA are notoriously hard to interpret since the “latent concepts” cannot be readily mapped to human knowledge (Gabrilovich and Markovitch, 2007). Our approach expli</context>
<context position="11651" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1888" endWordPosition="1891">ts for “Oracle Database” or “programming languages”, we will find little relatedness between them and Cp. This suggests that if term ta in context c is more informative than tb, then most likely the contexts from ta’s featured context set will be more related to c than will tb. Thus, given a term t and its featured context set Uf(t) = {c1, ..., ck}, we define the term informativeness of t in context ci as I(t, ci) = � l,(ci,cj) · CA(cj) (7) cj∈Uf (t) where l,(ci,cj) is the semantic relatedness of ci and cj, which can be computed by various semantic relatedness metrics such as Wikipedia based (Gabrilovich and Markovitch, 2007; Yazdani 261 and Popescu-Belis, 2012), Wordnet based (Agirre et al., 2009; Budanitsky and Hirst, 2006), or simple cosine similarity and Jaccard similarity based (Zobel and Moffat, 1998). The context-aware term informativeness (CTI) introduced above is a formal and general definition. As such the definition in Equation (7) includes several features such as context authority score, featured context set, semantic relatedness, and knowledge base, any or all of which could be flexible for different applications. 3.3 Implementation Here, we present a simple practical implementation using Wikipedia </context>
<context position="21168" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3484" endWordPosition="3487"> CTI works as follows. 1. For each context ci in a document, compute the semantic relatedness s(ci, d) between ci and d 2. For each n-gram (1 &lt; n &lt; 4) t in ci, calculate I(t, ci) using Wikipedia based implementation 3. Select the top keywords with the highest Ei I(t, ci) * s(ci, d) Note that for the last step keywords are selected based on a summarized weighted informativeness score over a document. Obviously, the pure cosine or Jaccard similarity is not a good choice to measure semantic relatedness between two text segments of very low lexical similarity. We thus use the Wikipedia based ESA (Gabrilovich and Markovitch, 2007) to compute the semantic relatedness s(ci, d) and n(ci, cj). To make the calculation more efficient, only the Wikipedia pages whose title is contained in the dataset are used to build the concept space. We ran the algorithm on several datasets including Wiki20 (Medelyan et al., 2008), citeulike180 (Medelyan et al., 2009) and SemEval2010 (Kim et al., 2010) 3. Though keyword extraction as a research topic has a rich literature, to the best of our knowledge there is no large scale datasets publicly available. The Wiki20 dataset contains 20 computer science articles each with around 5 terms labele</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of IJCAI 2007, pp. 6– 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q He</author>
<author>J Pei</author>
<author>D Kifer</author>
<author>P Mitra</author>
<author>C L Giles</author>
</authors>
<title>Context-aware citation recommendation.</title>
<date>2010</date>
<booktitle>In Proceedings of WWW</booktitle>
<pages>421--430</pages>
<contexts>
<context position="7971" citStr="He et al., 2010" startWordPosition="1260" endWordPosition="1263">sing existing knowledge bases. variance(w) = 1 D (tdw − ¯tw) (2) D − 1d_1 260 Previous methods, all corpus-based, might be effective in identifying informative words at the document or corpus level, but do not the ability to capture term informativeness in a particular context due to their absence of semantics and obliviousness of context. Our method measures the term informativeness within a context in a semantic-based approach, regardless of the absence of statistical information. 3 Context-aware Term Informativeness 3.1 Context A context of a word or phrase may refer to a few words nearby (He et al., 2010), a sentence or paragraph (Soricut and Marcu, 2003), or even a set of documents containing it (Cilibrasi and Vitanyi, 2007). Here we define context as a syntactic unit of discourse such as a sentence or paragraph, for example, “PL/SQL is one of three key programming languages embedded in the Oracle Database”, or “There are two types of functions in PL/SQL”. The universal context set U(t) of a word t is defined as all the contexts containing it in the web. Different contexts vary in their authority just like web pages vary. For the two examples, we could argue that the first context is much mor</context>
</contexts>
<marker>He, Pei, Kifer, Mitra, Giles, 2010</marker>
<rawString>Q. He, J. Pei, D. Kifer, P. Mitra, and C. L. Giles. 2010. Context-aware citation recommendation. In Proceedings of WWW 2010, pp. 421–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hornyak</author>
</authors>
<title>Indexing Specialties: Psychology.</title>
<date>2002</date>
<publisher>Information Today, Inc.</publisher>
<location>Medford, NJ :</location>
<contexts>
<context position="26718" citStr="Hornyak, 2002" startWordPosition="4395" endWordPosition="4396">rm or phrase points to its proper location in the text. For example, in “pattern recognition and machine learning” by Bishop, “hidden Markov model” appears in more than 20 pages while the actual index entry has only 2 pages as its locators. Thus the actual problem is to identify a index term with its context. As such, learning a robust and efficient model for real book indexes is challenging. First, books from different domains vary in vocabulary composition and structure style, requiring various indexing specialties. There are different indexing guides for medicine (Wyman, 1999), psychology (Hornyak, 2002), and law (Kendrick and Zafran, 2001). Second, book indexing is a highly subjective work and indexes of different books are always created by different professional indexers who have their own preferences and background (Diodato and Gandt, 1991; Diodato, 1994). Third, the training set is extremely unbalanced. As we found in our dataset, the index size is only 0.42% of the length of book on average. All these motivate us to explore the automatic creation of index terms that are aware of the context at the term’s locations (locators). To do so we propose the following efficient training-free and</context>
</contexts>
<marker>Hornyak, 2002</marker>
<rawString>B. Hornyak. 2002. Indexing Specialties: Psychology. Medford, NJ : Information Today, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hulth</author>
</authors>
<title>Improved automatic keyword extraction given more linguistic knowledge.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>216--223</pages>
<marker>Hulth, 2003</marker>
<rawString>A. Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of EMNLP 2003, pp. 216-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sprck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="4992" citStr="Jones, 1972" startWordPosition="758" endWordPosition="759"> practical implementation using Web knowledge. Section 4 studies the three applications. Finally, we conclude with discussion and future work. 2 Related Work Most known approaches to measure term informativeness fall into basically two categories: statisticsbased and semantic-based. Statistics-based methods, such as TFIDF (Salton and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). Sprck Jones defines IDF or inverse document frequency as: IDF(w) = −log2(dfw/D) (1) where D is the size of the corpus (Jones, 1972; Jones, 1973). Based on a finding that informative words tend to have large deviation between IDF and collection frequency fw(the total number of occurrence of a word), many other informativeness scores have been proposed. Bookstein and Swanson (Bookstein and Swanson, 1974) introduced the xI as: XI = fw − dfw Church and Gale (1995) introduced where tdw denotes w’s TF in d and ¯tw = fw/D indicates its mean expected word rate. Another measure suggested by them is burstiness(w) = fw (3) dfw which tends to compare collection frequency and document frequency directly. Informative words were found </context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sprck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sprck Jones</author>
</authors>
<title>Index term weighting.</title>
<date>1973</date>
<journal>Information Storage and Retrieval,</journal>
<volume>9</volume>
<issue>11</issue>
<contexts>
<context position="5006" citStr="Jones, 1973" startWordPosition="760" endWordPosition="761">plementation using Web knowledge. Section 4 studies the three applications. Finally, we conclude with discussion and future work. 2 Related Work Most known approaches to measure term informativeness fall into basically two categories: statisticsbased and semantic-based. Statistics-based methods, such as TFIDF (Salton and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). Sprck Jones defines IDF or inverse document frequency as: IDF(w) = −log2(dfw/D) (1) where D is the size of the corpus (Jones, 1972; Jones, 1973). Based on a finding that informative words tend to have large deviation between IDF and collection frequency fw(the total number of occurrence of a word), many other informativeness scores have been proposed. Bookstein and Swanson (Bookstein and Swanson, 1974) introduced the xI as: XI = fw − dfw Church and Gale (1995) introduced where tdw denotes w’s TF in d and ¯tw = fw/D indicates its mean expected word rate. Another measure suggested by them is burstiness(w) = fw (3) dfw which tends to compare collection frequency and document frequency directly. Informative words were found to have IDF sc</context>
</contexts>
<marker>Jones, 1973</marker>
<rawString>Karen Sprck Jones. 1973. Index term weighting. Information Storage and Retrieval, 9(11):619–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kendrick</author>
<author>E L Zafran</author>
</authors>
<title>Indexing Specialties: Law.</title>
<date>2001</date>
<publisher>Information Today, Inc.</publisher>
<location>Medford, NJ :</location>
<contexts>
<context position="26755" citStr="Kendrick and Zafran, 2001" startWordPosition="4399" endWordPosition="4402">s proper location in the text. For example, in “pattern recognition and machine learning” by Bishop, “hidden Markov model” appears in more than 20 pages while the actual index entry has only 2 pages as its locators. Thus the actual problem is to identify a index term with its context. As such, learning a robust and efficient model for real book indexes is challenging. First, books from different domains vary in vocabulary composition and structure style, requiring various indexing specialties. There are different indexing guides for medicine (Wyman, 1999), psychology (Hornyak, 2002), and law (Kendrick and Zafran, 2001). Second, book indexing is a highly subjective work and indexes of different books are always created by different professional indexers who have their own preferences and background (Diodato and Gandt, 1991; Diodato, 1994). Third, the training set is extremely unbalanced. As we found in our dataset, the index size is only 0.42% of the length of book on average. All these motivate us to explore the automatic creation of index terms that are aware of the context at the term’s locations (locators). To do so we propose the following efficient training-free and domain independent approach: 1. For </context>
</contexts>
<marker>Kendrick, Zafran, 2001</marker>
<rawString>P. Kendrick and E. L. Zafran. 2001. Indexing Specialties: Law. Medford, NJ : Information Today, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Olena Medelyan</author>
<author>Min-Yen Kan</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semeval-2010 task 5 : Automatic keyphrase extraction from scientic articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation.</booktitle>
<pages>21--26</pages>
<contexts>
<context position="21525" citStr="Kim et al., 2010" startWordPosition="3545" endWordPosition="3548">veness score over a document. Obviously, the pure cosine or Jaccard similarity is not a good choice to measure semantic relatedness between two text segments of very low lexical similarity. We thus use the Wikipedia based ESA (Gabrilovich and Markovitch, 2007) to compute the semantic relatedness s(ci, d) and n(ci, cj). To make the calculation more efficient, only the Wikipedia pages whose title is contained in the dataset are used to build the concept space. We ran the algorithm on several datasets including Wiki20 (Medelyan et al., 2008), citeulike180 (Medelyan et al., 2009) and SemEval2010 (Kim et al., 2010) 3. Though keyword extraction as a research topic has a rich literature, to the best of our knowledge there is no large scale datasets publicly available. The Wiki20 dataset contains 20 computer science articles each with around 5 terms labeled by 15 different teams. Every term is a Wikipedia title. 3http://code.google.com/p/maui-indexer/downloads/list The citeulike180 contains a set of 180 papers each tagged with around three tags by 332 users. For each dataset, the collection of all labeled keywords by different taggers are considered as the gold standard for a document. We use the set of al</context>
<context position="24598" citStr="Kim et al., 2010" startWordPosition="4048" endWordPosition="4051">sophy, Literature, Zoology Open Book 213 22,279,530 1,135,919 Computer Science, Engineering, Information Science Table 5: Datasets for book index generation evaluation The SemEval2010 dataset contains a set of 284 scientific papers with 15 keyphrases assigned by readers and authors. 144 of them are selected as training set while the other 100 are for testing. A comparison of CTI to the results from TFIDF and the best reported results HUMB (Lopez and Romary, 2010) is shown in Table 4. It achieves 19.8% by micro-averaged F1 score, ranking 11th out of the 19 systems submitted to the competition (Kim et al., 2010). However, by adding the structural features used by HUMB into CTI, we can improve the performance by around 6%, making our results close to that of HUMB. The structural information is encoded as weights for context that is located in title, abstract, section titles and general content. Each weight can be regarded as the prior probability that a keyword will appear in the corresponding location, whose value can be set according to the fraction of the number of keyword occurrences of this type of location with respect to the number of all keyword occurrences in the entire training set. Here the</context>
</contexts>
<marker>Kim, Medelyan, Kan, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timothy Baldwin. 2010. Semeval-2010 task 5 : Automatic keyphrase extraction from scientic articles. In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation. 2010, pp. 21–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kireyev</author>
</authors>
<title>Semantic-based Estimation of Term Informativeness.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>530--538</pages>
<contexts>
<context position="1288" citStr="Kireyev, 2009" startWordPosition="180" endWordPosition="181">knowledge. Given a term and its context, we model contextaware term informativeness based on semantic similarity between the context and the term’s most featured context in a knowledge base, Wikipedia. We apply our method to three applications: core term extraction from snippets (text segment), scientific keywords extraction (paper), and back-of-the-book index generation (book). The performance is state-of-theart or close to it for each application, demonstrating its effectiveness and generality. 1 Introduction Computationally measuring importance of a word in text, or “term informativeness” (Kireyev, 2009; Rennie and Jaakkola, 2005), is fundamental to many NLP tasks such as keyword extraction, text categorization, clustering, and summarization, etc. Various features derived from statistical and linguistic information can be helpful in encoding term informativeness, whereas practical feature definition and selection are usually ad hoc, data-driven and application dependent. Statistical information based on term frequency (TF) and document frequency (DF) tend to be more effective in finding keywords in large corpora, but can have issues with small amounts of text or small corpora. Linguistic inf</context>
<context position="6889" citStr="Kireyev (2009)" startWordPosition="1079" endWordPosition="1080">formation Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to measure the term necessity in a query by considering it as a context. Despite extensive research on semantic analysis and understanding of word and text (Deerwester et al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and Vitanyi, 2007; Gabrilovich and Markovitch, 2007; Agirre et al., 2009; Yazdani and Popescu-Belis, 2012), little work studied the measurement of the semantics of term informativeness. An exception is the LSAspec from Kireyev (2009), based on latent semantic analysis (Deerwester et al., 1990), which is defined as the ratio of a term’s LSA vector length to its document frequency and thus can be interpreted as the rate of vector length growth. However, latent semantic models such as LSA are notoriously hard to interpret since the “latent concepts” cannot be readily mapped to human knowledge (Gabrilovich and Markovitch, 2007). Our approach explicitly leverages the semantics of word and text using existing knowledge bases. variance(w) = 1 D (tdw − ¯tw) (2) D − 1d_1 260 Previous methods, all corpus-based, might be effective i</context>
</contexts>
<marker>Kireyev, 2009</marker>
<rawString>K. Kireyev. 2009. Semantic-based Estimation of Term Informativeness. In Proceedings of NAACL-HLT 2009, pp. 530–538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Liu</author>
<author>W Huang</author>
<author>Y Zheng</author>
<author>M Sun</author>
</authors>
<title>Automatic keyphrase extraction via topic decomposition.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>366--376</pages>
<marker>Liu, Huang, Zheng, Sun, 2010</marker>
<rawString>Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010. Automatic keyphrase extraction via topic decomposition. In Proceedings of EMNLP 2010, pp. 366–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lopez</author>
<author>L Romary</author>
</authors>
<title>HUMB: Automatic Key Term Extraction from Scientic Articles in GROBID.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation.</booktitle>
<pages>248--251</pages>
<contexts>
<context position="24448" citStr="Lopez and Romary, 2010" startWordPosition="4021" endWordPosition="4024">pedia title words. 4http://www.nzdl.org/Kea/ 264 Dataset #Books #Words #Contexts Main domains Gutenberg 55 7,164,463 301,581 History, Art, Psychology, Philosophy, Literature, Zoology Open Book 213 22,279,530 1,135,919 Computer Science, Engineering, Information Science Table 5: Datasets for book index generation evaluation The SemEval2010 dataset contains a set of 284 scientific papers with 15 keyphrases assigned by readers and authors. 144 of them are selected as training set while the other 100 are for testing. A comparison of CTI to the results from TFIDF and the best reported results HUMB (Lopez and Romary, 2010) is shown in Table 4. It achieves 19.8% by micro-averaged F1 score, ranking 11th out of the 19 systems submitted to the competition (Kim et al., 2010). However, by adding the structural features used by HUMB into CTI, we can improve the performance by around 6%, making our results close to that of HUMB. The structural information is encoded as weights for context that is located in title, abstract, section titles and general content. Each weight can be regarded as the prior probability that a keyword will appear in the corresponding location, whose value can be set according to the fraction of</context>
</contexts>
<marker>Lopez, Romary, 2010</marker>
<rawString>P. Lopez and L. Romary. 2010. HUMB: Automatic Key Term Extraction from Scientic Articles in GROBID. In Proceedings of the 5th SIGLEX Workshop on Semantic Evaluation. 2010, pp. 248-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
</authors>
<title>Domainindependent automatic keyphrase indexing with small training sets.</title>
<date>2008</date>
<journal>J. Am. Soc. Inf. Sci. Technol.</journal>
<pages>59--1026</pages>
<contexts>
<context position="20201" citStr="Medelyan and Witten, 2008" startWordPosition="3318" endWordPosition="3321"> literature on keyword extraction problem (Frank et al., 1999; Witten et al., 1999; Turney, 2000; Hulth et al., 2003; Tomokiyo and Hurst, 2003; 2http://www.labautopedia.org/mw/index.php/List of programming and computer science terms 263 Wiki20 citeulike180 Method P R F P R F TFIDF 13.7 17.8 15.5 14.4 16.0 15.2 KEA 18.4 21.5 19.8 20.4 22.3 21.3 CTI 19.6 22.7 21.0 18.5 21.4 19.8 Table 3: Results on Wiki20 and citeulike180 Method Precision (%) Recall (%) F1(%) TFIDF 14.9 15.3 15.1 HUMB 27.2 27.8 27.5 CTI 19.3 20.1 19.7 CTI+ 25.3 26.2 25.7 Table 4: Results on SemEval2010 Mihalcea and Tarau, 2004; Medelyan and Witten, 2008; Liu, 2010), most of which is treated as a classification or ranking problem with corresponding machine learning algorithms that use statistical and linguistic features in a corpus. Here, we consider the task as finding the most informative keywords in a document. Given a document d = {ci}, our keyword extraction algorithm based on CTI works as follows. 1. For each context ci in a document, compute the semantic relatedness s(ci, d) between ci and d 2. For each n-gram (1 &lt; n &lt; 4) t in ci, calculate I(t, ci) using Wikipedia based implementation 3. Select the top keywords with the highest Ei I(t</context>
</contexts>
<marker>Medelyan, Witten, 2008</marker>
<rawString>O. Medelyan and I. H. Witten. 2008. Domainindependent automatic keyphrase indexing with small training sets. J. Am. Soc. Inf. Sci. Technol. 59:1026-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
<author>D Milne</author>
</authors>
<title>Topic indexing with Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI08 Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy</booktitle>
<pages>pp.</pages>
<contexts>
<context position="21452" citStr="Medelyan et al., 2008" startWordPosition="3533" endWordPosition="3536">r the last step keywords are selected based on a summarized weighted informativeness score over a document. Obviously, the pure cosine or Jaccard similarity is not a good choice to measure semantic relatedness between two text segments of very low lexical similarity. We thus use the Wikipedia based ESA (Gabrilovich and Markovitch, 2007) to compute the semantic relatedness s(ci, d) and n(ci, cj). To make the calculation more efficient, only the Wikipedia pages whose title is contained in the dataset are used to build the concept space. We ran the algorithm on several datasets including Wiki20 (Medelyan et al., 2008), citeulike180 (Medelyan et al., 2009) and SemEval2010 (Kim et al., 2010) 3. Though keyword extraction as a research topic has a rich literature, to the best of our knowledge there is no large scale datasets publicly available. The Wiki20 dataset contains 20 computer science articles each with around 5 terms labeled by 15 different teams. Every term is a Wikipedia title. 3http://code.google.com/p/maui-indexer/downloads/list The citeulike180 contains a set of 180 papers each tagged with around three tags by 332 users. For each dataset, the collection of all labeled keywords by different taggers</context>
</contexts>
<marker>Medelyan, Witten, Milne, 2008</marker>
<rawString>O. Medelyan, I. H. Witten, and D. Milne. 2008. Topic indexing with Wikipedia. In Proceedings of AAAI08 Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy 2008, pp. 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>E Frank</author>
<author>I H Witten</author>
</authors>
<title>Humancompetitive tagging using automatic keyphrase extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>1318--1327</pages>
<contexts>
<context position="21490" citStr="Medelyan et al., 2009" startWordPosition="3538" endWordPosition="3541">based on a summarized weighted informativeness score over a document. Obviously, the pure cosine or Jaccard similarity is not a good choice to measure semantic relatedness between two text segments of very low lexical similarity. We thus use the Wikipedia based ESA (Gabrilovich and Markovitch, 2007) to compute the semantic relatedness s(ci, d) and n(ci, cj). To make the calculation more efficient, only the Wikipedia pages whose title is contained in the dataset are used to build the concept space. We ran the algorithm on several datasets including Wiki20 (Medelyan et al., 2008), citeulike180 (Medelyan et al., 2009) and SemEval2010 (Kim et al., 2010) 3. Though keyword extraction as a research topic has a rich literature, to the best of our knowledge there is no large scale datasets publicly available. The Wiki20 dataset contains 20 computer science articles each with around 5 terms labeled by 15 different teams. Every term is a Wikipedia title. 3http://code.google.com/p/maui-indexer/downloads/list The citeulike180 contains a set of 180 papers each tagged with around three tags by 332 users. For each dataset, the collection of all labeled keywords by different taggers are considered as the gold standard f</context>
</contexts>
<marker>Medelyan, Frank, Witten, 2009</marker>
<rawString>O. Medelyan, E. Frank, and I. H. Witten. 2009. Humancompetitive tagging using automatic keyphrase extraction. In Proceedings of EMNLP 2009, pp. 1318–1327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: Bringing Order into Texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>404--411</pages>
<contexts>
<context position="20174" citStr="Mihalcea and Tarau, 2004" startWordPosition="3314" endWordPosition="3317">Extraction There is a rich literature on keyword extraction problem (Frank et al., 1999; Witten et al., 1999; Turney, 2000; Hulth et al., 2003; Tomokiyo and Hurst, 2003; 2http://www.labautopedia.org/mw/index.php/List of programming and computer science terms 263 Wiki20 citeulike180 Method P R F P R F TFIDF 13.7 17.8 15.5 14.4 16.0 15.2 KEA 18.4 21.5 19.8 20.4 22.3 21.3 CTI 19.6 22.7 21.0 18.5 21.4 19.8 Table 3: Results on Wiki20 and citeulike180 Method Precision (%) Recall (%) F1(%) TFIDF 14.9 15.3 15.1 HUMB 27.2 27.8 27.5 CTI 19.3 20.1 19.7 CTI+ 25.3 26.2 25.7 Table 4: Results on SemEval2010 Mihalcea and Tarau, 2004; Medelyan and Witten, 2008; Liu, 2010), most of which is treated as a classification or ranking problem with corresponding machine learning algorithms that use statistical and linguistic features in a corpus. Here, we consider the task as finding the most informative keywords in a document. Given a document d = {ci}, our keyword extraction algorithm based on CTI works as follows. 1. For each context ci in a document, compute the semantic relatedness s(ci, d) between ci and d 2. For each n-gram (1 &lt; n &lt; 4) t in ci, calculate I(t, ci) using Wikipedia based implementation 3. Select the top keywo</context>
<context position="22878" citStr="Mihalcea and Tarau, 2004" startWordPosition="3767" endWordPosition="3770">o investigate other weighting schemes. However, the datasets here are relatively small and the number of tags on which at least two annotators agreed is significantly small; weighting the keywords might not make too much difference. KEA 4 builds a Naive Bayes model using features TFIDF, first occurrence, length of a phrase, and node degree (number of candidates that are semantically related to this phrase) (Witten et al., 1999). First occurrence is computed as the percentage of the document preceding the first occurrence of the term in the document. We compute the node degree as the textrank (Mihalcea and Tarau, 2004) degree in a document by simply relating two candidate terms with each other if they are in the same context. KEA uses 5 fold cross validation. All precision P, recall R and F1 F results are over the top 10 candidate keywords and the micro-averaged results of the first two datasets are shown in Table 3. The CTIbased algorithm works better than KEA on Wiki20 but slightly worse on citeulike180. We argue that the reason might be two-fold. First, CTI does not use any inter-document or corpus information while KEA learns from the corpus. As such, CTI might not perform as well as supervised learning</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. TextRank: Bringing Order into Texts. In Proceedings of EMNLP 2004, pp. 404-411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Page</author>
<author>S Brin</author>
<author>R Motwani</author>
<author>T Winograd</author>
</authors>
<title>The PageRank Citation Ranking: Bringing Order to the Web. Technical Report. Stanford InfoLab.</title>
<date>1998</date>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>L. Page, S. Brin, R. Motwani, and T. Winograd. 1998. The PageRank Citation Ranking: Bringing Order to the Web. Technical Report. Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
</authors>
<title>Why inverse document frequency?</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5841" citStr="Papineni (2001)" startWordPosition="902" endWordPosition="903">and Swanson (Bookstein and Swanson, 1974) introduced the xI as: XI = fw − dfw Church and Gale (1995) introduced where tdw denotes w’s TF in d and ¯tw = fw/D indicates its mean expected word rate. Another measure suggested by them is burstiness(w) = fw (3) dfw which tends to compare collection frequency and document frequency directly. Informative words were found to have IDF scores that are larger than what would be expected according to the Poisson model; residual IDF (RIDF) was introduced to measure this deviation RIDF(w) = IDF(w) − IDF(w) (4) where � IDF(w) = −log2(1 − e−�t-). In addition, Papineni (2001) introduced the notion of gain as dfw (dfw D D f 1 gain(w) = — 1 — log() J (5) More recently, Rennie and Jaakkola (2005) introduced an informativeness score based on the fit of a word’s frequency to a mixture of 2 Unigram distribution and applied it to named entity detection. It is worth noting that term necessity, which measures the probability that a term occurs in documents relevant to a given query, has been well studied in Information Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to me</context>
</contexts>
<marker>Papineni, 2001</marker>
<rawString>K. Papineni. 2001. Why inverse document frequency? In Proceedings of NAACL-HLT 2001, pp. 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D M Rennie</author>
<author>T Jaakkola</author>
</authors>
<title>Using Term Informativeness for Named Entity Detection.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>353--360</pages>
<contexts>
<context position="1316" citStr="Rennie and Jaakkola, 2005" startWordPosition="182" endWordPosition="185">n a term and its context, we model contextaware term informativeness based on semantic similarity between the context and the term’s most featured context in a knowledge base, Wikipedia. We apply our method to three applications: core term extraction from snippets (text segment), scientific keywords extraction (paper), and back-of-the-book index generation (book). The performance is state-of-theart or close to it for each application, demonstrating its effectiveness and generality. 1 Introduction Computationally measuring importance of a word in text, or “term informativeness” (Kireyev, 2009; Rennie and Jaakkola, 2005), is fundamental to many NLP tasks such as keyword extraction, text categorization, clustering, and summarization, etc. Various features derived from statistical and linguistic information can be helpful in encoding term informativeness, whereas practical feature definition and selection are usually ad hoc, data-driven and application dependent. Statistical information based on term frequency (TF) and document frequency (DF) tend to be more effective in finding keywords in large corpora, but can have issues with small amounts of text or small corpora. Linguistic information such as POS tag pat</context>
<context position="5961" citStr="Rennie and Jaakkola (2005)" startWordPosition="926" endWordPosition="929">ed where tdw denotes w’s TF in d and ¯tw = fw/D indicates its mean expected word rate. Another measure suggested by them is burstiness(w) = fw (3) dfw which tends to compare collection frequency and document frequency directly. Informative words were found to have IDF scores that are larger than what would be expected according to the Poisson model; residual IDF (RIDF) was introduced to measure this deviation RIDF(w) = IDF(w) − IDF(w) (4) where � IDF(w) = −log2(1 − e−�t-). In addition, Papineni (2001) introduced the notion of gain as dfw (dfw D D f 1 gain(w) = — 1 — log() J (5) More recently, Rennie and Jaakkola (2005) introduced an informativeness score based on the fit of a word’s frequency to a mixture of 2 Unigram distribution and applied it to named entity detection. It is worth noting that term necessity, which measures the probability that a term occurs in documents relevant to a given query, has been well studied in Information Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to measure the term necessity in a query by considering it as a context. Despite extensive research on semantic analysis and </context>
</contexts>
<marker>Rennie, Jaakkola, 2005</marker>
<rawString>J. D. M. Rennie, and T. Jaakkola. 2005. Using Term Informativeness for Named Entity Detection. In Proceedings of SIGIR 2005, pp. 353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<journal>Information Processing &amp; Management</journal>
<volume>24</volume>
<issue>5</issue>
<contexts>
<context position="4731" citStr="Salton and Buckley, 1988" startWordPosition="715" endWordPosition="718">her methods, or as a feature for learning algorithms. The remainder of this paper is organized as follows. Section 2 reviews the literature of term informativeness measurements. Section 3 proposes the formal definition of the context-aware term informativeness as well as its practical implementation using Web knowledge. Section 4 studies the three applications. Finally, we conclude with discussion and future work. 2 Related Work Most known approaches to measure term informativeness fall into basically two categories: statisticsbased and semantic-based. Statistics-based methods, such as TFIDF (Salton and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). Sprck Jones defines IDF or inverse document frequency as: IDF(w) = −log2(dfw/D) (1) where D is the size of the corpus (Jones, 1972; Jones, 1973). Based on a finding that informative words tend to have large deviation between IDF and collection frequency fw(the total number of occurrence of a word), many other informativeness scores have been proposed. Bookstein and Swanson (Bookstein and Swanson, 1974) introduced the xI as: XI = fw − dfw Church and Gale (1995) intr</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton, and C. Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing &amp; Management 24(5):513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>149--156</pages>
<contexts>
<context position="8022" citStr="Soricut and Marcu, 2003" startWordPosition="1269" endWordPosition="1272">= 1 D (tdw − ¯tw) (2) D − 1d_1 260 Previous methods, all corpus-based, might be effective in identifying informative words at the document or corpus level, but do not the ability to capture term informativeness in a particular context due to their absence of semantics and obliviousness of context. Our method measures the term informativeness within a context in a semantic-based approach, regardless of the absence of statistical information. 3 Context-aware Term Informativeness 3.1 Context A context of a word or phrase may refer to a few words nearby (He et al., 2010), a sentence or paragraph (Soricut and Marcu, 2003), or even a set of documents containing it (Cilibrasi and Vitanyi, 2007). Here we define context as a syntactic unit of discourse such as a sentence or paragraph, for example, “PL/SQL is one of three key programming languages embedded in the Oracle Database”, or “There are two types of functions in PL/SQL”. The universal context set U(t) of a word t is defined as all the contexts containing it in the web. Different contexts vary in their authority just like web pages vary. For the two examples, we could argue that the first context is much more “authoritative” than the second. This can be veri</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of NAACL-HLT 2003, pp. 149–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Terra</author>
<author>C L Clarke</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>165--172</pages>
<contexts>
<context position="28309" citStr="Terra and Clarke, 2003" startWordPosition="4665" endWordPosition="4668"> = 1 − cid (c)−cid(title,) Ntitlec measures the weight based on the le� normalized distance from the context to its direct chapter or sub-chapter title, where cid(c) denotes the id of context c, titlec the title of context c and Ntitlec the number of contexts under titlec. To select candidate terms, we first filter the improbable index terms 265 based on POS patterns using the Standard POS Tagger (Toutanova et al., 2003). We then select multiword keyphrases based on Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which was shown to be the best metric to measure word associations (Terra and Clarke, 2003). To evaluate our back-of-the-book index generation method, we conduct extensive experiments on books in various domains, from the Gutenberg dataset and the open book dataset described in Table 5. The first one was created by (Csomai and Mihalcea, 2006), containing 55 free books collected from Gutenburg6. Since the dataset does not provide the locators of index terms, we can only serve the evaluation as a keyword extraction task. The second dataset was collected from CiteSeer repository, most of which are in computer science and engineering. We extracted the paged body text and the back index </context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>E. Terra and C. L. Clarke. 2003. Frequency estimates for statistical word similarity measures. In Proceedings of NAACL-HLT 2003, pp. 165–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tomokiyo</author>
<author>M Hurst</author>
</authors>
<title>A language model approach to keyphrase extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>3340</pages>
<contexts>
<context position="19718" citStr="Tomokiyo and Hurst, 2003" startWordPosition="3238" endWordPosition="3241">zed, sequentially runtime on all snippets took only slightly more than a minute on a 2.35GHz Intel(R) Xeon(R) 4 processors, 23GB of RAM, and Red Hat Enterprise Linux Server(5.7) machine. However, the time could vary due to network conditions. Though these results look promising, but it could be due to the high lexical similarity between this dataset and Wikipedia content. To test on a more general corpora, we explore more real world tasks. 4.2 Keyword Extraction There is a rich literature on keyword extraction problem (Frank et al., 1999; Witten et al., 1999; Turney, 2000; Hulth et al., 2003; Tomokiyo and Hurst, 2003; 2http://www.labautopedia.org/mw/index.php/List of programming and computer science terms 263 Wiki20 citeulike180 Method P R F P R F TFIDF 13.7 17.8 15.5 14.4 16.0 15.2 KEA 18.4 21.5 19.8 20.4 22.3 21.3 CTI 19.6 22.7 21.0 18.5 21.4 19.8 Table 3: Results on Wiki20 and citeulike180 Method Precision (%) Recall (%) F1(%) TFIDF 14.9 15.3 15.1 HUMB 27.2 27.8 27.5 CTI 19.3 20.1 19.7 CTI+ 25.3 26.2 25.7 Table 4: Results on SemEval2010 Mihalcea and Tarau, 2004; Medelyan and Witten, 2008; Liu, 2010), most of which is treated as a classification or ranking problem with corresponding machine learning alg</context>
</contexts>
<marker>Tomokiyo, Hurst, 2003</marker>
<rawString>T. Tomokiyo and M. Hurst. 2003. A language model approach to keyphrase extraction. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment, 2003, pp. 3340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<pages>252--259</pages>
<contexts>
<context position="28110" citStr="Toutanova et al., 2003" startWordPosition="4631" endWordPosition="4634"> using Wikipedia based implementation 3. Select term-context pairs with the highest wi � I(t, ci) as index entries The weight in step 1 represents the relative importance of a context in a book. w(c) = 1 − cid (c)−cid(title,) Ntitlec measures the weight based on the le� normalized distance from the context to its direct chapter or sub-chapter title, where cid(c) denotes the id of context c, titlec the title of context c and Ntitlec the number of contexts under titlec. To select candidate terms, we first filter the improbable index terms 265 based on POS patterns using the Standard POS Tagger (Toutanova et al., 2003). We then select multiword keyphrases based on Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which was shown to be the best metric to measure word associations (Terra and Clarke, 2003). To evaluate our back-of-the-book index generation method, we conduct extensive experiments on books in various domains, from the Gutenberg dataset and the open book dataset described in Table 5. The first one was created by (Csomai and Mihalcea, 2006), containing 55 free books collected from Gutenburg6. Since the dataset does not provide the locators of index terms, we can only serve the evaluati</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of NAACL-HLT 2003, pp. 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Learning Algorithms for Keyphrae Extraction.</title>
<date>2000</date>
<journal>Information Retrieval</journal>
<pages>2--303</pages>
<contexts>
<context position="19672" citStr="Turney, 2000" startWordPosition="3232" endWordPosition="3233"> algorithm can be easily parallelized, sequentially runtime on all snippets took only slightly more than a minute on a 2.35GHz Intel(R) Xeon(R) 4 processors, 23GB of RAM, and Red Hat Enterprise Linux Server(5.7) machine. However, the time could vary due to network conditions. Though these results look promising, but it could be due to the high lexical similarity between this dataset and Wikipedia content. To test on a more general corpora, we explore more real world tasks. 4.2 Keyword Extraction There is a rich literature on keyword extraction problem (Frank et al., 1999; Witten et al., 1999; Turney, 2000; Hulth et al., 2003; Tomokiyo and Hurst, 2003; 2http://www.labautopedia.org/mw/index.php/List of programming and computer science terms 263 Wiki20 citeulike180 Method P R F P R F TFIDF 13.7 17.8 15.5 14.4 16.0 15.2 KEA 18.4 21.5 19.8 20.4 22.3 21.3 CTI 19.6 22.7 21.0 18.5 21.4 19.8 Table 3: Results on Wiki20 and citeulike180 Method Precision (%) Recall (%) F1(%) TFIDF 14.9 15.3 15.1 HUMB 27.2 27.8 27.5 CTI 19.3 20.1 19.7 CTI+ 25.3 26.2 25.7 Table 4: Results on SemEval2010 Mihalcea and Tarau, 2004; Medelyan and Witten, 2008; Liu, 2010), most of which is treated as a classification or ranking p</context>
</contexts>
<marker>Turney, 2000</marker>
<rawString>P. D. Turney. 2000. Learning Algorithms for Keyphrae Extraction. Information Retrieval 2:303-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>G W Paynter</author>
<author>E Frank</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>Kea: practical automatic keyphrase extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the fourth ACM conference on Digital libraries,</booktitle>
<pages>254--255</pages>
<contexts>
<context position="19658" citStr="Witten et al., 1999" startWordPosition="3228" endWordPosition="3231"> to 76.3%. Though the algorithm can be easily parallelized, sequentially runtime on all snippets took only slightly more than a minute on a 2.35GHz Intel(R) Xeon(R) 4 processors, 23GB of RAM, and Red Hat Enterprise Linux Server(5.7) machine. However, the time could vary due to network conditions. Though these results look promising, but it could be due to the high lexical similarity between this dataset and Wikipedia content. To test on a more general corpora, we explore more real world tasks. 4.2 Keyword Extraction There is a rich literature on keyword extraction problem (Frank et al., 1999; Witten et al., 1999; Turney, 2000; Hulth et al., 2003; Tomokiyo and Hurst, 2003; 2http://www.labautopedia.org/mw/index.php/List of programming and computer science terms 263 Wiki20 citeulike180 Method P R F P R F TFIDF 13.7 17.8 15.5 14.4 16.0 15.2 KEA 18.4 21.5 19.8 20.4 22.3 21.3 CTI 19.6 22.7 21.0 18.5 21.4 19.8 Table 3: Results on Wiki20 and citeulike180 Method Precision (%) Recall (%) F1(%) TFIDF 14.9 15.3 15.1 HUMB 27.2 27.8 27.5 CTI 19.3 20.1 19.7 CTI+ 25.3 26.2 25.7 Table 4: Results on SemEval2010 Mihalcea and Tarau, 2004; Medelyan and Witten, 2008; Liu, 2010), most of which is treated as a classificatio</context>
<context position="22684" citStr="Witten et al., 1999" startWordPosition="3733" endWordPosition="3736">red as the gold standard for a document. We use the set of all keywords for evaluation; otherwise a more complicated evaluation metrics for each dataset will be needed. It would be better to investigate other weighting schemes. However, the datasets here are relatively small and the number of tags on which at least two annotators agreed is significantly small; weighting the keywords might not make too much difference. KEA 4 builds a Naive Bayes model using features TFIDF, first occurrence, length of a phrase, and node degree (number of candidates that are semantically related to this phrase) (Witten et al., 1999). First occurrence is computed as the percentage of the document preceding the first occurrence of the term in the document. We compute the node degree as the textrank (Mihalcea and Tarau, 2004) degree in a document by simply relating two candidate terms with each other if they are in the same context. KEA uses 5 fold cross validation. All precision P, recall R and F1 F results are over the top 10 candidate keywords and the micro-averaged results of the first two datasets are shown in Table 3. The CTIbased algorithm works better than KEA on Wiki20 but slightly worse on citeulike180. We argue t</context>
</contexts>
<marker>Witten, Paynter, Frank, Gutwin, Nevill-Manning, 1999</marker>
<rawString>I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. Nevill-Manning. 1999. Kea: practical automatic keyphrase extraction. In Proceedings of the fourth ACM conference on Digital libraries, 1999, pp. 254– 255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L P Wyman</author>
</authors>
<title>Indexing Specialities: Medicine.</title>
<date>1999</date>
<publisher>Information Today, Inc.</publisher>
<location>Medford, NJ :</location>
<contexts>
<context position="26690" citStr="Wyman, 1999" startWordPosition="4391" endWordPosition="4392">sindexing.org/ an index term or phrase points to its proper location in the text. For example, in “pattern recognition and machine learning” by Bishop, “hidden Markov model” appears in more than 20 pages while the actual index entry has only 2 pages as its locators. Thus the actual problem is to identify a index term with its context. As such, learning a robust and efficient model for real book indexes is challenging. First, books from different domains vary in vocabulary composition and structure style, requiring various indexing specialties. There are different indexing guides for medicine (Wyman, 1999), psychology (Hornyak, 2002), and law (Kendrick and Zafran, 2001). Second, book indexing is a highly subjective work and indexes of different books are always created by different professional indexers who have their own preferences and background (Diodato and Gandt, 1991; Diodato, 1994). Third, the training set is extremely unbalanced. As we found in our dataset, the index size is only 0.42% of the length of book on average. All these motivate us to explore the automatic creation of index terms that are aware of the context at the term’s locations (locators). To do so we propose the following</context>
</contexts>
<marker>Wyman, 1999</marker>
<rawString>L. P. Wyman. 1999. Indexing Specialities: Medicine. Medford, NJ : Information Today, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yazdani</author>
<author>A Popescu-Belis</author>
</authors>
<title>Computing text semantic relatedness using the contents and links of a hypertext encyclopedia.</title>
<date>2012</date>
<journal>Artificial Intelligence</journal>
<pages>194--176</pages>
<contexts>
<context position="6762" citStr="Yazdani and Popescu-Belis, 2012" startWordPosition="1058" endWordPosition="1061">noting that term necessity, which measures the probability that a term occurs in documents relevant to a given query, has been well studied in Information Retrieval community (Zhao and Callan, 2010; Yang and Callan, 2010). Though our CIT is not designed for probabilistic retrieval models, we may apply it to measure the term necessity in a query by considering it as a context. Despite extensive research on semantic analysis and understanding of word and text (Deerwester et al., 1990; Budanitsky and Hirst, 2006; Cilibrasi and Vitanyi, 2007; Gabrilovich and Markovitch, 2007; Agirre et al., 2009; Yazdani and Popescu-Belis, 2012), little work studied the measurement of the semantics of term informativeness. An exception is the LSAspec from Kireyev (2009), based on latent semantic analysis (Deerwester et al., 1990), which is defined as the ratio of a term’s LSA vector length to its document frequency and thus can be interpreted as the rate of vector length growth. However, latent semantic models such as LSA are notoriously hard to interpret since the “latent concepts” cannot be readily mapped to human knowledge (Gabrilovich and Markovitch, 2007). Our approach explicitly leverages the semantics of word and text using ex</context>
</contexts>
<marker>Yazdani, Popescu-Belis, 2012</marker>
<rawString>M. Yazdani and A. Popescu-Belis. 2012. Computing text semantic relatedness using the contents and links of a hypertext encyclopedia. Artificial Intelligence 194:176–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zobel</author>
<author>A Moffat</author>
</authors>
<title>Exploring the similarity space.</title>
<date>1998</date>
<journal>ACM SIGIR Forum</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="11837" citStr="Zobel and Moffat, 1998" startWordPosition="1916" endWordPosition="1919">the contexts from ta’s featured context set will be more related to c than will tb. Thus, given a term t and its featured context set Uf(t) = {c1, ..., ck}, we define the term informativeness of t in context ci as I(t, ci) = � l,(ci,cj) · CA(cj) (7) cj∈Uf (t) where l,(ci,cj) is the semantic relatedness of ci and cj, which can be computed by various semantic relatedness metrics such as Wikipedia based (Gabrilovich and Markovitch, 2007; Yazdani 261 and Popescu-Belis, 2012), Wordnet based (Agirre et al., 2009; Budanitsky and Hirst, 2006), or simple cosine similarity and Jaccard similarity based (Zobel and Moffat, 1998). The context-aware term informativeness (CTI) introduced above is a formal and general definition. As such the definition in Equation (7) includes several features such as context authority score, featured context set, semantic relatedness, and knowledge base, any or all of which could be flexible for different applications. 3.3 Implementation Here, we present a simple practical implementation using Wikipedia as the knowledge base and the context authority estimated by the discounted rank of the Wikipedia document. Note that the problem is how to compute CA(cj) for each context in Uf. We rewr</context>
</contexts>
<marker>Zobel, Moffat, 1998</marker>
<rawString>J. Zobel and A. Moffat. 1998. Exploring the similarity space. ACM SIGIR Forum 32(1):18–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhao</author>
<author>Jamie Callan</author>
</authors>
<title>Term necessity prediction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>259--268</pages>
<marker>Le Zhao, Callan, 2010</marker>
<rawString>Le Zhao and Jamie Callan. 2010. Term necessity prediction. In Proceedings of the 19th ACM international conference on Information and knowledge management, 2010, pp. 259–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>A metric-based framework for automatic taxonomy induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>271--279</pages>
<marker>Yang, Callan, 2009</marker>
<rawString>Hui Yang and Jamie Callan. 2009. A metric-based framework for automatic taxonomy induction. In Proceedings of the ACL, 2009, pp. 271–279 .</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>