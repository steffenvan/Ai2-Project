<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9974985">
A Maximum Entropy Approach to HowNet-Based
Chinese Word Sense Disambiguation
</title>
<note confidence="0.551732666666667">
WONG Ping Wai
Intendi Inc.,
Clear Water Bay, Hong Kong.
</note>
<email confidence="0.986213">
wongpw@intendi.com
</email>
<sectionHeader confidence="0.993807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953133333333">
This paper presents a maximum entropy
method for the disambiguation of word
senses as defined in HowNet. With the
release of this bilingual (Chinese and
English) knowledge base in 1999, a corpus
of 30,000 words was sense tagged and
released in January 2002. Concepts
meanings in HowNet are constructed by a
closed set of sememes, the smallest meaning
units, which can be treated as semantic tags.
The maximum entropy model treats
semantic tags like parts-of-speech tags and
achieves an overall accuracy of 89.39%,
outperforming a baseline system, which
picks the most frequent sense.
</bodyText>
<sectionHeader confidence="0.998115" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.972198465116279">
A word usually has more than one meaning or
sense, which are listed in the dictionary. The task
of Word Sense Disambiguation (WSD) is to make
the choice between the senses for a particular
usage of the word in context. There are, however,
several difficulties to WSD (Yang et al, 2000): (i)
The evaluation of word sense disambiguation
system is not yet standardized. (ii) The potential
for WSD varies by task. (iii) Sense-tagged corpora
are crucial resources for WSD but they are
difficult to obtain. Efforts in building large
Chinese corpora started in the 90s, for example,
the Sinica corpus (CKIP, 1995) and the Chinese
Penn Tree Bank (Xia et al., 2000). However, these
two corpora concentrate on the tagging of
parts-of-speech and syntactic structures, while
little work has been done on semantic annotation.
Of the few efforts that were carried out, Lua1
annotated 340,000 words with semantic classes
defined in a thesaurus (Mei, 1983). This resource,
however, was not publicly accessible. With the
release of HowNet (Dong, 1999; Dong, 2000) in
YANG Yongsheng
Department of Computer Science,
HKUST, Clear Water Bay, Hong Kong.
ysyang@cs.ust.hk
1999, Gan and Tham (1999) manually annotated a
Chinese corpus of 30,000 words with the senses
from HowNet. The corpus is a subset of the Sinica
balanced corpus, and consists of 103 narratives on
news stories, in which the words have already
been segmented and tagged with parts-of-speech.
Gan and Tham (1999) added sense tagging and
subsequently Gan and Wong (2000) annotated the
corpus with semantic dependency relations as
defined in HowNet. The corpus was released to the
public in January 2002 2 , providing essential
resources for Chinese word sense disambiguation.
This paper is organized as follows: Section 2 gives
an introduction of HowNet. Section 3 describes the
WSD task and the experiment results. Section 4
describes the previous work, followed by a
conclusion in Section 5.
</bodyText>
<sectionHeader confidence="0.931919" genericHeader="method">
2. An Introduction to HowNet
</sectionHeader>
<bodyText confidence="0.924727941176471">
HowNet is a bilingual general knowledge base that
encodes inter-concept semantic relations and the
inter-attribute semantic relations. In contrast to
WordNet (Miller, 1990), HowNet adopts a
constructive approach of meaning representation
(Miller, 1993). Basic meaning units called
sememes, which cannot be decomposed further,
combine to construct concepts in HowNet. So far,
there are 65,000 Chinese concepts and 75,000
English equivalents defined with a set of 1503
sememes.
NO.=the record number of the lexical entries
W_X=concept of the language X
E_X=example of W_X
G_X=Part-of-speech of the W_X
DEF=Definition, which is constructed by sememes and
pointers
</bodyText>
<figureCaption confidence="0.950155">
Figure 1: A sample lexical entry in HowNet.
Figure 1 gives an idea of how word concepts are
organized in HowNet. “X” represents some
</figureCaption>
<footnote confidence="0.414714">
1 http://www.cslp.com.nus.edu/sg/cslp/ 2 http://godel.iis.sinica.edu.tw/CKIP/hk/index.html
</footnote>
<bodyText confidence="0.998805888888889">
language and each language has three specific
items: W_X, E_X and G_X. The current version of
HowNet has entries in two languages (Chinese and
English) with the possibility of extending it to
other languages. Therefore, W_C, E_C and G_C
would be entries for the words, the examples and
the parts-of-speech respectively in Chinese,
whereas W_E, E_E and G_E are the corresponding
entries for English.
</bodyText>
<figureCaption confidence="0.983612">
Figure 2: An example entry in HowNet.
</figureCaption>
<bodyText confidence="0.98380872">
Figure 2 shows an example word, “journalist”, as
entered in HowNet. As mentioned in Miller (1993),
the definition of a common noun typically consists
of (i) its immediate superordinate term and (ii)
some distinguishing features. HowNet represents
this with pointers3 and the order of the sememes
in concept definitions. In the example above, the
sememe appearing in the first position ‘human |’
is called the categorical attribute. It names the
hypernym or the superordinate term, which gives a
general classification of the concept. The sememes
appearing in other positions: ‘occupation |’,
‘gather |’, ‘compile |’, ‘news |’ are
additional attributes, which provide more
specific, distinguishing features. Two types of
pointers are used in this concept. The pointer “#”
means “related” and thus ‘#occupation |’
shows that there is a relation between the word
“journalist” and occupations. The pointer “*”
means ‘agent’, and thus, ‘*gather |’ and
‘*compile |’ tell us that “journalist” is the
agent of ‘gather |’ and ‘compile |’. The
sememe ‘#news |’ that follows tells us that the
function of “journalist” is to compile and gather
news.
</bodyText>
<footnote confidence="0.7769725">
3 The function of pointers is to describe various
inter-concept and inter-attribute relations. Please refer
to HowNet’s homepage (http://www.keenage.com) or
Gan and Wong (2000) for details.
</footnote>
<subsectionHeader confidence="0.998554">
2.1. Classification of content words
</subsectionHeader>
<bodyText confidence="0.832578923076923">
Concepts of content words in HowNet are
classified into six categories: Entity, Event,
Attribute, Quantity, Attribute Value and Quantity
value. The sememes in each category are
organized hierarchically in an ontology tree. The
six categories can be grouped into four main types:
(i) Entity, (ii) Event, (iii) Attribute and Quantity,
(iv) Attribute Value and Quantity Value. Most
nominal concepts, such as “journalist”, belong to
the Entity category and some of them belong to the
Attribute category. Verbal concepts always belong
to the Event category whereas adjectives are
Attribute Values.
</bodyText>
<subsectionHeader confidence="0.7592965">
2.1.1. Convention of meaning represent-
ation of content words
</subsectionHeader>
<bodyText confidence="0.984180428571429">
The first sememe in concept definitions indicates
which of the four categories the concept belongs to,
and it is therefore called the categorical attribute.
For Attribute, Quantity, Attribute Value and
Quantity Value, the first sememe clearly names the
categories, as illustrated in (iii) and (iv) of Table 1.
Table 2 shows an example entry: the category of
“ ” (brightness) is indicated by the first
sememe ‘Attribute |’. The second sememe is a
node in the hierarchy of Attribute or Quantity that
names the subcategory. For example, ‘brightness|
’ is a node under the ontological hierarchy of
‘Attribute |’ 4 , and can be viewed as a
subcategory of Attribute.
</bodyText>
<tableCaption confidence="0.992456">
Table 1: An overview of the order of sememes
in concept definitions of HowNet
</tableCaption>
<table confidence="0.976870555555555">
Category Sememes in concept definitions
Categorical Additional
Attribute Attribute
1st position 2nd and thereafter position
(optional)
(i) Entity node in “secondary feature” OR
Entity “node in (iv)” 5 OR
“pointer”“node in (i), (ii),
(iii) or (iv) ”
</table>
<tableCaption confidence="0.375707">
4 Sememes are organized hierarchically so that
</tableCaption>
<bodyText confidence="0.842038857142857">
brightness |is the hyponym of Attribute |,
other words, brightness |is a kind of Attribute|
.
5 (i) stands for Entity, (ii) Event, (iii) Attribute and
Quantity, (iv) Attribute Value and Quantity Value.
Secondary features include the sememes that cannot be
categorized into types (i) – (iv).
</bodyText>
<equation confidence="0.591453222222222">
NO.=040263
W_C=
G_C=N
E_C=
W_E=journalist
G_E=N
E_E=
DEF=human |,#occupation |,*gather |,
*compile |,#news|
</equation>
<table confidence="0.991387272727273">
Event node in Event “secondary feature” OR
“event role”=“node in (i),
(ii), (iii) or (iv)”
Category Sememes in concept definitions
Categorical Attribute Additional
Attribute
1st position 2nd position 3rd position
Attribute attribute |node in Attribute &amp;“Host”
Quantity quantity |node in Quantity &amp;“Host”
aValue6 aValue |node in Attribute “Value”
qValue qValue |node in Quantity “Value”
</table>
<tableCaption confidence="0.9143145">
Table 2: Examples of concepts of the categories
of Attribute and Attribute value
</tableCaption>
<table confidence="0.667591166666667">
Concepts Sememes in concept definitions
1st position 2nd position 3rd position
(brightness) attribute |brightness |, &amp;physical|
, 7
(bright) aValue |brightness |, bright |8
,
</table>
<bodyText confidence="0.953540888888889">
For the categories of Entity and Event, it is not
necessary to name the main categories, because
this information is conveyed by their subcategories.
Table 3 shows two examples. The first sememe of
“ ” (letter paper) is ‘paper |’, a node in the
Entity hierarchy and its function is to indicate the
subcategory of Entity. ‘ |SetAside’, as the first
sememe of the concept “ ” (deposit money),
names the subcategory of Event.
</bodyText>
<tableCaption confidence="0.929557">
Table 3: Examples of concepts of the categories
of Event and Enti
</tableCaption>
<table confidence="0.9840445">
Concepts Sememes in concept definitions
1st position 2nd position 3rd position
(letter paper |, @write|
paper)
(deposit SetAside |patient=money |commercial|
money) , ,
</table>
<subsubsectionHeader confidence="0.836789">
2.1.2. Categorical Attribute
</subsubsectionHeader>
<bodyText confidence="0.988993">
The categories of Attribute and Attribute Value
share parallel subcategories. As an example, Table
2 shows one of them: the subcategory
‘brightness |’. Therefore, it is not adequate to
</bodyText>
<footnote confidence="0.419536">
6 &amp;quot;aValue&amp;quot; stands for attribute value whereas &amp;quot;qValue&amp;quot;
stands for quantity value.
</footnote>
<page confidence="0.972769">
7 &amp;quot;, &amp;quot; separates one sememe from the other in the
</page>
<bodyText confidence="0.996672692307693">
definitions, and is not part of the sememe. &amp;quot;&amp;&amp;quot;
represents attriubte-host relation.
8 “ ”(bight) is a value of the attribute ‘brightness|
’. &amp;quot;Value&amp;quot; is the terminal node of Attribute Value.
It is optional in some cases.
identify only the subcategory when dealing with
Attributes or Attribute Values. That is why these
two categories (along with Quantity and Quantity
Value) use the first two sememes for the
subcategorization of concepts, whereas Entity and
Event can achieve this by using the first sememe
only. We call such types of sememes “categorical
attributes”.
</bodyText>
<subsectionHeader confidence="0.992342">
2.2. Function Words
</subsectionHeader>
<bodyText confidence="0.999973888888889">
Unlike WordNet, HowNet has a sense inventory
for function words, and thus our WSD system
includes both content words and function words.
For function words such as prepositions, pronouns
and conjunctions, the sememes in the definitions
are marked by curly brackets in order to
distinguish senses of function words from those of
content words. For example, the pronoun “ ” (he)
is defined as {ThirdPerson |,male |}.
</bodyText>
<sectionHeader confidence="0.991056" genericHeader="method">
3. Task Description
</sectionHeader>
<subsectionHeader confidence="0.998786">
3.1. Preprocessing of the corpus
</subsectionHeader>
<bodyText confidence="0.999899857142857">
The HowNet corpus is written in XML format, and
contains the part-of-speech, sense and semantic
dependency relation information for each word.
There are 30,976 word tokens and 3,178
sentences 9 in the HowNet corpus, which is
divided into two sets in the experiment: 2,400
sentences (23,191 word tokens) are reserved for
training, and 778 sentences (7,785 word tokens)
for testing. Since off-the-shelf software systems
usually have a default cut-off value that may not
be appropriate for such a small corpus, we create a
larger corpus by concatenating 3 copies of the
training data. As a result, the final training corpus
consists of 7,200 sentences (69,573 words).
</bodyText>
<subsectionHeader confidence="0.557878">
3.2. Experiments
</subsectionHeader>
<subsubsectionHeader confidence="0.436232">
3.2.1. Maximum Entropy Tagger
</subsubsectionHeader>
<bodyText confidence="0.999954875">
The goal of this work is to investigate the
possibility of applying standard POS taggers to
identify word sense tags. For this work, an
off-the-shelf maximum entropy tagger 10
(Ratnaparkhi, 1996) was used. Each word is
therefore tagged with a sememe (categorical
attribute), which is treated equivalently to a POS
tag by the tagger, whose goal it is to generate a
</bodyText>
<tableCaption confidence="0.238196">
9 Sentences are delimited by the following
punctuations : ; ! ?
10 ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz
</tableCaption>
<bodyText confidence="0.9997845">
sense tag dictionary from the training data. In the
following subsections, we will first explain the
semantic tags used in the current research, its
limitations and suggestion for resolving the
problem, and then illustrate how to build the tag
dictionary for the MaxEnt sense tagger.
</bodyText>
<subsectionHeader confidence="0.817604">
3.2.2. Using categorical attributes as
semantic tags
</subsectionHeader>
<bodyText confidence="0.996628935483871">
As illustrated in section 2, there are about 65,000
concepts in HowNet dictionary, defined by 17216
sense definitions. The number of definitions will
still increase in future, but the closed set of 1503
sememes is not likely to expand. Definitions are
represented by a sequence of sememes in HowNet.
It is possible to use the whole sequences of
sememes as semantic tags, but the complexity can
be greatly reduced by using the 1503 sememes as
semantic tags.
As illustrated earlier, in HowNet, the category for
a particular word concept is determined by the first
sememe (for Entities and Events) or the first two
sememes (for Attributes, Quantities or Attribute
Values). These sememes are thus referred to as
categorical attributes. On observation, it became
apparent that just picking the categorical attribute
would be enough to differentiate one sense from
the other. For example, none of the 27 senses for
the polysemous word “ ” (hit) in Chinese share
the same first sememe.
Using sememes as semantic tags has an advantage
over using a simple sense id. Assigning a sense id
such as 1, 2.... 27 to each sense of the word
“ ” can distinguish different senses but will not
give us any idea of the meanings of the ambiguous
words. Sememes convey meanings while helping
to differentiate senses. For example, the first sense
is ‘associate |’, which indicates an association
with friends or partners. The second sense is
‘build |’, which is self-explanatory.
</bodyText>
<subsubsectionHeader confidence="0.638574">
3.2.3. Limitation of the semantic tags
</subsubsectionHeader>
<bodyText confidence="0.9850488">
There is a limitation to this strategy. It is found
that this strategy can discriminate the senses for
about 90% of the words in the corpus. The
remaining 10% of the words are still ambiguous
(Table 4).
</bodyText>
<tableCaption confidence="0.86085">
Table 4: Word tokens still have ambiguity after the
tagging of categorical attribute
</tableCaption>
<table confidence="0.9929305">
Training Testing
Total word tokens 69573 7785
Word tokens still have 7461 878
ambiguity after the tagging
of categorical attribute
Percentage 10.72% 11.28%
</table>
<tableCaption confidence="0.687247">
Table 5 shows the senses for the word “ ” (one).
</tableCaption>
<bodyText confidence="0.9075142">
Since all the senses are Quantities (qValue |)
and Attribute Value (aValue |) types, the
categorical attribute is defined as the first two
sememes. However, there is still ambiguity to be
resolved for two of the senses.
</bodyText>
<tableCaption confidence="0.921103">
Table 5: Senses for the word “ ” (one)
</tableCaption>
<bodyText confidence="0.958475864864865">
Categorical Sense
Attribute
qValue |, qValue |,amount |,
amount|
cardinal|
qValue |, qValue |,amount |,
amount |single|
aValue |, aValue |,range |,all|
range|
aValue |, aValue |,frequency|
frequency|
aValue |, aValue |,sequence |,
sequence |ordinal|
3.2.4. Mapping categorical attribute to sense
definition
In this work, the ambiguity problem is solved by
building a mapping table which maps the (word ;
categorical attribute) pairs to sense definitions.
First a frequency table is built, which accounts for
the frequency of occurrence that a (word ;
categorical attribute) pair should be mapped to a
sense in the training corpus. Table 5 shows the
categorical attributes for the word “ ” (one). The
‘qValue |,amount |,cardinal |’ sense
appears 145 times, while the ‘qValue|
,amount |,single |’ sense appears only 16
times. In this work, we simply disregard the
second sense for this situation, and assume that
when the word “ ” (one) is tagged with the
categorical attribute ‘qValue |,amount |’,
it corresponds to the ‘qValue |,amount|
,cardinal |’ sense in all contexts. There is a
one-to-one direct mapping of the categorical
attributes to the 3rd, 4th and the 5th senses, so
frequency information is not needed for them.
corresponding sense according to the mapping
table.
</bodyText>
<note confidence="0.427978">
3.2.5. Sense Tag dictionary for MaxEnt
Tagger
</note>
<bodyText confidence="0.9999582">
Section 3.2.4 illustrates the mapping of a sense tag
to a sense definition, and this section will briefly
describe the building of the tag dictionary. There
are two sources for the sense tag dictionary. One
comes from the training corpus and one from the
HowNet dictionary. The MaxEnt tagger
automatically creates a tag dictionary from the
training corpus. By default, this dictionary only
includes words that appear more than four times in
the training corpus (total 753 word types). 11
Another source is the HowNet dictionary, which
has the information of semantic tags for 51275
word types. The two sources of information are
combined in the sense tag dictionary for the
maximum entropy tagger.
</bodyText>
<subsectionHeader confidence="0.998981">
3.3. Testing results
</subsectionHeader>
<bodyText confidence="0.999921">
The input of the testing component is the testing
corpus, which is already segmented. The output is
the most likely senses of words given by the WSD
systems.
</bodyText>
<subsectionHeader confidence="0.524666">
3.3.1. Baseline system
</subsectionHeader>
<bodyText confidence="0.999983333333333">
As a baseline system, the most frequent sense
(MFS) of a word is chosen as the correct sense.
The frequency of word senses is calculated from
the occurrences of the word senses in the training
corpus, with ties broken randomly. For all
instances of unknown words, the baseline system
just tags them with the most frequent sense for the
rare words (that is, ‘human |,ProperName |’ as
shown in Table 7).
</bodyText>
<subsectionHeader confidence="0.580916">
3.3.2. Maximum entropy
</subsectionHeader>
<bodyText confidence="0.929298652173913">
The model first checks if the word in context can
be found in HowNet dictionary. In case the word
has only one sense in the dictionary, there is no
need to perform disambiguation for this word and
the system returns this sense as the answer.
For words with more than one sense, the
maximum entropy model chooses one (categorical
attribute) from the closed set of sememes. The
categorical attribute is mapped to the
11 Words occurring less than 5 times in the training
corpus are treated as rare words. The tagging of rare
words are illustrated in section 3.3.
Table 6 shows the results for both the baseline and
the maximum entropy system. It can be seen that
the MaxEnt tagger achieves an accuracy of
88.94%, which outperforms that of the baseline
system. An upper bound can also be calculated
by imagining that we could employ an oracle
system that would indicate, for each ambiguous
semantic tag (described in Section 3.2.4), the
correct sense of the word. In that case, the
performance of the maximum entropy tagger
would improve to 89.73%.
</bodyText>
<tableCaption confidence="0.9985795">
Table 6: The accuracy rate of MFS and MaxEnt for
overall, polysemous and unknown word
</tableCaption>
<table confidence="0.9998905">
Accuracy
MFS MaxEnt
Performance Overall 84.63% 88.94%
Unknown 45.83% 72.50%
Polysemous 69.65% 77.33%
Semantic tag Overall 86.48% 89.73%
(categorical
attribute only,
effective upper
bound)
Unknown 46.39% 75.00%
Polysemous 71.72% 77.42%
</table>
<tableCaption confidence="0.990258">
Table 7: Sense distribution and tagging accuracy of
unknown words
</tableCaption>
<table confidence="0.999575444444444">
Sense Freq. Accuracy
MFS MaxEnt
165 100% 95.15%
84 0% 96.43%
28 0% 75.00%
31 0% 93.55%
20 0% 40.00%
Other senses 30 0% 3.33%
Total 360 45.83% 82.50%
</table>
<bodyText confidence="0.9993765625">
Even though it does not look like the maximum
entropy tagger outperforms the baseline system by
much, it should be noted that the nature of the
corpus makes the task simple for the baseline
system. Since the corpus is composed of a
collection of news stories, certain senses of
polysemous words will tend to appear more often
in the corpus --- indeed, it was observed that more
than half of the word tokens appearing in the
training and testing corpus have only one sense.
The average sense per word token is 1.14 and 1.09
in the training and the testing sets, respectively.
However, it should be noted that the MaxEnt
model performs much better on polysemous words
and unknown words, which bodes well for using
the MaxEnt model with more diverse corpora.
</bodyText>
<tableCaption confidence="0.992129">
Table 8: Average senses per word in the training
data and the testing data
</tableCaption>
<table confidence="0.994205">
Training Testing
word tokens 69,573 7,785
word tokens with one 4,2990 4,905
sense only (61.78%) (63.01%)
average sense per word 1.14 1.09
token
</table>
<bodyText confidence="0.989099">
One of the strengths of maximum entropy lies in
its ability to use contextual information to
disambiguate polysemous words and predict the
senses of unknown words. The following shows an
unknown word “ ” with the context
information:
</bodyText>
<table confidence="0.887789058823529">
Word previous current next
Tag time |Unknown
Table 10: Features and possible tags of the unknown
word “ ”
Features Possible tags of current word
“ ”
prefix is qValue |,sequence|
“ ”(twenty)
suffix is qValue |,sequence|
“ ”(eight)
next word is qValue |,sequence |OR
“ ”(day)
qValue |,amount|
previous tag qValue |,sequence |OR
is ‘time |’
qValue |,amount |OR
time|
</table>
<bodyText confidence="0.916955076923077">
The MaxEnt tagger defines a set of feature patterns
including the previous word, the next word, the
previous tag, the prefix and the suffix of current
word. In this example, the features extracted from
the context are shown above. Accordingly, the
MaxEnt tagger predicts ‘qValue |,sequence|
’ as the most likely sense tag for the word “
12 The meaning of the phrase is &amp;quot;the
twenty-eighth of January&amp;quot;. The correct sesne of “ ”
”. The tag ‘qValue |,sequence |’ is
then mapped to the sense definition ‘qValue|
,sequence |,cardinal |’ according to the
mapping table.
</bodyText>
<sectionHeader confidence="0.994933" genericHeader="method">
4. Previous Work
</sectionHeader>
<bodyText confidence="0.999985923076923">
To our knowledge, there currently exist three
previous studies of word sense disambiguation
using HowNet. Yang et al (2000) pioneered this
work by using sememe co-occurrence information
in sentences from a large corpus to achieve an
accuracy of 71%. Yang and Li (2002), collecting
sememe co-occurrence information from a large
corpus, transferred the information to restricted
rules for sense disambiguation. They reported a
precision rate of 92% and 82% for lexical
disambiguation and structural disambiguation,
respectively.
Wang (2002) pioneered the work of sense pruning
using the hand-coded knowledge base of HowNet.
Unlike sense disambiguation, sense pruning seeks
to narrow down the possible senses of a word in a
text. Using databases of features such as
information structure and object-attribute relations
which were compiled from HowNet, Wang
reported a recall rate of 97.13% and a per sentence
complexity reduction rate of 47.63%.
The current study and Wang (2002) used the sense
tagged HowNet corpus with different approaches.
There is one similarity between our work and
Wang (2002), though. Wang applied a sense
pruning method to reduce the complexity of word
senses. The strategy of the current study reduces
the complexity of sense tagging by using the
categorical attributes (first or the first two
sememes) as semantic tags. About 10% of the
words are still ambiguous, but the ambiguity can
be reduced in future studies which extend to the
tagging of the sememes in the third and the
thereafter position of concept definitions. It is also
interesting to see if the ambiguity can be resolved
by integrating a diverse set of various knowledge
sources, such as HowNet knowledge bases,
sememe cooccurrence database and the tagged
corpus.
</bodyText>
<sectionHeader confidence="0.997322" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.94929575">
This paper has presented the method of maximum
entropy to perform word sense disambiguation in
Chinese with HowNet senses. The closed set of
sememes is treated as semantic tags, similar to
</bodyText>
<tableCaption confidence="0.754002">
Table 9: Example of an unknown word: “ ”12
</tableCaption>
<bodyText confidence="0.970338647058824">
is ‘ordinal number’, defined by ‘qValue|
,sequence |,cardinal |’ in HowNet.
parts-of-speech tagging in the model. Our system
performs better than the baseline system that
chooses the most frequent sense. Our strategy of
sememe tagging reduces the complexity of
semantic tagging in spite of some limitations.
Some possible ways to resolve the limitations are
also suggested in the paper. Unlike the work of
Yang et al (2000) and Wang (2002) that applied
unsupervised methods using sense definitions in
HowNet, the paper is the first study to use a
supervised learning method with the availability of
the HowNet sense tagged corpus. Much research
remains to be done on the corpus and the HowNet
knowledge base to get further improvement on the
WSD task.
</bodyText>
<sectionHeader confidence="0.998492" genericHeader="references">
6. Acknowledgement
</sectionHeader>
<bodyText confidence="0.883958666666667">
Our thanks go to Dr. Grace Ngai for her helpful
comments. This work was supported and funded
by Intendi Inc.
</bodyText>
<reference confidence="0.984574763157895">
,
95-02), Institute of Information Science,
Academia Sinica.
Dong, Zhendong (1999) Bigger Context and Better
Understanding – Expectation on Future MT
Technology. In Proceedings of International
Conference on Machine Translation and Computer
Language Information Processing, 26-28 June,
1999, Beijing, China, pp. 17-25.
Report 43, Cognitive Science Laboratory,
Princeton University.
Ratnaparkhi, Adwait (1996) A Maximum Entropy
Model for Part-of-Speech Tagging. In Proceedings
of the First Empirical Methods in Natural
Language Processing Conference, pp. 133-141,
Philadelphia, USA.
Yang, Erhong, Guoqing Zhang and Yongkui Zhang
(2000) The Research of Word Sense
Disambiguation Method Based on Co-occurrence
Frequency of HowNet. In Proceedings of the
second Chinese language processing workshop,
ACL 2000 Conference, October 2000, pp.60-65.
Wang, Chi-Yung (2002) Sense Pruning by HowNet – a
knowledge-based Word Sense Disambiguation.
MPhil Thesis. Hong Kong University of Science
and Technology.
Xia, Fei, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe
Huang, Tony Kroch and Mitch Mrcus (2000)
Developing Guidelines and Ensuring Consistency
for Chinese Text Annotation. In Proceedings of the
second International Conference on Language
Resources and Evaluation (LREC-2000), Athens,
Greece.
Yang, Xiaofeng, Tangqiu Li (2002) A Study of Semantic
Disambiguation Based on HowNet, International
Journal of Computational Linguistics and Chinese
Language Processing, vol. 7, no.1 , 2002, pp.47-78.
</reference>
<figure confidence="0.89136425">
7. References
CKIP (1995) The Content and Illustration of Sinica
Corpus of Academia Sinica, Technical Report no.
95-02 (
</figure>
<reference confidence="0.971971">
Dong, Zhendong (2000) HowNet:
http://www.keenage.com.
Gan, Kok-Wee and Wai-Mun Tham (1999) General
Knowledge Annotation Based on HowNet (
). Computational Linguistics
and Chinese Language Processing, vol. 4, 1999, pp.
39-86.
Gan, Kok-Wee and Ping-Wai Wong (2000) Annotating
Information Structures in Chinese Text using
HowNet. In Proceedings of the 2nd Chinese
Language Processing Workshop, Association for
Computational Linguistics 2000 Conference,
October 2000, Hong Kong, pp. 85-92.
Mei, Jiaju, Yiming Lau, Yunqi Gao, Yongxiang Ying
(1983) A Dictionary of Synonyms ( ✰),
Shanghai Cishu Chubanshe.
Miller, George A. (1990) WordNet: An Online Lexical
Database. In Special Issue of International Journal
of Lexicography, Vol 3, No. 4.
Miller, George A. (1993) Nouns in WordNet: a lexical
inheritance system. Five Papers on WordNet, CSL
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552431">
<title confidence="0.89959325">A Maximum Entropy Approach to Chinese Word Sense Disambiguation WONG Ping Intendi</title>
<author confidence="0.659231">Clear Water Bay</author>
<author confidence="0.659231">Hong</author>
<email confidence="0.999704">wongpw@intendi.com</email>
<abstract confidence="0.9995775">This paper presents a maximum entropy method for the disambiguation of word senses as defined in HowNet. With the release of this bilingual (Chinese and English) knowledge base in 1999, a corpus of 30,000 words was sense tagged and released in January 2002. Concepts meanings in HowNet are constructed by a closed set of sememes, the smallest meaning units, which can be treated as semantic tags. The maximum entropy model treats semantic tags like parts-of-speech tags and achieves an overall accuracy of 89.39%, outperforming a baseline system, which picks the most frequent sense.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<institution>Institute of Information Science, Academia Sinica.</institution>
<marker></marker>
<rawString>95-02), Institute of Information Science, Academia Sinica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
</authors>
<title>Bigger Context and Better Understanding – Expectation on Future MT Technology.</title>
<date>1999</date>
<booktitle>In Proceedings of International Conference on Machine Translation and Computer Language Information Processing,</booktitle>
<pages>17--25</pages>
<location>Beijing, China,</location>
<contexts>
<context position="1794" citStr="Dong, 1999" startWordPosition="284" endWordPosition="285">agged corpora are crucial resources for WSD but they are difficult to obtain. Efforts in building large Chinese corpora started in the 90s, for example, the Sinica corpus (CKIP, 1995) and the Chinese Penn Tree Bank (Xia et al., 2000). However, these two corpora concentrate on the tagging of parts-of-speech and syntactic structures, while little work has been done on semantic annotation. Of the few efforts that were carried out, Lua1 annotated 340,000 words with semantic classes defined in a thesaurus (Mei, 1983). This resource, however, was not publicly accessible. With the release of HowNet (Dong, 1999; Dong, 2000) in YANG Yongsheng Department of Computer Science, HKUST, Clear Water Bay, Hong Kong. ysyang@cs.ust.hk 1999, Gan and Tham (1999) manually annotated a Chinese corpus of 30,000 words with the senses from HowNet. The corpus is a subset of the Sinica balanced corpus, and consists of 103 narratives on news stories, in which the words have already been segmented and tagged with parts-of-speech. Gan and Tham (1999) added sense tagging and subsequently Gan and Wong (2000) annotated the corpus with semantic dependency relations as defined in HowNet. The corpus was released to the public in</context>
</contexts>
<marker>Dong, 1999</marker>
<rawString>Dong, Zhendong (1999) Bigger Context and Better Understanding – Expectation on Future MT Technology. In Proceedings of International Conference on Machine Translation and Computer Language Information Processing, 26-28 June, 1999, Beijing, China, pp. 17-25.</rawString>
</citation>
<citation valid="false">
<tech>Report 43,</tech>
<institution>Cognitive Science Laboratory, Princeton University.</institution>
<marker></marker>
<rawString>Report 43, Cognitive Science Laboratory, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the First Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>133--141</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="11112" citStr="Ratnaparkhi, 1996" startWordPosition="1731" endWordPosition="1732"> word tokens) are reserved for training, and 778 sentences (7,785 word tokens) for testing. Since off-the-shelf software systems usually have a default cut-off value that may not be appropriate for such a small corpus, we create a larger corpus by concatenating 3 copies of the training data. As a result, the final training corpus consists of 7,200 sentences (69,573 words). 3.2. Experiments 3.2.1. Maximum Entropy Tagger The goal of this work is to investigate the possibility of applying standard POS taggers to identify word sense tags. For this work, an off-the-shelf maximum entropy tagger 10 (Ratnaparkhi, 1996) was used. Each word is therefore tagged with a sememe (categorical attribute), which is treated equivalently to a POS tag by the tagger, whose goal it is to generate a 9 Sentences are delimited by the following punctuations : ; ! ? 10 ftp://ftp.cis.upenn.edu/pub/adwait/jmx/jmx.tar.gz sense tag dictionary from the training data. In the following subsections, we will first explain the semantic tags used in the current research, its limitations and suggestion for resolving the problem, and then illustrate how to build the tag dictionary for the MaxEnt sense tagger. 3.2.2. Using categorical attri</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, Adwait (1996) A Maximum Entropy Model for Part-of-Speech Tagging. In Proceedings of the First Empirical Methods in Natural Language Processing Conference, pp. 133-141, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhong Yang</author>
</authors>
<title>Guoqing Zhang and Yongkui Zhang</title>
<date>2000</date>
<booktitle>In Proceedings of the second Chinese language processing workshop, ACL 2000 Conference,</booktitle>
<pages>60--65</pages>
<marker>Yang, 2000</marker>
<rawString>Yang, Erhong, Guoqing Zhang and Yongkui Zhang (2000) The Research of Word Sense Disambiguation Method Based on Co-occurrence Frequency of HowNet. In Proceedings of the second Chinese language processing workshop, ACL 2000 Conference, October 2000, pp.60-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Yung Wang</author>
</authors>
<title>Sense Pruning by HowNet – a knowledge-based Word Sense Disambiguation. MPhil Thesis.</title>
<date>2002</date>
<institution>Hong Kong University of Science and Technology.</institution>
<contexts>
<context position="20902" citStr="Wang (2002)" startWordPosition="3348" endWordPosition="3349">,sequence |,cardinal |’ according to the mapping table. 4. Previous Work To our knowledge, there currently exist three previous studies of word sense disambiguation using HowNet. Yang et al (2000) pioneered this work by using sememe co-occurrence information in sentences from a large corpus to achieve an accuracy of 71%. Yang and Li (2002), collecting sememe co-occurrence information from a large corpus, transferred the information to restricted rules for sense disambiguation. They reported a precision rate of 92% and 82% for lexical disambiguation and structural disambiguation, respectively. Wang (2002) pioneered the work of sense pruning using the hand-coded knowledge base of HowNet. Unlike sense disambiguation, sense pruning seeks to narrow down the possible senses of a word in a text. Using databases of features such as information structure and object-attribute relations which were compiled from HowNet, Wang reported a recall rate of 97.13% and a per sentence complexity reduction rate of 47.63%. The current study and Wang (2002) used the sense tagged HowNet corpus with different approaches. There is one similarity between our work and Wang (2002), though. Wang applied a sense pruning met</context>
</contexts>
<marker>Wang, 2002</marker>
<rawString>Wang, Chi-Yung (2002) Sense Pruning by HowNet – a knowledge-based Word Sense Disambiguation. MPhil Thesis. Hong Kong University of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
<author>Mary Ellen Okurowski</author>
<author>John Kovarik</author>
</authors>
<title>Fu-Dong Chiou, Shizhe Huang, Tony Kroch and Mitch Mrcus</title>
<date>2000</date>
<booktitle>In Proceedings of the second International Conference on Language Resources and Evaluation (LREC-2000),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="1417" citStr="Xia et al., 2000" startWordPosition="225" endWordPosition="228">g or sense, which are listed in the dictionary. The task of Word Sense Disambiguation (WSD) is to make the choice between the senses for a particular usage of the word in context. There are, however, several difficulties to WSD (Yang et al, 2000): (i) The evaluation of word sense disambiguation system is not yet standardized. (ii) The potential for WSD varies by task. (iii) Sense-tagged corpora are crucial resources for WSD but they are difficult to obtain. Efforts in building large Chinese corpora started in the 90s, for example, the Sinica corpus (CKIP, 1995) and the Chinese Penn Tree Bank (Xia et al., 2000). However, these two corpora concentrate on the tagging of parts-of-speech and syntactic structures, while little work has been done on semantic annotation. Of the few efforts that were carried out, Lua1 annotated 340,000 words with semantic classes defined in a thesaurus (Mei, 1983). This resource, however, was not publicly accessible. With the release of HowNet (Dong, 1999; Dong, 2000) in YANG Yongsheng Department of Computer Science, HKUST, Clear Water Bay, Hong Kong. ysyang@cs.ust.hk 1999, Gan and Tham (1999) manually annotated a Chinese corpus of 30,000 words with the senses from HowNet. </context>
</contexts>
<marker>Xia, Palmer, Xue, Okurowski, Kovarik, 2000</marker>
<rawString>Xia, Fei, Martha Palmer, Nianwen Xue, Mary Ellen Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe Huang, Tony Kroch and Mitch Mrcus (2000) Developing Guidelines and Ensuring Consistency for Chinese Text Annotation. In Proceedings of the second International Conference on Language Resources and Evaluation (LREC-2000), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
</authors>
<title>Tangqiu Li (2002) A Study of Semantic Disambiguation Based on HowNet,</title>
<date>2002</date>
<journal>International Journal of Computational Linguistics and Chinese Language Processing,</journal>
<volume>7</volume>
<pages>47--78</pages>
<location>no.1 ,</location>
<marker>Yang, 2002</marker>
<rawString>Yang, Xiaofeng, Tangqiu Li (2002) A Study of Semantic Disambiguation Based on HowNet, International Journal of Computational Linguistics and Chinese Language Processing, vol. 7, no.1 , 2002, pp.47-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
</authors>
<date>2000</date>
<note>HowNet: http://www.keenage.com.</note>
<contexts>
<context position="1807" citStr="Dong, 2000" startWordPosition="286" endWordPosition="287">a are crucial resources for WSD but they are difficult to obtain. Efforts in building large Chinese corpora started in the 90s, for example, the Sinica corpus (CKIP, 1995) and the Chinese Penn Tree Bank (Xia et al., 2000). However, these two corpora concentrate on the tagging of parts-of-speech and syntactic structures, while little work has been done on semantic annotation. Of the few efforts that were carried out, Lua1 annotated 340,000 words with semantic classes defined in a thesaurus (Mei, 1983). This resource, however, was not publicly accessible. With the release of HowNet (Dong, 1999; Dong, 2000) in YANG Yongsheng Department of Computer Science, HKUST, Clear Water Bay, Hong Kong. ysyang@cs.ust.hk 1999, Gan and Tham (1999) manually annotated a Chinese corpus of 30,000 words with the senses from HowNet. The corpus is a subset of the Sinica balanced corpus, and consists of 103 narratives on news stories, in which the words have already been segmented and tagged with parts-of-speech. Gan and Tham (1999) added sense tagging and subsequently Gan and Wong (2000) annotated the corpus with semantic dependency relations as defined in HowNet. The corpus was released to the public in January 2002</context>
</contexts>
<marker>Dong, 2000</marker>
<rawString>Dong, Zhendong (2000) HowNet: http://www.keenage.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
<author>Wai-Mun Tham</author>
</authors>
<title>General Knowledge Annotation Based on</title>
<date>1999</date>
<booktitle>HowNet ( ). Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>4</volume>
<pages>39--86</pages>
<contexts>
<context position="1935" citStr="Gan and Tham (1999)" startWordPosition="303" endWordPosition="306">he 90s, for example, the Sinica corpus (CKIP, 1995) and the Chinese Penn Tree Bank (Xia et al., 2000). However, these two corpora concentrate on the tagging of parts-of-speech and syntactic structures, while little work has been done on semantic annotation. Of the few efforts that were carried out, Lua1 annotated 340,000 words with semantic classes defined in a thesaurus (Mei, 1983). This resource, however, was not publicly accessible. With the release of HowNet (Dong, 1999; Dong, 2000) in YANG Yongsheng Department of Computer Science, HKUST, Clear Water Bay, Hong Kong. ysyang@cs.ust.hk 1999, Gan and Tham (1999) manually annotated a Chinese corpus of 30,000 words with the senses from HowNet. The corpus is a subset of the Sinica balanced corpus, and consists of 103 narratives on news stories, in which the words have already been segmented and tagged with parts-of-speech. Gan and Tham (1999) added sense tagging and subsequently Gan and Wong (2000) annotated the corpus with semantic dependency relations as defined in HowNet. The corpus was released to the public in January 2002 2 , providing essential resources for Chinese word sense disambiguation. This paper is organized as follows: Section 2 gives an</context>
</contexts>
<marker>Gan, Tham, 1999</marker>
<rawString>Gan, Kok-Wee and Wai-Mun Tham (1999) General Knowledge Annotation Based on HowNet ( ). Computational Linguistics and Chinese Language Processing, vol. 4, 1999, pp. 39-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
<author>Ping-Wai Wong</author>
</authors>
<title>Annotating Information Structures in Chinese Text using HowNet.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd Chinese Language Processing Workshop, Association for Computational Linguistics 2000 Conference,</booktitle>
<pages>85--92</pages>
<contexts>
<context position="2275" citStr="Gan and Wong (2000)" startWordPosition="359" endWordPosition="362">ic classes defined in a thesaurus (Mei, 1983). This resource, however, was not publicly accessible. With the release of HowNet (Dong, 1999; Dong, 2000) in YANG Yongsheng Department of Computer Science, HKUST, Clear Water Bay, Hong Kong. ysyang@cs.ust.hk 1999, Gan and Tham (1999) manually annotated a Chinese corpus of 30,000 words with the senses from HowNet. The corpus is a subset of the Sinica balanced corpus, and consists of 103 narratives on news stories, in which the words have already been segmented and tagged with parts-of-speech. Gan and Tham (1999) added sense tagging and subsequently Gan and Wong (2000) annotated the corpus with semantic dependency relations as defined in HowNet. The corpus was released to the public in January 2002 2 , providing essential resources for Chinese word sense disambiguation. This paper is organized as follows: Section 2 gives an introduction of HowNet. Section 3 describes the WSD task and the experiment results. Section 4 describes the previous work, followed by a conclusion in Section 5. 2. An Introduction to HowNet HowNet is a bilingual general knowledge base that encodes inter-concept semantic relations and the inter-attribute semantic relations. In contrast </context>
<context position="5364" citStr="Gan and Wong (2000)" startWordPosition="828" endWordPosition="831">g features. Two types of pointers are used in this concept. The pointer “#” means “related” and thus ‘#occupation |’ shows that there is a relation between the word “journalist” and occupations. The pointer “*” means ‘agent’, and thus, ‘*gather |’ and ‘*compile |’ tell us that “journalist” is the agent of ‘gather |’ and ‘compile |’. The sememe ‘#news |’ that follows tells us that the function of “journalist” is to compile and gather news. 3 The function of pointers is to describe various inter-concept and inter-attribute relations. Please refer to HowNet’s homepage (http://www.keenage.com) or Gan and Wong (2000) for details. 2.1. Classification of content words Concepts of content words in HowNet are classified into six categories: Entity, Event, Attribute, Quantity, Attribute Value and Quantity value. The sememes in each category are organized hierarchically in an ontology tree. The six categories can be grouped into four main types: (i) Entity, (ii) Event, (iii) Attribute and Quantity, (iv) Attribute Value and Quantity Value. Most nominal concepts, such as “journalist”, belong to the Entity category and some of them belong to the Attribute category. Verbal concepts always belong to the Event catego</context>
</contexts>
<marker>Gan, Wong, 2000</marker>
<rawString>Gan, Kok-Wee and Ping-Wai Wong (2000) Annotating Information Structures in Chinese Text using HowNet. In Proceedings of the 2nd Chinese Language Processing Workshop, Association for Computational Linguistics 2000 Conference, October 2000, Hong Kong, pp. 85-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiaju Mei</author>
</authors>
<title>Yiming Lau, Yunqi Gao, Yongxiang Ying</title>
<date>1983</date>
<journal>A Dictionary of Synonyms ( ✰), Shanghai Cishu Chubanshe.</journal>
<contexts>
<context position="1701" citStr="Mei, 1983" startWordPosition="270" endWordPosition="271">tion system is not yet standardized. (ii) The potential for WSD varies by task. (iii) Sense-tagged corpora are crucial resources for WSD but they are difficult to obtain. Efforts in building large Chinese corpora started in the 90s, for example, the Sinica corpus (CKIP, 1995) and the Chinese Penn Tree Bank (Xia et al., 2000). However, these two corpora concentrate on the tagging of parts-of-speech and syntactic structures, while little work has been done on semantic annotation. Of the few efforts that were carried out, Lua1 annotated 340,000 words with semantic classes defined in a thesaurus (Mei, 1983). This resource, however, was not publicly accessible. With the release of HowNet (Dong, 1999; Dong, 2000) in YANG Yongsheng Department of Computer Science, HKUST, Clear Water Bay, Hong Kong. ysyang@cs.ust.hk 1999, Gan and Tham (1999) manually annotated a Chinese corpus of 30,000 words with the senses from HowNet. The corpus is a subset of the Sinica balanced corpus, and consists of 103 narratives on news stories, in which the words have already been segmented and tagged with parts-of-speech. Gan and Tham (1999) added sense tagging and subsequently Gan and Wong (2000) annotated the corpus with</context>
</contexts>
<marker>Mei, 1983</marker>
<rawString>Mei, Jiaju, Yiming Lau, Yunqi Gao, Yongxiang Ying (1983) A Dictionary of Synonyms ( ✰), Shanghai Cishu Chubanshe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: An Online Lexical Database.</title>
<date>1990</date>
<journal>In Special Issue of International Journal of Lexicography,</journal>
<volume>3</volume>
<contexts>
<context position="2900" citStr="Miller, 1990" startWordPosition="456" endWordPosition="457">he corpus with semantic dependency relations as defined in HowNet. The corpus was released to the public in January 2002 2 , providing essential resources for Chinese word sense disambiguation. This paper is organized as follows: Section 2 gives an introduction of HowNet. Section 3 describes the WSD task and the experiment results. Section 4 describes the previous work, followed by a conclusion in Section 5. 2. An Introduction to HowNet HowNet is a bilingual general knowledge base that encodes inter-concept semantic relations and the inter-attribute semantic relations. In contrast to WordNet (Miller, 1990), HowNet adopts a constructive approach of meaning representation (Miller, 1993). Basic meaning units called sememes, which cannot be decomposed further, combine to construct concepts in HowNet. So far, there are 65,000 Chinese concepts and 75,000 English equivalents defined with a set of 1503 sememes. NO.=the record number of the lexical entries W_X=concept of the language X E_X=example of W_X G_X=Part-of-speech of the W_X DEF=Definition, which is constructed by sememes and pointers Figure 1: A sample lexical entry in HowNet. Figure 1 gives an idea of how word concepts are organized in HowNet</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, George A. (1990) WordNet: An Online Lexical Database. In Special Issue of International Journal of Lexicography, Vol 3, No. 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Nouns in WordNet: a lexical inheritance system. Five Papers on WordNet,</title>
<date>1993</date>
<location>CSL</location>
<contexts>
<context position="2980" citStr="Miller, 1993" startWordPosition="466" endWordPosition="467">s released to the public in January 2002 2 , providing essential resources for Chinese word sense disambiguation. This paper is organized as follows: Section 2 gives an introduction of HowNet. Section 3 describes the WSD task and the experiment results. Section 4 describes the previous work, followed by a conclusion in Section 5. 2. An Introduction to HowNet HowNet is a bilingual general knowledge base that encodes inter-concept semantic relations and the inter-attribute semantic relations. In contrast to WordNet (Miller, 1990), HowNet adopts a constructive approach of meaning representation (Miller, 1993). Basic meaning units called sememes, which cannot be decomposed further, combine to construct concepts in HowNet. So far, there are 65,000 Chinese concepts and 75,000 English equivalents defined with a set of 1503 sememes. NO.=the record number of the lexical entries W_X=concept of the language X E_X=example of W_X G_X=Part-of-speech of the W_X DEF=Definition, which is constructed by sememes and pointers Figure 1: A sample lexical entry in HowNet. Figure 1 gives an idea of how word concepts are organized in HowNet. “X” represents some 1 http://www.cslp.com.nus.edu/sg/cslp/ 2 http://godel.iis.</context>
</contexts>
<marker>Miller, 1993</marker>
<rawString>Miller, George A. (1993) Nouns in WordNet: a lexical inheritance system. Five Papers on WordNet, CSL</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>