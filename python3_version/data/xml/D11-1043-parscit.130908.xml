<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001311">
<title confidence="0.966463">
Ranking Human and Machine Summarization Systems
</title>
<author confidence="0.997155">
Peter Rankel
</author>
<affiliation confidence="0.7417905">
University of Maryland
College Park, Maryland
</affiliation>
<email confidence="0.982563">
rankel@math.umd.edu
</email>
<author confidence="0.998214">
Eric V. Slud
</author>
<affiliation confidence="0.999699">
University of Maryland
</affiliation>
<address confidence="0.524161">
College Park, Maryland
</address>
<email confidence="0.997963">
evs@math.umd.edu
</email>
<sectionHeader confidence="0.995673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.979192714285714">
The Text Analysis Conference (TAC) ranks
summarization systems by their average score
over a collection of document sets. We in-
vestigate the statistical appropriateness of this
score and propose an alternative that better
distinguishes between human and machine
evaluation systems.
</bodyText>
<sectionHeader confidence="0.998793" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999825043478261">
For the past several years, the National Institute of
Standards and Technology (NIST) has hosted the
Text Analysis Conference (TAC) (previously called
the Document Understanding Conference (DUC))
(Nat, 2010). A major theme of this conference is
multi-document summarization: machine summa-
rization of sets of related documents, sometimes
query-focused and sometimes generic. The sum-
marizers are judged by how well the summaries
match human-generated summaries in either auto-
matic metrics such as ROUGE (Lin and Hovy, 2003)
or manual metrics such as responsiveness or pyra-
mid evaluation (Nenkova et al., 2007). Typically the
systems are ranked by their average score over all
document sets.
Ranking by average score is quite appropriate un-
der certain statistical hypotheses, for example, when
each sample is drawn from a distribution which
differs from the distribution of other samples only
through a location shift (Randles and Wolfe, 1979).
However, a non-parametric (rank-based) analysis of
variance on the summarizers’ scores on each docu-
ment set revealed an impossibly small p-value (less
</bodyText>
<author confidence="0.967459">
John M. Conroy
</author>
<affiliation confidence="0.906954">
IDA/Center for Computing Sciences
Bowie, Maryland
</affiliation>
<email confidence="0.924659">
conroyjohnm@gmail.com
</email>
<author confidence="0.974222">
Dianne P. O’Leary
</author>
<affiliation confidence="0.8383275">
University of Maryland
College Park, Maryland
</affiliation>
<email confidence="0.955139">
oleary@cs.umd.edu
</email>
<figureCaption confidence="0.99158">
Figure 1: Confidence Intervals from a non-parametric
</figureCaption>
<bodyText confidence="0.701593666666667">
Tukey’s honestly significant difference test for 46 TAC
2010 update document sets. The blue confidence interval
(for document set d1032) does not overlap any of the 30
red intervals. Hence, the test concludes that 30 document
sets have mean significantly different from the mean of
d1032.
</bodyText>
<page confidence="0.980765">
467
</page>
<note confidence="0.981674">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 467–473,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.993696666666667">
Figure 2: Overall Responsiveness scores.
Figure 3: Linguistic scores.
Figure 4: Pyramid scores.
Figure 5: ROUGE-2 scores for the TAC 2010 update
summary task, organized by document set (y-axis) and
summarizer (x-axis). The 51 summarizers fall into two
distinct groups: machine systems (first 43 columns) and
humans (last 8 columns). Note that each human only
summarized half of the document sets, thus creating 23
missing values in each of the last 8 columns. Black is
used to indicate missing values in the last 8 columns and
low scores in the first 43 columns.
</figureCaption>
<bodyText confidence="0.999767722222222">
than 10−12 using Matlab’s kruskalwallis 1),
providing evidence that a summary’s score is not
independent of the document set. This effect can
be seen in Figure 1, showing the confidence bands,
as computed by a Tukey honestly significant differ-
ence test for each document set’s difficulty as mea-
sured by the mean rank responsiveness score for
TAC 2010. The test clearly shows that the summa-
rizer performances on different document sets have
different averages.
We further illustrate this in Figures 2 – 5, which
show the scores of various summarizers on vari-
ous document sets using standard human and au-
tomatic evaluation methods (Dang and Owczarzak,
2008) of overall responsiveness, linguistic quality,
pyramid scores, and ROUGE-2 using color to indi-
cate the value of the score. Some rows are clearly
darker, indicating overall lower scores for the sum-
</bodyText>
<footnote confidence="0.692306166666667">
1The Kruskal-Wallis test performs a one-way analysis of
variance of document-set differences after first converting the
summary scores for each sample to their ranks within the pooled
sample. Computed from the converted scores, the Kruskal-
Wallis test statistic is essentially the ratio of the between-group
sum of squares to the combined within-group sum of squares.
</footnote>
<page confidence="0.998852">
468
</page>
<bodyText confidence="0.999974558139535">
maries of these documents, and the variances of the
scores differ row-by-row. These plots show qualita-
tively what the non-parametric analysis of variance
demonstrates statistically. While the data presented
was for the TAC 2010 update document sets, similar
results hold for all the TAC 2008, 2009, and 2010
data. Hence, it may be advantageous to measure
summarizer quality by accounting for heterogeneity
of documents within each test set. A non-parametric
paired test like the Wilcoxon signed-rank is one way
to do this. Another way would be paired t-tests.
In the paper (Conroy and Dang, 2008) the authors
noted that while there is a significant gap in perfor-
mance between machine systems and human sum-
marizers when measured by average manual met-
rics, this gap is not present when measured by the
averages of the best automatic metric (ROUGE). In
particular, in the DUC 2005-2007 data some systems
have ROUGE performance within the 95% confi-
dence intervals of several human summarizers, but
their pyramid, linguistic, and responsiveness scores
do not achieve this level of performance. Thus,
the inexpensive automatic metrics, as currently em-
ployed, do not predict well how machine summaries
compare to human summaries.
In this work we explore the use of document-
paired testing for summarizer comparison. Our main
approach is to consider each pair of two summa-
rizers’ sets of scores (over all documents) as a bal-
anced two-sample dataset, and to assess that pair’s
mean difference in scores through a two-sample T
or Wilcoxon test, paired or unpaired. Our goal has
been to confirm that human summarizer scores are
uniformly different and better on average than ma-
chine summarizer scores, and to rate the quality of
the statistical method (T or W, paired or unpaired)
by the consistency with which the human versus
machine scores show superior human performance.
Our hope is that paired testing, using either the stan-
dard paired two-sample t-test or the distribution-
free Wilcoxon signed-rank test, can provide greater
power in the statistical analysis of automatic metrics
such as ROUGE.
</bodyText>
<sectionHeader confidence="0.782555" genericHeader="introduction">
2 Size and Power of Tests
</sectionHeader>
<bodyText confidence="0.999892333333333">
Statistical tests are generally compared by choosing
rejection thresholds to achieve a certain small prob-
ability of Type I error (usually as α = .05). Given
multiple tests with the same Type I error, one prefers
the test with the smallest probability of Type II error.
Since power is defined to be one minus the Type II
error probability, we prefer the test with the most
power. Recall that a test-statistic S depending on
available data-samples gives rise to a rejection re-
gion by defining rejection of the null hypothesis Ho
as the event IS &gt; c} for a cutoff or rejection thresh-
old c chosen so that
</bodyText>
<equation confidence="0.975768">
P(S &gt; c) &lt; α
</equation>
<bodyText confidence="0.9934596">
for all probability laws compatible with the null hy-
pothesis where the (nominal) significance level α
is chosen in advance by the statistician, usually as
α = .05. However, in many settings, the null hy-
pothesis comprises many possible probability laws,
as here where the null hypothesis is that the under-
lying probability laws for the score-samples of two
separate summarizers are equal, without specifying
exactly what that probability distribution is. In this
case, the significance level is an upper bound for the
attained size of the test, defined as supP∈H0 P(S &gt;
c), the largest rejection probability P(S &gt; c)
achieved by any probability law compatible with the
null hypothesis. The power of the test then depends
on the specific probability law Q from the consid-
ered alternatives in HA. For each such Q, and given
a threshold c, the power for the test at Q is the re-
jection probability Q(S &gt; c). These definitions re-
flect the fact that the null and alternative hypothe-
ses are composite, that is, each consists of multiple
probability laws for the data. One of the advan-
tages of considering a distribution-free two-sample
test statistic such as the Wilcoxon is that the proba-
bility distribution for the statistic S is then the same
for all (continuous, or non-discrete) probability laws
P E Ho, so that one cutoff c serves for all of Ho
with all rejection probabilities equal to α. 2
Two test statistics, say S and ˜S, are generally
compared in terms of their powers at fixed alterna-
tives Q in the alternative hypothesis HA, when their
respective thresholds c, c∗ have been defined so that
the sizes of the respective tests, supP∈H0 P(S &gt;
2The Wilcoxon test is not distribution-free for discrete data.
However, the discrete TAC data can be thought of as rounded
continuous data, rather than as truly discrete data.
</bodyText>
<page confidence="0.998228">
469
</page>
<bodyText confidence="0.999673363636364">
c) and supPEH0 P(S˜ ≥ c∗), are approximately
equal. In this paper, the test statistics under consid-
eration are – in one-sided testing — the (unpaired)
two-sample t test with pooled sample variance (T),
the paired two-sample t test (Tp), and the (paired)
signed-rank Wilcoxon test (W); and for two-sided
testing, S is defined by the absolute value of one
of these statistics. The thresholds c for the tests
can be defined either by theoretical distributions, by
large-sample approximations, or by data-resampling
(bootstrap) techniques, and (only) in the last case
are these thresholds data-dependent, or random. We
explain these notions with respect to the two-sample
data-structure in which the scores from the first sum-
marizer are denoted X1, ... , Xn, where n is the
number of documents with non-missing scores for
both summarizers, and the scores from the second
summarizer are Y1, ... , Yn. Let Zk = Xk − Yk
denote the document-wise differences between the
summarizers’ scores, and Z¯ = n−1 Enk=1 Zk be
their average. Then the paired statistics are defined
as
</bodyText>
<equation confidence="0.9650835">
V
Tp = n(n − 1) ¯Z/(
and
n
W= E sgn(Zk) R+k
k=1
</equation>
<bodyText confidence="0.999977947368421">
where R+k is the rank of |Zk |among
|Z1|, ... , |Zn|. Note that under both null and alter-
native hypotheses, the variates Zk are assumed in-
dependent identically distributed (iid), while under
H0, the random variables Zk are symmetric about 0.
The t-statistic Tp is ‘parametric’ in the sense that
exact theoretical calculations of probabilities P(a &lt;
Tp &lt; b) depend on the assumption of normality of
the differences Zk, and when that holds, the two-
sided cutoff c = c(Tp) is defined as the 1 − α/2
quantile of the tn−1 distribution with n − 1 degrees
of freedom. However, when n is moderately or
very large, the cutoff is well approximated by the
standard-normal 1 − α/2 quantile zα/2, and Tp be-
comes approximately nonparametrically valid with
this cutoff, by the Central Limit Theorem. The
Wilcoxon signed-rank statistic W has theoretical
cutoff c = c(W) which depends only on n, when-
ever the data Zk are continuously distributed; but for
</bodyText>
<equation confidence="0.533719">
V
</equation>
<bodyText confidence="0.987177936170213">
large n, the cutoff is given simply as n3/12 · zα/2.
When there are ties (as might be common in discrete
data), the calculation of cutoffs and p-values for
Wilcoxon becomes slightly more complicated and
is no longer fully nonparametric except in a large-
sample approximate sense.
The situation for the two-sample unpaired t-
statistic T currently used in TAC evaluation is not
so neat. Even when the two samples X = {Xk}nk=1
and Y = {Yk}nk=1 are independent, exact theoret-
ical distribution of cutoffs is known only under the
parametric assumption that the scores are normally
distributed (and in the case of the pooled-sample-
variance statistic, that Var(Xk) = Var(Yk).) How-
ever, an essential element of the summarization data
is the heterogeneity of documents. This means that
while {Xk}nk=1 can be viewed as iid scores when
documents are selected randomly – and not neces-
sarily equiprobably – from the ensemble of all pos-
sible documents, the Yk and Xk samples are de-
pendent. Still, the pairs {(Xk, Yk)}nk=1, and there-
fore the differences {Zk}nk=1, are iid which is what
makes paired testing valid. However, there is no the-
oretical distribution for T from which to calculate
valid quantiles c for cutoffs, and therefore the use of
the unpaired t-statistic cannot be recommended for
TAC evaluation.
What can be done in a particular dataset, like the
TAC summarization score datsets we consider, to
ascertain the approximate validity of theoretically
derived large-sample cutoffs for test statistics? In
the age of plentiful and fast computers, quite a lot,
through the powerful computational machinery of
the bootstrap (Efron and Tibshirani, 1993).
The idea of bootstrap hypothesis testing (Efron
and Tibshirani, 1993), (Bickel and Ren, 2001) is to
randomly sample with replacement (the rows with
non-missing data in) the dataset {(Xk,Yk)}nk=1 in
such a way as to generate representative data that
plausibly would have been seen if two-sample score
data had been generated from two equally effec-
tive summarizers with score distributional charac-
teristics like the pooled scores from the two ob-
served summarizers. We have done this in two dis-
tinct ways, each creating 2000 datasets with n paired
scores:
MC Monte Carlo Method. For each of many it-
</bodyText>
<equation confidence="0.810656">
n
E (Zk − ¯Z)2)1/2
k=1
</equation>
<page confidence="0.983458">
470
</page>
<bodyText confidence="0.994646">
erations (in our case 2000), define a new
dataset {(X0k, Y0k)}nk=1 by independently swap-
ping Xk and Yk with probability 1/2. Hence,
(X0k, Y0k) = (Xk, Yk) with probability 1/2 and
(Yk, Xk) with probability 1/2.
HB Hybrid MC/Bootstrap. For each of 2000
iterations, create a re-sampled dataset
{(X00 k ,Y 00
k )}nk=1 in the following way. First,
sample n pairs (Xk, Yk) with replacement
from the original dataset. Then, as above,
randomly swap the components of each pair,
each with 1/2 probability.
Both of these two methods can be seen to gener-
ate two-sample data satisfying H0, with each score-
sample’s distribution obtained as a mixture of the
distributions actually generating the X and Y sam-
ples. The empirical qth quantiles for a statistic
S = S(X, Y) such as |W  |or |Tp |are estimated
from the resampled data as Fˆ −1
S (q), where ˆFS(t) is
simply the fraction of times (out of 2000) that the
statistic S applied to the constructed dataset had a
value less than or equal to t. The upshot is that the
1 − α empirical quantile for S based on either of
these simulation methods serves as a data-dependent
cutoff c attaining approximate size α for all H0-
generated data. The MC and HB methods will be
employed in Section 4 to check the theoretical p-
values.
</bodyText>
<sectionHeader confidence="0.992799" genericHeader="method">
3 Relative Efficiency of W versus Tp
</sectionHeader>
<bodyText confidence="0.99999485">
Statistical theory does have something to say about
the comparative powers of paired W versus Tp
statistics. These statistics have been studied (Ran-
dles and Wolfe, 1979), in terms of their asymp-
totic relative efficiency for location-shift alternatives
based on symmetric densities (f(z−V) is a location-
shift of f(z)). For many pairs of parametric and
rank-based statistics S, ˜S, including W and Tp, the
following assertion has been proved for testing H0
at significance level α.
First assume the Zk are distributed according to
some density f(z − V), where f(z) is a symmet-
ric function (f(−z) = f(z)). Next assume V = 0
under H0. When n gets large the powers at any al-
ternatives with very small V = -y/Vn, -y =� 0, can
be made asymptotically equal by using samples of
size n with statistic S and of size p · n with statistic
˜S. Here p = ARE(S, ˜S) is a constant not depend-
ing on n or -y but definitely depending on f, called
asymptotic relative efficiency of S with respect to ˜S.
(The smaller p &lt; 1 is, the more statistic S˜ is pre-
ferred among the two.)
Using this definition, it is known (Randles and
Wolfe 1979, Sec. 5.4 leading up to Table 5.4.7 on
p. 167) that the Wilcoxon signed-rank statistic W
provides greater robustness and often much greater
efficiency than the paired T, with ARE which is 0.95
with f a standard normal density, and which is never
less than 0.864 for any symmmetric density f. How-
ever, in our context, continuous scores such as pyra-
mid exhibit document-specific score differences be-
tween summarizers which often have approximately
normal-looking histograms, and although the alter-
natives perhaps cannot be viewed as pure location
shifts, it is unsurprising in view of the ARE theory
cited above that the W and T paired tests have very
similar performance. Nevertheless, as we found by
statistical analysis of the TAC data, both are far su-
perior to the unpaired T-statistic, with either theoret-
ical or empirical bootstrapped p-values.
</bodyText>
<sectionHeader confidence="0.539003" genericHeader="method">
4 Testing Setup and Results
</sectionHeader>
<bodyText confidence="0.958446090909091">
To evaluate our ideas, we used the TAC data from
2008-2010 and focused on three manual metrics
(overall responsiveness, pyramid score, and lin-
guistic quality score) and two automatic metrics
(ROUGE-2 and ROUGE-SU4). We make the as-
sumption, backed by both the scores given and com-
ments made by NIST summary assessors 3, that au-
tomatic summarization systems do not perform at
the human level of performance. As such, if a statis-
tic based on an automatic metric, such as ROUGE-
2, were to show fewer systems performing at human
level of performance than the statistic of averaging
scores, such a statistic would be preferable because
3Assessors have commented privately at the Text Analysis
Conference 2008, that while the origin of the summary is hid-
den from them, “we know which ones are machine generated.”
Thus, automatic summarization fails the Turing test of machine
intelligence (Turing, 1950). This belief is also supported by
(Conroy and Dang, 2008) and (Dang and Owczarzak, 2008). Fi-
nally, our own results show no matter how you compare human
and machine scores all machines systems score significantly
worse than humans.
</bodyText>
<page confidence="0.995709">
471
</page>
<table confidence="0.999297714285714">
2008: 2145 = ( 26) pairs 2009: 1830 = (21) pairs 2010: 1275 = ( 21) pairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 1234 1416 1410 1000 1182 1173 841 939 934
Overall 1202 1353 1342 982 1149 1146 845 894 889
Pyramid 1263 1417 1418 1075 1238 1216 875 933 926
ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939
ROUGE-SU4 1333 1493 1507 1059 1241 1254 894 983 976
</table>
<tableCaption confidence="0.978934">
Table 1: Number of significant differences found when testing for the difference of all pairs of summarization systems
(including humans).
</tableCaption>
<table confidence="0.999704142857143">
2008: 464 = 58 x 8 pairs 2009: 424 = 53 x 8 pairs 2010: 344 = 43 x 8 pairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 464 464 464 424 424 424 344 344 344
Overall 464 464 464 424 424 424 344 344 344
Pyramid 464 464 464 424 424 424 344 344 344
ROUGE-2 375 409 402 323 350 341 275 309 305
ROUGE-SU4 391 418 414 354 378 373 324 331 328
</table>
<tableCaption confidence="0.994203">
Table 2: Number of significant differences resulting from 8 x (N − 8) tests for human-machine system means or
signed-rank comparisons.
</tableCaption>
<bodyText confidence="0.999913636363637">
of its greater power in the machine vs. human sum-
marization domain.
For each of these metrics, we first created a score
matrix whose (i, j)-entry represents the score for
summarizer j on document set i (these matrices gen-
erated the colorplots in Figures 2 – 5). We then per-
formed a Wilcoxon signed-rank test on certain pairs
of columns of this matrix (any pair consisting of one
machine system and one human summarizer). As a
baseline, we did the same testing with a paired and
an unpaired t-test. Each of these tests resulted in a
p-value, and we counted how many were less than
.05 and called these the significant differences.
The results of these tests (shown in Table 2),
were somewhat surprising. Although we expected
the nonparametric signed-rank test to perform better
than an unpaired t-test, we were surprised to see that
a paired t-test performed even better. All three tests
always reject the null hypotheses when human met-
rics are used. This is what we’d like to happen with
automatic metrics as well. As seen from the table,
the paired t-test and Wilcoxon signed-rank test offer
a good improvement over the unpaired t-test.
The results in Table 1 are less clear, but still posi-
tive. In this case, we are comparing pairs of machine
summarization systems. In contrast to the human vs.
machine case, we do not know the truth here. How-
ever, since the number of significant differences in-
creases with paired testing here as well, we believe
this also reflects the greater discriminatory power of
paired testing.
We now apply the Monte Carlo and Hybrid Monte
Carlo to check the theoretical p-values reported in
Tables 1 and 2. The empirical quantiles found
by these methods generally confirm the theoreti-
cal p-value test results reported there, especially
in Table 2. In the overall tallies of all compar-
isons (Table 1), it seems that the bootstrap results
(comparing only W and the un-paired T) make
W look still stronger for linguistic and overall re-
sponsiveness versus the T; but for the pyramid
and ROUGE scores, the bootstrap p-values bring T
slightly closer to W although it still remains clearly
inferior, achieving roughly 10% fewer rejections.
</bodyText>
<sectionHeader confidence="0.998754" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998266">
In this paper we observed that summarization sys-
tems’ performance varied significantly across doc-
ument sets on the Text Analysis Conference (TAC)
data. This variance in performance suggested that
paired testing may be more appropriate than the
t-test currently employed at TAC to compare the
</bodyText>
<page confidence="0.995491">
472
</page>
<bodyText confidence="0.999811113207547">
performance of summarization systems. We pro-
posed a non-parametric test, the Wilcoxon signed-
rank test, as a robust more powerful alternative to
the t-test. We estimated the statistical power of the
t-test and the Wilcoxon signed-rank test by calcu-
lating the number of machine systems whose per-
formance was significantly different than that of hu-
man summarizers. As human assessors score ma-
chine systems as not achieving human performance
in either content or responsiveness, automatic met-
rics such as ROUGE should ideally indicate this dis-
tinction. We found that the paired Wilcoxon test
significantly increases the number of machine sys-
tems that score significantly different than humans
when the pairwise test is performed on ROUGE-2
and ROUGE-SU4 scores. Thus, we demonstrated
that the Wilcoxon paired test shows more statistical
power than the t-test for comparing summarization
systems.
Consequently, the use of paired testing should not
only be used in formal evaluations such as TAC, but
also should be employed by summarization devel-
opers to more accurately assess whether changes to
an automatic system give rise to improved perfor-
mance.
Further study needs to analyze more summariza-
tion metrics such as those proposed at the recent
NIST evaluation of automatic metrics, Automati-
cally Evaluating Summaries of Peers (AESOP) (Nat,
2010). As metrics become more sophisticated and
aim to more accurately predict human judgements
such as overall responsiveness and linguistic qual-
ity, paired testing seems likely to be a more power-
ful statistical procedure than the unpaired t-test for
head-to-head summarizer comparisons.
Throughout our research in this paper, we treated
each separate kind of scores on a document set as
data for one summarizer to be compared with the
same kind of scores for other summarizers. How-
ever, it might be more fruitful to treat all the scores
as multivariate data and compare the summarizers
that way. Multivariate statistical techniques such as
Principal Component Analysis may play a construc-
tive role in suggesting highly discriminating new
composite scores, perhaps leading to statistics with
even more power to measure a summary’s quality.
ROUGE was inspired by the success of the
BLEU (BiLingual Evaluation Understudy), an n-
gram based evaluation for machine translation (Pa-
pineni et al., 2002). It is likely that paired testing
may also be appropriate for BLEU as well and will
give additional discriminating power between ma-
chine translations and human translations.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778186046512">
Peter J. Bickel and Jian-Jian Ren. 2001. The Bootstrap
in Hypothesis Testing. In State of the Art in Statistics
and Probability Theory, Festschrift for Willem R. van
Zwet, volume 36 of Lecture Notes– Monograph Series,
pages 91–112. Institute of Mathematical Statistics.
John M. Conroy and Hoa Trang Dang. 2008. Mind the
Gap: Dangers of Divorcing Evaluations of Summary
Content from Linguistic Quality. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ’08, pages 145–152,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hoa T. Dang and Karolina Owczarzak. 2008. Overview
of the tac 2008 update summarization task. In Pro-
ceedings of the 1st Text Analysis Conference (TAC),
Gaithersburg, Maryland, USA.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman &amp; Hall, New York.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Eval-
uation of Summaries Using N-gram Co-Occurrences
Statistics. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Edmonton, Alberta.
National Institute of Standards and Technology. 2010.
Text Analysis Conference, http://www.nist.gov/tac.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The Pyramid Method: Incorporating
Human Content Selection Variation in Summarization
Evaluation. ACM Transactions on Speech and Lan-
guage Processing, 4(2).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
R.H. Randles and D.A. Wolfe. 1979. Introduction to
the Theory of Nonparametric Statistics. Wiley series
in probability and mathematical statistics. Probability
and mathematical statistics. Wiley.
Alan Turing. 1950. Computing Machinery and Intelli-
gence. Mind, 59(236):433–460.
</reference>
<page confidence="0.999361">
473
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985177">
<title confidence="0.999939">Ranking Human and Machine Summarization Systems</title>
<author confidence="0.999409">Peter Rankel</author>
<affiliation confidence="0.999989">University of Maryland</affiliation>
<address confidence="0.996735">College Park, Maryland</address>
<email confidence="0.999872">rankel@math.umd.edu</email>
<author confidence="0.99994">Eric V Slud</author>
<affiliation confidence="0.999984">University of Maryland</affiliation>
<address confidence="0.996714">College Park, Maryland</address>
<email confidence="0.999899">evs@math.umd.edu</email>
<abstract confidence="0.999073875">The Text Analysis Conference (TAC) ranks summarization systems by their average score over a collection of document sets. We investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter J Bickel</author>
<author>Jian-Jian Ren</author>
</authors>
<title>The Bootstrap in Hypothesis Testing.</title>
<date>2001</date>
<booktitle>In State of the Art in Statistics and Probability Theory, Festschrift for</booktitle>
<volume>36</volume>
<pages>91--112</pages>
<institution>Institute of Mathematical Statistics.</institution>
<contexts>
<context position="12442" citStr="Bickel and Ren, 2001" startWordPosition="2019" endWordPosition="2022">theoretical distribution for T from which to calculate valid quantiles c for cutoffs, and therefore the use of the unpaired t-statistic cannot be recommended for TAC evaluation. What can be done in a particular dataset, like the TAC summarization score datsets we consider, to ascertain the approximate validity of theoretically derived large-sample cutoffs for test statistics? In the age of plentiful and fast computers, quite a lot, through the powerful computational machinery of the bootstrap (Efron and Tibshirani, 1993). The idea of bootstrap hypothesis testing (Efron and Tibshirani, 1993), (Bickel and Ren, 2001) is to randomly sample with replacement (the rows with non-missing data in) the dataset {(Xk,Yk)}nk=1 in such a way as to generate representative data that plausibly would have been seen if two-sample score data had been generated from two equally effective summarizers with score distributional characteristics like the pooled scores from the two observed summarizers. We have done this in two distinct ways, each creating 2000 datasets with n paired scores: MC Monte Carlo Method. For each of many itn E (Zk − ¯Z)2)1/2 k=1 470 erations (in our case 2000), define a new dataset {(X0k, Y0k)}nk=1 by i</context>
</contexts>
<marker>Bickel, Ren, 2001</marker>
<rawString>Peter J. Bickel and Jian-Jian Ren. 2001. The Bootstrap in Hypothesis Testing. In State of the Art in Statistics and Probability Theory, Festschrift for Willem R. van Zwet, volume 36 of Lecture Notes– Monograph Series, pages 91–112. Institute of Mathematical Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Mind the Gap: Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>145--152</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4694" citStr="Conroy and Dang, 2008" startWordPosition="709" endWordPosition="712">group sum of squares. 468 maries of these documents, and the variances of the scores differ row-by-row. These plots show qualitatively what the non-parametric analysis of variance demonstrates statistically. While the data presented was for the TAC 2010 update document sets, similar results hold for all the TAC 2008, 2009, and 2010 data. Hence, it may be advantageous to measure summarizer quality by accounting for heterogeneity of documents within each test set. A non-parametric paired test like the Wilcoxon signed-rank is one way to do this. Another way would be paired t-tests. In the paper (Conroy and Dang, 2008) the authors noted that while there is a significant gap in performance between machine systems and human summarizers when measured by average manual metrics, this gap is not present when measured by the averages of the best automatic metric (ROUGE). In particular, in the DUC 2005-2007 data some systems have ROUGE performance within the 95% confidence intervals of several human summarizers, but their pyramid, linguistic, and responsiveness scores do not achieve this level of performance. Thus, the inexpensive automatic metrics, as currently employed, do not predict well how machine summaries c</context>
<context position="17212" citStr="Conroy and Dang, 2008" startWordPosition="2847" endWordPosition="2850">ic summarization systems do not perform at the human level of performance. As such, if a statistic based on an automatic metric, such as ROUGE2, were to show fewer systems performing at human level of performance than the statistic of averaging scores, such a statistic would be preferable because 3Assessors have commented privately at the Text Analysis Conference 2008, that while the origin of the summary is hidden from them, “we know which ones are machine generated.” Thus, automatic summarization fails the Turing test of machine intelligence (Turing, 1950). This belief is also supported by (Conroy and Dang, 2008) and (Dang and Owczarzak, 2008). Finally, our own results show no matter how you compare human and machine scores all machines systems score significantly worse than humans. 471 2008: 2145 = ( 26) pairs 2009: 1830 = (21) pairs 2010: 1275 = ( 21) pairs Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Linguistic 1234 1416 1410 1000 1182 1173 841 939 934 Overall 1202 1353 1342 982 1149 1146 845 894 889 Pyramid 1263 1417 1418 1075 1238 1216 875 933 926 ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939 ROUGE-SU4 1333 1493 1507 1059 1241 1254 894 983 976 Table 1: Number of si</context>
</contexts>
<marker>Conroy, Dang, 2008</marker>
<rawString>John M. Conroy and Hoa Trang Dang. 2008. Mind the Gap: Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 145–152, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa T Dang</author>
<author>Karolina Owczarzak</author>
</authors>
<title>Overview of the tac</title>
<date>2008</date>
<booktitle>In Proceedings of the 1st Text Analysis Conference (TAC),</booktitle>
<location>Gaithersburg, Maryland, USA.</location>
<contexts>
<context position="3530" citStr="Dang and Owczarzak, 2008" startWordPosition="527" endWordPosition="530">kalwallis 1), providing evidence that a summary’s score is not independent of the document set. This effect can be seen in Figure 1, showing the confidence bands, as computed by a Tukey honestly significant difference test for each document set’s difficulty as measured by the mean rank responsiveness score for TAC 2010. The test clearly shows that the summarizer performances on different document sets have different averages. We further illustrate this in Figures 2 – 5, which show the scores of various summarizers on various document sets using standard human and automatic evaluation methods (Dang and Owczarzak, 2008) of overall responsiveness, linguistic quality, pyramid scores, and ROUGE-2 using color to indicate the value of the score. Some rows are clearly darker, indicating overall lower scores for the sum1The Kruskal-Wallis test performs a one-way analysis of variance of document-set differences after first converting the summary scores for each sample to their ranks within the pooled sample. Computed from the converted scores, the KruskalWallis test statistic is essentially the ratio of the between-group sum of squares to the combined within-group sum of squares. 468 maries of these documents, and t</context>
<context position="17243" citStr="Dang and Owczarzak, 2008" startWordPosition="2852" endWordPosition="2855">not perform at the human level of performance. As such, if a statistic based on an automatic metric, such as ROUGE2, were to show fewer systems performing at human level of performance than the statistic of averaging scores, such a statistic would be preferable because 3Assessors have commented privately at the Text Analysis Conference 2008, that while the origin of the summary is hidden from them, “we know which ones are machine generated.” Thus, automatic summarization fails the Turing test of machine intelligence (Turing, 1950). This belief is also supported by (Conroy and Dang, 2008) and (Dang and Owczarzak, 2008). Finally, our own results show no matter how you compare human and machine scores all machines systems score significantly worse than humans. 471 2008: 2145 = ( 26) pairs 2009: 1830 = (21) pairs 2010: 1275 = ( 21) pairs Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Linguistic 1234 1416 1410 1000 1182 1173 841 939 934 Overall 1202 1353 1342 982 1149 1146 845 894 889 Pyramid 1263 1417 1418 1075 1238 1216 875 933 926 ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939 ROUGE-SU4 1333 1493 1507 1059 1241 1254 894 983 976 Table 1: Number of significant differences found whe</context>
</contexts>
<marker>Dang, Owczarzak, 2008</marker>
<rawString>Hoa T. Dang and Karolina Owczarzak. 2008. Overview of the tac 2008 update summarization task. In Proceedings of the 1st Text Analysis Conference (TAC), Gaithersburg, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>New York.</location>
<contexts>
<context position="12347" citStr="Efron and Tibshirani, 1993" startWordPosition="2005" endWordPosition="2008">ore the differences {Zk}nk=1, are iid which is what makes paired testing valid. However, there is no theoretical distribution for T from which to calculate valid quantiles c for cutoffs, and therefore the use of the unpaired t-statistic cannot be recommended for TAC evaluation. What can be done in a particular dataset, like the TAC summarization score datsets we consider, to ascertain the approximate validity of theoretically derived large-sample cutoffs for test statistics? In the age of plentiful and fast computers, quite a lot, through the powerful computational machinery of the bootstrap (Efron and Tibshirani, 1993). The idea of bootstrap hypothesis testing (Efron and Tibshirani, 1993), (Bickel and Ren, 2001) is to randomly sample with replacement (the rows with non-missing data in) the dataset {(Xk,Yk)}nk=1 in such a way as to generate representative data that plausibly would have been seen if two-sample score data had been generated from two equally effective summarizers with score distributional characteristics like the pooled scores from the two observed summarizers. We have done this in two distinct ways, each creating 2000 datasets with n paired scores: MC Monte Carlo Method. For each of many itn E</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman &amp; Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Co-Occurrences Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="1030" citStr="Lin and Hovy, 2003" startWordPosition="140" endWordPosition="143">lternative that better distinguishes between human and machine evaluation systems. 1 Introduction For the past several years, the National Institute of Standards and Technology (NIST) has hosted the Text Analysis Conference (TAC) (previously called the Document Understanding Conference (DUC)) (Nat, 2010). A major theme of this conference is multi-document summarization: machine summarization of sets of related documents, sometimes query-focused and sometimes generic. The summarizers are judged by how well the summaries match human-generated summaries in either automatic metrics such as ROUGE (Lin and Hovy, 2003) or manual metrics such as responsiveness or pyramid evaluation (Nenkova et al., 2007). Typically the systems are ranked by their average score over all document sets. Ranking by average score is quite appropriate under certain statistical hypotheses, for example, when each sample is drawn from a distribution which differs from the distribution of other samples only through a location shift (Randles and Wolfe, 1979). However, a non-parametric (rank-based) analysis of variance on the summarizers’ scores on each document set revealed an impossibly small p-value (less John M. Conroy IDA/Center fo</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Co-Occurrences Statistics. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<date>2010</date>
<booktitle>Text Analysis Conference,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>http://www.nist.gov/tac.</location>
<marker>2010</marker>
<rawString>National Institute of Standards and Technology. 2010. Text Analysis Conference, http://www.nist.gov/tac.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The Pyramid Method: Incorporating Human Content Selection Variation in Summarization Evaluation.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="1116" citStr="Nenkova et al., 2007" startWordPosition="154" endWordPosition="157"> Introduction For the past several years, the National Institute of Standards and Technology (NIST) has hosted the Text Analysis Conference (TAC) (previously called the Document Understanding Conference (DUC)) (Nat, 2010). A major theme of this conference is multi-document summarization: machine summarization of sets of related documents, sometimes query-focused and sometimes generic. The summarizers are judged by how well the summaries match human-generated summaries in either automatic metrics such as ROUGE (Lin and Hovy, 2003) or manual metrics such as responsiveness or pyramid evaluation (Nenkova et al., 2007). Typically the systems are ranked by their average score over all document sets. Ranking by average score is quite appropriate under certain statistical hypotheses, for example, when each sample is drawn from a distribution which differs from the distribution of other samples only through a location shift (Randles and Wolfe, 1979). However, a non-parametric (rank-based) analysis of variance on the summarizers’ scores on each document set revealed an impossibly small p-value (less John M. Conroy IDA/Center for Computing Sciences Bowie, Maryland conroyjohnm@gmail.com Dianne P. O’Leary Universit</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The Pyramid Method: Incorporating Human Content Selection Variation in Summarization Evaluation. ACM Transactions on Speech and Language Processing, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Randles</author>
<author>D A Wolfe</author>
</authors>
<title>Introduction to the Theory of Nonparametric Statistics. Wiley series in probability and mathematical statistics. Probability and mathematical statistics.</title>
<date>1979</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="1449" citStr="Randles and Wolfe, 1979" startWordPosition="206" endWordPosition="209">documents, sometimes query-focused and sometimes generic. The summarizers are judged by how well the summaries match human-generated summaries in either automatic metrics such as ROUGE (Lin and Hovy, 2003) or manual metrics such as responsiveness or pyramid evaluation (Nenkova et al., 2007). Typically the systems are ranked by their average score over all document sets. Ranking by average score is quite appropriate under certain statistical hypotheses, for example, when each sample is drawn from a distribution which differs from the distribution of other samples only through a location shift (Randles and Wolfe, 1979). However, a non-parametric (rank-based) analysis of variance on the summarizers’ scores on each document set revealed an impossibly small p-value (less John M. Conroy IDA/Center for Computing Sciences Bowie, Maryland conroyjohnm@gmail.com Dianne P. O’Leary University of Maryland College Park, Maryland oleary@cs.umd.edu Figure 1: Confidence Intervals from a non-parametric Tukey’s honestly significant difference test for 46 TAC 2010 update document sets. The blue confidence interval (for document set d1032) does not overlap any of the 30 red intervals. Hence, the test concludes that 30 document</context>
<context position="14435" citStr="Randles and Wolfe, 1979" startWordPosition="2369" endWordPosition="2373">) is simply the fraction of times (out of 2000) that the statistic S applied to the constructed dataset had a value less than or equal to t. The upshot is that the 1 − α empirical quantile for S based on either of these simulation methods serves as a data-dependent cutoff c attaining approximate size α for all H0- generated data. The MC and HB methods will be employed in Section 4 to check the theoretical pvalues. 3 Relative Efficiency of W versus Tp Statistical theory does have something to say about the comparative powers of paired W versus Tp statistics. These statistics have been studied (Randles and Wolfe, 1979), in terms of their asymptotic relative efficiency for location-shift alternatives based on symmetric densities (f(z−V) is a locationshift of f(z)). For many pairs of parametric and rank-based statistics S, ˜S, including W and Tp, the following assertion has been proved for testing H0 at significance level α. First assume the Zk are distributed according to some density f(z − V), where f(z) is a symmetric function (f(−z) = f(z)). Next assume V = 0 under H0. When n gets large the powers at any alternatives with very small V = -y/Vn, -y =� 0, can be made asymptotically equal by using samples of </context>
</contexts>
<marker>Randles, Wolfe, 1979</marker>
<rawString>R.H. Randles and D.A. Wolfe. 1979. Introduction to the Theory of Nonparametric Statistics. Wiley series in probability and mathematical statistics. Probability and mathematical statistics. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Turing</author>
</authors>
<date>1950</date>
<journal>Computing Machinery and Intelligence. Mind,</journal>
<volume>59</volume>
<issue>236</issue>
<contexts>
<context position="17154" citStr="Turing, 1950" startWordPosition="2839" endWordPosition="2840">ts made by NIST summary assessors 3, that automatic summarization systems do not perform at the human level of performance. As such, if a statistic based on an automatic metric, such as ROUGE2, were to show fewer systems performing at human level of performance than the statistic of averaging scores, such a statistic would be preferable because 3Assessors have commented privately at the Text Analysis Conference 2008, that while the origin of the summary is hidden from them, “we know which ones are machine generated.” Thus, automatic summarization fails the Turing test of machine intelligence (Turing, 1950). This belief is also supported by (Conroy and Dang, 2008) and (Dang and Owczarzak, 2008). Finally, our own results show no matter how you compare human and machine scores all machines systems score significantly worse than humans. 471 2008: 2145 = ( 26) pairs 2009: 1830 = (21) pairs 2010: 1275 = ( 21) pairs Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Linguistic 1234 1416 1410 1000 1182 1173 841 939 934 Overall 1202 1353 1342 982 1149 1146 845 894 889 Pyramid 1263 1417 1418 1075 1238 1216 875 933 926 ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939 ROUGE-SU4 1333 </context>
</contexts>
<marker>Turing, 1950</marker>
<rawString>Alan Turing. 1950. Computing Machinery and Intelligence. Mind, 59(236):433–460.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>