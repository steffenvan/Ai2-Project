<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000066">
<title confidence="0.993551">
Word Error Rates: Decomposition over POS Classes and Applications for
Error Analysis
</title>
<author confidence="0.927856">
Maja Popovi´c
</author>
<affiliation confidence="0.864456666666667">
Lehrstuhl f¨ur Informatik 6
RWTH Aachen University
Aachen, Germany
</affiliation>
<email confidence="0.987266">
popovic@cs.rwth-aachen.de
</email>
<author confidence="0.989121">
Hermann Ney
</author>
<affiliation confidence="0.883126666666667">
Lehrstuhl f¨ur Informatik 6
RWTH Aachen University
Aachen, Germany
</affiliation>
<email confidence="0.994287">
ney@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.995583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988142857143">
Evaluation and error analysis of machine
translation output are important but difficult
tasks. In this work, we propose a novel
method for obtaining more details about ac-
tual translation errors in the generated output
by introducing the decomposition of Word
Error Rate (WER) and Position independent
word Error Rate (PER) over different Part-
of-Speech (POS) classes. Furthermore, we
investigate two possible aspects of the use
of these decompositions for automatic er-
ror analysis: estimation of inflectional errors
and distribution of missing words over POS
classes. The obtained results are shown to
correspond to the results of a human error
analysis. The results obtained on the Euro-
pean Parliament Plenary Session corpus in
Spanish and English give a better overview
of the nature of translation errors as well as
ideas of where to put efforts for possible im-
provements of the translation system.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998518888888889">
Evaluation of machine translation output is a very
important but difficult task. Human evaluation is
expensive and time consuming. Therefore a variety
of automatic evaluation measures have been studied
over the last years. The most widely used are Word
Error Rate (WER), Position independent word Error
Rate (PER), the BLEU score (Papineni et al., 2002)
and the NIST score (Doddington, 2002). These mea-
sures have shown to be valuable tools for comparing
</bodyText>
<page confidence="0.987992">
48
</page>
<bodyText confidence="0.999405925925926">
different systems as well as for evaluating improve-
ments within one system. However, these measures
do not give any details about the nature of translation
errors. Therefore some more detailed analysis of the
generated output is needed in order to identify the
main problems and to focus the research efforts. A
framework for human error analysis has been pro-
posed in (Vilar et al., 2006), but as every human
evaluation, this is also a time consuming task.
This article presents a framework for calculating
the decomposition of WER and PER over different
POS classes, i.e. for estimating the contribution of
each POS class to the overall word error rate. Al-
though this work focuses on POS classes, the method
can be easily extended to other types of linguis-
tic information. In addition, two methods for error
analysis using the WER and PER decompositons to-
gether with base forms are proposed: estimation of
inflectional errors and distribution of missing words
over POS classes. The translation corpus used for
our error analysis is built in the framework of the
TC-STAR project (tcs, 2005) and contains the tran-
scriptions of the European Parliament Plenary Ses-
sions (EPPS) in Spanish and English. The translation
system used is the phrase-based statistical machine
translation system described in (Vilar et al., 2005;
Matusov et al., 2006).
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9861492">
Automatic evaluation measures for machine trans-
lation output are receiving more and more atten-
tion in the last years. The BLEU metric (Pap-
ineni et al., 2002) and the closely related NIST met-
ric (Doddington, 2002) along with WER and PER
</bodyText>
<note confidence="0.8269965">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99992476">
have been widely used by many machine translation
researchers. An extended version of BLEU which
uses n-grams weighted according to their frequency
estimated from a monolingual corpus is proposed
in (Babych and Hartley, 2004). (Leusch et al., 2005)
investigate preprocessing and normalisation meth-
ods for improving the evaluation using the standard
measures WER, PER, BLEU and NIST. The same set
of measures is examined in (Matusov et al., 2005)
in combination with automatic sentence segmenta-
tion in order to enable evaluation of translation out-
put without sentence boundaries (e.g. translation of
speech recognition output). A new automatic met-
ric METEOR (Banerjee and Lavie, 2005) uses stems
and synonyms of the words. This measure counts
the number of exact word matches between the out-
put and the reference. In a second step, unmatched
words are converted into stems or synonyms and
then matched. The TER metric (Snover et al., 2006)
measures the amount of editing that a human would
have to perform to change the system output so that
it exactly matches the reference. The CDER mea-
sure (Leusch et al., 2006) is based on edit distance,
such as the well-known WER, but allows reordering
of blocks. Nevertheless, none of these measures or
extensions takes into account linguistic knowledge
about actual translation errors, for example what is
the contribution of verbs in the overall error rate,
how many full forms are wrong whereas their base
forms are correct, etc. A framework for human error
analysis has been proposed in (Vilar et al., 2006)
and a detailed analysis of the obtained results has
been carried out. However, human error analysis,
like any human evaluation, is a time consuming task.
Whereas the use of linguistic knowledge for im-
proving the performance of a statistical machine
translation system is investigated in many publi-
cations for various language pairs (like for exam-
ple (Nießen and Ney, 2000), (Goldwater and Mc-
Closky, 2005)), its use for the analysis of translation
errors is still a rather unexplored area. Some auto-
matic methods for error analysis using base forms
and POS tags are proposed in (Popovi´c et al., 2006;
Popovi´c and Ney, 2006). These measures are based
on differences between WER and PER which are cal-
culated separately for each POS class using subsets
extracted from the original texts. Standard overall
WER and PER of the original texts are not at all
taken into account. In this work, the standard WER
and PER are decomposed and analysed.
</bodyText>
<sectionHeader confidence="0.8160615" genericHeader="method">
3 Decomposition of WER and PER over
POS classes
</sectionHeader>
<bodyText confidence="0.971825">
The standard procedure for evaluating machine
translation output is done by comparing the hypoth-
esis document hyp with given reference translations
ref, each one consisting of K sentences (or seg-
ments). The reference document ref consists of
R reference translations for each sentence. Let the
length of the hypothesis sentence hypk be denoted
as Nhypk, and the reference lengths of each sentence
Nref k�r. Then, the total hypothesis length of the doc-
ument is Nhyp = Ek Nhypk, and the total reference
length is Nref = Ek N*ref k where N* f k is defined
re
as the length of the reference sentence with the low-
est sentence-level error rate as shown to be optimal
in (Leusch et al., 2005).
</bodyText>
<subsectionHeader confidence="0.998164">
3.1 Standard word error rates (overview)
</subsectionHeader>
<bodyText confidence="0.99993505">
The word error rate (WER) is based on the Lev-
enshtein distance (Levenshtein, 1966) - the mini-
mum number of substitutions, deletions and inser-
tions that have to be performed to convert the gen-
erated text hyp into the reference text ref. A short-
coming of the WER is the fact that it does not allow
reorderings of words, whereas the word order of the
hypothesis can be different from word order of the
reference even though it is correct translation. In
order to overcome this problem, the position inde-
pendent word error rate (PER) compares the words
in the two sentences without taking the word order
into account. The PER is always lower than or equal
to the WER. On the other hand, shortcoming of the
PER is the fact that the word order can be impor-
tant in some cases. Therefore the best solution is to
calculate both word error rates.
Calculation of WER: The WER of the hypothe-
sis hyp with respect to the reference ref is calculated
as:
</bodyText>
<subsubsectionHeader confidence="0.297739">
dL(ref k,r, hypk)
</subsubsectionHeader>
<bodyText confidence="0.997776333333333">
where dL(ref k,r, hypk) is the Levenshtein dis-
tance between the reference sentence ref k,r and the
hypothesis sentence hypk. The calculation of WER
</bodyText>
<equation confidence="0.99643175">
1
WER =
min
r
N*
ref
K
k=1
</equation>
<page confidence="0.991831">
49
</page>
<bodyText confidence="0.97649575">
is performed using a dynamic programming algo-
rithm.
Calculation of PER: The PER can be calcu-
lated using the counts n(e, hypk) and n(e, ref k,r)
of a word e in the hypothesis sentence hypk and the
reference sentence ref k,r respectively:
dPER(refk,r, hypk)
where
</bodyText>
<equation confidence="0.890114666666667">
1
dPER(ref k,r, hypk) = 2  |Nref k,r − Nhypk |+
)|n(e, ref k,r) − n(e, hypk)|
</equation>
<subsectionHeader confidence="0.985069">
3.2 WER decomposition over POS classes
</subsectionHeader>
<bodyText confidence="0.9996955">
The dynamic programming algorithm for WER en-
ables a simple and straightforward identification of
each erroneous word which actually contributes to
WER. Let errk denote the set of erroneous words
in sentence k with respect to the best reference and
p be a POS class. Then n(p, errk) is the number of
errors in errk produced by words with POS class p.
It should be noted that for the substitution errors, the
POS class of the involved reference word is taken
into account. POS tags of the reference words are
also used for the deletion errors, and for the inser-
tion errors the POS class of the hypothesis word is
taken. The WER for the word class p can be calcu-
lated as:
</bodyText>
<equation confidence="0.9607295">
1
WER(p) =
N*
ref
</equation>
<bodyText confidence="0.999736083333333">
The sum over all classes is equal to the standard
overall WER.
An example of a reference sentence and hypothe-
sis sentence along with the corresponding POS tags
is shown in Table 1. The WER errors, i.e. actual
words participating in WER together with their POS
classes can be seen in Table 2. The reference words
involved in WER are denoted as reference errors,
and hypothesis errors refer to the hypothesis words
participating in WER.
Standard WER of the whole sentence is equal
to 4/12 = 33.3%. The contribution of nouns is
</bodyText>
<equation confidence="0.9872728">
reference:
Mister#N Commissioner#N ,#PUN
twenty-four#NUM hours#N
sometimes#ADV can#V be#V too#ADV
much#PRON time#N .#PUN
hypothesis:
Mrs#N Commissioner#N ,#PUN
twenty-four#NUM hours#N is#V
sometimes#ADV too#ADV
much#PRON time#N .#PUN
</equation>
<tableCaption confidence="0.933843">
Table 1: Example for illustration of actual errors: a
POS tagged reference sentence and a corresponding
hypothesis sentence
</tableCaption>
<table confidence="0.9940194">
reference errors hypothesis errors error type
Mister#N Mrs#N substitution
sometimes#ADV is#V substitution
can#V sometimes#ADV deletion
be#V substitution
</table>
<tableCaption confidence="0.779716666666667">
Table 2: WER errors: actual words which are partici-
pating in the word error rate and their corresponding
POS classes
</tableCaption>
<equation confidence="0.609330666666667">
WER(N) = 1/12 = 8.3%, of verbs WER(V) =
2/12 = 16.7% and of adverbs WER(ADV) =
1/12 = 8.3%
</equation>
<subsectionHeader confidence="0.945341">
3.3 PER decomposition over POS classes
</subsectionHeader>
<bodyText confidence="0.9998735">
In contrast to WER, standard efficient algorithms for
the calculation of PER do not give precise informa-
tion about contributing words. However, it is pos-
sible to identify all words in the hypothesis which
do not have a counterpart in the reference, and vice
versa. These words will be referred to as PER errors.
</bodyText>
<table confidence="0.7713625">
reference errors hypothesis errors
Mister#N Mrs#N
be#V is#V
can#V
</table>
<tableCaption confidence="0.932192">
Table 3: PER errors: actual words which are partic-
</tableCaption>
<bodyText confidence="0.898511">
ipating in the position independent word error rate
and their corresponding POS classes
An illustration of PER errors is given in Table 3.
</bodyText>
<equation confidence="0.998482307692308">
1
PER =
min
r
N*
ref
K
k=1
�
e
K
n(p, errk)
k=1
</equation>
<page confidence="0.948558">
50
</page>
<bodyText confidence="0.999375875">
The number of errors contributing to the standard
PER according to the algorithm described in 3.1 is 3
- there are two substitutions and one deletion. The
problem with standard PER is that it is not possible
to detect which words are the deletion errors, which
are the insertion errors, and which words are the sub-
stitution errors. Therefore we introduce an alterna-
tive PER based measure which corresponds to the
F-measure. Let herrk refer to the set of words in the
hypothesis sentence k which do not appear in the
reference sentence k (referred to as hypothesis er-
rors). Analogously, let rerrk denote the set of words
in the reference sentence k which do not appear in
the hypothesis sentence k (referred to as reference
errors). Then the following measures can be calcu-
lated:
</bodyText>
<listItem confidence="0.973053">
• reference PER (RPER) (similar to recall):
</listItem>
<bodyText confidence="0.908655166666667">
Nref = 12 thus being equal to 25%. The FPER is the
sum of hypothesis and reference errors divided by
the sum of hypothesis and reference length: FPER =
(2 + 3)/(11 + 12) = 5/23 = 21.7%. The contribu-
tion of nouns is FPER(N) = 2/23 = 8.7% and the
contribution of verbs is FPER(V) = 3/23 = 13%.
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="method">
4 Applications for error analysis
</sectionHeader>
<bodyText confidence="0.999785833333333">
The decomposed error rates described in Section 3.2
and Section 3.3 contain more details than the stan-
dard error rates. However, for more precise informa-
tion about certain phenomena some kind of further
analysis is required. In this work, we investigate two
possible aspects for error analysis:
</bodyText>
<listItem confidence="0.984251">
• estimation of inflectional errors by the use of
FPER errors and base forms
</listItem>
<table confidence="0.66841175">
1 K n(p, rerrk) • extracting the distribution of missing words
RPER(p) = k=1 over POS classes using WER errors, FPER er-
N� rors and base forms.
ref
</table>
<listItem confidence="0.947882">
• hypothesis PER (HPER) (similar to precision):
</listItem>
<equation confidence="0.984289666666667">
K
·1: (n(p, rerrk) + n(p, herrk))
k=1
</equation>
<bodyText confidence="0.999102923076923">
Since we are basically interested in all words with-
out a counterpart, both in the reference and in the
hypothesis, this work will be focused on FPER. The
sum of FPER over all POS classes is equal to the
overall FPER, and the latter is always less or equal
to the standard PER.
For the example sentence presented in Table 1, the
number of hypothesis errors n(e, herrk) is 2 and the
number of reference errors n(e, rerrk) is 3 where e
denotes the word. The number of errors contributing
to the standard PER is 3, since |Nref − Nhyp |= 1
and Ee |n(e, ref k) − n(e, hypk) |= 5. The stan-
dard PER is normalised over the reference length
</bodyText>
<subsectionHeader confidence="0.937951">
4.1 Inflectional errors
</subsectionHeader>
<bodyText confidence="0.997133692307692">
Inflectional errors can be estimated using FPER
errors and base forms. From each reference-
hypothesis sentence pair, only erroneous words
which have the common base forms are taken
into account. The inflectional error rate of each POS
class is then calculated in the same way as FPER.
For example, from the PER errors presented in Ta-
ble 3, the words “is” and “be” are candidates for an
inflectional error because they are sharing the same
base form “be”. Inflectional error rate in this exam-
ple is present only for the verbs, and is calculated in
the same way as FPER, i.e. IFPER(V) = 2/23 =
8.7%.
</bodyText>
<subsectionHeader confidence="0.990158">
4.2 Missing words
</subsectionHeader>
<bodyText confidence="0.9998965">
Distribution of missing words over POS classes can
be extracted from the WER and FPER errors in the
following way: the words considered as missing are
those which occur as deletions in WER errors and
at the same time occur only as reference PER errors
without sharing the base form with any hypothesis
error. The use of both WER and PER errors is much
more reliable than using only the WER deletion er-
ros because not all deletion errors are produced by
missing words: a number of WER deletions appears
</bodyText>
<equation confidence="0.966655375">
K
HPER(p) = 1 1: n(p, herrk)
Nhyp k=1
• F-based PER (FPER):
1
FPER(p) =
N�f + Nhyp
·
</equation>
<page confidence="0.984423">
51
</page>
<bodyText confidence="0.99997225">
due to reordering errors. The information about the
base form is used in order to eliminate inflectional
errors. The number of missing words is extracted for
each word class and then normalised over the sum of
all classes. For the example sentence pair presented
in Table 1, from the WER errors in Table 2 and the
PER errors in Table 3 the word “can” will be identi-
fied as missing.
</bodyText>
<sectionHeader confidence="0.993145" genericHeader="method">
5 Experimental settings
</sectionHeader>
<subsectionHeader confidence="0.993694">
5.1 Translation System
</subsectionHeader>
<bodyText confidence="0.999930222222222">
The machine translation system used in this work
is based on the statistical aproach. It is built as
a log-linear combination of seven different statisti-
cal models: phrase based models in both directions,
IBM1 models at the phrase level in both directions,
as well as target language model, phrase penalty and
length penalty are used. A detailed description of the
system can be found in (Vilar et al., 2005; Matusov
et al., 2006).
</bodyText>
<subsectionHeader confidence="0.998705">
5.2 Task and corpus
</subsectionHeader>
<bodyText confidence="0.999911363636364">
The corpus analysed in this work is built in the
framework of the TC-STAR project. The training
corpus contains more than one million sentences and
about 35 million running words of the European Par-
liament Plenary Sessions (EPPS) in Spanish and En-
glish. The test corpus contains about 1000 sentences
and 28 000 running words. The OOV rates are low,
about 0.5% of the running words for Spanish and
0.2% for English. The corpus statistics can be seen
in Table 4. More details about the EPPS data can be
found in (Vilar et al., 2005).
</bodyText>
<table confidence="0.9964515">
TRAIN Spanish English
Sentences 1 167 627
Running words 35 320 646 33 945 468
Vocabulary 159 080 110 636
TEST
Sentences 894 1117
Running words 28 591 28 492
OOVs 0.52% 0.25%
</table>
<tableCaption confidence="0.928566666666667">
Table 4: Statistics of the training and test corpora
of the TC-STAR EPPS Spanish-English task. Test
corpus is provided with two references.
</tableCaption>
<sectionHeader confidence="0.989004" genericHeader="method">
6 Error analysis
</sectionHeader>
<bodyText confidence="0.999994434782609">
The translation is performed in both directions
(Spanish to English and English to Spanish) and the
error analysis is done on both the English and the
Spanish output. Morpho-syntactic annotation of the
English references and hypotheses is performed us-
ing the constraint grammar parser ENGCG (Vouti-
lainen, 1995), and the Spanish texts are annotated
using the FreeLing analyser (Carreras et al., 2004).
In this way, all references and hypotheses are pro-
vided with POS tags and base forms. The decom-
position of WER and FPER is done over the ten
main POS classes: nouns (N), verbs (V), adjectives
(A), adverbs (ADV), pronouns (PRON), determiners
(DET), prepositions (PREP), conjunctions (CON),
numerals (NUM) and punctuation marks (PUN). In-
flectional error rates are also estimated for each POS
class using FPER counts and base forms. Addition-
ally, details about the verb tense and person inflec-
tions for both languages as well as about the adjec-
tive gender and person inflections for the Spanish
output are extracted. Apart from that, the distribu-
tion of missing words over the ten POS classes is
estimated using the WER and FPER errors.
</bodyText>
<subsectionHeader confidence="0.981561">
6.1 WER and PER (FPER) decompositions
</subsectionHeader>
<bodyText confidence="0.997612380952381">
Figure 1 presents the decompositions of WER and
FPER over the ten basic POS classes for both lan-
guages. The largest part of both word error rates
comes from the two most important word classes,
namely nouns and verbs, and that the least critical
classes are punctuations, conjunctions and numbers.
Adjectives, determiners and prepositions are sig-
nificantly worse in the Spanish output. This is partly
due to the richer morphology of the Spanish lan-
guage. Furthermore, the histograms indicate that the
number of erroneus nouns and pronouns is higher
in the English output. As for verbs, WER is higher
for English and FPER for Spanish. This indicates
that there are more problems with word order in the
English output, and more problems with the correct
verb or verb form in the Spanish output.
In addition, the decomposed error rates give an
idea of where to put efforts for possible improve-
ments of the system. For example, working on im-
provements of verb translations could reduce up to
about 10% WER and 7% FPER, working on nouns
</bodyText>
<page confidence="0.988984">
52
</page>
<figure confidence="0.998240730769231">
WER over POS classes [%] inflectional errors [%]
English
Spanish
2.5
2
1.5
1
0.5
0
English
Spanish
11
10
9
8
7
6
5
4
3
2
1
0
N V A ADV PRON DET PREP CON NUM PUN N V A ADV PRON DET PREP CON NUM PUN
FPER over POS classes [%]
N V A ADV PRON DET PREP CON NUM PUN
</figure>
<figureCaption confidence="0.802191">
Figure 1: Decomposition of WER and FPER [%]
over the ten basic POS classes for English and Span-
ish output
</figureCaption>
<bodyText confidence="0.7280415">
up to 8% WER and 5% FPER, whereas there is no
reason to put too much efforts on e.g. adverbs since
this could lead only to about 2% of WER and FPER
reduction. 1
</bodyText>
<subsectionHeader confidence="0.998122">
6.2 Inflectional errors
</subsectionHeader>
<bodyText confidence="0.999700666666667">
Inflectional error rates for the ten POS classes are
presented in Figure 2. For the English language,
these errors are significant only for two POS classes:
nouns and verbs. The verbs are the most problem-
atic category in both languages, for Spanish having
almost two times higher error rate than for English.
This is due to the very rich morphology of Spanish
verbs - one base form might have up to about fourty
different inflections.
</bodyText>
<footnote confidence="0.851029">
1Reduction of FPER leads to a similar reduction of PER.
</footnote>
<figureCaption confidence="0.9110965">
Figure 2: Inflectional error rates [%] for English and
Spanish output
</figureCaption>
<bodyText confidence="0.9992175">
Nouns have a higher error rate for English than
for Spanish. The reason for this difference is not
clear, since the noun morphology of neither of the
languages is particularly rich - there is only distinc-
tion between singular and plural. One possible ex-
planation might be the numerous occurences of dif-
ferent variants of the same word, like for example
“Mr” and “Mister”.
In the Spanish output, two additional POS classes
are showing significant error rate: determiners and
adjectives. This is due to the gender and number in-
flections of those classes which do not exist in the
English language - for each determiner or adjective,
there are four variants in Spanish and only one in En-
glish. Working on inflections of Spanish verbs might
reduce approximately 2% of FPER, on English verbs
about 1%. Improvements of Spanish determiners
could lead up to about 2% of improvements.
</bodyText>
<subsectionHeader confidence="0.979094">
6.2.1 Comparison with human error analysis
</subsectionHeader>
<bodyText confidence="0.999157545454545">
The results obtained for inflectional errors are
comparable with the results of a human error anal-
ysis carried out in (Vilar et al., 2006). Although it
is difficult to compare all the numbers directly, the
overall tendencies are the same: the largest num-
ber of translation errors are caused by Spanish verbs,
and much less but still a large number of errors by
English verbs. A much smaller but still significant
number of errors is due to Spanish adjectives, and
only a few errors of English adjectives are present.
Human analysis was done also for the tense and
</bodyText>
<figure confidence="0.98531725">
9
8
7
6
5
4
3
2
1
0
English
Spanish
</figure>
<page confidence="0.997301">
53
</page>
<bodyText confidence="0.989631333333333">
person of verbs, as well as for the number and gen-
der of adjectives. We use more detailed POS tags in
order to extract this additional information and cal-
culate inflectional error rates for such tags. It should
be noted that in contrast to all previous error rates,
these error rates are not disjunct but overlapping:
many words are contributing to both.
The results are shown in Figure 3, and the tenden-
cies are again the same as those reported in (Vilar
et al., 2006). As for verbs, tense errors are much
more frequent than person errors for both languages.
Adjective inflections cause certain amount of errors
only in the Spanish output. Contributions of gender
and of number are aproximately equal.
inflectional errors of verbs and adjectives [%]
</bodyText>
<figureCaption confidence="0.966056">
Figure 3: More details about inflections: verb tense
and person error rates and adjective gender and num-
ber error rates [%]
</figureCaption>
<subsectionHeader confidence="0.997384">
6.3 Missing words
</subsectionHeader>
<bodyText confidence="0.969531466666667">
Figure 4 presents the distribution of missing words
over POS classes. This distribution has a same be-
haviour as the one obtained by human error analysis.
Most missing words for both languages are verbs.
For English, the percentage of missing verbs is sig-
nificantly higher than for Spanish. The same thing
happens for pronouns. The probable reason for this
is the nature of Spanish verbs. Since person and
tense are contained in the suffix, Spanish pronouns
are often omitted, and auxiliary verbs do not exist
for all tenses. This could be problematic for a trans-
lation system, because it processes only one Spanish
word which actually contains two (or more) English
words.
missing words [%]
</bodyText>
<figureCaption confidence="0.933566">
Figure 4: Distribution of missing words over POS
classes [%] for English and Spanish output
</figureCaption>
<bodyText confidence="0.883579">
Prepositions are more often missing in Spanish
than in English, as well as determiners. A probable
reason is the disproportion of the number of occur-
rences for those classes between two languages.
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999918125">
This work presents a framework for extraction of lin-
guistic details from standard word error rates WER
and PER and their use for an automatic error analy-
sis. We presented a method for the decomposition of
standard word error rates WER and PER over ten ba-
sic POS classes. We also carried out a detailed anal-
ysis of inflectional errors which has shown that the
results obtained by our method correspond to those
obtained by a human error analysis. In addition, we
proposed a method for analysing missing word er-
rors.
We plan to extend the proposed methods in order
to carry out a more detailed error analysis, for ex-
ample examining different types of verb inflections.
We also plan to examine other types of translation
errors like for example errors caused by word order.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998666">
This work was partly funded by the European Union
under the integrated project TC-STAR– Technology
and Corpora for Speech to Speech Translation (IST-
2002-FP6-506738).
</bodyText>
<figure confidence="0.9983265">
0.5
1.5
2
0
1
English
Spanish
V tense V person A gender A number
30
28
26
24
22
20
18
16
14
12
10
8
6
4
2
0
N V A ADV PRON DET
PREP CON NUM PUN
eng
esp
</figure>
<page confidence="0.987995">
54
</page>
<sectionHeader confidence="0.988718" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998962237113402">
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing BLEU MT Evaluation Method with Frequency
Weighting. In Proc. of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL),
Barcelona, Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In 43rd
Annual Meeting of the Assoc. for Computational Lin-
guistics: Proc. Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 65–72, Ann Arbor, MI, June.
Xavier Carreras, Isaac Chao, Lluis Padr´o, and Muntsa
Padr´o. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proc. 4th Int. Conf. on Lan-
guage Resources and Evaluation (LREC), pages 239–
242, Lisbon, Portugal, May.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128–132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Gregor Leusch, Nicola Ueffing, David Vilar, and Her-
mann Ney. 2005. Preprocessing and Normalization
for Automatic Evaluation of Machine Translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 17–24, Ann Arbor, MI, June. Association
for Computational Linguistics.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In EACL06, pages 241–248, Trento, Italy,
April.
Vladimir Iosifovich Levenshtein. 1966. Binary Codes
Capable of Correcting Deletions, Insertions and Re-
versals. Soviet Physics Doklady, 10(8):707–710,
February.
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating Machine Transla-
tion Output with Automatic Sentence Segmentation.
In Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 148–154,
Pittsburgh, PA, October.
Evgeny Matusov, Richard Zens, David Vilar, Arne
Mauser, Maja Popovi´c, and Hermann Ney. 2006.
The RWTH Machine Translation System. In TC-Star
Workshop on Speech-to-Speech Translation, pages 31–
36, Barcelona, Spain, June.
Sonja Nießen and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
’00: The 18th Int. Conf. on Computational Linguistics,
pages 1081–1085, Saarbr¨ucken, Germany, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311–318, Philadelphia, PA,
July.
Maja Popovi´c and Hermann Ney. 2006. Error Analysis
of Verb Inflections in Spanish Translation Output. In
TC-Star Workshop on Speech-to-Speech Translation,
pages 99–103, Barcelona, Spain, June.
Maja Popovi´c, Adri`a de Gispert, Deepa Gupta, Patrik
Lambert, Hermann Ney, Jos´e B. Mari˜no, Marcello
Federico, and Rafael Banchs. 2006. Morpho-syntactic
Information for Automatic Error Analysis of Statisti-
cal Machine Translation Output. In Proc. of the HLT-
NAACL Workshop on Statistical Machine Translation,
pages 1–6, New York, NY, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Error Rate with Targeted Human An-
notation. In Proc. of the 7th Conf. of the Association
for Machine Translation in the Americas (AMTA 06),
pages 223–231, Boston, MA.
2005. TC-STAR - technology and corpora for speech to
speech translation. Integrated project TCSTAR (IST-
2002-FP6-506738) funded by the European Commis-
sion. http://www.tc-star.org/.
David Vilar, Evgeny Matusov, Saˇsa Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical Machine Transla-
tion of European Parliamentary Speeches. In Proc. MT
Summit X, pages 259–266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proc. of the Fifth Int.
Conf. on Language Resources and Evaluation (LREC),
pages 697–702, Genoa, Italy, May.
Atro Voutilainen. 1995. ENGCG -
Constraint Grammar Parser of English.
http://www2.lingsoft.fi/doc/engcg/intro/.
</reference>
<page confidence="0.999027">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.131258">
<title confidence="0.820451666666667">Error Rates: Decomposition over and Applications Error Analysis Maja</title>
<author confidence="0.678767">Lehrstuhl f¨ur Informatik</author>
<affiliation confidence="0.862423">RWTH Aachen</affiliation>
<address confidence="0.892762">Aachen,</address>
<email confidence="0.986948">popovic@cs.rwth-aachen.de</email>
<author confidence="0.957273">Hermann</author>
<affiliation confidence="0.724319">Lehrstuhl f¨ur Informatik RWTH Aachen</affiliation>
<address confidence="0.913633">Aachen,</address>
<email confidence="0.997573">ney@cs.rwth-aachen.de</email>
<abstract confidence="0.999165954545455">Evaluation and error analysis of machine translation output are important but difficult tasks. In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Rate and Position independent Error Rate over different Partclasses. Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inflectional errors distribution of missing words over classes. The obtained results are shown to correspond to the results of a human error analysis. The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Extending BLEU MT Evaluation Method with Frequency Weighting.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3645" citStr="Babych and Hartley, 2004" startWordPosition="566" endWordPosition="569">elated Work Automatic evaluation measures for machine translation output are receiving more and more attention in the last years. The BLEU metric (Papineni et al., 2002) and the closely related NIST metric (Doddington, 2002) along with WER and PER Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, June 2007. c�2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, BLEU and NIST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second </context>
</contexts>
<marker>Babych, Hartley, 2004</marker>
<rawString>Bogdan Babych and Anthony Hartley. 2004. Extending BLEU MT Evaluation Method with Frequency Weighting. In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgements.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="4103" citStr="Banerjee and Lavie, 2005" startWordPosition="636" endWordPosition="639">ers. An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, BLEU and NIST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The TER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference. The CDER measure (Leusch et al., 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. Nevertheless, none of these measures or extensions takes into account linguistic </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgements. In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65–72, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Isaac Chao</author>
<author>Lluis Padr´o</author>
<author>Muntsa Padr´o</author>
</authors>
<title>FreeLing: An Open-Source Suite of Language Analyzers.</title>
<date>2004</date>
<booktitle>In Proc. 4th Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>239--242</pages>
<location>Lisbon, Portugal,</location>
<marker>Carreras, Chao, Padr´o, Padr´o, 2004</marker>
<rawString>Xavier Carreras, Isaac Chao, Lluis Padr´o, and Muntsa Padr´o. 2004. FreeLing: An Open-Source Suite of Language Analyzers. In Proc. 4th Int. Conf. on Language Resources and Evaluation (LREC), pages 239– 242, Lisbon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology,</booktitle>
<pages>128--132</pages>
<location>San Diego.</location>
<contexts>
<context position="1609" citStr="Doddington, 2002" startWordPosition="240" endWordPosition="241"> Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system. 1 Introduction Evaluation of machine translation output is a very important but difficult task. Human evaluation is expensive and time consuming. Therefore a variety of automatic evaluation measures have been studied over the last years. The most widely used are Word Error Rate (WER), Position independent word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). These measures have shown to be valuable tools for comparing 48 different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some more detailed analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. A framework for human error analysis has been proposed in (Vilar et al., 2006), but as every human evaluation, this is also a time consuming task. This article presents a framework for calculating the decomposition of WER an</context>
<context position="3244" citStr="Doddington, 2002" startWordPosition="510" endWordPosition="511">ng words over POS classes. The translation corpus used for our error analysis is built in the framework of the TC-STAR project (tcs, 2005) and contains the transcriptions of the European Parliament Plenary Sessions (EPPS) in Spanish and English. The translation system used is the phrase-based statistical machine translation system described in (Vilar et al., 2005; Matusov et al., 2006). 2 Related Work Automatic evaluation measures for machine translation output are receiving more and more attention in the last years. The BLEU metric (Papineni et al., 2002) and the closely related NIST metric (Doddington, 2002) along with WER and PER Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, June 2007. c�2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, BLEU and NIST. The same set of measures is examined in (</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ARPA Workshop on Human Language Technology, pages 128–132, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving stastistical machine translation through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="5375" citStr="Goldwater and McClosky, 2005" startWordPosition="848" endWordPosition="852">for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. However, human error analysis, like any human evaluation, is a time consuming task. Whereas the use of linguistic knowledge for improving the performance of a statistical machine translation system is investigated in many publications for various language pairs (like for example (Nießen and Ney, 2000), (Goldwater and McClosky, 2005)), its use for the analysis of translation errors is still a rather unexplored area. Some automatic methods for error analysis using base forms and POS tags are proposed in (Popovi´c et al., 2006; Popovi´c and Ney, 2006). These measures are based on differences between WER and PER which are calculated separately for each POS class using subsets extracted from the original texts. Standard overall WER and PER of the original texts are not at all taken into account. In this work, the standard WER and PER are decomposed and analysed. 3 Decomposition of WER and PER over POS classes The standard pro</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving stastistical machine translation through morphological analysis. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>David Vilar</author>
<author>Hermann Ney</author>
</authors>
<title>Preprocessing and Normalization for Automatic Evaluation of Machine Translation.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="3668" citStr="Leusch et al., 2005" startWordPosition="570" endWordPosition="573">tion measures for machine translation output are receiving more and more attention in the last years. The BLEU metric (Papineni et al., 2002) and the closely related NIST metric (Doddington, 2002) along with WER and PER Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, June 2007. c�2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, BLEU and NIST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words a</context>
<context position="6644" citStr="Leusch et al., 2005" startWordPosition="1070" endWordPosition="1073">is done by comparing the hypothesis document hyp with given reference translations ref, each one consisting of K sentences (or segments). The reference document ref consists of R reference translations for each sentence. Let the length of the hypothesis sentence hypk be denoted as Nhypk, and the reference lengths of each sentence Nref k�r. Then, the total hypothesis length of the document is Nhyp = Ek Nhypk, and the total reference length is Nref = Ek N*ref k where N* f k is defined re as the length of the reference sentence with the lowest sentence-level error rate as shown to be optimal in (Leusch et al., 2005). 3.1 Standard word error rates (overview) The word error rate (WER) is based on the Levenshtein distance (Levenshtein, 1966) - the minimum number of substitutions, deletions and insertions that have to be performed to convert the generated text hyp into the reference text ref. A shortcoming of the WER is the fact that it does not allow reorderings of words, whereas the word order of the hypothesis can be different from word order of the reference even though it is correct translation. In order to overcome this problem, the position independent word error rate (PER) compares the words in the t</context>
</contexts>
<marker>Leusch, Ueffing, Vilar, Ney, 2005</marker>
<rawString>Gregor Leusch, Nicola Ueffing, David Vilar, and Hermann Ney. 2005. Preprocessing and Normalization for Automatic Evaluation of Machine Translation. In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 17–24, Ann Arbor, MI, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDER: Efficient MT Evaluation Using Block Movements. In</title>
<date>2006</date>
<booktitle>EACL06,</booktitle>
<pages>241--248</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="4533" citStr="Leusch et al., 2006" startWordPosition="713" endWordPosition="716">tion in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The TER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference. The CDER measure (Leusch et al., 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. Nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. However, human error analysis, like any human evaluation, is a time consuming task. Whereas </context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In EACL06, pages 241–248, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary Codes Capable of Correcting Deletions, Insertions and Reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="6769" citStr="Levenshtein, 1966" startWordPosition="1092" endWordPosition="1093">segments). The reference document ref consists of R reference translations for each sentence. Let the length of the hypothesis sentence hypk be denoted as Nhypk, and the reference lengths of each sentence Nref k�r. Then, the total hypothesis length of the document is Nhyp = Ek Nhypk, and the total reference length is Nref = Ek N*ref k where N* f k is defined re as the length of the reference sentence with the lowest sentence-level error rate as shown to be optimal in (Leusch et al., 2005). 3.1 Standard word error rates (overview) The word error rate (WER) is based on the Levenshtein distance (Levenshtein, 1966) - the minimum number of substitutions, deletions and insertions that have to be performed to convert the generated text hyp into the reference text ref. A shortcoming of the WER is the fact that it does not allow reorderings of words, whereas the word order of the hypothesis can be different from word order of the reference even though it is correct translation. In order to overcome this problem, the position independent word error rate (PER) compares the words in the two sentences without taking the word order into account. The PER is always lower than or equal to the WER. On the other hand,</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady, 10(8):707–710, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Gregor Leusch</author>
<author>Oliver Bender</author>
<author>Hermann Ney</author>
</authors>
<title>Evaluating Machine Translation Output with Automatic Sentence Segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>148--154</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="3865" citStr="Matusov et al., 2005" startWordPosition="601" endWordPosition="604"> along with WER and PER Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, June 2007. c�2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, BLEU and NIST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The TER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it e</context>
</contexts>
<marker>Matusov, Leusch, Bender, Ney, 2005</marker>
<rawString>Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005. Evaluating Machine Translation Output with Automatic Sentence Segmentation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 148–154, Pittsburgh, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Richard Zens</author>
<author>David Vilar</author>
<author>Arne Mauser</author>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<date>2006</date>
<marker>Matusov, Zens, Vilar, Mauser, Popovi´c, Ney, 2006</marker>
<rawString>Evgeny Matusov, Richard Zens, David Vilar, Arne Mauser, Maja Popovi´c, and Hermann Ney. 2006.</rawString>
</citation>
<citation valid="true">
<title>The RWTH Machine Translation System. In TC-Star Workshop on Speech-to-Speech Translation,</title>
<date></date>
<pages>31--36</pages>
<location>Barcelona, Spain,</location>
<marker></marker>
<rawString>The RWTH Machine Translation System. In TC-Star Workshop on Speech-to-Speech Translation, pages 31– 36, Barcelona, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In COLING ’00: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1081--1085</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="5343" citStr="Nießen and Ney, 2000" startWordPosition="844" endWordPosition="847">ual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. However, human error analysis, like any human evaluation, is a time consuming task. Whereas the use of linguistic knowledge for improving the performance of a statistical machine translation system is investigated in many publications for various language pairs (like for example (Nießen and Ney, 2000), (Goldwater and McClosky, 2005)), its use for the analysis of translation errors is still a rather unexplored area. Some automatic methods for error analysis using base forms and POS tags are proposed in (Popovi´c et al., 2006; Popovi´c and Ney, 2006). These measures are based on differences between WER and PER which are calculated separately for each POS class using subsets extracted from the original texts. Standard overall WER and PER of the original texts are not at all taken into account. In this work, the standard WER and PER are decomposed and analysed. 3 Decomposition of WER and PER o</context>
</contexts>
<marker>Nießen, Ney, 2000</marker>
<rawString>Sonja Nießen and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In COLING ’00: The 18th Int. Conf. on Computational Linguistics, pages 1081–1085, Saarbr¨ucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WieJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1571" citStr="Papineni et al., 2002" startWordPosition="232" endWordPosition="235">lysis. The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system. 1 Introduction Evaluation of machine translation output is a very important but difficult task. Human evaluation is expensive and time consuming. Therefore a variety of automatic evaluation measures have been studied over the last years. The most widely used are Word Error Rate (WER), Position independent word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). These measures have shown to be valuable tools for comparing 48 different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some more detailed analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. A framework for human error analysis has been proposed in (Vilar et al., 2006), but as every human evaluation, this is also a time consuming task. This article presents a framework for c</context>
<context position="3189" citStr="Papineni et al., 2002" startWordPosition="498" endWordPosition="502"> estimation of inflectional errors and distribution of missing words over POS classes. The translation corpus used for our error analysis is built in the framework of the TC-STAR project (tcs, 2005) and contains the transcriptions of the European Parliament Plenary Sessions (EPPS) in Spanish and English. The translation system used is the phrase-based statistical machine translation system described in (Vilar et al., 2005; Matusov et al., 2006). 2 Related Work Automatic evaluation measures for machine translation output are receiving more and more attention in the last years. The BLEU metric (Papineni et al., 2002) and the closely related NIST metric (Doddington, 2002) along with WER and PER Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, June 2007. c�2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004). (Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, B</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WieJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Error Analysis of Verb Inflections in Spanish Translation Output.</title>
<date>2006</date>
<booktitle>In TC-Star Workshop on Speech-to-Speech Translation,</booktitle>
<pages>99--103</pages>
<location>Barcelona, Spain,</location>
<marker>Popovi´c, Ney, 2006</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2006. Error Analysis of Verb Inflections in Spanish Translation Output. In TC-Star Workshop on Speech-to-Speech Translation, pages 99–103, Barcelona, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Adri`a de Gispert</author>
<author>Deepa Gupta</author>
<author>Patrik Lambert</author>
<author>Hermann Ney</author>
<author>Jos´e B Mari˜no</author>
<author>Marcello Federico</author>
<author>Rafael Banchs</author>
</authors>
<title>Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output.</title>
<date>2006</date>
<booktitle>In Proc. of the HLTNAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>1--6</pages>
<location>New York, NY,</location>
<marker>Popovi´c, de Gispert, Gupta, Lambert, Ney, Mari˜no, Federico, Banchs, 2006</marker>
<rawString>Maja Popovi´c, Adri`a de Gispert, Deepa Gupta, Patrik Lambert, Hermann Ney, Jos´e B. Mari˜no, Marcello Federico, and Rafael Banchs. 2006. Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output. In Proc. of the HLTNAACL Workshop on Statistical Machine Translation, pages 1–6, New York, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Error Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA 06),</booktitle>
<pages>223--231</pages>
<location>Boston, MA.</location>
<contexts>
<context position="4358" citStr="Snover et al., 2006" startWordPosition="681" endWordPosition="684"> evaluation using the standard measures WER, PER, BLEU and NIST. The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g. translation of speech recognition output). A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words. This measure counts the number of exact word matches between the output and the reference. In a second step, unmatched words are converted into stems or synonyms and then matched. The TER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference. The CDER measure (Leusch et al., 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. Nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc. A framework for human error analysis has been proposed in (Vilar e</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Error Rate with Targeted Human Annotation. In Proc. of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA 06), pages 223–231, Boston, MA.</rawString>
</citation>
<citation valid="true">
<title>TC-STAR - technology and corpora for speech to speech translation. Integrated project TCSTAR (IST2002-FP6-506738) funded by the European Commission.</title>
<date>2005</date>
<location>http://www.tc-star.org/.</location>
<marker>2005</marker>
<rawString>2005. TC-STAR - technology and corpora for speech to speech translation. Integrated project TCSTAR (IST2002-FP6-506738) funded by the European Commission. http://www.tc-star.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Evgeny Matusov</author>
<author>Saˇsa Hasan</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical Machine Translation of European Parliamentary Speeches.</title>
<date>2005</date>
<booktitle>In Proc. MT Summit X,</booktitle>
<pages>259--266</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="2992" citStr="Vilar et al., 2005" startWordPosition="465" endWordPosition="468">he method can be easily extended to other types of linguistic information. In addition, two methods for error analysis using the WER and PER decompositons together with base forms are proposed: estimation of inflectional errors and distribution of missing words over POS classes. The translation corpus used for our error analysis is built in the framework of the TC-STAR project (tcs, 2005) and contains the transcriptions of the European Parliament Plenary Sessions (EPPS) in Spanish and English. The translation system used is the phrase-based statistical machine translation system described in (Vilar et al., 2005; Matusov et al., 2006). 2 Related Work Automatic evaluation measures for machine translation output are receiving more and more attention in the last years. The BLEU metric (Papineni et al., 2002) and the closely related NIST metric (Doddington, 2002) along with WER and PER Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, June 2007. c�2007 Association for Computational Linguistics have been widely used by many machine translation researchers. An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolin</context>
<context position="15238" citStr="Vilar et al., 2005" startWordPosition="2591" endWordPosition="2594">ses. For the example sentence pair presented in Table 1, from the WER errors in Table 2 and the PER errors in Table 3 the word “can” will be identified as missing. 5 Experimental settings 5.1 Translation System The machine translation system used in this work is based on the statistical aproach. It is built as a log-linear combination of seven different statistical models: phrase based models in both directions, IBM1 models at the phrase level in both directions, as well as target language model, phrase penalty and length penalty are used. A detailed description of the system can be found in (Vilar et al., 2005; Matusov et al., 2006). 5.2 Task and corpus The corpus analysed in this work is built in the framework of the TC-STAR project. The training corpus contains more than one million sentences and about 35 million running words of the European Parliament Plenary Sessions (EPPS) in Spanish and English. The test corpus contains about 1000 sentences and 28 000 running words. The OOV rates are low, about 0.5% of the running words for Spanish and 0.2% for English. The corpus statistics can be seen in Table 4. More details about the EPPS data can be found in (Vilar et al., 2005). TRAIN Spanish English S</context>
</contexts>
<marker>Vilar, Matusov, Hasan, Zens, Ney, 2005</marker>
<rawString>David Vilar, Evgeny Matusov, Saˇsa Hasan, Richard Zens, and Hermann Ney. 2005. Statistical Machine Translation of European Parliamentary Speeches. In Proc. MT Summit X, pages 259–266, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error Analysis of Statistical Machine Translation Output.</title>
<date>2006</date>
<booktitle>In Proc. of the Fifth Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>697--702</pages>
<location>Genoa, Italy,</location>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error Analysis of Statistical Machine Translation Output. In Proc. of the Fifth Int. Conf. on Language Resources and Evaluation (LREC), pages 697–702, Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atro Voutilainen</author>
</authors>
<title>ENGCG -Constraint Grammar Parser of English.</title>
<date>1995</date>
<note>http://www2.lingsoft.fi/doc/engcg/intro/.</note>
<contexts>
<context position="16456" citStr="Voutilainen, 1995" startWordPosition="2801" endWordPosition="2803">entences 1 167 627 Running words 35 320 646 33 945 468 Vocabulary 159 080 110 636 TEST Sentences 894 1117 Running words 28 591 28 492 OOVs 0.52% 0.25% Table 4: Statistics of the training and test corpora of the TC-STAR EPPS Spanish-English task. Test corpus is provided with two references. 6 Error analysis The translation is performed in both directions (Spanish to English and English to Spanish) and the error analysis is done on both the English and the Spanish output. Morpho-syntactic annotation of the English references and hypotheses is performed using the constraint grammar parser ENGCG (Voutilainen, 1995), and the Spanish texts are annotated using the FreeLing analyser (Carreras et al., 2004). In this way, all references and hypotheses are provided with POS tags and base forms. The decomposition of WER and FPER is done over the ten main POS classes: nouns (N), verbs (V), adjectives (A), adverbs (ADV), pronouns (PRON), determiners (DET), prepositions (PREP), conjunctions (CON), numerals (NUM) and punctuation marks (PUN). Inflectional error rates are also estimated for each POS class using FPER counts and base forms. Additionally, details about the verb tense and person inflections for both lang</context>
</contexts>
<marker>Voutilainen, 1995</marker>
<rawString>Atro Voutilainen. 1995. ENGCG -Constraint Grammar Parser of English. http://www2.lingsoft.fi/doc/engcg/intro/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>