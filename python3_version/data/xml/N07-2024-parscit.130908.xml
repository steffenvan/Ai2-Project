<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000521">
<title confidence="0.969242">
Detection of Non-native Sentences using Machine-translated Training Data
</title>
<author confidence="0.557809">
John Lee
</author>
<sectionHeader confidence="0.4530795" genericHeader="abstract">
Spoken Language Systems
MIT CSAIL
</sectionHeader>
<address confidence="0.583558">
Cambridge, MA 02139, USA
</address>
<email confidence="0.997957">
jsylee@csail.mit.edu
</email>
<author confidence="0.76319">
Ming Zhou, Xiaohua Liu
</author>
<affiliation confidence="0.6297745">
Natural Language Computing Group
Microsoft Research Asia
</affiliation>
<address confidence="0.550795">
Beijing, 100080, China
</address>
<email confidence="0.996517">
{mingzhou,xiaoliu}@microsoft.com
</email>
<sectionHeader confidence="0.998583" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.9998965">
Training statistical models to detect non-
native sentences requires a large corpus
of non-native writing samples, which is
often not readily available. This paper
examines the extent to which machine-
translated (MT) sentences can substitute
as training data.
Two tasks are examined. For the na-
tive vs non-native classification task, non-
native training data yields better perfor-
mance; for the ranking task, however,
models trained with a large, publicly avail-
able set of MT data perform as well as
those trained with non-native data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946733333333">
For non-native speakers writing in a foreign lan-
guage, feedback from native speakers is indispens-
able. While humans are likely to provide higher-
quality feedback, a computer system can offer bet-
ter availability and privacy. A system that can dis-
tinguish non-native (“ill-formed”) English sentences
from native (“well-formed”) ones would provide
valuable assistance in improving their writing.
Classifying a sentence into discrete categories can
be difficult: a sentence that seems fluent to one judge
might not be good enough to another. An alternative
is to rank sentences by their relative fluency. This
would be useful when a non-native speaker is un-
sure which one of several possible ways of writing a
sentence is the best.
</bodyText>
<page confidence="0.986732">
93
</page>
<bodyText confidence="0.999966933333333">
We therefore formulate two tasks on this problem.
The classification task gives one sentence to the sys-
tem, and asks whether it is native or non-native. The
ranking task submits sentences with the same in-
tended meaning, and asks which one is best.
To tackle these tasks, hand-crafting formal rules
would be daunting. Statistical methods, however,
require a large corpus of non-native writing sam-
ples, which can be difficult to compile. Since
machine-translated (MT) sentences are readily avail-
able in abundance, we wish to address the question
of whether they can substitute as training data.
The next section provides background on related
research. Sections 3 and 4 describe our experiments,
followed by conclusions and future directions.
</bodyText>
<sectionHeader confidence="0.998438" genericHeader="method">
2 Related Research
</sectionHeader>
<bodyText confidence="0.999637294117647">
Previous research has paid little attention to rank-
ing sentences by fluency. As for classification, one
line of research in MT evaluation is to evaluate the
fluency of an output sentence without its reference
translations, such as in (Corston-Oliver et al., 2001)
and (Gamon et al., 2005). Our task here is simi-
lar, but is applied on non-native sentences, arguably
more challenging than MT output.
Evaluation of non-native writing has encom-
passed both the document and sentence levels. At
the document level, automatic essay scorers, such
as (Burstein et al., 2004) and (Ishioka and Kameda,
2006), can provide holistic scores that correlate well
with those of human judges.
At the sentence level, which is the focus of this
paper, previous work follows two trends. Some re-
searchers explicitly focus on individual classes of er-
</bodyText>
<note confidence="0.39366">
Proceedings of NAACL HLT 2007, Companion Volume, pages 93–96,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99976125">
rors, e.g., mass vs count nouns in (Brockett et al.,
2006) and (Nagata et al., 2006). Others implicitly do
so with hand-crafted rules, via templates (Heidorn,
2000) or mal-rules in context-free grammars, such
as (Michaud et al., 2000) and (Bender et al., 2004).
Typically, however, non-native writing exhibits a
wide variety of errors, in grammar, style and word
collocations. In this research, we allow unrestricted
classes of errors1, and in this regard our goal is clos-
est to that of (Tomokiyo and Jones, 2001). How-
ever, they focus on non-native speech, and assume
the availability of non-native training data.
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="method">
3 Experimental Set-Up
</sectionHeader>
<subsectionHeader confidence="0.983468">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999884578947368">
Our data consists of pairs of English sentences, one
native and the other non-native, with the same “in-
tended meaning”. In our MT data (MT), both sen-
tences are translated, by machine or human, from
the same sentence in a foreign language. In our non-
native data (JLE), the non-native sentence has been
edited by a native speaker2. Table 1 gives some ex-
amples, and Table 2 presents some statistics.
MT (Multiple-Translation Chinese and Multiple-
Translation Arabic corpora) English MT out-
put, and human reference translations, of Chi-
nese and Arabic newspaper articles.
JLE (Japanese Learners of English Corpus) Tran-
scripts of Japanese examinees in the Standard
Speaking Test. False starts and disfluencies
were then cleaned up, and grammatical mis-
takes tagged (Izumi et al., 2003). The speaking
style is more formal than spontaneous English,
due to the examination setting.
</bodyText>
<subsectionHeader confidence="0.999156">
3.2 Machine Learning Framework
</subsectionHeader>
<bodyText confidence="0.999816428571428">
SVM-Light (Joachims, 1999), an implementation
of Support Vector Machines (SVM), is used for the
classification task.
For the ranking task, we utilize the ranking mode
of SVM-Light. In this mode, the SVM algorithm
is adapted for learning ranking functions, origi-
nally used for ranking web pages with respect to a
</bodyText>
<footnote confidence="0.68764725">
1Except spelling mistakes, which we consider to be a sepa-
rate problem that should be dealt with in a pre-processing step.
2The nature of the non-native data constrains the ranking to
two sentences at a time.
</footnote>
<bodyText confidence="0.9993212">
query (Joachims, 2002). In our context, given a set
of English sentences with similar semantic content,
say s1, ... , sr,,, and a ranking based on their fluency,
the learning algorithm estimates the weights w� to
satisfy the inequalities:
</bodyText>
<equation confidence="0.974036">
w -&apos;b(si) &gt; w - `b(sk) (1)
</equation>
<bodyText confidence="0.9996422">
where si is more fluent than sk, and where 4b maps
a sentence to a feature vector. This is in contrast to
standard SVMs, which learn a hyperplane boundary
between native and non-native sentences from the
inequalities:
</bodyText>
<equation confidence="0.989893">
yz(w - 4b(sz) + w0) − 1 &gt; 0 (2)
</equation>
<bodyText confidence="0.999703">
where yz = +1 are the labels. Linear kernels are
used in our experiments, and the regularization pa-
rameter is tuned on the development sets.
</bodyText>
<subsectionHeader confidence="0.850527">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999914">
The following features are extracted from each sen-
tence. The first two are real numbers; the rest are
indicator functions of the presence of the lexical
and/or syntactic properties in question.
Ent Entropy3 from a trigram language model
trained on 4.4 million English sentences with
the SRILM toolkit (Stolcke, 2002). The tri-
grams are intended to detect local mistakes.
Parse Parse score from Model 2 of the statisti-
cal parser (Collins, 1997), normalized by the
number of words. We hypothesize that non-
native sentences are more likely to receive
lower scores.
Deriv Parse tree derivations, i.e., from each parent
node to its children nodes, such as S —* NP VP.
Some non-native sentences have plausible N-
grams, but have derivations infrequently seen
in well-formed sentences, due to their unusual
syntactic structures.
DtNoun Head word of a base noun phrase, and its
determiner, e.g., (the, markets) from the human
non-native sentence in Table 1. The usage of ar-
ticles has been found to be the most frequent er-
ror class in the JLE corpus (Izumi et al., 2003).
</bodyText>
<footnote confidence="0.587614">
3Entropy H(x) is related to perplexity PP(x) by the equa-
H(x).
tion PP(x) = 2
</footnote>
<page confidence="0.975198">
94
</page>
<table confidence="0.9615705">
Type Sentence
Native Human New York and London stock markets went up
Non-native Human The stock markets in New York and London were increasing together
MT The same step of stock market of London of New York rises
</table>
<tableCaption confidence="0.9283435">
Table 1: Examples of sentences translated from a Chinese source sentence by a native speaker, by a non-
native speaker, and by a machine translation system.
</tableCaption>
<table confidence="0.999839555555556">
Data Set Corpus # sentences (for classification) # pairs (for
ranking)
total native non-native
MT train LDC{2002T01,2003T18,2006T04} 30075 17508 12567 91795
MT dev LDC2003T17 (Zaobao only) 1995 1328 667 2668
MT test LDC2003T17 (Xinhua only) 3255 2184 1071 4284
JLE train Japanese Learners of English 9848 4924 4924 4924
JLE dev 1000 500 500 500
JLE test 1000 500 500 500
</table>
<tableCaption confidence="0.995042">
Table 2: Data sets used in this paper.
</tableCaption>
<bodyText confidence="0.988527571428571">
Colloc An in-house dependency parser extracts
five types of word dependencies4: subject-verb,
verb-object, adjective-noun, verb-adverb and
preposition-object. For the human non-native
sentence in Table 1, the unusual subject-verb
collocation “market increase” is a useful clue
in this otherwise well-formed sentence.
</bodyText>
<sectionHeader confidence="0.998766" genericHeader="method">
4 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999907">
4.1 An Upper Bound
</subsectionHeader>
<bodyText confidence="0.999003846153846">
To gauge the performance upper bound, we first at-
tempt to classify and rank the MT test data, which
should be less challenging than non-native data. Af-
ter training the SVM on MT train, classification
accuracy on MT test improves with the addition
of each feature, culminating at 89.24% with all
five features. This result compares favorably with
the state-of-the-art5. Ranking performance reaches
96.73% with all five features.
We now turn our attention to non-native test data,
and contrast the performance on JLE test using
models trained by MT data (MT train), and by
non-native data (JLE train).
</bodyText>
<footnote confidence="0.912368571428572">
4Proper nouns and numbers are replaced with special sym-
bols. The words are further stemmed using Porter’s Stemmer.
5Direct comparison is impossible since the corpora were dif-
ferent. (Corston-Oliver et al., 2001) reports 82.89% accuracy
on English software manuals and online help documents, and
(Gamon et al., 2005) reports 77.59% on French technical docu-
ments.
</footnote>
<table confidence="0.999928">
Test Set: Train Set
JLE test
MT train JLE train
Ent+ 57.2 57.7
Parse (+) 48.6 (+) 70.6
(-) 65.8 (-) 44.8
+Deriv 58.4 64.7
(+) 54.6 (+)72.2
(-) 62.2 (-) 57.2
+DtNoun 59.0 66.4
(+) 57.6 (+) 72.8
(-) 60.4 (-) 60.0
+Colloc 58.6 65.9
(+) 54.2 (+) 72.6
(-) 63.2 (-) 59.2
</table>
<tableCaption confidence="0.999213">
Table 3: Classification accuracy on JLE test. (-)
</tableCaption>
<construct confidence="0.616633">
indicates accuracy on non-native sentences, and (+)
indicates accuracy on native sentences. The overall
accuracy is their average.
</construct>
<subsectionHeader confidence="0.990477">
4.2 Classification
</subsectionHeader>
<bodyText confidence="0.999914444444445">
As shown in Table 3, classification accuracy on JLE
test is higher with the JLE train set (66.4%)
than with the larger MT train set (59.0%). The
SVM trained on MT train consistently misclas-
sifies more native sentences than non-native ones.
One reason might be that speech transcripts have a
less formal style than written news sentences. Tran-
scripts of even good conversational English do not
always resemble sentences in the news domain.
</bodyText>
<subsectionHeader confidence="0.998229">
4.3 Ranking
</subsectionHeader>
<bodyText confidence="0.9991315">
In the ranking task, the relative performance be-
tween MT and non-native training data is reversed.
</bodyText>
<page confidence="0.994968">
95
</page>
<table confidence="0.999716285714286">
Test Set: Train Set
JLE test
MT train JLE train
Ent+Parse 72.8 71.4
+Deriv 73.4 73.6
+DtNoun 75.4 73.8
+Colloc 76.2 74.6
</table>
<tableCaption confidence="0.999859">
Table 4: Ranking accuracy on JLE test.
</tableCaption>
<bodyText confidence="0.999907111111111">
As shown in Table 4, models trained on MT train
yield higher ranking accuracy (76.2%) than those
trained on JLE train (74.6%). This indicates that
MT training data can generalize well enough to per-
form better than a non-native training corpus of size
up to 10000.
The contrast between the classification and rank-
ing results suggests that train/test data mismatch is
less harmful for the latter task. Weights trained on
the classification inequalities in (2) and on the rank-
ing inequalities in (1) both try to separate native and
MT sentences maximally. The absolute boundary
learned in (2) is inherently specific to the nature
of the training sentences, as we have seen in §4.2.
In comparison, the relative scores learned from (1)
have a better chance to carry over to other domains,
as long as some gap still exists between the scores
of the native and non-native sentences.
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="conclusions">
5 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999984894736842">
We explored two tasks in sentence-level fluency
evaluation: ranking and classifying native vs. non-
native sentences. In an SVM framework, we exam-
ined how well MT data can replace non-native data
in training.
For the classification task, training with MT data
is less effective than with non-native data. How-
ever, for the ranking task, models trained on pub-
licly available MT data generalize well, performing
as well as those trained with a non-native corpus of
size 10000.
In the future, we would like to search for more
salient features through a careful study of non-native
errors, using error-tagged corpora such as (Izumi et
al., 2003). We also plan to explore techniques for
combining large MT training corpora and smaller
non-native training corpora. Our ultimate goal is to
identify the errors in the non-native sentences and
propose corrections.
</bodyText>
<sectionHeader confidence="0.989632" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774574468085">
E. Bender, D. Flickinger, S. Oepen, A. Walsh, and T.
Baldwin. 2004. Arboretum: Using a Precision Gram-
mar for Grammar Checking in CALL. Proc. In-
STIL/ICALL Symposium on Computer Assisted Learn-
ing.
C. Brockett, W. Dolan, and M. Gamon. 2006. Correct-
ing ESL Errors using Phrasal SMT Techniques. Proc.
ACL.
J. Burstein, M. Chodorow and C. Leacock. 2004. Auto-
mated Essay Evaluation: The Criterion online Writing
Service. AI Magazine, 25(3):27–36.
M. Collins. 1997. Three Generative, Lexicalised Models
for Statistical Parsing. Proc. ACL.
S. Corston-Oliver, M. Gamon and C. Brockett. 2001. A
Machine Learning Approach to the Automatic Evalu-
ation of Machine Translation. Proc. ACL.
M. Gamon, A. Aue, and M. Smets. 2005. Sentence-
Level MT Evaluation without Reference Translations:
Beyond Language Modeling. Proc. EAMT.
G. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert
Dale, Hermann Moisi and Harold Somers (ed.). Mar-
cel Dekker, Inc.
T. Ishioka and M. Kameda. 2006. Automated Japanese
Essay Scoring System based on Articles Written by
Experts. Proc. ACL.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H.
Isahara. 2003. Automatic Error Detection in the
Japanese Learners’ English Spoken Data. Proc. ACL.
T. Joachims. 1999. Making Large-Scale SVM Learning
Practical. Advances in Kernel Methods - Support Vec-
tor Learning. B. Sch¨olkopf, C. Burges and A. Smola
(ed.), MIT-Press.
T. Joachims. 2002. Optimizing Search Engines using
Clickthrough Data. Proc. SIGKDD.
L. Michaud, K. McCoy and C. Pennington. 2000. An In-
telligent Tutoring System for Deaf Learners of Written
English. Proc. 4th International ACM Conference on
Assistive Technologies.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A
Feedback-Augmented Method for Detecting Errors in
the Writing of Learners of English. Proc. ACL.
A. Stolcke. 2002. SRILM — An Extensible Language
Modeling Toolkit Proc. ICSLP.
L. Tomokiyo and R. Jones. 2001. You’re not from ’round
here, are you? Naive Bayes Detection of Non-native
Utterance Text. Proc. NAACL.
</reference>
<page confidence="0.998462">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.432581">
<title confidence="0.984297">Detection of Non-native Sentences using Machine-translated Training Data</title>
<author confidence="0.666928">John</author>
<affiliation confidence="0.864863">Spoken Language MIT</affiliation>
<address confidence="0.999397">Cambridge, MA 02139,</address>
<email confidence="0.999817">jsylee@csail.mit.edu</email>
<author confidence="0.982387">Ming Zhou</author>
<author confidence="0.982387">Xiaohua</author>
<affiliation confidence="0.9581555">Natural Language Computing Microsoft Research</affiliation>
<address confidence="0.996667">Beijing, 100080,</address>
<abstract confidence="0.999399533333333">Training statistical models to detect nonnative sentences requires a large corpus of non-native writing samples, which is often not readily available. This paper examines the extent to which machinetranslated (MT) sentences can substitute as training data. Two tasks are examined. For the navs non-native nonnative training data yields better perforfor the however, models trained with a large, publicly available set of MT data perform as well as those trained with non-native data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Bender</author>
<author>D Flickinger</author>
<author>S Oepen</author>
<author>A Walsh</author>
<author>T Baldwin</author>
</authors>
<title>Arboretum: Using a Precision Grammar for Grammar Checking in CALL.</title>
<date>2004</date>
<booktitle>Proc. InSTIL/ICALL Symposium on Computer Assisted Learning.</booktitle>
<contexts>
<context position="3567" citStr="Bender et al., 2004" startWordPosition="548" endWordPosition="551">ovide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1, and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 Experimental Set-Up 3.1 Data Our data consists of pairs of English sentences, one native and the other non-native, with the same “intended meaning”. In our MT data (MT), both sentences are translated, by machine or human, from the same sentence</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Walsh, Baldwin, 2004</marker>
<rawString>E. Bender, D. Flickinger, S. Oepen, A. Walsh, and T. Baldwin. 2004. Arboretum: Using a Precision Grammar for Grammar Checking in CALL. Proc. InSTIL/ICALL Symposium on Computer Assisted Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brockett</author>
<author>W Dolan</author>
<author>M Gamon</author>
</authors>
<date>2006</date>
<booktitle>Correcting ESL Errors using Phrasal SMT Techniques. Proc. ACL.</booktitle>
<contexts>
<context position="3365" citStr="Brockett et al., 2006" startWordPosition="516" endWordPosition="519">aluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1, and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 Experimental Set-Up 3.1 Data Our data cons</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>C. Brockett, W. Dolan, and M. Gamon. 2006. Correcting ESL Errors using Phrasal SMT Techniques. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burstein</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Automated Essay Evaluation: The Criterion online Writing Service.</title>
<date>2004</date>
<journal>AI Magazine,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="2908" citStr="Burstein et al., 2004" startWordPosition="444" endWordPosition="447">by conclusions and future directions. 2 Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammar</context>
</contexts>
<marker>Burstein, Chodorow, Leacock, 2004</marker>
<rawString>J. Burstein, M. Chodorow and C. Leacock. 2004. Automated Essay Evaluation: The Criterion online Writing Service. AI Magazine, 25(3):27–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing.</title>
<date>1997</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="6485" citStr="Collins, 1997" startWordPosition="1033" endWordPosition="1034">+ w0) − 1 &gt; 0 (2) where yz = +1 are the labels. Linear kernels are used in our experiments, and the regularization parameter is tuned on the development sets. 3.3 Features The following features are extracted from each sentence. The first two are real numbers; the rest are indicator functions of the presence of the lexical and/or syntactic properties in question. Ent Entropy3 from a trigram language model trained on 4.4 million English sentences with the SRILM toolkit (Stolcke, 2002). The trigrams are intended to detect local mistakes. Parse Parse score from Model 2 of the statistical parser (Collins, 1997), normalized by the number of words. We hypothesize that nonnative sentences are more likely to receive lower scores. Deriv Parse tree derivations, i.e., from each parent node to its children nodes, such as S —* NP VP. Some non-native sentences have plausible Ngrams, but have derivations infrequently seen in well-formed sentences, due to their unusual syntactic structures. DtNoun Head word of a base noun phrase, and its determiner, e.g., (the, markets) from the human non-native sentence in Table 1. The usage of articles has been found to be the most frequent error class in the JLE corpus (Izum</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three Generative, Lexicalised Models for Statistical Parsing. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>M Gamon</author>
<author>C Brockett</author>
</authors>
<title>A Machine Learning Approach to the Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="2606" citStr="Corston-Oliver et al., 2001" startWordPosition="395" endWordPosition="398">ples, which can be difficult to compile. Since machine-translated (MT) sentences are readily available in abundance, we wish to address the question of whether they can substitute as training data. The next section provides background on related research. Sections 3 and 4 describe our experiments, followed by conclusions and future directions. 2 Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Compan</context>
<context position="9118" citStr="Corston-Oliver et al., 2001" startWordPosition="1461" endWordPosition="1464">, classification accuracy on MT test improves with the addition of each feature, culminating at 89.24% with all five features. This result compares favorably with the state-of-the-art5. Ranking performance reaches 96.73% with all five features. We now turn our attention to non-native test data, and contrast the performance on JLE test using models trained by MT data (MT train), and by non-native data (JLE train). 4Proper nouns and numbers are replaced with special symbols. The words are further stemmed using Porter’s Stemmer. 5Direct comparison is impossible since the corpora were different. (Corston-Oliver et al., 2001) reports 82.89% accuracy on English software manuals and online help documents, and (Gamon et al., 2005) reports 77.59% on French technical documents. Test Set: Train Set JLE test MT train JLE train Ent+ 57.2 57.7 Parse (+) 48.6 (+) 70.6 (-) 65.8 (-) 44.8 +Deriv 58.4 64.7 (+) 54.6 (+)72.2 (-) 62.2 (-) 57.2 +DtNoun 59.0 66.4 (+) 57.6 (+) 72.8 (-) 60.4 (-) 60.0 +Colloc 58.6 65.9 (+) 54.2 (+) 72.6 (-) 63.2 (-) 59.2 Table 3: Classification accuracy on JLE test. (-) indicates accuracy on non-native sentences, and (+) indicates accuracy on native sentences. The overall accuracy is their average. 4.2</context>
</contexts>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>S. Corston-Oliver, M. Gamon and C. Brockett. 2001. A Machine Learning Approach to the Automatic Evaluation of Machine Translation. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>A Aue</author>
<author>M Smets</author>
</authors>
<title>SentenceLevel MT Evaluation without Reference Translations: Beyond Language Modeling.</title>
<date>2005</date>
<booktitle>Proc. EAMT.</booktitle>
<contexts>
<context position="2631" citStr="Gamon et al., 2005" startWordPosition="400" endWordPosition="403">mpile. Since machine-translated (MT) sentences are readily available in abundance, we wish to address the question of whether they can substitute as training data. The next section provides background on related research. Sections 3 and 4 describe our experiments, followed by conclusions and future directions. 2 Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, </context>
<context position="9222" citStr="Gamon et al., 2005" startWordPosition="1477" endWordPosition="1480">ive features. This result compares favorably with the state-of-the-art5. Ranking performance reaches 96.73% with all five features. We now turn our attention to non-native test data, and contrast the performance on JLE test using models trained by MT data (MT train), and by non-native data (JLE train). 4Proper nouns and numbers are replaced with special symbols. The words are further stemmed using Porter’s Stemmer. 5Direct comparison is impossible since the corpora were different. (Corston-Oliver et al., 2001) reports 82.89% accuracy on English software manuals and online help documents, and (Gamon et al., 2005) reports 77.59% on French technical documents. Test Set: Train Set JLE test MT train JLE train Ent+ 57.2 57.7 Parse (+) 48.6 (+) 70.6 (-) 65.8 (-) 44.8 +Deriv 58.4 64.7 (+) 54.6 (+)72.2 (-) 62.2 (-) 57.2 +DtNoun 59.0 66.4 (+) 57.6 (+) 72.8 (-) 60.4 (-) 60.0 +Colloc 58.6 65.9 (+) 54.2 (+) 72.6 (-) 63.2 (-) 59.2 Table 3: Classification accuracy on JLE test. (-) indicates accuracy on non-native sentences, and (+) indicates accuracy on native sentences. The overall accuracy is their average. 4.2 Classification As shown in Table 3, classification accuracy on JLE test is higher with the JLE train se</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>M. Gamon, A. Aue, and M. Smets. 2005. SentenceLevel MT Evaluation without Reference Translations: Beyond Language Modeling. Proc. EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heidorn</author>
</authors>
<title>Intelligent Writing Assistance. Handbook of Natural Language Processing.</title>
<date>2000</date>
<editor>Robert Dale, Hermann Moisi and Harold Somers (ed.).</editor>
<publisher>Marcel Dekker, Inc.</publisher>
<contexts>
<context position="3471" citStr="Heidorn, 2000" startWordPosition="534" endWordPosition="535">atic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1, and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 Experimental Set-Up 3.1 Data Our data consists of pairs of English sentences, one native and the other non-native, with the same “intended meaning”.</context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>G. Heidorn. 2000. Intelligent Writing Assistance. Handbook of Natural Language Processing. Robert Dale, Hermann Moisi and Harold Somers (ed.). Marcel Dekker, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ishioka</author>
<author>M Kameda</author>
</authors>
<title>Automated Japanese Essay Scoring System based on Articles Written by Experts.</title>
<date>2006</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="2939" citStr="Ishioka and Kameda, 2006" startWordPosition="449" endWordPosition="452">rections. 2 Related Research Previous research has paid little attention to ranking sentences by fluency. As for classification, one line of research in MT evaluation is to evaluate the fluency of an output sentence without its reference translations, such as in (Corston-Oliver et al., 2001) and (Gamon et al., 2005). Our task here is similar, but is applied on non-native sentences, arguably more challenging than MT output. Evaluation of non-native writing has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 200</context>
</contexts>
<marker>Ishioka, Kameda, 2006</marker>
<rawString>T. Ishioka and M. Kameda. 2006. Automated Japanese Essay Scoring System based on Articles Written by Experts. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<date>2003</date>
<booktitle>Automatic Error Detection in the Japanese Learners’ English Spoken Data. Proc. ACL.</booktitle>
<contexts>
<context position="4728" citStr="Izumi et al., 2003" startWordPosition="735" endWordPosition="738">re translated, by machine or human, from the same sentence in a foreign language. In our nonnative data (JLE), the non-native sentence has been edited by a native speaker2. Table 1 gives some examples, and Table 2 presents some statistics. MT (Multiple-Translation Chinese and MultipleTranslation Arabic corpora) English MT output, and human reference translations, of Chinese and Arabic newspaper articles. JLE (Japanese Learners of English Corpus) Transcripts of Japanese examinees in the Standard Speaking Test. False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al., 2003). The speaking style is more formal than spontaneous English, due to the examination setting. 3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task. For the ranking task, we utilize the ranking mode of SVM-Light. In this mode, the SVM algorithm is adapted for learning ranking functions, originally used for ranking web pages with respect to a 1Except spelling mistakes, which we consider to be a separate problem that should be dealt with in a pre-processing step. 2The nature of the non-native data constr</context>
<context position="7100" citStr="Izumi et al., 2003" startWordPosition="1136" endWordPosition="1139">997), normalized by the number of words. We hypothesize that nonnative sentences are more likely to receive lower scores. Deriv Parse tree derivations, i.e., from each parent node to its children nodes, such as S —* NP VP. Some non-native sentences have plausible Ngrams, but have derivations infrequently seen in well-formed sentences, due to their unusual syntactic structures. DtNoun Head word of a base noun phrase, and its determiner, e.g., (the, markets) from the human non-native sentence in Table 1. The usage of articles has been found to be the most frequent error class in the JLE corpus (Izumi et al., 2003). 3Entropy H(x) is related to perplexity PP(x) by the equaH(x). tion PP(x) = 2 94 Type Sentence Native Human New York and London stock markets went up Non-native Human The stock markets in New York and London were increasing together MT The same step of stock market of London of New York rises Table 1: Examples of sentences translated from a Chinese source sentence by a native speaker, by a nonnative speaker, and by a machine translation system. Data Set Corpus # sentences (for classification) # pairs (for ranking) total native non-native MT train LDC{2002T01,2003T18,2006T04} 30075 17508 12567</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic Error Detection in the Japanese Learners’ English Spoken Data. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<date>1999</date>
<booktitle>Making Large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning.</booktitle>
<pages>MIT-Press.</pages>
<editor>B. Sch¨olkopf, C. Burges and A. Smola (ed.),</editor>
<contexts>
<context position="4879" citStr="Joachims, 1999" startWordPosition="758" endWordPosition="759"> native speaker2. Table 1 gives some examples, and Table 2 presents some statistics. MT (Multiple-Translation Chinese and MultipleTranslation Arabic corpora) English MT output, and human reference translations, of Chinese and Arabic newspaper articles. JLE (Japanese Learners of English Corpus) Transcripts of Japanese examinees in the Standard Speaking Test. False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al., 2003). The speaking style is more formal than spontaneous English, due to the examination setting. 3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task. For the ranking task, we utilize the ranking mode of SVM-Light. In this mode, the SVM algorithm is adapted for learning ranking functions, originally used for ranking web pages with respect to a 1Except spelling mistakes, which we consider to be a separate problem that should be dealt with in a pre-processing step. 2The nature of the non-native data constrains the ranking to two sentences at a time. query (Joachims, 2002). In our context, given a set of English sentences with similar semantic content, sa</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making Large-Scale SVM Learning Practical. Advances in Kernel Methods - Support Vector Learning. B. Sch¨olkopf, C. Burges and A. Smola (ed.), MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing Search Engines using Clickthrough Data.</title>
<date>2002</date>
<booktitle>Proc. SIGKDD.</booktitle>
<contexts>
<context position="5395" citStr="Joachims, 2002" startWordPosition="844" endWordPosition="845"> English, due to the examination setting. 3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task. For the ranking task, we utilize the ranking mode of SVM-Light. In this mode, the SVM algorithm is adapted for learning ranking functions, originally used for ranking web pages with respect to a 1Except spelling mistakes, which we consider to be a separate problem that should be dealt with in a pre-processing step. 2The nature of the non-native data constrains the ranking to two sentences at a time. query (Joachims, 2002). In our context, given a set of English sentences with similar semantic content, say s1, ... , sr,,, and a ranking based on their fluency, the learning algorithm estimates the weights w� to satisfy the inequalities: w -&apos;b(si) &gt; w - `b(sk) (1) where si is more fluent than sk, and where 4b maps a sentence to a feature vector. This is in contrast to standard SVMs, which learn a hyperplane boundary between native and non-native sentences from the inequalities: yz(w - 4b(sz) + w0) − 1 &gt; 0 (2) where yz = +1 are the labels. Linear kernels are used in our experiments, and the regularization parameter</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Optimizing Search Engines using Clickthrough Data. Proc. SIGKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Michaud</author>
<author>K McCoy</author>
<author>C Pennington</author>
</authors>
<title>An Intelligent Tutoring System for Deaf Learners of Written English.</title>
<date>2000</date>
<booktitle>Proc. 4th International ACM Conference on Assistive Technologies.</booktitle>
<contexts>
<context position="3541" citStr="Michaud et al., 2000" startWordPosition="543" endWordPosition="546">a and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1, and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 Experimental Set-Up 3.1 Data Our data consists of pairs of English sentences, one native and the other non-native, with the same “intended meaning”. In our MT data (MT), both sentences are translated, by machine or hum</context>
</contexts>
<marker>Michaud, McCoy, Pennington, 2000</marker>
<rawString>L. Michaud, K. McCoy and C. Pennington. 2000. An Intelligent Tutoring System for Deaf Learners of Written English. Proc. 4th International ACM Conference on Assistive Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>A Kawai</author>
<author>K Morihiro</author>
<author>N Isu</author>
</authors>
<title>A Feedback-Augmented Method for Detecting Errors in the Writing of Learners of English.</title>
<date>2006</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="3391" citStr="Nagata et al., 2006" startWordPosition="521" endWordPosition="524">ng has encompassed both the document and sentence levels. At the document level, automatic essay scorers, such as (Burstein et al., 2004) and (Ishioka and Kameda, 2006), can provide holistic scores that correlate well with those of human judges. At the sentence level, which is the focus of this paper, previous work follows two trends. Some researchers explicitly focus on individual classes of erProceedings of NAACL HLT 2007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1, and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 Experimental Set-Up 3.1 Data Our data consists of pairs of English s</context>
</contexts>
<marker>Nagata, Kawai, Morihiro, Isu, 2006</marker>
<rawString>R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A Feedback-Augmented Method for Detecting Errors in the Writing of Learners of English. Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM — An Extensible Language Modeling Toolkit</title>
<date>2002</date>
<booktitle>Proc. ICSLP.</booktitle>
<contexts>
<context position="6359" citStr="Stolcke, 2002" startWordPosition="1011" endWordPosition="1012">standard SVMs, which learn a hyperplane boundary between native and non-native sentences from the inequalities: yz(w - 4b(sz) + w0) − 1 &gt; 0 (2) where yz = +1 are the labels. Linear kernels are used in our experiments, and the regularization parameter is tuned on the development sets. 3.3 Features The following features are extracted from each sentence. The first two are real numbers; the rest are indicator functions of the presence of the lexical and/or syntactic properties in question. Ent Entropy3 from a trigram language model trained on 4.4 million English sentences with the SRILM toolkit (Stolcke, 2002). The trigrams are intended to detect local mistakes. Parse Parse score from Model 2 of the statistical parser (Collins, 1997), normalized by the number of words. We hypothesize that nonnative sentences are more likely to receive lower scores. Deriv Parse tree derivations, i.e., from each parent node to its children nodes, such as S —* NP VP. Some non-native sentences have plausible Ngrams, but have derivations infrequently seen in well-formed sentences, due to their unusual syntactic structures. DtNoun Head word of a base noun phrase, and its determiner, e.g., (the, markets) from the human no</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM — An Extensible Language Modeling Toolkit Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tomokiyo</author>
<author>R Jones</author>
</authors>
<title>You’re not from ’round here, are you? Naive Bayes Detection of Non-native Utterance Text.</title>
<date>2001</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="3820" citStr="Tomokiyo and Jones, 2001" startWordPosition="589" endWordPosition="592">007, Companion Volume, pages 93–96, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics rors, e.g., mass vs count nouns in (Brockett et al., 2006) and (Nagata et al., 2006). Others implicitly do so with hand-crafted rules, via templates (Heidorn, 2000) or mal-rules in context-free grammars, such as (Michaud et al., 2000) and (Bender et al., 2004). Typically, however, non-native writing exhibits a wide variety of errors, in grammar, style and word collocations. In this research, we allow unrestricted classes of errors1, and in this regard our goal is closest to that of (Tomokiyo and Jones, 2001). However, they focus on non-native speech, and assume the availability of non-native training data. 3 Experimental Set-Up 3.1 Data Our data consists of pairs of English sentences, one native and the other non-native, with the same “intended meaning”. In our MT data (MT), both sentences are translated, by machine or human, from the same sentence in a foreign language. In our nonnative data (JLE), the non-native sentence has been edited by a native speaker2. Table 1 gives some examples, and Table 2 presents some statistics. MT (Multiple-Translation Chinese and MultipleTranslation Arabic corpora</context>
</contexts>
<marker>Tomokiyo, Jones, 2001</marker>
<rawString>L. Tomokiyo and R. Jones. 2001. You’re not from ’round here, are you? Naive Bayes Detection of Non-native Utterance Text. Proc. NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>