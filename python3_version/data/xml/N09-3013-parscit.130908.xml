<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013138">
<title confidence="0.998306">
Towards Building a Competitive Opinion Summarization System:
Challenges and Keys
</title>
<author confidence="0.999162">
Elena Lloret*, Alexandra Balahur, Manuel Palomar and Andrés Montoyo
</author>
<affiliation confidence="0.942341333333333">
Department of Software and Computing Systems
University of Alicante
Apartado de Correos 99, E-03080, Alicante, Spain
</affiliation>
<email confidence="0.910843">
{elloret, abalahur, mpalomar, montoyo}@dlsi.ua.es
</email>
<sectionHeader confidence="0.994084" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820363636364">
This paper presents an overview of our participation in
the TAC 2008 Opinion Pilot Summarization task, as
well as the proposed and evaluated post-competition
improvements. We first describe our opinion
summarization system and the results obtained. Further
on, we identify the system’s weak points and suggest
several improvements, focused both on information
content, as well as linguistic and readability aspects. We
obtain encouraging results, especially as far as F-
measure is concerned, outperforming the competition
results by approximately 80%.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9973622">
The Opinion Summarization Pilot (OSP) task
within the TAC 2008 competition consisted in
generating summaries from answers to opinion
questions retrieved from blogs (the Blog061
collection). The questions were organized around
25 targets – persons, events, organizations etc.
Additionally, a set of text snippets that contained
the answers to the questions were provided by the
organizers, their use being optional. An example of
target, question and provided snippet is given in
</bodyText>
<figureCaption confidence="0.588561">
Figure 1.
Target : George Clooney
Question: Why do people like George Clooney?
Snippet 1: 1050 BLOG06-20060125-015-
0025581509 he is a great actor
Figure 1. Examples of target, question and snippet
</figureCaption>
<footnote confidence="0.99689875">
*Elena Lloret is funded by the FPI program (BES-2007-
16268) from the Spanish Ministry of Science and Innovation,
under the project TEXT-MESS (TIN-2006-15265)
1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html
</footnote>
<bodyText confidence="0.999548133333333">
The techniques employed by the participants were
mainly based on the already existing
summarization systems. While most participants
added new features (sentiment, pos/neg sentiment,
pos/neg opinion) to account for the presence of
positive opinions or negative ones - CLASSY
(Conroy and Schlessinger, 2008); CCNU (He et
al.,2008); LIPN (Bossard et al., 2008); IIITSum08
(Varma et al., 2008) -, efficient methods were
proposed focusing on the retrieval and filtering
stage, based on polarity – DLSIUAES (Balahur et
al., 2008) - or on separating information rich
clauses - italica (Cruz et al., 2008). In general,
previous work in opinion mining includes
document level sentiment classification using
supervised (Chaovalit and Zhou, 2005) and
unsupervised methods (Turney, 2002), machine
learning techniques and sentiment classification
considering rating scales (Pang, Lee and
Vaithyanathan, 2002), and scoring of features
(Dave, Lawrence and Pennock, 2003). Other
research has been conducted in analysing
sentiment at a sentence level using bootstrapping
techniques (Riloff and Wiebe, 2003), finding
strength of opinions (Wilson, Wiebe and Hwa,
2004), summing up orientations of opinion words
in a sentence (Kim and Hovy, 2004), and
identifying opinion holders (Stoyanov and Cardie,
2006). Finally, fine grained, feature-based opinion
summarization is defined in (Hu and Liu, 2004).
</bodyText>
<sectionHeader confidence="0.986859" genericHeader="method">
2 Opinion Summarization System
</sectionHeader>
<bodyText confidence="0.9999402">
In order to tackle the OSP task, we considered the
use of two different methods for opinion mining
and summarization, differing mainly with respect
to the use of the optional text snippets provided.
Our first approach (the Snippet-driven Approach)
</bodyText>
<page confidence="0.996125">
72
</page>
<note confidence="0.8154615">
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72–77,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.9985892">
used these snippets, whereas the second one (Blog-
driven Approach) found the answers directly in the
corresponding blogs. A general overview of the
system’s architecture is shown in Figure 2, where
three main parts can be distinguished: the question
processing stage, the snippets processing stage
(only carried out for the first approach), and the
final summary generation module. Next, the main
steps involved in each process will be explained in
more detail.
</bodyText>
<figureCaption confidence="0.99913">
Figure 2. System architecture
</figureCaption>
<bodyText confidence="0.999827964285714">
The first step was to determine the polarity of each
question, extract the keywords from each of them
and finally, build some patterns of reformulation.
The latter were defined in order to give the final
summary an abstract nature, rather than a simple
joining of sentences. The polarity of the question
was determined using a set of created patterns,
whose goal was to extract for further classification
the nouns, verbs, adverbs or adjectives indicating
some kind of polarity (positive or negative). These
extracted words, together with their determiners,
were classified using the emotions lists in
WordNet Affect (Strapparava and Valitutti, 2005),
jointly with the emotions lists of attitudes, triggers
resource (Balahur and Montoyo, 2008 [1]), four
created lists of attitudes, expressing criticism,
support, admiration and rejection and two
categories for value (good and bad), taking for the
opinion mining systems in (Balahur and Montoyo,
2008 [2]). Moreover, the focus of each question
was automatically extracted using the Freeling2
Named Entity Recognizer module. This
information was used to determine whether or not
all the questions within the same topic had the
same focus, as well as be able to decide later on
which text snippet belonged to which question.
Regarding the given text snippets, we also
computed their polarity and their focus. The
</bodyText>
<footnote confidence="0.715397">
2 http://garraf.epsevg.upc.es/freeling/
</footnote>
<bodyText confidence="0.999895956521739">
polarity was calculated as a vector similarity
between the snippets and vectors constructed from
the list of sentences contained in the ISEAR corpus
(Scherer and Wallbot, 1997), WordNet Affect
emotion lists of anger, sadness, disgust and joy and
the emotion triggers resource, using Pedersen&apos;s
Text Similarity Package.3
Concerning the blogs, our opinion mining and
summarization system is focused only on plain
text; therefore, as pre processing stage, we
removed all unnecessary tags and irrelevant
information, such as links, images etc. Further on,
we split the remaining text into individual
sentences. A matching between blogs&apos; sentences
and text snippets was performed so that a
preliminary set of potential meaningful sentences
was recorded for further processing. To achieve
this, snippets not literally contained in the blogs
were tokenized and stemmed using Porter&apos;s
Stemmer,4 and stop words were removed in order
to find the most similar possible sentence
associated with it. Subsequently, by means of the
same Pedersen Text Similarity Package as for
computing the snippets&apos; polarity, we computed the
similarity between the given snippets and this
created set of potential sentences. We extracted the
complete blog sentences to which each snippet was
related. Further on, we extracted the focus for each
blog phrase sentence as well. Then, we filtered
redundant sentences using a naïve similarity based
approach. Once we obtained the possible answers,
we used Minipar5 to filter out incomplete
sentences.
Having computed the polarity for the questions and
snippets, and set out the final set of sentences to
produce the summary, we bound each sentence to
its corresponding question, and we grouped all
sentences which were related to the same question
together, so that we could generate the language
for this group, according to the patterns of
reformulation previously mentioned. Finally, the
speech style was changed to an impersonal one, in
order to avoid directly expressed opinion
sentences. A POS-tagger tool (TreeTagger6) was
used to identify third person verbs and change
them to a neutral style. A set of rules to identify
</bodyText>
<footnote confidence="0.99990475">
3http://www.d.umn.edu/—tpederse/text-similarity.html
4http://tartarus.org/—martin/PorterStemmer/
5http://www.cs.ualberta.ca/—lindek/minipar.htm
6http://www.ims.uni-tuttgart.de/projekte/corplex/TreeTagger/
</footnote>
<page confidence="0.998951">
73
</page>
<bodyText confidence="0.999831">
pronouns was created, and they were also changed
to the more general pronoun “they” and its
corresponding forms, to avoid personal opinions.
</bodyText>
<sectionHeader confidence="0.996279" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.996987769230769">
Table 1 shows the final results obtained by our
approaches in the TAC 2008 Opinion Pilot (the
rank among the 36 participating systems is shown
in brackets for each evaluation measure). Both of
our approaches were totally automatic, and the
only difference between them was the use of the
given snippets in the first one (A1) and not in the
second (A2). The column numbers stand for the
following average scores: summarizerID (1);
pyramid F-score (Beta=1) (2), grammaticality (3);
non-redundancy (4); structure/coherence
(including focus and referential clarity) (5); overall
fluency/readability (6); overall responsiveness (7).
</bodyText>
<table confidence="0.613236">
1 2 3 4 5 6 7
A1 0.357 4.727 5.364 3.409 3.636 5.045
(7) (8) (28) (4) (16) (5)
A2 0.155 3.545 4.364 3.091 2.636 2.227
(23) (36) (36) (13) (36) (28)
</table>
<tableCaption confidence="0.995045">
Table 1. Evaluation results
</tableCaption>
<bodyText confidence="0.999979533333333">
As it can be noticed from Table 1, our system
performed well regarding F-measure, the first run
being classified 7th among the 36 evaluated. As far
as the structure and coherence are concerned, the
results were also good, placing the first approach
in the fourth. Also worth mentioning is the good
performance obtained regarding the overall
responsiveness, where A1 ranked 5th. Generally
speaking, the results for A1 showed well-balanced
among all the criteria evaluated, except for non
redundancy and grammaticality. For the second
approach, results were not as good, due to the
difficulty in selecting the appropriate opinion blog
sentence by only taking into account the keywords
of the question.
</bodyText>
<sectionHeader confidence="0.630386" genericHeader="method">
4 Post-competition tests, experiments
and improvements
</sectionHeader>
<bodyText confidence="0.986825571428571">
When an exhaustive examination of the nuggets
used for evaluating the summaries was done, we
found some problems that are worth mentioning.
a) Some nuggets with high score did not exist in
the snippet list (e.g. “When buying from
CARMAX, got a better than blue book trade-in
on old car” (0.9)).
</bodyText>
<listItem confidence="0.985205470588235">
b) Some nuggets for the same target express the
same idea, despite their not being identical
(e.g. “NAFTA needs to be renegotiated to
protect Canadian sovereignty” and “Green
Party: Renegotiate NAFTA to protect
Canadian Sovereignty”).
c) The meaning of one nugget can be deduced
from another&apos;s (e.g. “reasonably healthy food”
and “sandwiches are healthy”).
d) Some nuggets are not very clear in meaning
(e.g. “hot”, “fun”).
e) A snippet can be covered by several nuggets
(e.g. both nuggets “it is an honest book” and
“it is a great book” correspond to the same
snippet “It was such a great book- honest and
hard to read (content not language
difficulty)”).
</listItem>
<bodyText confidence="0.999793714285714">
On the other hand, regarding the use of the
optional snippets, the main problem to address is to
remove redundancy, because many of them are
repeated for the same target, and we have to
determine which snippet represents better the idea
for the final summary, in order to avoid noisy
irrelevant information.
</bodyText>
<subsectionHeader confidence="0.999455">
4.1 Measuring the Performance of a
Generic Summarization System
</subsectionHeader>
<bodyText confidence="0.99994045">
Several participants in the TAC 2008 edition
performed the OSP task by using generic
summarization systems. Most were adjusted by
integrating an opinion classifier module so that the
task could be fulfilled, but some were not (Bossard
et al., 2008), (Hendrickx and Bosma, 2008). This
fact made us realize that a generic summarizer
could be used to achieve this task. We wanted to
analyze the effects of such a kind of summarizer to
produce opinion summaries. We followed the
approach described in (Lloret et al., 2008). The
main idea employed is to score sentences of a
document with regard to the word frequency count
(WF), which can be combined with a Textual
Entailment (TE) module.
Although the first approach suggested for opinion
summarization obtained much better results in the
evaluation than the second one (see Section 3.1),
we decided to run the generic system over both
approaches, with and without applying TE, to
</bodyText>
<page confidence="0.993547">
74
</page>
<bodyText confidence="0.999979444444444">
provide a more extent analysis and conclusions.
After preprocessing the blogs and having all the
possible candidate sentences grouped together, we
considered these as the input for the generic
summarizer. The goal of these experiments was to
determine whether the techniques used for a
generic summarizer would have a positive
influence in selecting the main relevant
information to become part of the final summary.
</bodyText>
<subsectionHeader confidence="0.737029">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999849962264151">
We re-evaluated the summaries generated by the
generic system following the nuggets’ list provided
by the TAC 2008 organization, and counting
manually the number of nuggets that were covered
in the summaries. This was a tedious task, but it
could not be automatically performed because of
the fact that many of the provided nuggets were
not found in the original blog collection. After the
manual matching of nuggets and sentences, we
computed the average Recall, Precision and F-
measure (Beta =1) in the same way as in the TAC
2008 was done, according to the number and
weight of the nuggets that were also covered in the
summary. Each nugget had a weight ranging from
0 to 1 reflecting its importance, and it was counted
only once, even though the information was
repeated within the summary.
The average for each value was calculated taking
into account the results for all the summaries in
each approach. Unfortunately, we could not
measure criteria such as readability or coherence as
they were manually evaluated by human experts.
Table 2 points out the results for all the approaches
reported. We have also considered the results
derived from our participation in the TAC 2008
conference (OpSum-1 and OpSum-2), in order to
analyze whether they have been improved or not.
From these results it can be stated that the TE
module in conjunction with the WF counts, have
been very appropriate in selecting the most
important information of a document. Although it
can be thought that applying TE can remove some
meaningful sentences which contained important
information, results show the opposite. It benefits
the Precision value, because a shorter summary
contains greater ratio of relevant information. On
the other hand, taking into consideration the F-
measure value only, it can be seen that the
approach combining TE and WF, for the sentences
in the first approach, has beaten significantly the
best F-measure result among the participants of
TAC 2008 (please see Table 3), increasing its
performance by 20% (with respect to WF only),
and improving by approximately 80% with respect
to our first approach submitted to TAC 2008.
However, a simple generic summarization system
like the one we have used here is not enough to
produce opinion oriented summaries, since
semantic coherence given by the grouping of
positive and negative opinions is not taken into
account. Therefore, the opinion classification stage
must be added in the same manner as used in the
competition.
</bodyText>
<table confidence="0.997505">
SYSTEM RECALL PRECISION F-MEASURE
OpSum-1 0.592 0.272 0.357
OpSum-2 0.251 0.141 0.155
WF-1 0.705 0.392 0.486
TE+WF -1 0.684 0.630 0.639
WF -2 0.322 0.234 0.241
TE+WF-2 0.292 0.282 0.262
</table>
<tableCaption confidence="0.999961">
Table 2. Comparison of the results
</tableCaption>
<subsectionHeader confidence="0.997705">
4.3 Improving the quality of summaries
</subsectionHeader>
<bodyText confidence="0.999974583333333">
In the evaluation performed by the TAC
organization, a manual quality evaluation was also
carried out. In this evaluation the important aspects
were grammaticality, non-redundancy, structure
and coherence, readability, and overall
responsiveness. Although our participating systems
obtained good F-measure values, in other scores,
especially in grammaticality and non-redundancy,
the results achieved were very low. Focusing all
our efforts in improving the first approach,
OpSum-1, non-redundancy and grammaticality
verification had to be performed. In this approach,
we wanted to test how much of the redundant
information would be possible to remove by using
a Textual Entailment system similar to (Iftene and
Balahur-Dobrescu, 2007), without it affecting the
quality of the remaining data. As input for the TE
system, we considered the snippets retrieved from
the original blog posts. We applied the entailment
verification on each of the possible pairs, taking in
turn all snippets as Text and Hypothesis with all
other snippets as Hypothesis and Text,
respectively. Thus, as output, we obtained the list
of snippets from which we eliminated those that
</bodyText>
<page confidence="0.997257">
75
</page>
<bodyText confidence="0.99776325">
are entailed by any of the other snippets. We
further eliminated those snippets which had a high
entailment score with any of the remaining
snippets.
</bodyText>
<table confidence="0.9918418">
SYSTEM F-MEASURE
Best system 0.534
Second best system 0.490
OpSum-1 + TE 0.530
OpSum-1 0.357
</table>
<tableCaption confidence="0.9991">
Table 3. F-measure results after improving the system
</tableCaption>
<bodyText confidence="0.993417857142857">
Table 3 shows that applying TE before generating
the final summary leads to very good results
increasing the F-measure by 48.50% with respect
to the original first approach. Moreover, it can be
seen form Table 3 that our improved approach
would have ranked in the second place among all
the participants, regarding F-measure. The main
problem with this approach is the long processing
time. We can apply Textual Entailment in the
manner described within the generic
summarization system presented, successively
testing the relation as Snippet1 entails Snippet2?,
Snippet1+Snippet2 entails Snippet3? and so on.
The problem then becomes the fact that this
approach is random, since different snippets come
from different sources, so there is no order among
them. Further on, we have seen that many
problems arise from the fact that extracting
information from blogs introduces a lot of noise. In
many cases, we had examples such as:
At 4:00 PM John said Starbucks coffee tastes great
John said Starbucks coffee tastes great, always get one
when reading New York Times.
To the final summary, the important information
that should be added is “Starbucks coffee tastes
great”. Our TE system contains a rule specifying
that the existence or not of a Named Entity in the
hypothesis and its not being mentioned in the text
leads to the decision of “NO” entailment. For the
example given, both snippets are maintained,
although they contain the same data.
Another issue to be addressed is the extra
information contained in final summaries that is
not scored as nugget. As we have seen from our
data, much of this information is also valid and
correctly answers the questions. Therefore, what
methods can be employed to give more weight to
some and penalize others automatically?
Regarding the grammaticality criteria, once we had
a summary generated we used the module
Language Tool7 as a post-processing step. The
errors that we needed correcting included the
number matching between nouns and determiners
as well as among subject and predicate, upper case
for sentence start, repeated words or punctuation
marks and lack of punctuation marks. The rules
present in the module and that we “switched off”,
due to the fact that they produced more errors,
were those concerning the limit in the number of
consecutive nouns and the need for an article
before a noun (since it always seemed to want to
correct “Vista” for “the Vista” a.o.). We evaluated
by observing the mistakes that the texts contained,
and counting the number of remaining or
introduced errors in the output. The results
obtained can be seen in Table 4.
</bodyText>
<table confidence="0.999680111111111">
Problem Rightly corrected Wrongly
corrected
Match S-P 90% 10%
Noun-det 75% 25%
Upper case 80% 20%
Repeated words 100% 0%
Repeated “.” 80% 20%
Spelling mistakes 60% 40%
Unpaired “”/() 100% 0%
</table>
<tableCaption confidence="0.999713">
Table 4. Grammaticality analysis
</tableCaption>
<bodyText confidence="0.999543333333333">
The greatest problem encountered was the fact that
bigrams are not detected and agreement is not
made in cases in which the noun does not appear
exactly after the determiner. All in all, using this
module, the grammaticality of our texts was
greatly improved.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999281083333333">
The Opinion Pilot in the TAC 2008 competition
was a difficult task, involving the development of
systems including components for QA, IR, polarity
classification and summarization. Our contribution
presented in this paper resides in proposing an
opinion mining and summarization method using
different approaches and resources, evaluating
each of them in turn. We have shown that using a
generic summarization system, we obtain 80%
improvement over the results obtained in the
competition, with coherence being maintained by
using the same polarity classification mechanisms.
</bodyText>
<footnote confidence="0.973836">
7http://community.languagetool.org/
</footnote>
<page confidence="0.986083">
76
</page>
<bodyText confidence="0.9980536">
Using redundancy removal with TE, as opposed to
our initial polarity strength based sentence filtering
improved the system performance by almost 50%.
Finally, we showed that grammaticality can be
checked and improved using an independent
solution given by Language Tool.
Further work includes the improvement of the
polarity classification component by using
machine learning over annotated corpora and other
techniques, such as anaphora resolution. As we
could see, the well functioning of this component
ensures logic, structure and coherence to the
produced summaries. Moreover, we plan to study
the manner in which opinion sentences of
blogs/bloggers can be coherently combined.
</bodyText>
<sectionHeader confidence="0.998923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999951440860215">
Balahur, A., Lloret, E., Ferrández, Ó., Montoyo, A.,
Palomar, M., Muñoz, R., The DLSIUAES Team’s
Participation in the TAC 2008 Tracks. In
Proceedings of the Text Analysis Conference (TAC),
2008.
Balahur, A. and Montoyo, A. [1]. An Incremental
Multilingual Approach to Forming a Culture
Dependent Emotion Triggers Database. In
Proceedings of the 8th International Conference on
Terminology and Knowledge Engineering, 2008.
Balahur, A. and Montoyo, A. [2]. Multilingual Feature--
driven Opinion Mining and Summarization from
Customer Reviews. In Lecture Notes in Computer
Science 5039, pg. 345-346.
Bossard, A., Généreux, M. and Poibeau, T.. Description
of the LIPN systems at TAC 2008: Summarizing
information and opinions. In Proceedings of the Text
Analysis Conference (TAC), 2008.
Chaovalit, P., Zhou, L. 2005. Movie Review Mining: a
Comparison between Supervised and Unsupervised
Classification Approaches. In Proceedings of HICSS-
05, the 38th Hawaii International Conference on
System Sciences.
Cruz, F., Troyani, J.A., Ortega, J., Enríquez, F. The
Italica System at TAC 2008 Opinion Summarization
Task. In Proceedings of the Text Analysis
Conference (TAC), 2008.
Cui, H., Mittal, V., Datar, M. 2006. Comparative
Experiments on Sentiment Classification for Online
Product Reviews. In Proceedings of the 21st National
Conference on Artificial Intelligence AAAI 2006.
Dave, K., Lawrence, S., Pennock, D. 2003. Mining the
Peanut Gallery: Opinion Extraction and Semantic
Classification of Product Reviews. In Proceedings of
WWW-03.
Lloret, E., Ferrá́ndez, O., Muñoz, R. and Palomar, M. A
Text Summarization Approach under the Influence of
Textual Entailment. In Proceedings of the 5th
International Workshop on Natural Language
Processing and Cognitive Science (NLPCS 2008),
pages 22–31, 2008.
Gamon, M., Aue, S., Corston-Oliver, S., Ringger, E.
2005. Mining Customer Opinions from Free Text.
Lecture Notes in Computer Science.
He, T., Chen, J., Gui, Z., Li, F. CCNU at TAC 2008:
Proceeding on Using Semantic Method for
Automated Summarization Yield. In Proceedings of
the Text Analysis Conference (TAC), 2008.
Hendrickx, I. and Bosma, W.. Using coreference links
and sentence compression in graph-based
summarization. In Proceedings of the Text Analysis
Conference (TAC), 2008.
Hu, M., Liu, B. 2004. Mining Opinion Features in
Customer Reviews. In Proceedings of 19th National
Conference on Artificial Intelligence AAAI.
Iftene, A., Balahur-Dobrescu, A. Hypothesis
Transformation and Semantic Variability Rules for
Recognizing Textual Entailment. In Proceedings of
the ACL 2007 Workshop on Textual Entailment and
Paraphrasis, 2007.
Kim, S.M., Hovy, E. 2004. Determining the Sentiment
of Opinions. In Proceedings of COLING 2004.
Pang, B., Lee, L., Vaithyanathan, S. 2002. Thumbs up?
Sentiment classification using machine learning
techniques. In Proceedings of EMNLP-02, the
Conference on Empirical Methods in Natural
Language Processing.
Riloff, E., Wiebe, J. 2003 Learning Extraction Patterns
for Subjective Expressions. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing.
Scherer, K. and Wallbott, H.G. The ISEAR
Questionnaire and Codebook, 1997.
Stoyanov, V., Cardie, C. 2006. Toward Opinion
Summarization: Linking the Sources. In: COLING-
ACL 2006 Workshop on Sentiment and Subjectivity
in Text.
Strapparava, C. and Valitutti, A. &amp;quot;WordNet-Affect: an
affective extension of WordNet&amp;quot;. In Proceedings
ofthe 4th International Conference on Language
Resources and Evaluation, 2004, pp. 1083-1086.
Turney, P., 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised
classification of reviews. In Proceedings of the 40th
Annual Meeting of the ACL
Varma, V., Pingali, P., Katragadda, R., Krisha, S.,
Ganesh, S., Sarvabhotla, K., Garapati, H., Gopisetty,
H.,, Reddy, V.B., Bysani, P., Bharadwaj, R. IIT
Hyderabad at TAC 2008. In Proceedings of the Text
Analysis Conference (TAC), 2008.
Wilson, T., Wiebe, J., Hwa, R. 2004. Just how mad are
you? Finding strong and weak opinion clauses. In:
Proceedings of AAAI 2004.
</reference>
<page confidence="0.999129">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.678160">
<title confidence="0.9986145">Towards Building a Competitive Opinion Summarization Challenges and Keys</title>
<author confidence="0.979187">Elena Lloret</author>
<author confidence="0.979187">Alexandra Balahur</author>
<author confidence="0.979187">Manuel Palomar</author>
<author confidence="0.979187">Andrés</author>
<affiliation confidence="0.9992655">Department of Software and Computing University of</affiliation>
<address confidence="0.784512">Apartado de Correos 99, E-03080, Alicante,</address>
<email confidence="0.986193">elloret@dlsi.ua.es</email>
<email confidence="0.986193">abalahur@dlsi.ua.es</email>
<email confidence="0.986193">mpalomar@dlsi.ua.es</email>
<email confidence="0.986193">montoyo@dlsi.ua.es</email>
<abstract confidence="0.990561166666667">This paper presents an overview of our participation in the TAC 2008 Opinion Pilot Summarization task, as well as the proposed and evaluated post-competition improvements. We first describe our opinion summarization system and the results obtained. Further on, we identify the system’s weak points and suggest several improvements, focused both on information content, as well as linguistic and readability aspects. We obtain encouraging results, especially as far as Fmeasure is concerned, outperforming the competition results by approximately 80%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>E Lloret</author>
<author>Ó Ferrández</author>
<author>A Montoyo</author>
<author>M Palomar</author>
<author>R Muñoz</author>
</authors>
<title>Tracks.</title>
<date>2008</date>
<booktitle>The DLSIUAES Team’s Participation in the TAC</booktitle>
<contexts>
<context position="2313" citStr="Balahur et al., 2008" startWordPosition="321" endWordPosition="324">project TEXT-MESS (TIN-2006-15265) 1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html The techniques employed by the participants were mainly based on the already existing summarization systems. While most participants added new features (sentiment, pos/neg sentiment, pos/neg opinion) to account for the presence of positive opinions or negative ones - CLASSY (Conroy and Schlessinger, 2008); CCNU (He et al.,2008); LIPN (Bossard et al., 2008); IIITSum08 (Varma et al., 2008) -, efficient methods were proposed focusing on the retrieval and filtering stage, based on polarity – DLSIUAES (Balahur et al., 2008) - or on separating information rich clauses - italica (Cruz et al., 2008). In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wi</context>
</contexts>
<marker>Balahur, Lloret, Ferrández, Montoyo, Palomar, Muñoz, 2008</marker>
<rawString>Balahur, A., Lloret, E., Ferrández, Ó., Montoyo, A., Palomar, M., Muñoz, R., The DLSIUAES Team’s Participation in the TAC 2008 Tracks. In Proceedings of the Text Analysis Conference (TAC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Balahur</author>
<author>A Montoyo</author>
</authors>
<title>An Incremental Multilingual Approach to Forming a Culture Dependent Emotion Triggers Database.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th International Conference on Terminology and Knowledge Engineering,</booktitle>
<contexts>
<context position="4859" citStr="Balahur and Montoyo, 2008" startWordPosition="698" endWordPosition="701">ild some patterns of reformulation. The latter were defined in order to give the final summary an abstract nature, rather than a simple joining of sentences. The polarity of the question was determined using a set of created patterns, whose goal was to extract for further classification the nouns, verbs, adverbs or adjectives indicating some kind of polarity (positive or negative). These extracted words, together with their determiners, were classified using the emotions lists in WordNet Affect (Strapparava and Valitutti, 2005), jointly with the emotions lists of attitudes, triggers resource (Balahur and Montoyo, 2008 [1]), four created lists of attitudes, expressing criticism, support, admiration and rejection and two categories for value (good and bad), taking for the opinion mining systems in (Balahur and Montoyo, 2008 [2]). Moreover, the focus of each question was automatically extracted using the Freeling2 Named Entity Recognizer module. This information was used to determine whether or not all the questions within the same topic had the same focus, as well as be able to decide later on which text snippet belonged to which question. Regarding the given text snippets, we also computed their polarity an</context>
</contexts>
<marker>Balahur, Montoyo, 2008</marker>
<rawString>Balahur, A. and Montoyo, A. [1]. An Incremental Multilingual Approach to Forming a Culture Dependent Emotion Triggers Database. In Proceedings of the 8th International Conference on Terminology and Knowledge Engineering, 2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Balahur</author>
<author>A Montoyo</author>
</authors>
<title>Multilingual Feature--driven Opinion Mining and Summarization from Customer Reviews.</title>
<booktitle>In Lecture Notes in Computer Science 5039,</booktitle>
<pages>345--346</pages>
<marker>Balahur, Montoyo, </marker>
<rawString>Balahur, A. and Montoyo, A. [2]. Multilingual Feature--driven Opinion Mining and Summarization from Customer Reviews. In Lecture Notes in Computer Science 5039, pg. 345-346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bossard</author>
<author>M Généreux</author>
<author>T Poibeau</author>
</authors>
<title>Description of the LIPN systems at TAC 2008: Summarizing information and opinions.</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<contexts>
<context position="2147" citStr="Bossard et al., 2008" startWordPosition="295" endWordPosition="298"> Examples of target, question and snippet *Elena Lloret is funded by the FPI program (BES-2007- 16268) from the Spanish Ministry of Science and Innovation, under the project TEXT-MESS (TIN-2006-15265) 1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html The techniques employed by the participants were mainly based on the already existing summarization systems. While most participants added new features (sentiment, pos/neg sentiment, pos/neg opinion) to account for the presence of positive opinions or negative ones - CLASSY (Conroy and Schlessinger, 2008); CCNU (He et al.,2008); LIPN (Bossard et al., 2008); IIITSum08 (Varma et al., 2008) -, efficient methods were proposed focusing on the retrieval and filtering stage, based on polarity – DLSIUAES (Balahur et al., 2008) - or on separating information rich clauses - italica (Cruz et al., 2008). In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). </context>
<context position="11155" citStr="Bossard et al., 2008" startWordPosition="1666" endWordPosition="1669"> On the other hand, regarding the use of the optional snippets, the main problem to address is to remove redundancy, because many of them are repeated for the same target, and we have to determine which snippet represents better the idea for the final summary, in order to avoid noisy irrelevant information. 4.1 Measuring the Performance of a Generic Summarization System Several participants in the TAC 2008 edition performed the OSP task by using generic summarization systems. Most were adjusted by integrating an opinion classifier module so that the task could be fulfilled, but some were not (Bossard et al., 2008), (Hendrickx and Bosma, 2008). This fact made us realize that a generic summarizer could be used to achieve this task. We wanted to analyze the effects of such a kind of summarizer to produce opinion summaries. We followed the approach described in (Lloret et al., 2008). The main idea employed is to score sentences of a document with regard to the word frequency count (WF), which can be combined with a Textual Entailment (TE) module. Although the first approach suggested for opinion summarization obtained much better results in the evaluation than the second one (see Section 3.1), we decided t</context>
</contexts>
<marker>Bossard, Généreux, Poibeau, 2008</marker>
<rawString>Bossard, A., Généreux, M. and Poibeau, T.. Description of the LIPN systems at TAC 2008: Summarizing information and opinions. In Proceedings of the Text Analysis Conference (TAC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chaovalit</author>
<author>L Zhou</author>
</authors>
<title>Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches.</title>
<date>2005</date>
<booktitle>In Proceedings of HICSS05, the 38th Hawaii International Conference on System Sciences.</booktitle>
<contexts>
<context position="2525" citStr="Chaovalit and Zhou, 2005" startWordPosition="352" endWordPosition="355">ile most participants added new features (sentiment, pos/neg sentiment, pos/neg opinion) to account for the presence of positive opinions or negative ones - CLASSY (Conroy and Schlessinger, 2008); CCNU (He et al.,2008); LIPN (Bossard et al., 2008); IIITSum08 (Varma et al., 2008) -, efficient methods were proposed focusing on the retrieval and filtering stage, based on polarity – DLSIUAES (Balahur et al., 2008) - or on separating information rich clauses - italica (Cruz et al., 2008). In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a sentence (Kim and Hovy, 2004), and identifying opinion holders (Stoyanov and Cardie, 2006). Finally, fine grained, feature-based opinion s</context>
</contexts>
<marker>Chaovalit, Zhou, 2005</marker>
<rawString>Chaovalit, P., Zhou, L. 2005. Movie Review Mining: a Comparison between Supervised and Unsupervised Classification Approaches. In Proceedings of HICSS05, the 38th Hawaii International Conference on System Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Cruz</author>
<author>J A Troyani</author>
<author>J Ortega</author>
<author>F Enríquez</author>
</authors>
<title>The Italica System at TAC</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<contexts>
<context position="2387" citStr="Cruz et al., 2008" startWordPosition="334" endWordPosition="337">access_to_data.html The techniques employed by the participants were mainly based on the already existing summarization systems. While most participants added new features (sentiment, pos/neg sentiment, pos/neg opinion) to account for the presence of positive opinions or negative ones - CLASSY (Conroy and Schlessinger, 2008); CCNU (He et al.,2008); LIPN (Bossard et al., 2008); IIITSum08 (Varma et al., 2008) -, efficient methods were proposed focusing on the retrieval and filtering stage, based on polarity – DLSIUAES (Balahur et al., 2008) - or on separating information rich clauses - italica (Cruz et al., 2008). In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a </context>
</contexts>
<marker>Cruz, Troyani, Ortega, Enríquez, 2008</marker>
<rawString>Cruz, F., Troyani, J.A., Ortega, J., Enríquez, F. The Italica System at TAC 2008 Opinion Summarization Task. In Proceedings of the Text Analysis Conference (TAC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>V Mittal</author>
<author>M Datar</author>
</authors>
<title>Comparative Experiments on Sentiment Classification for Online Product Reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence AAAI</booktitle>
<marker>Cui, Mittal, Datar, 2006</marker>
<rawString>Cui, H., Mittal, V., Datar, M. 2006. Comparative Experiments on Sentiment Classification for Online Product Reviews. In Proceedings of the 21st National Conference on Artificial Intelligence AAAI 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D Pennock</author>
</authors>
<title>Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW-03.</booktitle>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Dave, K., Lawrence, S., Pennock, D. 2003. Mining the Peanut Gallery: Opinion Extraction and Semantic Classification of Product Reviews. In Proceedings of WWW-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Lloret</author>
<author>O Ferrá́ndez</author>
<author>R Muñoz</author>
<author>M Palomar</author>
</authors>
<title>A Text Summarization Approach under the Influence of Textual Entailment.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Workshop on Natural Language Processing and Cognitive Science (NLPCS</booktitle>
<pages>22--31</pages>
<marker>Lloret, Ferrá́ndez, Muñoz, Palomar, 2008</marker>
<rawString>Lloret, E., Ferrá́ndez, O., Muñoz, R. and Palomar, M. A Text Summarization Approach under the Influence of Textual Entailment. In Proceedings of the 5th International Workshop on Natural Language Processing and Cognitive Science (NLPCS 2008), pages 22–31, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>S Aue</author>
<author>S Corston-Oliver</author>
<author>E Ringger</author>
</authors>
<title>Mining Customer Opinions from Free Text.</title>
<date>2005</date>
<journal>Lecture Notes in Computer Science.</journal>
<marker>Gamon, Aue, Corston-Oliver, Ringger, 2005</marker>
<rawString>Gamon, M., Aue, S., Corston-Oliver, S., Ringger, E. 2005. Mining Customer Opinions from Free Text. Lecture Notes in Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T He</author>
<author>J Chen</author>
<author>Z Gui</author>
<author>F Li</author>
</authors>
<title>CCNU at TAC 2008: Proceeding on Using Semantic Method for Automated Summarization Yield.</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<marker>He, Chen, Gui, Li, 2008</marker>
<rawString>He, T., Chen, J., Gui, Z., Li, F. CCNU at TAC 2008: Proceeding on Using Semantic Method for Automated Summarization Yield. In Proceedings of the Text Analysis Conference (TAC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Hendrickx</author>
<author>W Bosma</author>
</authors>
<title>Using coreference links and sentence compression in graph-based summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<contexts>
<context position="11184" citStr="Hendrickx and Bosma, 2008" startWordPosition="1670" endWordPosition="1673">rding the use of the optional snippets, the main problem to address is to remove redundancy, because many of them are repeated for the same target, and we have to determine which snippet represents better the idea for the final summary, in order to avoid noisy irrelevant information. 4.1 Measuring the Performance of a Generic Summarization System Several participants in the TAC 2008 edition performed the OSP task by using generic summarization systems. Most were adjusted by integrating an opinion classifier module so that the task could be fulfilled, but some were not (Bossard et al., 2008), (Hendrickx and Bosma, 2008). This fact made us realize that a generic summarizer could be used to achieve this task. We wanted to analyze the effects of such a kind of summarizer to produce opinion summaries. We followed the approach described in (Lloret et al., 2008). The main idea employed is to score sentences of a document with regard to the word frequency count (WF), which can be combined with a Textual Entailment (TE) module. Although the first approach suggested for opinion summarization obtained much better results in the evaluation than the second one (see Section 3.1), we decided to run the generic system over</context>
</contexts>
<marker>Hendrickx, Bosma, 2008</marker>
<rawString>Hendrickx, I. and Bosma, W.. Using coreference links and sentence compression in graph-based summarization. In Proceedings of the Text Analysis Conference (TAC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining Opinion Features in Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of 19th National Conference on Artificial Intelligence AAAI.</booktitle>
<contexts>
<context position="3170" citStr="Hu and Liu, 2004" startWordPosition="442" endWordPosition="445">urney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a sentence (Kim and Hovy, 2004), and identifying opinion holders (Stoyanov and Cardie, 2006). Finally, fine grained, feature-based opinion summarization is defined in (Hu and Liu, 2004). 2 Opinion Summarization System In order to tackle the OSP task, we considered the use of two different methods for opinion mining and summarization, differing mainly with respect to the use of the optional text snippets provided. Our first approach (the Snippet-driven Approach) 72 Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72–77, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics used these snippets, whereas the second one (Blogdriven Approach) found the answers directly in the corresponding blogs. A general overview of t</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, M., Liu, B. 2004. Mining Opinion Features in Customer Reviews. In Proceedings of 19th National Conference on Artificial Intelligence AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Iftene</author>
<author>A Balahur-Dobrescu</author>
</authors>
<title>Hypothesis Transformation and Semantic Variability Rules for Recognizing Textual Entailment.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Workshop on Textual Entailment and Paraphrasis,</booktitle>
<contexts>
<context position="15756" citStr="Iftene and Balahur-Dobrescu, 2007" startWordPosition="2399" endWordPosition="2402"> In this evaluation the important aspects were grammaticality, non-redundancy, structure and coherence, readability, and overall responsiveness. Although our participating systems obtained good F-measure values, in other scores, especially in grammaticality and non-redundancy, the results achieved were very low. Focusing all our efforts in improving the first approach, OpSum-1, non-redundancy and grammaticality verification had to be performed. In this approach, we wanted to test how much of the redundant information would be possible to remove by using a Textual Entailment system similar to (Iftene and Balahur-Dobrescu, 2007), without it affecting the quality of the remaining data. As input for the TE system, we considered the snippets retrieved from the original blog posts. We applied the entailment verification on each of the possible pairs, taking in turn all snippets as Text and Hypothesis with all other snippets as Hypothesis and Text, respectively. Thus, as output, we obtained the list of snippets from which we eliminated those that 75 are entailed by any of the other snippets. We further eliminated those snippets which had a high entailment score with any of the remaining snippets. SYSTEM F-MEASURE Best sys</context>
</contexts>
<marker>Iftene, Balahur-Dobrescu, 2007</marker>
<rawString>Iftene, A., Balahur-Dobrescu, A. Hypothesis Transformation and Semantic Variability Rules for Recognizing Textual Entailment. In Proceedings of the ACL 2007 Workshop on Textual Entailment and Paraphrasis, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the Sentiment of Opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="3016" citStr="Kim and Hovy, 2004" startWordPosition="421" endWordPosition="424">al, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a sentence (Kim and Hovy, 2004), and identifying opinion holders (Stoyanov and Cardie, 2006). Finally, fine grained, feature-based opinion summarization is defined in (Hu and Liu, 2004). 2 Opinion Summarization System In order to tackle the OSP task, we considered the use of two different methods for opinion mining and summarization, differing mainly with respect to the use of the optional text snippets provided. Our first approach (the Snippet-driven Approach) 72 Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72–77, Boulder, Colorado, June 2009. c�2009 Association for Computational Li</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Kim, S.M., Hovy, E. 2004. Determining the Sentiment of Opinions. In Proceedings of COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, B., Lee, L., Vaithyanathan, S. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP-02, the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning Extraction Patterns for Subjective Expressions.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2879" citStr="Riloff and Wiebe, 2003" startWordPosition="399" endWordPosition="402">tage, based on polarity – DLSIUAES (Balahur et al., 2008) - or on separating information rich clauses - italica (Cruz et al., 2008). In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a sentence (Kim and Hovy, 2004), and identifying opinion holders (Stoyanov and Cardie, 2006). Finally, fine grained, feature-based opinion summarization is defined in (Hu and Liu, 2004). 2 Opinion Summarization System In order to tackle the OSP task, we considered the use of two different methods for opinion mining and summarization, differing mainly with respect to the use of the optional text snippets provided. Our first approach (the Snippet-driven Approach) 72 Proceedings of the NAACL </context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>Riloff, E., Wiebe, J. 2003 Learning Extraction Patterns for Subjective Expressions. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Scherer</author>
<author>H G Wallbott</author>
</authors>
<date>1997</date>
<booktitle>The ISEAR Questionnaire and Codebook,</booktitle>
<marker>Scherer, Wallbott, 1997</marker>
<rawString>Scherer, K. and Wallbott, H.G. The ISEAR Questionnaire and Codebook, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Toward Opinion Summarization: Linking the Sources. In:</title>
<date>2006</date>
<booktitle>COLINGACL 2006 Workshop on Sentiment and Subjectivity in Text.</booktitle>
<contexts>
<context position="3077" citStr="Stoyanov and Cardie, 2006" startWordPosition="429" endWordPosition="432">level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a sentence (Kim and Hovy, 2004), and identifying opinion holders (Stoyanov and Cardie, 2006). Finally, fine grained, feature-based opinion summarization is defined in (Hu and Liu, 2004). 2 Opinion Summarization System In order to tackle the OSP task, we considered the use of two different methods for opinion mining and summarization, differing mainly with respect to the use of the optional text snippets provided. Our first approach (the Snippet-driven Approach) 72 Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 72–77, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics used these snippets, whereas the second one (Blogdr</context>
</contexts>
<marker>Stoyanov, Cardie, 2006</marker>
<rawString>Stoyanov, V., Cardie, C. 2006. Toward Opinion Summarization: Linking the Sources. In: COLINGACL 2006 Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>WordNet-Affect: an affective extension of WordNet&amp;quot;.</title>
<date>2004</date>
<booktitle>In Proceedings ofthe 4th International Conference on Language Resources and Evaluation,</booktitle>
<pages>1083--1086</pages>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>Strapparava, C. and Valitutti, A. &amp;quot;WordNet-Affect: an affective extension of WordNet&amp;quot;. In Proceedings ofthe 4th International Conference on Language Resources and Evaluation, 2004, pp. 1083-1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL</booktitle>
<contexts>
<context position="2565" citStr="Turney, 2002" startWordPosition="359" endWordPosition="360"> pos/neg sentiment, pos/neg opinion) to account for the presence of positive opinions or negative ones - CLASSY (Conroy and Schlessinger, 2008); CCNU (He et al.,2008); LIPN (Bossard et al., 2008); IIITSum08 (Varma et al., 2008) -, efficient methods were proposed focusing on the retrieval and filtering stage, based on polarity – DLSIUAES (Balahur et al., 2008) - or on separating information rich clauses - italica (Cruz et al., 2008). In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques (Riloff and Wiebe, 2003), finding strength of opinions (Wilson, Wiebe and Hwa, 2004), summing up orientations of opinion words in a sentence (Kim and Hovy, 2004), and identifying opinion holders (Stoyanov and Cardie, 2006). Finally, fine grained, feature-based opinion summarization is defined in (Hu and Liu, </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, P., 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Varma</author>
<author>P Pingali</author>
<author>R Katragadda</author>
<author>S Krisha</author>
<author>S Ganesh</author>
<author>K Sarvabhotla</author>
<author>H Garapati</author>
<author>H Gopisetty</author>
</authors>
<title>IIT Hyderabad at TAC</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Analysis Conference (TAC),</booktitle>
<contexts>
<context position="2179" citStr="Varma et al., 2008" startWordPosition="300" endWordPosition="303">snippet *Elena Lloret is funded by the FPI program (BES-2007- 16268) from the Spanish Ministry of Science and Innovation, under the project TEXT-MESS (TIN-2006-15265) 1http://ir.dcs.gla.ac.uk/test_collections/access_to_data.html The techniques employed by the participants were mainly based on the already existing summarization systems. While most participants added new features (sentiment, pos/neg sentiment, pos/neg opinion) to account for the presence of positive opinions or negative ones - CLASSY (Conroy and Schlessinger, 2008); CCNU (He et al.,2008); LIPN (Bossard et al., 2008); IIITSum08 (Varma et al., 2008) -, efficient methods were proposed focusing on the retrieval and filtering stage, based on polarity – DLSIUAES (Balahur et al., 2008) - or on separating information rich clauses - italica (Cruz et al., 2008). In general, previous work in opinion mining includes document level sentiment classification using supervised (Chaovalit and Zhou, 2005) and unsupervised methods (Turney, 2002), machine learning techniques and sentiment classification considering rating scales (Pang, Lee and Vaithyanathan, 2002), and scoring of features (Dave, Lawrence and Pennock, 2003). Other research has been conducte</context>
</contexts>
<marker>Varma, Pingali, Katragadda, Krisha, Ganesh, Sarvabhotla, Garapati, Gopisetty, 2008</marker>
<rawString>Varma, V., Pingali, P., Katragadda, R., Krisha, S., Ganesh, S., Sarvabhotla, K., Garapati, H., Gopisetty, H.,, Reddy, V.B., Bysani, P., Bharadwaj, R. IIT Hyderabad at TAC 2008. In Proceedings of the Text Analysis Conference (TAC), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses. In:</title>
<date>2004</date>
<booktitle>Proceedings of AAAI</booktitle>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>Wilson, T., Wiebe, J., Hwa, R. 2004. Just how mad are you? Finding strong and weak opinion clauses. In: Proceedings of AAAI 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>