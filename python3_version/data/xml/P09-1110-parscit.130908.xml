<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.844916">
Learning Context-Dependent Mappings from Sentences to Logical Form
</title>
<note confidence="0.712165666666667">
Luke S. Zettlemoyer and Michael Collins
MIT CSAIL
Cambridge, MA 02139
</note>
<email confidence="0.975018">
{lsz,mcollins}@csail.mit.com
</email>
<sectionHeader confidence="0.997043" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866294117647">
We consider the problem of learning
context-dependent mappings from sen-
tences to logical form. The training ex-
amples are sequences of sentences anno-
tated with lambda-calculus meaning rep-
resentations. We develop an algorithm that
maintains explicit, lambda-calculus repre-
sentations of salient discourse entities and
uses a context-dependent analysis pipeline
to recover logical forms. The method uses
a hidden-variable variant of the percep-
tion algorithm to learn a linear model used
to select the best analysis. Experiments
on context-dependent utterances from the
ATIS corpus show that the method recov-
ers fully correct logical forms with 83.7%
accuracy.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999819666666667">
Recently, researchers have developed algorithms
that learn to map natural language sentences to
representations of their underlying meaning (He
and Young, 2006; Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005). For instance, a
training example might be:
</bodyText>
<equation confidence="0.960794333333333">
Sent. 1: List flights to Boston on Friday night.
LF 1: ax.flight(x) ∧ to(x, bos)
∧ day(x, fri) ∧ during(x, night)
</equation>
<bodyText confidence="0.999967888888889">
Here the logical form (LF) is a lambda-calculus
expression defining a set of entities that are flights
to Boston departing on Friday night.
Most of this work has focused on analyzing sen-
tences in isolation. In this paper, we consider the
problem of learning to interpret sentences whose
underlying meanings can depend on the context in
which they appear. For example, consider an inter-
action where Sent. 1 is followed by the sentence:
</bodyText>
<equation confidence="0.670877">
Sent. 2: Show me the flights after 3pm.
LF 2: ax.flight(x) ∧ to(x, bos)
∧day(x, fri) ∧ depart(x) &gt; 1500
</equation>
<bodyText confidence="0.999809314285714">
In this case, the fact that Sent. 2 describes flights
to Boston on Friday must be determined based on
the context established by the first sentence.
We introduce a supervised, hidden-variable ap-
proach for learning to interpret sentences in con-
text. Each training example is a sequence of sen-
tences annotated with logical forms. Figure 1
shows excerpts from three training examples in the
ATIS corpus (Dahl et al., 1994).
For context-dependent analysis, we develop an
approach that maintains explicit, lambda-calculus
representations of salient discourse entities and
uses a two-stage pipeline to construct context-
dependent logical forms. The first stage uses
a probabilistic Combinatory Categorial Grammar
(CCG) parsing algorithm to produce a context-
independent, underspecified meaning representa-
tion. The second stage resolves this underspecified
meaning representation by making a sequence of
modifications to it that depend on the context pro-
vided by previous utterances.
In general, there are a large number of possi-
ble context-dependent analyses for each sentence.
To select the best one, we present a weighted lin-
ear model that is used to make a range of parsing
and context-resolution decisions. Since the train-
ing data contains only the final logical forms, we
model these intermediate decisions as hidden vari-
ables that must be estimated without explicit su-
pervision. We show that this model can be effec-
tively trained with a hidden-variable variant of the
perceptron algorithm.
In experiments on the ATIS DEC94 test set, the
approach recovers fully correct logical forms with
83.7% accuracy.
</bodyText>
<sectionHeader confidence="0.987437" genericHeader="introduction">
2 The Learning Problem
</sectionHeader>
<bodyText confidence="0.99784575">
We assume access to a training set that consists of
n interactions D = (Ii, ... , In). The i’th interac-
tion Ii contains ni sentences, wi,1,... , wi,nz. Each
sentence wi,� is paired with a lambda-calculus ex-
</bodyText>
<page confidence="0.97572">
976
</page>
<note confidence="0.999636">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 976–984,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.912049172413793">
Example #1:
(a) show me the flights from boston to philly
ax.flight(x) ∧ from(x, bos) ∧ to(x, phi)
(b) show me the ones that leave in the morning
ax.flight(x) ∧ from(x, bos) ∧ to(x, phi)
∧ during(x, morning)
(c) what kind of plane is used on these flights
Ay.∃x.flight(x) ∧ from(x, bos) ∧ to(x, phi)
∧ during(x, morning) ∧ aircraft(x) = y
Example #2:
(a) show me flights from milwaukee to orlando
ax.flight(x) ∧ from(x, mil) ∧ to(x, orl)
(b) cheapest
argmin(Ax.flight(x) ∧ from(x, mil) ∧ to(x, orl),
Ay.fare(y))
(c) departing wednesday after 5 o’clock
argmin(Ax.flight(x) ∧ from(x, mil) ∧ to(x, orl)
∧ day(x, wed) ∧ depart(x) &gt; 1700 ,
Ay.fare(y))
Example #3:
(a) show me flights from pittsburgh to la thursday evening
ax.flight(x) ∧ from(x, pit) ∧ to(x, la)
∧ day(x,thur) ∧ during(x, evening)
(b) thursday afternoon
ax.flight(x) ∧ from(x, pit) ∧ to(x, la)
∧ day(x,thur) ∧ during(x, afternoon)
(c) thursday after 1700 hours
ax.flight(x) ∧ from(x, pit) ∧ to(x, la)
∧ day(x, thur) ∧ depart(x) &gt; 1700
</figure>
<figureCaption confidence="0.987772">
Figure 1: ATIS interaction excerpts.
pression zi,j specifying the target logical form.
Figure 1 contains example interactions.
</figureCaption>
<bodyText confidence="0.999933285714286">
The logical forms in the training set are repre-
sentations of each sentence’s underlying meaning.
In most cases, context (the previous utterances and
their interpretations) is required to recover the log-
ical form for a sentence. For instance, in Exam-
ple 1(b) in Figure 1, the sentence “show me the
ones that leave in the morning” is paired with
</bodyText>
<equation confidence="0.988786">
ax.flight(x) ∧ from(x, bos) ∧ to(x, phi)
∧ during(x, morning)
</equation>
<bodyText confidence="0.967734272727273">
Some parts of this logical form (from(x, bos) and
to(x, phi)) depend on the context. They have to be
recovered from the previous logical forms.
At step j in interaction i, we define the con-
text (zi,1, ... , zi,j−1) to be the j − 1 preceding
logical forms.1 Now, given the training data, we
can create training examples (xi,j, zi,j) for i =
1... n, j = 1... ni. Each xi,j is a sentence and
a context, xi,j = (wi,j, (zi,1, . . . , zi,j−1)). Given
this set up, we have a supervised learning problem
with input xi,j and output zi,j.
</bodyText>
<footnote confidence="0.995698">
1In general, the context could also include the previous
sentences wi,k for k &lt; j. In our data, we never observed any
interactions where the choice of the correct logical form zi,j
depended on the words in the previous sentences.
</footnote>
<sectionHeader confidence="0.799819" genericHeader="method">
3 Overview of Approach
</sectionHeader>
<bodyText confidence="0.9961832">
In general, the mapping from a sentence and a con-
text to a logical form can be quite complex. In this
section, we present an overview of our learning
approach. We assume the learning algorithm has
access to:
</bodyText>
<listItem confidence="0.9993925">
• A training set D, defined in Section 2.
• A CCG lexicon.2 See Section 4 for an
overview of CCG. Each entry in the lexicon
pairs a word (or sequence of words), with
a CCG category specifying both the syntax
and semantics for that word. One example
CCG entry would pair flights with the cate-
gory N : Ax.flight(x).
</listItem>
<bodyText confidence="0.585907">
Derivations A derivation for the j’th sentence
in an interaction takes as input a pair x = (wj, C),
where C = (z1 ... zj−1) is the current context. It
produces a logical form z. There are two stages:
</bodyText>
<listItem confidence="0.9998605">
• First, the sentence wj is parsed using
the CCG lexicon to form an intermediate,
context-independent logical form 7r.
• Second, in a series of steps, 7r is mapped to z.
</listItem>
<bodyText confidence="0.973870066666667">
These steps depend on the context C.
As one sketch of a derivation, consider how we
might analyze Example 1(b) in Figure 1. In this
case the sentence is “show me the ones that leave
in the morning.” The CCG parser would produce
the following context-independent logical form:
Ax.!(e, t)(x) n during(x, morning)
The subexpression !(e, t) results directly from the
referential phrase the ones; we discuss this in more
detail in Section 4.2, but intuitively this subexpres-
sion specifies that a lambda-calculus expression of
type (e, t) must be recovered from the context and
substituted in its place.
In the second (contextually dependent) stage of
the derivation, the expression
</bodyText>
<footnote confidence="0.809580625">
Ax.flight(x) n from(x, bos) n to(x, phi)
is recovered from the context, and substituted for
the !(e, t) subexpression, producing the desired fi-
nal logical form, seen in Example 1(b).
2Developing algorithms that learn the CCG lexicon from
the data described in this paper is an important area for future
work. We could possibly extend algorithms that learn from
context-independent data (Zettlemoyer and Collins, 2005).
</footnote>
<page confidence="0.997443">
977
</page>
<bodyText confidence="0.999975931034483">
In addition to substitutions of this type, we will
also perform other types of context-dependent res-
olution steps, as described in Section 5.
In general, both of the stages of the derivation
involve considerable ambiguity – there will be a
large number of possible context-independent log-
ical forms 7r for wj and many ways of modifying
each 7r to create a final logical form zj.
Learning We model the problem of selecting
the best derivation as a structured prediction prob-
lem (Johnson et al., 1999; Lafferty et al., 2001;
Collins, 2002; Taskar et al., 2004). We present
a linear model with features for both the parsing
and context resolution stages of the derivation. In
our setting, the choice of the context-independent
logical form 7r and all of the steps that map 7r to
the output z are hidden variables; these steps are
not annotated in the training data. To estimate the
parameters of the model, we use a hidden-variable
version of the perceptron algorithm. We use an ap-
proximate search procedure to find the best deriva-
tion both while training the model and while ap-
plying it to test examples.
Evaluation We evaluate the approach on se-
quences of sentences (wi, ... , wk). For each wj,
the algorithm constructs an output logical form zj
which is compared to a gold standard annotation to
check correctness. At step j, the context contains
the previous zi, for i &lt; j, output by the system.
</bodyText>
<sectionHeader confidence="0.995977" genericHeader="method">
4 Context-independent Parsing
</sectionHeader>
<bodyText confidence="0.9999596">
In this section, we first briefly review the CCG
parsing formalism. We then define a set of ex-
tensions that allow the parser to construct logical
forms containing references, such as the !(e, t) ex-
pression from the example derivation in Section 3.
</bodyText>
<subsectionHeader confidence="0.991456">
4.1 Background: CCG
</subsectionHeader>
<bodyText confidence="0.9999745">
CCG is a lexicalized, mildly context-sensitive
parsing formalism that models a wide range of
linguistic phenomena (Steedman, 1996; Steed-
man, 2000). Parses are constructed by combining
lexical entries according to a small set of relatively
simple rules. For example, consider the lexicon
</bodyText>
<equation confidence="0.987044666666667">
flights :� N : ax.flight(x)
to :� (N\N)/NP : Ay.Af.Ax.f(x) ∧ to(x, y)
boston :� NP : boston
</equation>
<bodyText confidence="0.999754384615384">
Each lexical entry consists of a word and a cat-
egory. Each category includes syntactic and se-
mantic content. For example, the first entry
pairs the word flights with the category N :
Ax.flight(x). This category has syntactic type N,
and includes the lambda-calculus semantic expres-
sion Ax.flight(x). In general, syntactic types can
either be simple types such as N, NP, or 5, or
can be more complex types that make use of slash
notation, for example (N\N)/NP.
CCG parses construct parse trees according to
a set of combinator rules. For example, consider
the functional application combinators:3
</bodyText>
<equation confidence="0.7984525">
A/B : f B : g =:&gt;. A : f(g) (&gt;)
B : g A\B : f =:&gt;. A : f(g) (&lt;)
</equation>
<bodyText confidence="0.999881">
The first rule is used to combine a category with
syntactic type A/B with a category to the right
of syntactic type B to create a new category of
type A. It also constructs a new lambda-calculus
expression by applying the function f to the
expression g. The second rule handles arguments
to the left. Using these rules, we can parse the
following phrase:
</bodyText>
<equation confidence="0.928021428571429">
flights to boston
N �N\N) NP NP
ax. flight(x) ay.af. x. f (x)∧ to(x, y) boston
(N\N)
af.ax.f(x) ∧ to(x, boston)
N
ax.flight(x) ∧ to(x, boston)
</equation>
<bodyText confidence="0.99991125">
The top-most parse operations pair each word with
a corresponding category from the lexicon. The
later steps are labeled with the rule that was ap-
plied (−� for the first and −&lt; for the second).
</bodyText>
<subsectionHeader confidence="0.99987">
4.2 Parsing with References
</subsectionHeader>
<bodyText confidence="0.999125333333333">
In this section, we extend the CCG parser to intro-
duce references. We use an exclamation point fol-
lowed by a type expression to specify references
in a logical form. For example, !e is a reference to
an entity and !(e, t) is a reference to a function. As
motivated in Section 3, we introduce these expres-
sions so they can later be replaced with appropriate
lambda-calculus expressions from the context.
Sometimes references are lexically triggered.
For example, consider parsing the phrase “show
me the ones that leave in the morning” from Ex-
ample 1(b) in Figure 1. Given the lexical entry:
</bodyText>
<construct confidence="0.238113">
ones := N : Ax.!(e, t)(x)
a CCG parser could produce the desired context-
</construct>
<bodyText confidence="0.94018425">
3In addition to application, we make use of composition,
type raising and coordination combinators. A full description
of these combinators is beyond the scope of this paper. Steed-
man (1996; 2000) presents a detailed description of CCG.
</bodyText>
<page confidence="0.936018">
978
</page>
<bodyText confidence="0.968635111111111">
independent logical form: develop an approach that learns when to introduce
references and how to best resolve them.
Ax.!(e, t)(x) ∧ during(x, morning)
Our first extension is to simply introduce lexical
items that include references into the CCG lexi-
con. They describe anaphoric words, for example
including “ones,” “those,” and “it.”
In addition, we sometimes need to introduce
references when there is no explicit lexical trig-
ger. For instance, Example 2(c) in Figure 1 con-
sists of the single word “cheapest.” This query has
the same meaning as the longer request “show me
the cheapest one,” but it does not include the lex-
ical reference. We add three CCG type-shifting
rules to handle these cases.
The first two new rules are applicable when
there is a category that is expecting an argument
with type (e, t). This argument is replaced with a
</bodyText>
<equation confidence="0.816752333333333">
!(e, t) reference:
A/B : f A : f(Ax.!(e, t)(x))
A\B : f A : f(Ax.!(e, t)(x))
</equation>
<bodyText confidence="0.99338">
For example, using the first rule, we could produce
the following parse for Example 2(c)
</bodyText>
<equation confidence="0.929971">
cheapest
NP/N
ag.argmin(ax.g(x), ay.fare(y))
NP
argmin(ax.!(e, t)(x), ay.fare(y))
</equation>
<bodyText confidence="0.999922">
where the final category has the desired lambda-
caculus expression.
The third rule is motivated by examples such as
“show me nonstop flights.” Consider this sentence
being uttered after Example 1(a) in Figure 1. Al-
though there is a complete, context-independent
meaning, the request actually restricts the salient
set of flights to include only the nonstop ones. To
achieve this analysis, we introduce the rule:
</bodyText>
<equation confidence="0.652387">
A : f A : Ax.f(x) ∧ !(e, t)(x)
</equation>
<bodyText confidence="0.8701">
where f is an function of type (e, t).
With this rule, we can construct the parse
nonstop flights
</bodyText>
<equation confidence="0.9942365">
N/N N
af.ax.f(x) n nonstop(x) ax.flight(x)
N �
ax.nonstop(x) n flight(x)
N
ax.nonstop(x) n flight(x) n !(e, t)(x)
</equation>
<bodyText confidence="0.9999375">
where the last parsing step is achieved with the
new type-shifting rule.
These three new parsing rules allow significant
flexibility when introducing references. Later, we
</bodyText>
<sectionHeader confidence="0.988713" genericHeader="method">
5 Contextual Analysis
</sectionHeader>
<bodyText confidence="0.9999625">
In this section, we first introduce the general pat-
terns of context-dependent analysis that we con-
sider. We then formally define derivations that
model these phenomena.
</bodyText>
<subsectionHeader confidence="0.92264">
5.1 Overview
</subsectionHeader>
<bodyText confidence="0.994523764705882">
This section presents an overview of the ways that
the context C is used during the analysis.
References Every reference expression (!e or
!(e, t)) must be replaced with an expression from
the context. For example, in Section 3, we consid-
ered the following logical form:
Ax.!(e, t)(x) ∧ during(x, morning)
In this case, we saw that replacing the !(e, t)
subexpression with the logical form for Exam-
ple 1(a), which is directly available in C, produces
the desired final meaning.
Elaborations Later statements can expand the
meaning of previous ones in ways that are diffi-
cult to model with references. For example, con-
sider analyzing Example 2(c) in Figure 1. Here the
phrase “departing wednesday after 5 o’clock” has
a context-independent logical form:4
</bodyText>
<equation confidence="0.561266">
ax.day(x, wed) n depart(x) &gt; 1700 (1)
</equation>
<bodyText confidence="0.9911895">
that must be combined with the meaning of the
previous sentence from the context C:
</bodyText>
<equation confidence="0.836925">
argmin(ax.fight(x) n from(x, mil) n to(x, orl),
ay.fare(y))
</equation>
<bodyText confidence="0.624779">
to produce the expression
</bodyText>
<equation confidence="0.852979">
argmin(ax.fight(x) n from(x, mil) n to(x, orl)
nday(x, wed) n depart(x) &gt; 1700,
ay.fare(y))
</equation>
<bodyText confidence="0.999931333333333">
Intuitively, the phrase “departing wednesday af-
ter 5 o’clock” is providing new constraints for the
set of flights embedded in the argmin expression.
We handle examples of this type by construct-
ing elaboration expressions from the zi in C. For
example, if we constructed the following function:
</bodyText>
<equation confidence="0.977978333333333">
af.argmin(ax.fight(x) n from(x, mil)
n to(x, orl) n f(x), (2)
ay.fare(y))
</equation>
<bodyText confidence="0.636145">
4Another possible option is the expression ax.!(e, t) n
day(x, wed) n depart(x) &gt; 1700. However, there is no ob-
vious way to resolve the !(e, t) expression that would produce
the desired final meaning.
</bodyText>
<page confidence="0.995603">
979
</page>
<bodyText confidence="0.996649444444444">
we could apply this function to Expression 1 and
produce the desired result. The introduction of the
new variable f provides a mechanism for expand-
ing the embedded subexpression.
References with Deletion When resolving ref-
erences, we will sometimes need to delete subparts
of the expressions that we substitute from the con-
text. For instance, consider Example 3(b) in Fig-
ure 1. The desired, final logical form is:
</bodyText>
<equation confidence="0.8523575">
ax.flight(x) n from(x, pit) n to(x, la)
n day(x, thur) n during(x, afternoon)
</equation>
<bodyText confidence="0.941545833333333">
We need to construct this from the context-
independent logical form:
Ax.!(e, t) n day(x, thur) n during(x, afternoon)
The reference !(e, t) must be resolved. The only
expression in the context C is the meaning from
the previous sentence, Example 3(a):
</bodyText>
<equation confidence="0.9879005">
Ax.flight(x) n from(x, pit) n to(x, la) (3)
n day(x, thur) n during(x, evening)
</equation>
<bodyText confidence="0.923466">
Substituting this expression directly would pro-
duce the following logical form:
</bodyText>
<equation confidence="0.975191333333333">
ax.flight(x) n from(x, pit) n to(x, la)
n day(x, thur) n during(x, evening)
n day(x, thur) n during(x, afternoon)
</equation>
<bodyText confidence="0.999900166666667">
which specifies the day twice and has two different
time spans.
We can achieve the desired analysis by deleting
parts of expressions before they are substituted.
For example, we could remove the day and time
constraints from Expression 3 to create:
</bodyText>
<equation confidence="0.765101">
ax.flight(x) n from(x, pit) n to(x, la)
</equation>
<bodyText confidence="0.969394">
which would produce the desired final meaning
when substituted into the original expression.
Elaborations with Deletion We also allow
deletions for elaborations. In this case, we delete
subexpressions of the elaboration expression that
is constructed from the context.
</bodyText>
<subsectionHeader confidence="0.983793">
5.2 Derivations
</subsectionHeader>
<bodyText confidence="0.97897925">
We now formally define a derivation that maps a
sentence wj and a context C = {z1, ... , zj_1} to
an output logical form zj. We first introduce no-
tation for expressions in C that we will use in the
derivation steps. We then present a definition of
deletion. Finally, we define complete derivations.
Context Sets Given a context C, our algorithm
constructs three sets of expressions:
</bodyText>
<listItem confidence="0.981711333333333">
• Re(C): A set of e-type expressions that can
be used to resolve references.
• R(e�t)(C): A set of (e, t)-type expressions
that can be used to resolve references.
• E(C): A set of possible elaboration expres-
sions (for example, see Expression 2).
</listItem>
<bodyText confidence="0.998625333333333">
We will provide the details of how these sets
are defined in Section 5.3. As an example, if C
contains only the logical form
</bodyText>
<figure confidence="0.399092">
Ax.flight(x) n from(x, pit) n to(x, la)
then Re(C) = {pit, la} and R(e�t)(C) is a set that
contains a single entry, the complete logical form.
</figure>
<bodyText confidence="0.988606764705882">
Deletion A deletion operator accepts a logical
form l and produces a new logical form l&apos;. It con-
structs l&apos; by removing a single subexpression that
appears in a coordination (conjunction or disjunc-
tion) in l. For example, if l is
Ax.flight(x) n from(x, pit) n to(x, la)
there are three possible deletion operations, each
of which removes a single subexpression.
Derivations We now formally define a deriva-
tion to be a sequence d = (II, s1, ... , sm). II is a
CCG parse that constructs a context-independent
logical form 7r with m − 1 reference expressions.5
Each si is a function that accepts as input a logi-
cal form, makes some change to it, and produces a
new logical form that is input to the next function
si+1. The initial si for i &lt; m are reference steps.
The final sm is an optional elaboration step.
</bodyText>
<listItem confidence="0.964229857142857">
• Reference Steps: A reference step is a tuple
(l, l&apos;, f, r, r1, ... , rp). This operator selects a
reference f in the input logical form l and
an appropriately typed expression r from ei-
ther Re(C) or R(e�t)(C). It then applies a se-
quence of p deletion operators to create new
expressions r1 ... rp. Finally, it constructs
the output logical form l&apos; by substituting rp
for the selected reference f in l.
• Elaboration Steps: An elaboration step is a
tuple (l, l&apos;, b, b1, ... , bq). This operator se-
lects an expression b from E(C) and ap-
plies q deletions to create new expressions
b1 ... bq. The output expression l&apos; is bq(l).
</listItem>
<footnote confidence="0.971021">
5In practice, it rarely contains more than one reference.
</footnote>
<page confidence="0.996804">
980
</page>
<bodyText confidence="0.999021">
In general, the space of possible derivations is
large. In Section 6, we describe a linear model
and decoding algorithm that we use to find high
scoring derivations.
produce the example elaboration Expression 2 and
elaborations that expand other embedded expres-
sions, such as the quantifier in Example 1(c).
</bodyText>
<sectionHeader confidence="0.949803" genericHeader="method">
6 A Linear Model
</sectionHeader>
<subsectionHeader confidence="0.998834">
5.3 Context Sets
</subsectionHeader>
<bodyText confidence="0.93532375">
For a context C = {z1,... , zj_1}, we define sets
Re(C), R(e,t)(C), and E(C) as follows.
e-type Expressions Re(z) is a set of e-type ex-
pressions extracted from a logical form z. We de-
</bodyText>
<equation confidence="0.991438166666667">
fine Re(C) = �j_1
i=1 Re(zi).
Re(z) includes all e-type subexpressions of z.6
For example, if z is
argmin(Ax.flight(x) ∧ from(x, mil) ∧ to(x, orl),
Ay.fare(y))
</equation>
<bodyText confidence="0.998483166666667">
the resulting set is Re(z) = {mil, orl, z}, where z
is included because the entire argmin expression
has type e.
(e, ti-type Expressions R(e,t)(z) is a set of
(e, ti-type expressions extracted from a logical
form z. We define R(e,t)(C) = �j_1
</bodyText>
<equation confidence="0.87165">
i=1 R(e,t)(zi).
</equation>
<bodyText confidence="0.984595333333333">
The set R(e,t)(z) contains all of the (e, ti-type
subexpressions of z. For each quantified vari-
able x in z, it also contains a function Ax.g. The
expression g contains the subexpressions in the
scope of x that do not have free variables. For
example, if z is
</bodyText>
<equation confidence="0.9936455">
Ay.∃x.flight(x) ∧ from(x, bos) ∧ to(x, phi)
∧ during(x, morning) ∧ aircraft(x) = y
</equation>
<bodyText confidence="0.6926295">
R(e,t)(z) would contain two functions: the entire
expression z and the function
</bodyText>
<equation confidence="0.944203">
Ax.flight(x) ∧ from(x, bos) ∧ to(x, phi)
∧ during(x, morning)
</equation>
<bodyText confidence="0.999342666666667">
constructed from the variable x, where the subex-
pression aircraft(x) = y has been removed be-
cause it contains the free variable y.
Elaboration Expressions Finally, E(z) is a set
of elaboration expressions constructed from a log-
ical form z. We define E(C) = �j_1
</bodyText>
<equation confidence="0.858162">
i=1 E(zi).
</equation>
<bodyText confidence="0.943028227272727">
E(z) is defined by enumerating the places
where embedded variables are found in z. For
each logical variable x and each coordination
(conjunction or disjunction) in the scope of x, a
new expression is created by defining a function
Af.z� where z� has the function f(x) added to the
appropriate coordination. This procedure would
6A lambda-calculus expression can be represented as a
tree structure with flat branching for coordination (conjunc-
tion and disjunction). The subexpressions are the subtrees.
In general, there will be many possible derivations
d for an input sentence w in the current context
C. In this section, we introduce a weighted lin-
ear model that scores derivations and a decoding
algorithm that finds high scoring analyses.
We define GEN(w; C) to be the set of possible
derivations d for an input sentence w given a con-
text C, as described in Section 5.2. Let O(d) E Rm
be an m-dimensional feature representation for a
derivation d and 0 E Rm be an m-dimensional pa-
rameter vector. The optimal derivation for a sen-
tence w given context C and parameters 0 is
</bodyText>
<equation confidence="0.995989">
d*(w; C) = arg max
dEGEN(w;C)
</equation>
<bodyText confidence="0.997536733333333">
Decoding We now describe an approximate al-
gorithm for computing d*(w; C).
The CCG parser uses a CKY-style chart parsing
algorithm that prunes to the top N = 50 entries
for each span in the chart.
We use a beam search procedure to find the
best contextual derivations, with beam size N =
50. The beam is initialized to the top N logi-
cal forms from the CCG parser. The derivations
are extended with reference and elaboration steps.
The only complication is selecting the sequence of
deletions. For each possible step, we use a greedy
search procedure that selects the sequence of dele-
tions that would maximize the score of the deriva-
tion after the step is applied.
</bodyText>
<sectionHeader confidence="0.962948" genericHeader="method">
7 Learning
</sectionHeader>
<bodyText confidence="0.996341727272727">
Figure 2 details the complete learning algorithm.
Training is online and error-driven. Step 1 parses
the current sentence in context. If the optimal logi-
cal form is not correct, Step 2 finds the best deriva-
tion that produces the labeled logical form7 and
does an additive, perceptron-style parameter up-
date. Step 3 updates the context. This algorithm is
a direct extension of the one introduced by Zettle-
moyer and Collins (2007). It maintains the context
but does not have the lexical induction step that
was previously used.
</bodyText>
<footnote confidence="0.991275333333333">
7For this computation, we use a modified version of the
beam search algorithm described in Section 6, which prunes
derivations that could not produce the desired logical form.
</footnote>
<equation confidence="0.557803">
0 · O(d)
</equation>
<page confidence="0.990702">
981
</page>
<bodyText confidence="0.9961678">
Inputs: Training examples {Ii|i = 1 ... n}. Each Ii is a
sequence {(wi,j, zi,j) : j = 1 ... ni} where wi,j is a
sentence and zi,j is a logical form. Number of training
iterations T. Initial parameters B.
Definitions: The function 0(d) represents the features de-
scribed in Section 8. GEN(w; C) is the set of deriva-
tions for sentence w in context C. GEN(w, z; C) is
the set of derivations for sentence w in context C that
produce the final logical form z. The function L(d)
maps a derivation to its associated final logical form.
</bodyText>
<sectionHeader confidence="0.308171" genericHeader="method">
Algorithm:
</sectionHeader>
<listItem confidence="0.911469416666667">
• For t = 1 ... T, i = 1 ... n: (Iterate interactions)
• Set C = {}. (Reset context)
• For j = 1 ... ni: (Iterate training examples)
Step 1: (Check correctness)
• Let d* = arg maxdEGEN(wi,j;C) B • 0(d) .
• If L(d*) = zi,j, go to Step 3.
Step 2: (Update parameters)
• Let d&apos; = arg maxdEGEN(wi,j,zi,j;C) B • 0(d) .
• Set B = B + 0(d&apos;) − 0(d*) .
Step 3: (Update context)
• Append zi,j to the current context C.
Output: Estimated parameters B.
</listItem>
<figureCaption confidence="0.982257">
Figure 2: An online learning algorithm.
</figureCaption>
<sectionHeader confidence="0.996921" genericHeader="method">
8 Features
</sectionHeader>
<bodyText confidence="0.9998865">
We now describe the features for both the parsing
and context resolution stages of the derivation.
</bodyText>
<subsectionHeader confidence="0.999733">
8.1 Parsing Features
</subsectionHeader>
<bodyText confidence="0.9999355">
The parsing features are used to score the context-
independent CCG parses during the first stage of
analysis. We use the set developed by Zettlemoyer
and Collins (2007), which includes features that
are sensitive to lexical choices and the structure of
the logical form that is constructed.
</bodyText>
<subsectionHeader confidence="0.999569">
8.2 Context Features
</subsectionHeader>
<bodyText confidence="0.99993103125">
The context features are functions of the deriva-
tion steps described in Section 5.2. In a deriva-
tion for sentence j of an interaction, let l be the
input logical form when considering a new step s
(a reference or elaboration step). Let c be the ex-
pression that s selects from a context set Re(zi),
R(e�t)(zi), or E(zi), where zi, i &lt; j, is an ex-
pression in the current context. Also, let r be a
subexpression deleted from c. Finally, let f1 and
f2 be predicates, for example from or to.
Distance Features The distance features are bi-
nary indicators on the distance j − i. These fea-
tures allow the model to, for example, favor re-
solving references with lambda-calculus expres-
sions recovered from recent sentences.
Copy Features For each possible f1 there is a
feature that tests if f1 is present in the context
expression c but not in the current expression l.
These features allow the model to learn to select
expressions from the context that introduce ex-
pected predicates. For example, flights usually
have a from predicate in the current expression.
Deletion Features For each pair (f1, f2) there
is a feature that tests if f1 is in the current expres-
sion l and f2 is in the deleted expression r. For
example, if f1 = f2 = days the model can favor
overriding old constraints about the departure day
with new ones introduced in the current utterance.
When f1 = during and f2 = depart time the
algorithm can learn that specific constraints on the
departure time override more general constraints
about the period of day.
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="method">
9 Related Work
</sectionHeader>
<bodyText confidence="0.999872689655173">
There has been a significant amount of work on
the problem of learning context-independent map-
pings from sentences to meaning representations.
Researchers have developed approaches using
models and algorithms from statistical machine
translation (Papineni et al., 1997; Ramaswamy
and Kleindienst, 2000; Wong and Mooney, 2007),
statistical parsing (Miller et al., 1996; Ge and
Mooney, 2005), inductive logic programming
(Zelle and Mooney, 1996; Tang and Mooney,
2000) and probabilistic push-down automata (He
and Young, 2006).
There were a large number of successful hand-
engineered systems developed for the original
ATIS task and other related tasks (e.g., (Carbonell
and Hayes, 1983; Seneff, 1992; Ward and Is-
sar, 1994; Levin et al., 2000; Popescu et al.,
2004)). We are only aware of one system that
learns to construct context-dependent interpreta-
tions (Miller et al., 1996). The Miller et al. (1996)
approach is fully supervised and produces a fi-
nal meaning representation in SQL. It requires
complete annotation of all of the syntactic, se-
mantic, and discourse decisions required to cor-
rectly analyze each training example. In contrast,
we learn from examples annotated with lambda-
calculus expressions that represent only the final,
context-dependent logical forms.
Finally, the CCG (Steedman, 1996; Steedman,
</bodyText>
<page confidence="0.988495">
982
</page>
<table confidence="0.990509333333333">
Train Dev. Test All
Interactions 300 99 127 526
Sentences 2956 857 826 4637
</table>
<tableCaption confidence="0.955147666666667">
Table 1: Statistics of the ATIS training, development and
test (DEC94) sets, including the total number of interactions
and sentences. Each interaction is a sequence of sentences.
</tableCaption>
<bodyText confidence="0.9932698">
2000) parsing setup is closely related to previous
CCG research, including work on learning parsing
models (Clark and Curran, 2003), wide-coverage
semantic parsing (Bos et al., 2004) and grammar
induction (Watkinson and Manandhar, 1999).
</bodyText>
<sectionHeader confidence="0.995287" genericHeader="evaluation">
10 Evaluation
</sectionHeader>
<bodyText confidence="0.998381162162162">
Data In this section, we present experiments in
the context-dependent ATIS domain (Dahl et al.,
1994). Table 1 presents statistics for the train-
ing, development, and test sets. To facilitate com-
parison with previous work, we used the standard
DEC94 test set. We randomly split the remaining
data to make training and development sets. We
manually converted the original SQL meaning an-
notations to lambda-calculus expressions.
Evaluation Metrics Miller et al. (1996) report
accuracy rates for recovering correct SQL annota-
tions on the test set. For comparison, we report ex-
act accuracy rates for recovering completely cor-
rect lambda-calculus expressions.
We also present precision, recall and F-measure
for partial match results that test if individual at-
tributes, such as the from and to cities, are cor-
rectly assigned. See the discussion by Zettlemoyer
and Collins (2007) (ZC07) for the full details.
Initialization and Parameters The CCG lexi-
con is hand engineered. We constructed it by run-
ning the ZC07 algorithm to learn a lexicon on
the context-independent ATIS data set and making
manual corrections to improve performance on the
training set. We also added lexical items with ref-
erence expressions, as described in Section 4.
We ran the learning algorithm for T = 4 train-
ing iterations. The parsing feature weights were
initialized as in ZC07, the context distance fea-
tures were given small negative weights, and all
other feature weights were initially set to zero.
Test Setup During evaluation, the context C =
{z1 ... zj_11 contains the logical forms output by
the learned system for the previous sentences. In
general, errors made while constructing these ex-
pressions can propogate if they are used in deriva-
tions for new sentences.
</bodyText>
<table confidence="0.9983345">
System Partial Match Exact
Prec. Rec. F1 Acc.
Full Method 95.0 96.5 95.7 83.7
Miller et al. – – – 78.4
</table>
<tableCaption confidence="0.98363">
Table 2: Performance on the ATIS DEC94 test set.
</tableCaption>
<table confidence="0.999444875">
Limited Context Partial Match Exact
Prec. Rec. F1 Acc.
M = 0 96.2 57.3 71.8 45.4
M = 1 94.9 91.6 93.2 79.8
M = 2 94.8 93.2 94.0 81.0
M = 3 94.5 94.3 94.4 82.1
M = 4 94.9 92.9 93.9 81.6
M = 10 94.2 94.0 94.1 81.4
</table>
<tableCaption confidence="0.9854145">
Table 3: Performance on the ATIS development set for
varying context window lengths M.
</tableCaption>
<bodyText confidence="0.99918584">
Results Table 2 shows performance on the ATIS
DEC94 test set. Our approach correctly recov-
ers 83.7% of the logical forms. This result com-
pares favorably to Miller et al.’s fully-supervised
approach (1996) while requiring significantly less
annotation effort.
We also evaluated performance when the con-
text is limited to contain only the M most recent
logical forms. Table 3 shows results on the devel-
opment set for different values of M. The poor
performance with no context (M = 0) demon-
strates the need for context-dependent analysis.
Limiting the context to the most recent statement
(M = 1) significantly improves performance
while using the last three utterances (M = 3) pro-
vides the best results.
Finally, we evaluated a variation where the con-
text contains gold-standard logical forms during
evaluation instead of the output of the learned
model. On the development set, this approach
achieved 85.5% exact-match accuracy, an im-
provement of approximately 3% over the standard
approach. This result suggests that incorrect log-
ical forms in the context have a relatively limited
impact on overall performance.
</bodyText>
<sectionHeader confidence="0.984124" genericHeader="conclusions">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.99990475">
In this paper, we addressed the problem of
learning context-dependent mappings from sen-
tences to logical form. We developed a context-
dependent analysis model and showed that it can
be effectively trained with a hidden-variable vari-
ant of the perceptron algorithm. In the experi-
ments, we showed that the approach recovers fully
correct logical forms with 83.7% accuracy.
</bodyText>
<page confidence="0.998249">
983
</page>
<sectionHeader confidence="0.996411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719853211009">
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the International Confer-
ence on Computational Linguistics.
Jaime G. Carbonell and Philip J. Hayes. 1983. Re-
covery strategies for parsing extragrammatical lan-
guage. American Journal of Computational Lin-
guistics, 9.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In ARPA HLT Workshop.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the Conference on Com-
putational Natural Language Learning.
Yulan He and Steve Young. 2006. Spoken language
understanding using the hidden vector state model.
Speech Communication, 48(3-4).
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic “unification-based” grammars. In Proc.
of the Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker.
2000. The AT&amp;T darpa communicator mixed-
initiative spoken dialogue system. In Proceedings of
the International Conference on Spoken Language
Processing.
Scott Miller, David Stallard, Robert J. Bobrow, and
Richard L. Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Proc. of
the Association for Computational Linguistics.
K. A. Papineni, S. Roukos, and T. R. Ward. 1997.
Feature-based language understanding. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
natural language interfaces to databases: Composing
statistical parsing with semantic tractability. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics.
Ganesh N. Ramaswamy and Jan Kleindienst. 2000.
Hierarchical feature-based translation for scalable
natural language understanding. In Proceedings of
International Conference on Spoken Language Pro-
cessing.
Stephanie Seneff. 1992. Robust parsing for spoken
language systems. In Proc. of the IEEE Conference
on Acoustics, Speech, and Signal Processing.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Lappoon R. Tang and Raymond J. Mooney. 2000.
Automated construction of database interfaces: In-
tegrating statistical and relational learning for se-
mantic parsing. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Wayne Ward and Sunil Issar. 1994. Recent improve-
ments in the CMU spoken language understanding
system. In Proceedings of the workshop on Human
Language Technology.
Stephen Watkinson and Suresh Manandhar. 1999. Un-
supervised lexical learning with categorial gram-
mars using the LLL corpus. In Proceedings of the
1st Workshop on Learning Language in Logic.
Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the Asso-
ciation for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proc. of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
</reference>
<page confidence="0.99845">
984
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852246">
<title confidence="0.999892">Learning Context-Dependent Mappings from Sentences to Logical Form</title>
<author confidence="0.99657">S Zettlemoyer Collins</author>
<affiliation confidence="0.999158">MIT CSAIL</affiliation>
<address confidence="0.999965">Cambridge, MA 02139</address>
<abstract confidence="0.990562055555556">We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Widecoverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="29096" citStr="Bos et al., 2004" startWordPosition="4926" endWordPosition="4929">, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, 982 Train Dev. Test All Interactions 300 99 127 526 Sentences 2956 857 826 4637 Table 1: Statistics of the ATIS training, development and test (DEC94) sets, including the total number of interactions and sentences. Each interaction is a sequence of sentences. 2000) parsing setup is closely related to previous CCG research, including work on learning parsing models (Clark and Curran, 2003), wide-coverage semantic parsing (Bos et al., 2004) and grammar induction (Watkinson and Manandhar, 1999). 10 Evaluation Data In this section, we present experiments in the context-dependent ATIS domain (Dahl et al., 1994). Table 1 presents statistics for the training, development, and test sets. To facilitate comparison with previous work, we used the standard DEC94 test set. We randomly split the remaining data to make training and development sets. We manually converted the original SQL meaning annotations to lambda-calculus expressions. Evaluation Metrics Miller et al. (1996) report accuracy rates for recovering correct SQL annotations on </context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Widecoverage semantic representations from a CCG parser. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Philip J Hayes</author>
</authors>
<title>Recovery strategies for parsing extragrammatical language.</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>9</volume>
<contexts>
<context position="28022" citStr="Carbonell and Hayes, 1983" startWordPosition="4759" endWordPosition="4762"> of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the</context>
</contexts>
<marker>Carbonell, Hayes, 1983</marker>
<rawString>Jaime G. Carbonell and Philip J. Hayes. 1983. Recovery strategies for parsing extragrammatical language. American Journal of Computational Linguistics, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Log-linear models for wide-coverage CCG parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="29045" citStr="Clark and Curran, 2003" startWordPosition="4919" endWordPosition="4922">d to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, 982 Train Dev. Test All Interactions 300 99 127 526 Sentences 2956 857 826 4637 Table 1: Statistics of the ATIS training, development and test (DEC94) sets, including the total number of interactions and sentences. Each interaction is a sequence of sentences. 2000) parsing setup is closely related to previous CCG research, including work on learning parsing models (Clark and Curran, 2003), wide-coverage semantic parsing (Bos et al., 2004) and grammar induction (Watkinson and Manandhar, 1999). 10 Evaluation Data In this section, we present experiments in the context-dependent ATIS domain (Dahl et al., 1994). Table 1 presents statistics for the training, development, and test sets. To facilitate comparison with previous work, we used the standard DEC94 test set. We randomly split the remaining data to make training and development sets. We manually converted the original SQL meaning annotations to lambda-calculus expressions. Evaluation Metrics Miller et al. (1996) report accura</context>
</contexts>
<marker>Clark, Curran, 2003</marker>
<rawString>Stephen Clark and James R. Curran. 2003. Log-linear models for wide-coverage CCG parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8588" citStr="Collins, 2002" startWordPosition="1422" endWordPosition="1423">hat learn from context-independent data (Zettlemoyer and Collins, 2005). 977 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms 7r for wj and many ways of modifying each 7r to create a final logical form zj. Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form 7r and all of the steps that map 7r to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examples. Evaluation We evaluate the approa</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah A Dahl</author>
<author>Madeleine Bates</author>
<author>Michael Brown</author>
<author>William Fisher</author>
<author>Kate Hunicke-Smith</author>
<author>David Pallett</author>
<author>Christine Pao</author>
<author>Alexander Rudnicky</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Expanding the scope of the ATIS task: the ATIS-3 corpus.</title>
<date>1994</date>
<booktitle>In ARPA HLT Workshop.</booktitle>
<contexts>
<context position="2177" citStr="Dahl et al., 1994" startWordPosition="336" endWordPosition="339">h they appear. For example, consider an interaction where Sent. 1 is followed by the sentence: Sent. 2: Show me the flights after 3pm. LF 2: ax.flight(x) ∧ to(x, bos) ∧day(x, fri) ∧ depart(x) &gt; 1500 In this case, the fact that Sent. 2 describes flights to Boston on Friday must be determined based on the context established by the first sentence. We introduce a supervised, hidden-variable approach for learning to interpret sentences in context. Each training example is a sequence of sentences annotated with logical forms. Figure 1 shows excerpts from three training examples in the ATIS corpus (Dahl et al., 1994). For context-dependent analysis, we develop an approach that maintains explicit, lambda-calculus representations of salient discourse entities and uses a two-stage pipeline to construct contextdependent logical forms. The first stage uses a probabilistic Combinatory Categorial Grammar (CCG) parsing algorithm to produce a contextindependent, underspecified meaning representation. The second stage resolves this underspecified meaning representation by making a sequence of modifications to it that depend on the context provided by previous utterances. In general, there are a large number of poss</context>
<context position="29267" citStr="Dahl et al., 1994" startWordPosition="4951" endWordPosition="4954">man, 982 Train Dev. Test All Interactions 300 99 127 526 Sentences 2956 857 826 4637 Table 1: Statistics of the ATIS training, development and test (DEC94) sets, including the total number of interactions and sentences. Each interaction is a sequence of sentences. 2000) parsing setup is closely related to previous CCG research, including work on learning parsing models (Clark and Curran, 2003), wide-coverage semantic parsing (Bos et al., 2004) and grammar induction (Watkinson and Manandhar, 1999). 10 Evaluation Data In this section, we present experiments in the context-dependent ATIS domain (Dahl et al., 1994). Table 1 presents statistics for the training, development, and test sets. To facilitate comparison with previous work, we used the standard DEC94 test set. We randomly split the remaining data to make training and development sets. We manually converted the original SQL meaning annotations to lambda-calculus expressions. Evaluation Metrics Miller et al. (1996) report accuracy rates for recovering correct SQL annotations on the test set. For comparison, we report exact accuracy rates for recovering completely correct lambda-calculus expressions. We also present precision, recall and F-measure</context>
</contexts>
<marker>Dahl, Bates, Brown, Fisher, Hunicke-Smith, Pallett, Pao, Rudnicky, Shriberg, 1994</marker>
<rawString>Deborah A. Dahl, Madeleine Bates, Michael Brown, William Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao, Alexander Rudnicky, and Elizabeth Shriberg. 1994. Expanding the scope of the ATIS task: the ATIS-3 corpus. In ARPA HLT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="27728" citStr="Ge and Mooney, 2005" startWordPosition="4715" endWordPosition="4718"> new ones introduced in the current utterance. When f1 = during and f2 = depart time the algorithm can learn that specific constraints on the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It r</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Steve Young</author>
</authors>
<title>Spoken language understanding using the hidden vector state model.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<pages>48--3</pages>
<contexts>
<context position="1005" citStr="He and Young, 2006" startWordPosition="137" endWordPosition="140">n algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy. 1 Introduction Recently, researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning (He and Young, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005). For instance, a training example might be: Sent. 1: List flights to Boston on Friday night. LF 1: ax.flight(x) ∧ to(x, bos) ∧ day(x, fri) ∧ during(x, night) Here the logical form (LF) is a lambda-calculus expression defining a set of entities that are flights to Boston departing on Friday night. Most of this work has focused on analyzing sentences in isolation. In this paper, we consider the problem of learning to interpret sentences whose underlying meanings can depend on the context in which they appear. For example, consider an intera</context>
<context position="27863" citStr="He and Young, 2006" startWordPosition="4734" endWordPosition="4737">n the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training exam</context>
</contexts>
<marker>He, Young, 2006</marker>
<rawString>Yulan He and Steve Young. 2006. Spoken language understanding using the hidden vector state model. Speech Communication, 48(3-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Steven Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In Proc. of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8550" citStr="Johnson et al., 1999" startWordPosition="1414" endWordPosition="1417">e work. We could possibly extend algorithms that learn from context-independent data (Zettlemoyer and Collins, 2005). 977 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms 7r for wj and many ways of modifying each 7r to create a final logical form zj. Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form 7r and all of the steps that map 7r to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examp</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In Proc. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="8573" citStr="Lafferty et al., 2001" startWordPosition="1418" endWordPosition="1421">bly extend algorithms that learn from context-independent data (Zettlemoyer and Collins, 2005). 977 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms 7r for wj and many ways of modifying each 7r to create a final logical form zj. Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form 7r and all of the steps that map 7r to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examples. Evaluation We eval</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>S Narayanan</author>
<author>R Pieraccini</author>
<author>K Biatov</author>
<author>E Bocchieri</author>
<author>G Di Fabbrizio</author>
<author>W Eckert</author>
<author>S Lee</author>
<author>A Pokrovsky</author>
<author>M Rahim</author>
<author>P Ruscitti</author>
<author>M Walker</author>
</authors>
<title>The AT&amp;T darpa communicator mixedinitiative spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<marker>Levin, Narayanan, Pieraccini, Biatov, Bocchieri, Di Fabbrizio, Eckert, Lee, Pokrovsky, Rahim, Ruscitti, Walker, 2000</marker>
<rawString>E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and M. Walker. 2000. The AT&amp;T darpa communicator mixedinitiative spoken dialogue system. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>David Stallard</author>
<author>Robert J Bobrow</author>
<author>Richard L Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proc. of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27706" citStr="Miller et al., 1996" startWordPosition="4711" endWordPosition="4714">he departure day with new ones introduced in the current utterance. When f1 = during and f2 = depart time the algorithm can learn that specific constraints on the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning repre</context>
<context position="29631" citStr="Miller et al. (1996)" startWordPosition="5007" endWordPosition="5010">ng models (Clark and Curran, 2003), wide-coverage semantic parsing (Bos et al., 2004) and grammar induction (Watkinson and Manandhar, 1999). 10 Evaluation Data In this section, we present experiments in the context-dependent ATIS domain (Dahl et al., 1994). Table 1 presents statistics for the training, development, and test sets. To facilitate comparison with previous work, we used the standard DEC94 test set. We randomly split the remaining data to make training and development sets. We manually converted the original SQL meaning annotations to lambda-calculus expressions. Evaluation Metrics Miller et al. (1996) report accuracy rates for recovering correct SQL annotations on the test set. For comparison, we report exact accuracy rates for recovering completely correct lambda-calculus expressions. We also present precision, recall and F-measure for partial match results that test if individual attributes, such as the from and to cities, are correctly assigned. See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. Initialization and Parameters The CCG lexicon is hand engineered. We constructed it by running the ZC07 algorithm to learn a lexicon on the context-independent ATI</context>
</contexts>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>Scott Miller, David Stallard, Robert J. Bobrow, and Richard L. Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proc. of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>T R Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="27607" citStr="Papineni et al., 1997" startWordPosition="4697" endWordPosition="4700">d expression r. For example, if f1 = f2 = days the model can favor overriding old constraints about the departure day with new ones introduced in the current utterance. When f1 = during and f2 = depart time the algorithm can learn that specific constraints on the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et a</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1997</marker>
<rawString>K. A. Papineni, S. Roukos, and T. R. Ward. 1997. Feature-based language understanding. In Proceedings of European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Alex Armanasu</author>
<author>Oren Etzioni</author>
<author>David Ko</author>
<author>Alexander Yates</author>
</authors>
<title>Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="28101" citStr="Popescu et al., 2004" startWordPosition="4774" endWordPosition="4777">. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, 982 Train Dev. Test All Interactions 300 99 127</context>
</contexts>
<marker>Popescu, Armanasu, Etzioni, Ko, Yates, 2004</marker>
<rawString>Ana-Maria Popescu, Alex Armanasu, Oren Etzioni, David Ko, and Alexander Yates. 2004. Modern natural language interfaces to databases: Composing statistical parsing with semantic tractability. In Proceedings of the International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ganesh N Ramaswamy</author>
<author>Jan Kleindienst</author>
</authors>
<title>Hierarchical feature-based translation for scalable natural language understanding.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="27640" citStr="Ramaswamy and Kleindienst, 2000" startWordPosition="4701" endWordPosition="4704">mple, if f1 = f2 = days the model can favor overriding old constraints about the departure day with new ones introduced in the current utterance. When f1 = during and f2 = depart time the algorithm can learn that specific constraints on the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (199</context>
</contexts>
<marker>Ramaswamy, Kleindienst, 2000</marker>
<rawString>Ganesh N. Ramaswamy and Jan Kleindienst. 2000. Hierarchical feature-based translation for scalable natural language understanding. In Proceedings of International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
</authors>
<title>Robust parsing for spoken language systems.</title>
<date>1992</date>
<booktitle>In Proc. of the IEEE Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="28036" citStr="Seneff, 1992" startWordPosition="4763" endWordPosition="4764">ndent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>Stephanie Seneff. 1992. Robust parsing for spoken language systems. In Proc. of the IEEE Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface Structure and Interpretation.</title>
<date>1996</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9876" citStr="Steedman, 1996" startWordPosition="1641" endWordPosition="1642">nstructs an output logical form zj which is compared to a gold standard annotation to check correctness. At step j, the context contains the previous zi, for i &lt; j, output by the system. 4 Context-independent Parsing In this section, we first briefly review the CCG parsing formalism. We then define a set of extensions that allow the parser to construct logical forms containing references, such as the !(e, t) expression from the example derivation in Section 3. 4.1 Background: CCG CCG is a lexicalized, mildly context-sensitive parsing formalism that models a wide range of linguistic phenomena (Steedman, 1996; Steedman, 2000). Parses are constructed by combining lexical entries according to a small set of relatively simple rules. For example, consider the lexicon flights :� N : ax.flight(x) to :� (N\N)/NP : Ay.Af.Ax.f(x) ∧ to(x, y) boston :� NP : boston Each lexical entry consists of a word and a category. Each category includes syntactic and semantic content. For example, the first entry pairs the word flights with the category N : Ax.flight(x). This category has syntactic type N, and includes the lambda-calculus semantic expression Ax.flight(x). In general, syntactic types can either be simple t</context>
<context position="12358" citStr="Steedman (1996" startWordPosition="2076" endWordPosition="2078">o a function. As motivated in Section 3, we introduce these expressions so they can later be replaced with appropriate lambda-calculus expressions from the context. Sometimes references are lexically triggered. For example, consider parsing the phrase “show me the ones that leave in the morning” from Example 1(b) in Figure 1. Given the lexical entry: ones := N : Ax.!(e, t)(x) a CCG parser could produce the desired context3In addition to application, we make use of composition, type raising and coordination combinators. A full description of these combinators is beyond the scope of this paper. Steedman (1996; 2000) presents a detailed description of CCG. 978 independent logical form: develop an approach that learns when to introduce references and how to best resolve them. Ax.!(e, t)(x) ∧ during(x, morning) Our first extension is to simply introduce lexical items that include references into the CCG lexicon. They describe anaphoric words, for example including “ones,” “those,” and “it.” In addition, we sometimes need to introduce references when there is no explicit lexical trigger. For instance, Example 2(c) in Figure 1 consists of the single word “cheapest.” This query has the same meaning as t</context>
<context position="28642" citStr="Steedman, 1996" startWordPosition="4858" endWordPosition="4859">ff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, 982 Train Dev. Test All Interactions 300 99 127 526 Sentences 2956 857 826 4637 Table 1: Statistics of the ATIS training, development and test (DEC94) sets, including the total number of interactions and sentences. Each interaction is a sequence of sentences. 2000) parsing setup is closely related to previous CCG research, including work on learning parsing models (Clark and Curran, 2003), wide-coverage semantic parsing (Bos et al., 2004) and grammar induction (Watkinson and Manandhar, 1999). 10 Evaluation Data In this section, we present experiments in the context-dependent ATIS d</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Mark Steedman. 1996. Surface Structure and Interpretation. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9893" citStr="Steedman, 2000" startWordPosition="1643" endWordPosition="1645">ut logical form zj which is compared to a gold standard annotation to check correctness. At step j, the context contains the previous zi, for i &lt; j, output by the system. 4 Context-independent Parsing In this section, we first briefly review the CCG parsing formalism. We then define a set of extensions that allow the parser to construct logical forms containing references, such as the !(e, t) expression from the example derivation in Section 3. 4.1 Background: CCG CCG is a lexicalized, mildly context-sensitive parsing formalism that models a wide range of linguistic phenomena (Steedman, 1996; Steedman, 2000). Parses are constructed by combining lexical entries according to a small set of relatively simple rules. For example, consider the lexicon flights :� N : ax.flight(x) to :� (N\N)/NP : Ay.Af.Ax.f(x) ∧ to(x, y) boston :� NP : boston Each lexical entry consists of a word and a category. Each category includes syntactic and semantic content. For example, the first entry pairs the word flights with the category N : Ax.flight(x). This category has syntactic type N, and includes the lambda-calculus semantic expression Ax.flight(x). In general, syntactic types can either be simple types such as N, N</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="27805" citStr="Tang and Mooney, 2000" startWordPosition="4726" endWordPosition="4729">part time the algorithm can learn that specific constraints on the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse </context>
</contexts>
<marker>Tang, Mooney, 2000</marker>
<rawString>Lappoon R. Tang and Raymond J. Mooney. 2000. Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Maxmargin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="8610" citStr="Taskar et al., 2004" startWordPosition="1424" endWordPosition="1427">context-independent data (Zettlemoyer and Collins, 2005). 977 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms 7r for wj and many ways of modifying each 7r to create a final logical form zj. Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with features for both the parsing and context resolution stages of the derivation. In our setting, the choice of the context-independent logical form 7r and all of the steps that map 7r to the output z are hidden variables; these steps are not annotated in the training data. To estimate the parameters of the model, we use a hidden-variable version of the perceptron algorithm. We use an approximate search procedure to find the best derivation both while training the model and while applying it to test examples. Evaluation We evaluate the approach on sequences of sen</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. 2004. Maxmargin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Ward</author>
<author>Sunil Issar</author>
</authors>
<title>Recent improvements in the CMU spoken language understanding system.</title>
<date>1994</date>
<booktitle>In Proceedings of the workshop on Human Language Technology.</booktitle>
<contexts>
<context position="28058" citStr="Ward and Issar, 1994" startWordPosition="4765" endWordPosition="4769"> from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, semantic, and discourse decisions required to correctly analyze each training example. In contrast, we learn from examples annotated with lambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, 982 </context>
</contexts>
<marker>Ward, Issar, 1994</marker>
<rawString>Wayne Ward and Sunil Issar. 1994. Recent improvements in the CMU spoken language understanding system. In Proceedings of the workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Watkinson</author>
<author>Suresh Manandhar</author>
</authors>
<title>Unsupervised lexical learning with categorial grammars using the LLL corpus.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1st Workshop on Learning Language in Logic.</booktitle>
<contexts>
<context position="29150" citStr="Watkinson and Manandhar, 1999" startWordPosition="4933" endWordPosition="4936">ambdacalculus expressions that represent only the final, context-dependent logical forms. Finally, the CCG (Steedman, 1996; Steedman, 982 Train Dev. Test All Interactions 300 99 127 526 Sentences 2956 857 826 4637 Table 1: Statistics of the ATIS training, development and test (DEC94) sets, including the total number of interactions and sentences. Each interaction is a sequence of sentences. 2000) parsing setup is closely related to previous CCG research, including work on learning parsing models (Clark and Curran, 2003), wide-coverage semantic parsing (Bos et al., 2004) and grammar induction (Watkinson and Manandhar, 1999). 10 Evaluation Data In this section, we present experiments in the context-dependent ATIS domain (Dahl et al., 1994). Table 1 presents statistics for the training, development, and test sets. To facilitate comparison with previous work, we used the standard DEC94 test set. We randomly split the remaining data to make training and development sets. We manually converted the original SQL meaning annotations to lambda-calculus expressions. Evaluation Metrics Miller et al. (1996) report accuracy rates for recovering correct SQL annotations on the test set. For comparison, we report exact accuracy</context>
</contexts>
<marker>Watkinson, Manandhar, 1999</marker>
<rawString>Stephen Watkinson and Suresh Manandhar. 1999. Unsupervised lexical learning with categorial grammars using the LLL corpus. In Proceedings of the 1st Workshop on Learning Language in Logic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1028" citStr="Wong and Mooney, 2007" startWordPosition="141" endWordPosition="144">ntains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy. 1 Introduction Recently, researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning (He and Young, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005). For instance, a training example might be: Sent. 1: List flights to Boston on Friday night. LF 1: ax.flight(x) ∧ to(x, bos) ∧ day(x, fri) ∧ during(x, night) Here the logical form (LF) is a lambda-calculus expression defining a set of entities that are flights to Boston departing on Friday night. Most of this work has focused on analyzing sentences in isolation. In this paper, we consider the problem of learning to interpret sentences whose underlying meanings can depend on the context in which they appear. For example, consider an interaction where Sent. 1 is </context>
<context position="27664" citStr="Wong and Mooney, 2007" startWordPosition="4705" endWordPosition="4708"> can favor overriding old constraints about the departure day with new ones introduced in the current utterance. When f1 = during and f2 = depart time the algorithm can learn that specific constraints on the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully sup</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="27781" citStr="Zelle and Mooney, 1996" startWordPosition="4722" endWordPosition="4725"> f1 = during and f2 = depart time the algorithm can learn that specific constraints on the departure time override more general constraints about the period of day. 9 Related Work There has been a significant amount of work on the problem of learning context-independent mappings from sentences to meaning representations. Researchers have developed approaches using models and algorithms from statistical machine translation (Papineni et al., 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2007), statistical parsing (Miller et al., 1996; Ge and Mooney, 2005), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000) and probabilistic push-down automata (He and Young, 2006). There were a large number of successful handengineered systems developed for the original ATIS task and other related tasks (e.g., (Carbonell and Hayes, 1983; Seneff, 1992; Ward and Issar, 1994; Levin et al., 2000; Popescu et al., 2004)). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al., 1996). The Miller et al. (1996) approach is fully supervised and produces a final meaning representation in SQL. It requires complete annotation of all of the syntactic, </context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="1060" citStr="Zettlemoyer and Collins, 2005" startWordPosition="145" endWordPosition="148">-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy. 1 Introduction Recently, researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning (He and Young, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005). For instance, a training example might be: Sent. 1: List flights to Boston on Friday night. LF 1: ax.flight(x) ∧ to(x, bos) ∧ day(x, fri) ∧ during(x, night) Here the logical form (LF) is a lambda-calculus expression defining a set of entities that are flights to Boston departing on Friday night. Most of this work has focused on analyzing sentences in isolation. In this paper, we consider the problem of learning to interpret sentences whose underlying meanings can depend on the context in which they appear. For example, consider an interaction where Sent. 1 is followed by the sentence: Sent. </context>
<context position="8046" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1328" endWordPosition="1331">xpression specifies that a lambda-calculus expression of type (e, t) must be recovered from the context and substituted in its place. In the second (contextually dependent) stage of the derivation, the expression Ax.flight(x) n from(x, bos) n to(x, phi) is recovered from the context, and substituted for the !(e, t) subexpression, producing the desired final logical form, seen in Example 1(b). 2Developing algorithms that learn the CCG lexicon from the data described in this paper is an important area for future work. We could possibly extend algorithms that learn from context-independent data (Zettlemoyer and Collins, 2005). 977 In addition to substitutions of this type, we will also perform other types of context-dependent resolution steps, as described in Section 5. In general, both of the stages of the derivation involve considerable ambiguity – there will be a large number of possible context-independent logical forms 7r for wj and many ways of modifying each 7r to create a final logical form zj. Learning We model the problem of selecting the best derivation as a structured prediction problem (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Taskar et al., 2004). We present a linear model with fea</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</booktitle>
<contexts>
<context position="24054" citStr="Zettlemoyer and Collins (2007)" startWordPosition="4064" endWordPosition="4068">uence of deletions. For each possible step, we use a greedy search procedure that selects the sequence of deletions that would maximize the score of the derivation after the step is applied. 7 Learning Figure 2 details the complete learning algorithm. Training is online and error-driven. Step 1 parses the current sentence in context. If the optimal logical form is not correct, Step 2 finds the best derivation that produces the labeled logical form7 and does an additive, perceptron-style parameter update. Step 3 updates the context. This algorithm is a direct extension of the one introduced by Zettlemoyer and Collins (2007). It maintains the context but does not have the lexical induction step that was previously used. 7For this computation, we use a modified version of the beam search algorithm described in Section 6, which prunes derivations that could not produce the desired logical form. 0 · O(d) 981 Inputs: Training examples {Ii|i = 1 ... n}. Each Ii is a sequence {(wi,j, zi,j) : j = 1 ... ni} where wi,j is a sentence and zi,j is a logical form. Number of training iterations T. Initial parameters B. Definitions: The function 0(d) represents the features described in Section 8. GEN(w; C) is the set of deriva</context>
<context position="25658" citStr="Zettlemoyer and Collins (2007)" startWordPosition="4362" endWordPosition="4365">correctness) • Let d* = arg maxdEGEN(wi,j;C) B • 0(d) . • If L(d*) = zi,j, go to Step 3. Step 2: (Update parameters) • Let d&apos; = arg maxdEGEN(wi,j,zi,j;C) B • 0(d) . • Set B = B + 0(d&apos;) − 0(d*) . Step 3: (Update context) • Append zi,j to the current context C. Output: Estimated parameters B. Figure 2: An online learning algorithm. 8 Features We now describe the features for both the parsing and context resolution stages of the derivation. 8.1 Parsing Features The parsing features are used to score the contextindependent CCG parses during the first stage of analysis. We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. 8.2 Context Features The context features are functions of the derivation steps described in Section 5.2. In a derivation for sentence j of an interaction, let l be the input logical form when considering a new step s (a reference or elaboration step). Let c be the expression that s selects from a context set Re(zi), R(e�t)(zi), or E(zi), where zi, i &lt; j, is an expression in the current context. Also, let r be a subexpression deleted from c. Finally, let f1 and f2 be predi</context>
<context position="30038" citStr="Zettlemoyer and Collins (2007)" startWordPosition="5071" endWordPosition="5074"> DEC94 test set. We randomly split the remaining data to make training and development sets. We manually converted the original SQL meaning annotations to lambda-calculus expressions. Evaluation Metrics Miller et al. (1996) report accuracy rates for recovering correct SQL annotations on the test set. For comparison, we report exact accuracy rates for recovering completely correct lambda-calculus expressions. We also present precision, recall and F-measure for partial match results that test if individual attributes, such as the from and to cities, are correctly assigned. See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. Initialization and Parameters The CCG lexicon is hand engineered. We constructed it by running the ZC07 algorithm to learn a lexicon on the context-independent ATIS data set and making manual corrections to improve performance on the training set. We also added lexical items with reference expressions, as described in Section 4. We ran the learning algorithm for T = 4 training iterations. The parsing feature weights were initialized as in ZC07, the context distance features were given small negative weights, and all other feature weights were initially set to zero</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>