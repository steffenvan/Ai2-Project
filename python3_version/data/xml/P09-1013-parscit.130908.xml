<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985955">
Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples
</title>
<author confidence="0.99919">
Suma Bhat Richard Sproat
</author>
<affiliation confidence="0.9992415">
Department of ECE Center for Spoken Language Understanding
University of Illinois Oregon Health &amp; Science University
</affiliation>
<email confidence="0.995937">
spbhat2@illinois.edu rws@xoba.com
</email>
<sectionHeader confidence="0.99381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918615384615">
Empirical studies on corpora involve mak-
ing measurements of several quantities for
the purpose of comparing corpora, creat-
ing language models or to make general-
izations about specific linguistic phenom-
ena in a language. Quantities such as av-
erage word length are stable across sam-
ple sizes and hence can be reliably esti-
mated from large enough samples. How-
ever, quantities such as vocabulary size
change with sample size. Thus measure-
ments based on a given sample will need
to be extrapolated to obtain their estimates
over larger unseen samples. In this work,
we propose a novel nonparametric estima-
tor of vocabulary size. Our main result is
to show the statistical consistency of the
estimator – the first of its kind in the lit-
erature. Finally, we compare our proposal
with the state of the art estimators (both
parametric and nonparametric) on large
standard corpora; apart from showing the
favorable performance of our estimator,
we also see that the classical Good-Turing
estimator consistently underestimates the
vocabulary size.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981729166667">
Empirical studies on corpora involve making mea-
surements of several quantities for the purpose of
comparing corpora, creating language models or
to make generalizations about specific linguistic
phenomena in a language. Quantities such as av-
erage word length or average sentence length are
stable across sample sizes. Hence empirical mea-
surements from large enough samples tend to be
reliable for even larger sample sizes. On the other
hand, quantities associated with word frequencies,
such as the number of hapax legomena or the num-
ber of distinct word types changes are strictly sam-
ple size dependent. Given a sample we can ob-
tain the seen vocabulary and the seen number of
hapax legomena. However, for the purpose of
comparison of corpora of different sizes or lin-
guistic phenomena based on samples of different
sizes it is imperative that these quantities be com-
pared based on similar sample sizes. We thus need
methods to extrapolate empirical measurements of
these quantities to arbitrary sample sizes.
Our focus in this study will be estimators of
vocabulary size for samples larger than the sam-
ple available. There is an abundance of estima-
tors of population size (in our case, vocabulary
size) in existing literature. Excellent survey arti-
cles that summarize the state-of-the-art are avail-
able in (Bunge and Fitzpatrick, 1993) and (Gan-
dolfi and Sastri, 2004). Of particular interest to
us is the set of estimators that have been shown
to model word frequency distributions well. This
study proposes a nonparametric estimator of vo-
cabulary size and evaluates its theoretical and em-
pirical performance. For comparison we consider
some state-of-the-art parametric and nonparamet-
ric estimators of vocabulary size.
The proposed non-parametric estimator for the
number of unseen elements assumes a regime
characterizing word frequency distributions. This
work is motivated by a scaling formulation to ad-
dress the problem of unlikely events proposed in
(Baayen, 2001; Khmaladze, 1987; Khmaladze and
Chitashvili, 1989; Wagner et al., 2006). We also
demonstrate that the estimator is strongly consis-
tent under the natural scaling formulation. While
compared with other vocabulary size estimates,
we see that our estimator performs at least as well
as some of the state of the art estimators.
</bodyText>
<sectionHeader confidence="0.996878" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.978853">
Many estimators of vocabulary size are available
in the literature and a comparison of several non
</bodyText>
<page confidence="0.98702">
109
</page>
<note confidence="0.9996125">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109–117,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9956574">
parametric estimators of population size occurs in
(Gandolfi and Sastri, 2004). While a definite com-
parison including parametric estimators is lacking,
there is also no known work comparing methods
of extrapolation of vocabulary size. Baroni and
Evert, in (Baroni and Evert, 2005), evaluate the
performance of some estimators in extrapolating
vocabulary size for arbitrary sample sizes but limit
the study to parametric estimators. Since we con-
sider both parametric and nonparametric estima-
tors here, we consider this to be the first study
comparing a set of estimators for extrapolating vo-
cabulary size.
Estimators of vocabulary size that we compare
can be broadly classified into two types:
</bodyText>
<listItem confidence="0.8423578">
1. Nonparametric estimators- here word fre-
quency information from the given sample
alone is used to estimate the vocabulary size.
A good survey of the state of the art is avail-
able in (Gandolfi and Sastri, 2004). In this
paper, we compare our proposed estimator
with the canonical estimators available in
(Gandolfi and Sastri, 2004).
2. Parametric estimators- here a probabilistic
model capturing the relation between ex-
pected vocabulary size and sample size is the
estimator. Given a sample of size n, the
sample serves to calculate the parameters of
the model. The expected vocabulary for a
given sample size is then determined using
the explicit relation. The parametric esti-
mators considered in this study are (Baayen,
2001; Baroni and Evert, 2005),
(a) Zipf-Mandelbrot estimator (ZM);
(b) finite Zipf-Mandelbrot estimator (fZM).
</listItem>
<bodyText confidence="0.9990402">
In addition to the above estimators we consider
a novel non parametric estimator. It is the nonpara-
metric estimator that we propose, taking into ac-
count the characteristic feature of word frequency
distributions, to which we will turn next.
</bodyText>
<sectionHeader confidence="0.973629" genericHeader="method">
3 Novel Estimator of Vocabulary size
</sectionHeader>
<bodyText confidence="0.9997558">
We observe (Xi, ... , Xn), an i.i.d. sequence
drawn according to a probability distribution P
from a large, but finite, vocabulary Q. Our goal
is in estimating the “essential” size of the vocabu-
lary Q using only the observations. In other words,
having seen a sample of size n we wish to know,
given another sample from the same population,
how many unseen elements we would expect to
see. Our nonparametric estimator for the number
of unseen elements is motivated by the character-
istic property of word frequency distributions, the
Large Number of Rare Events (LNRE) (Baayen,
2001). We also demonstrate that the estimator is
strongly consistent under a natural scaling formu-
lation described in (Khmaladze, 1987).
</bodyText>
<subsectionHeader confidence="0.999672">
3.1 A Scaling Formulation
</subsectionHeader>
<bodyText confidence="0.982313263157895">
Our main interest is in probability distributions P
with the property that a large number of words in
the vocabulary Q are unlikely, i.e., the chance any
word appears eventually in an arbitrarily long ob-
servation is strictly between 0 and 1. The authors
in (Baayen, 2001; Khmaladze and Chitashvili,
1989; Wagner et al., 2006) propose a natural scal-
ing formulation to study this problem; specifically,
(Baayen, 2001) has a tutorial-like summary of the
theoretical work in (Khmaladze, 1987; Khmaladze
and Chitashvili, 1989). In particular, the authors
consider a sequence of vocabulary sets and prob-
ability distributions, indexed by the observation
size n. Specifically, the observation (Xi, ... , Xn)
is drawn i.i.d. from a vocabulary Qn according to
probability Pn. If the probability of a word, say
w E Qn is p, then the probability that this specific
word w does not occur in an observation of size n
is
</bodyText>
<equation confidence="0.875918">
(1 − p)n .
</equation>
<bodyText confidence="0.982975333333333">
For w to be an unlikely word, we would like this
probability for large n to remain strictly between
0 and 1. This implies that
</bodyText>
<equation confidence="0.960136">
, (1)
</equation>
<bodyText confidence="0.9998744">
for some strictly positive constants 0 &lt; c� &lt; c� &lt;
oc. We will assume throughout this paper that c�
and c� are the same for every word w E Qn. This
implies that the vocabulary size is growing lin-
early with the observation size:
</bodyText>
<equation confidence="0.453938666666667">
n
&lt; |Qn |&lt; .
c
</equation>
<bodyText confidence="0.997656333333333">
This model is called the LNRE zone and its appli-
cability in natural language corpora is studied in
detail in (Baayen, 2001).
</bodyText>
<subsectionHeader confidence="0.996503">
3.2 Shadows
</subsectionHeader>
<bodyText confidence="0.986683">
Consider the observation string (Xi, ... , Xn) and
let us denote the quantity of interest – the number
</bodyText>
<equation confidence="0.985784">
c� c�
n &lt; p &lt; n
n
c�
</equation>
<page confidence="0.984826">
110
</page>
<bodyText confidence="0.999494263157895">
of word types in the vocabulary Ωn that are not
observed – by On. This quantity is random since
the observation string itself is. However, we note
that the distribution of On is unaffected if one re-
labels the words in Ωn. This motivates studying
of the probabilities assigned by Pn without refer-
ence to the labeling of the word; this is done in
(Khmaladze and Chitashvili, 1989) via the struc-
tural distribution function and in (Wagner et al.,
2006) via the shadow. Here we focus on the latter
description:
Definition 1 Let Xn be a random variable on Ωn
with distribution Pn. The shadow of Pn is de-
fined to be the distribution of the random variable
Pn({Xn}).
For the finite vocabulary situation we are con-
sidering, specifying the shadow is exactly equiv-
alent to specifying the unordered components of
Pn, viewed as a probability vector.
</bodyText>
<subsectionHeader confidence="0.998951">
3.3 Scaled Shadows Converge
</subsectionHeader>
<bodyText confidence="0.99999525">
We will follow (Wagner et al., 2006) and sup-
pose that the scaled shadows, the distribution of
n · Pn(Xn), denoted by Qn converge to a distribu-
tion Q. As an example, if Pn is a uniform distribu-
tion over a vocabulary of size cn, then n · Pn(Xn)
equals 1 c almost surely for each n (and hence it
converges in distribution). From this convergence
assumption we can, further, infer the following:
</bodyText>
<listItem confidence="0.981229">
1. Since the probability of each word ω is lower
and upper bounded as in Equation (1), we
know that the distribution Qn is non-zero
only in the range [ˇc, ˆc].
2. The “essential” size of the vocabulary, i.e.,
the number of words of Ωn on which Pn
puts non-zero probability can be evaluated di-
rectly from the scaled shadow, scaled by 1n as
</listItem>
<equation confidence="0.876232333333333">
Zc�1 dQn(y). (2)
y
c�
</equation>
<bodyText confidence="0.9998645">
Using the dominated convergence theorem,
we can conclude that the convergence of the
scaled shadows guarantees that the size of the
vocabulary, scaled by 1/n, converges as well:
</bodyText>
<subsectionHeader confidence="0.98619">
3.4 Profiles and their Limits
</subsectionHeader>
<bodyText confidence="0.9990325">
Our goal in this paper is to estimate the size of the
underlying vocabulary, i.e., the expression in (2),
</bodyText>
<equation confidence="0.8931115">
Z c�n (4)
dQn (y)
y
c�
</equation>
<bodyText confidence="0.998699625">
from the observations (X1, ... , Xn). We observe
that since the scaled shadow Qn does not de-
pend on the labeling of the words in Ωn, a suf-
ficient statistic to estimate (4) from the observa-
tion (X1, ... , Xn) is the profile of the observation:
(ϕn1, ... , ϕnn), defined as follows. ϕnk is the num-
ber of word types that appear exactly k times in
the observation, for k = 1, ... , n. Observe that
</bodyText>
<equation confidence="0.939696666666667">
n
kϕnk = n,
k=1
</equation>
<bodyText confidence="0.64432">
and that
</bodyText>
<equation confidence="0.8548014">
ϕn (5)
k
is the number of observed words. Thus, the object
of our interest is,
On = |Ωn |− V. (6)
</equation>
<subsectionHeader confidence="0.998839">
3.5 Convergence of Scaled Profiles
</subsectionHeader>
<bodyText confidence="0.991498714285714">
One of the main results of (Wagner et al., 2006) is
that the scaled profiles converge to a deterministic
probability vector under the scaling model intro-
duced in Section 3.3. Specifically, we have from
Proposition 1 of (Wagner et al., 2006):
−→ 0, almost surely, (7)
where
</bodyText>
<equation confidence="0.997269">
λk := c k!
c yk exp(−y) dQ(y) k = 0, 1, 2, ... .
(8)
</equation>
<bodyText confidence="0.9997625">
This convergence result suggests a natural estima-
tor for On, expressed in Equation (6).
</bodyText>
<subsectionHeader confidence="0.986141">
3.6 A Consistent Estimator of On
</subsectionHeader>
<bodyText confidence="0.999984571428571">
We start with the limiting expression for scaled
profiles in Equation (7) and come up with a natu-
ral estimator for On. Our development leading to
the estimator is somewhat heuristic and is aimed
at motivating the structure of the estimator for the
number of unseen words, On. We formally state
and prove its consistency at the end of this section.
</bodyText>
<equation confidence="0.9208764">
�
fc 1
y dQ(y). (3)
|Ωn|
n
n
V def =
k=1
i
kϕk
− λk−1
n
����
n
k=1
</equation>
<page confidence="0.974685">
111
</page>
<bodyText confidence="0.7956005">
It helps to write the truncated geometric series as
a power series in y:
</bodyText>
<subsectionHeader confidence="0.433265">
3.6.1 A Heuristic Derivation
</subsectionHeader>
<bodyText confidence="0.784567">
Starting from (7), let us first make the approxima-
tion that
</bodyText>
<equation confidence="0.7107">
kϕk � λk−1, k = 1,... ,n. (9) 1 XM (1 − y �ℓ
cˆ ℓ=0 cˆ
n
</equation>
<bodyText confidence="0.7438995">
We now have the formal calculation
!
</bodyText>
<equation confidence="0.944834272727273">
N Ic
ˆe−yXn yk
dQ(y) (11) y k! k=1
ˆ
Z c e−y
� y (ey − 1) dQ(y) (12)
cˇ
Z cˆ
|Ωn |e−y
− y dQ(y). (13)
n cˇ
</equation>
<bodyText confidence="0.974440615384615">
Here the approximation in Equation (10) follows
from the approximation in Equation (9), the ap-
proximation in Equation (11) involves swapping
the outer discrete summation with integration and
is justified formally later in the section, the ap-
proximation in Equation (12) follows because
Xn
k=1
as n —* oc, and the approximation in Equa-
tion (13) is justified from the convergence in Equa-
tion (3). Now, comparing Equation (13) with
Equation (6), we arrive at an approximation for
our quantity of interest:
</bodyText>
<equation confidence="0.420286">
�
c e−y dQ(y). (14)
I
y
</equation>
<bodyText confidence="0.897466">
The geometric series allows us to write
</bodyText>
<sectionHeader confidence="0.488543" genericHeader="method">
(1 VP
</sectionHeader>
<bodyText confidence="0.83288975">
− , , by E (0, ˆc) . (15)
c
Approximating this infinite series by a finite sum-
mation, we have for all y E (ˇc, ˆc),
</bodyText>
<equation confidence="0.966895571428571">
ℓ ℓ
X ( � (−1)k �y�k
k cˆ
k=0
XM � ℓ �! (−1)k(y)k
k cˆ
ℓ=k
</equation>
<bodyText confidence="0.8290835">
(−1)k aMk yk, (17)
where we have written
</bodyText>
<equation confidence="0.776526">
XM ~ ℓ)! .
k
ℓ=k
</equation>
<bodyText confidence="0.9991242">
Substituting the finite summation approximation
in Equation 16 and its power series expression in
Equation (17) into Equation (14) and swapping the
discrete summation with the integral, we can con-
tinue
</bodyText>
<equation confidence="0.9705966">
Z cˆ
(−1)k aM e−yyk dQ(y)
k
cˇ
(−1)k aMk k!λk. (18)
</equation>
<bodyText confidence="0.999433">
Here, in Equation (18), we used the definition of
λk from Equation (8). From the convergence in
Equation (7), we finally arrive at our estimate:
</bodyText>
<equation confidence="0.792198">
(−1)k aMk (k + 1)! ϕk+1. (19)
</equation>
<subsectionHeader confidence="0.523054">
3.6.2 Consistency
</subsectionHeader>
<bodyText confidence="0.992582">
Our main result is the demonstration of the consis-
tency of the estimator in Equation (19).
</bodyText>
<figure confidence="0.914478841269841">
Theorem 1 For any ǫ &gt; 0,
On − PM~~~
k=0 (−1)k aMk (k + 1)! ϕk+1
n
λk−1 (10)
k
Xn Z cˆ e−yyk−1
k! dQ(y)
cˇ
Xn
k=1
k =1
�
ϕn
k
n
Xn
k=1
�
yk
—* ey − 1,
k!
On
n
1
=
cˆ
1
y
X∞
ℓ=0
1
cˆ
XM
ℓ=0
XM
k=0
M
X
k=0
1
cˆ
aMk := 1 ˆck+1
On � M
n X
k=0
XM
k=0
XM
k=0
On �
lim
n→∞
G ǫ
1
y
1
−cˆ
XM
ℓ=0
~M
~ �1 − y
1 − y �ℓ cˆ
</figure>
<equation confidence="0.989307777777778">
=
cˆ y
(1 − cˇ ~M
cˆ
G cˇ
almost surely, as long as
. (16) M &gt; cˇlog2 e + log2 (ǫˇc)
(20)
log2 (ˆc − ˇc) − 1 − log2 (ˆc)
</equation>
<page confidence="0.875581">
112
</page>
<note confidence="0.248568">
Proof: From Equation (6), we have Combining Equations (22) and (24), we have that,
</note>
<equation confidence="0.860915533333333">
almost surely,
lim On − PMk=0 (−1)k aMk (k + 1)! ϕk+1
n→oo n
Z cˆ 1 !
cˇ e−y y − (−1)k aM k yk dQ(y). (25)
k =0
XM
On = |Ωn |− Xn ϕk
n n k=1 n
= |Ωn |− Xn λk−1 −
n k=1 k
=
Xn 1�kϕk � Combining Equation (16) with Equation (17), we
k=1 n − λk−1 . (21) have
k
</equation>
<bodyText confidence="0.95071">
The first term in the right hand side (RHS) of
Equation (21) converges as seen in Equation (3).
The third term in the RHS of Equation (21) con-
verges to zero, almost surely, as seen from Equa-
tion (7). The second term in the RHS of Equa-
tion (21), on the other hand,
!
</bodyText>
<equation confidence="0.999208833333333">
Z cˆ e−y Xn yk
= dQ(y)
y k!
cˇ k=1
Z cˆ e−y
→ y (ey − 1) dQ(y), n → ∞,
cˇ
ˆ
Z c Z c
1 e−y
= y dQ(y) − y dQ(y).
cˇ cˇ
</equation>
<bodyText confidence="0.999513333333333">
The monotone convergence theorem justifies the
convergence in the second step above. Thus we
conclude that
</bodyText>
<equation confidence="0.983602666666667">
Z cˆ e−y
= y dQ(y) (22)
cˇ
</equation>
<bodyText confidence="0.8247255">
almost surely. Coming to the estimator, we can
write it as the sum of two terms:
</bodyText>
<equation confidence="0.9579775">
XM (−1)k aMk k!λk (23)
k=0
</equation>
<bodyText confidence="0.977664285714286">
The second term in Equation (23) above is seen to
converge to zero almost surely as n → ∞, using
Equation (7) and noting that M is a constant not
depending on n. The first term in Equation (23)
can be written as, using the definition of λk from
Equation (8),
!
</bodyText>
<equation confidence="0.995538090909091">
Z cˆ XM
e−y (−1)k aM k yk dQ(y). (24)
cˇ k=0
(1 − cˇ ~M . (26)
(−1)k aM k yk ≤ cˆ
cˇ
The quantity in Equation (25) can now be upper
bounded by, using Equation (26),
e−ˇc �1 − cˇ ~M
cˆ .
cˇ
</equation>
<bodyText confidence="0.836584">
For M that satisfy Equation (20) this term is less
than ǫ. The proof concludes.
</bodyText>
<subsectionHeader confidence="0.969447">
3.7 Uniform Consistent Estimation
</subsectionHeader>
<bodyText confidence="0.999872888888889">
One of the main issues with actually employing
the estimator for the number of unseen elements
(cf. Equation (19)) is that it involves knowing the
parameter ˆc. In practice, there is no natural way to
obtain any estimate on this parameter ˆc. It would
be most useful if there were a way to modify the
estimator in a way that it does not depend on the
unobservable quantity ˆc. In this section we see that
such a modification is possible, while still retain-
ing the main theoretical performance result of con-
sistency (cf. Theorem 1).
The first step to see the modification is in ob-
serving where the need for cˆ arises: it is in writing
the geometric series for the function y1 (cf. Equa-
tions (15) and (16)). If we could let cˆ along with
the number of elements M itself depend on the
sample size n, then we could still have the geo-
metric series formula. More precisely, we have
</bodyText>
<equation confidence="0.9009075">
y
1 − ˆcn ˆcn y ˆcn
1 XMn ~Mn
ℓ=0 � �ℓ �
1 − y = 1 1 − y
→ 0, n → ∞,
</equation>
<bodyText confidence="0.851041166666667">
as long as
→ 0, n → ∞. (27)
This simple calculation suggests that we can re-
place cˆ and M in the formula for the estimator (cf.
Equation (19)) by terms that depend on n and sat-
isfy the condition expressed by Equation (27).
</bodyText>
<figure confidence="0.985885235294118">
Xn
k=1
λk−1
k
lim On
n→oo n
XM ~(k + 1) ϕk+1 �− λk .
+ (−1)k aM k k!
k=0 n
1
y
−
XM
k=0
0 &lt;
ˆcn
Mn
</figure>
<page confidence="0.997548">
113
</page>
<sectionHeader confidence="0.998916" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998956">
4.1 Corpora
</subsectionHeader>
<bodyText confidence="0.998533">
In our experiments we used the following corpora:
</bodyText>
<listItem confidence="0.992897357142857">
1. The British National Corpus (BNC): A cor-
pus of about 100 million words of written and
spoken British English from the years 1975-
1994.
2. The New York Times Corpus (NYT): A cor-
pus of about 5 million words.
3. The Malayalam Corpus (MAL): A collection
of about 2.5 million words from varied ar-
ticles in the Malayalam language from the
Central Institute of Indian Languages.
4. The Hindi Corpus (HIN): A collection of
about 3 million words from varied articles in
the Hindi language also from the Central In-
stitute of Indian Languages.
</listItem>
<subsectionHeader confidence="0.968555">
4.2 Methodology
</subsectionHeader>
<bodyText confidence="0.99273876">
We would like to see how well our estimator per-
forms in terms of estimating the number of unseen
elements. A natural way to study this is to ex-
pose only half of an existing corpus to be observed
and estimate the number of unseen elements (as-
suming the the actual corpus is twice the observed
size). We can then check numerically how well
our estimator performs with respect to the “true”
value. We use a subset (the first 10%, 20%, 30%,
40% and 50%) of the corpus as the observed sam-
ple to estimate the vocabulary over twice the sam-
ple size. The following estimators have been com-
pared.
Nonparametric: Along with our proposed esti-
mator (in Section 3), the following canonical es-
timators available in (Gandolfi and Sastri, 2004)
and (Baayen, 2001) are studied.
1. Our proposed estimator ®n (cf. Section 3):
since the estimator is rather involved we con-
sider only small values of M (we see empir-
ically that the estimator converges for very
small values of M itself) and choose cˆ = M.
This allows our estimator for the number of
unseen elements to be of the following form,
for different values of M:
</bodyText>
<footnote confidence="0.7506945">
M ®n
1 2 (ϕ1 − ϕ2)
2 3
2 (ϕ1 − ϕ2) + 4ϕ3
3 43(ϕ1 − +8−
ϕ2) 9(ϕ3 3)
</footnote>
<bodyText confidence="0.8461695">
Using this, the estimator of the true vocabu-
lary size is simply,
</bodyText>
<equation confidence="0.9724285">
®n + V. (28)
Here (cf. Equation (5))
V = Xn ϕnk. (29)
k=1
</equation>
<bodyText confidence="0.9997856">
In the simulations below, we have considered
M large enough until we see numerical con-
vergence of the estimators: in all the cases,
no more than a value of 4 is needed for M.
For the English corpora, very small values of
M suffice – in particular, we have considered
the average of the first three different estima-
tors (corresponding to the first three values
of M). For the non-English corpora, we have
needed to consider M = 4.
</bodyText>
<sectionHeader confidence="0.68011" genericHeader="method">
2. Gandolfi-Sastri estimator,
</sectionHeader>
<figure confidence="0.84297045">
def n �V + ϕ1γ2� , (30)
VGS = n − ϕ1
where
γ2 = ϕ1 − n − V +
2n
p
5n2 + 2n(V − 3ϕ1) + (V − ϕ1)2
;
2n
3. Chao estimator,
def 1
VChao = V + ϕ2 ; (31)
2ϕ2
4. Good-Turing estimator,
def V
VGT =
5. “Simplistic” estimator,
�nnew �
def
VSmpl = V ;(33)
</figure>
<sectionHeader confidence="0.231447" genericHeader="method">
n
</sectionHeader>
<bodyText confidence="0.999821333333333">
here the supposition is that the vocabulary
size scales linearly with the sample size (here
nnew is the new sample size);
</bodyText>
<sectionHeader confidence="0.930919" genericHeader="method">
6. Baayen estimator,
</sectionHeader>
<bodyText confidence="0.895758166666667">
VByn def V + r n11 nnew; (34)
here the supposition is that the vocabulary
growth rate at the observed sample size is
given by the ratio of the number of hapax
legomena to the sample size (cf. (Baayen,
2001) pp. 50).
</bodyText>
<equation confidence="0.8577215">
(32)
(1 − ϕn1) ;
</equation>
<page confidence="0.893568">
114
</page>
<figure confidence="0.8280296">
BNC NYT Malayalam Hindi
% error
−40 −30 −20 −10 0 10
Our GT ZM Our GT ZM Our GT ZM Our GT ZM
% error of top 2 and Good−Turing estimates compared
</figure>
<figureCaption confidence="0.561781125">
Figure 1: Comparison of error estimates of the 2
best estimators-ours and the ZM, with the Good-
Turing estimator using 10% sample size of all the
corpora. A bar with a positive height indicates
and overestimate and that with a negative height
indicates and underestimate. Our estimator out-
performs ZM. Good-Turing estimator widely un-
derestimates vocabulary size.
</figureCaption>
<bodyText confidence="0.998804363636364">
Parametric: Parametric estimators use the ob-
servations to first estimate the parameters. Then
the corresponding models are used to estimate the
vocabulary size over the larger sample. Thus the
frequency spectra of the observations are only in-
directly used in extrapolating the vocabulary size.
In this study we consider state of the art paramet-
ric estimators, as surveyed by (Baroni and Evert,
2005). We are aided in this study by the availabil-
ity of the implementations provided by the ZipfR
package and their default settings.
</bodyText>
<sectionHeader confidence="0.999668" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9640695">
The performance of the different estimators as per-
centage errors of the true vocabulary size using
different corpora are tabulated in tables 1-4. We
now summarize some important observations.
• From the Figure 1, we see that our estima-
tor compares quite favorably with the best of
the state of the art estimators. The best of the
state of the art estimator is a parametric one
(ZM), while ours is a nonparametric estima-
tor.
</bodyText>
<listItem confidence="0.914833285714286">
• In table 1 and table 2 we see that our esti-
mate is quite close to the true vocabulary, at
all sample sizes. Further, it compares very fa-
vorably to the state of the art estimators (both
parametric and nonparametric).
• Again, on the two non-English corpora (ta-
bles 3 and 4) we see that our estimator com-
</listItem>
<bodyText confidence="0.970789333333333">
pares favorably with the best estimator of vo-
cabulary size and at some sample sizes even
surpasses it.
</bodyText>
<listItem confidence="0.734279714285714">
• Our estimator has theoretical performance
guarantees and its empirical performance is
comparable to that of the state of the art es-
timators. However, this performance comes
at a very small fraction of the computational
cost of the parametric estimators.
• The state of the art nonparametric Good-
</listItem>
<bodyText confidence="0.949524666666667">
Turing estimator wildly underestimates the
vocabulary; this is true in each of the four
corpora studied and at all sample sizes.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999514">
In this paper, we have proposed a new nonpara-
metric estimator of vocabulary size that takes into
account the LNRE property of word frequency
distributions and have shown that it is statistically
consistent. We then compared the performance of
the proposed estimator with that of the state of the
art estimators on large corpora. While the perfor-
mance of our estimator seems favorable, we also
see that the widely used classical Good-Turing
estimator consistently underestimates the vocabu-
lary size. Although as yet untested, with its com-
putational simplicity and favorable performance,
our estimator may serve as a more reliable alter-
native to the Good-Turing estimator for estimating
vocabulary sizes.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999646">
This research was partially supported by Award
IIS-0623805 from the National Science Founda-
tion.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999358909090909">
R. H. Baayen. 2001. Word Frequency Distributions,
Kluwer Academic Publishers.
Marco Baroni and Stefan Evert. 2001. “Testing the ex-
trapolation quality of word frequency models”, Pro-
ceedings of Corpus Linguistics , volume 1 of The
Corpus Linguistics Conference Series, P. Danielsson
and M. Wagenmakers (eds.).
J. Bunge and M. Fitzpatrick. 1993. “Estimating the
number of species: a review”, Journal of the Amer-
ican Statistical Association, Vol. 88(421), pp. 364-
373.
</reference>
<page confidence="0.99804">
115
</page>
<table confidence="0.9998165">
Sample True % error w.r.t the true value
(% of corpus) value
Our GT ZM fZM Smpl Byn Chao GS
10 153912 1 -27 -4 -8 46 23 8 -11
20 220847 -3 -30 -9 -12 39 19 4 -15
30 265813 -2 -30 -9 -11 39 20 6 -15
40 310351 1 -29 -7 -9 42 23 9 -13
50 340890 2 -28 -6 -8 43 24 10 -12
</table>
<tableCaption confidence="0.984595666666667">
Table 1: Comparison of estimates of vocabulary size for the BNC corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators
at all sample sizes.
</tableCaption>
<table confidence="0.999696875">
Sample True % error w.r.t the true value
(% of corpus) value
Our GT ZM fZM Smpl Byn Chao GS
10 37346 1 -24 5 -8 48 28 4 -8
20 51200 -3 -26 0 -11 46 22 -1 -11
30 60829 -2 -25 1 -10 48 23 1 -10
40 68774 -3 -25 0 -10 49 21 -1 -11
50 75526 -2 -25 0 -10 50 21 0 -10
</table>
<tableCaption confidence="0.923561">
Table 2: Comparison of estimates of vocabulary size for the NYT corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator compares favorably with ZM and
Chao.
</tableCaption>
<table confidence="0.99906">
Sample True % error w.r.t the true value
(% of corpus) value
Our GT ZM fZM Smpl Byn Chao GS
10 146547 -2 -27 -5 -10 9 34 82 -2
20 246723 8 -23 4 -2 19 47 105 5
30 339196 4 -27 0 -5 16 42 93 -1
40 422010 5 -28 1 -4 17 43 95 -1
50 500166 5 -28 1 -4 18 44 94 -2
</table>
<tableCaption confidence="0.963601">
Table 3: Comparison of estimates of vocabulary size for the Malayalam corpus as percentage errors
w.r.t the true value. A negative value indicates an underestimate. Our estimator compares favorably with
ZM and GS.
</tableCaption>
<table confidence="0.999510375">
Sample True % error w.r.t the true value
(% of corpus) value
Our GT ZM fZM Smpl Byn Chao GS
10 47639 -2 -34 -4 -9 25 32 31 -12
20 71320 7 -30 2 -1 34 43 51 -7
30 93259 2 -33 -1 -5 30 38 42 -10
40 113186 0 -35 -5 -7 26 34 39 -13
50 131715 -1 -36 -6 -8 24 33 40 -14
</table>
<tableCaption confidence="0.948160666666667">
Table 4: Comparison of estimates of vocabulary size for the Hindi corpus as percentage errors w.r.t the
true value. A negative value indicates an underestimate. Our estimator outperforms the other estimators
at certain sample sizes.
</tableCaption>
<page confidence="0.997875">
116
</page>
<reference confidence="0.999448631578947">
A. Gandolfi and C. C. A. Sastri. 2004. “Nonparamet-
ric Estimations about Species not Observed in a
Random Sample”, Milan Journal of Mathematics,
Vol. 72, pp. 81-105.
E. V. Khmaladze. 1987. “The statistical analysis of
large number of rare events”, Technical Report, De-
partment of Mathematics and Statistics., CWI, Am-
sterdam, MS-R8804.
E. V. Khmaladze and R. J. Chitashvili. 1989. “Statis-
tical analysis of large number of rate events and re-
lated problems”, Probability theory and mathemati-
cal statistics (Russian), Vol. 92, pp. 196-245.
. P. Santhanam, A. Orlitsky, and K. Viswanathan, “New
tricks for old dogs: Large alphabet probability es-
timation”, in Proc. 2007 IEEE Information Theory
Workshop, Sept. 2007, pp. 638–643.
A. B. Wagner, P. Viswanath and S. R. Kulkarni. 2006.
“Strong Consistency of the Good-Turing estimator”,
IEEE Symposium on Information Theory, 2006.
</reference>
<page confidence="0.998026">
117
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978519">
<title confidence="0.999924">Knowing the Unseen: Estimating Vocabulary Size over Unseen Samples</title>
<author confidence="0.999819">Suma Bhat Richard Sproat</author>
<affiliation confidence="0.999962">Department of ECE Center for Spoken Language Understanding University of Illinois Oregon Health &amp; Science University</affiliation>
<email confidence="0.985649">spbhat2@illinois.edurws@xoba.com</email>
<abstract confidence="0.999737259259259">Empirical studies on corpora involve making measurements of several quantities for the purpose of comparing corpora, creating language models or to make generalizations about specific linguistic phenomena in a language. Quantities such as average word length are stable across sample sizes and hence can be reliably estimated from large enough samples. Howquantities such as size change with sample size. Thus measurements based on a given sample will need be obtain their estimates over larger unseen samples. In this work, propose a novel estimator of vocabulary size. Our main result is show the consistency the estimator – the first of its kind in the literature. Finally, we compare our proposal with the state of the art estimators (both parametric and nonparametric) on large standard corpora; apart from showing the favorable performance of our estimator, we also see that the classical Good-Turing estimator consistently underestimates the vocabulary size.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
</authors>
<title>Word Frequency Distributions,</title>
<date>2001</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3273" citStr="Baayen, 2001" startWordPosition="509" endWordPosition="510">stri, 2004). Of particular interest to us is the set of estimators that have been shown to model word frequency distributions well. This study proposes a nonparametric estimator of vocabulary size and evaluates its theoretical and empirical performance. For comparison we consider some state-of-the-art parametric and nonparametric estimators of vocabulary size. The proposed non-parametric estimator for the number of unseen elements assumes a regime characterizing word frequency distributions. This work is motivated by a scaling formulation to address the problem of unlikely events proposed in (Baayen, 2001; Khmaladze, 1987; Khmaladze and Chitashvili, 1989; Wagner et al., 2006). We also demonstrate that the estimator is strongly consistent under the natural scaling formulation. While compared with other vocabulary size estimates, we see that our estimator performs at least as well as some of the state of the art estimators. 2 Previous Work Many estimators of vocabulary size are available in the literature and a comparison of several non 109 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109–117, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP pa</context>
<context position="5291" citStr="Baayen, 2001" startWordPosition="831" endWordPosition="832">cabulary size. A good survey of the state of the art is available in (Gandolfi and Sastri, 2004). In this paper, we compare our proposed estimator with the canonical estimators available in (Gandolfi and Sastri, 2004). 2. Parametric estimators- here a probabilistic model capturing the relation between expected vocabulary size and sample size is the estimator. Given a sample of size n, the sample serves to calculate the parameters of the model. The expected vocabulary for a given sample size is then determined using the explicit relation. The parametric estimators considered in this study are (Baayen, 2001; Baroni and Evert, 2005), (a) Zipf-Mandelbrot estimator (ZM); (b) finite Zipf-Mandelbrot estimator (fZM). In addition to the above estimators we consider a novel non parametric estimator. It is the nonparametric estimator that we propose, taking into account the characteristic feature of word frequency distributions, to which we will turn next. 3 Novel Estimator of Vocabulary size We observe (Xi, ... , Xn), an i.i.d. sequence drawn according to a probability distribution P from a large, but finite, vocabulary Q. Our goal is in estimating the “essential” size of the vocabulary Q using only the</context>
<context position="6686" citStr="Baayen, 2001" startWordPosition="1058" endWordPosition="1059">arametric estimator for the number of unseen elements is motivated by the characteristic property of word frequency distributions, the Large Number of Rare Events (LNRE) (Baayen, 2001). We also demonstrate that the estimator is strongly consistent under a natural scaling formulation described in (Khmaladze, 1987). 3.1 A Scaling Formulation Our main interest is in probability distributions P with the property that a large number of words in the vocabulary Q are unlikely, i.e., the chance any word appears eventually in an arbitrarily long observation is strictly between 0 and 1. The authors in (Baayen, 2001; Khmaladze and Chitashvili, 1989; Wagner et al., 2006) propose a natural scaling formulation to study this problem; specifically, (Baayen, 2001) has a tutorial-like summary of the theoretical work in (Khmaladze, 1987; Khmaladze and Chitashvili, 1989). In particular, the authors consider a sequence of vocabulary sets and probability distributions, indexed by the observation size n. Specifically, the observation (Xi, ... , Xn) is drawn i.i.d. from a vocabulary Qn according to probability Pn. If the probability of a word, say w E Qn is p, then the probability that this specific word w does not o</context>
<context position="17664" citStr="Baayen, 2001" startWordPosition="3250" endWordPosition="3251">s is to expose only half of an existing corpus to be observed and estimate the number of unseen elements (assuming the the actual corpus is twice the observed size). We can then check numerically how well our estimator performs with respect to the “true” value. We use a subset (the first 10%, 20%, 30%, 40% and 50%) of the corpus as the observed sample to estimate the vocabulary over twice the sample size. The following estimators have been compared. Nonparametric: Along with our proposed estimator (in Section 3), the following canonical estimators available in (Gandolfi and Sastri, 2004) and (Baayen, 2001) are studied. 1. Our proposed estimator ®n (cf. Section 3): since the estimator is rather involved we consider only small values of M (we see empirically that the estimator converges for very small values of M itself) and choose cˆ = M. This allows our estimator for the number of unseen elements to be of the following form, for different values of M: M ®n 1 2 (ϕ1 − ϕ2) 2 3 2 (ϕ1 − ϕ2) + 4ϕ3 3 43(ϕ1 − +8− ϕ2) 9(ϕ3 3) Using this, the estimator of the true vocabulary size is simply, ®n + V. (28) Here (cf. Equation (5)) V = Xn ϕnk. (29) k=1 In the simulations below, we have considered M large enou</context>
<context position="19263" citStr="Baayen, 2001" startWordPosition="3574" endWordPosition="3575">astri estimator, def n �V + ϕ1γ2� , (30) VGS = n − ϕ1 where γ2 = ϕ1 − n − V + 2n p 5n2 + 2n(V − 3ϕ1) + (V − ϕ1)2 ; 2n 3. Chao estimator, def 1 VChao = V + ϕ2 ; (31) 2ϕ2 4. Good-Turing estimator, def V VGT = 5. “Simplistic” estimator, �nnew � def VSmpl = V ;(33) n here the supposition is that the vocabulary size scales linearly with the sample size (here nnew is the new sample size); 6. Baayen estimator, VByn def V + r n11 nnew; (34) here the supposition is that the vocabulary growth rate at the observed sample size is given by the ratio of the number of hapax legomena to the sample size (cf. (Baayen, 2001) pp. 50). (32) (1 − ϕn1) ; 114 BNC NYT Malayalam Hindi % error −40 −30 −20 −10 0 10 Our GT ZM Our GT ZM Our GT ZM Our GT ZM % error of top 2 and Good−Turing estimates compared Figure 1: Comparison of error estimates of the 2 best estimators-ours and the ZM, with the GoodTuring estimator using 10% sample size of all the corpora. A bar with a positive height indicates and overestimate and that with a negative height indicates and underestimate. Our estimator outperforms ZM. Good-Turing estimator widely underestimates vocabulary size. Parametric: Parametric estimators use the observations to firs</context>
</contexts>
<marker>Baayen, 2001</marker>
<rawString>R. H. Baayen. 2001. Word Frequency Distributions, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Stefan Evert</author>
</authors>
<title>Testing the extrapolation quality of word frequency models”,</title>
<date>2001</date>
<booktitle>Proceedings of Corpus Linguistics ,</booktitle>
<volume>1</volume>
<editor>P. Danielsson and M. Wagenmakers (eds.).</editor>
<marker>Baroni, Evert, 2001</marker>
<rawString>Marco Baroni and Stefan Evert. 2001. “Testing the extrapolation quality of word frequency models”, Proceedings of Corpus Linguistics , volume 1 of The Corpus Linguistics Conference Series, P. Danielsson and M. Wagenmakers (eds.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bunge</author>
<author>M Fitzpatrick</author>
</authors>
<title>Estimating the number of species: a review”,</title>
<date>1993</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>88</volume>
<issue>421</issue>
<pages>364--373</pages>
<contexts>
<context position="2640" citStr="Bunge and Fitzpatrick, 1993" startWordPosition="412" endWordPosition="415">e purpose of comparison of corpora of different sizes or linguistic phenomena based on samples of different sizes it is imperative that these quantities be compared based on similar sample sizes. We thus need methods to extrapolate empirical measurements of these quantities to arbitrary sample sizes. Our focus in this study will be estimators of vocabulary size for samples larger than the sample available. There is an abundance of estimators of population size (in our case, vocabulary size) in existing literature. Excellent survey articles that summarize the state-of-the-art are available in (Bunge and Fitzpatrick, 1993) and (Gandolfi and Sastri, 2004). Of particular interest to us is the set of estimators that have been shown to model word frequency distributions well. This study proposes a nonparametric estimator of vocabulary size and evaluates its theoretical and empirical performance. For comparison we consider some state-of-the-art parametric and nonparametric estimators of vocabulary size. The proposed non-parametric estimator for the number of unseen elements assumes a regime characterizing word frequency distributions. This work is motivated by a scaling formulation to address the problem of unlikely</context>
</contexts>
<marker>Bunge, Fitzpatrick, 1993</marker>
<rawString>J. Bunge and M. Fitzpatrick. 1993. “Estimating the number of species: a review”, Journal of the American Statistical Association, Vol. 88(421), pp. 364-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gandolfi</author>
<author>C C A Sastri</author>
</authors>
<title>Nonparametric Estimations about Species not Observed in a Random Sample”,</title>
<date>2004</date>
<journal>Milan Journal of Mathematics,</journal>
<volume>72</volume>
<pages>81--105</pages>
<contexts>
<context position="2672" citStr="Gandolfi and Sastri, 2004" startWordPosition="417" endWordPosition="421"> of different sizes or linguistic phenomena based on samples of different sizes it is imperative that these quantities be compared based on similar sample sizes. We thus need methods to extrapolate empirical measurements of these quantities to arbitrary sample sizes. Our focus in this study will be estimators of vocabulary size for samples larger than the sample available. There is an abundance of estimators of population size (in our case, vocabulary size) in existing literature. Excellent survey articles that summarize the state-of-the-art are available in (Bunge and Fitzpatrick, 1993) and (Gandolfi and Sastri, 2004). Of particular interest to us is the set of estimators that have been shown to model word frequency distributions well. This study proposes a nonparametric estimator of vocabulary size and evaluates its theoretical and empirical performance. For comparison we consider some state-of-the-art parametric and nonparametric estimators of vocabulary size. The proposed non-parametric estimator for the number of unseen elements assumes a regime characterizing word frequency distributions. This work is motivated by a scaling formulation to address the problem of unlikely events proposed in (Baayen, 200</context>
<context position="3949" citStr="Gandolfi and Sastri, 2004" startWordPosition="616" endWordPosition="619">; Wagner et al., 2006). We also demonstrate that the estimator is strongly consistent under the natural scaling formulation. While compared with other vocabulary size estimates, we see that our estimator performs at least as well as some of the state of the art estimators. 2 Previous Work Many estimators of vocabulary size are available in the literature and a comparison of several non 109 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109–117, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parametric estimators of population size occurs in (Gandolfi and Sastri, 2004). While a definite comparison including parametric estimators is lacking, there is also no known work comparing methods of extrapolation of vocabulary size. Baroni and Evert, in (Baroni and Evert, 2005), evaluate the performance of some estimators in extrapolating vocabulary size for arbitrary sample sizes but limit the study to parametric estimators. Since we consider both parametric and nonparametric estimators here, we consider this to be the first study comparing a set of estimators for extrapolating vocabulary size. Estimators of vocabulary size that we compare can be broadly classified i</context>
<context position="17645" citStr="Gandolfi and Sastri, 2004" startWordPosition="3245" endWordPosition="3248">ents. A natural way to study this is to expose only half of an existing corpus to be observed and estimate the number of unseen elements (assuming the the actual corpus is twice the observed size). We can then check numerically how well our estimator performs with respect to the “true” value. We use a subset (the first 10%, 20%, 30%, 40% and 50%) of the corpus as the observed sample to estimate the vocabulary over twice the sample size. The following estimators have been compared. Nonparametric: Along with our proposed estimator (in Section 3), the following canonical estimators available in (Gandolfi and Sastri, 2004) and (Baayen, 2001) are studied. 1. Our proposed estimator ®n (cf. Section 3): since the estimator is rather involved we consider only small values of M (we see empirically that the estimator converges for very small values of M itself) and choose cˆ = M. This allows our estimator for the number of unseen elements to be of the following form, for different values of M: M ®n 1 2 (ϕ1 − ϕ2) 2 3 2 (ϕ1 − ϕ2) + 4ϕ3 3 43(ϕ1 − +8− ϕ2) 9(ϕ3 3) Using this, the estimator of the true vocabulary size is simply, ®n + V. (28) Here (cf. Equation (5)) V = Xn ϕnk. (29) k=1 In the simulations below, we have cons</context>
</contexts>
<marker>Gandolfi, Sastri, 2004</marker>
<rawString>A. Gandolfi and C. C. A. Sastri. 2004. “Nonparametric Estimations about Species not Observed in a Random Sample”, Milan Journal of Mathematics, Vol. 72, pp. 81-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E V Khmaladze</author>
</authors>
<title>The statistical analysis of large number of rare events”,</title>
<date>1987</date>
<tech>Technical Report,</tech>
<institution>Department of Mathematics and Statistics.,</institution>
<location>CWI, Amsterdam, MS-R8804.</location>
<contexts>
<context position="3290" citStr="Khmaladze, 1987" startWordPosition="511" endWordPosition="512">f particular interest to us is the set of estimators that have been shown to model word frequency distributions well. This study proposes a nonparametric estimator of vocabulary size and evaluates its theoretical and empirical performance. For comparison we consider some state-of-the-art parametric and nonparametric estimators of vocabulary size. The proposed non-parametric estimator for the number of unseen elements assumes a regime characterizing word frequency distributions. This work is motivated by a scaling formulation to address the problem of unlikely events proposed in (Baayen, 2001; Khmaladze, 1987; Khmaladze and Chitashvili, 1989; Wagner et al., 2006). We also demonstrate that the estimator is strongly consistent under the natural scaling formulation. While compared with other vocabulary size estimates, we see that our estimator performs at least as well as some of the state of the art estimators. 2 Previous Work Many estimators of vocabulary size are available in the literature and a comparison of several non 109 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109–117, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parametric estimato</context>
<context position="6388" citStr="Khmaladze, 1987" startWordPosition="1007" endWordPosition="1008">om a large, but finite, vocabulary Q. Our goal is in estimating the “essential” size of the vocabulary Q using only the observations. In other words, having seen a sample of size n we wish to know, given another sample from the same population, how many unseen elements we would expect to see. Our nonparametric estimator for the number of unseen elements is motivated by the characteristic property of word frequency distributions, the Large Number of Rare Events (LNRE) (Baayen, 2001). We also demonstrate that the estimator is strongly consistent under a natural scaling formulation described in (Khmaladze, 1987). 3.1 A Scaling Formulation Our main interest is in probability distributions P with the property that a large number of words in the vocabulary Q are unlikely, i.e., the chance any word appears eventually in an arbitrarily long observation is strictly between 0 and 1. The authors in (Baayen, 2001; Khmaladze and Chitashvili, 1989; Wagner et al., 2006) propose a natural scaling formulation to study this problem; specifically, (Baayen, 2001) has a tutorial-like summary of the theoretical work in (Khmaladze, 1987; Khmaladze and Chitashvili, 1989). In particular, the authors consider a sequence of</context>
</contexts>
<marker>Khmaladze, 1987</marker>
<rawString>E. V. Khmaladze. 1987. “The statistical analysis of large number of rare events”, Technical Report, Department of Mathematics and Statistics., CWI, Amsterdam, MS-R8804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E V Khmaladze</author>
<author>R J Chitashvili</author>
</authors>
<title>Statistical analysis of large number of rate events and related problems”,</title>
<date>1989</date>
<booktitle>Probability theory and mathematical statistics (Russian),</booktitle>
<volume>92</volume>
<pages>196--245</pages>
<contexts>
<context position="3323" citStr="Khmaladze and Chitashvili, 1989" startWordPosition="513" endWordPosition="516">rest to us is the set of estimators that have been shown to model word frequency distributions well. This study proposes a nonparametric estimator of vocabulary size and evaluates its theoretical and empirical performance. For comparison we consider some state-of-the-art parametric and nonparametric estimators of vocabulary size. The proposed non-parametric estimator for the number of unseen elements assumes a regime characterizing word frequency distributions. This work is motivated by a scaling formulation to address the problem of unlikely events proposed in (Baayen, 2001; Khmaladze, 1987; Khmaladze and Chitashvili, 1989; Wagner et al., 2006). We also demonstrate that the estimator is strongly consistent under the natural scaling formulation. While compared with other vocabulary size estimates, we see that our estimator performs at least as well as some of the state of the art estimators. 2 Previous Work Many estimators of vocabulary size are available in the literature and a comparison of several non 109 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109–117, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parametric estimators of population size occurs in (</context>
<context position="6719" citStr="Khmaladze and Chitashvili, 1989" startWordPosition="1060" endWordPosition="1063">mator for the number of unseen elements is motivated by the characteristic property of word frequency distributions, the Large Number of Rare Events (LNRE) (Baayen, 2001). We also demonstrate that the estimator is strongly consistent under a natural scaling formulation described in (Khmaladze, 1987). 3.1 A Scaling Formulation Our main interest is in probability distributions P with the property that a large number of words in the vocabulary Q are unlikely, i.e., the chance any word appears eventually in an arbitrarily long observation is strictly between 0 and 1. The authors in (Baayen, 2001; Khmaladze and Chitashvili, 1989; Wagner et al., 2006) propose a natural scaling formulation to study this problem; specifically, (Baayen, 2001) has a tutorial-like summary of the theoretical work in (Khmaladze, 1987; Khmaladze and Chitashvili, 1989). In particular, the authors consider a sequence of vocabulary sets and probability distributions, indexed by the observation size n. Specifically, the observation (Xi, ... , Xn) is drawn i.i.d. from a vocabulary Qn according to probability Pn. If the probability of a word, say w E Qn is p, then the probability that this specific word w does not occur in an observation of size n </context>
<context position="8352" citStr="Khmaladze and Chitashvili, 1989" startWordPosition="1363" endWordPosition="1366">is called the LNRE zone and its applicability in natural language corpora is studied in detail in (Baayen, 2001). 3.2 Shadows Consider the observation string (Xi, ... , Xn) and let us denote the quantity of interest – the number c� c� n &lt; p &lt; n n c� 110 of word types in the vocabulary Ωn that are not observed – by On. This quantity is random since the observation string itself is. However, we note that the distribution of On is unaffected if one relabels the words in Ωn. This motivates studying of the probabilities assigned by Pn without reference to the labeling of the word; this is done in (Khmaladze and Chitashvili, 1989) via the structural distribution function and in (Wagner et al., 2006) via the shadow. Here we focus on the latter description: Definition 1 Let Xn be a random variable on Ωn with distribution Pn. The shadow of Pn is defined to be the distribution of the random variable Pn({Xn}). For the finite vocabulary situation we are considering, specifying the shadow is exactly equivalent to specifying the unordered components of Pn, viewed as a probability vector. 3.3 Scaled Shadows Converge We will follow (Wagner et al., 2006) and suppose that the scaled shadows, the distribution of n · Pn(Xn), denoted</context>
</contexts>
<marker>Khmaladze, Chitashvili, 1989</marker>
<rawString>E. V. Khmaladze and R. J. Chitashvili. 1989. “Statistical analysis of large number of rate events and related problems”, Probability theory and mathematical statistics (Russian), Vol. 92, pp. 196-245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Santhanam</author>
<author>A Orlitsky</author>
<author>K Viswanathan</author>
</authors>
<title>New tricks for old dogs: Large alphabet probability estimation”, in</title>
<date>2007</date>
<booktitle>Proc. 2007 IEEE Information Theory Workshop,</booktitle>
<pages>638--643</pages>
<marker>Santhanam, Orlitsky, Viswanathan, 2007</marker>
<rawString>. P. Santhanam, A. Orlitsky, and K. Viswanathan, “New tricks for old dogs: Large alphabet probability estimation”, in Proc. 2007 IEEE Information Theory Workshop, Sept. 2007, pp. 638–643.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Wagner</author>
<author>P Viswanath</author>
<author>S R Kulkarni</author>
</authors>
<date>2006</date>
<booktitle>Strong Consistency of the Good-Turing estimator”, IEEE Symposium on Information Theory,</booktitle>
<contexts>
<context position="3345" citStr="Wagner et al., 2006" startWordPosition="517" endWordPosition="520">rs that have been shown to model word frequency distributions well. This study proposes a nonparametric estimator of vocabulary size and evaluates its theoretical and empirical performance. For comparison we consider some state-of-the-art parametric and nonparametric estimators of vocabulary size. The proposed non-parametric estimator for the number of unseen elements assumes a regime characterizing word frequency distributions. This work is motivated by a scaling formulation to address the problem of unlikely events proposed in (Baayen, 2001; Khmaladze, 1987; Khmaladze and Chitashvili, 1989; Wagner et al., 2006). We also demonstrate that the estimator is strongly consistent under the natural scaling formulation. While compared with other vocabulary size estimates, we see that our estimator performs at least as well as some of the state of the art estimators. 2 Previous Work Many estimators of vocabulary size are available in the literature and a comparison of several non 109 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 109–117, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parametric estimators of population size occurs in (Gandolfi and Sastri, 2</context>
<context position="6741" citStr="Wagner et al., 2006" startWordPosition="1064" endWordPosition="1067">ements is motivated by the characteristic property of word frequency distributions, the Large Number of Rare Events (LNRE) (Baayen, 2001). We also demonstrate that the estimator is strongly consistent under a natural scaling formulation described in (Khmaladze, 1987). 3.1 A Scaling Formulation Our main interest is in probability distributions P with the property that a large number of words in the vocabulary Q are unlikely, i.e., the chance any word appears eventually in an arbitrarily long observation is strictly between 0 and 1. The authors in (Baayen, 2001; Khmaladze and Chitashvili, 1989; Wagner et al., 2006) propose a natural scaling formulation to study this problem; specifically, (Baayen, 2001) has a tutorial-like summary of the theoretical work in (Khmaladze, 1987; Khmaladze and Chitashvili, 1989). In particular, the authors consider a sequence of vocabulary sets and probability distributions, indexed by the observation size n. Specifically, the observation (Xi, ... , Xn) is drawn i.i.d. from a vocabulary Qn according to probability Pn. If the probability of a word, say w E Qn is p, then the probability that this specific word w does not occur in an observation of size n is (1 − p)n . For w to</context>
<context position="8422" citStr="Wagner et al., 2006" startWordPosition="1375" endWordPosition="1378">ed in detail in (Baayen, 2001). 3.2 Shadows Consider the observation string (Xi, ... , Xn) and let us denote the quantity of interest – the number c� c� n &lt; p &lt; n n c� 110 of word types in the vocabulary Ωn that are not observed – by On. This quantity is random since the observation string itself is. However, we note that the distribution of On is unaffected if one relabels the words in Ωn. This motivates studying of the probabilities assigned by Pn without reference to the labeling of the word; this is done in (Khmaladze and Chitashvili, 1989) via the structural distribution function and in (Wagner et al., 2006) via the shadow. Here we focus on the latter description: Definition 1 Let Xn be a random variable on Ωn with distribution Pn. The shadow of Pn is defined to be the distribution of the random variable Pn({Xn}). For the finite vocabulary situation we are considering, specifying the shadow is exactly equivalent to specifying the unordered components of Pn, viewed as a probability vector. 3.3 Scaled Shadows Converge We will follow (Wagner et al., 2006) and suppose that the scaled shadows, the distribution of n · Pn(Xn), denoted by Qn converge to a distribution Q. As an example, if Pn is a uniform</context>
<context position="10529" citStr="Wagner et al., 2006" startWordPosition="1771" endWordPosition="1774">�n (4) dQn (y) y c� from the observations (X1, ... , Xn). We observe that since the scaled shadow Qn does not depend on the labeling of the words in Ωn, a sufficient statistic to estimate (4) from the observation (X1, ... , Xn) is the profile of the observation: (ϕn1, ... , ϕnn), defined as follows. ϕnk is the number of word types that appear exactly k times in the observation, for k = 1, ... , n. Observe that n kϕnk = n, k=1 and that ϕn (5) k is the number of observed words. Thus, the object of our interest is, On = |Ωn |− V. (6) 3.5 Convergence of Scaled Profiles One of the main results of (Wagner et al., 2006) is that the scaled profiles converge to a deterministic probability vector under the scaling model introduced in Section 3.3. Specifically, we have from Proposition 1 of (Wagner et al., 2006): −→ 0, almost surely, (7) where λk := c k! c yk exp(−y) dQ(y) k = 0, 1, 2, ... . (8) This convergence result suggests a natural estimator for On, expressed in Equation (6). 3.6 A Consistent Estimator of On We start with the limiting expression for scaled profiles in Equation (7) and come up with a natural estimator for On. Our development leading to the estimator is somewhat heuristic and is aimed at mot</context>
</contexts>
<marker>Wagner, Viswanath, Kulkarni, 2006</marker>
<rawString>A. B. Wagner, P. Viswanath and S. R. Kulkarni. 2006. “Strong Consistency of the Good-Turing estimator”, IEEE Symposium on Information Theory, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>