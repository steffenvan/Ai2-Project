<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010393">
<title confidence="0.996275">
Hierarchical A* Parsing with Bridge Outside Scores
</title>
<author confidence="0.997585">
Adam Pauls and Dan Klein
</author>
<affiliation confidence="0.996886">
Computer Science Division
University of California at Berkeley
</affiliation>
<email confidence="0.990537">
{adpauls,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.993679" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999414176470588">
Hierarchical A* (HA*) uses of a hierarchy
of coarse grammars to speed up parsing
without sacrificing optimality. HA* pri-
oritizes search in refined grammars using
Viterbi outside costs computed in coarser
grammars. We present Bridge Hierarchi-
cal A* (BHA*), a modified Hierarchial A*
algorithm which computes a novel outside
cost called a bridge outside cost. These
bridge costs mix finer outside scores with
coarser inside scores, and thus consti-
tute tighter heuristics than entirely coarse
scores. We show that BHA* substan-
tially outperforms HA* when the hierar-
chy contains only very coarse grammars,
while achieving comparable performance
on more refined hierarchies.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999847447368421">
The Hierarchical A* (HA*) algorithm of Felzen-
szwalb and McAllester (2007) allows the use of a
hierarchy of coarse grammars to speed up pars-
ing without sacrificing optimality. Pauls and
Klein (2009) showed that a hierarchy of coarse
grammars outperforms standard A* parsing for a
range of grammars. HA* operates by computing
Viterbi inside and outside scores in an agenda-
based way, using outside scores computed under
coarse grammars as heuristics which guide the
search in finer grammars. The outside scores com-
puted by HA* are auxiliary quantities, useful only
because they form admissible heuristics for search
in finer grammars.
We show that a modification of the HA* algo-
rithm can compute modified bridge outside scores
which are tighter bounds on the true outside costs
in finer grammars. These bridge outside scores
mix inside and outside costs from finer grammars
with inside costs from coarser grammars. Because
the bridge costs represent tighter estimates of the
true outside costs, we expect them to reduce the
work of computing inside costs in finer grammars.
At the same time, because bridge costs mix com-
putation from coarser and finer levels of the hier-
archy, they are more expensive to compute than
purely coarse outside costs. Whether the work
saved by using tighter estimates outweighs the ex-
tra computation needed to compute them is an em-
pirical question.
In this paper, we show that the use of bridge out-
side costs substantially outperforms the HA* al-
gorithm when the coarsest levels of the hierarchy
are very loose approximations of the target gram-
mar. For hierarchies with tighter estimates, we
show that BHA* obtains comparable performance
to HA*. In other words, BHA* is more robust to
poorly constructed hierarchies.
</bodyText>
<sectionHeader confidence="0.993569" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99994125">
In this section, we introduce notation and review
HA*. Our presentation closely follows Pauls and
Klein (2009), and we refer the reader to that work
for a more detailed presentation.
</bodyText>
<subsectionHeader confidence="0.939553">
2.1 Notation
</subsectionHeader>
<bodyText confidence="0.999274235294118">
Assume we have input sentence s0 ... sn_1 of
length n, and a hierarchy of m weighted context-
free grammars !g1 ... !gryn. We call the most refined
grammar !gryn the target grammar, and all other
(coarser) grammars auxiliary grammars. Each
grammar !gt has a set of symbols denoted with cap-
ital letters and a subscript indicating the level in
the hierarchy, including a distinguished goal (root)
symbol Gt. Without loss of generality, we assume
Chomsky normal form, so each non-terminal rule
r in !gt has the form r = At —* Bt Ct with weight
wr.
Edges are labeled spans e = (At, i, j). The
weight of a derivation is the sum of rule weights
in the derivation. The weight of the best (mini-
mum) inside derivation for an edge e is called the
Viterbi inside score Q(e), and the weight of the
</bodyText>
<page confidence="0.973789">
348
</page>
<note confidence="0.843701">
Proceedings of the ACL 2010 Conference Short Papers, pages 348–352,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.940692">
Figure 1: Representations of the different types of items
used in parsing and how they depend on each other. (a)
In HA*, the inside item I(VPt, 3, 5) relies on the coarse
outside item O(irt(VPt), 3, 5) for outside estimates. (b) In
BHA*, the same inside item relies on the bridge outside item
O(VPt, 3, 5), which mixes coarse and refined outside costs.
The coarseness of an item is indicated with dotted lines.
</figureCaption>
<bodyText confidence="0.999930235294118">
best derivation of G → s0 ... si−1 At sj ... sn−1
is called the Viterbi outside score α(e). The goal
of a 1-best parsing algorithm is to compute the
Viterbi inside score of the edge (Gm, 0, n); the
actual best parse can be reconstructed from back-
pointers in the standard way.
We assume that each auxiliary grammar Gt−1
forms a relaxed projection of Gt. A grammar Gt−1
is a projection of Gt if there exists some many-
to-one onto function 7rt which maps each symbol
in Gt to a symbol in Gt−1; hereafter, we will use
A0t to represent 7rt(At). A projection is relaxed
if, for every rule r = At → Bt Ct with weight
wr the projection r0 = A0t → B0t C0t has weight
wr, ≤ wr in Gt−1. In other words, the weight of r0
is a lower bound on the weight of all rules r in Gt
which project to r0.
</bodyText>
<subsectionHeader confidence="0.998938">
2.2 Deduction Rules
</subsectionHeader>
<bodyText confidence="0.9999412">
HA∗ and our modification BHA∗ can be formu-
lated in terms of prioritized weighted deduction
rules (Shieber et al., 1995; Felzenszwalb and
McAllester, 2007). A prioritized weighted deduc-
tion rule has the form
</bodyText>
<equation confidence="0.9979495">
p(w1,...,wn)
01 : w1, ... , 0n : wn V 00 : g(w1, ... ,wn)
</equation>
<bodyText confidence="0.999974727272727">
where 01, ... , 0n are the antecedent items of the
deduction rule and 00 is the conclusion item. A
deduction rule states that, given the antecedents
01, ... , 0n with weights w1, ... , wn, the conclu-
sion 00 can be formed with weight g(w1, ... , wn)
and priority p(w1, ... , wn).
These deduction rules are “executed” within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of
items), as well as a chart of items already pro-
cessed. The fundamental operation of the algo-
rithm is to pop the highest priority item 0 from
the agenda, put it into the chart with its current
weight, and form using deduction rules any items
which can be built by combining 0 with items al-
ready in the chart. If new or improved, resulting
items are put on the agenda with priority given by
p(·). Because all antecedents must be constructed
before a deduction rule is executed, we sometimes
refer to particular conclusion item as “waiting” on
an other item(s) before it can be built.
</bodyText>
<subsectionHeader confidence="0.976503">
2.3 HA∗
</subsectionHeader>
<bodyText confidence="0.999827228571428">
HA∗ can be formulated in terms of two types of
items. Inside items I(At, i, j) represent possible
derivations of the edge (At, i, j), while outside
items O(At, i, j) represent derivations of G →
s1 ... si−1 At sj ... sn rooted at (Gt, 0, n). See
Figure 1(a) for a graphical depiction of these
edges. Inside items are used to compute Viterbi in-
side scores under grammar Gt, while outside items
are used to compute Viterbi outside scores.
The deduction rules which construct inside and
outside items are given in Table 1. The IN deduc-
tion rule combines two inside items over smaller
spans with a grammar rule to form an inside item
over larger spans. The weight of the resulting item
is the sum of the weights of the smaller inside
items and the grammar rule. However, the IN rule
also requires that an outside score in the coarse
grammar1 be computed before an inside item is
built. Once constructed, this coarse outside score
is added to the weight of the conclusion item to
form the priority of the resulting item. In other
words, the coarse outside score computed by the
algorithm plays the same role as a heuristic in stan-
dard A∗ parsing (Klein and Manning, 2003).
Outside scores are computed by the OUT-L and
OUT-R deduction rules. These rules combine an
outside item over a large span and inside items
over smaller spans to form outside items over
smaller spans. Unlike the IN deduction, the OUT
deductions only involve items from the same level
of the hierarchy. That is, whereas inside scores
wait on coarse outside scores to be constructed,
outside scores wait on inside scores at the same
level in the hierarchy.
Conceptually, these deduction rules operate by
</bodyText>
<footnote confidence="0.976897">
1For the coarsest grammar 91, the IN rule builds rules
using 0 as an outside score.
</footnote>
<figure confidence="0.970968333333333">
s0 .. s2 s3 s4 s5 .. sn-1
s0 s2 s3 s4 s5 .. sn-1
..
(a) Gt (b) Gt
VPt
VPt
</figure>
<page confidence="0.927868">
349
</page>
<table confidence="0.442744571428571">
HA*
w1+w2+wr+w3
IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O(A� t, i, j) : w3 � I(At, i, j) : w1 + w2 + wr
w1+w3+wr+w2
OUT-L: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3 O(Bt, i, l) : w1 + w3 + wr
w1+w2+wr+w3
OUT-R: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3 O(Ct, l, j) : w1 + w2 + wr
</table>
<tableCaption confidence="0.986222">
Table 1: HA* deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.
</tableCaption>
<table confidence="0.977086428571429">
BHA*
B-IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O(At, i, j) : w3 w1
+w2+wr+w3 � I(At, i, j) : w1 + w2 + wr
w1+wr+w2+w3
B-OUT-L: �O(At, i, j) : w1 I(B� t, i, l) : w2 I(C� t, l, j) : w3 ����������� O(Bt, i, l) : w1 + wr + w3
B-OUT-R: w1+w2+wr+w3
�O(At, i, j) : w1 I(Bt, i, l) : w2 I(C� t, l, j) : w3 ����������� �O(Ct, l, j) : w1 + w2 + wr
</table>
<tableCaption confidence="0.99891">
Table 2: BHA* deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.
</tableCaption>
<bodyText confidence="0.999318916666667">
first computing inside scores bottom-up in the
coarsest grammar, then outside scores top-down
in the same grammar, then inside scores in the
next finest grammar, and so on. However, the cru-
cial aspect of HA* is that items from all levels
of the hierarchy compete on the same queue, in-
terleaving the computation of inside and outside
scores at all levels. The HA* deduction rules come
with three important guarantees. The first is a
monotonicity guarantee: each item is popped off
the agenda in order of its intrinsic priority p(·).
For inside items I(e) over edge e, this priority
</bodyText>
<equation confidence="0.504911666666667">
p(I(e)) = Q(e) + α(e&apos;) where e&apos; is the projec-
tion of e. For outside items O(·) over edge e, this
priority is p(O(e)) = Q(e) + α(e).
</equation>
<bodyText confidence="0.999745285714286">
The second is a correctness guarantee: when
an inside/outside item is popped of the agenda, its
weight is its true Viterbi inside/outside cost. Taken
together, these two imply an efficiency guarantee,
which states that only items x whose intrinsic pri-
ority p(x) is less than or equal to the Viterbi inside
score of the goal are removed from the agenda.
</bodyText>
<subsectionHeader confidence="0.999124">
2.4 HA* with Bridge Costs
</subsectionHeader>
<bodyText confidence="0.999819285714286">
The outside scores computed by HA* are use-
ful for prioritizing computation in more refined
grammars. The key property of these scores is
that they form consistent and admissible heuristic
costs for more refined grammars, but coarse out-
side costs are not the only quantity which satisfy
this requirement. As an alternative, we propose
a novel “bridge” outside cost a(e). Intuitively,
this cost represents the cost of the best deriva-
tion where rules “above” and “left” of an edge e
come from gt, and rules “below” and “right” of
the e come from gt_1; see Figure 2 for a graph-
ical depiction. More formally, let the spine of
an edge e = (At, i, j) for some derivation d be
</bodyText>
<figure confidence="0.573901">
Gt
</figure>
<figureCaption confidence="0.989780833333333">
Figure 2: A concrete example of a possible bridge outside
derivation for the bridge item 6(VPt, 1, 4). This edge is
boxed for emphasis. The spine of the derivation is shown
in bold and colored in blue. Rules from a coarser grammar
are shown with dotted lines, and colored in red. Here we have
the simple projection πt(A) = X, `dA.
</figureCaption>
<bodyText confidence="0.976567047619048">
the sequence of rules between e and the root edge
(Gt, 0, n). A bridge outside derivation of e is a
derivation d of G —* s1 ... si At sj+1 ... sn such
that every rule on or left of the spine comes from
gt, and all other rules come from gt_1. The score
of the best such derivation for e is the bridge out-
side cost a(e).
Like ordinary outside costs, bridge outside costs
form consistent and admissible estimates of the
true Viterbi outside score α(e) of an edge e. Be-
cause bridge costs mix rules from the finer and
coarser grammar, bridge costs are at least as good
an estimate of the true outside score as entirely
coarse outside costs, and will in general be much
tighter. That is, we have
α(e&apos;) G a(e) G α(e)
In particular, note that the bridge costs become
better approximations farther right in the sentence,
and the bridge cost of the last word in the sentence
is equal to the Viterbi outside cost of that word.
To compute bridge outside costs, we introduce
</bodyText>
<figure confidence="0.996038611111111">
St
NPt
Xt-1
NPt
VPt
s0
s1 s2 s3
s4 s5
sn-1
VPt
NPt
Xt-1
NNt
VPt
NPt
Xt-1
Xt-1 Xt-1
Xt-1
</figure>
<page confidence="0.987694">
350
</page>
<bodyText confidence="0.9999655">
bridge outside items O(At, i, j), shown graphi-
cally in Figure 1(b). The deduction rules which
build both inside items and bridge outside items
are shown in Table 2. The rules are very simi-
lar to those which define HA*, but there are two
important differences. First, inside items wait for
bridge outside items at the same level, while out-
side items wait for inside items from the previous
level. Second, the left and right outside deductions
are no longer symmetric – bridge outside items
can extended to the left given two coarse inside
items, but can only be extended to the right given
an exact inside item on the left and coarse inside
item on the right.
</bodyText>
<subsectionHeader confidence="0.964867">
2.5 Guarantees
</subsectionHeader>
<bodyText confidence="0.9997165">
These deduction rules come with guarantees anal-
ogous to those of HA*. The monotonicity guaran-
tee ensures that inside and (bridge) outside items
are processed in order of:
</bodyText>
<equation confidence="0.960956">
AI(e)) = O(e) + a(e)
A O(e)) = 61(e) + O(e�)
</equation>
<bodyText confidence="0.999994">
The correctness guarantee ensures that when an
item is removed from the agenda, its weight will
be equal to Q(e) for inside items and a(e) for
bridge items. The efficiency guarantee remains the
same, though because the intrinsic priorities are
different, the set of items processed will be differ-
ent from those processed by HA*.
A proof of these guarantees is not possible
due to space restrictions. The proof for BHA*
follows the proof for HA* in Felzenszwalb and
McAllester (2007) with minor modifications. The
key property of HA* needed for these proofs is
that coarse outside costs form consistent and ad-
missible heuristics for inside items, and exact in-
side costs form consistent and admissible heuris-
tics for outside items. BHA* also has this prop-
erty, with bridge outside costs forming admissi-
ble and consistent heuristics for inside items, and
coarse inside costs forming admissible and consis-
tent heuristics for outside items.
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9993954">
The performance of BHA* is determined by the
efficiency guarantee given in the previous sec-
tion. However, we cannot determine in advance
whether BHA* will be faster than HA*. In fact,
BHA* has the potential to be slower – BHA*
</bodyText>
<figureCaption confidence="0.9970175">
Figure 3: Performance of HA* and BHA* as a function of
increasing refinement of the coarse grammar. Lower is faster.
</figureCaption>
<figure confidence="0.946624">
3 3-5 0-5
</figure>
<figureCaption confidence="0.9956414">
Figure 4: Performance of BHA* on hierarchies of varying
size. Lower is faster. Along the x-axis, we show which coarse
grammars were used in the hierarchy. For example, 3-5 in-
dicates the 3-,4-, and 5-split grammars were used as coarse
grammars.
</figureCaption>
<bodyText confidence="0.999520173913043">
builds both inside and bridge outside items under
the target grammar, where HA* only builds inside
items. It is an empirical, grammar- and hierarchy-
dependent question whether the increased tight-
ness of the outside estimates outweighs the addi-
tional cost needed to compute them. We demon-
strate empirically in this section that for hier-
archies with very loosely approximating coarse
grammars, BHA* can outperform HA*, while
for hierarchies with good approximations, perfor-
mance of the two algorithms is comparable.
We performed experiments with the grammars
of Petrov et al. (2006). The training procedure for
these grammars produces a hierarchy of increas-
ingly refined grammars through state-splitting, so
a natural projection function 7rt is given. We used
the Berkeley Parser2 to learn such grammars from
Sections 2-21 of the Penn Treebank (Marcus et al.,
1993). We trained with 6 split-merge cycles, pro-
ducing 7 grammars. We tested these grammars on
300 sentences of length &lt; 25 of Section 23 of the
Treebank. Our “target grammar” was in all cases
the most split grammar.
</bodyText>
<footnote confidence="0.859164">
2http://berkeleyparser.googlecode.com
</footnote>
<figure confidence="0.9972914">
0-split 1-split 2-split 3-split 4-split 5-split
Items Pushed (Billions)
40
20
30
10
0
BHA*
HA*
10
7.5
5
Edges Pushed (billions)
2.5
0
</figure>
<page confidence="0.993266">
351
</page>
<bodyText confidence="0.999976933333333">
In our first experiment, we construct 2-level hi-
erarchies consisting of one coarse grammar and
the target grammar. By varying the coarse gram-
mar from the 0-split (X-bar) through 5-split gram-
mars, we can investigate the performance of each
algorithm as a function of the coarseness of the
coarse grammar. We follow Pauls and Klein
(2009) in using the number of items pushed as
a machine- and implementation-independent mea-
sure of speed. In Figure 3, we show the perfor-
mance of HA* and BHA* as a function of the
total number of items pushed onto the agenda.
We see that for very coarse approximating gram-
mars, BHA* substantially outperforms HA*, but
for more refined approximating grammars the per-
formance is comparable, with HA* slightly out-
performing BHA* on the 3-split grammar.
Finally, we verify that BHA* can benefit from
multi-level hierarchies as HA* can. We con-
structed two multi-level hierarchies: a 4-level hier-
archy consisting of the 3-,4-,5-, and 6- split gram-
mars, and 7-level hierarchy consisting of all gram-
mars. In Figure 4, we show the performance of
BHA* on these multi-level hierarchies, as well as
the best 2-level hierarchy from the previous exper-
iment. Our results echo the results of Pauls and
Klein (2009): although the addition of the rea-
sonably refined 4- and 5-split grammars produces
modest performance gains, the addition of coarser
grammars can actually hurt overall performance.
</bodyText>
<sectionHeader confidence="0.996005" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999663666666667">
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.
</bodyText>
<sectionHeader confidence="0.999274" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999848333333333">
P. Felzenszwalb and D. McAllester. 2007. The gener-
alized A* architecture. Journal of Artificial Intelli-
gence Research.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In
Proceedings of the Human Language Technology
Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119–126.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proccedings of the
Association for Computational Linguistics (ACL).
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3–36.
</reference>
<page confidence="0.998258">
352
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.995832">
<title confidence="0.999977">Parsing with Bridge Outside Scores</title>
<author confidence="0.999613">Pauls Klein</author>
<affiliation confidence="0.9999295">Computer Science Division University of California at Berkeley</affiliation>
<abstract confidence="0.999789833333333">uses of a hierarchy of coarse grammars to speed up parsing sacrificing optimality. prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchia modified Hierarchial algorithm which computes a novel outside called a cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse We show that substanoutperforms when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Felzenszwalb</author>
<author>D McAllester</author>
</authors>
<title>The generalized A* architecture.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="937" citStr="Felzenszwalb and McAllester (2007)" startWordPosition="131" endWordPosition="135">tizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A* (BHA*), a modified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies. 1 Introduction The Hierarchical A* (HA*) algorithm of Felzenszwalb and McAllester (2007) allows the use of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. Pauls and Klein (2009) showed that a hierarchy of coarse grammars outperforms standard A* parsing for a range of grammars. HA* operates by computing Viterbi inside and outside scores in an agendabased way, using outside scores computed under coarse grammars as heuristics which guide the search in finer grammars. The outside scores computed by HA* are auxiliary quantities, useful only because they form admissible heuristics for search in finer grammars. We show that a modification of the HA* al</context>
<context position="5122" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="864" endWordPosition="867">rms a relaxed projection of Gt. A grammar Gt−1 is a projection of Gt if there exists some manyto-one onto function 7rt which maps each symbol in Gt to a symbol in Gt−1; hereafter, we will use A0t to represent 7rt(At). A projection is relaxed if, for every rule r = At → Bt Ct with weight wr the projection r0 = A0t → B0t C0t has weight wr, ≤ wr in Gt−1. In other words, the weight of r0 is a lower bound on the weight of all rules r in Gt which project to r0. 2.2 Deduction Rules HA∗ and our modification BHA∗ can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form p(w1,...,wn) 01 : w1, ... , 0n : wn V 00 : g(w1, ... ,wn) where 01, ... , 0n are the antecedent items of the deduction rule and 00 is the conclusion item. A deduction rule states that, given the antecedents 01, ... , 0n with weights w1, ... , wn, the conclusion 00 can be formed with weight g(w1, ... , wn) and priority p(w1, ... , wn). These deduction rules are “executed” within a generic agenda-driven algorithm, which constructs items in a prioritized fashion. The algorithm maintains an agenda (a priority queue of items), as well as a chart </context>
<context position="13503" citStr="Felzenszwalb and McAllester (2007)" startWordPosition="2433" endWordPosition="2436"> HA*. The monotonicity guarantee ensures that inside and (bridge) outside items are processed in order of: AI(e)) = O(e) + a(e) A O(e)) = 61(e) + O(e�) The correctness guarantee ensures that when an item is removed from the agenda, its weight will be equal to Q(e) for inside items and a(e) for bridge items. The efficiency guarantee remains the same, though because the intrinsic priorities are different, the set of items processed will be different from those processed by HA*. A proof of these guarantees is not possible due to space restrictions. The proof for BHA* follows the proof for HA* in Felzenszwalb and McAllester (2007) with minor modifications. The key property of HA* needed for these proofs is that coarse outside costs form consistent and admissible heuristics for inside items, and exact inside costs form consistent and admissible heuristics for outside items. BHA* also has this property, with bridge outside costs forming admissible and consistent heuristics for inside items, and coarse inside costs forming admissible and consistent heuristics for outside items. 3 Experiments The performance of BHA* is determined by the efficiency guarantee given in the previous section. However, we cannot determine in adv</context>
</contexts>
<marker>Felzenszwalb, McAllester, 2007</marker>
<rawString>P. Felzenszwalb and D. McAllester. 2007. The generalized A* architecture. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>119--126</pages>
<contexts>
<context position="7451" citStr="Klein and Manning, 2003" startWordPosition="1291" endWordPosition="1294">combines two inside items over smaller spans with a grammar rule to form an inside item over larger spans. The weight of the resulting item is the sum of the weights of the smaller inside items and the grammar rule. However, the IN rule also requires that an outside score in the coarse grammar1 be computed before an inside item is built. Once constructed, this coarse outside score is added to the weight of the conclusion item to form the priority of the resulting item. In other words, the coarse outside score computed by the algorithm plays the same role as a heuristic in standard A∗ parsing (Klein and Manning, 2003). Outside scores are computed by the OUT-L and OUT-R deduction rules. These rules combine an outside item over a large span and inside items over smaller spans to form outside items over smaller spans. Unlike the IN deduction, the OUT deductions only involve items from the same level of the hierarchy. That is, whereas inside scores wait on coarse outside scores to be constructed, outside scores wait on inside scores at the same level in the hierarchy. Conceptually, these deduction rules operate by 1For the coarsest grammar 91, the IN rule builds rules using 0 as an outside score. s0 .. s2 s3 s</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL), pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="15431" citStr="Marcus et al., 1993" startWordPosition="2745" endWordPosition="2748">tional cost needed to compute them. We demonstrate empirically in this section that for hierarchies with very loosely approximating coarse grammars, BHA* can outperform HA*, while for hierarchies with good approximations, performance of the two algorithms is comparable. We performed experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through state-splitting, so a natural projection function 7rt is given. We used the Berkeley Parser2 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 300 sentences of length &lt; 25 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 2http://berkeleyparser.googlecode.com 0-split 1-split 2-split 3-split 4-split 5-split Items Pushed (Billions) 40 20 30 10 0 BHA* HA* 10 7.5 5 Edges Pushed (billions) 2.5 0 351 In our first experiment, we construct 2-level hierarchies consisting of one coarse grammar and the target grammar. By varying the coarse grammar from the 0-split (X-bar) through 5-split grammars, we can investiga</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Hierarchical search for parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<contexts>
<context position="1061" citStr="Pauls and Klein (2009)" startWordPosition="153" endWordPosition="156">odified Hierarchial A* algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA* substantially outperforms HA* when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies. 1 Introduction The Hierarchical A* (HA*) algorithm of Felzenszwalb and McAllester (2007) allows the use of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. Pauls and Klein (2009) showed that a hierarchy of coarse grammars outperforms standard A* parsing for a range of grammars. HA* operates by computing Viterbi inside and outside scores in an agendabased way, using outside scores computed under coarse grammars as heuristics which guide the search in finer grammars. The outside scores computed by HA* are auxiliary quantities, useful only because they form admissible heuristics for search in finer grammars. We show that a modification of the HA* algorithm can compute modified bridge outside scores which are tighter bounds on the true outside costs in finer grammars. The</context>
<context position="2731" citStr="Pauls and Klein (2009)" startWordPosition="425" endWordPosition="428">ether the work saved by using tighter estimates outweighs the extra computation needed to compute them is an empirical question. In this paper, we show that the use of bridge outside costs substantially outperforms the HA* algorithm when the coarsest levels of the hierarchy are very loose approximations of the target grammar. For hierarchies with tighter estimates, we show that BHA* obtains comparable performance to HA*. In other words, BHA* is more robust to poorly constructed hierarchies. 2 Previous Work In this section, we introduce notation and review HA*. Our presentation closely follows Pauls and Klein (2009), and we refer the reader to that work for a more detailed presentation. 2.1 Notation Assume we have input sentence s0 ... sn_1 of length n, and a hierarchy of m weighted contextfree grammars !g1 ... !gryn. We call the most refined grammar !gryn the target grammar, and all other (coarser) grammars auxiliary grammars. Each grammar !gt has a set of symbols denoted with capital letters and a subscript indicating the level in the hierarchy, including a distinguished goal (root) symbol Gt. Without loss of generality, we assume Chomsky normal form, so each non-terminal rule r in !gt has the form r =</context>
<context position="16155" citStr="Pauls and Klein (2009)" startWordPosition="2865" endWordPosition="2868">s of length &lt; 25 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 2http://berkeleyparser.googlecode.com 0-split 1-split 2-split 3-split 4-split 5-split Items Pushed (Billions) 40 20 30 10 0 BHA* HA* 10 7.5 5 Edges Pushed (billions) 2.5 0 351 In our first experiment, we construct 2-level hierarchies consisting of one coarse grammar and the target grammar. By varying the coarse grammar from the 0-split (X-bar) through 5-split grammars, we can investigate the performance of each algorithm as a function of the coarseness of the coarse grammar. We follow Pauls and Klein (2009) in using the number of items pushed as a machine- and implementation-independent measure of speed. In Figure 3, we show the performance of HA* and BHA* as a function of the total number of items pushed onto the agenda. We see that for very coarse approximating grammars, BHA* substantially outperforms HA*, but for more refined approximating grammars the performance is comparable, with HA* slightly outperforming BHA* on the 3-split grammar. Finally, we verify that BHA* can benefit from multi-level hierarchies as HA* can. We constructed two multi-level hierarchies: a 4-level hierarchy consisting</context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>Adam Pauls and Dan Klein. 2009. Hierarchical search for parsing. In Proceedings of The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proccedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="15148" citStr="Petrov et al. (2006)" startWordPosition="2701" endWordPosition="2704">t grammars were used as coarse grammars. builds both inside and bridge outside items under the target grammar, where HA* only builds inside items. It is an empirical, grammar- and hierarchydependent question whether the increased tightness of the outside estimates outweighs the additional cost needed to compute them. We demonstrate empirically in this section that for hierarchies with very loosely approximating coarse grammars, BHA* can outperform HA*, while for hierarchies with good approximations, performance of the two algorithms is comparable. We performed experiments with the grammars of Petrov et al. (2006). The training procedure for these grammars produces a hierarchy of increasingly refined grammars through state-splitting, so a natural projection function 7rt is given. We used the Berkeley Parser2 to learn such grammars from Sections 2-21 of the Penn Treebank (Marcus et al., 1993). We trained with 6 split-merge cycles, producing 7 grammars. We tested these grammars on 300 sentences of length &lt; 25 of Section 23 of the Treebank. Our “target grammar” was in all cases the most split grammar. 2http://berkeleyparser.googlecode.com 0-split 1-split 2-split 3-split 4-split 5-split Items Pushed (Billi</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proccedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="5086" citStr="Shieber et al., 1995" startWordPosition="860" endWordPosition="863">iliary grammar Gt−1 forms a relaxed projection of Gt. A grammar Gt−1 is a projection of Gt if there exists some manyto-one onto function 7rt which maps each symbol in Gt to a symbol in Gt−1; hereafter, we will use A0t to represent 7rt(At). A projection is relaxed if, for every rule r = At → Bt Ct with weight wr the projection r0 = A0t → B0t C0t has weight wr, ≤ wr in Gt−1. In other words, the weight of r0 is a lower bound on the weight of all rules r in Gt which project to r0. 2.2 Deduction Rules HA∗ and our modification BHA∗ can be formulated in terms of prioritized weighted deduction rules (Shieber et al., 1995; Felzenszwalb and McAllester, 2007). A prioritized weighted deduction rule has the form p(w1,...,wn) 01 : w1, ... , 0n : wn V 00 : g(w1, ... ,wn) where 01, ... , 0n are the antecedent items of the deduction rule and 00 is the conclusion item. A deduction rule states that, given the antecedents 01, ... , 0n with weights w1, ... , wn, the conclusion 00 can be formed with weight g(w1, ... , wn) and priority p(w1, ... , wn). These deduction rules are “executed” within a generic agenda-driven algorithm, which constructs items in a prioritized fashion. The algorithm maintains an agenda (a priority </context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>