<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000589">
<title confidence="0.9982285">
Optimizing Language Model Information Retrieval System with
Expectation Maximization Algorithm
</title>
<author confidence="0.99098">
Justin Liang-Te Chiu
</author>
<affiliation confidence="0.988037">
Department of Computer Science
and Information Engineering,
National Taiwan University
</affiliation>
<address confidence="0.9512505">
#1 Roosevelt Rd. Sec. 4, Taipei,
Taiwan 106, ROC
</address>
<email confidence="0.991439">
b94902009@ntu.edu.tw
</email>
<author confidence="0.96899">
Jyun-Wei Huang
</author>
<affiliation confidence="0.987694">
Department of Computer Science
</affiliation>
<address confidence="0.73929425">
and Engineering,
Yuan Ze University
#135 Yuan-Tung Road, Chungli,
Taoyuan,Taiwan,ROC
</address>
<email confidence="0.985662">
s976017@mail.yzu.edu.tw
</email>
<sectionHeader confidence="0.996436" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995176875">
Statistical language modeling (SLM) has
been used in many different domains for dec-
ades and has also been applied to information
retrieval (IR) recently. Documents retrieved
using this approach are ranked according
their probability of generating the given
query. In this paper, we present a novel ap-
proach that employs the generalized Expecta-
tion Maximization (EM) algorithm to im-
prove language models by representing their
parameters as observation probabilities of
Hidden Markov Models (HMM). In the expe-
riments, we demonstrate that our method out-
performs standard SLM-based and tf.idf-
based methods on TREC 2005 HARD Track
data.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883416666667">
In 1945, soon after the computer was invented,
Vannevar Bush wrote a famous article---“As we
may think” (V. Bush, 1996), which formed the
basis of research into Information Retrieval (IR).
The pioneers in IR developed two models for
ranking: the vector space model (G. Salton and
M. J. McGill, 1986) and the probabilistic model
(S. E. Robertson and S. Jones, 1976). Since then,
the research of classical probabilistic models of
relevance has been widely studied. For example,
Robertson (S. E. Robertson and S. Walker, 1994;
S. E. Robertson, 1977) modeled word occur-
rences into relevant or non-relevant classes, and
ranked documents according to the probabilities
they belong to the relevant one. In 1998, Ponte
and Croft (1998) proposed a language modeling
framework which opens a new point of view in
IR. In this approach, they gave up the model of
relevance; instead, they treated query generation
as random sampling from every document model.
The retrieval results were based on the probabili-
ties that a document can generate the query string.
Several improvements were proposed after their
work. Song and Croft (1999), for example, was
the first to bring up a model with bi-grams and
Good Turing re-estimation to smooth the docu-
ment models. Latter, Miller et al. (1999) used
Hidden Markov Model (HMM) for ranking,
which also included the use of bigrams.
HMM, firstly introduced by Rabiner and Juain
(1986) in 1986, has been successfully applied
into many domains, such as named entity recog-
nition (D. M. Bikel et al., 1997), topic classifica-
tion (R. Schwartz et al., 1997), or speech recog-
nition (J. Makhoul and R. Schwartz, 1995). In
practice, the model requires solving three basic
problems. Given the parameters of the model,
computing the probability of a particular output
sequence is the first problem. This process is of-
ten referred to as decoding. Both Forward and
Backward procedure are solutions for this prob-
lem. The second problem is finding the most
possible state sequence with the parameters of
the model and a particular output sequence. This
is usually completed with Viterbi algorithm. The
third problem is the learning problem of HMM
models. It is often solved by Baum-Welch algo-
rithm (L. E. Bmjm et al., 1970). Given training
</bodyText>
<page confidence="0.993645">
63
</page>
<note confidence="0.997005">
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 63–71,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999849305555556">
data, the algorithm computes the maximum like-
lihood estimates and posterior mode estimate. It
is in essence a generalized Expectation Maximi-
zation (EM) algorithm which was first explained
and given name by Dempster, Laird and Rubin
(1977) in 1977. EM can estimate the maximum
likelihood of parameters in probabilistic models
which has unseen variables. Nonetheless, in our
knowledge, the EM procedure in HMM has nev-
er been used in IR domain.
In this paper, we proposed a new language
model approach which models the user query
and documents as HMM models. We then used
EM algorithm to maximize the probability of
query words in our model. Our assumption is
that if the word’s probability in a document is
maximized, we can estimate the probability of
generating the query word from documents more
confidently. Because they not only been calcu-
lated by language modeling view features, but
also been maximized with statistical methods.
Therefore the imprecise cases caused by special
distribution in language modeling approach can
be further prevented in this way.
The remainders of this paper are organized as
follows. We review two related works in Section
2. In Section 3, we introduce our EM IR ap-
proach. Section 4 compares our results to two
other approaches proposed by Song and Corft
(1999) and Robertson (1995) based on the data
from TREC HARD track (J. Allan, 2005). Sec-
tion 5 discusses the effectiveness of our EM
training and the EM-based document weighting
we proposed. Finally, we conclude our paper in
Section 6 and provide some future directions at
Section 7.
</bodyText>
<sectionHeader confidence="0.999032" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.99834275">
Even if we only focus on the probabilistic ap-
proach to IR, it is still impossible to discuss all
up-to-date research. Instead we focus on two
previous works which have inspired the work
reported in this paper: the first is a general lan-
guage model approach proposed by Song and
Croft (1999) and the second is a HMM approach
by Miller et al. (1999).
</bodyText>
<subsectionHeader confidence="0.817401">
2.1 A General Language Model for IR
</subsectionHeader>
<bodyText confidence="0.999751833333333">
In 1999, Song and Croft (1999) introduced a lan-
guage model based on a range of data smoothing
technique. The following are some of the fea-
tures they used:
Good-Turing estimate: Since the effect of
Good-Turing estimate was verified as one of the
best discount methods (C. D. Manning and H.
Schutze, 1999), Song and Croft used Good-
Turing estimate for allocating proper probability
for the missing terms in the documents. The
smoothed probability for term t in document d
can be obtained with the following formula:
</bodyText>
<equation confidence="0.955846">
(t� � 1)S(N����)
P�T(t|d) �
S(Ntf)Nd
</equation>
<bodyText confidence="0.999142214285714">
where Ntf is the number of terms with frequency
tf in a document. Nd is the total number of terms
occurred in document d, and a powerful smooth-
ing function S(Ntf), which is used for calculating
the expected value of Ntf regardless of the Ntf ap-
pears in the corpus or not.
Expanding document model: The document
model can be viewed as a smaller part of whole
corpus. Due to its limited size, there is a large
number of missing terms in documents, and can
lead to incorrect distributions of known terms.
For dealing with the problem, documents can be
expanded with the following weighted
sum/product approach:
</bodyText>
<equation confidence="0.999954">
PsuM(t|d) = w X Pdoc(t|d) + (1 — w) X Pcorpus(t)
Pproduct(t|d) = Pdoc(t|d)0) X Pcorpus(t)(1-0))
</equation>
<bodyText confidence="0.998662375">
where w is a weighting parameter between 0 and
1.
Modeling Query as a Sequence of Terms:
Treating a query as a set of terms is commonly
seen in IR researches. Song and Croft treated
queries as a sequence of terms, and obtained the
probability of generating the query by multiply-
ing the individual term probabilities.
</bodyText>
<equation confidence="0.998187333333333">
M
Ps equ enc e (Q|d) _ fj P(ti|d)
i=1
</equation>
<bodyText confidence="0.973293666666667">
where t1, t2 ..., tm is the sequence of terms in a
query Q.
Combining the Unigram Model with the
Bigram Model: This is commonly implemented
with interpolation in statistical language model-
ing:
</bodyText>
<equation confidence="0.910903">
P(ti-1, ti|d) = Al X Pl(ti|d) + A2 X P2(ti-1, ti|d)
</equation>
<bodyText confidence="0.9993574">
where Al and A2 are two parameters, and Al + A2
= 1. Such interpolation can be modeled by HMM,
and can learn the appropriate value from the cor-
pus through EM procedure. A similar procedure
is described in Hiemstra and Vries (2000).
</bodyText>
<subsectionHeader confidence="0.885642">
2.2 A HMM Information Retrieval System
</subsectionHeader>
<page confidence="0.995305">
64
</page>
<bodyText confidence="0.949882714285714">
∑k number of times q appears in Dk
Miller et al. demonstrated an IR system based on
HMM. With a query Q, Miller et al. tried to rank
the documents according to the probability that
D is relevant (R) with it, which can be written as
P(D is R|Q). With Baye’s rule, the core formula
of their approach is:
</bodyText>
<equation confidence="0.928206">
P(q|GE) _ ∑k length of Dk
</equation>
<bodyText confidence="0.913603">
with these estimated parameters, they state the
formula for P(Q|D is R) corresponding to Figure
1 as:
</bodyText>
<equation confidence="0.798723666666667">
P(D is R|Q) _ P(Q|D is R) · P(D is R) P(Q|Dk is R) _ fj(a0P(q|GE) + a,P(q|Dk))
qEQ
P(Q)
</equation>
<bodyText confidence="0.999909285714286">
where P(Q|D is R) is the probability of query Q
being posed by a relevant document D; P(D is R)
is the prior probability that D is relevant; P(Q) is
the prior probability of Q. Because P(Q) will be
identical, and the P(D is R) is assumed to be con-
stant across all documents, they place their focus
on P(Q|D is R).
To figure out the value of P(Q|D is R), they
established a HMM. The union of all words ap-
pearing in the corpus is taken as the observation,
and each different mechanism of query word
generation represent a state. So the observation
probability from different states is according to
the output distribution of the state.
</bodyText>
<figureCaption confidence="0.9297425">
Figure 1. HMM proposed in “A Hidden Markov
Model Information Retrieval System”
</figureCaption>
<bodyText confidence="0.997608789473684">
To estimate the transition and observation
probabilities of HMM, EM algorithm is the stan-
dard method for parameter estimation. However,
due to some difficulty, they make two practical
simplifications. First, they assume the transition
probabilities are same for all documents, since
they establish an individual HMM for each doc-
ument. Second, they completely abandon the EM
algorithm for the estimation of observation prob-
abilities. Instead, they use simple maximum like-
lihood estimates for each documents. So the
probabilities which their HMM generate term q
from their HMM states become:
number of times q appears in Dk
length of Dk
the probabilities obtained through this formula
is then used for calculating the P(D is R|Q). The
document is then ranked according to the value
of P(D is R|Q).
The HMM model we proposed is far different
from Miller et al. (1999). They build HMM for
every document, and treat all words in the docu-
ment as one state’s observation, and word that is
unrelated to the document, but occurs commonly
in natural language queries as another state’s ob-
servation. Hence, their approach requires infor-
mation about the words which appears common-
ly in natural language. The content of the pro-
vided information will also affect the IR result,
hence it is unstable. We assume that every doc-
ument is an individual state, and the probabilities
of query words generated by this document as
the observation probabilities. Our HMM model
is built on the corpus we used and does not need
further information. This will make our IR result
fit on our corpus and not affected by outside in-
formation. It will be detailed introduced at Sec-
tion 3.
</bodyText>
<sectionHeader confidence="0.988575" genericHeader="method">
3 Our EM IR approach
</sectionHeader>
<bodyText confidence="0.996182105263158">
We formulate the IR problem as follows: given a
query string and a set of documents, we rank the
documents according to the probability of each
document for generating the query terms. Since
the EM procedure is very sensitive to the number
of states, while a large number of states take
much time for one run, we firstly apply a basic
language modeling method to reduce our docu-
ment set. This language modeling method will be
detailed at Section 3.1. Based on the reduced
document set, we then describe how to build our
HMM model, and demonstrate how to obtain the
special-designed observance sequence for our
HMM training in Section 3.2 and 3.3, respective-
ly. Finally, Section 3.4 introduces the evaluation
mechanism to the probability of generating the
query for each document.
3.1 The basic language modeling method
for document reduction
</bodyText>
<equation confidence="0.973731">
P(q|Dk) _
</equation>
<page confidence="0.989635">
65
</page>
<bodyText confidence="0.999390466666667">
Suppose we have a huge document set D, and a
query Q, we firstly reduce the document set to
obtain the document Dr. We require the reducing
method can be efficiently computed, therefore
two methods proposed by Song and Croft (1999)
are consulted with some modifications: Good-
Turing estimation and modeling query as a se-
quence of terms.
In our modified Good-Turing estimation, we
gathered the number of terms to calculate the
term frequency (tf) information in our document
set. Table 1 shows the term distribution of the
AQUAINT corpus which is used in the TREC
2005 HARD Track (J. Allan, 2005). The detail of
the dataset is described in Section 4.1.
</bodyText>
<table confidence="0.741477833333334">
tf Ntf tf Ntf
0 1,140,854,966,460 5 3,327,633
1 166,056,563 6 2,163,538
2 29,905,324 7 1,491,244
3 11,191,786 8 1,089,490
4 5,668,929 9 819,517
</table>
<tableCaption confidence="0.997479">
Table 1. Term distribution in AQUAINT corpus
</tableCaption>
<bodyText confidence="0.999941666666667">
In this table, Ntf is the number of terms with
frequency tf in a document. The tf = 0 case in the
table means the number of words not appear in a
document. If the number of all word in our cor-
pus is W, and the number of word in a document
d is wd, then for each document, the tf = 0 will
add W – wd. By listing all frequency in our doc-
ument set, we adapt the formula defined in (Song
and Croft, 1999) as follows:
</bodyText>
<equation confidence="0.7717345">
(tf + 1)Ntf+1
NtfNd
</equation>
<bodyText confidence="0.999634714285714">
In our formula, the Nd means the number of word
tokens in the document d. Moreover, the smooth-
ing function is replaced with accurate frequency
information, Ntf and Ntf+1. Obviously, there could
be two problems in our method: First, while in
high frequency, there might be some missing
Ntf+1, because not all frequency is continuously
appear. Second, the Ntf+1 for the highest tf is zero,
this will lead to its PmGT become zero. Therefore,
we make an assumption to solve these problems:
If the Ntf+1 is missing, then its value is the same
as Ntf. According to Table 1, we can find out that
the difference between tf and tf+1 is decreasing
when the tf becomes higher. So we assume the
difference becomes zero when we faced the
missing frequency at a high number. This as-
sumption can help us ensure the completeness of
our frequency distribution.
Aside from our Good-Turing estimation de-
sign, we also treat query as a sequence of terms.
There are two reasons to make us made this deci-
sion. By doing so, we will be able to handle the
duplicate terms in the query. Furthermore, it will
enable us to model query phrase with local con-
texts. So our document score with this basic me-
thod can be calculated by multiplying PmGT(q|d)
for every q in Q. We can obtain Dr with the top
50 scores in this scoring method.
</bodyText>
<subsectionHeader confidence="0.995766">
3.2 HMM model for EM IR
</subsectionHeader>
<bodyText confidence="0.999008833333333">
Once we have the reduced document set Dr, we
can start to establish our HMM model for EM IR.
This HMM is designed to use the EM procedure
to modify its parameters, and its original parame-
ters are given by the basic language modeling
approach calculation.
</bodyText>
<figureCaption confidence="0.96238">
Figure 2. HMM model for EM IR
</figureCaption>
<bodyText confidence="0.999794130434783">
We define our HMM model as a four-tuple,
{S,A,B,π}, where S is a set of N states, A is a
NXN matrix of state transition probabilities, B is
a set of N probability functions, each describing
the observation probability with respect to a state
and π is the vector of the initial state probabili-
ties.
In our HMM model, it composes of |Dr|+1
states. Every document in the document set is
treated as an individual state in our HMM model.
Aside from these document states, we add a spe-
cial state called “Initial State”. This state is the
only one not associate with any document in our
document sets. Figure 2 illustrates the proposed
HMM IR model.
The transition probabilities in our HMM can
be classified into two types. For the “Initial
State”, the transition to the other state can be re-
gard as the probability of choosing that docu-
ment. We assume that every document has the
same probability to be chosen at the beginning,
so the transition probabilities for “Initial State”
are 1/|Dr |to every document state. For the docu-
</bodyText>
<equation confidence="0.975471">
PMGT(t|d) =
</equation>
<page confidence="0.771994">
66
</page>
<bodyText confidence="0.998761078947369">
ment states, their transition probabilities are
fixed: 100% to the “Initial State”. Since the tran-
sition between documents has no statistical
meaning, we make the state transition after the
document state back to the Initial State. This de-
sign helps us to keep the independency between
the query words. We will detail this part at Sec-
tion 3.3.
The observation probabilities for each state are
similar with the concept of language modeling.
There are three types of observations in our
HMM model.
Firstly, for every document, we can obtain the
observation probability for each query term ac-
cording to our basic language modeling method.
Even if the query term is not in the document, it
will be assigned a small value according to the
method described in Section 3.1.
Secondly, for the terms in a document, which
is not part of our query terms, are treating as
another observation. Since we mainly focus on
the probability of generating the query terms
from the documents, the rest terms are treated as
the same type which means “not the query term”.
The last type of observation is a special im-
posed token “$” which has 100% observation
probability at the Initial State.
Figure 3 shows a complete built HMM model
for EM IR. The transition probability from Initial
State is labeled with trans(dn), and the observa-
tion probability in the document state and Initial
State is showed with “ob”. The “N” symbol
represents the “not the query term”. Summing all
the token mentioned above, all possible observa-
tions for our HMM model are |Q|+2. The possi-
ble observation for each state is bolded, so we
can see the difference between Initial State and
Document State.
</bodyText>
<figureCaption confidence="0.946464">
Figure 3. A complete built HMM model for EM
IR with parameters
</figureCaption>
<bodyText confidence="0.993072120689655">
For Initial State, the observations are fixed with
100% for $ token. This special token help we
ensure the independency between the query
terms. The effect of this token will be discussed
in Section 3.3. For the document states, the prob-
abilities for the query terms are calculated with
the simple language modeling approach. Even if
the query term is not in the document, it will be
assigned a small value according to the basic
language modeling method. The rest of the terms
in a document are treating as another kind of ob-
servation, which is the “N” symbol in the Figure
3. Since we mainly focus on the probability of
generating the query terms from the documents,
the rest of the words are treated as the same kind
which means “not the query term”. Additionally,
each document state represents a document, so
the $ token will never been observed in them.
3.3 The observance sequence and EMM
training procedure
After establishing the HMM model, the observa-
tion sequence is another necessary part for our
HMM training procedure. The observation se-
quence used in HMM training means the trend
for the observation while running HMM. In our
approach, since we want to find out the docu-
ment which is more related with our query, so we
use the query terms as our observation sequence.
During the state transition with query, we can
maximize the probability for each document to
generate our query. This will help us figure out
which document is more related with our query.
Due to the state transitions in the proposed
HMM model are required to go back to the Ini-
tial State after transiting to the document state,
generating the pure query terms observation se-
quence is impossible, because the Initial State
won’t produce any query term. Therefore, we
add the $ token into our observation sequence
before each query terms. For instance, if we are
running a HMM training with query “a b c“, the
exact observation sequence for our HMM train-
ing becomes “$ a $ b $ c”. Additionally, each
document state represents a document, so the $
token will never been observed in them. By tun-
ing our HMM model with the data from our
query instead of other validation data, we can
focus on the document we want more precisely.
The reason why we use this special setting for
EM training procedure is because we are trying
to maintain the independency assumption for
query terms in HMM. The HMM observance
sequence not only shows the trend of this mod-
el’s observation, but also indicate the dependen-
cy between these observations. However, the
independency between all query terms is a com-
mon assumption for IR system (F. Song and W.
B. Croft, 1999; V. Lavrenko and W. B. Croft,
</bodyText>
<page confidence="0.998841">
67
</page>
<bodyText confidence="0.999190833333333">
2001; A. Berger and J. Lafferty, 1999). To en-
sure this assumption still works in our HMM
system, we use the Initial State to separate each
transition to the document state and observe the
query terms. No matter the early or late the query
term t occurs, the training procedure is fixed as
“Starting from the Initial state and observed $,
transit to a document state, and observe t”.
We’ve made experiments to verify the indepen-
dency assumption still work, and the result re-
mains the same no matter how we change the
order of our query terms.
After constructing the HMM model and the
observance sequence, we can start our EM train-
ing procedure. EM algorithm is used for finding
maximum likelihood estimates of parameters in
probabilistic models, where the model depends
on unobserved latent variables. In our experi-
ment, we use EM algorithm to find the parame-
ters of our HMM model. These parameters will
be used for information retrieval. The detail im-
plementation information can be found in (C. D.
Manning and H. Schutze, 1999), which introduce
HMM and the training procedure very well.
</bodyText>
<subsectionHeader confidence="0.986323">
3.4 Scoring the documents with EM-trained
HMM model
</subsectionHeader>
<bodyText confidence="0.999946571428572">
When the training procedure is completed, each
document will have new parameters for the
word’s observation probability. Moreover, the
transition probabilities from Initial State to the
document state are no longer uniform due to the
EM training. So the probability for a document d
to generate the query Q becomes:
</bodyText>
<equation confidence="0.977363">
P(Q|d) = trans(d) * fJP(q|d)
qGQ
</equation>
<bodyText confidence="0.999991090909091">
In this formula, the trans(d) means the transi-
tion probability from the Initial State to the doc-
ument state of d, which we called “EM-based
document weighting”. The P(q|d) means the ob-
servation probability for query term q in docu-
ment state of d, which is also tuned in our EM
training procedure. With this formula, we can
rank the IR result according to this probability.
This performs better than the GLM when the
document size is relatively small, since GLM
gives those documents as with too high score.
</bodyText>
<sectionHeader confidence="0.997356" genericHeader="method">
4 Experiment Results
</sectionHeader>
<subsectionHeader confidence="0.999906">
4.1 Data Set
</subsectionHeader>
<bodyText confidence="0.99989175">
We use the AQUAINT corpus as our training
data set. It is used in the TREC 2005 HARD
Track (J. Allan, 2005). The AQUAINT corpus is
prepared by the LDC for the AQUAINT Project,
and is used in official benchmark evaluations
conducted by National Institute of Standards and
Technology (NIST). It contains news from three
sources: the Xinhua News Service (People&apos;s Re-
public of China), the New York Times News
Service, and the Associated Press Worldstream
News Service.
The topics we used are the same as the TREC
Robust track (E. M. Voorhees, 2005), which are
the topics from number 303 to number 689 of the
TREC topics. Each topic is described in three
formats including titles, descriptions and narra-
tives. In our experiment, due to the fact that our
observation sequence is very sensitive to the
query terms, we only focus on the title part of the
topic. In this way, we can avoid some commonly
appeared words in narratives or descriptions,
which may reduce the precision of our training
procedure for finding the real document. Table 2
shows the detail about the corpus.
</bodyText>
<table confidence="0.993171">
Datasize 2.96GB
#Documents 1,030,561
#Querys 50
Term Types 2,002,165
Term Tokens 431,823,255
</table>
<tableCaption confidence="0.999743">
Table 2. Statistics of the AQUAINT corpus
</tableCaption>
<subsectionHeader confidence="0.998488">
4.2 Experiment Design and Results
</subsectionHeader>
<bodyText confidence="0.999926153846154">
By using the AQUAINT corpus, two different
traditional IR methods are implemented for com-
paring. The two IR methods which we use as
baselines are the General Language Modeling
(GLM) proposed by Song and Croft (1999) and
the tf.idf measure proposed by Robertson (1995).
The GLM has been introduced in Section 2. The
following formulas show the core of tf.idf:
N is the number of documents in the corpus; nq is
the number of documents in the corpus contain-
ing q; tf(q, D) is the number of times q appears in
D; l(D) is the length of D in words and the al is
the average length in words of a D in the corpus.
</bodyText>
<equation confidence="0.995653">
tf. idf(Q, D) = I wtf(qi, D) · idf(qi)
!RIM
tf(q, D)
idf(q) = N
log
a
N + 1
wtf(q, D) =
tf(q, D) + 0.5 + 1.5 1 (al
)
</equation>
<page confidence="0.994752">
68
</page>
<bodyText confidence="0.9994775">
For the proposed EM IR approach, two confi-
gurations are listed to compare. The first (Con-
fig.1) is the proposed HMM model without mak-
ing use of the EM-based document weighting
that is don’t multiply the transition probability,
trans(d), in equation (2). The second (Config.2)
is the HMM model with EM-based document
weighting. The comparison is based on precision.
For each problem, we retrieved the documents
with the highest 20 scores, and divided the num-
ber of correct answer with the number of re-
trieved document to obtain precision. If there are
documents with same score at the rank of 20, all
of them will be retrieved.
</bodyText>
<table confidence="0.9981228">
Methods Precision %Change %Change
tf.idf 29.7% -
GLM 30.5% 2.69% -
Config.1 28.8% -5.58% -3.14%
Config.2 32.2% 8.41% 5.57%
</table>
<tableCaption confidence="0.940605">
Table 3. Experiment Results of three IR methods
on the AQUAINT corpus
</tableCaption>
<bodyText confidence="0.984064666666667">
As shown in Table 3, our EM IR system out-
performs tf.idf method 8.41% and GLM method
5.57%.
</bodyText>
<sectionHeader confidence="0.997675" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99963375">
In this section, we will discuss the effective-
ness of the EM-based document weighting and
the EM procedure. Both of them rely on the
HMM design we have proposed.
</bodyText>
<subsectionHeader confidence="0.985912">
5.1 The effectiveness of EM-based docu-
ment weighting
</subsectionHeader>
<bodyText confidence="0.9999754">
When we establish our HMM model, the transi-
tion probability from Initial State to the docu-
ment state is assigned as uniform, since we don’t
have any information about the importance of
every document. These transition probabilities
represent the probability of choosing the docu-
ment with the given observation sequence.
During EM training procedure, the transition
probability, exclusive the transition probability
from document states which is fixed to 100% to
the Initial State, will be re-estimated according to
the observation sequence (the query) and the ob-
servation probabilities of each state. As shown in
Table 3, two configurations (Config.1 and Con-
fig.2) are conducted to verify the effectiveness of
using the transition probability.
The transition probability works due to the
EM training procedure. The training procedure
works for maximizing the probability for gene-
rating the query words, so the weight for each
document will be given according to mathemati-
cal formula. The advantage of this mechanism is
it will use the same formula regardless of differ-
ent content of document. Yet other statistical me-
thods will have to fix the content or formula pre-
viously to avoid the noise or other disturbance.
Some researches employee the number of terms
in the document to calculate the document
weighting. Since the observation probability al-
ready use the number of words in a document Nd
as a parameter, using number of words as docu-
ment weight will make it affect too much in our
system.
The experiment results show an improvement
of 11.80% by using the transition probability of
Initial State. Accordingly, we can understand that
the EM procedure helps our HMM model not
only on the observation probability of generating
query words, but also suggests a useful weight
for each document.
</bodyText>
<subsectionHeader confidence="0.995167">
5.2 The effectiveness of EM training
</subsectionHeader>
<bodyText confidence="0.9998576">
In HMM model training, the iteration numbers of
EM procedure is always a tricky issue for expe-
riment design. While training with too much ite-
ration will lead to overfitting for the observation
sequence, to less iteration will weaken the effect
of EM training.
For our EM IR system, we’ve made a series of
experiments with different iterations for examin-
ing the effect of EM training. Figure 3 shows the
results.
</bodyText>
<sectionHeader confidence="0.7173705" genericHeader="method">
Precision
(ON
</sectionHeader>
<page confidence="0.923970125">
32.4
32.2
32
31.8
31.6
31.4
31.2
31
</page>
<figure confidence="0.7166572">
30.8
30.6
30.4
0 1 2 3 4 5
Iterations
</figure>
<figureCaption confidence="0.998703">
Figure 4. The precision change with the EM
</figureCaption>
<bodyText confidence="0.9361813">
training iterations
As you can see in Figure 4, the precision in-
creased with the iteration numbers. Still, the
growing rate of precision becomes very slow
after 2 iterations. We have analysis this result
and find out two possible causes for this evi-
dence. First, the training document sets are li-
mited in a small size due to the computation time
complexity for our approach. Therefore we can
only retrieve correct document with high score in
</bodyText>
<page confidence="0.997718">
69
</page>
<bodyText confidence="0.99963475">
basic language modeling, which is used for doc-
ument reduction. So the precision is also limited
with the performance of our reducing methods.
The number of correct answer is limited by the
basic language modeling, so as the highest preci-
sion our system can achieve. Second, our obser-
vation only composed query terms, which gives a
limited improving space.
</bodyText>
<sectionHeader confidence="0.998911" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999978842105263">
We have proposed a method for using EM algo-
rithm to improve the precision in information
retrieval. This method employees the concept of
language model approach, and merge it with the
HMM. The transition probability in HMM is
treated as the probability of choosing the docu-
ment, and the observation probability in HMM is
treated as the probability of generating the terms
for the document. We also implement this me-
thod, and compare it with two existing IR me-
thods with the dataset from TREC 2005 HARD
Track. The experiment results show that the pro-
posed approach outperforms two existing me-
thods by 2.4% and 1.6% in precision, which are
8.08% and 5.24% increasing for the existing me-
thod. The effectiveness of using the tuned transi-
tion probability and EM training procedure is
also discussed, and been proved can work effec-
tively.
</bodyText>
<sectionHeader confidence="0.999699" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999980736842105">
Since we have achieved such improvement with
EM algorithm, other kinds of algorithm with
similar functions can also be tried in IR system.
It might be work in the form of parameter re-
estimation, tuning or even generating parameters
by statistical measure.
For the method we have proposed, we also
have some part can be done in the future. Finding
a better observance sequence will be an impor-
tant issue. Since we use the exact query terms as
our observance sequence, it’s possible to use the
method like statistical translation to generate
more words which are also related with the doc-
uments we want and used as observance se-
quence.
Another possible issue is to integrate the bi-
gram or trigram information into our training
procedure. Corpus information might be used in
more delicate way to improve the performance.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.997957972972973">
A. Berger and J. Lafferty, &amp;quot;Information retrieval as
statistical translation,&amp;quot; 1999, pp. 222-229.
A. P. Dempster, N. M. Laird, and D. B. Rubin, &amp;quot;Max-
imum likelihood from incomplete data via the EM
algorithm,&amp;quot; Journal of the Royal Statistical Society,
vol. 39, pp. 1-38, 1977.
C. D. Manning and H. Schutze, Foundations of statis-
tical natural language processing: MIT Press,
1999.
D. Hiemstra and A. P. de Vries, Relating the new lan-
guage models of information retrieval to the tradi-
tional retrieval models: University of Twente
[Host]; University of Twente, Centre for Telemat-
ics and Information Technology, 2000.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weische-
del, &amp;quot;Nymble: a high-performance learning name-
finder,&amp;quot; 1997, pp. 194-201.
D. R. H. Miller, T. Leek, and R. M. Schwartz, &amp;quot;A
hidden Markov model information retrieval sys-
tem,&amp;quot; 1999, pp. 214-221.
E. M. Voorhees, &amp;quot;The TREC robust retrieval track,&amp;quot;
2005, pp. 11-20.
F. Song and W. B. Croft, &amp;quot;A general language model
for information retrieval,&amp;quot; 1999, pp. 316-321.
G. Salton and M. J. McGill, Introduction to Modern
Information Retrieval: McGraw-Hill, Inc. New
York, NY, USA, 1986.
J. Allan, &amp;quot;HARD track overview in TREC 2005: High
accuracy retrieval from documents,&amp;quot; 2005.
J. Makhoul and R. Schwartz, &amp;quot;State of the Art in Con-
tinuous Speech Recognition,&amp;quot; Proceedings of the
National Academy of Sciences, vol. 92, pp. 9956-
9963, 1995.
J. M. Ponte and W. B. Croft, &amp;quot;A language modeling
approach to information retrieval,&amp;quot; 1998, pp. 275-
281.
L. E. Bmjm, T. Petrie, G. Soules, and N. Weiss, &amp;quot;A
</reference>
<sectionHeader confidence="0.997831333333333" genericHeader="method">
MAXIMIZATION TECHNIQUE OCCURRING
IN THE STATISTICAL ANALYSIS OF PROB-
ABILISTIC FUNCTIONS OF MARKOV
</sectionHeader>
<reference confidence="0.993311333333333">
CHAINS,&amp;quot; The Annals of Mathematical Statistics,
vol. 41, pp. 164-171, 1970.
L. Rabiner and B. Juang, &amp;quot;An introduction to hidden
Markov models,&amp;quot; ASSP Magazine, IEEE [see also
IEEE Signal Processing Magazine], vol. 3, pp. 4-
16, 1986.
R. Schwartz, T. Imai, F. Kubala, L. Nguyen, and J.
Makhoul, &amp;quot;A Maximum Likelihood Model for
Topic Classification of Broadcast News,&amp;quot; 1997.
S. E. Robertson, &amp;quot;The probability ranking principle in
IR,&amp;quot; Journal of Documentation, vol. 33, pp. 294-
304, 1977.
</reference>
<page confidence="0.968778">
70
</page>
<reference confidence="0.995633857142857">
S. E. Robertson and S. Jones, &amp;quot;Relevance Weighting
of Search Terms,&amp;quot; Journal of the American Society
for Information Science, vol. 27, pp. 129-46, 1976.
S. E. Robertson and S. Walker, &amp;quot;Some simple effec-
tive approximations to the 2-Poisson model for
probabilistic weighted retrieval,&amp;quot; 1994, pp. 232-
241.
S. E. Robertson, S. Walker, and S. Jones, &amp;quot;M. Han-
cock-Beaulieu, M., and Gatford, M.(1995). Okapi
at TREC-3,&amp;quot; pp. 109–126.
V. Bush, &amp;quot;As we may think,&amp;quot; interactions, vol. 3, pp.
35-46, 1996.
V. Lavrenko and W. B. Croft, &amp;quot;Relevance based lan-
guage models,&amp;quot; 2001, pp. 120-127.
</reference>
<page confidence="0.999144">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.421566">
<title confidence="0.9994945">Optimizing Language Model Information Retrieval System with Expectation Maximization Algorithm</title>
<author confidence="0.999056">Justin Liang-Te Chiu</author>
<affiliation confidence="0.999819666666667">Department of Computer Science and Information Engineering, National Taiwan University</affiliation>
<address confidence="0.979037">Taiwan 106, ROC</address>
<email confidence="0.87895">b94902009@ntu.edu.tw</email>
<author confidence="0.999632">Jyun-Wei Huang</author>
<affiliation confidence="0.992476333333333">Department of Computer Science and Engineering, Yuan Ze University</affiliation>
<address confidence="0.637759">Taoyuan,Taiwan,ROC</address>
<email confidence="0.880126">s976017@mail.yzu.edu.tw</email>
<abstract confidence="0.989004588235294">Statistical language modeling (SLM) has been used in many different domains for decades and has also been applied to information retrieval (IR) recently. Documents retrieved using this approach are ranked according their probability of generating the given query. In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM). In the experiments, we demonstrate that our method outperforms standard SLM-based and tf.idfbased methods on TREC 2005 HARD Track data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Information retrieval as statistical translation,&amp;quot;</title>
<date>1999</date>
<pages>222--229</pages>
<marker>Berger, Lafferty, 1999</marker>
<rawString>A. Berger and J. Lafferty, &amp;quot;Information retrieval as statistical translation,&amp;quot; 1999, pp. 222-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm,&amp;quot;</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<pages>1--38</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin, &amp;quot;Maximum likelihood from incomplete data via the EM algorithm,&amp;quot; Journal of the Royal Statistical Society, vol. 39, pp. 1-38, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schutze</author>
</authors>
<title>Foundations of statistical natural language processing:</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. D. Manning and H. Schutze, Foundations of statistical natural language processing: MIT Press, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hiemstra</author>
<author>A P</author>
</authors>
<title>de Vries, Relating the new language models of information retrieval to the traditional retrieval models:</title>
<date>2000</date>
<institution>University of Twente [Host]; University of Twente, Centre for Telematics and Information Technology,</institution>
<marker>Hiemstra, P, 2000</marker>
<rawString>D. Hiemstra and A. P. de Vries, Relating the new language models of information retrieval to the traditional retrieval models: University of Twente [Host]; University of Twente, Centre for Telematics and Information Technology, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder,&amp;quot;</title>
<date>1997</date>
<pages>194--201</pages>
<contexts>
<context position="2611" citStr="Bikel et al., 1997" startWordPosition="397" endWordPosition="400">m sampling from every document model. The retrieval results were based on the probabilities that a document can generate the query string. Several improvements were proposed after their work. Song and Croft (1999), for example, was the first to bring up a model with bi-grams and Good Turing re-estimation to smooth the document models. Latter, Miller et al. (1999) used Hidden Markov Model (HMM) for ranking, which also included the use of bigrams. HMM, firstly introduced by Rabiner and Juain (1986) in 1986, has been successfully applied into many domains, such as named entity recognition (D. M. Bikel et al., 1997), topic classification (R. Schwartz et al., 1997), or speech recognition (J. Makhoul and R. Schwartz, 1995). In practice, the model requires solving three basic problems. Given the parameters of the model, computing the probability of a particular output sequence is the first problem. This process is often referred to as decoding. Both Forward and Backward procedure are solutions for this problem. The second problem is finding the most possible state sequence with the parameters of the model and a particular output sequence. This is usually completed with Viterbi algorithm. The third problem i</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel, &amp;quot;Nymble: a high-performance learning namefinder,&amp;quot; 1997, pp. 194-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R H Miller</author>
<author>T Leek</author>
<author>R M Schwartz</author>
</authors>
<title>A hidden Markov model information retrieval system,&amp;quot;</title>
<date>1999</date>
<pages>214--221</pages>
<contexts>
<context position="2357" citStr="Miller et al. (1999)" startWordPosition="354" endWordPosition="357">abilities they belong to the relevant one. In 1998, Ponte and Croft (1998) proposed a language modeling framework which opens a new point of view in IR. In this approach, they gave up the model of relevance; instead, they treated query generation as random sampling from every document model. The retrieval results were based on the probabilities that a document can generate the query string. Several improvements were proposed after their work. Song and Croft (1999), for example, was the first to bring up a model with bi-grams and Good Turing re-estimation to smooth the document models. Latter, Miller et al. (1999) used Hidden Markov Model (HMM) for ranking, which also included the use of bigrams. HMM, firstly introduced by Rabiner and Juain (1986) in 1986, has been successfully applied into many domains, such as named entity recognition (D. M. Bikel et al., 1997), topic classification (R. Schwartz et al., 1997), or speech recognition (J. Makhoul and R. Schwartz, 1995). In practice, the model requires solving three basic problems. Given the parameters of the model, computing the probability of a particular output sequence is the first problem. This process is often referred to as decoding. Both Forward </context>
<context position="5403" citStr="Miller et al. (1999)" startWordPosition="863" endWordPosition="866"> based on the data from TREC HARD track (J. Allan, 2005). Section 5 discusses the effectiveness of our EM training and the EM-based document weighting we proposed. Finally, we conclude our paper in Section 6 and provide some future directions at Section 7. 2 Related Works Even if we only focus on the probabilistic approach to IR, it is still impossible to discuss all up-to-date research. Instead we focus on two previous works which have inspired the work reported in this paper: the first is a general language model approach proposed by Song and Croft (1999) and the second is a HMM approach by Miller et al. (1999). 2.1 A General Language Model for IR In 1999, Song and Croft (1999) introduced a language model based on a range of data smoothing technique. The following are some of the features they used: Good-Turing estimate: Since the effect of Good-Turing estimate was verified as one of the best discount methods (C. D. Manning and H. Schutze, 1999), Song and Croft used GoodTuring estimate for allocating proper probability for the missing terms in the documents. The smoothed probability for term t in document d can be obtained with the following formula: (t� � 1)S(N����) P�T(t|d) � S(Ntf)Nd where Ntf is</context>
<context position="9664" citStr="Miller et al. (1999)" startWordPosition="1617" endWordPosition="1620">ties are same for all documents, since they establish an individual HMM for each document. Second, they completely abandon the EM algorithm for the estimation of observation probabilities. Instead, they use simple maximum likelihood estimates for each documents. So the probabilities which their HMM generate term q from their HMM states become: number of times q appears in Dk length of Dk the probabilities obtained through this formula is then used for calculating the P(D is R|Q). The document is then ranked according to the value of P(D is R|Q). The HMM model we proposed is far different from Miller et al. (1999). They build HMM for every document, and treat all words in the document as one state’s observation, and word that is unrelated to the document, but occurs commonly in natural language queries as another state’s observation. Hence, their approach requires information about the words which appears commonly in natural language. The content of the provided information will also affect the IR result, hence it is unstable. We assume that every document is an individual state, and the probabilities of query words generated by this document as the observation probabilities. Our HMM model is built on </context>
</contexts>
<marker>Miller, Leek, Schwartz, 1999</marker>
<rawString>D. R. H. Miller, T. Leek, and R. M. Schwartz, &amp;quot;A hidden Markov model information retrieval system,&amp;quot; 1999, pp. 214-221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>The TREC robust retrieval track,&amp;quot;</title>
<date>2005</date>
<pages>11--20</pages>
<contexts>
<context position="22209" citStr="Voorhees, 2005" startWordPosition="3824" endWordPosition="3825">se documents as with too high score. 4 Experiment Results 4.1 Data Set We use the AQUAINT corpus as our training data set. It is used in the TREC 2005 HARD Track (J. Allan, 2005). The AQUAINT corpus is prepared by the LDC for the AQUAINT Project, and is used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST). It contains news from three sources: the Xinhua News Service (People&apos;s Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. The topics we used are the same as the TREC Robust track (E. M. Voorhees, 2005), which are the topics from number 303 to number 689 of the TREC topics. Each topic is described in three formats including titles, descriptions and narratives. In our experiment, due to the fact that our observation sequence is very sensitive to the query terms, we only focus on the title part of the topic. In this way, we can avoid some commonly appeared words in narratives or descriptions, which may reduce the precision of our training procedure for finding the real document. Table 2 shows the detail about the corpus. Datasize 2.96GB #Documents 1,030,561 #Querys 50 Term Types 2,002,165 Term</context>
</contexts>
<marker>Voorhees, 2005</marker>
<rawString>E. M. Voorhees, &amp;quot;The TREC robust retrieval track,&amp;quot; 2005, pp. 11-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Song</author>
<author>W B Croft</author>
</authors>
<title>A general language model for information retrieval,&amp;quot;</title>
<date>1999</date>
<pages>316--321</pages>
<contexts>
<context position="2205" citStr="Song and Croft (1999)" startWordPosition="327" endWordPosition="330">on and S. Walker, 1994; S. E. Robertson, 1977) modeled word occurrences into relevant or non-relevant classes, and ranked documents according to the probabilities they belong to the relevant one. In 1998, Ponte and Croft (1998) proposed a language modeling framework which opens a new point of view in IR. In this approach, they gave up the model of relevance; instead, they treated query generation as random sampling from every document model. The retrieval results were based on the probabilities that a document can generate the query string. Several improvements were proposed after their work. Song and Croft (1999), for example, was the first to bring up a model with bi-grams and Good Turing re-estimation to smooth the document models. Latter, Miller et al. (1999) used Hidden Markov Model (HMM) for ranking, which also included the use of bigrams. HMM, firstly introduced by Rabiner and Juain (1986) in 1986, has been successfully applied into many domains, such as named entity recognition (D. M. Bikel et al., 1997), topic classification (R. Schwartz et al., 1997), or speech recognition (J. Makhoul and R. Schwartz, 1995). In practice, the model requires solving three basic problems. Given the parameters of</context>
<context position="5346" citStr="Song and Croft (1999)" startWordPosition="851" endWordPosition="854">hes proposed by Song and Corft (1999) and Robertson (1995) based on the data from TREC HARD track (J. Allan, 2005). Section 5 discusses the effectiveness of our EM training and the EM-based document weighting we proposed. Finally, we conclude our paper in Section 6 and provide some future directions at Section 7. 2 Related Works Even if we only focus on the probabilistic approach to IR, it is still impossible to discuss all up-to-date research. Instead we focus on two previous works which have inspired the work reported in this paper: the first is a general language model approach proposed by Song and Croft (1999) and the second is a HMM approach by Miller et al. (1999). 2.1 A General Language Model for IR In 1999, Song and Croft (1999) introduced a language model based on a range of data smoothing technique. The following are some of the features they used: Good-Turing estimate: Since the effect of Good-Turing estimate was verified as one of the best discount methods (C. D. Manning and H. Schutze, 1999), Song and Croft used GoodTuring estimate for allocating proper probability for the missing terms in the documents. The smoothed probability for term t in document d can be obtained with the following f</context>
<context position="11562" citStr="Song and Croft (1999)" startWordPosition="1944" endWordPosition="1947">educed document set, we then describe how to build our HMM model, and demonstrate how to obtain the special-designed observance sequence for our HMM training in Section 3.2 and 3.3, respectively. Finally, Section 3.4 introduces the evaluation mechanism to the probability of generating the query for each document. 3.1 The basic language modeling method for document reduction P(q|Dk) _ 65 Suppose we have a huge document set D, and a query Q, we firstly reduce the document set to obtain the document Dr. We require the reducing method can be efficiently computed, therefore two methods proposed by Song and Croft (1999) are consulted with some modifications: GoodTuring estimation and modeling query as a sequence of terms. In our modified Good-Turing estimation, we gathered the number of terms to calculate the term frequency (tf) information in our document set. Table 1 shows the term distribution of the AQUAINT corpus which is used in the TREC 2005 HARD Track (J. Allan, 2005). The detail of the dataset is described in Section 4.1. tf Ntf tf Ntf 0 1,140,854,966,460 5 3,327,633 1 166,056,563 6 2,163,538 2 29,905,324 7 1,491,244 3 11,191,786 8 1,089,490 4 5,668,929 9 819,517 Table 1. Term distribution in AQUAIN</context>
<context position="23120" citStr="Song and Croft (1999)" startWordPosition="3973" endWordPosition="3976"> part of the topic. In this way, we can avoid some commonly appeared words in narratives or descriptions, which may reduce the precision of our training procedure for finding the real document. Table 2 shows the detail about the corpus. Datasize 2.96GB #Documents 1,030,561 #Querys 50 Term Types 2,002,165 Term Tokens 431,823,255 Table 2. Statistics of the AQUAINT corpus 4.2 Experiment Design and Results By using the AQUAINT corpus, two different traditional IR methods are implemented for comparing. The two IR methods which we use as baselines are the General Language Modeling (GLM) proposed by Song and Croft (1999) and the tf.idf measure proposed by Robertson (1995). The GLM has been introduced in Section 2. The following formulas show the core of tf.idf: N is the number of documents in the corpus; nq is the number of documents in the corpus containing q; tf(q, D) is the number of times q appears in D; l(D) is the length of D in words and the al is the average length in words of a D in the corpus. tf. idf(Q, D) = I wtf(qi, D) · idf(qi) !RIM tf(q, D) idf(q) = N log a N + 1 wtf(q, D) = tf(q, D) + 0.5 + 1.5 1 (al ) 68 For the proposed EM IR approach, two configurations are listed to compare. The first (Con</context>
</contexts>
<marker>Song, Croft, 1999</marker>
<rawString>F. Song and W. B. Croft, &amp;quot;A general language model for information retrieval,&amp;quot; 1999, pp. 316-321.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval:</title>
<date>1986</date>
<publisher>McGraw-Hill, Inc.</publisher>
<location>New York, NY, USA,</location>
<marker>Salton, McGill, 1986</marker>
<rawString>G. Salton and M. J. McGill, Introduction to Modern Information Retrieval: McGraw-Hill, Inc. New York, NY, USA, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>HARD track overview in TREC 2005: High accuracy retrieval from documents,&amp;quot;</title>
<date>2005</date>
<contexts>
<context position="4839" citStr="Allan, 2005" startWordPosition="765" endWordPosition="766">g the query word from documents more confidently. Because they not only been calculated by language modeling view features, but also been maximized with statistical methods. Therefore the imprecise cases caused by special distribution in language modeling approach can be further prevented in this way. The remainders of this paper are organized as follows. We review two related works in Section 2. In Section 3, we introduce our EM IR approach. Section 4 compares our results to two other approaches proposed by Song and Corft (1999) and Robertson (1995) based on the data from TREC HARD track (J. Allan, 2005). Section 5 discusses the effectiveness of our EM training and the EM-based document weighting we proposed. Finally, we conclude our paper in Section 6 and provide some future directions at Section 7. 2 Related Works Even if we only focus on the probabilistic approach to IR, it is still impossible to discuss all up-to-date research. Instead we focus on two previous works which have inspired the work reported in this paper: the first is a general language model approach proposed by Song and Croft (1999) and the second is a HMM approach by Miller et al. (1999). 2.1 A General Language Model for I</context>
<context position="11925" citStr="Allan, 2005" startWordPosition="2007" endWordPosition="2008">tion P(q|Dk) _ 65 Suppose we have a huge document set D, and a query Q, we firstly reduce the document set to obtain the document Dr. We require the reducing method can be efficiently computed, therefore two methods proposed by Song and Croft (1999) are consulted with some modifications: GoodTuring estimation and modeling query as a sequence of terms. In our modified Good-Turing estimation, we gathered the number of terms to calculate the term frequency (tf) information in our document set. Table 1 shows the term distribution of the AQUAINT corpus which is used in the TREC 2005 HARD Track (J. Allan, 2005). The detail of the dataset is described in Section 4.1. tf Ntf tf Ntf 0 1,140,854,966,460 5 3,327,633 1 166,056,563 6 2,163,538 2 29,905,324 7 1,491,244 3 11,191,786 8 1,089,490 4 5,668,929 9 819,517 Table 1. Term distribution in AQUAINT corpus In this table, Ntf is the number of terms with frequency tf in a document. The tf = 0 case in the table means the number of words not appear in a document. If the number of all word in our corpus is W, and the number of word in a document d is wd, then for each document, the tf = 0 will add W – wd. By listing all frequency in our document set, we adapt</context>
<context position="21772" citStr="Allan, 2005" startWordPosition="3752" endWordPosition="3753">the transition probability from the Initial State to the document state of d, which we called “EM-based document weighting”. The P(q|d) means the observation probability for query term q in document state of d, which is also tuned in our EM training procedure. With this formula, we can rank the IR result according to this probability. This performs better than the GLM when the document size is relatively small, since GLM gives those documents as with too high score. 4 Experiment Results 4.1 Data Set We use the AQUAINT corpus as our training data set. It is used in the TREC 2005 HARD Track (J. Allan, 2005). The AQUAINT corpus is prepared by the LDC for the AQUAINT Project, and is used in official benchmark evaluations conducted by National Institute of Standards and Technology (NIST). It contains news from three sources: the Xinhua News Service (People&apos;s Republic of China), the New York Times News Service, and the Associated Press Worldstream News Service. The topics we used are the same as the TREC Robust track (E. M. Voorhees, 2005), which are the topics from number 303 to number 689 of the TREC topics. Each topic is described in three formats including titles, descriptions and narratives. In</context>
</contexts>
<marker>Allan, 2005</marker>
<rawString>J. Allan, &amp;quot;HARD track overview in TREC 2005: High accuracy retrieval from documents,&amp;quot; 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Makhoul</author>
<author>R Schwartz</author>
</authors>
<title>State of the Art in Continuous Speech Recognition,&amp;quot;</title>
<date>1995</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>92</volume>
<pages>9956--9963</pages>
<marker>Makhoul, Schwartz, 1995</marker>
<rawString>J. Makhoul and R. Schwartz, &amp;quot;State of the Art in Continuous Speech Recognition,&amp;quot; Proceedings of the National Academy of Sciences, vol. 92, pp. 9956-9963, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>W B Croft</author>
</authors>
<title>A language modeling approach to information retrieval,&amp;quot;</title>
<date>1998</date>
<pages>275--281</pages>
<contexts>
<context position="1811" citStr="Ponte and Croft (1998)" startWordPosition="263" endWordPosition="266">V. Bush, 1996), which formed the basis of research into Information Retrieval (IR). The pioneers in IR developed two models for ranking: the vector space model (G. Salton and M. J. McGill, 1986) and the probabilistic model (S. E. Robertson and S. Jones, 1976). Since then, the research of classical probabilistic models of relevance has been widely studied. For example, Robertson (S. E. Robertson and S. Walker, 1994; S. E. Robertson, 1977) modeled word occurrences into relevant or non-relevant classes, and ranked documents according to the probabilities they belong to the relevant one. In 1998, Ponte and Croft (1998) proposed a language modeling framework which opens a new point of view in IR. In this approach, they gave up the model of relevance; instead, they treated query generation as random sampling from every document model. The retrieval results were based on the probabilities that a document can generate the query string. Several improvements were proposed after their work. Song and Croft (1999), for example, was the first to bring up a model with bi-grams and Good Turing re-estimation to smooth the document models. Latter, Miller et al. (1999) used Hidden Markov Model (HMM) for ranking, which als</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>J. M. Ponte and W. B. Croft, &amp;quot;A language modeling approach to information retrieval,&amp;quot; 1998, pp. 275-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Bmjm</author>
<author>T Petrie</author>
<author>G Soules</author>
<author>N Weiss</author>
</authors>
<title>A CHAINS,&amp;quot;</title>
<date>1970</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>41</volume>
<pages>164--171</pages>
<contexts>
<context position="3317" citStr="Bmjm et al., 1970" startWordPosition="514" endWordPosition="517"> R. Schwartz, 1995). In practice, the model requires solving three basic problems. Given the parameters of the model, computing the probability of a particular output sequence is the first problem. This process is often referred to as decoding. Both Forward and Backward procedure are solutions for this problem. The second problem is finding the most possible state sequence with the parameters of the model and a particular output sequence. This is usually completed with Viterbi algorithm. The third problem is the learning problem of HMM models. It is often solved by Baum-Welch algorithm (L. E. Bmjm et al., 1970). Given training 63 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 63–71, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP data, the algorithm computes the maximum likelihood estimates and posterior mode estimate. It is in essence a generalized Expectation Maximization (EM) algorithm which was first explained and given name by Dempster, Laird and Rubin (1977) in 1977. EM can estimate the maximum likelihood of parameters in probabilistic models which has unseen variables. Nonetheless, in our knowledge, the EM procedure in HMM has never been used in IR domain. In this </context>
</contexts>
<marker>Bmjm, Petrie, Soules, Weiss, 1970</marker>
<rawString>L. E. Bmjm, T. Petrie, G. Soules, and N. Weiss, &amp;quot;A CHAINS,&amp;quot; The Annals of Mathematical Statistics, vol. 41, pp. 164-171, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
<author>B Juang</author>
</authors>
<title>An introduction to hidden Markov models,&amp;quot;</title>
<date>1986</date>
<journal>ASSP Magazine, IEEE [see also IEEE Signal Processing Magazine],</journal>
<volume>3</volume>
<pages>4--16</pages>
<marker>Rabiner, Juang, 1986</marker>
<rawString>L. Rabiner and B. Juang, &amp;quot;An introduction to hidden Markov models,&amp;quot; ASSP Magazine, IEEE [see also IEEE Signal Processing Magazine], vol. 3, pp. 4-16, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Schwartz</author>
<author>T Imai</author>
<author>F Kubala</author>
<author>L Nguyen</author>
<author>J Makhoul</author>
</authors>
<title>A Maximum Likelihood Model for Topic Classification of Broadcast News,&amp;quot;</title>
<date>1997</date>
<contexts>
<context position="2660" citStr="Schwartz et al., 1997" startWordPosition="405" endWordPosition="408">ieval results were based on the probabilities that a document can generate the query string. Several improvements were proposed after their work. Song and Croft (1999), for example, was the first to bring up a model with bi-grams and Good Turing re-estimation to smooth the document models. Latter, Miller et al. (1999) used Hidden Markov Model (HMM) for ranking, which also included the use of bigrams. HMM, firstly introduced by Rabiner and Juain (1986) in 1986, has been successfully applied into many domains, such as named entity recognition (D. M. Bikel et al., 1997), topic classification (R. Schwartz et al., 1997), or speech recognition (J. Makhoul and R. Schwartz, 1995). In practice, the model requires solving three basic problems. Given the parameters of the model, computing the probability of a particular output sequence is the first problem. This process is often referred to as decoding. Both Forward and Backward procedure are solutions for this problem. The second problem is finding the most possible state sequence with the parameters of the model and a particular output sequence. This is usually completed with Viterbi algorithm. The third problem is the learning problem of HMM models. It is often</context>
</contexts>
<marker>Schwartz, Imai, Kubala, Nguyen, Makhoul, 1997</marker>
<rawString>R. Schwartz, T. Imai, F. Kubala, L. Nguyen, and J. Makhoul, &amp;quot;A Maximum Likelihood Model for Topic Classification of Broadcast News,&amp;quot; 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
</authors>
<title>The probability ranking principle in IR,&amp;quot;</title>
<date>1977</date>
<journal>Journal of Documentation,</journal>
<volume>33</volume>
<pages>294--304</pages>
<contexts>
<context position="1630" citStr="Robertson, 1977" startWordPosition="237" endWordPosition="238"> and tf.idfbased methods on TREC 2005 HARD Track data. 1 Introduction In 1945, soon after the computer was invented, Vannevar Bush wrote a famous article---“As we may think” (V. Bush, 1996), which formed the basis of research into Information Retrieval (IR). The pioneers in IR developed two models for ranking: the vector space model (G. Salton and M. J. McGill, 1986) and the probabilistic model (S. E. Robertson and S. Jones, 1976). Since then, the research of classical probabilistic models of relevance has been widely studied. For example, Robertson (S. E. Robertson and S. Walker, 1994; S. E. Robertson, 1977) modeled word occurrences into relevant or non-relevant classes, and ranked documents according to the probabilities they belong to the relevant one. In 1998, Ponte and Croft (1998) proposed a language modeling framework which opens a new point of view in IR. In this approach, they gave up the model of relevance; instead, they treated query generation as random sampling from every document model. The retrieval results were based on the probabilities that a document can generate the query string. Several improvements were proposed after their work. Song and Croft (1999), for example, was the fi</context>
</contexts>
<marker>Robertson, 1977</marker>
<rawString>S. E. Robertson, &amp;quot;The probability ranking principle in IR,&amp;quot; Journal of Documentation, vol. 33, pp. 294-304, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Jones</author>
</authors>
<title>Relevance Weighting of Search Terms,&amp;quot;</title>
<date>1976</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>27</volume>
<pages>129--46</pages>
<marker>Robertson, Jones, 1976</marker>
<rawString>S. E. Robertson and S. Jones, &amp;quot;Relevance Weighting of Search Terms,&amp;quot; Journal of the American Society for Information Science, vol. 27, pp. 129-46, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
</authors>
<title>Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval,&amp;quot;</title>
<date>1994</date>
<pages>232--241</pages>
<marker>Robertson, Walker, 1994</marker>
<rawString>S. E. Robertson and S. Walker, &amp;quot;Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval,&amp;quot; 1994, pp. 232-241.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M Hancock-Beaulieu</author>
<author>M</author>
<author>M Gatford</author>
</authors>
<title>Okapi at TREC-3,&amp;quot;</title>
<pages>109--126</pages>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, M, Gatford, </marker>
<rawString>S. E. Robertson, S. Walker, and S. Jones, &amp;quot;M. Hancock-Beaulieu, M., and Gatford, M.(1995). Okapi at TREC-3,&amp;quot; pp. 109–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Bush</author>
</authors>
<title>As we may think,&amp;quot;</title>
<date>1996</date>
<journal>interactions,</journal>
<volume>3</volume>
<pages>35--46</pages>
<contexts>
<context position="1203" citStr="Bush, 1996" startWordPosition="168" endWordPosition="169">ocuments retrieved using this approach are ranked according their probability of generating the given query. In this paper, we present a novel approach that employs the generalized Expectation Maximization (EM) algorithm to improve language models by representing their parameters as observation probabilities of Hidden Markov Models (HMM). In the experiments, we demonstrate that our method outperforms standard SLM-based and tf.idfbased methods on TREC 2005 HARD Track data. 1 Introduction In 1945, soon after the computer was invented, Vannevar Bush wrote a famous article---“As we may think” (V. Bush, 1996), which formed the basis of research into Information Retrieval (IR). The pioneers in IR developed two models for ranking: the vector space model (G. Salton and M. J. McGill, 1986) and the probabilistic model (S. E. Robertson and S. Jones, 1976). Since then, the research of classical probabilistic models of relevance has been widely studied. For example, Robertson (S. E. Robertson and S. Walker, 1994; S. E. Robertson, 1977) modeled word occurrences into relevant or non-relevant classes, and ranked documents according to the probabilities they belong to the relevant one. In 1998, Ponte and Crof</context>
</contexts>
<marker>Bush, 1996</marker>
<rawString>V. Bush, &amp;quot;As we may think,&amp;quot; interactions, vol. 3, pp. 35-46, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>W B Croft</author>
</authors>
<title>Relevance based language models,&amp;quot;</title>
<date>2001</date>
<pages>120--127</pages>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>V. Lavrenko and W. B. Croft, &amp;quot;Relevance based language models,&amp;quot; 2001, pp. 120-127.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>