<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000144">
<title confidence="0.991708">
Descending-Path Convolution Kernel for Syntactic Structures
</title>
<author confidence="0.7640635">
Chen Lin1, Timothy Miller1, Alvin Kho1, Steven Bethard2,
Dmitriy Dligach1, Sameer Pradhan1 and Guergana Savova1,
</author>
<affiliation confidence="0.484394">
1 Children’s Hospital Boston Informatics Program and Harvard Medical School
</affiliation>
<email confidence="0.961421">
{firstname.lastname}@childrens.harvard.edu
</email>
<affiliation confidence="0.869136">
2 Department of Computer and Information Sciences, University of Alabama at Birmingham
</affiliation>
<email confidence="0.99397">
bethard@cis.uab.edu
</email>
<sectionHeader confidence="0.982776" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998075">
Convolution tree kernels are an efficient
and effective method for comparing syntac-
tic structures in NLP methods. However,
current kernel methods such as subset tree
kernel and partial tree kernel understate the
similarity of very similar tree structures.
Although soft-matching approaches can im-
prove the similarity scores, they are corpus-
dependent and match relaxations may be
task-specific. We propose an alternative ap-
proach called descending path kernel which
gives intuitive similarity scores on compa-
rable structures. This method is evaluated
on two temporal relation extraction tasks
and demonstrates its advantage over rich
syntactic representations.
</bodyText>
<sectionHeader confidence="0.995172" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996665">
Syntactic structure can provide useful features for
many natural language processing (NLP) tasks
such as semantic role labeling, coreference resolu-
tion, temporal relation discovery, and others. How-
ever, the choice of features to be extracted from a
tree for a given task is not always clear. Convolu-
tion kernels over syntactic trees (tree kernels) offer
a potential solution to this problem by providing
relatively efficient algorithms for computing sim-
ilarities between entire discrete structures. These
kernels use tree fragments as features and count
the number of common fragments as a measure of
similarity between any two trees.
However, conventional tree kernels are sensitive
to pattern variations. For example, two trees in Fig-
ure 1(a) sharing the same structure except for one
terminal symbol are deemed at most 67% similar
by the conventional tree kernel (PTK) (Moschitti,
2006). Yet one might expect a higher similarity
given their structural correspondence.
The similarity is further attenuated by trivial
structure changes such as the insertion of an ad-
jective in one of the trees in Figure 1(a), which
would reduce the similarity close to zero. Such
an abrupt attenuation would potentially propel a
model to memorize training instances rather than
generalize from trends, leading towards overfitting.
In this paper, we describe a new kernel over
syntactic trees that operates on descending paths
through the tree rather than production rules as
used in most existing methods. This representation
is reminiscent of Sampson’s (2000) leaf-ancestor
paths for scoring parse similarities, but here it is
generalized over all ancestor paths, not just those
from the root to a leaf. This approach assigns more
robust similarity scores (e.g., 78% similarity in the
above example) than other soft matching tree ker-
nels, is faster than the partial tree kernel (Moschitti,
2006), and is less ad hoc than the grammar-based
convolution kernel (Zhang et al., 2007).
</bodyText>
<sectionHeader confidence="0.988663" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.99844">
2.1 Syntax-based Tree Kernels
</subsectionHeader>
<bodyText confidence="0.999956">
Syntax-based tree kernels quantify the similarity
between two constituent parses by counting their
common sub-structures. They differ in their defini-
tion of the sub-structures.
Collins and Duffy (2001) use a subset tree (SST)
representation for their sub-structures. In the SST
representation, a subtree is defined as a subgraph
with more than one node, in which only full pro-
duction rules are expanded. While this approach is
widely used and has been successful in many tasks,
the production rule-matching constraint may be un-
necessarily restrictive, giving zero credit to rules
that have only minor structural differences. For
example, the similarity score between the NPs in
Figure 1(b) would be zero since the production rule
is different (the overall similarity score is above-
zero because of matching pre-terminals).
The partial tree kernel (PTK) relaxes the defi-
nition of subtrees to allow partial production rule
</bodyText>
<page confidence="0.571251">
81
</page>
<note confidence="0.6170755">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81–86,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.988922909090909">
NP
l=0: [NP],[DT],[NN]
l=1: [NP-DT],[NP-NN],
[DT-a],[NN-cat]
l=2: [NP-DT-a],[NP-NN-cat]
a)
NN
DT
a
cat
NP
NP
NP
DT
NN
DT
NN
a
dog
a
cat
NP
</figure>
<bodyText confidence="0.910411866666667">
matching (Moschitti, 2006). In the PTK, a subtree
may or may not expand any child in a production
rule, while maintaining the ordering of the child
nodes. Thus it generates a very large but sparse
feature space. To Figure 1(b), the PTK generates
fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a]
[NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among
others, for the second tree. This allows for partial
matching – substructure (ii) – while also generating
some fragments that violate grammatical intuitions.
Zhang et al. (2007) address the restrictiveness
of SST by allowing soft matching of production
rules. They allow partial matching of optional
nodes based on the Treebank. For example, the
rule NP —* DT JJ NN indicates a noun phrase
consisting of a determiner, adjective, and common
noun. Zhang et al.’s method designates the JJ as
optional, since the Treebank contains instances of
a reduced version of the rule without the JJ node
(NP —* DT NN). They also allow node match-
ing among similar preterminals such as JJ, JJR, and
JJS, mapping them to one equivalence class.
Other relevant approaches are the spectrum tree
(SpT) (Kuboyama et al., 2007) and the route kernel
(RtT) (Aiolli et al., 2009). SpT uses a q-gram
– a sequence of connected vertices of length q –
as their sub-structure. It observes grammar rules
by recording the orientation of edges: a+—b—*c is
different from a—*b—*c. RtT uses a set of routes as
basic structures, which observes grammar rules by
</bodyText>
<figureCaption confidence="0.895299">
Figure 2: A parse tree (left) and its descending
paths according to Definition 1 (l - length).
</figureCaption>
<bodyText confidence="0.938867">
recording the index of a neighbor node.
</bodyText>
<subsectionHeader confidence="0.995643">
2.2 Temporal Relation Extraction
</subsectionHeader>
<bodyText confidence="0.99999784375">
Among NLP tasks that use syntactic informa-
tion, temporal relation extraction has been draw-
ing growing attention because of its wide applica-
tions in multiple domains. As subtasks in Tem-
pEval 2007, 2010 and 2013, multiple systems
were built to create labeled links from events
to events/timestamps by using a variety of fea-
tures (Bethard and Martin, 2007; Llorens et al.,
2010; Chambers, 2013). Many methods exist for
synthesizing syntactic information for temporal
relation extraction, and most use traditional tree
kernels with various feature representations. Mir-
roshandel et al. (2009) used the path-enclosed tree
(PET) representation to represent syntactic informa-
tion for temporal relation extraction on the Time-
Bank (Pustejovsky et al., 2003) and the AQUAINT
TimeML corpus1. The PET is the smallest subtree
that contains both proposed arguments of a relation.
Hovy et al. (2012) used bag tree structures to rep-
resent the bag of words (BOW) and bag of part of
speech tags (BOP) between the event and time in
addition to a set of baseline features, and improved
the temporal linking performance on the TempEval
2007 and Machine Reading corpora (Strassel et
al., 2010). Miller at al. (2013) used PET tree, bag
tree, and path tree (PT, which is similar to a PET
tree with the internal nodes removed) to represent
syntactic information and improved the temporal
relation discovery performance on THYME data2
(Styler et al., 2014). In this paper, we also use
syntactic structure-enriched temporal relation dis-
covery as a vehicle to test our proposed kernel.
</bodyText>
<sectionHeader confidence="0.996874" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999554">
Here we decribe the Descending Path Kernel
(DPK).
</bodyText>
<footnote confidence="0.9425885">
1http://www.timeml.org
2http://thyme.healthnlp.org
</footnote>
<figure confidence="0.999901869565217">
a cat
cat
a
fat
S
NP
VP
PRP
VBZ
she
ADVP
RB
here
comes
S
NP VP
VBZ
comes
PRP
she
ADVP
RB
here
</figure>
<figureCaption confidence="0.994227">
Figure 1: Three example tree pairs.
</figureCaption>
<figure confidence="0.9449585">
JJ
DT
NN
DT
NN
82
</figure>
<bodyText confidence="0.98731753125">
Definition 1 (Descending Path): Let T be a
parse tree, v any non-terminal node in T, dv a
descendant of v, including terminals. A descending
path is the sequence of indexes of edges connecting
v and dv, denoted by [v − · · · − dv]. The length l
of a descending path is the number of connecting
edges. When l = 0, a descending path is the non-
terminal node itself, [v]. Figure 2 illustrates a parse
tree and its descending paths of different lengths.
Suppose that all descending paths of a tree T are
indexed 1, · · · , n, and pathi(T) is the frequency
of the i-th descending path in T. We represent T as
a vector of frequencies of all its descending paths:
Φ(T) = (path1(T), · · · , pathn(T)).
The similarity between any two trees T1 and T2
can be assessed via the dot product of their respec-
tive descending path frequency vector representa-
tions: K(T1,T2) = (Φ(T1),Φ(T2)).
Compared with the previous tree kernels, our
descending path kernel has the following advan-
tages: 1) the sub-structures are simplified so that
they are more likely to be shared among trees,
and therefore the sparse feature issues of previous
kernels could be alleviated by this representation;
2) soft matching between two similar structures
(e.g., NP—*DT JJ NN versus NP—*DT NN) have
high similarity without reference to any corpus or
grammar rules;
Following Collins and Duffy (2001), we derive
a recursive algorithm to compute the dot product
of the descending path frequency vector represen-
tations of two trees T1 and T2:
</bodyText>
<equation confidence="0.994859">
K(T1, T2) = (Φ(T1), Φ(T2))
�= pathi(T1) · pathi(T2)
i
�= � � jpathi(n1) · jpathi(n2)
n1EN1 n2EN2 i
�= C(n1, n2)
n1EN1
n2EN2
(1)
</equation>
<bodyText confidence="0.999959">
where N1 and N2 are the sets of nodes in T1 and
T2 respectively, i indexes the set of possible paths,
jpathi(n) is an indicator function that is 1 iff the
descending pathi is rooted at node n or 0 other-
wise. C(n1, n2) counts the number of common
descending paths rooted at nodes n1 and n2:
</bodyText>
<equation confidence="0.827611">
�C(n1, n2) = jpathi(n1) · jpathi(n2)
i
C(n1, n2) can be computed in polynomial time by
</equation>
<table confidence="0.7470668">
the following recursive rules:
Rule 1: If n1 and n2 have different labels (e.g.,
”DT” versus “NN”), then C(n1, n2) = 0;
Rule 2: Else if n1 and n2 have the same labels
and are both pre-terminals (POS tags), then
</table>
<equation confidence="0.89623225">
�
1 if term(n1) = term(n2)
C(n1, n2) = 1 +
0 otherwise.
</equation>
<bodyText confidence="0.661586666666667">
where term(n) is the terminal symbol under n;
Rule 3: Else if n1 and n2 have the same labels
and they are not both pre-terminals, then:
</bodyText>
<equation confidence="0.997637666666667">
C(n1, n2) = 1 + � C(ni, nj)
niEchildren(n1)
njEchildren(n2)
</equation>
<bodyText confidence="0.998803166666666">
where children(m) are the child nodes of m.
As in other tree kernel approaches (Collins and
Duffy, 2001; Moschitti, 2006), we use a discount
parameter A to control for the disproportionately
large similarity values of large tree structures.
Therefore, Rule 2 becomes:
</bodyText>
<equation confidence="0.790491">
C(n1, n2) = 1 + �
A if term(n1) = term(n2)
0 otherwise.
</equation>
<bodyText confidence="0.763008">
and Rule 3 becomes:
</bodyText>
<equation confidence="0.96654">
C(n1, n2) = 1 + A � C(ni, nj)
niEchildren(n1)
njEchildren(n2)
</equation>
<bodyText confidence="0.999839181818182">
Note that Eq. (1) is a convolution kernel under
the kernel closure properties described in Haus-
sler (1999). Rules 1-3 show the equivalence be-
tween the number of common descending paths
rooted at nodes n1 and n2, and the number of
matching nodes below n1 and n2.
In practice, there are many non-matching nodes,
and most matching nodes will have only a few
matching children, so the running time, as in SST,
will be approximated by the number of matching
nodes between trees.
</bodyText>
<subsectionHeader confidence="0.999179">
3.1 Relationship with other kernels
</subsectionHeader>
<bodyText confidence="0.999965833333333">
For a given tree, DPK will generate significantly
fewer sub-structures than PTK, since it does not
consider all ordered permutations of a production
rule. Moreover, the fragments generated by DPK
are more likely to be shared among different trees.
For the number of corpus-wide fragments, it is
</bodyText>
<page confidence="0.862157">
83
</page>
<table confidence="0.9997448">
Kernel ID #Frag Sim N(Sim)
SST a 9 3 0.50
O(p|N1||N2|) b 15 2 0.25
c 63 7 0.20
DPK a 11 7 0.78
O(p2|N1||N2|) b 13 9 0.83
c 31 22 0.83
PTK a 20 10 0.67
O(p3|N1||N2|) b 36 15 0.65
c 127 34 0.42
</table>
<tableCaption confidence="0.999538">
Table 1: Comparison of the worst case computa-
</tableCaption>
<bodyText confidence="0.997582279411765">
tional complexicity (p - the maximum branching
factor) and kernel performance on the 3 examples
from Figure 1. #Frag is the number of fragments,
N(Sim) is the normalized similarity. Please see
the online supplementary note for detailed frag-
ments of example (a).
possible that DPK G SST G PTK. In Table 1, given
A = 1, we compare the performance of 3 kernels
on the three examples in Figure 1. Note that for
more complicated structures, i.e., examples b and
c, DPK generates fewer fragments than SST and
PTK, with more shared fragments among trees.
The complexity for all three kernels are at least
O(|N1||N2|) since they share the pairwise summa-
tion at the end of Equation 1. SST, due to its re-
quirement of exact production rule matching, only
takes one pass in the inner loop which adds a factor
of p (the maximum branching factor of any pro-
duction rule). DPK does a pairwise summation
of children, which adds a factor of p2 to the com-
plexity. Finally, the efficient algorithm for PTK
is proved by Moschitti (2006) to contain a con-
stant factor of p3. Table 1 orders the tree kernels
according by their listed complexity.
It may seem that the value of DPK is strictly in its
ability to evaluate all paths, which is not explicitly
accounted for by other kernels. However, another
view of the DPK is possible by thinking of it as
cheaply calculating rule production similarity by
taking advantage of relatively strict English word
ordering. Like SST and PTK, the DPK requires
the root category of two subtrees to be the same
for the similarity to be greater than zero. Unlike
SST and PTK, once the root category comparison
is successfully completed, DPK looks at all paths
that go through it and accumulates their similarity
scores independent of ordering – in other words, it
will ignore the ordering of the children in its pro-
duction rule. This means, for example, that if the
rule production NP —* NN JJ DT were ever found
in a tree, to DPK it would be indistinguishable from
the common production NP —* DT JJ NN, despite
having inverted word order, and thus would have
a maximal similarity score. SST and PTK would
assign this pair a much lower score for having com-
pletely different ordering, but we suggest that cases
such as these are very rare due to the relatively
strict word ordering of English. In most cases, the
determiner of a noun phrase will be at the front, the
nouns will be at the end, and the adjectives in the
middle. So with small differences in production
rules (one or two adjectives, extra nominal modifier,
etc.) the PTK will capture similarity by compar-
ing every possible partial rule completion, but the
DPK can obtain higher and faster scores by just
comparing one child at a time because the ordering
is constrained by the language. This analysis does
lead to a hypothesis for the general viability of the
DPK, suggesting that in languages with freer word
order it may give inflated scores to structures that
are syntactically dissimilar if they have the same
constituent components in different order.
Formally, Moschitti (2006) showed that SST is
a special case of PTK when only the longest child
sequence from each tree is considered. On the other
end of the spectrum, DPK is a special case of PTK
where the similarity between rules only considers
child subsequences of length one.
</bodyText>
<sectionHeader confidence="0.994608" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99991225">
We applied DPK to two published temporal relation
extraction systems: (Miller et al., 2013) in the
clinical domain and Cleartk-TimeML (Bethard,
2013) in the general domain respectively.
</bodyText>
<subsectionHeader confidence="0.99388">
4.1 Narrative Container Discovery
</subsectionHeader>
<bodyText confidence="0.998509142857143">
The task here as described by Miller et al. (2013) is
to identify the CONTAINS relation between a time
expression and a same-sentence event from clinical
notes in the THYME corpus, which has 78 notes
of 26 patients. We obtained this corpus from the
authors and followed their linear composite kernel
setting:
</bodyText>
<equation confidence="0.997542333333333">
P
KC(s1, s2) = τ KT(tp1, tp2)+KF(f1, f2) (2)
p=1
</equation>
<bodyText confidence="0.99975">
where si is an instance object composed of flat fea-
tures fi and a syntactic tree ti. A syntactic tree ti
</bodyText>
<page confidence="0.792331">
84
</page>
<bodyText confidence="0.999994066666667">
can have multiple representations, as in Bag Tree
(BT), Path-enclosed Tree (PET), and Path Tree
(PT). For the tree kernel KT, subset tree (SST) ker-
nel was applied on each tree representation p. The
final similarity score between two instances is the
τ-weighted sum of the similarities of all representa-
tions, combined with the flat feature (FF) similarity
as measured by a feature kernel KF (linear or poly-
nomial). Here we replaced the SST kernel with
DPK and tested two feature combinations FF+PET
and FF+BT+PET+PT. To fine tune parameters, we
used grid search by testing on the default develop-
ment data. Once the parameters were tuned, we
tested the system performance on the testing data,
which was set up by the original system split.
</bodyText>
<subsectionHeader confidence="0.997745">
4.2 Cleartk-TimeML
</subsectionHeader>
<bodyText confidence="0.999992733333333">
We tested one sub-task from TempEval-2013 –
the extraction of temporal relations between an
event and time expression within the same sen-
tence. We obtained the training corpus (Time-
Bank + AQUAINT) and testing data from the au-
thors (Bethard, 2013). Since the original features
didn’t contain syntactic features, we created a PET
tree extractor for this system. The kernel setting
was similar to equation (2), while there was only
one tree representation, PET tree, P=1. A linear
kernel was used as KF to evaluate the exact same
flat features as used by the original system. We
used the built-in cross validation to do grid search
for tuning the parameters. The final system was
tested on the testing data for reporting results.
</bodyText>
<subsectionHeader confidence="0.991094">
4.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999786611111111">
Results are shown in Table 2. The top section
shows THYME results. For these experiments,
the DPK is superior when a syntactically-rich PET
representation is used. Using the full feature set of
Miller et al. (2013), SST is superior to DPK and
obtains the best overall performance. The bottom
section shows results on TempEval-2013 data, for
which there is little benefit from either tree kernel.
Our experiments with THYME data show that
DPK can capture something in the linguistically
richer PET representation that the SST kernel can-
not, but adding BT and PT representations decrease
the DPK performance. As a shallow representation,
BT does not have much in the way of descending
paths for DPK to use. PT already ignores the pro-
duction grammar by removing the inner tree nodes.
DPK therefore cannot get useful information and
may even get misleading cues from these two rep-
</bodyText>
<table confidence="0.999308">
Features KT P R F
THYME
FF+PET DPK 0.756 0.667 0.708
SST 0.698 0.630 0.662
FF+BT+ DPK 0.759 0.626 0.686
PET+PT SST 0.754 0.711 0.732
TempEval
FF+PET DPK 0.328 0.263 0.292
SST 0.325 0.263 0.290
FF - 0.309 0.266 0.286
</table>
<tableCaption confidence="0.711478333333333">
Table 2: Comparison of tree kernel performance
for temporal relation extraction on THYME and
TempEval-2013 data.
</tableCaption>
<bodyText confidence="0.999731166666667">
resentations. These results show that, while DPK
should not always replace SST, there are represen-
tations in which it is superior to existing methods.
This suggests an approach in which tree representa-
tions are matched to different convolution kernels,
for example by tuning on held-out data.
For TempEval-2013 data, adding syntactic fea-
tures did not improve the performance significantly
(comparing F-score of 0.290 with 0.286 in Ta-
ble 3). Probably, syntactic information is not a
strong feature for all types of temporal relations on
TempEval-2013 data.
</bodyText>
<sectionHeader confidence="0.997695" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999972615384615">
In this paper, we developed a novel convolution
tree kernel (DPK) for measuring syntactic similar-
ity. This kernel uses a descending path represen-
tation in trees to allow higher similarity scores on
partially matching structures, while being simpler
and faster than other methods for doing the same.
Future work will explore 1) a composite kernel
which uses DPK for PET trees, SST for BT and PT,
and feature kernel for flat features, so that different
tree kernels can work with their ideal syntactic rep-
resentations; 2) incorporate dependency structures
for tree kernel analysis 3) applying DPK to other
relation extraction tasks on various corpora.
</bodyText>
<sectionHeader confidence="0.998395" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.96661925">
Thanks to Sean Finan for technically supporting the
experiments. The project described was supported
by R01LM010090 (THYME) from the National
Library Of Medicine.
</bodyText>
<page confidence="0.962975">
85
</page>
<sectionHeader confidence="0.899577" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991324906976745">
Fabio Aiolli, Giovanni Da San Martino, and Alessan-
dro Sperduti. 2009. Route kernels for trees. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 17–24. ACM.
Steven Bethard and James H Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129–132. Association for Computational Linguis-
tics.
Steven Bethard. 2013. Cleartk-timeml: A minimalist
approach to TempEval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 10–14.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73–77, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Neural Information
Processing Systems.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia in Santa Cruz.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Pat-
wardhan, and Chris Welty. 2012. When did that
happen?: linking events and relations to timestamps.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 185–193. Association for Compu-
tational Linguistics.
Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,
Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.
2007. A spectrum tree kernel. Information and Me-
dia Technologies, 2(1):292–299.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284–291. Association for
Computational Linguistics.
Timothy Miller, Steven Bethard, Dmitriy Dligach,
Sameer Pradhan, Chen Lin, and Guergana Savova.
2013. Discovering temporal narrative containers
in clinical text. In Proceedings of the 2013 Work-
shop on Biomedical Natural Language Processing,
pages 18–26, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for
classifying temporal relations between events. Proc.
of the PACLIC23, pages 355–364.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318–329.
Springer.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al. 2003. The TimeBank corpus. In Cor-
pus linguistics, volume 2003, page 40.
Geoffrey Sampson. 2000. A proposal for improving
the measurement of parse accuracy. International
Journal of Corpus Linguistics, 5(1):53–68.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA machine reading
program-encouraging linguistic and reasoning re-
search with a series of reading tasks. In LREC.
William Styler, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Er-
ickson, Timothy Miller, Lin Chen, Guergana K.
Savova, and James Pustejovsky. 2014. Temporal
annotations in the clinical domain. Transactions
of the Association for Computational Linguistics,
2(2):143−154.
Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for seman-
tic role classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 200–207.
</reference>
<page confidence="0.896458">
86
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.452487">
<title confidence="0.999966">Descending-Path Convolution Kernel for Syntactic Structures</title>
<author confidence="0.999984">Timothy Alvin Steven</author>
<affiliation confidence="0.636828666666667">and 1Children’s Hospital Boston Informatics Program and Harvard Medical 2Department of Computer and Information Sciences, University of Alabama at</affiliation>
<email confidence="0.978558">bethard@cis.uab.edu</email>
<abstract confidence="0.999226235294118">Convolution tree kernels are an efficient and effective method for comparing syntactic structures in NLP methods. However, current kernel methods such as subset tree kernel and partial tree kernel understate the similarity of very similar tree structures. Although soft-matching approaches can improve the similarity scores, they are corpusdependent and match relaxations may be task-specific. We propose an alternative approach called descending path kernel which gives intuitive similarity scores on comparable structures. This method is evaluated on two temporal relation extraction tasks and demonstrates its advantage over rich syntactic representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fabio Aiolli</author>
<author>Giovanni Da San Martino</author>
<author>Alessandro Sperduti</author>
</authors>
<title>Route kernels for trees.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>17--24</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5565" citStr="Aiolli et al., 2009" startWordPosition="851" endWordPosition="854">ching of production rules. They allow partial matching of optional nodes based on the Treebank. For example, the rule NP —* DT JJ NN indicates a noun phrase consisting of a determiner, adjective, and common noun. Zhang et al.’s method designates the JJ as optional, since the Treebank contains instances of a reduced version of the rule without the JJ node (NP —* DT NN). They also allow node matching among similar preterminals such as JJ, JJR, and JJS, mapping them to one equivalence class. Other relevant approaches are the spectrum tree (SpT) (Kuboyama et al., 2007) and the route kernel (RtT) (Aiolli et al., 2009). SpT uses a q-gram – a sequence of connected vertices of length q – as their sub-structure. It observes grammar rules by recording the orientation of edges: a+—b—*c is different from a—*b—*c. RtT uses a set of routes as basic structures, which observes grammar rules by Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). recording the index of a neighbor node. 2.2 Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple doma</context>
</contexts>
<marker>Aiolli, Martino, Sperduti, 2009</marker>
<rawString>Fabio Aiolli, Giovanni Da San Martino, and Alessandro Sperduti. 2009. Route kernels for trees. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 17–24. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>Cu-tmp: temporal relation classification using syntactic and semantic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>129--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6356" citStr="Bethard and Martin, 2007" startWordPosition="983" endWordPosition="986">s different from a—*b—*c. RtT uses a set of routes as basic structures, which observes grammar rules by Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). recording the index of a neighbor node. 2.2 Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1. The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag</context>
</contexts>
<marker>Bethard, Martin, 2007</marker>
<rawString>Steven Bethard and James H Martin. 2007. Cu-tmp: temporal relation classification using syntactic and semantic features. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 129–132. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
</authors>
<title>Cleartk-timeml: A minimalist approach to TempEval</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (* SEM),</booktitle>
<volume>2</volume>
<pages>10--14</pages>
<contexts>
<context position="15212" citStr="Bethard, 2013" startWordPosition="2543" endWordPosition="2544">guages with freer word order it may give inflated scores to structures that are syntactically dissimilar if they have the same constituent components in different order. Formally, Moschitti (2006) showed that SST is a special case of PTK when only the longest child sequence from each tree is considered. On the other end of the spectrum, DPK is a special case of PTK where the similarity between rules only considers child subsequences of length one. 4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the CONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: P KC(s1, s2) = τ KT(tp1, tp2)+KF(f1, f2) (2) p=1 where si is an instance object composed of flat features fi and a syntactic tree ti. A syntactic tree ti 84 can have multiple representations, as in Bag Tree (BT), Path-en</context>
<context position="16753" citStr="Bethard, 2013" startWordPosition="2803" endWordPosition="2804"> (linear or polynomial). Here we replaced the SST kernel with DPK and tested two feature combinations FF+PET and FF+BT+PET+PT. To fine tune parameters, we used grid search by testing on the default development data. Once the parameters were tuned, we tested the system performance on the testing data, which was set up by the original system split. 4.2 Cleartk-TimeML We tested one sub-task from TempEval-2013 – the extraction of temporal relations between an event and time expression within the same sentence. We obtained the training corpus (TimeBank + AQUAINT) and testing data from the authors (Bethard, 2013). Since the original features didn’t contain syntactic features, we created a PET tree extractor for this system. The kernel setting was similar to equation (2), while there was only one tree representation, PET tree, P=1. A linear kernel was used as KF to evaluate the exact same flat features as used by the original system. We used the built-in cross validation to do grid search for tuning the parameters. The final system was tested on the testing data for reporting results. 4.3 Results and Discussion Results are shown in Table 2. The top section shows THYME results. For these experiments, th</context>
</contexts>
<marker>Bethard, 2013</marker>
<rawString>Steven Bethard. 2013. Cleartk-timeml: A minimalist approach to TempEval 2013. In Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 2, pages 10–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Chambers</author>
</authors>
<title>Navytime: Event and time ordering from raw text.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>73--77</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="6395" citStr="Chambers, 2013" startWordPosition="991" endWordPosition="992">es as basic structures, which observes grammar rules by Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). recording the index of a neighbor node. 2.2 Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1. The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between t</context>
</contexts>
<marker>Chambers, 2013</marker>
<rawString>Nate Chambers. 2013. Navytime: Event and time ordering from raw text. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 73–77, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="3293" citStr="Collins and Duffy (2001)" startWordPosition="478" endWordPosition="481">milarities, but here it is generalized over all ancestor paths, not just those from the root to a leaf. This approach assigns more robust similarity scores (e.g., 78% similarity in the above example) than other soft matching tree kernels, is faster than the partial tree kernel (Moschitti, 2006), and is less ad hoc than the grammar-based convolution kernel (Zhang et al., 2007). 2 Background 2.1 Syntax-based Tree Kernels Syntax-based tree kernels quantify the similarity between two constituent parses by counting their common sub-structures. They differ in their definition of the sub-structures. Collins and Duffy (2001) use a subset tree (SST) representation for their sub-structures. In the SST representation, a subtree is defined as a subgraph with more than one node, in which only full production rules are expanded. While this approach is widely used and has been successful in many tasks, the production rule-matching constraint may be unnecessarily restrictive, giving zero credit to rules that have only minor structural differences. For example, the similarity score between the NPs in Figure 1(b) would be zero since the production rule is different (the overall similarity score is abovezero because of matc</context>
<context position="9177" citStr="Collins and Duffy (2001)" startWordPosition="1465" endWordPosition="1468">and T2 can be assessed via the dot product of their respective descending path frequency vector representations: K(T1,T2) = (Φ(T1),Φ(T2)). Compared with the previous tree kernels, our descending path kernel has the following advantages: 1) the sub-structures are simplified so that they are more likely to be shared among trees, and therefore the sparse feature issues of previous kernels could be alleviated by this representation; 2) soft matching between two similar structures (e.g., NP—*DT JJ NN versus NP—*DT NN) have high similarity without reference to any corpus or grammar rules; Following Collins and Duffy (2001), we derive a recursive algorithm to compute the dot product of the descending path frequency vector representations of two trees T1 and T2: K(T1, T2) = (Φ(T1), Φ(T2)) �= pathi(T1) · pathi(T2) i �= � � jpathi(n1) · jpathi(n2) n1EN1 n2EN2 i �= C(n1, n2) n1EN1 n2EN2 (1) where N1 and N2 are the sets of nodes in T1 and T2 respectively, i indexes the set of possible paths, jpathi(n) is an indicator function that is 1 iff the descending pathi is rooted at node n or 0 otherwise. C(n1, n2) counts the number of common descending paths rooted at nodes n1 and n2: �C(n1, n2) = jpathi(n1) · jpathi(n2) i C(</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>University of California in Santa Cruz.</institution>
<contexts>
<context position="10798" citStr="Haussler (1999)" startWordPosition="1765" endWordPosition="1767">and they are not both pre-terminals, then: C(n1, n2) = 1 + � C(ni, nj) niEchildren(n1) njEchildren(n2) where children(m) are the child nodes of m. As in other tree kernel approaches (Collins and Duffy, 2001; Moschitti, 2006), we use a discount parameter A to control for the disproportionately large similarity values of large tree structures. Therefore, Rule 2 becomes: C(n1, n2) = 1 + � A if term(n1) = term(n2) 0 otherwise. and Rule 3 becomes: C(n1, n2) = 1 + A � C(ni, nj) niEchildren(n1) njEchildren(n2) Note that Eq. (1) is a convolution kernel under the kernel closure properties described in Haussler (1999). Rules 1-3 show the equivalence between the number of common descending paths rooted at nodes n1 and n2, and the number of matching nodes below n1 and n2. In practice, there are many non-matching nodes, and most matching nodes will have only a few matching children, so the running time, as in SST, will be approximated by the number of matching nodes between trees. 3.1 Relationship with other kernels For a given tree, DPK will generate significantly fewer sub-structures than PTK, since it does not consider all ordered permutations of a production rule. Moreover, the fragments generated by DPK </context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report, University of California in Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>James Fan</author>
<author>Alfio Gliozzo</author>
<author>Siddharth Patwardhan</author>
<author>Chris Welty</author>
</authors>
<title>When did that happen?: linking events and relations to timestamps.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>185--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6887" citStr="Hovy et al. (2012)" startWordPosition="1061" endWordPosition="1064">om events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1. The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enric</context>
</contexts>
<marker>Hovy, Fan, Gliozzo, Patwardhan, Welty, 2012</marker>
<rawString>Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patwardhan, and Chris Welty. 2012. When did that happen?: linking events and relations to timestamps. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 185–193. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Kuboyama</author>
<author>Kouichi Hirata</author>
<author>Hisashi Kashima</author>
<author>Kiyoko F Aoki-Kinoshita</author>
<author>Hiroshi Yasuda</author>
</authors>
<title>A spectrum tree kernel.</title>
<date>2007</date>
<journal>Information and Media Technologies,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5516" citStr="Kuboyama et al., 2007" startWordPosition="842" endWordPosition="845">ess the restrictiveness of SST by allowing soft matching of production rules. They allow partial matching of optional nodes based on the Treebank. For example, the rule NP —* DT JJ NN indicates a noun phrase consisting of a determiner, adjective, and common noun. Zhang et al.’s method designates the JJ as optional, since the Treebank contains instances of a reduced version of the rule without the JJ node (NP —* DT NN). They also allow node matching among similar preterminals such as JJ, JJR, and JJS, mapping them to one equivalence class. Other relevant approaches are the spectrum tree (SpT) (Kuboyama et al., 2007) and the route kernel (RtT) (Aiolli et al., 2009). SpT uses a q-gram – a sequence of connected vertices of length q – as their sub-structure. It observes grammar rules by recording the orientation of edges: a+—b—*c is different from a—*b—*c. RtT uses a set of routes as basic structures, which observes grammar rules by Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). recording the index of a neighbor node. 2.2 Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention </context>
</contexts>
<marker>Kuboyama, Hirata, Kashima, Aoki-Kinoshita, Yasuda, 2007</marker>
<rawString>Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima, Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda. 2007. A spectrum tree kernel. Information and Media Technologies, 2(1):292–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector Llorens</author>
<author>Estela Saquete</author>
<author>Borja Navarro</author>
</authors>
<title>Tipsem (english and spanish): Evaluating CRFs and semantic roles in TempEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>284--291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6378" citStr="Llorens et al., 2010" startWordPosition="987" endWordPosition="990">RtT uses a set of routes as basic structures, which observes grammar rules by Figure 2: A parse tree (left) and its descending paths according to Definition 1 (l - length). recording the index of a neighbor node. 2.2 Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1. The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tag</context>
</contexts>
<marker>Llorens, Saquete, Navarro, 2010</marker>
<rawString>Hector Llorens, Estela Saquete, and Borja Navarro. 2010. Tipsem (english and spanish): Evaluating CRFs and semantic roles in TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 284–291. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Miller</author>
<author>Steven Bethard</author>
<author>Dmitriy Dligach</author>
<author>Sameer Pradhan</author>
<author>Chen Lin</author>
<author>Guergana Savova</author>
</authors>
<title>Discovering temporal narrative containers in clinical text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Workshop on Biomedical Natural Language Processing,</booktitle>
<pages>18--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="15154" citStr="Miller et al., 2013" startWordPosition="2533" endWordPosition="2536">sis for the general viability of the DPK, suggesting that in languages with freer word order it may give inflated scores to structures that are syntactically dissimilar if they have the same constituent components in different order. Formally, Moschitti (2006) showed that SST is a special case of PTK when only the longest child sequence from each tree is considered. On the other end of the spectrum, DPK is a special case of PTK where the similarity between rules only considers child subsequences of length one. 4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the CONTAINS relation between a time expression and a same-sentence event from clinical notes in the THYME corpus, which has 78 notes of 26 patients. We obtained this corpus from the authors and followed their linear composite kernel setting: P KC(s1, s2) = τ KT(tp1, tp2)+KF(f1, f2) (2) p=1 where si is an instance object composed of flat features fi and a syntactic tree ti. A syntactic tree ti 84 can h</context>
<context position="17475" citStr="Miller et al. (2013)" startWordPosition="2922" endWordPosition="2925">his system. The kernel setting was similar to equation (2), while there was only one tree representation, PET tree, P=1. A linear kernel was used as KF to evaluate the exact same flat features as used by the original system. We used the built-in cross validation to do grid search for tuning the parameters. The final system was tested on the testing data for reporting results. 4.3 Results and Discussion Results are shown in Table 2. The top section shows THYME results. For these experiments, the DPK is superior when a syntactically-rich PET representation is used. Using the full feature set of Miller et al. (2013), SST is superior to DPK and obtains the best overall performance. The bottom section shows results on TempEval-2013 data, for which there is little benefit from either tree kernel. Our experiments with THYME data show that DPK can capture something in the linguistically richer PET representation that the SST kernel cannot, but adding BT and PT representations decrease the DPK performance. As a shallow representation, BT does not have much in the way of descending paths for DPK to use. PT already ignores the production grammar by removing the inner tree nodes. DPK therefore cannot get useful i</context>
</contexts>
<marker>Miller, Bethard, Dligach, Pradhan, Lin, Savova, 2013</marker>
<rawString>Timothy Miller, Steven Bethard, Dmitriy Dligach, Sameer Pradhan, Chen Lin, and Guergana Savova. 2013. Discovering temporal narrative containers in clinical text. In Proceedings of the 2013 Workshop on Biomedical Natural Language Processing, pages 18–26, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seyed Abolghasem Mirroshandel</author>
<author>M Khayyamian</author>
<author>GR Ghassem-Sani</author>
</authors>
<title>Using tree kernels for classifying temporal relations between events.</title>
<date>2009</date>
<booktitle>Proc. of the PACLIC23,</booktitle>
<pages>355--364</pages>
<contexts>
<context position="6591" citStr="Mirroshandel et al. (2009)" startWordPosition="1014" endWordPosition="1018">node. 2.2 Temporal Relation Extraction Among NLP tasks that use syntactic information, temporal relation extraction has been drawing growing attention because of its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1. The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (</context>
</contexts>
<marker>Mirroshandel, Khayyamian, Ghassem-Sani, 2009</marker>
<rawString>Seyed Abolghasem Mirroshandel, M Khayyamian, and GR Ghassem-Sani. 2009. Using tree kernels for classifying temporal relations between events. Proc. of the PACLIC23, pages 355–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>318--329</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1972" citStr="Moschitti, 2006" startWordPosition="277" endWordPosition="278"> not always clear. Convolution kernels over syntactic trees (tree kernels) offer a potential solution to this problem by providing relatively efficient algorithms for computing similarities between entire discrete structures. These kernels use tree fragments as features and count the number of common fragments as a measure of similarity between any two trees. However, conventional tree kernels are sensitive to pattern variations. For example, two trees in Figure 1(a) sharing the same structure except for one terminal symbol are deemed at most 67% similar by the conventional tree kernel (PTK) (Moschitti, 2006). Yet one might expect a higher similarity given their structural correspondence. The similarity is further attenuated by trivial structure changes such as the insertion of an adjective in one of the trees in Figure 1(a), which would reduce the similarity close to zero. Such an abrupt attenuation would potentially propel a model to memorize training instances rather than generalize from trends, leading towards overfitting. In this paper, we describe a new kernel over syntactic trees that operates on descending paths through the tree rather than production rules as used in most existing methods</context>
<context position="4389" citStr="Moschitti, 2006" startWordPosition="650" endWordPosition="651">ure 1(b) would be zero since the production rule is different (the overall similarity score is abovezero because of matching pre-terminals). The partial tree kernel (PTK) relaxes the definition of subtrees to allow partial production rule 81 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81–86, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics NP l=0: [NP],[DT],[NN] l=1: [NP-DT],[NP-NN], [DT-a],[NN-cat] l=2: [NP-DT-a],[NP-NN-cat] a) NN DT a cat NP NP NP DT NN DT NN a dog a cat NP matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates a very large but sparse feature space. To Figure 1(b), the PTK generates fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a] [NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among others, for the second tree. This allows for partial matching – substructure (ii) – while also generating some fragments that violate grammatical intuitions. Zhang et al. (2007) address the restrictiveness of SST by allowing soft matching of production rules. They allow partia</context>
<context position="10407" citStr="Moschitti, 2006" startWordPosition="1698" endWordPosition="1699"> computed in polynomial time by the following recursive rules: Rule 1: If n1 and n2 have different labels (e.g., ”DT” versus “NN”), then C(n1, n2) = 0; Rule 2: Else if n1 and n2 have the same labels and are both pre-terminals (POS tags), then � 1 if term(n1) = term(n2) C(n1, n2) = 1 + 0 otherwise. where term(n) is the terminal symbol under n; Rule 3: Else if n1 and n2 have the same labels and they are not both pre-terminals, then: C(n1, n2) = 1 + � C(ni, nj) niEchildren(n1) njEchildren(n2) where children(m) are the child nodes of m. As in other tree kernel approaches (Collins and Duffy, 2001; Moschitti, 2006), we use a discount parameter A to control for the disproportionately large similarity values of large tree structures. Therefore, Rule 2 becomes: C(n1, n2) = 1 + � A if term(n1) = term(n2) 0 otherwise. and Rule 3 becomes: C(n1, n2) = 1 + A � C(ni, nj) niEchildren(n1) njEchildren(n2) Note that Eq. (1) is a convolution kernel under the kernel closure properties described in Haussler (1999). Rules 1-3 show the equivalence between the number of common descending paths rooted at nodes n1 and n2, and the number of matching nodes below n1 and n2. In practice, there are many non-matching nodes, and m</context>
<context position="12752" citStr="Moschitti (2006)" startWordPosition="2119" endWordPosition="2120">hat for more complicated structures, i.e., examples b and c, DPK generates fewer fragments than SST and PTK, with more shared fragments among trees. The complexity for all three kernels are at least O(|N1||N2|) since they share the pairwise summation at the end of Equation 1. SST, due to its requirement of exact production rule matching, only takes one pass in the inner loop which adds a factor of p (the maximum branching factor of any production rule). DPK does a pairwise summation of children, which adds a factor of p2 to the complexity. Finally, the efficient algorithm for PTK is proved by Moschitti (2006) to contain a constant factor of p3. Table 1 orders the tree kernels according by their listed complexity. It may seem that the value of DPK is strictly in its ability to evaluate all paths, which is not explicitly accounted for by other kernels. However, another view of the DPK is possible by thinking of it as cheaply calculating rule production similarity by taking advantage of relatively strict English word ordering. Like SST and PTK, the DPK requires the root category of two subtrees to be the same for the similarity to be greater than zero. Unlike SST and PTK, once the root category compa</context>
<context position="14794" citStr="Moschitti (2006)" startWordPosition="2473" endWordPosition="2474">ith small differences in production rules (one or two adjectives, extra nominal modifier, etc.) the PTK will capture similarity by comparing every possible partial rule completion, but the DPK can obtain higher and faster scores by just comparing one child at a time because the ordering is constrained by the language. This analysis does lead to a hypothesis for the general viability of the DPK, suggesting that in languages with freer word order it may give inflated scores to structures that are syntactically dissimilar if they have the same constituent components in different order. Formally, Moschitti (2006) showed that SST is a special case of PTK when only the longest child sequence from each tree is considered. On the other end of the spectrum, DPK is a special case of PTK where the similarity between rules only considers child subsequences of length one. 4 Evaluation We applied DPK to two published temporal relation extraction systems: (Miller et al., 2013) in the clinical domain and Cleartk-TimeML (Bethard, 2013) in the general domain respectively. 4.1 Narrative Container Discovery The task here as described by Miller et al. (2013) is to identify the CONTAINS relation between a time expressi</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Machine Learning: ECML 2006, pages 318–329. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
</authors>
<title>The TimeBank corpus.</title>
<date>2003</date>
<booktitle>In Corpus linguistics,</booktitle>
<volume>volume</volume>
<pages>40</pages>
<contexts>
<context position="6751" citStr="Pustejovsky et al., 2003" startWordPosition="1038" endWordPosition="1041"> its wide applications in multiple domains. As subtasks in TempEval 2007, 2010 and 2013, multiple systems were built to create labeled links from events to events/timestamps by using a variety of features (Bethard and Martin, 2007; Llorens et al., 2010; Chambers, 2013). Many methods exist for synthesizing syntactic information for temporal relation extraction, and most use traditional tree kernels with various feature representations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1. The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improve</context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, et al. 2003. The TimeBank corpus. In Corpus linguistics, volume 2003, page 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>A proposal for improving the measurement of parse accuracy.</title>
<date>2000</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Sampson, 2000</marker>
<rawString>Geoffrey Sampson. 2000. A proposal for improving the measurement of parse accuracy. International Journal of Corpus Linguistics, 5(1):53–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Strassel</author>
<author>Dan Adams</author>
<author>Henry Goldberg</author>
<author>Jonathan Herr</author>
<author>Ron Keesing</author>
<author>Daniel Oblinger</author>
<author>Heather Simpson</author>
<author>Robert Schrag</author>
<author>Jonathan Wright</author>
</authors>
<title>The DARPA machine reading program-encouraging linguistic and reasoning research with a series of reading tasks.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="7174" citStr="Strassel et al., 2010" startWordPosition="1112" endWordPosition="1115">esentations. Mirroshandel et al. (2009) used the path-enclosed tree (PET) representation to represent syntactic information for temporal relation extraction on the TimeBank (Pustejovsky et al., 2003) and the AQUAINT TimeML corpus1. The PET is the smallest subtree that contains both proposed arguments of a relation. Hovy et al. (2012) used bag tree structures to represent the bag of words (BOW) and bag of part of speech tags (BOP) between the event and time in addition to a set of baseline features, and improved the temporal linking performance on the TempEval 2007 and Machine Reading corpora (Strassel et al., 2010). Miller at al. (2013) used PET tree, bag tree, and path tree (PT, which is similar to a PET tree with the internal nodes removed) to represent syntactic information and improved the temporal relation discovery performance on THYME data2 (Styler et al., 2014). In this paper, we also use syntactic structure-enriched temporal relation discovery as a vehicle to test our proposed kernel. 3 Methods Here we decribe the Descending Path Kernel (DPK). 1http://www.timeml.org 2http://thyme.healthnlp.org a cat cat a fat S NP VP PRP VBZ she ADVP RB here comes S NP VP VBZ comes PRP she ADVP RB here Figure 1</context>
</contexts>
<marker>Strassel, Adams, Goldberg, Herr, Keesing, Oblinger, Simpson, Schrag, Wright, 2010</marker>
<rawString>Stephanie Strassel, Dan Adams, Henry Goldberg, Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather Simpson, Robert Schrag, and Jonathan Wright. 2010. The DARPA machine reading program-encouraging linguistic and reasoning research with a series of reading tasks. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Styler</author>
<author>Steven Bethard</author>
<author>Sean Finan</author>
<author>Martha Palmer</author>
<author>Sameer Pradhan</author>
<author>Piet de Groen</author>
<author>Brad Erickson</author>
<author>Timothy Miller</author>
<author>Lin Chen</author>
<author>Guergana K Savova</author>
<author>James Pustejovsky</author>
</authors>
<title>Temporal annotations in the clinical domain.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<issue>2</issue>
<marker>Styler, Bethard, Finan, Palmer, Pradhan, de Groen, Erickson, Miller, Chen, Savova, Pustejovsky, 2014</marker>
<rawString>William Styler, Steven Bethard, Sean Finan, Martha Palmer, Sameer Pradhan, Piet de Groen, Brad Erickson, Timothy Miller, Lin Chen, Guergana K. Savova, and James Pustejovsky. 2014. Temporal annotations in the clinical domain. Transactions of the Association for Computational Linguistics, 2(2):143−154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ai Ti Aw</author>
<author>Chew Lim Tan</author>
<author>Guodong Zhou</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A grammar-driven convolution tree kernel for semantic role classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="3047" citStr="Zhang et al., 2007" startWordPosition="444" endWordPosition="447">new kernel over syntactic trees that operates on descending paths through the tree rather than production rules as used in most existing methods. This representation is reminiscent of Sampson’s (2000) leaf-ancestor paths for scoring parse similarities, but here it is generalized over all ancestor paths, not just those from the root to a leaf. This approach assigns more robust similarity scores (e.g., 78% similarity in the above example) than other soft matching tree kernels, is faster than the partial tree kernel (Moschitti, 2006), and is less ad hoc than the grammar-based convolution kernel (Zhang et al., 2007). 2 Background 2.1 Syntax-based Tree Kernels Syntax-based tree kernels quantify the similarity between two constituent parses by counting their common sub-structures. They differ in their definition of the sub-structures. Collins and Duffy (2001) use a subset tree (SST) representation for their sub-structures. In the SST representation, a subtree is defined as a subgraph with more than one node, in which only full production rules are expanded. While this approach is widely used and has been successful in many tasks, the production rule-matching constraint may be unnecessarily restrictive, giv</context>
<context position="4889" citStr="Zhang et al. (2007)" startWordPosition="736" endWordPosition="739">DT-a],[NN-cat] l=2: [NP-DT-a],[NP-NN-cat] a) NN DT a cat NP NP NP DT NN DT NN a dog a cat NP matching (Moschitti, 2006). In the PTK, a subtree may or may not expand any child in a production rule, while maintaining the ordering of the child nodes. Thus it generates a very large but sparse feature space. To Figure 1(b), the PTK generates fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a] [NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among others, for the second tree. This allows for partial matching – substructure (ii) – while also generating some fragments that violate grammatical intuitions. Zhang et al. (2007) address the restrictiveness of SST by allowing soft matching of production rules. They allow partial matching of optional nodes based on the Treebank. For example, the rule NP —* DT JJ NN indicates a noun phrase consisting of a determiner, adjective, and common noun. Zhang et al.’s method designates the JJ as optional, since the Treebank contains instances of a reduced version of the rule without the JJ node (NP —* DT NN). They also allow node matching among similar preterminals such as JJ, JJR, and JJS, mapping them to one equivalence class. Other relevant approaches are the spectrum tree (S</context>
</contexts>
<marker>Zhang, Che, Aw, Tan, Zhou, Liu, Li, 2007</marker>
<rawString>Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan, Guodong Zhou, Ting Liu, and Sheng Li. 2007. A grammar-driven convolution tree kernel for semantic role classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200–207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>