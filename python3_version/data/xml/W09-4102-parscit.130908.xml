<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003428">
<title confidence="0.9942605">
Maximal Phrases Based Analysis for Prototyping Online
Discussion Forums Postings
</title>
<author confidence="0.99494">
Gaston Burek Dale Gerdemann
</author>
<affiliation confidence="0.995897">
Department of Linguistics
Tuebingen University
</affiliation>
<address confidence="0.814193">
72074 Tuebingen, Germany
</address>
<email confidence="0.998103">
[gaston.burek, dale.gerdemann]@googlemail.com
</email>
<sectionHeader confidence="0.975765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.866637083333333">
Chat texts produced in an educational environ-
ment are categorized and rated for the purpose of
positioning (or placement) of the learner with re-
spect to a learning program (appropriate courses,
textbooks, etc). The difficulty lies in the fact
that the texts are short and informal. A standard
LSA/vector-space model is therefore combined
with techniques appropriate for short texts. The
approach uses phrases rather than words in the
term-document matrix, and for determining pro-
totypical documents of each category, a nonpara-
metric permutation test is used.
</bodyText>
<sectionHeader confidence="0.994049" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999943307692308">
Text categorization is a well-established sub-field of
computational linguistics. We are interested in apply-
ing the techniques of text categorization for the pur-
pose of positioning Life Long Learners with respect
to educational programs and instructional materials.
Quite simply, we can rate learner texts, likely to be
short and generated informal educational settings, by
using a vector-space comparison to gold-standard, ex-
pert texts. Then if the similarity is high enough, the
learner will proficiency out of the course. This ap-
proach is straightforward, but is unlikely to be suc-
cessful. The problems concern accuracy, suitability
and justification of the categorization.
</bodyText>
<subsectionHeader confidence="0.988719">
1.1 Accuracy
</subsectionHeader>
<bodyText confidence="0.999984666666667">
The categorization obviously must be accurate. A false
positive, indicating learner proficiency in a particular
domain, could be dangerous as it could lead to work
place incompetence. A false negative, on the other
hand, could lead to boredom, as the learner is forced
to take courses on topics that he or she has already
mastered.
The problem of accuracy is compounded by the fact
that texts (selected from text collections known as
ePortfolios) are often short. To deal with this prob-
lem, our approach attempts to lose as little informa-
tion from the text as possible. Traditional approaches
to categorization lose information by case normaliza-
tion, stemming and ignoring word order. The idea
of the traditional approach is to deal with the data
</bodyText>
<page confidence="0.984962">
12
</page>
<bodyText confidence="0.999887470588235">
sparseness problem by collapsing textual features into
equivalence classes, losing information in the process.
In our approach, we attempt to balance the problem
of data sparseness with the goal of not losing informa-
tion. This balance is obtained in two ways. First, as
discussed in section 2, we use Latent Semantic Anal-
ysis (LSA) as a technique for dimensionality reduc-
tion [8]. It is well known that LSA can be used to
discover weighted clusters of words, which are loosely
understood to be “concepts.” Since these clusters can
contain derivationally related terms, the need for stem-
ming (and also case normalization) is reduced. Second,
as discussed in 3, our more innovative contribution is
to flexibly use bigrams, trigrams and other ngrams as
opposed to strictly unigrams in the traditional bag-of-
words model. Our approach to extracting such ngrams
is to use a extension of the suffix array approach of [12].
</bodyText>
<subsectionHeader confidence="0.881536">
1.2 Suitability
</subsectionHeader>
<bodyText confidence="0.999995103448276">
Suppose that learner texts could be accurately clas-
sified as similar or not similar to the gold standard
text (or set of texts). Then the question arises as to
whether or not the gold standard text is a suitable
prototype for a good learner text. One approach to
choosing a gold standard text would be to use a pub-
lished journal article in the field. But such a text is
unlikely to be similar to learner texts either in tone
or in content. It is well known that effective teachers
use scaffolding to present material within the zone of
proximal development of the learner.
So perhaps a better gold standard would be a text-
book, or other learning material, written at the level of
the student. This is certainly an improvement, but on
the other hand, it is still rather unreasonable to expect
learners’ texts to closely match the tone of a textbook,
unless of course the learners are copying from the text.
In fact, the texts that we have consist of online discus-
sions of medical students on several topics related to
safe prescribing. These texts have been categorized as
to subtopic and graded for quality (excellent, good,
fair, poor) by annotaters at the University of Manch-
ester. The texts contain serious conversations, with
very little off-topic wandering. But the tone of the
texts is chatty, and not at all similar to textbook writ-
ing. So rather than to use an external gold standard,
we have opted for an internal gold standard. The pro-
totypical “excellent” text is simply one that was rated
as “excellent” by the Manchester annotators. But not
</bodyText>
<subsectionHeader confidence="0.702097">
Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 12–18
</subsectionHeader>
<bodyText confidence="0.9996185">
all “excellent” texts are equally good as prototypes.
Clearly, for any text t, the remaining texts can be
ranked in order of similarity to t. If t is a good proto-
type, then this ranking should other “excellent” texts
as most similar to t and “poor” should be least simi-
lar. In subsection 4, we discuss a method for choosing
good prototypes that uses the nonparametric permu-
tation test.
</bodyText>
<subsectionHeader confidence="0.65584">
1.3 Justification: communities of prac-
tice distinctive language
</subsectionHeader>
<bodyText confidence="0.990889392857143">
It is widely accepted that experts can provide answers
to problems that average people can not e.g. evidence
given in court can be accepted or rejected on the ba-
sis of expertise, relevance and reliability. In this con-
texts expertise is defined as knowledge beyond com-
mon sense [6].
Communities of practice are at the center of expert
knowledge development. According to [4] new com-
munities of practice develop conditions for effective
local creation and communication of knowledge and
therefore that knowledge is not shared with other com-
munities until it settles. In linguistic terms this can
be understood as a distinctive use of language shared
among individuals members of a specific community to
describe knowledge that is not shared with other com-
munities of practice. According to [2] expert language
in medicine consists in shared specific terms formalized
in medical term banks, thesauri and lexicons. Patients
get familiar with that established medical terminology
relevant to their own health conditions medical lan-
guage after exposure to treatments, personal doctor
visits, etc.
1.3.1 Language technologies supporting the
identification of expertise
As life long learners do not have common learning
goals nor common educational backgrounds as it is the
case in traditional learning settings, long life learning
educational providers need to rely on available writ-
ten materials produced by individual learner to iden-
tify their degree of expertise within areas of knowledge
that are relevant to study programs in offer.
Given this scenario there is a need for tools that can
provide support in determining the learner’s degree
of expertise or position by means of state of the art
language technologies. Those technologies should be
capable of identify linguistics features that reflect the
degree of learner’s expertise by analyzing learners’ text
repositories.
But, will learners affected by the use of such tech-
nologies (e.g. LSA) be happy to hear about a expert
system suggestion that implies the need of studying a
microbiology course on account of a low cosine simi-
larity value between the learners essays and texts pro-
duced by microbiology experts? It is well known that
users are more inclined to trust an expert system when
such tool can give some reasons for its judgement.
Here, in addition to use LSA for comparing simi-
larities between learner written text and expert texts,
we present the use suffix array analysis for character-
izing text in a way that users of that technologies can
understand .
Grammar and language constructions can be used to
identify language that is characteristic of people that is
not familiarized with a relevant communities of prac-
tice e.g. microbiology . This linguistic feedback could
be combined with a tentative suggestion that taking
a microbiology course would be one way to acquire
these linguistic conventions that tend to correlate with
knowledge of microbiology.
Our work focus on the use analysis of distinctive
phrases for identifying the degree of expertise of it au-
thor in a specific domain where that expertise may be
expressed by linguistics features that is not be evident
at surface level. This approach determine that degree
of expertise as the probability of language misuse in re-
lation to average language use as sampled from expert
written texts.
To give an example from and available, consider the
terms ”prescription charts” and ”drug charts.” These
terms apparently mean the same thing, but it turns
out that ”prescription charts” occurs predominately in
texts rated excellent or good, and ”drug charts” occurs
predominately in texts rated fair or poor. Suppose
that the hypothesis that doctors either consciously or
unconsciously prefer the term ”prescription charts.”
Then if a learner uses the term ”drug charts,” this us-
age could be tagged as less favored terminology, and
a tentative suggestion could possibly be made that
the learner could take a pharmacology course. If the
learner ultimately rejects this suggestion, then at least
this learner wouldn’t go home empty handed. The
learner would at least have received some linguistic
feedback that would be unlikely to come from a hu-
man evaluator.
</bodyText>
<sectionHeader confidence="0.719602" genericHeader="method">
2 Latent Semantic Analysis
</sectionHeader>
<bodyText confidence="0.99988825">
In recent years, LSA has been proposed as a suit-
able language technology for the automatic determin-
ing the degree of expertise of a specific text’s author
[10]. Although, singular value decomposition (SVD)
of word co-occurrence frequencies matrices has been
successfully used in the context of language technolo-
gies enhanced learning (e.g. automatic assessment of
student essays), learner positioning presents new chal-
lenges that expose the limitation of such an approach.
In particular, learners produce text repositories con-
taining few samples of text, many of them of small size
and using language that is rich in non domain-specific
expressions. Such repositories may also be generated
by individuals from different backgrounds in informal
learning environments (chats, online forums, etc.). In
addition, those texts are generated in contexts where
learners feel encouraged to hide their poor usage of
language by articulating redundant expressions and
making extensive use keywords. Moreover, linking the
semantics of high level descriptions of learning goals
and domain specific terminology used by learners in
a non-formal context restrict the usability of LSA as
word usage may not be very stable across corpora.
While semantic spaces approaches such as the Vec-
tor Space Model (VSM) captures surface features of
a semantic space such as first or second order word
co-occurrence, the semantic similarity theory behind
LSA is based on a model that captures different higher
</bodyText>
<page confidence="0.994508">
13
</page>
<bodyText confidence="0.99996662962963">
orders of word co-occurrence (i.e. third order co-
occurrence and higher) by means of using singular
value decomposition. LSA uses SVD to project the
semantic space over a lower dimensional space. LSA
supporters claim that by reducing the space dimen-
sionality, the semantic space that models the human
cognitive process becomes more accurate due to the re-
duction of the noise that is added during the process
of generating language.
LSA belongs to the family of semantic space mod-
els and is capable of inferring relations between words
that are not explicit at surface level. LSA is popular
with psychologists since the dimensionality reduction
can be interpreted as reducing the word space to con-
cept space. But ultimately these ”concepts” are load-
ings on word counts, which are not easily interpreted
or explained.
LSA as well the other semantic spaces approaches
represents the semantics of words, sentences and para-
graphs using word co-occurrence information that does
not take in consideration the position of words within
the sentence. However, according to a general con-
ception of semantics, syntax plays a significant role in
representing meaning of sentences. Thus, intuitively
to using information about the order words occupy
within expressions in addition to their frequency of oc-
currence seems to be a theoretically sounded approach
to improve LSA performance that is in line with mayor
trends in language acquisition theories that stress the
significance that syntactic structure play in the com-
prehension of language.
Although, semantic spaces approaches (e.g. La-
tent Semantic Analysis) have been successful in rea-
soning about ambiguity and semantic similarity when
analysing texts at the level of words (linguistic units),
sentences (grammatical units) and paragraphs (dis-
course units) they are yet not capable of reasoning
beyond the analysis of a bag of words.
Typically, LSA projects a semantic space imple-
mented as a sparse matrix into a dimensionally re-
duced VSM generating the best statistical approxima-
tion to the original model. The dimensionally reduced
model results in a matrix with non 0 values that allows
the computation of similarity between matrix columns
or rows that were orthogonal in the original model.
LSA theory assumes that many words have a simi-
lar probabilistic distribution that results in a compar-
atively lower number of concepts.
As the variation of word choice, introduce noise to
the text, SVD, the algorithm behind LSA bridges the
gaps in words by conflating word used into word senses
. Furthermore, LSA facilitates matrix manipulation in
terms of hardware operative memory by reducing the
dimensionality of the VSM.
</bodyText>
<sectionHeader confidence="0.997237" genericHeader="method">
3 Phrases
</sectionHeader>
<bodyText confidence="0.999982757575758">
Traditionally, text categorization by means of LSA has
relied on a bag-of-words model. It seems, in some
sense, obvious that a model based on phrases should be
better. But it turns out that this is not necessarily the
case. Recently, Bekkerman &amp; Allan [3] reviewed the
literature on text categorization and found no general
improvement when unigram models were replaced with
bigram models. The problem is that using bigrams
contributes heavily to the data sparseness problem.
Bekkerman &amp; Allan have, however, compared two
rather extreme positions. Our idea is to extract
phrases of any length from the the training corpus, as
long as the phrases are distinctive (occurring predom-
inately in particular categories of documents). It may
well be that the most distinctive phrases are gener-
ally phrases of length one (concurring with the bag-of-
words model), but if there are phrases of other lengths
that are more distinctive, then there seems to be no
reason not to use these phrases.
To give an idea of the approach, consider the word
side. In the medical discussions in our corpus, this
word almost always occurs as part of the phrase side
effect(s). In a few cases, side occurs in a unique con-
text or as part of another phrase, such as flip side. In
this case, the distinctive phrase is apparently side ef-
fect, and the other occurrences are just noise. These
noise phrases are not only unhelpful for text catego-
rization, they are are also unhelpful for generating ex-
planations that would be useful for learners and ex-
aminers.
The example above raises some interesting counting
issues. But first we need to specify more precisely what
it means for a phrase to be distinctive.
</bodyText>
<subsectionHeader confidence="0.983432">
3.1 Distinctiveness
</subsectionHeader>
<bodyText confidence="0.967565628571428">
In general, phrases that are evenly distributed across
document categories are not very distinctive, whereas
phrases that tend to cluster in one particular category
are distinctive. This general principle must be applied
carefully, however, since with small numbers, cluster-
ing may occur due to chance.
A common measure of distinctiveness used for
weighting in vector space models is tf-idf (term fre-
quency multiplied by inverse document frequency) [9].
It is unclear, however, that this is the best measure
for picking out which phrases to consider and which
phrases to ignore. It is problematic, for example, that
idf simply prefers terms that cluster in a small number
of documents, regardless of the classifications. Given
the ordinal classification of our data as excellent, good,
fair and poor, we are not interested, for example, in
terms that cluster in the excellent and poor texts.
So a distinctive term should be one that occurs pre-
dominately in excellent and good texts or predomi-
nately in fair and poor texts. Consider, for example,
the bullet point , with occurrence vector (31, 5, 0, 0).l
The interpretation is that there are 31 occurrences in
excellent documents, 5 occurrences in good documents
and no occurrences in either of the poorer texts, this
term appears be very distinctive of better texts. But
if we count instead the number of different documents
the bullet point occurs (4, 1, 0, 0), we see a very dif-
ferent picture. The bullet point does occur in higher
rated texts, but it is very bursty (cite Church) and is
therefore not very useful for categorization.
There are various approaches in the literature for
dealing with burstiness. Since this is not our primary
1 Optionally the tokenizer could be set to eliminate such punc-
tuation marks. The bullet point makes a good example here,
however, due to its burstiness.
</bodyText>
<page confidence="0.996476">
14
</page>
<bodyText confidence="0.9999834375">
concern here, we deal with the problem by counting
the number of texts containing a term rather than the
total number of occurrences of the term. Thus, for the
bullet point, we use the vector (4, 1, 0, 0).
To rate a term such as the bullet point, we need some
measure of goodness for the vector (4, 1, 0, 0). There
is clearly no objective measure that can be used here.
As a fairly reasonable score, we simply assign 1 point
for every excellent text, 0.8 points for every good text
and 0.2 points for every fair text. So, the bullet point
receives a score of 4.8. This appears to be a good
score, but what is the probability that a randomly
chosen term appearing in 5 texts would have a higher
or equally high score? We can generate random vectors
as in (3.1) (where e, g, f and p are the total numbers
of excellent, good, fair and poor texts, respectively.
</bodyText>
<equation confidence="0.936854875">
(1, 0, 0, 0) with probability e
e+g+f+p
(0, 1, 0, 0) with probability g
e+g+f+p
f
(0, 0, 1, 0) with probability e+g+f
(0, 0, 0,1) with probability p
e+g+f+p
</equation>
<bodyText confidence="0.9996165">
A vector for a random term occurring in n texts is then
En i=0 Xi. So for a good score such as 4.8, the idea is
to see what proportion of randomly generated vectors
have an equally high or higher score. And for a low
score, the opposite idea is to count the proportion of
randomly generated scores that are equal or lower.
</bodyText>
<subsectionHeader confidence="0.998758">
3.2 Phrase Extraction
</subsectionHeader>
<bodyText confidence="0.99997587912088">
In principle, the distinctness measure given above can
be used with phrases of any length. If longer phrases
can be found that are more distinct than single words,
then there is no reason not to use the longer phrase.
The problem is that the simulation-based distinctness
test is very expensive, and it is certainly not possible
to run this test for ngrams of every length in a text.
The solution to this problem comes from Yamamoto
&amp; Church [12], who show suffix arrays can be used to
put the large number of ngrams into a much smaller
number of equivalence classes. Using suffix arrays, it
is very easy to pick out just the phrases that are re-
peated n times for some n, and it is very easy to extend
phrases to the right: if mumbo jumbo repeatedly oc-
curs together as a phrase, then it makes no sense to
count mumbo by itself. Yamamoto &amp; Church’s suf-
fix array program will put these two phrases into an
equivalence class, so that that statistics can be calcu-
lated for the class as a whole rather than individually
for all the members of the class.
Since the time of Yamamoto &amp; Church’s paper, suf-
fix arrays have been an active area of research, primer-
ily in bioinformatics. One of the weaknesses of the
suffix array approach used by Yamamoto &amp; Church is
that extensions to the left are difficult to discover. So it
is difficult to discover, for example, that jumbo always
combines to the left to form the phrase mumbo jumbo.
Simply stated, the problem is that suffixes are exten-
sions of phrases to the right, so it is hard to look to
the left. This problem was solved, however, by Abouel-
hoda et al [1], who added a BurrowsWheeler transform
table to their extended suffix array data structure, giv-
ing this this data structure properties of suffix trees.
One weakness of Abouelhoda et al’s approach, how-
ever, is that it does not adapt well to large alpha-
bets. This is, of course, a serious weakness for use in
text processing, where one wants at least to work with
some subset of Unicode, or even worse, to treat each
tokenized word as an alphabet symbol. Fortunately,
the restriction to small alphabet size has recently been
eliminated in the approach of Kim et al [7], who deal
with the large alphabet by using binary trees, which
are linearly encoded using the child table of Abouel-
hoda et al along with a longest common prefix table
(lcp).
Using extended suffix arrays makes it possible to
count different kinds of occurrences of phrases in dif-
ferent ways. To begin with, we are only interested in
counting phrases that repeat. In the text S = to be or
not to be, the occurrence of the phrase to be at S[1, 2] is
said to be a repeat since the same sequence of tokens
occurs at S[5,6].2 An occurrence of a phrase S[i, j]
is left maximal is left maximal if the longer phrase
S[i−1, j] is not a repeat. Thus, for example, the phrase
to at S[1,1] is left maximal since the phrase at S[0,1]
is not a repeat.3 Similarly, an occurrence of a phrase
at S[i, j] is right maximal if S[i, j + 1] is not a repeat.
If an occurrence of a phrase is both left and right max-
imal, then the occurrence is said to be maximal. Note
that the occurrence of the phrase or not at S[3,4] is
maximal, though it is not a repeat. Since non-repeats
are rarely of interest, we generally assume that we are
talking about repeats unless otherwise stated.
A phrase is also said to be maximal in a text if there
exists a maximal occurrence of the phrase in the text.
For example, in the text mining engineering, tokenized
by characters, the phrase in is maximal since there
are maximal occurrences at S[2,3] and S[11,12]. But
the longer phrase ing is also maximal since it occurs
maximally at S[4,6] and S[16,18]. So the occurrence
of in at S[16,17] is a non-maximal occurrence of a
maximal phrase. A maximal repeated phrase that is
not a subsequence of a longer maximal repeated phrase
is said to be supermaximal. Thus the phrase ing is
supermaximal in this text.
Generally, we are only interested in counting occur-
rences of maximal phrases since a phrase that never oc-
curs maximally is unlikely to be of interest. But what
kind of occurrences should we count? Should we count
all occurrences, or only the left maximal, right maxi-
mal or maximal occurrences? The answer is that we
don’t need to decide ahead of time. We can simply test
each of these four cases for distinctness, and chose the
most distinct case. Take, for example the word side,
which is a maximal phrase in our texts. Should we
count all instances of this phrase? Or should we per-
haps restrict the count to right maximal occurrences
so as to avoid counting those instances that are ex-
tended to the right to create the longer phrase side ef-
fect? Or maybe left maximal occurrences to avoid the
longer phrase flip side? Or perhaps we should restrict
</bodyText>
<footnote confidence="0.984077571428571">
2 This definition and the following definitions are similar to
those found in Abouelhoda et al [1]. The difference is that
Abouelhoda et al apply the terms to a pair of occurrences,
whereas we apply the terms to a single occurrence.
3 We assume here that the text is padded with unique beginning
of string and end of string sentinels so that indexing at 0 or
7 makes sense.
</footnote>
<equation confidence="0.726178">
Xi = I
</equation>
<page confidence="0.976575">
15
</page>
<bodyText confidence="0.979283612903226">
in both directions to avoid either kind of extension.
Since it is not generally possible to predict which is
best, the reasonable approach is to try all possibilities
to see what works best.
One counterintuitive feature of our approach is that
it also makes sense to count 0-grams. A left maxi-
mal occurrence of a 0-gram, for example, must have
a hapax legomena to its left, and a maximal occur-
rence of a 0-gram must have hapax legomena on both
sides. These sequences of two hapax legomena may
well be distinctive, since they often are an indication
of a named entity or a foreign phrases. Counting all
occurrences of the empty sequence is, of course, equiv-
alent to counting the text length, which may well also
be a distinctive feature.
4 Permutation test for proto-
typing chat texts
Our approach to categorization is based on similar-
ity to prototypical documents of each category. Here
again, we are concerned with finding an approach of
identifying good prototypes in a way that is suitable
for use with a small data set. We note that each
candidate prototype induces a ranking of the remain-
ing documents from ”most similar” to ”least similar.”
For a good prototype, this ranking should be signifi-
cantly different from a random permutation. To test
this hypothesis, we can use a standard nonparamet-
ric test, known (appropriately enough) as the ”per-
mutation test.” The permutation test is a nonpara-
metric method for testing whether two distributions
are the same. The test is ”exact,” meaning that it is
not based on large sample theory approximations [11].
Suppose that X1, ..., Xm ∼ FX and Y1, ..., Yn ∼ FY
are two independent samples and H0 is the hypoth-
esis that the two samples are identically distributed.
This is the type of hypothesis we would consider when
testing whether a treatment differs from a placebo.
More precisely we are testing H0 : FX = FY ver-
sus H1 : FX =6 FY . Let T(X1...,Xm,Y1,...,Yn)
=|Xn − Y n |and let N = m + n and consider forming
all N! permutations of the data X1..., Xm, Y1, ..., Yn.
For each permutation, compute the test statistic T.
Denote these values by T1,..., TNI. Under the null hy-
pothesis, each of these values is equally likely. Let tobs
be the observed value of the test statistic. We reject
the hypothesis when T is large.
Usually, it is not practical to evaluate all N! per-
mutatioons. We can approximate the p − value by
sampling randomly from the set of permutations. The
fraction of times Tj &gt; tobs among these samples ap-
proximates the p − value. In general If T is smaller
than some significance level a , the results are signifi-
cant at that level. In our case we are not concern about
a particular significant level as we are looking to the
most significant texts on certain topic and with certain
grade that can be used use as gold standards. Table
1 shows a toy example for the cosine similarity vector
(X1,X2,Y1) = (0.1, 0.5,0.8) where, |Xn − Y n |= 0.50.
The methodology described in this section can be
used as a reasonable approach for measuring how sig-
nificant (representative) chat texts are for a particular
permutation value of T probability
</bodyText>
<table confidence="0.999934">
( 0.1, 0.5, 0.8) 0.50 1/6
( 0.1, 0.8, 0.5) 0.05 1/6
( 0.5, 0.1, 0.8) 0.50 1/6
( 0.5, 0.8, 0.1) 0.55 1/6
( 0.8, 0.1, 0.5) 0.05 1/6
( 0.8, 0.5, 0.1) 0.55 1/6
</table>
<tableCaption confidence="0.999861">
Table 1: Permutation example
</tableCaption>
<bodyText confidence="0.9726595">
category and grade. The estimation is calculated on
the basis of cosine similarity between the texts and
does not assume any particular distribution for those
values.
</bodyText>
<sectionHeader confidence="0.996501" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999038125">
The experiments described in this section compare
the performance of the traditional bag of words LSA
configuration against an alternative configuration that
uses maximal phrases as unit of analysis.
The alternative LSA configuration starts with a vec-
tor space model that (instead of using words counts)
uses counts of highly distinctive phrases that occur at
least one time as maximal phrase within the text col-
lection under analysis.
The use of a ”bag of phrases” model instead of a
”bag of words” model is motivated by the small size of
our sample annotated of chat texts. Since our infor-
mation source is scarce, we cannot afford lose any of it.
If a medical text contains the phrase ”side effect,” for
example, it is significant information that these two
words occur as a phrase, which would be lost in the
traditional bag-of-words approach.
Our ongoing work in word co-occurrence models for
learner positioning extends existent LSA based ap-
proaches and is aimed at analyzing and then scoring
texts posted on an online medical student discussion
forum where students discuss issues related to the safe
prescribing of medicines, guided by clearly defined ex-
pected learning outcomes associated to six subtopics.
The 504 postings have been annotated by experts with
four grades (i.e. 109 poor, 200 fair, 142 good, 50 ex-
cellent) and one of six topics (i.e. 42 of topic a , 50 of
b, 130 of c, 22 of d , 247 of e and 13 of f). Each grade
is based on the individual posting’s textual contribu-
tion to a series of expected learning outcomes. Highly
scored postings can then be used as evidence of learner
proficiency in the corresponding topic.
</bodyText>
<subsectionHeader confidence="0.948833">
5.1 Building the bag of words and
phrases based semantic spaces
</subsectionHeader>
<bodyText confidence="0.9997076">
As explained in 3.2 to identify and extract the max-
imal phrases we analyze suffix arrays using an ex-
tended version of the the Yamamoto and Church al-
gorithm to generate all ngrams from a text and avoid-
ing the combinatorial explosion by grouping these n-
grams into equivalence classes. This approach, how-
ever, has the disadvantage of generating also ”uninter-
esting” ngrams, which are neither maximal nor super-
maximal. To overcome this deficiency, we employ the
Burrows and Wheeler transformation table.
</bodyText>
<page confidence="0.99607">
16
</page>
<figureCaption confidence="0.971932">
Fig. 1: kNN results for neighborhood sizes 5 and 10
and semantic spaces built from phrases (p) and bags of
words (w) using 5, 100 and 200 singular values
</figureCaption>
<subsectionHeader confidence="0.860568">
Permutations results
</subsectionHeader>
<bodyText confidence="0.99995655">
Each phrase has been counted in one of 4 ways: all
instances, left-maximal, right-maximal and maximal.
To avoid an unmanageable level of sparseness we in-
clude in the analysis all instances of all phrases that oc-
curs at least one time as maximal. Phrases are sorted
by their scores absolute values.
We then built a 19730 phrases to 504 chat texts ma-
trix that contains the frequency of occurrence of each
phrase in each texts. We then weighted the matrix
using the tf − idf weighting scheme. We then gener-
ate three LSA semantics spaces by reducing the SVD
resulting matrix singular values to 50, 100, and 200
respectively.
In addition we created another set of 3 bag of words
based LSA semantic spaces using the same weighting
scheme and respective number singular values. In this
case using a 6320 tokens to 504 chat texts matrix. The
number of token used is the results of choosing the
tokens that occurs at least two times within the chat
texts collection.
</bodyText>
<subsectionHeader confidence="0.875574">
5.2 k Nearest Neighbor algorithm
</subsectionHeader>
<bodyText confidence="0.9901844">
based classification
The k Nearest Neighbors algorithm (kNN) [5] is a
learning algorithm that classifies texts on the basis
of a measure of distance (e.g. cosine) between them.
The algorithm classifies each text by looking at a k
number of its nearest neighbors and then assigning it
to the most common category represented by those
neighbors. If no class is associated to a majority of
neighbors, the text is assigned to the category rep-
resented by texts with higher cosine similarity. We
arbitrarily used a low k value (i.e. k=5) as we ex-
pect that noise from the semantic space will be re-
duced by means of LSA. A common criticism of kNN
is that, since it doesn’t make any generalizations from
the data, it is prone to overfitting. We assume that
this criticism should not apply completely to semantic
spaces generated by means of LSA as the SVD dimen-
sional reduction smoothes and therefore reduces the
effect of over fitting that is usually present in kNN
based classification.
</bodyText>
<subsubsectionHeader confidence="0.804983">
5.2.1 kNN results
</subsubsectionHeader>
<bodyText confidence="0.999958">
Experimental results showed in Figure 1 demonstrate
that for some topics and grades using maximal phrases
as units of analysis can improve the performance of
LSA. In fact the best results for two of the four grades
(e.g. excellent and poor) were yielded by semantic
spaces build from phrases. Different results are pro-
duced by kNN classification for topics where semantic
spaces built from bags of words produced the best re-
sults clearly for at least four of the six topics.
</bodyText>
<figure confidence="0.999599451612903">
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
exCellent
good
fair
Poor
a
b
C
d
e
f
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.0 0.2 0.4 0.6 0.8 1.0
Probability
0 100 200 300 400 500 5.3 Prototyping
</figure>
<figureCaption confidence="0.9767305">
Chat texts We run the permutation test described in section 4 us-
Fig. 2: Permutation test results for each chat text ing the already available semantic space built from a
19730 phrases to 504 chat texts matrix and singular
values reduced to 200. We then evaluate the signifi-
cance of each text for its annotated grade and topic
category. Figure 2 shows that the majority of texts
</figureCaption>
<page confidence="0.992968">
17
</page>
<bodyText confidence="0.9968025">
have low probability as the majority of them may not
be a good representative of their class.
</bodyText>
<sectionHeader confidence="0.999367" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999871">
For particular grades and topics phrase based LSA (i.e.
using semantic spaces built from phrases occurring at
least one time as maximal) appears to improve over
LSA results that have been obtained with the tradi-
tional bags of words approach. These results are en-
couraging and therefore we plan to test alternative se-
mantic space configurations in particular using more
distinctive phrases (e.g. all maximal, left maximal and
right maximal ). We expect that as we collect a larger
text sample we will be able to afford the use of those
phrases without facing unmanageable levels of sparse-
ness in detrimental of results already obtained. In ad-
dition, we want to stress the fact that the suffix array
analysis presented in this paper is independent from
its LSA application as it can be used to characterize
learners and experts use of language. In addition, as
our approach to categorization is based on similarity to
prototypical texts for each class we presented here an
non parametric test (permutation test) for identifying
those prototypes.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999957714285714">
[1] M. I. Abouelhoda, S. Kurtz, and E. Ohlebusch. Replacing suffix
trees with enhanced suffix arrays. J. of Discrete Algorithms,
2(1):53–86, 2004.
[2] H. ˚Ahlfeldt, L. Borin, P. Daumke, N. Grabar, C. Hal-
lett, D. Hardcastle, D. Kokkinakis, C. Mancini, K. Mark´o,
M. Merkel, C. Pietsch, R. Power, D. Scott, A. Silvervarg,
M. T. Gronostaj, S. Williams, and A. Willis. Literature review
on patient-friendly documentation systems. Technical Report
2006/04, Centre for Research in Computing, The Open Uni-
versity, Milton Keynes, UK, May 2006. ISSN: 1744-1986.
[3] R. Bekkerman and J. Allan. Using bigrams in text categoriza-
tion. Technical Report IR-408, Center of Intelligent Informa-
tion Retrieval, UMass Amherst, 2004.
[4] J. S. Brown and P. Duguid. Knowledge and organization: A
social-practice perspective. Organization Science, 12(2):198–
213, March 2001.
[5] T. Cover and P. Hart. Nearest neighbor pattern classifica-
tion. Information Theory, IEEE Transactions on, 13(1):21–
27, 1967.
[6] A. Davies and C. Elder, editors. The Handbook of Applied Lin-
guistics. Blackwell Handbooks in Linguistics. Wiley-Blackwell,
Malden, 2006.
[7] D. K. Kim, M. Kim, and H. Park. Linearized suffix tree: an
efficient index data structure with the capabilities of suffix trees
and suffix arrays. Algorithmica, 52(3):350–377, 2008.
[8] T. K. Landauer and S. T. Dumais. A solution to plato’s prob-
lem: The latent semantic analysis theory of acquisition, induc-
tion, and representation of knowledge. Psychological Review,
104(2):211–240, April 1997.
[9] G. Salton and C. Buckley. Term-weighting approaches in au-
tomatic text retrieval. In Information Processing and Man-
agement, pages 513–523, 1988.
[10] J. van Bruggen, P. Sloep, P. van Rosmalen, F. Brouns,
H. Vogten, R. Koper, and C. Tattersall. Latent semantic anal-
ysis as a tool for learner positioning in learning networks for
lifelong learning. In British Journal of Educational Technol-
ogy, number 6, pages 729–738, 2004.
[11] L. Wasserman. All of Statistics: A Concise Course in Statis-
tical Inference. Springer, September 2004.
[12] M. Yamamoto and K. W. Church. Using suffix arrays to com-
pute term frequency and document frequency for all substrings
in a corpus. Comput. Linguist., 27(1):1–30, 2001.
</reference>
<page confidence="0.999293">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.733534">
<title confidence="0.999534">Maximal Phrases Based Analysis for Prototyping Online Discussion Forums Postings</title>
<author confidence="0.999682">Gaston Burek Dale</author>
<affiliation confidence="0.885341">Department of Tuebingen</affiliation>
<address confidence="0.904105">72074 Tuebingen,</address>
<abstract confidence="0.997137692307692">Chat texts produced in an educational environment are categorized and rated for the purpose of positioning (or placement) of the learner with respect to a learning program (appropriate courses, textbooks, etc). The difficulty lies in the fact that the texts are short and informal. A standard LSA/vector-space model is therefore combined with techniques appropriate for short texts. The approach uses phrases rather than words in the term-document matrix, and for determining prototypical documents of each category, a nonparametric permutation test is used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M I Abouelhoda</author>
<author>S Kurtz</author>
<author>E Ohlebusch</author>
</authors>
<title>Replacing suffix trees with enhanced suffix arrays.</title>
<date>2004</date>
<journal>J. of Discrete Algorithms,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="20165" citStr="[1]" startWordPosition="3325" endWordPosition="3325">an individually for all the members of the class. Since the time of Yamamoto &amp; Church’s paper, suffix arrays have been an active area of research, primerily in bioinformatics. One of the weaknesses of the suffix array approach used by Yamamoto &amp; Church is that extensions to the left are difficult to discover. So it is difficult to discover, for example, that jumbo always combines to the left to form the phrase mumbo jumbo. Simply stated, the problem is that suffixes are extensions of phrases to the right, so it is hard to look to the left. This problem was solved, however, by Abouelhoda et al [1], who added a BurrowsWheeler transform table to their extended suffix array data structure, giving this this data structure properties of suffix trees. One weakness of Abouelhoda et al’s approach, however, is that it does not adapt well to large alphabets. This is, of course, a serious weakness for use in text processing, where one wants at least to work with some subset of Unicode, or even worse, to treat each tokenized word as an alphabet symbol. Fortunately, the restriction to small alphabet size has recently been eliminated in the approach of Kim et al [7], who deal with the large alphabet</context>
<context position="21408" citStr="[1,1]" startWordPosition="3552" endWordPosition="3552">arly encoded using the child table of Abouelhoda et al along with a longest common prefix table (lcp). Using extended suffix arrays makes it possible to count different kinds of occurrences of phrases in different ways. To begin with, we are only interested in counting phrases that repeat. In the text S = to be or not to be, the occurrence of the phrase to be at S[1, 2] is said to be a repeat since the same sequence of tokens occurs at S[5,6].2 An occurrence of a phrase S[i, j] is left maximal is left maximal if the longer phrase S[i−1, j] is not a repeat. Thus, for example, the phrase to at S[1,1] is left maximal since the phrase at S[0,1] is not a repeat.3 Similarly, an occurrence of a phrase at S[i, j] is right maximal if S[i, j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non-repeats are rarely of interest, we generally assume that we are talking about repeats unless otherwise stated. A phrase is also said to be maximal in a text if there exists a maximal occurrence of the phrase in the text. For example</context>
<context position="23443" citStr="[1]" startWordPosition="3919" endWordPosition="3919"> time. We can simply test each of these four cases for distinctness, and chose the most distinct case. Take, for example the word side, which is a maximal phrase in our texts. Should we count all instances of this phrase? Or should we perhaps restrict the count to right maximal occurrences so as to avoid counting those instances that are extended to the right to create the longer phrase side effect? Or maybe left maximal occurrences to avoid the longer phrase flip side? Or perhaps we should restrict 2 This definition and the following definitions are similar to those found in Abouelhoda et al [1]. The difference is that Abouelhoda et al apply the terms to a pair of occurrences, whereas we apply the terms to a single occurrence. 3 We assume here that the text is padded with unique beginning of string and end of string sentinels so that indexing at 0 or 7 makes sense. Xi = I 15 in both directions to avoid either kind of extension. Since it is not generally possible to predict which is best, the reasonable approach is to try all possibilities to see what works best. One counterintuitive feature of our approach is that it also makes sense to count 0-grams. A left maximal occurrence of a 0</context>
</contexts>
<marker>[1]</marker>
<rawString>M. I. Abouelhoda, S. Kurtz, and E. Ohlebusch. Replacing suffix trees with enhanced suffix arrays. J. of Discrete Algorithms, 2(1):53–86, 2004.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H ˚Ahlfeldt</author>
<author>L Borin</author>
<author>P Daumke</author>
<author>N Grabar</author>
<author>C Hallett</author>
<author>D Hardcastle</author>
<author>D Kokkinakis</author>
<author>C Mancini</author>
<author>K Mark´o</author>
<author>M Merkel</author>
<author>C Pietsch</author>
<author>R Power</author>
<author>D Scott</author>
<author>A Silvervarg</author>
<author>M T Gronostaj</author>
<author>S Williams</author>
<author>A Willis</author>
</authors>
<title>Literature review on patient-friendly documentation systems.</title>
<date>2006</date>
<tech>Technical Report 2006/04,</tech>
<pages>1744--1986</pages>
<institution>Centre for Research in Computing, The Open University,</institution>
<location>Milton Keynes, UK,</location>
<contexts>
<context position="6088" citStr="[2]" startWordPosition="984" endWordPosition="984"> and reliability. In this contexts expertise is defined as knowledge beyond common sense [6]. Communities of practice are at the center of expert knowledge development. According to [4] new communities of practice develop conditions for effective local creation and communication of knowledge and therefore that knowledge is not shared with other communities until it settles. In linguistic terms this can be understood as a distinctive use of language shared among individuals members of a specific community to describe knowledge that is not shared with other communities of practice. According to [2] expert language in medicine consists in shared specific terms formalized in medical term banks, thesauri and lexicons. Patients get familiar with that established medical terminology relevant to their own health conditions medical language after exposure to treatments, personal doctor visits, etc. 1.3.1 Language technologies supporting the identification of expertise As life long learners do not have common learning goals nor common educational backgrounds as it is the case in traditional learning settings, long life learning educational providers need to rely on available written materials p</context>
<context position="21175" citStr="[1, 2]" startWordPosition="3505" endWordPosition="3506">treat each tokenized word as an alphabet symbol. Fortunately, the restriction to small alphabet size has recently been eliminated in the approach of Kim et al [7], who deal with the large alphabet by using binary trees, which are linearly encoded using the child table of Abouelhoda et al along with a longest common prefix table (lcp). Using extended suffix arrays makes it possible to count different kinds of occurrences of phrases in different ways. To begin with, we are only interested in counting phrases that repeat. In the text S = to be or not to be, the occurrence of the phrase to be at S[1, 2] is said to be a repeat since the same sequence of tokens occurs at S[5,6].2 An occurrence of a phrase S[i, j] is left maximal is left maximal if the longer phrase S[i−1, j] is not a repeat. Thus, for example, the phrase to at S[1,1] is left maximal since the phrase at S[0,1] is not a repeat.3 Similarly, an occurrence of a phrase at S[i, j] is right maximal if S[i, j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non</context>
</contexts>
<marker>[2]</marker>
<rawString>H. ˚Ahlfeldt, L. Borin, P. Daumke, N. Grabar, C. Hallett, D. Hardcastle, D. Kokkinakis, C. Mancini, K. Mark´o, M. Merkel, C. Pietsch, R. Power, D. Scott, A. Silvervarg, M. T. Gronostaj, S. Williams, and A. Willis. Literature review on patient-friendly documentation systems. Technical Report 2006/04, Centre for Research in Computing, The Open University, Milton Keynes, UK, May 2006. ISSN: 1744-1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bekkerman</author>
<author>J Allan</author>
</authors>
<title>Using bigrams in text categorization.</title>
<date>2004</date>
<tech>Technical Report IR-408,</tech>
<institution>Center of Intelligent</institution>
<location>UMass Amherst,</location>
<contexts>
<context position="13916" citStr="[3]" startWordPosition="2221" endWordPosition="2221">s in a comparatively lower number of concepts. As the variation of word choice, introduce noise to the text, SVD, the algorithm behind LSA bridges the gaps in words by conflating word used into word senses . Furthermore, LSA facilitates matrix manipulation in terms of hardware operative memory by reducing the dimensionality of the VSM. 3 Phrases Traditionally, text categorization by means of LSA has relied on a bag-of-words model. It seems, in some sense, obvious that a model based on phrases should be better. But it turns out that this is not necessarily the case. Recently, Bekkerman &amp; Allan [3] reviewed the literature on text categorization and found no general improvement when unigram models were replaced with bigram models. The problem is that using bigrams contributes heavily to the data sparseness problem. Bekkerman &amp; Allan have, however, compared two rather extreme positions. Our idea is to extract phrases of any length from the the training corpus, as long as the phrases are distinctive (occurring predominately in particular categories of documents). It may well be that the most distinctive phrases are generally phrases of length one (concurring with the bag-ofwords model), bu</context>
<context position="21726" citStr="[3,4]" startWordPosition="3617" endWordPosition="3617">t to be, the occurrence of the phrase to be at S[1, 2] is said to be a repeat since the same sequence of tokens occurs at S[5,6].2 An occurrence of a phrase S[i, j] is left maximal is left maximal if the longer phrase S[i−1, j] is not a repeat. Thus, for example, the phrase to at S[1,1] is left maximal since the phrase at S[0,1] is not a repeat.3 Similarly, an occurrence of a phrase at S[i, j] is right maximal if S[i, j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non-repeats are rarely of interest, we generally assume that we are talking about repeats unless otherwise stated. A phrase is also said to be maximal in a text if there exists a maximal occurrence of the phrase in the text. For example, in the text mining engineering, tokenized by characters, the phrase in is maximal since there are maximal occurrences at S[2,3] and S[11,12]. But the longer phrase ing is also maximal since it occurs maximally at S[4,6] and S[16,18]. So the occurrence of in at S[16,17] is a non-maximal occurrence of a maximal phras</context>
</contexts>
<marker>[3]</marker>
<rawString>R. Bekkerman and J. Allan. Using bigrams in text categorization. Technical Report IR-408, Center of Intelligent Information Retrieval, UMass Amherst, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Brown</author>
<author>P Duguid</author>
</authors>
<title>Knowledge and organization: A social-practice perspective.</title>
<date>2001</date>
<journal>Organization Science,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>213</pages>
<contexts>
<context position="5670" citStr="[4]" startWordPosition="918" endWordPosition="918">most similar to t and “poor” should be least similar. In subsection 4, we discuss a method for choosing good prototypes that uses the nonparametric permutation test. 1.3 Justification: communities of practice distinctive language It is widely accepted that experts can provide answers to problems that average people can not e.g. evidence given in court can be accepted or rejected on the basis of expertise, relevance and reliability. In this contexts expertise is defined as knowledge beyond common sense [6]. Communities of practice are at the center of expert knowledge development. According to [4] new communities of practice develop conditions for effective local creation and communication of knowledge and therefore that knowledge is not shared with other communities until it settles. In linguistic terms this can be understood as a distinctive use of language shared among individuals members of a specific community to describe knowledge that is not shared with other communities of practice. According to [2] expert language in medicine consists in shared specific terms formalized in medical term banks, thesauri and lexicons. Patients get familiar with that established medical terminolog</context>
<context position="21726" citStr="[3,4]" startWordPosition="3617" endWordPosition="3617">t to be, the occurrence of the phrase to be at S[1, 2] is said to be a repeat since the same sequence of tokens occurs at S[5,6].2 An occurrence of a phrase S[i, j] is left maximal is left maximal if the longer phrase S[i−1, j] is not a repeat. Thus, for example, the phrase to at S[1,1] is left maximal since the phrase at S[0,1] is not a repeat.3 Similarly, an occurrence of a phrase at S[i, j] is right maximal if S[i, j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non-repeats are rarely of interest, we generally assume that we are talking about repeats unless otherwise stated. A phrase is also said to be maximal in a text if there exists a maximal occurrence of the phrase in the text. For example, in the text mining engineering, tokenized by characters, the phrase in is maximal since there are maximal occurrences at S[2,3] and S[11,12]. But the longer phrase ing is also maximal since it occurs maximally at S[4,6] and S[16,18]. So the occurrence of in at S[16,17] is a non-maximal occurrence of a maximal phras</context>
</contexts>
<marker>[4]</marker>
<rawString>J. S. Brown and P. Duguid. Knowledge and organization: A social-practice perspective. Organization Science, 12(2):198– 213, March 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>P Hart</author>
</authors>
<title>Nearest neighbor pattern classification. Information Theory,</title>
<date>1967</date>
<journal>IEEE Transactions on,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="21249" citStr="[5,6]" startWordPosition="3521" endWordPosition="3521">on to small alphabet size has recently been eliminated in the approach of Kim et al [7], who deal with the large alphabet by using binary trees, which are linearly encoded using the child table of Abouelhoda et al along with a longest common prefix table (lcp). Using extended suffix arrays makes it possible to count different kinds of occurrences of phrases in different ways. To begin with, we are only interested in counting phrases that repeat. In the text S = to be or not to be, the occurrence of the phrase to be at S[1, 2] is said to be a repeat since the same sequence of tokens occurs at S[5,6].2 An occurrence of a phrase S[i, j] is left maximal is left maximal if the longer phrase S[i−1, j] is not a repeat. Thus, for example, the phrase to at S[1,1] is left maximal since the phrase at S[0,1] is not a repeat.3 Similarly, an occurrence of a phrase at S[i, j] is right maximal if S[i, j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non-repeats are rarely of interest, we generally assume that we are talking a</context>
<context position="30635" citStr="[5]" startWordPosition="5185" endWordPosition="5185"> using the tf − idf weighting scheme. We then generate three LSA semantics spaces by reducing the SVD resulting matrix singular values to 50, 100, and 200 respectively. In addition we created another set of 3 bag of words based LSA semantic spaces using the same weighting scheme and respective number singular values. In this case using a 6320 tokens to 504 chat texts matrix. The number of token used is the results of choosing the tokens that occurs at least two times within the chat texts collection. 5.2 k Nearest Neighbor algorithm based classification The k Nearest Neighbors algorithm (kNN) [5] is a learning algorithm that classifies texts on the basis of a measure of distance (e.g. cosine) between them. The algorithm classifies each text by looking at a k number of its nearest neighbors and then assigning it to the most common category represented by those neighbors. If no class is associated to a majority of neighbors, the text is assigned to the category represented by texts with higher cosine similarity. We arbitrarily used a low k value (i.e. k=5) as we expect that noise from the semantic space will be reduced by means of LSA. A common criticism of kNN is that, since it doesn’t</context>
</contexts>
<marker>[5]</marker>
<rawString>T. Cover and P. Hart. Nearest neighbor pattern classification. Information Theory, IEEE Transactions on, 13(1):21– 27, 1967.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>The Handbook of Applied Linguistics. Blackwell Handbooks in Linguistics. Wiley-Blackwell,</booktitle>
<editor>A. Davies and C. Elder, editors.</editor>
<location>Malden,</location>
<contexts>
<context position="5577" citStr="[6]" startWordPosition="904" endWordPosition="904">milarity to t. If t is a good prototype, then this ranking should other “excellent” texts as most similar to t and “poor” should be least similar. In subsection 4, we discuss a method for choosing good prototypes that uses the nonparametric permutation test. 1.3 Justification: communities of practice distinctive language It is widely accepted that experts can provide answers to problems that average people can not e.g. evidence given in court can be accepted or rejected on the basis of expertise, relevance and reliability. In this contexts expertise is defined as knowledge beyond common sense [6]. Communities of practice are at the center of expert knowledge development. According to [4] new communities of practice develop conditions for effective local creation and communication of knowledge and therefore that knowledge is not shared with other communities until it settles. In linguistic terms this can be understood as a distinctive use of language shared among individuals members of a specific community to describe knowledge that is not shared with other communities of practice. According to [2] expert language in medicine consists in shared specific terms formalized in medical term</context>
<context position="21249" citStr="[5,6]" startWordPosition="3521" endWordPosition="3521">on to small alphabet size has recently been eliminated in the approach of Kim et al [7], who deal with the large alphabet by using binary trees, which are linearly encoded using the child table of Abouelhoda et al along with a longest common prefix table (lcp). Using extended suffix arrays makes it possible to count different kinds of occurrences of phrases in different ways. To begin with, we are only interested in counting phrases that repeat. In the text S = to be or not to be, the occurrence of the phrase to be at S[1, 2] is said to be a repeat since the same sequence of tokens occurs at S[5,6].2 An occurrence of a phrase S[i, j] is left maximal is left maximal if the longer phrase S[i−1, j] is not a repeat. Thus, for example, the phrase to at S[1,1] is left maximal since the phrase at S[0,1] is not a repeat.3 Similarly, an occurrence of a phrase at S[i, j] is right maximal if S[i, j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non-repeats are rarely of interest, we generally assume that we are talking a</context>
</contexts>
<marker>[6]</marker>
<rawString>A. Davies and C. Elder, editors. The Handbook of Applied Linguistics. Blackwell Handbooks in Linguistics. Wiley-Blackwell, Malden, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Kim</author>
<author>M Kim</author>
<author>H Park</author>
</authors>
<title>Linearized suffix tree: an efficient index data structure with the capabilities of suffix trees and suffix arrays.</title>
<date>2008</date>
<journal>Algorithmica,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="20731" citStr="[7]" startWordPosition="3423" endWordPosition="3423">ved, however, by Abouelhoda et al [1], who added a BurrowsWheeler transform table to their extended suffix array data structure, giving this this data structure properties of suffix trees. One weakness of Abouelhoda et al’s approach, however, is that it does not adapt well to large alphabets. This is, of course, a serious weakness for use in text processing, where one wants at least to work with some subset of Unicode, or even worse, to treat each tokenized word as an alphabet symbol. Fortunately, the restriction to small alphabet size has recently been eliminated in the approach of Kim et al [7], who deal with the large alphabet by using binary trees, which are linearly encoded using the child table of Abouelhoda et al along with a longest common prefix table (lcp). Using extended suffix arrays makes it possible to count different kinds of occurrences of phrases in different ways. To begin with, we are only interested in counting phrases that repeat. In the text S = to be or not to be, the occurrence of the phrase to be at S[1, 2] is said to be a repeat since the same sequence of tokens occurs at S[5,6].2 An occurrence of a phrase S[i, j] is left maximal is left maximal if the longer</context>
</contexts>
<marker>[7]</marker>
<rawString>D. K. Kim, M. Kim, and H. Park. Linearized suffix tree: an efficient index data structure with the capabilities of suffix trees and suffix arrays. Algorithmica, 52(3):350–377, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="2629" citStr="[8]" startWordPosition="400" endWordPosition="400">ttle information from the text as possible. Traditional approaches to categorization lose information by case normalization, stemming and ignoring word order. The idea of the traditional approach is to deal with the data 12 sparseness problem by collapsing textual features into equivalence classes, losing information in the process. In our approach, we attempt to balance the problem of data sparseness with the goal of not losing information. This balance is obtained in two ways. First, as discussed in section 2, we use Latent Semantic Analysis (LSA) as a technique for dimensionality reduction [8]. It is well known that LSA can be used to discover weighted clusters of words, which are loosely understood to be “concepts.” Since these clusters can contain derivationally related terms, the need for stemming (and also case normalization) is reduced. Second, as discussed in 3, our more innovative contribution is to flexibly use bigrams, trigrams and other ngrams as opposed to strictly unigrams in the traditional bag-ofwords model. Our approach to extracting such ngrams is to use a extension of the suffix array approach of [12]. 1.2 Suitability Suppose that learner texts could be accurately </context>
</contexts>
<marker>[8]</marker>
<rawString>T. K. Landauer and S. T. Dumais. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240, April 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>In Information Processing and Management,</booktitle>
<pages>513--523</pages>
<contexts>
<context position="15807" citStr="[9]" startWordPosition="2531" endWordPosition="2531">raises some interesting counting issues. But first we need to specify more precisely what it means for a phrase to be distinctive. 3.1 Distinctiveness In general, phrases that are evenly distributed across document categories are not very distinctive, whereas phrases that tend to cluster in one particular category are distinctive. This general principle must be applied carefully, however, since with small numbers, clustering may occur due to chance. A common measure of distinctiveness used for weighting in vector space models is tf-idf (term frequency multiplied by inverse document frequency) [9]. It is unclear, however, that this is the best measure for picking out which phrases to consider and which phrases to ignore. It is problematic, for example, that idf simply prefers terms that cluster in a small number of documents, regardless of the classifications. Given the ordinal classification of our data as excellent, good, fair and poor, we are not interested, for example, in terms that cluster in the excellent and poor texts. So a distinctive term should be one that occurs predominately in excellent and good texts or predominately in fair and poor texts. Consider, for example, the bu</context>
</contexts>
<marker>[9]</marker>
<rawString>G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. In Information Processing and Management, pages 513–523, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J van Bruggen</author>
<author>P Sloep</author>
<author>P van Rosmalen</author>
<author>F Brouns</author>
<author>H Vogten</author>
<author>R Koper</author>
<author>C Tattersall</author>
</authors>
<title>Latent semantic analysis as a tool for learner positioning in learning networks for lifelong learning.</title>
<date>2004</date>
<journal>In British Journal of Educational Technology, number</journal>
<volume>6</volume>
<pages>729--738</pages>
<contexts>
<context position="9663" citStr="[10]" startWordPosition="1555" endWordPosition="1555">r uses the term ”drug charts,” this usage could be tagged as less favored terminology, and a tentative suggestion could possibly be made that the learner could take a pharmacology course. If the learner ultimately rejects this suggestion, then at least this learner wouldn’t go home empty handed. The learner would at least have received some linguistic feedback that would be unlikely to come from a human evaluator. 2 Latent Semantic Analysis In recent years, LSA has been proposed as a suitable language technology for the automatic determining the degree of expertise of a specific text’s author [10]. Although, singular value decomposition (SVD) of word co-occurrence frequencies matrices has been successfully used in the context of language technologies enhanced learning (e.g. automatic assessment of student essays), learner positioning presents new challenges that expose the limitation of such an approach. In particular, learners produce text repositories containing few samples of text, many of them of small size and using language that is rich in non domain-specific expressions. Such repositories may also be generated by individuals from different backgrounds in informal learning enviro</context>
</contexts>
<marker>[10]</marker>
<rawString>J. van Bruggen, P. Sloep, P. van Rosmalen, F. Brouns, H. Vogten, R. Koper, and C. Tattersall. Latent semantic analysis as a tool for learner positioning in learning networks for lifelong learning. In British Journal of Educational Technology, number 6, pages 729–738, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wasserman</author>
</authors>
<title>All of Statistics: A Concise Course in Statistical Inference.</title>
<date>2004</date>
<publisher>Springer,</publisher>
<contexts>
<context position="22150" citStr="[11,12]" startWordPosition="3690" endWordPosition="3690">j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non-repeats are rarely of interest, we generally assume that we are talking about repeats unless otherwise stated. A phrase is also said to be maximal in a text if there exists a maximal occurrence of the phrase in the text. For example, in the text mining engineering, tokenized by characters, the phrase in is maximal since there are maximal occurrences at S[2,3] and S[11,12]. But the longer phrase ing is also maximal since it occurs maximally at S[4,6] and S[16,18]. So the occurrence of in at S[16,17] is a non-maximal occurrence of a maximal phrase. A maximal repeated phrase that is not a subsequence of a longer maximal repeated phrase is said to be supermaximal. Thus the phrase ing is supermaximal in this text. Generally, we are only interested in counting occurrences of maximal phrases since a phrase that never occurs maximally is unlikely to be of interest. But what kind of occurrences should we count? Should we count all occurrences, or only the left maximal,</context>
<context position="25285" citStr="[11]" startWordPosition="4241" endWordPosition="4241">od prototypes in a way that is suitable for use with a small data set. We note that each candidate prototype induces a ranking of the remaining documents from ”most similar” to ”least similar.” For a good prototype, this ranking should be significantly different from a random permutation. To test this hypothesis, we can use a standard nonparametric test, known (appropriately enough) as the ”permutation test.” The permutation test is a nonparametric method for testing whether two distributions are the same. The test is ”exact,” meaning that it is not based on large sample theory approximations [11]. Suppose that X1, ..., Xm ∼ FX and Y1, ..., Yn ∼ FY are two independent samples and H0 is the hypothesis that the two samples are identically distributed. This is the type of hypothesis we would consider when testing whether a treatment differs from a placebo. More precisely we are testing H0 : FX = FY versus H1 : FX =6 FY . Let T(X1...,Xm,Y1,...,Yn) =|Xn − Y n |and let N = m + n and consider forming all N! permutations of the data X1..., Xm, Y1, ..., Yn. For each permutation, compute the test statistic T. Denote these values by T1,..., TNI. Under the null hypothesis, each of these values is </context>
</contexts>
<marker>[11]</marker>
<rawString>L. Wasserman. All of Statistics: A Concise Course in Statistical Inference. Springer, September 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yamamoto</author>
<author>K W Church</author>
</authors>
<title>Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus.</title>
<date>2001</date>
<journal>Comput. Linguist.,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="3164" citStr="[12]" startWordPosition="488" endWordPosition="488">mantic Analysis (LSA) as a technique for dimensionality reduction [8]. It is well known that LSA can be used to discover weighted clusters of words, which are loosely understood to be “concepts.” Since these clusters can contain derivationally related terms, the need for stemming (and also case normalization) is reduced. Second, as discussed in 3, our more innovative contribution is to flexibly use bigrams, trigrams and other ngrams as opposed to strictly unigrams in the traditional bag-ofwords model. Our approach to extracting such ngrams is to use a extension of the suffix array approach of [12]. 1.2 Suitability Suppose that learner texts could be accurately classified as similar or not similar to the gold standard text (or set of texts). Then the question arises as to whether or not the gold standard text is a suitable prototype for a good learner text. One approach to choosing a gold standard text would be to use a published journal article in the field. But such a text is unlikely to be similar to learner texts either in tone or in content. It is well known that effective teachers use scaffolding to present material within the zone of proximal development of the learner. So perhap</context>
<context position="19009" citStr="[12]" startWordPosition="3108" endWordPosition="3108">r higher score. And for a low score, the opposite idea is to count the proportion of randomly generated scores that are equal or lower. 3.2 Phrase Extraction In principle, the distinctness measure given above can be used with phrases of any length. If longer phrases can be found that are more distinct than single words, then there is no reason not to use the longer phrase. The problem is that the simulation-based distinctness test is very expensive, and it is certainly not possible to run this test for ngrams of every length in a text. The solution to this problem comes from Yamamoto &amp; Church [12], who show suffix arrays can be used to put the large number of ngrams into a much smaller number of equivalence classes. Using suffix arrays, it is very easy to pick out just the phrases that are repeated n times for some n, and it is very easy to extend phrases to the right: if mumbo jumbo repeatedly occurs together as a phrase, then it makes no sense to count mumbo by itself. Yamamoto &amp; Church’s suffix array program will put these two phrases into an equivalence class, so that that statistics can be calculated for the class as a whole rather than individually for all the members of the clas</context>
<context position="22150" citStr="[11,12]" startWordPosition="3690" endWordPosition="3690">j + 1] is not a repeat. If an occurrence of a phrase is both left and right maximal, then the occurrence is said to be maximal. Note that the occurrence of the phrase or not at S[3,4] is maximal, though it is not a repeat. Since non-repeats are rarely of interest, we generally assume that we are talking about repeats unless otherwise stated. A phrase is also said to be maximal in a text if there exists a maximal occurrence of the phrase in the text. For example, in the text mining engineering, tokenized by characters, the phrase in is maximal since there are maximal occurrences at S[2,3] and S[11,12]. But the longer phrase ing is also maximal since it occurs maximally at S[4,6] and S[16,18]. So the occurrence of in at S[16,17] is a non-maximal occurrence of a maximal phrase. A maximal repeated phrase that is not a subsequence of a longer maximal repeated phrase is said to be supermaximal. Thus the phrase ing is supermaximal in this text. Generally, we are only interested in counting occurrences of maximal phrases since a phrase that never occurs maximally is unlikely to be of interest. But what kind of occurrences should we count? Should we count all occurrences, or only the left maximal,</context>
</contexts>
<marker>[12]</marker>
<rawString>M. Yamamoto and K. W. Church. Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus. Comput. Linguist., 27(1):1–30, 2001.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>