<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000071">
<title confidence="0.995665">
Toward Multimedia: A String Pattern-based Passage Ranking Model for
Video Question Answering
</title>
<author confidence="0.998756">
Yu-Chieh Wu Jie-Chi Yang
</author>
<affiliation confidence="0.9933525">
Dept. of Computer Science and Infor- Graduate Institute of Network
mation Engineering Learning Technology
National Central University National Central University
Taoyuan, Taiwan Taoyuan, Taiwan
</affiliation>
<email confidence="0.989318">
bcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw
</email>
<sectionHeader confidence="0.997284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960142857143">
In this paper, we present a new string pat-
tern matching-based passage ranking al-
gorithm for extending traditional text-
based QA toward videoQA. Users interact
with our videoQA system through natural
language questions, while our system re-
turns passage fragments with correspond-
ing video clips as answers. We collect
75.6 hours videos and 253 Chinese ques-
tions for evaluation. The experimental re-
sults showed that our method
outperformed six top-performed ranking
models. It is 10.16% better than the sec-
ond best method (language model) in rela-
tively MRR score and 6.12% in precision
rate. Besides, we also show that the use of
a trained Chinese word segmentation tool
did decrease the overall videoQA per-
formance where most ranking algorithms
dropped at least 10% in relatively MRR,
precision, and answer pattern recall rates.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971733333334">
With the drastic growth of video sources, effective
indexing and retrieving video contents has recently
been addressed. The well-known Informedia pro-
ject (Wactlar, 2000) and TREC-VID track (Over et
al., 2005) are the two famous examples. Although
text-based question answering (QA) has become a
key research issue in past decade, to support mul-
timedia such as video, it is still beginning.
Over the past five years, several video QA stud-
ies had investigated. Lin et al. (2001) presented an
earlier work on combining videoOCR and term
weighting models. Yang et al. (2003) proposed a
complex videoQA approach by employing abun-
dant external knowledge such as, Web, WordNet,
shallow parsers, named entity taggers, and human-
made rules. They adopted the term-weighting
method (Pasca, and Harabagiu, 2001) to rank the
video segments by weighting the pre-defined key-
words. Cao and Nunamaker (2004) developed a
lexical pattern matching-based ranking method for
a domain-specific videoQA. In the same year, Wu
et al. (2004) designed a cross-language (English-
to-Chinese) video question answering system
based on extracting pre-defined named entity
words in captions. On the other hand, Zhang and
Nunamaker (2004) made use of the simple TFIDF
term weighting schema to retrieve the manual-
segmented clips for video caption word retrieval.
They also manually developed the ontology to im-
prove system performance.
In this paper, we present a new string pattern
matching-based passage ranking algorithm for
video question answering. We consider that the
passage is able to answer questions and also suit-
able for videos because itself forms a very natural
unit. Lin et al. (2003) showed that users prefer pas-
sage-level answers over short answer phrases since
it contains rich context information. Our method
makes use of the string pattern searching in the
suffix trees to find common subsequences between
a passage and question. The proposed term weight-
ing schema is then designed to compute passage
score. In addition, to avoid generating over-length
subsequence, we also present two algorithms for
re-tokenization and weighting.
</bodyText>
<sectionHeader confidence="0.988335" genericHeader="method">
2 The Framework of our VideoQA System
</sectionHeader>
<bodyText confidence="0.999906">
An overview of the proposed videoQA system can
be shown in Figure 1. The video processing com-
ponent recognizes the input video as an OCR docu-
ment at the first stage. Second, each three
consecutive sentences were grouped into a passage.
We tokenized the Chinese words with three
grained sizes: unigram, bigram, and trigram. Simi-
larly, the input question is also tokenized to uni-
</bodyText>
<page confidence="0.954648">
540
</page>
<note confidence="0.799001">
Proceedings of NAACL HLT 2007, pages 540–547,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998053">
gram, bigram, and trigram level of words. To re-
duce most irrelevant passages, we adopted the BM-
25 ranking model (Robertson et al., 2000) to re-
trieve top-1000 passages as the “input passages”.
Finally, the proposed passage ranking algorithm
retrieved top-N passages as answers in response to
the question. In the following parts, we briefly in-
troduce the employed videoOCR approach. Section
2.2 presents the sentence and passage segmentation
schemes. The proposed ranking algorithms will be
described in Section 3.
were considered as containing the same text com-
ponents. We then merge the two frames by averag-
ing the gray-intensity for each pixel in the same
text component. For the binarization stage, we em-
ploy the Lyu’s text extraction algorithm (Lyu et al.,
2005) to binarize text pixels for the text compo-
nents. Unlike previous approaches (Lin et al., 2001;
Chang et al., 2005), this method does not need to
assume the text is in either bright or dark color (but
assume the text color is stable). At the end of this
step, the output text components are prepared for
OCR.
The target of OCR is to identify the binarized
text image to the ASCII text. In this paper, we de-
veloped a naïve OCR system based on nearest
neighbor classification algorithms and clustering
techniques (Chang et al., 2005). We also adopted
the word re-ranking methods (Lin et al., 2001,
strategy 3) to improve the OCR errors.
</bodyText>
<figureCaption confidence="0.303479">
Figure1: System Architecture of the proposed
videoQA system
</figureCaption>
<subsectionHeader confidence="0.998107">
2.1 Video Processing
</subsectionHeader>
<bodyText confidence="0.999973958333333">
Our video processing takes a video and recognizes
the closed captions as texts. An example of the
input and output associated with the whole video
processing component can be seen in Figure 2. The
videoOCR technique consists of four important
steps: text detection, binarization, frame tracking,
and OCR. The goal of text detection is to locate the
text area precisely. In this paper, we employ the
edge-based filtering (Lyu et al., 2005) and slightly
modify the coarse-to-fine top-down block segmen-
tation methods (Lienhart and Wernicke, 2002) to
find each text component in a frame. The former
removes most non-edge areas with global and local
thresholding strategy (Fan et al., 2001) while the
latter incrementally segments and refines text
blocks using horizontal and vertical projection pro-
files.
The next steps are text binarization and frame
tracking. As we know, the main constituent of
video is a sequence of image frames. A text com-
ponent almost appears more than once. To remove
redundancy, we count the proportion of overlap-
ping edge pixels between two consecutive frames.
If the portion is above 70%, then the two frames
</bodyText>
<figureCaption confidence="0.854237">
Figure 2: Text extraction results of an input image
</figureCaption>
<subsectionHeader confidence="0.997394">
2.2 Sentence and Passage Segmentation
</subsectionHeader>
<bodyText confidence="0.999951826086956">
In this paper, we treat all words appear in the same
frame as a sentence and group every three consecu-
tive sentences as a passage. Usually, words that
occur in the same frame provide a sufficient and
complete description. We thus consider these
words as a sentence unit for sentence segmentation.
An example of a sentence can be found in Figure 2.
The sentence of this frame is the cascading of the
two text lines, i.e. “speed-up to 17.5 thousand
miles per hour in less than six minutes” For each
OCR document we grouped every three continuous
sentences with one previous sentence overlapping
to represent a passage. Subsequently, we tokenized
Chinese word with unigram, bigram, and trigram
levels.
Searching answers in the whole video collection
is impractical since most of them are irrelevant to
the question. By means of text retrieval technology,
the search space can be largely reduced and limited
in a small set of relevant document. The document
retrieval methods have been developed well and
successfully been applied for retrieving relevant
passages for question answering (Tellex et al.,
</bodyText>
<page confidence="0.99577">
541
</page>
<bodyText confidence="0.9999612">
2003). We replicated the Okapi BM-25 (Robertson
et al., 2000), which is the effective and efficient
retrieval algorithms to find the related segmented
passages. For each input question, the top-1000
relevant passages are input to our ranking model.
</bodyText>
<sectionHeader confidence="0.990587" genericHeader="method">
3 The Algorithm
</sectionHeader>
<bodyText confidence="0.999904959183673">
Tellex et al. (2003) compared seven passage re-
trieval models for text QA except for several ad-
hoc approaches that needed either human-
generated patterns or inference ontology which
were not available. In their experiments, they
showed that the density-based methods (Lee et al.,
2001) achieved the best results, while the BM-25
(Robertson, 2000) reached slightly worse retrieval
result than the density-based approaches, which
adopted named entity taggers, thesaurus, and
WordNet. Cui et al. (2005) showed that their fuzzy
relation syntactic matching method outperformed
the density-based methods. But the limitation is
that it required a dependency parser, thesaurus, and
training data. In many Asian languages like Chi-
nese, Japanese, parsing is more difficult since it is
necessary to resolve the word segmentation prob-
lem before part-of-speech (POS) tagging, and pars-
ing (Fung et al., 2004). This does not only make
the parsing task harder but also required to train a
high-performance word segmentor. The situation is
even worse when text contains a number of OCR
error words. In addition, to develop a thesaurus and
labeled training set for QA is far time-consuming.
In comparison to Cui’s method, the term weight-
ing-based retrieval models are much less cost,
portable and more practical. Furthermore, the OCR
document is not like traditional text articles that
have been human-typed well where some words
were error predicted, unrecognizable, and false-
alarm. These unexpected words deeply affect the
performance of Chinese word segmentation, and
further for parsing. In our experiments (see Table 2
and Table 3), we also showed that the use of a
well-trained high-performance Chinese word seg-
mentation tool gave the worse result than using the
unigram-level of Chinese word (13.95% and
13.92% relative precision and recall rates dropped
for language model method).
To alleviate this problem, we treat the atomic
Chinese unigram as word and present a weighted
string pattern matching algorithm. Our solution is
to integrate the suffix tree for finding, and encod-
ing important subsequence information in trees.
Nevertheless, it is known that the suffix tree con-
struction and pattern searching can be accom-
plished in linear time (Ukkonen, 1995). Before
introducing our method, we give the following no-
tations.
</bodyText>
<equation confidence="0.91603">
passage P = PW1, PW2, ..., PWT
question Q = QW1, QW2, ..., QWT’
a common subsequence for passage
Sub = PW , PW + ,..., PW + − if  |Sub|
P P
i k k 1 k x 1 i
a common subsequence for question
Sub =QW , QW + ,...,QW +− if  |Sub |=
Q Q j y
j l l 1 l y 1
</equation>
<bodyText confidence="0.9992636875">
A common subsequence represents a continuous
string matching between P and Q. We further im-
pose two symbols on a subsequence. For example,
SubiP means i-th matched continuous string (com-
mon subsequence) in the passage, while SubjQ in-
dicates the j-th matched continuous string in the
question. The common subsequences can be ex-
tracted through the suffix tree building and pattern
searching. For example, to extract the set of SubiP,
we firstly build the suffix tree of P and incremen-
tally insert substring of Q and label the matched
common string between P and Q. Similarly, one
can apply a similar approach to generate the set of
SubjQ. By extracting all subsequences for P and Q,
we then compute the following score (see equation
(1)) to rank passages.
</bodyText>
<equation confidence="0.942199">
= ×
λ QW_Density (Q, P)+ (1)
λ)QW_Weight( Q, P)
×
</equation>
<bodyText confidence="0.99982225">
The first term of equation (1) “QW_Density(Q,
P)” estimates the question word density degree in
the passage P, while “QW_Weight(Q, P)” meas-
ures the matched question word weights in P. λ is a
parameter, which is used to adjust the importance
of the QW_Density(Q, P). Both the two estima-
tions make use of the subsequence information for
P and Q. In the following parts, we introduce the
computation of QW_Density(Q,P) and
QW_Weight(Q, P) separately. The time complex-
ity analysis of our method is then discussed in the
tail of this section.
The QW_Density(Q, P) is designed for quantify-
ing “how dense the matched question words in the
passage P”. It also takes the term weight into ac-
count. By means of extracting common subse-
quence in the question, the set of SubjQ can be used
to measures the question word density. At the be-
ginning, we define equation (2) for weighting a
subsequence SubjQ.
</bodyText>
<equation confidence="0.6353168">
x
=
Passage_Score(P)
-
( 1
</equation>
<page confidence="0.749001">
542
</page>
<bodyText confidence="0.633349">
Weight(Sub ) length (Sub ) DP(Sub )
</bodyText>
<equation confidence="0.837779333333333">
Q Q α × Q
1
j = j (2)
j
Where length(SubjQ) is merely the length of Q
Sub j
</equation>
<bodyText confidence="0.999918833333334">
i.e., the number of words in SubjQ. α1 is a parameter
that controls the weight of length for SubjQ. In this
paper, we consider the long subsequence match is
useful. A long N-gram is usually much less am-
biguous than its individual unigram. The second
term in equation (2) estimates the “discriminative
power” (DP) of the subsequence. Some high-
frequent and common words should be given less
weight. To measure the DP score, we extend the
BM-25 (Robertson et al., 2000) term weighting
schema. Equation (3), (4), and (5) list our DP scor-
ing functions.
</bodyText>
<equation confidence="0.999278545454545">
( 1) TF(Sub , P)
Q
k + × ( 1) TF(Sub , Q)
Q
k (3)
j 3 + ×
DP(Sub ) &apos;
Q j
1
= ×
W ×j TF(Sub , P)
Q TF(Sub , Q)
Q
K+ k
j 3+ j
Q
NP − PF (Sub ) 0.5
+ (4)
j
log( )
 |P  |(5)
K = (1 − b ) + b ×
</equation>
<bodyText confidence="0.9713095">
k1 , b, k3 are constants, which empirically set as 1.2,
0.75, 500 respectively (Robertson et al., 2000).
TF(SubQ , Q) and TF(SubQ, P)
j represent the term
j
frequency of SubjQ in question Q and passage P.
Equation (4) computes the inverse “passage fre-
quency” (PF) of SubjQ as against to the traditional
inverse “document frequency” (DF) where Np is
the total number of passages. The collected Dis-
covery video is a small but “long” OCR document
set, which results the estimation of DF value unre-
liable. On the contrary, a passage is more coherent
than a long document, thus we replace the DF es-
timation with PF score. It is worth to note that
some SubjQ might be too long to be further re-
tokenized into finer grained size. We therefore
propose two algorithms to 1): re-tokenize an input
subsequence, and 2): compute the DP score for a
subsequence. Figure 3, and Figure 4 list the pro-
posed two algorithms.
The proposed algorithm 1, and 2 can be used to
compute and tokenize the DP score of not only
SubjQ for question but also SubjP for passage. As
seeing in Figure 4, it requires DP information for
different length of N-gram. As noted in Section 2.2,
the unigram, bigram, and trigram level of words
had been stored in indexed files for efficient re-
trieving and computing DP score at this step. By
applying algorithm 1 for the set of SubjQ, we can
obtain all retokenized subsequences (TSubj). We
then use the re-tokenized subsequences to compute
the final density score. Equation (6) lists the
QW_Density scoring function.
min_distance_between (TSub
T_CNT is the total number of retokenized subse-
quences in Q, which can be extracted through ap-
plying algorithm 1 for all SubjQ. Equation (7)
merely counts the minimum number of words be-
tween two neighboring TSubi, and TSubi+1 in the
passage. α2 is the parameter that controls the im-
pact of distance measurement.
</bodyText>
<table confidence="0.982494613636364">
Algorithm 1: Retokenizing_a_subsequence
Input:
A subsequence SubjQ where startj is the position of first word in
question and endj is the position of last word in question
Output:
A set of retokenized subsequence {TSub1, TSub2, }
Nt: the number of retokenized subsequence
Algorithm:
Initially, we set Nt := 1; TSub1:=QWstartj;
if (SubjQ≠ψ)
{ /*** from the start to the end positions in the string ***/
for ( k := startj+1 to endj)
{
/***Check the two question words is bigram in the passage***/
if (bigram(QWk-1,QWk) is_found_in_passage)
add QWk into TSubNt;
Otherwise
{ Nt ++;
TSubNt : = QWk ;
} /*** End otherwise***/
} /*** End for ***/
} /*** End if ***/
else
Nt := 0;
Figure 3: An algorithm for retokenizing subsequence
Algorithm 2: Copmuting_DP_score
Input:
A subsequence SubjQ where startj is the position of first word
of SubjQ in question endj is the position of last word of SubjQ in
question
Output:
The score of DP(SubjQ)
Algorithm:
head := startj;
tail := endj;
Max_score := 0;
for (k := head ~ tail)
{ let WORD := QWk, QWk+1,..., QWtail;
/*** look-up WORD in the index files ***/
compute DP(WORD) using equation (3);
if (DP(WORD) &gt; Max_score)
Max_score := DP(WORD);
} /*** End for ***/
DP(WORD) := Max_score;
</table>
<figureCaption confidence="0.936006">
Figure 4: An algorithm for computing DP score for a
subsequence
</figureCaption>
<equation confidence="0.9749065">
dist
(
(7)
TSub , TSub ) =
i i+1
QW_Density (Q, P)
T
= ∑dist(TSub ,TSub
i = 1 i i+1
CNT−1
Weight(TSub) Weight(TSub )
+
i i+1
)
α 2
(6)
W &apos;=
PF
(
Subj ) + 0.5
Q
(|P |)
AVG
,TSub )_in_P 1
+
i i + 1
</equation>
<page confidence="0.988136">
543
</page>
<bodyText confidence="0.999877727272727">
The density scoring can be thought as measuring
“how much information the passage preserves in
response to the question”. On the contrary, the
QW_Weight (second term in equation (1)) aims to
estimate “how much content information the pas-
sage has given the question”. To achieve this, we
further take the other extracted common subse-
quences, i.e., SubjP into account. By means of the
same term weighting schema for the set of SubjP,
the QW_Weight is then produced. Equation (8)
gives the overall QW_Weight measurement.
</bodyText>
<equation confidence="0.9936062">
SCNT
P) Weight(Sub )
P
= ∑ =
i
i =
SCNT
_
∑
i =
</equation>
<bodyText confidence="0.999958636363636">
where the DP score of the input subsequence can
be obtained via the algorithm 2 (Figure 5). S_CNT
is the number of subsequence in P. The parameter
α1 is also set as equal as equation (2).
In addition, the neighboring contexts of a sen-
tence, which contains high QW_Density score
might include the answers. Hence, we stress on
either head or tail fragments of the passage. In
other words, the passage score is determined by
computing equation (1) for head and tail parts of
passage. We thus extend equation (1) as follows.
</bodyText>
<equation confidence="0.987011333333333">
Passage_Sc ore(P) max{ QW_Density (Q, P ) ( 1 - )QW_Weight( Q, P ),
= λ × + λ ×
1 1
</equation>
<bodyText confidence="0.995708826086957">
Instead of estimating the whole passage, the two
divided parts: P1, and P2 are used. We select the
maximum passage score from either head (P1) or
tail (P2) part. When the passage contains only one
sentence, then this sentence is indispensable to be
used for estimation.
Now we turn to analyze the time complexity of
our algorithm. It is known that the suffix tree con-
struction costs is linear time (assume it requires
O(T), T: the passage length for passage and O(T’),
T’: the question length for question). Assume the
search time for a pattern in the suffix trees is at
most O(hlogm) where h is the tree height, and m is
the number of branch nodes. To generate the sets
of SubjQ and SubjP, it involves in building suffix
trees and incrementally searching substrings, i.e.,
O((T+T’)+(T+T’)(hlogm)). Intuitively, both algo-
rithm 1, and algorithm 2 are linear time algorithms,
which depends on the length of “common” subse-
quence, i.e., at most O(min(T, T’)). Consequently,
the overall time complexity of our method for
computing a passage is O((T+T’)(1+hlogm)+
min(T, T’)).
</bodyText>
<sectionHeader confidence="0.999369" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.926949">
4.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999976744186047">
We should carefully select the use of videoQA col-
lection for evaluation. Unfortunately, there is no
benchmark corpus for this task. Thus, we develop
an annotated collection by following the similar
tasks as TREC, CLEF, and NTCIR. The Discovery
videos are one of the popular raw video sources
and widely evaluated in many literatures (Lin et al.,
2001; Wu et al., 2004; Lee et al., 2005). Totally,
75.6 hours of Discovery videos (93 video names)
were used. Table 1 lists the statistics of the Dis-
covery films.
The questions were created in two different
ways: one set (about 73) was collected from previ-
ous studies (Lin et al., 2001; Wu et al., 2004)
which came from the “Project: Assignment of Dis-
covery”; while the other was derived from a real
log from users. Video collections are difficult to be
general-purpose since hundreds hours of videos
might take tens of hundreds GB storage space.
Therefore, general questions are quite difficult to
be found in the video database. Hence, we provide
a list of short introductions collected from the
cover-page of the videos and enable users to
browse the descriptions. Users were then asked for
the system with limited to the collected video top-
ics. We finally filter the (1) keyword-like queries
(2) non-Chinese and (3) un-supported questions.
Finally, there were 253 questions for evaluation.
For the answer assessment, we followed the
TREC-QA track (Voorhees, 2001) and NTCIR to
annotate answers in the pool that collected from
the outputs of different passage retrieval methods.
Unlike traditional text QA task, most of the OCR
sentences contain a number of OCR error words.
Furthermore, some sentence did include the answer
string but error recognized as different words. Thus,
instead of annotating the recognized transcripts, we
used the corresponding video frames for evaluation
because users can directly find the answers in the
retrieved video clips and recognized text. Among
253 questions, 56 of which did not have an answer,
while 368 passage&amp;frame segments (i.e., answer
patterns) in the pool were labeled as answers. On
</bodyText>
<figure confidence="0.949996754716981">
)QW_Weight(Q, P ) }
× 2
:
λ × QW_Density(Q, P2)+ ( 1- λ
=
=
+
+
,
if P has 3 sentences
and
,
P1
S2
S 2
S2
P 2
S1
S1
S3
, S then
3
:
else if P has 2 sentences
,
⎧
⎪⎨
⎪⎩
then, P S and P
= =
1 1 2
S 2
S2
S 1
n, P P S
= =
1 2 1
else if P has 1 sentence
: S the
1
QW_Weight(Q,
(8)
1
P
P
α1
i
) × DP(Sub
)) i
(length
Sub
(
1
</figure>
<page confidence="0.996515">
544
</page>
<bodyText confidence="0.999488">
averagely, there are 1.45 labeled answers for each
question.
The MRR (Voorhees, 2001) score, precision and
pattern-recall are used for evaluation. We measure
the MRR scores for both top1 and top5 ranks, and
precision and pattern-recall rates for top5 retrieved
answers.
</bodyText>
<tableCaption confidence="0.999632">
Table 1: Statistics of the collected Discovery videos
</tableCaption>
<table confidence="0.998730333333333">
# of videos # of sentence # of words # of passages
93 49950 746276 25001
AVG # of AVG # of AVG # of AVG # of words
words per words per sentences per video
sentence passage per passage
14.94 48.78 537.09 8024.47
</table>
<sectionHeader confidence="0.606535" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.978628282051282">
In this paper, we employed six top-performed yet
portable ranking models, TFIDF, BM-25 (Robert-
son et al., 2000), INQUERY, language model
(Zhai and Lafferty, 2001), cosine, and density-
based (Lee et al., 2001) approaches for compari-
son1. For the language model, the Jelinek-Mercer
smoothing method was employed with the parame-
ter settings X=0.5 which was selected via several
trials. In our preliminary experiments, we found
that the query term expansion does not improve but
decrease the overall ranking performance for all
the ranking models. Thus, we only compare with
the “pure” retrieval performance without pseudo-
feedback.
The system performance was evaluated through
the returned passages. We set α1=1.25, α2= 0.25,
and X=0.8 which were observed via the following
parameter validations. More detail parameter ex-
periments are presented and discussed later. Table
2 lists the overall videoQA results with different
ranking models.
Among all ranking models, the proposed method
achieves the best system performance. Our ap-
proach produced 0.596 and 0.654 MRR scores
when evaluating the top1 and top5 passages and
the precision rate achieves 0.208. Compared to the
second best method (language model), our method
is 10.16% better in relatively percentage in terms
of MRR(top1) score. For the MRR(top5) score, our
method is 7.39 relative percentage better. In terms
of the non-answered questions, our method also
covers the most questions (253-69=184) compared
1 For the TFIDF/BM-25/INQUERY/Language Model approaches
were performed using the Lemur toolkit
to the other ranking models. Overall, the experi-
ment shows that the proposed weighted string pat-
tern matching algorithm outperforms the other six
methods in terms of MRR, non-answered question
numbers, precision and pattern recall rates.
</bodyText>
<tableCaption confidence="0.9884415">
Table 2: Overall videoQA performance with differ-
ent ranking models (using unigram Chinese word
</tableCaption>
<table confidence="0.9999672">
Word-Level MRR MRR (Top5) Precision Pattern
(Top1) Non-answered Recall
Questions
TFIDF 0.498 0.572 81 0.189 0.649
BM-25 0.501 0.581 78 0.186 0.638
Language Model 0.541 0.609 74 0.196 0.671
INQUERY 0.505 0.583 78 0.188 0.644
Cosine 0.418 0.489 102 0.151 0.519
Density 0.323 0.421 102 0.137 0.471
Our Method 0.596 0.654 69 0.208 0.711
</table>
<tableCaption confidence="0.989843">
Table 3: Overall videoQA performance with differ-
ent ranking models using word segmentation tools
</tableCaption>
<table confidence="0.9994192">
Word-Level MRR MRR (Top5) Precision Pattern
(Top1) Non-answered Recall
Questions
TFIDF 0.509 0.567 89 0.145 0.597
BM-25 0.438 0.500 104 0.159 0.543
Language Model 0.486 0.551 89 0.172 0.589
INQUERY 0.430 0.503 97 0.164 0.562
Cosine 0.403 0.480 100 0.158 0.548
Density 0.304 0.380 125 0.133 0.451
Our Method 0.509 0.561 89 0.181 0.608
</table>
<bodyText confidence="0.99997744">
Next, we evaluate the performance with adopt-
ing a trained Chinese word segmentation tool in-
stead of unigram level of word. In this paper, we
employed the Chinese word segmentation tool (Wu
et al., 2006) that achieved about 0.93-0.96 re-
call/precision rates in the SIGHAN-3 word seg-
mentation task (Levow, 2006). Table 3 lists the
overall experimental results with the adopted word
segmentation tool. In comparison to unigram
grained level (Table 2), it is shown that the use of
word segmentation tool does not improve the
videoQA result for most top-performed ranking
models, BM-25, language model, INQUERY, and
our method. For example, our method is relatively
17.92% and 16.57% worse in MRR(Top1) and
MRR(Top5) scores. In terms of precision and pat-
tern-recall rates, it drops 14.91, and 16.94 relative
percentages, respectively. For the TFIDF method,
the MRR score is almost the same as previous re-
sult whereas it decreased 30.34%, and 8.71% pre-
cision and pattern-recall rates. On averagely, the
four models, BM-25, language model, INQUERY,
and our method dropped at least relatively 10% in
MRR, precision, and pattern-recall rates. In this
experiment, our ranking algorithm also achieved
</bodyText>
<page confidence="0.997977">
545
</page>
<figureCaption confidence="0.999247666666667">
Figure 5: Experimental results with different
settings of parameter α1 using MRR evaluation
Figure 6: Verify parameter α2 with α1=1.25, and
</figureCaption>
<figure confidence="0.999454709090909">
variant X
MRR
0.65
0.55
0.45
0.35
0.25
0.7
0.6
0.5
0.4
0.3
0.2
0 0.5 1 1.5 2 2.5 3 3.5 4
α2
Lambda = 0.00
Lambda = 0.25
Lambda = 0.50
Lambda = 0.75
Lambda = 1.00
MRR
0.65
0.55
0.45
0.35
0.25
0.7
0.6
0.5
0.4
0.3
0.2
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 2.8 3
α2
Lambda = 0.00
Lambda = 0.25
Lambda = 0.50
Lambda = 0.75
Lambda = 1.00
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
α2
MRR
0.65
0.55
0.7
0.6
0.5
5 20 40 100 300 500 1500 3000 5000
TopN
0.71
0.7
MRR
0.69
0.68
0.67
</figure>
<figureCaption confidence="0.9886035">
Figure 7: Verify parameter X in the two vali-
dation sets with α1=1.25 and α2=0.25
</figureCaption>
<bodyText confidence="0.995599172413793">
the best results in terms of precision and pattern
recall rates while marginally worse than the TFIDF
for the MRR(top5) score.
There are three parameters: λ, α1, α2, in our rank-
ing algorithm. λ controls the weight of the
QW_Density(Q, P), while α1, and α2 were set for
the power of subsequence length and the distance
measurement. We randomly select 100 questions
for parameter validations. Firstly, we tried to verify
the optimal α1 via different settings of the remain-
ing two parameters. The best α1 is then set to verify
α2 via various λ values. The optimal λ is subse-
quently confirmed through the observed α1 and α2
values. Figure 5, 6, 7 show the performance
evaluations of different settings for the three pa-
rameters.
As shown in Figure 5, the optimal settings of
(α1=1.25) is obtained when and α2=0.25, and
λ=0.75. When α1 is set more than 1.5, our method
quickly decreased. In this experiment, we also
found that large α2 negatively affects the perform-
ance. The small α2 values often lead to better rank-
ing performance. Thus, in the next experiment, we
limited the α2 value in 0.0~3.0. As seeing in Figure
6, again the abnormal high or zero α2 values give
the poor results. This implies the over-weight and
no-weight on the distance measurement (equation
(7)) is not useful. Instead, a small α2 value yields to
improve the performance. In our experiment,
</bodyText>
<figureCaption confidence="0.7774825">
Figure 8: Experimental results with different
number of initial retrieval passages (TopN)
</figureCaption>
<bodyText confidence="0.998025428571429">
α2=0.25 is quite effective. Finally, in Figure 7, we
can see that both taking the QW_Density, and
QW_Weight into account gives better ranking re-
sult, especially QW_Density. This experiment in-
dicates that the combination of QW_Density and
QW_Weight is better than its individual term
weighting strategy. When λ=0.8, the best ranking
result (MRR = 0.700) is reached.
Next, we address on the impact of different
number of initial retrieved passages using BM-25
ranking models. Due to the length limitation of this
paper, we did not present the experiments over all
the compared ranking models, while we left the
further results at our web site2. For the three pa-
rameters, we select the optimal settings derived
from previous experimental results, i.e., λ=0.8,
α1=1.25, α2=0.25. Figure 8 shows the experimental
results with different number of initial retrieved
passages. When employing exactly five initial re-
trieved passages, it can be viewed as the re-ranking
improvement over the BM-25 ranking model. As
seeing in Figure 8, our method does improve the
conventional BM-25 ranking approach (MRR
score 0.690 v.s. 0.627) with relatively 10.04%
MRR value. The best system performance is
MRR=0.700 when there are merely 20 initial re-
trieved passages. The ranking result converges
when retrieving more than 40 passages. Besides,
</bodyText>
<footnote confidence="0.934281">
2 http://140.115.112.118/bcbb/TVQS2/
</footnote>
<page confidence="0.996553">
546
</page>
<bodyText confidence="0.9989182">
we also continue the experiments using only top-
20 retrieved passages on the actual 253 testing
questions. The ranking performance is then further
enhanced from MRR=0.654 to 0.663 with 1.37%
relatively improved.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99989">
More and more users are interested in searching for
answers in videos, while existing question answer-
ing systems do not support multimedia accessing.
This paper presents a weighted string pattern
matching-based passage ranking algorithm for ex-
tending text QA toward video question answering.
We compare our method with six top-performed
ranking models and show that our method outper-
forms the second best approach (language model)
in relatively 10.16 % MRR score, and 6.12% pre-
cision rates.
In the future, we plan to integrate the other use-
ful features in videos to support multi-model-based
multimedia question answering. The video-demo
version of our videoQA system can be found at the
web site (http://140.115.112.118/bcbb/TVQS2/).
</bodyText>
<sectionHeader confidence="0.99945" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999945161290323">
Cao, J., and Nunamaker J. F. Question answering on lecture
videos: a multifaceted approach, International Conference
on Digital Libraries, pages 214 – 215, 2004.
Chang, F., Chen, G. C., Lin, C. C., and Lin, W. H. Caption
analysis and recognition for building video indexing sys-
tems. Multimedia systems, 10: 344-355, 2005.
Cui, H., Sun, R., Li, K., Kan, M., and Chua, T. Question an-
swering passage retrieval using dependency relations. In
Proceedings of the 28th ACM SIGIR Conference on Re-
search and Development in Information Retrieval, pages
400-407, 2005.
Fan, J., Yau, D. K. Y., Elmagarmid, A. K., and Aref, W. G.
Automatic image segmentation by integrating color-edge
extraction and seeded region growing. IEEE Trans. On Im-
age Processing, 10(10): 1454-1464, 2001.
Fung, P., Ngai, G., Yuan, Y., and Chen, B. A maximum en-
tropy Chinese parser augmented by transformation-based
learning. ACM Trans. Asian Language Information Proc-
essing, 3: 159-168, 2004.
Lee et al. SiteQ: Engineering high performance QA system
using lexico-semantic pattern matching and shallow NLP.
In Proceedings of the 10th Text Retrieval Conference,
pages 437-446, 2001.
Lee, Y. S., Wu, Y. C., and Chang, C. H. Integrating Web in-
formation to generate Chinese video summaries. In Pro-
ceedings of 17th international conference on software
engineering and knowledge engineering (SEKE), pages
514-519, 2005.
Levow, G. A. The third international Chinese language proc-
essing Bakeoff: word segmentation and named entity rec-
ognition, In Proceedings of the 5th SIGHAN Workshop on
Chinese Language Processing, pages 108-117, 2006.
Lin, C. J., Liu, C. C., and Chen, H. H. A simple method for
Chinese videoOCR and its application to question answer-
ing. Journal of Computational linguistics and Chinese lan-
guage processing, 6: 11-30, 2001.
Lin, J., Quan, D., Sinha, V., Bakshi, K., Huynh, D., Katz, B.,
and Karger, D. R. What makes a good answer? the role of
context in question answering. In Proceedings of the 9th in-
ternational conference on human-computer interaction
(INTERACT), pages 25-32, 2003.
Lienhart, R. and Wernicke, A. Localizing and segmenting text
in images and videos. IEEE Trans. Circuits and Systems
for Video Technology, 12(4): 243-255, 2002.
Lyu, M. R., Song, J., and Cai, M. A comprehensive method
for multilingual video text detection, localization, and ex-
traction. IEEE Trans. Circuits and Systems for Video
Technology, 15(2): 243-255, 2005.
Over, P., Ianeva, T., Kraaij, W., and Smeaton, A. F.
TRECVID 2005 - an overview. In Proceedings of the 14th
text retrieval conference (TREC), 2005.
Pasca, M., and Harabagiu, S. High-performance question an-
swering. In Proceedings of the 24th ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 366-374, 2001.
Robertson, E., Walker, S., and Beaulieu, M. Experimentation
as a way of life: Okapi at TREC. Journal of Information
processing and management, 36: 95-108, 2000.
Tellex, S., Katz, B., Lin, J. J., Fernandes, A., and Marton, G.
Quantitative evaluation of passage retrieval algorithms for
question answering. In Proceedings of the 26th ACM
SIGIR Conference on Research and Development in In-
formation Retrieval, pages 41-47, 2003.
Voorhees, E. M. Overview of the TREC 2001 question an-
swering track. In Proceedings of the 10th Text Retrieval
Conference , pages 42-52, 2001.
Ukkonen, E. Constructing suffix trees on-line in linear time. In
Proceedings of the international federation of information
processing, pages 484-492, 1995.
Wactlar, H. D. Informedia search and summarization in the
video medium, In Proceedings of Imagina 2000 Confer-
ence, 2000.
Wu, Y. C., Lee, Y. S., Chang, C. H. CLVQ: Cross-language
video question/answering system. In Proceedings of 6th
IEEE International Symposium on Multimedia Software
Engineering, pages 294-301, 2004.
Wu, Y. C., Yang, J. C., and Lin, Q. X. Description of the NCU
Chinese Word Segmentation and Named Entity Recogni-
tion System for SIGHAN Bakeoff 2006. In Proceedings of
the 5th SIGHAN Workshop on Chinese Language Process-
ing, pages 209-212, 2006.
Yang, H., Chaison, L., Zhao, Y., Neo, S. Y., and Chua, T. S.
VideoQA: Question answering on news video. In Proceed-
ings of the 11th ACM International Conference on Multi-
media, pages 632-641, 2003.
Zhai, C., and Lafferty, J. A study of smoothing methods for
language models applied to ad hoc information retrieval, In
Proceedings of the 24th Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval (SIGIR), pages 334-342, 2001.
Zhang, D., and Nunamaker, J. A natural language approach to
content-based video indexing and retrieval for interactive
E-learning. IEEE Trans. on Multimedia, 6: 450-458, 2004.
</reference>
<page confidence="0.997656">
547
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770872">
<title confidence="0.9960195">Toward Multimedia: A String Pattern-based Passage Ranking Model Video Question Answering</title>
<author confidence="0.999115">Yu-Chieh Wu Jie-Chi Yang</author>
<affiliation confidence="0.996708666666667">Dept. of Computer Science and Infor- Graduate Institute of Network mation Engineering Learning Technology National Central University National Central University</affiliation>
<address confidence="0.987392">Taoyuan, Taiwan Taoyuan, Taiwan</address>
<email confidence="0.885448">bcbb@db.csie.ncu.edu.twyang@cl.ncu.edu.tw</email>
<abstract confidence="0.9946115">In this paper, we present a new string pattern matching-based passage ranking algorithm for extending traditional textbased QA toward videoQA. Users interact with our videoQA system through natural language questions, while our system returns passage fragments with corresponding video clips as answers. We collect 75.6 hours videos and 253 Chinese questions for evaluation. The experimental results showed that our method outperformed six top-performed ranking models. It is 10.16% better than the second best method (language model) in relatively MRR score and 6.12% in precision rate. Besides, we also show that the use of a trained Chinese word segmentation tool did decrease the overall videoQA performance where most ranking algorithms dropped at least 10% in relatively MRR, precision, and answer pattern recall rates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Cao</author>
<author>J F Nunamaker</author>
</authors>
<title>Question answering on lecture videos: a multifaceted approach,</title>
<date>2004</date>
<booktitle>International Conference on Digital Libraries,</booktitle>
<pages>214--215</pages>
<contexts>
<context position="2094" citStr="Cao and Nunamaker (2004)" startWordPosition="313" endWordPosition="316">n answering (QA) has become a key research issue in past decade, to support multimedia such as video, it is still beginning. Over the past five years, several video QA studies had investigated. Lin et al. (2001) presented an earlier work on combining videoOCR and term weighting models. Yang et al. (2003) proposed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules. They adopted the term-weighting method (Pasca, and Harabagiu, 2001) to rank the video segments by weighting the pre-defined keywords. Cao and Nunamaker (2004) developed a lexical pattern matching-based ranking method for a domain-specific videoQA. In the same year, Wu et al. (2004) designed a cross-language (Englishto-Chinese) video question answering system based on extracting pre-defined named entity words in captions. On the other hand, Zhang and Nunamaker (2004) made use of the simple TFIDF term weighting schema to retrieve the manualsegmented clips for video caption word retrieval. They also manually developed the ontology to improve system performance. In this paper, we present a new string pattern matching-based passage ranking algorithm for</context>
</contexts>
<marker>Cao, Nunamaker, 2004</marker>
<rawString>Cao, J., and Nunamaker J. F. Question answering on lecture videos: a multifaceted approach, International Conference on Digital Libraries, pages 214 – 215, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Chang</author>
<author>G C Chen</author>
<author>C C Lin</author>
<author>W H Lin</author>
</authors>
<title>Caption analysis and recognition for building video indexing systems.</title>
<date>2005</date>
<journal>Multimedia systems,</journal>
<volume>10</volume>
<pages>344--355</pages>
<contexts>
<context position="4741" citStr="Chang et al., 2005" startWordPosition="733" endWordPosition="736">es as answers in response to the question. In the following parts, we briefly introduce the employed videoOCR approach. Section 2.2 presents the sentence and passage segmentation schemes. The proposed ranking algorithms will be described in Section 3. were considered as containing the same text components. We then merge the two frames by averaging the gray-intensity for each pixel in the same text component. For the binarization stage, we employ the Lyu’s text extraction algorithm (Lyu et al., 2005) to binarize text pixels for the text components. Unlike previous approaches (Lin et al., 2001; Chang et al., 2005), this method does not need to assume the text is in either bright or dark color (but assume the text color is stable). At the end of this step, the output text components are prepared for OCR. The target of OCR is to identify the binarized text image to the ASCII text. In this paper, we developed a naïve OCR system based on nearest neighbor classification algorithms and clustering techniques (Chang et al., 2005). We also adopted the word re-ranking methods (Lin et al., 2001, strategy 3) to improve the OCR errors. Figure1: System Architecture of the proposed videoQA system 2.1 Video Processing</context>
</contexts>
<marker>Chang, Chen, Lin, Lin, 2005</marker>
<rawString>Chang, F., Chen, G. C., Lin, C. C., and Lin, W. H. Caption analysis and recognition for building video indexing systems. Multimedia systems, 10: 344-355, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>R Sun</author>
<author>K Li</author>
<author>M Kan</author>
<author>T Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>400--407</pages>
<contexts>
<context position="8430" citStr="Cui et al. (2005)" startWordPosition="1332" endWordPosition="1335">ented passages. For each input question, the top-1000 relevant passages are input to our ranking model. 3 The Algorithm Tellex et al. (2003) compared seven passage retrieval models for text QA except for several adhoc approaches that needed either humangenerated patterns or inference ontology which were not available. In their experiments, they showed that the density-based methods (Lee et al., 2001) achieved the best results, while the BM-25 (Robertson, 2000) reached slightly worse retrieval result than the density-based approaches, which adopted named entity taggers, thesaurus, and WordNet. Cui et al. (2005) showed that their fuzzy relation syntactic matching method outperformed the density-based methods. But the limitation is that it required a dependency parser, thesaurus, and training data. In many Asian languages like Chinese, Japanese, parsing is more difficult since it is necessary to resolve the word segmentation problem before part-of-speech (POS) tagging, and parsing (Fung et al., 2004). This does not only make the parsing task harder but also required to train a high-performance word segmentor. The situation is even worse when text contains a number of OCR error words. In addition, to d</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Cui, H., Sun, R., Li, K., Kan, M., and Chua, T. Question answering passage retrieval using dependency relations. In Proceedings of the 28th ACM SIGIR Conference on Research and Development in Information Retrieval, pages 400-407, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fan</author>
<author>D K Y Yau</author>
<author>A K Elmagarmid</author>
<author>W G Aref</author>
</authors>
<title>Automatic image segmentation by integrating color-edge extraction and seeded region growing.</title>
<date>2001</date>
<journal>IEEE Trans. On Image Processing,</journal>
<volume>10</volume>
<issue>10</issue>
<pages>1454--1464</pages>
<contexts>
<context position="6027" citStr="Fan et al., 2001" startWordPosition="947" endWordPosition="950">s as texts. An example of the input and output associated with the whole video processing component can be seen in Figure 2. The videoOCR technique consists of four important steps: text detection, binarization, frame tracking, and OCR. The goal of text detection is to locate the text area precisely. In this paper, we employ the edge-based filtering (Lyu et al., 2005) and slightly modify the coarse-to-fine top-down block segmentation methods (Lienhart and Wernicke, 2002) to find each text component in a frame. The former removes most non-edge areas with global and local thresholding strategy (Fan et al., 2001) while the latter incrementally segments and refines text blocks using horizontal and vertical projection profiles. The next steps are text binarization and frame tracking. As we know, the main constituent of video is a sequence of image frames. A text component almost appears more than once. To remove redundancy, we count the proportion of overlapping edge pixels between two consecutive frames. If the portion is above 70%, then the two frames Figure 2: Text extraction results of an input image 2.2 Sentence and Passage Segmentation In this paper, we treat all words appear in the same frame as </context>
</contexts>
<marker>Fan, Yau, Elmagarmid, Aref, 2001</marker>
<rawString>Fan, J., Yau, D. K. Y., Elmagarmid, A. K., and Aref, W. G. Automatic image segmentation by integrating color-edge extraction and seeded region growing. IEEE Trans. On Image Processing, 10(10): 1454-1464, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>G Ngai</author>
<author>Y Yuan</author>
<author>B Chen</author>
</authors>
<title>A maximum entropy Chinese parser augmented by transformation-based learning.</title>
<date>2004</date>
<journal>ACM Trans. Asian Language Information Processing,</journal>
<volume>3</volume>
<pages>159--168</pages>
<contexts>
<context position="8825" citStr="Fung et al., 2004" startWordPosition="1392" endWordPosition="1395">l., 2001) achieved the best results, while the BM-25 (Robertson, 2000) reached slightly worse retrieval result than the density-based approaches, which adopted named entity taggers, thesaurus, and WordNet. Cui et al. (2005) showed that their fuzzy relation syntactic matching method outperformed the density-based methods. But the limitation is that it required a dependency parser, thesaurus, and training data. In many Asian languages like Chinese, Japanese, parsing is more difficult since it is necessary to resolve the word segmentation problem before part-of-speech (POS) tagging, and parsing (Fung et al., 2004). This does not only make the parsing task harder but also required to train a high-performance word segmentor. The situation is even worse when text contains a number of OCR error words. In addition, to develop a thesaurus and labeled training set for QA is far time-consuming. In comparison to Cui’s method, the term weighting-based retrieval models are much less cost, portable and more practical. Furthermore, the OCR document is not like traditional text articles that have been human-typed well where some words were error predicted, unrecognizable, and falsealarm. These unexpected words deepl</context>
</contexts>
<marker>Fung, Ngai, Yuan, Chen, 2004</marker>
<rawString>Fung, P., Ngai, G., Yuan, Y., and Chen, B. A maximum entropy Chinese parser augmented by transformation-based learning. ACM Trans. Asian Language Information Processing, 3: 159-168, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee</author>
</authors>
<title>SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th Text Retrieval Conference,</booktitle>
<pages>437--446</pages>
<marker>Lee, 2001</marker>
<rawString>Lee et al. SiteQ: Engineering high performance QA system using lexico-semantic pattern matching and shallow NLP. In Proceedings of the 10th Text Retrieval Conference, pages 437-446, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Lee</author>
<author>Y C Wu</author>
<author>C H Chang</author>
</authors>
<title>Integrating Web information to generate Chinese video summaries.</title>
<date>2005</date>
<booktitle>In Proceedings of 17th international conference on software engineering and knowledge engineering (SEKE),</booktitle>
<pages>514--519</pages>
<contexts>
<context position="19070" citStr="Lee et al., 2005" startWordPosition="3235" endWordPosition="3238">hms, which depends on the length of “common” subsequence, i.e., at most O(min(T, T’)). Consequently, the overall time complexity of our method for computing a passage is O((T+T’)(1+hlogm)+ min(T, T’)). 4 Experiments 4.1 Evaluation We should carefully select the use of videoQA collection for evaluation. Unfortunately, there is no benchmark corpus for this task. Thus, we develop an annotated collection by following the similar tasks as TREC, CLEF, and NTCIR. The Discovery videos are one of the popular raw video sources and widely evaluated in many literatures (Lin et al., 2001; Wu et al., 2004; Lee et al., 2005). Totally, 75.6 hours of Discovery videos (93 video names) were used. Table 1 lists the statistics of the Discovery films. The questions were created in two different ways: one set (about 73) was collected from previous studies (Lin et al., 2001; Wu et al., 2004) which came from the “Project: Assignment of Discovery”; while the other was derived from a real log from users. Video collections are difficult to be general-purpose since hundreds hours of videos might take tens of hundreds GB storage space. Therefore, general questions are quite difficult to be found in the video database. Hence, we</context>
</contexts>
<marker>Lee, Wu, Chang, 2005</marker>
<rawString>Lee, Y. S., Wu, Y. C., and Chang, C. H. Integrating Web information to generate Chinese video summaries. In Proceedings of 17th international conference on software engineering and knowledge engineering (SEKE), pages 514-519, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Levow</author>
</authors>
<title>The third international Chinese language processing Bakeoff: word segmentation and named entity recognition,</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>108--117</pages>
<contexts>
<context position="24594" citStr="Levow, 2006" startWordPosition="4164" endWordPosition="4165">ision Pattern (Top1) Non-answered Recall Questions TFIDF 0.509 0.567 89 0.145 0.597 BM-25 0.438 0.500 104 0.159 0.543 Language Model 0.486 0.551 89 0.172 0.589 INQUERY 0.430 0.503 97 0.164 0.562 Cosine 0.403 0.480 100 0.158 0.548 Density 0.304 0.380 125 0.133 0.451 Our Method 0.509 0.561 89 0.181 0.608 Next, we evaluate the performance with adopting a trained Chinese word segmentation tool instead of unigram level of word. In this paper, we employed the Chinese word segmentation tool (Wu et al., 2006) that achieved about 0.93-0.96 recall/precision rates in the SIGHAN-3 word segmentation task (Levow, 2006). Table 3 lists the overall experimental results with the adopted word segmentation tool. In comparison to unigram grained level (Table 2), it is shown that the use of word segmentation tool does not improve the videoQA result for most top-performed ranking models, BM-25, language model, INQUERY, and our method. For example, our method is relatively 17.92% and 16.57% worse in MRR(Top1) and MRR(Top5) scores. In terms of precision and pattern-recall rates, it drops 14.91, and 16.94 relative percentages, respectively. For the TFIDF method, the MRR score is almost the same as previous result where</context>
</contexts>
<marker>Levow, 2006</marker>
<rawString>Levow, G. A. The third international Chinese language processing Bakeoff: word segmentation and named entity recognition, In Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing, pages 108-117, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Lin</author>
<author>C C Liu</author>
<author>H H Chen</author>
</authors>
<title>A simple method for Chinese videoOCR and its application to question answering.</title>
<date>2001</date>
<journal>Journal of Computational linguistics and Chinese language processing,</journal>
<volume>6</volume>
<pages>11--30</pages>
<contexts>
<context position="1681" citStr="Lin et al. (2001)" startWordPosition="250" endWordPosition="253">performance where most ranking algorithms dropped at least 10% in relatively MRR, precision, and answer pattern recall rates. 1 Introduction With the drastic growth of video sources, effective indexing and retrieving video contents has recently been addressed. The well-known Informedia project (Wactlar, 2000) and TREC-VID track (Over et al., 2005) are the two famous examples. Although text-based question answering (QA) has become a key research issue in past decade, to support multimedia such as video, it is still beginning. Over the past five years, several video QA studies had investigated. Lin et al. (2001) presented an earlier work on combining videoOCR and term weighting models. Yang et al. (2003) proposed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules. They adopted the term-weighting method (Pasca, and Harabagiu, 2001) to rank the video segments by weighting the pre-defined keywords. Cao and Nunamaker (2004) developed a lexical pattern matching-based ranking method for a domain-specific videoQA. In the same year, Wu et al. (2004) designed a cross-language (Englishto-Chinese) video question a</context>
<context position="4720" citStr="Lin et al., 2001" startWordPosition="729" endWordPosition="732">ieved top-N passages as answers in response to the question. In the following parts, we briefly introduce the employed videoOCR approach. Section 2.2 presents the sentence and passage segmentation schemes. The proposed ranking algorithms will be described in Section 3. were considered as containing the same text components. We then merge the two frames by averaging the gray-intensity for each pixel in the same text component. For the binarization stage, we employ the Lyu’s text extraction algorithm (Lyu et al., 2005) to binarize text pixels for the text components. Unlike previous approaches (Lin et al., 2001; Chang et al., 2005), this method does not need to assume the text is in either bright or dark color (but assume the text color is stable). At the end of this step, the output text components are prepared for OCR. The target of OCR is to identify the binarized text image to the ASCII text. In this paper, we developed a naïve OCR system based on nearest neighbor classification algorithms and clustering techniques (Chang et al., 2005). We also adopted the word re-ranking methods (Lin et al., 2001, strategy 3) to improve the OCR errors. Figure1: System Architecture of the proposed videoQA system</context>
<context position="19034" citStr="Lin et al., 2001" startWordPosition="3227" endWordPosition="3230">algorithm 2 are linear time algorithms, which depends on the length of “common” subsequence, i.e., at most O(min(T, T’)). Consequently, the overall time complexity of our method for computing a passage is O((T+T’)(1+hlogm)+ min(T, T’)). 4 Experiments 4.1 Evaluation We should carefully select the use of videoQA collection for evaluation. Unfortunately, there is no benchmark corpus for this task. Thus, we develop an annotated collection by following the similar tasks as TREC, CLEF, and NTCIR. The Discovery videos are one of the popular raw video sources and widely evaluated in many literatures (Lin et al., 2001; Wu et al., 2004; Lee et al., 2005). Totally, 75.6 hours of Discovery videos (93 video names) were used. Table 1 lists the statistics of the Discovery films. The questions were created in two different ways: one set (about 73) was collected from previous studies (Lin et al., 2001; Wu et al., 2004) which came from the “Project: Assignment of Discovery”; while the other was derived from a real log from users. Video collections are difficult to be general-purpose since hundreds hours of videos might take tens of hundreds GB storage space. Therefore, general questions are quite difficult to be fo</context>
</contexts>
<marker>Lin, Liu, Chen, 2001</marker>
<rawString>Lin, C. J., Liu, C. C., and Chen, H. H. A simple method for Chinese videoOCR and its application to question answering. Journal of Computational linguistics and Chinese language processing, 6: 11-30, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Quan</author>
<author>V Sinha</author>
<author>K Bakshi</author>
<author>D Huynh</author>
<author>B Katz</author>
<author>D R Karger</author>
</authors>
<title>What makes a good answer? the role of context in question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th international conference on human-computer interaction (INTERACT),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2866" citStr="Lin et al. (2003)" startWordPosition="433" endWordPosition="436">lishto-Chinese) video question answering system based on extracting pre-defined named entity words in captions. On the other hand, Zhang and Nunamaker (2004) made use of the simple TFIDF term weighting schema to retrieve the manualsegmented clips for video caption word retrieval. They also manually developed the ontology to improve system performance. In this paper, we present a new string pattern matching-based passage ranking algorithm for video question answering. We consider that the passage is able to answer questions and also suitable for videos because itself forms a very natural unit. Lin et al. (2003) showed that users prefer passage-level answers over short answer phrases since it contains rich context information. Our method makes use of the string pattern searching in the suffix trees to find common subsequences between a passage and question. The proposed term weighting schema is then designed to compute passage score. In addition, to avoid generating over-length subsequence, we also present two algorithms for re-tokenization and weighting. 2 The Framework of our VideoQA System An overview of the proposed videoQA system can be shown in Figure 1. The video processing component recognize</context>
</contexts>
<marker>Lin, Quan, Sinha, Bakshi, Huynh, Katz, Karger, 2003</marker>
<rawString>Lin, J., Quan, D., Sinha, V., Bakshi, K., Huynh, D., Katz, B., and Karger, D. R. What makes a good answer? the role of context in question answering. In Proceedings of the 9th international conference on human-computer interaction (INTERACT), pages 25-32, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lienhart</author>
<author>A Wernicke</author>
</authors>
<title>Localizing and segmenting text in images and videos.</title>
<date>2002</date>
<booktitle>IEEE Trans. Circuits and Systems for Video Technology,</booktitle>
<volume>12</volume>
<issue>4</issue>
<pages>243--255</pages>
<contexts>
<context position="5885" citStr="Lienhart and Wernicke, 2002" startWordPosition="923" endWordPosition="926">rs. Figure1: System Architecture of the proposed videoQA system 2.1 Video Processing Our video processing takes a video and recognizes the closed captions as texts. An example of the input and output associated with the whole video processing component can be seen in Figure 2. The videoOCR technique consists of four important steps: text detection, binarization, frame tracking, and OCR. The goal of text detection is to locate the text area precisely. In this paper, we employ the edge-based filtering (Lyu et al., 2005) and slightly modify the coarse-to-fine top-down block segmentation methods (Lienhart and Wernicke, 2002) to find each text component in a frame. The former removes most non-edge areas with global and local thresholding strategy (Fan et al., 2001) while the latter incrementally segments and refines text blocks using horizontal and vertical projection profiles. The next steps are text binarization and frame tracking. As we know, the main constituent of video is a sequence of image frames. A text component almost appears more than once. To remove redundancy, we count the proportion of overlapping edge pixels between two consecutive frames. If the portion is above 70%, then the two frames Figure 2: </context>
</contexts>
<marker>Lienhart, Wernicke, 2002</marker>
<rawString>Lienhart, R. and Wernicke, A. Localizing and segmenting text in images and videos. IEEE Trans. Circuits and Systems for Video Technology, 12(4): 243-255, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Lyu</author>
<author>J Song</author>
<author>M Cai</author>
</authors>
<title>A comprehensive method for multilingual video text detection, localization, and extraction.</title>
<date>2005</date>
<booktitle>IEEE Trans. Circuits and Systems for Video Technology,</booktitle>
<volume>15</volume>
<issue>2</issue>
<pages>243--255</pages>
<contexts>
<context position="4626" citStr="Lyu et al., 2005" startWordPosition="713" endWordPosition="716">top-1000 passages as the “input passages”. Finally, the proposed passage ranking algorithm retrieved top-N passages as answers in response to the question. In the following parts, we briefly introduce the employed videoOCR approach. Section 2.2 presents the sentence and passage segmentation schemes. The proposed ranking algorithms will be described in Section 3. were considered as containing the same text components. We then merge the two frames by averaging the gray-intensity for each pixel in the same text component. For the binarization stage, we employ the Lyu’s text extraction algorithm (Lyu et al., 2005) to binarize text pixels for the text components. Unlike previous approaches (Lin et al., 2001; Chang et al., 2005), this method does not need to assume the text is in either bright or dark color (but assume the text color is stable). At the end of this step, the output text components are prepared for OCR. The target of OCR is to identify the binarized text image to the ASCII text. In this paper, we developed a naïve OCR system based on nearest neighbor classification algorithms and clustering techniques (Chang et al., 2005). We also adopted the word re-ranking methods (Lin et al., 2001, stra</context>
</contexts>
<marker>Lyu, Song, Cai, 2005</marker>
<rawString>Lyu, M. R., Song, J., and Cai, M. A comprehensive method for multilingual video text detection, localization, and extraction. IEEE Trans. Circuits and Systems for Video Technology, 15(2): 243-255, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Over</author>
<author>T Ianeva</author>
<author>W Kraaij</author>
<author>A F Smeaton</author>
</authors>
<title>an overview.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th text retrieval conference (TREC),</booktitle>
<tech>TRECVID</tech>
<contexts>
<context position="1413" citStr="Over et al., 2005" startWordPosition="204" endWordPosition="207">rmed six top-performed ranking models. It is 10.16% better than the second best method (language model) in relatively MRR score and 6.12% in precision rate. Besides, we also show that the use of a trained Chinese word segmentation tool did decrease the overall videoQA performance where most ranking algorithms dropped at least 10% in relatively MRR, precision, and answer pattern recall rates. 1 Introduction With the drastic growth of video sources, effective indexing and retrieving video contents has recently been addressed. The well-known Informedia project (Wactlar, 2000) and TREC-VID track (Over et al., 2005) are the two famous examples. Although text-based question answering (QA) has become a key research issue in past decade, to support multimedia such as video, it is still beginning. Over the past five years, several video QA studies had investigated. Lin et al. (2001) presented an earlier work on combining videoOCR and term weighting models. Yang et al. (2003) proposed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules. They adopted the term-weighting method (Pasca, and Harabagiu, 2001) to rank t</context>
</contexts>
<marker>Over, Ianeva, Kraaij, Smeaton, 2005</marker>
<rawString>Over, P., Ianeva, T., Kraaij, W., and Smeaton, A. F. TRECVID 2005 - an overview. In Proceedings of the 14th text retrieval conference (TREC), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S Harabagiu</author>
</authors>
<title>High-performance question answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>366--374</pages>
<marker>Pasca, Harabagiu, 2001</marker>
<rawString>Pasca, M., and Harabagiu, S. High-performance question answering. In Proceedings of the 24th ACM SIGIR Conference on Research and Development in Information Retrieval, pages 366-374, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Robertson</author>
<author>S Walker</author>
<author>M Beaulieu</author>
</authors>
<title>Experimentation as a way of life: Okapi at TREC.</title>
<date>2000</date>
<journal>Journal of Information processing and management,</journal>
<volume>36</volume>
<pages>95--108</pages>
<contexts>
<context position="3996" citStr="Robertson et al., 2000" startWordPosition="613" endWordPosition="616">he proposed videoQA system can be shown in Figure 1. The video processing component recognizes the input video as an OCR document at the first stage. Second, each three consecutive sentences were grouped into a passage. We tokenized the Chinese words with three grained sizes: unigram, bigram, and trigram. Similarly, the input question is also tokenized to uni540 Proceedings of NAACL HLT 2007, pages 540–547, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics gram, bigram, and trigram level of words. To reduce most irrelevant passages, we adopted the BM25 ranking model (Robertson et al., 2000) to retrieve top-1000 passages as the “input passages”. Finally, the proposed passage ranking algorithm retrieved top-N passages as answers in response to the question. In the following parts, we briefly introduce the employed videoOCR approach. Section 2.2 presents the sentence and passage segmentation schemes. The proposed ranking algorithms will be described in Section 3. were considered as containing the same text components. We then merge the two frames by averaging the gray-intensity for each pixel in the same text component. For the binarization stage, we employ the Lyu’s text extractio</context>
<context position="7729" citStr="Robertson et al., 2000" startWordPosition="1226" endWordPosition="1229">us sentences with one previous sentence overlapping to represent a passage. Subsequently, we tokenized Chinese word with unigram, bigram, and trigram levels. Searching answers in the whole video collection is impractical since most of them are irrelevant to the question. By means of text retrieval technology, the search space can be largely reduced and limited in a small set of relevant document. The document retrieval methods have been developed well and successfully been applied for retrieving relevant passages for question answering (Tellex et al., 541 2003). We replicated the Okapi BM-25 (Robertson et al., 2000), which is the effective and efficient retrieval algorithms to find the related segmented passages. For each input question, the top-1000 relevant passages are input to our ranking model. 3 The Algorithm Tellex et al. (2003) compared seven passage retrieval models for text QA except for several adhoc approaches that needed either humangenerated patterns or inference ontology which were not available. In their experiments, they showed that the density-based methods (Lee et al., 2001) achieved the best results, while the BM-25 (Robertson, 2000) reached slightly worse retrieval result than the de</context>
<context position="12797" citStr="Robertson et al., 2000" startWordPosition="2092" endWordPosition="2095">quence SubjQ. x = Passage_Score(P) - ( 1 542 Weight(Sub ) length (Sub ) DP(Sub ) Q Q α × Q 1 j = j (2) j Where length(SubjQ) is merely the length of Q Sub j i.e., the number of words in SubjQ. α1 is a parameter that controls the weight of length for SubjQ. In this paper, we consider the long subsequence match is useful. A long N-gram is usually much less ambiguous than its individual unigram. The second term in equation (2) estimates the “discriminative power” (DP) of the subsequence. Some highfrequent and common words should be given less weight. To measure the DP score, we extend the BM-25 (Robertson et al., 2000) term weighting schema. Equation (3), (4), and (5) list our DP scoring functions. ( 1) TF(Sub , P) Q k + × ( 1) TF(Sub , Q) Q k (3) j 3 + × DP(Sub ) &apos; Q j 1 = × W ×j TF(Sub , P) Q TF(Sub , Q) Q K+ k j 3+ j Q NP − PF (Sub ) 0.5 + (4) j log( ) |P |(5) K = (1 − b ) + b × k1 , b, k3 are constants, which empirically set as 1.2, 0.75, 500 respectively (Robertson et al., 2000). TF(SubQ , Q) and TF(SubQ, P) j represent the term j frequency of SubjQ in question Q and passage P. Equation (4) computes the inverse “passage frequency” (PF) of SubjQ as against to the traditional inverse “document frequency”</context>
<context position="21745" citStr="Robertson et al., 2000" startWordPosition="3722" endWordPosition="3726">nswers for each question. The MRR (Voorhees, 2001) score, precision and pattern-recall are used for evaluation. We measure the MRR scores for both top1 and top5 ranks, and precision and pattern-recall rates for top5 retrieved answers. Table 1: Statistics of the collected Discovery videos # of videos # of sentence # of words # of passages 93 49950 746276 25001 AVG # of AVG # of AVG # of AVG # of words words per words per sentences per video sentence passage per passage 14.94 48.78 537.09 8024.47 4.2 Results In this paper, we employed six top-performed yet portable ranking models, TFIDF, BM-25 (Robertson et al., 2000), INQUERY, language model (Zhai and Lafferty, 2001), cosine, and densitybased (Lee et al., 2001) approaches for comparison1. For the language model, the Jelinek-Mercer smoothing method was employed with the parameter settings X=0.5 which was selected via several trials. In our preliminary experiments, we found that the query term expansion does not improve but decrease the overall ranking performance for all the ranking models. Thus, we only compare with the “pure” retrieval performance without pseudofeedback. The system performance was evaluated through the returned passages. We set α1=1.25, </context>
</contexts>
<marker>Robertson, Walker, Beaulieu, 2000</marker>
<rawString>Robertson, E., Walker, S., and Beaulieu, M. Experimentation as a way of life: Okapi at TREC. Journal of Information processing and management, 36: 95-108, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>B Katz</author>
<author>J J Lin</author>
<author>A Fernandes</author>
<author>G Marton</author>
</authors>
<title>Quantitative evaluation of passage retrieval algorithms for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="7953" citStr="Tellex et al. (2003)" startWordPosition="1261" endWordPosition="1264">most of them are irrelevant to the question. By means of text retrieval technology, the search space can be largely reduced and limited in a small set of relevant document. The document retrieval methods have been developed well and successfully been applied for retrieving relevant passages for question answering (Tellex et al., 541 2003). We replicated the Okapi BM-25 (Robertson et al., 2000), which is the effective and efficient retrieval algorithms to find the related segmented passages. For each input question, the top-1000 relevant passages are input to our ranking model. 3 The Algorithm Tellex et al. (2003) compared seven passage retrieval models for text QA except for several adhoc approaches that needed either humangenerated patterns or inference ontology which were not available. In their experiments, they showed that the density-based methods (Lee et al., 2001) achieved the best results, while the BM-25 (Robertson, 2000) reached slightly worse retrieval result than the density-based approaches, which adopted named entity taggers, thesaurus, and WordNet. Cui et al. (2005) showed that their fuzzy relation syntactic matching method outperformed the density-based methods. But the limitation is t</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Tellex, S., Katz, B., Lin, J. J., Fernandes, A., and Marton, G. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of the 26th ACM SIGIR Conference on Research and Development in Information Retrieval, pages 41-47, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th Text Retrieval Conference ,</booktitle>
<pages>42--52</pages>
<contexts>
<context position="20097" citStr="Voorhees, 2001" startWordPosition="3405" endWordPosition="3406">e general-purpose since hundreds hours of videos might take tens of hundreds GB storage space. Therefore, general questions are quite difficult to be found in the video database. Hence, we provide a list of short introductions collected from the cover-page of the videos and enable users to browse the descriptions. Users were then asked for the system with limited to the collected video topics. We finally filter the (1) keyword-like queries (2) non-Chinese and (3) un-supported questions. Finally, there were 253 questions for evaluation. For the answer assessment, we followed the TREC-QA track (Voorhees, 2001) and NTCIR to annotate answers in the pool that collected from the outputs of different passage retrieval methods. Unlike traditional text QA task, most of the OCR sentences contain a number of OCR error words. Furthermore, some sentence did include the answer string but error recognized as different words. Thus, instead of annotating the recognized transcripts, we used the corresponding video frames for evaluation because users can directly find the answers in the retrieved video clips and recognized text. Among 253 questions, 56 of which did not have an answer, while 368 passage&amp;frame segmen</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Voorhees, E. M. Overview of the TREC 2001 question answering track. In Proceedings of the 10th Text Retrieval Conference , pages 42-52, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ukkonen</author>
</authors>
<title>Constructing suffix trees on-line in linear time.</title>
<date>1995</date>
<booktitle>In Proceedings of the international federation of information processing,</booktitle>
<pages>484--492</pages>
<contexts>
<context position="10179" citStr="Ukkonen, 1995" startWordPosition="1608" endWordPosition="1609"> that the use of a well-trained high-performance Chinese word segmentation tool gave the worse result than using the unigram-level of Chinese word (13.95% and 13.92% relative precision and recall rates dropped for language model method). To alleviate this problem, we treat the atomic Chinese unigram as word and present a weighted string pattern matching algorithm. Our solution is to integrate the suffix tree for finding, and encoding important subsequence information in trees. Nevertheless, it is known that the suffix tree construction and pattern searching can be accomplished in linear time (Ukkonen, 1995). Before introducing our method, we give the following notations. passage P = PW1, PW2, ..., PWT question Q = QW1, QW2, ..., QWT’ a common subsequence for passage Sub = PW , PW + ,..., PW + − if |Sub| P P i k k 1 k x 1 i a common subsequence for question Sub =QW , QW + ,...,QW +− if |Sub |= Q Q j y j l l 1 l y 1 A common subsequence represents a continuous string matching between P and Q. We further impose two symbols on a subsequence. For example, SubiP means i-th matched continuous string (common subsequence) in the passage, while SubjQ indicates the j-th matched continuous string in the que</context>
</contexts>
<marker>Ukkonen, 1995</marker>
<rawString>Ukkonen, E. Constructing suffix trees on-line in linear time. In Proceedings of the international federation of information processing, pages 484-492, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H D Wactlar</author>
</authors>
<title>Informedia search and summarization in the video medium,</title>
<date>2000</date>
<booktitle>In Proceedings of Imagina 2000 Conference,</booktitle>
<contexts>
<context position="1374" citStr="Wactlar, 2000" startWordPosition="199" endWordPosition="200">lts showed that our method outperformed six top-performed ranking models. It is 10.16% better than the second best method (language model) in relatively MRR score and 6.12% in precision rate. Besides, we also show that the use of a trained Chinese word segmentation tool did decrease the overall videoQA performance where most ranking algorithms dropped at least 10% in relatively MRR, precision, and answer pattern recall rates. 1 Introduction With the drastic growth of video sources, effective indexing and retrieving video contents has recently been addressed. The well-known Informedia project (Wactlar, 2000) and TREC-VID track (Over et al., 2005) are the two famous examples. Although text-based question answering (QA) has become a key research issue in past decade, to support multimedia such as video, it is still beginning. Over the past five years, several video QA studies had investigated. Lin et al. (2001) presented an earlier work on combining videoOCR and term weighting models. Yang et al. (2003) proposed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules. They adopted the term-weighting method</context>
</contexts>
<marker>Wactlar, 2000</marker>
<rawString>Wactlar, H. D. Informedia search and summarization in the video medium, In Proceedings of Imagina 2000 Conference, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>Y S Lee</author>
<author>C H Chang</author>
</authors>
<title>CLVQ: Cross-language video question/answering system.</title>
<date>2004</date>
<booktitle>In Proceedings of 6th IEEE International Symposium on Multimedia Software Engineering,</booktitle>
<pages>294--301</pages>
<contexts>
<context position="2218" citStr="Wu et al. (2004)" startWordPosition="332" endWordPosition="335">e past five years, several video QA studies had investigated. Lin et al. (2001) presented an earlier work on combining videoOCR and term weighting models. Yang et al. (2003) proposed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules. They adopted the term-weighting method (Pasca, and Harabagiu, 2001) to rank the video segments by weighting the pre-defined keywords. Cao and Nunamaker (2004) developed a lexical pattern matching-based ranking method for a domain-specific videoQA. In the same year, Wu et al. (2004) designed a cross-language (Englishto-Chinese) video question answering system based on extracting pre-defined named entity words in captions. On the other hand, Zhang and Nunamaker (2004) made use of the simple TFIDF term weighting schema to retrieve the manualsegmented clips for video caption word retrieval. They also manually developed the ontology to improve system performance. In this paper, we present a new string pattern matching-based passage ranking algorithm for video question answering. We consider that the passage is able to answer questions and also suitable for videos because its</context>
<context position="19051" citStr="Wu et al., 2004" startWordPosition="3231" endWordPosition="3234">near time algorithms, which depends on the length of “common” subsequence, i.e., at most O(min(T, T’)). Consequently, the overall time complexity of our method for computing a passage is O((T+T’)(1+hlogm)+ min(T, T’)). 4 Experiments 4.1 Evaluation We should carefully select the use of videoQA collection for evaluation. Unfortunately, there is no benchmark corpus for this task. Thus, we develop an annotated collection by following the similar tasks as TREC, CLEF, and NTCIR. The Discovery videos are one of the popular raw video sources and widely evaluated in many literatures (Lin et al., 2001; Wu et al., 2004; Lee et al., 2005). Totally, 75.6 hours of Discovery videos (93 video names) were used. Table 1 lists the statistics of the Discovery films. The questions were created in two different ways: one set (about 73) was collected from previous studies (Lin et al., 2001; Wu et al., 2004) which came from the “Project: Assignment of Discovery”; while the other was derived from a real log from users. Video collections are difficult to be general-purpose since hundreds hours of videos might take tens of hundreds GB storage space. Therefore, general questions are quite difficult to be found in the video </context>
</contexts>
<marker>Wu, Lee, Chang, 2004</marker>
<rawString>Wu, Y. C., Lee, Y. S., Chang, C. H. CLVQ: Cross-language video question/answering system. In Proceedings of 6th IEEE International Symposium on Multimedia Software Engineering, pages 294-301, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Wu</author>
<author>J C Yang</author>
<author>Q X Lin</author>
</authors>
<title>Description of the NCU Chinese Word Segmentation and Named Entity Recognition System for SIGHAN Bakeoff</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>209--212</pages>
<contexts>
<context position="24488" citStr="Wu et al., 2006" startWordPosition="4146" endWordPosition="4149">videoQA performance with different ranking models using word segmentation tools Word-Level MRR MRR (Top5) Precision Pattern (Top1) Non-answered Recall Questions TFIDF 0.509 0.567 89 0.145 0.597 BM-25 0.438 0.500 104 0.159 0.543 Language Model 0.486 0.551 89 0.172 0.589 INQUERY 0.430 0.503 97 0.164 0.562 Cosine 0.403 0.480 100 0.158 0.548 Density 0.304 0.380 125 0.133 0.451 Our Method 0.509 0.561 89 0.181 0.608 Next, we evaluate the performance with adopting a trained Chinese word segmentation tool instead of unigram level of word. In this paper, we employed the Chinese word segmentation tool (Wu et al., 2006) that achieved about 0.93-0.96 recall/precision rates in the SIGHAN-3 word segmentation task (Levow, 2006). Table 3 lists the overall experimental results with the adopted word segmentation tool. In comparison to unigram grained level (Table 2), it is shown that the use of word segmentation tool does not improve the videoQA result for most top-performed ranking models, BM-25, language model, INQUERY, and our method. For example, our method is relatively 17.92% and 16.57% worse in MRR(Top1) and MRR(Top5) scores. In terms of precision and pattern-recall rates, it drops 14.91, and 16.94 relative </context>
</contexts>
<marker>Wu, Yang, Lin, 2006</marker>
<rawString>Wu, Y. C., Yang, J. C., and Lin, Q. X. Description of the NCU Chinese Word Segmentation and Named Entity Recognition System for SIGHAN Bakeoff 2006. In Proceedings of the 5th SIGHAN Workshop on Chinese Language Processing, pages 209-212, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>L Chaison</author>
<author>Y Zhao</author>
<author>S Y Neo</author>
<author>T S Chua</author>
</authors>
<title>VideoQA: Question answering on news video.</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th ACM International Conference on Multimedia,</booktitle>
<pages>632--641</pages>
<contexts>
<context position="1775" citStr="Yang et al. (2003)" startWordPosition="265" endWordPosition="268">and answer pattern recall rates. 1 Introduction With the drastic growth of video sources, effective indexing and retrieving video contents has recently been addressed. The well-known Informedia project (Wactlar, 2000) and TREC-VID track (Over et al., 2005) are the two famous examples. Although text-based question answering (QA) has become a key research issue in past decade, to support multimedia such as video, it is still beginning. Over the past five years, several video QA studies had investigated. Lin et al. (2001) presented an earlier work on combining videoOCR and term weighting models. Yang et al. (2003) proposed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules. They adopted the term-weighting method (Pasca, and Harabagiu, 2001) to rank the video segments by weighting the pre-defined keywords. Cao and Nunamaker (2004) developed a lexical pattern matching-based ranking method for a domain-specific videoQA. In the same year, Wu et al. (2004) designed a cross-language (Englishto-Chinese) video question answering system based on extracting pre-defined named entity words in captions. On the other h</context>
</contexts>
<marker>Yang, Chaison, Zhao, Neo, Chua, 2003</marker>
<rawString>Yang, H., Chaison, L., Zhao, Y., Neo, S. Y., and Chua, T. S. VideoQA: Question answering on news video. In Proceedings of the 11th ACM International Conference on Multimedia, pages 632-641, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to ad hoc information retrieval,</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>334--342</pages>
<contexts>
<context position="21796" citStr="Zhai and Lafferty, 2001" startWordPosition="3730" endWordPosition="3733"> score, precision and pattern-recall are used for evaluation. We measure the MRR scores for both top1 and top5 ranks, and precision and pattern-recall rates for top5 retrieved answers. Table 1: Statistics of the collected Discovery videos # of videos # of sentence # of words # of passages 93 49950 746276 25001 AVG # of AVG # of AVG # of AVG # of words words per words per sentences per video sentence passage per passage 14.94 48.78 537.09 8024.47 4.2 Results In this paper, we employed six top-performed yet portable ranking models, TFIDF, BM-25 (Robertson et al., 2000), INQUERY, language model (Zhai and Lafferty, 2001), cosine, and densitybased (Lee et al., 2001) approaches for comparison1. For the language model, the Jelinek-Mercer smoothing method was employed with the parameter settings X=0.5 which was selected via several trials. In our preliminary experiments, we found that the query term expansion does not improve but decrease the overall ranking performance for all the ranking models. Thus, we only compare with the “pure” retrieval performance without pseudofeedback. The system performance was evaluated through the returned passages. We set α1=1.25, α2= 0.25, and X=0.8 which were observed via the fol</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Zhai, C., and Lafferty, J. A study of smoothing methods for language models applied to ad hoc information retrieval, In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 334-342, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>J Nunamaker</author>
</authors>
<title>A natural language approach to content-based video indexing and retrieval for interactive E-learning.</title>
<date>2004</date>
<journal>IEEE Trans. on Multimedia,</journal>
<volume>6</volume>
<pages>450--458</pages>
<contexts>
<context position="2406" citStr="Zhang and Nunamaker (2004)" startWordPosition="358" endWordPosition="361">osed a complex videoQA approach by employing abundant external knowledge such as, Web, WordNet, shallow parsers, named entity taggers, and humanmade rules. They adopted the term-weighting method (Pasca, and Harabagiu, 2001) to rank the video segments by weighting the pre-defined keywords. Cao and Nunamaker (2004) developed a lexical pattern matching-based ranking method for a domain-specific videoQA. In the same year, Wu et al. (2004) designed a cross-language (Englishto-Chinese) video question answering system based on extracting pre-defined named entity words in captions. On the other hand, Zhang and Nunamaker (2004) made use of the simple TFIDF term weighting schema to retrieve the manualsegmented clips for video caption word retrieval. They also manually developed the ontology to improve system performance. In this paper, we present a new string pattern matching-based passage ranking algorithm for video question answering. We consider that the passage is able to answer questions and also suitable for videos because itself forms a very natural unit. Lin et al. (2003) showed that users prefer passage-level answers over short answer phrases since it contains rich context information. Our method makes use o</context>
</contexts>
<marker>Zhang, Nunamaker, 2004</marker>
<rawString>Zhang, D., and Nunamaker, J. A natural language approach to content-based video indexing and retrieval for interactive E-learning. IEEE Trans. on Multimedia, 6: 450-458, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>