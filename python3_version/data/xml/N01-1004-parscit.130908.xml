<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.7786936">
Why Inverse Document Frequency?
Kishore Papineni
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, USA
papineniAus.ibm.com
</note>
<sectionHeader confidence="0.942796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998517875">
Inverse Document Frequency (IDF) is a popular
measure of a word&apos;s importance. The IDF invari-
ably appears in a host of heuristic measures used in
information retrieval. However, so far the IDF has
itself been a heuristic. In this paper, we show IDF to
be optimal in a principled sense. We show that IDF
is the optimal weight of a word with respect to mini-
mization of a Kullback-Leibler distance suitably gen-
eralized to nonnegative functions which need not be
probability distributions. This optimization prob-
lem is closely related to maximum entropy problem.
We show that the IDF is the optimal weight associ-
ated with a word-feature in an information retrieval
setting where we treat each document as the query
that retrieves itself. That is, IDF is optimal for doc-
ument self-retrieval.
</bodyText>
<sectionHeader confidence="0.995382" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999750318681319">
Inverse Document Frequency (IDF) is a popular
measure of a word&apos;s importance. It is defined as
the logarithm of the ratio of number of documents
in a collection to the number of documents con-
taining the given word (Sparck Jones, 1973). This
means rare words have high IDF and common func-
tion words like &amp;quot;the&amp;quot; have low IDF. IDF is be-
lieved to measure a word&apos;s ability to discriminate
between documents. IDF invariably appears in a
host of heuristic measures used in information re-
trieval (Salton and McGill, 1983). However, so far
IDF has itself been a heuristic, although a good one.
In this paper, we show IDF to be optimal in a pre-
cise sense. To show this, we first view information
retrieval as a classification problem with each docu-
ment in the collection being a class. We then build
a classifier that scores the documents given a query.
To train this classifier, we treat each document as
a query that retrieves itself. Our classifier is an ex-
ponential model similar to the one in the maximum
entropy framework, but without the usual normal-
ization. Therefore, our classifier does not produce a
conditional probability distribution. In the regular
maximum entropy framework, one chooses weights
in the exponential model to maximize the likelihood
of the training data. However, we must now max-
imize a relaxed likelihood since our classifier uses a
generalized distribution. Our relaxed likelihood is
globally convex just like the traditional likelihood.
In the case when there is a single binary feature in
the model, the optimal solution is stunningly simple
in contrast to the solution in the regular conditional
maximum entropy framework. A word-feature that
examines the occurrence of the word in both the
query and the document is a binary feature. IDF is
the optimal weight of this feature. That is, IDF is
optimal for document self-retrieval.
In the exponential modeling framework, the
weight associated with a feature is not the same as
the &amp;quot;importance&amp;quot; or gain associated with the fea-
ture. The gain is the improvement of the relaxed
likelihood of the model (prior plus the single fea-
ture) over that of the prior. In our self-retrieval set-
ting with a fixed collection of documents, the least
important words are those with a single count or
those occurring in almost all the documents.
IDF is the optimal weight when there is a sin-
gle feature in the model. But how do we share the
word&apos;s weight across a unigram and a bigram? The
traditional justifications of IDF and similar mea-
sures assume that words occur independently in doc-
uments, e.g. (Robertson and Sparck Jones, 1976).
Those approaches do not easily generalize to account
for word co-occurrences. Our method does. It can
treat simultaneous features that examine presence
or absence of single words, phrases, and more so-
phisticated linguistic relations such as synonyms and
hyponyms. With the independence assumption, our
approach indeed produces the IDF. In the more gen-
eral case, we iteratively solve the optimization prob-
lem with word co-occurrences. Another approach
that does not make the independence assumption is
in (Wong and Yao, 1992). They show that IDF of
a word is the complement of relative entropy, upto
scaling by a constant independent of the word, if we
distribute the probability mass equally among doc-
uments containing the word. Their framework does
not decouple the weight of the word from its good-
ness as evidence.
For the interesting question of sharing a word&apos;s
weight across a unigram and a bigram, we have an
explicit closed-form solution. The unigram and bi-
gram are overlapping features. With a simple lin-
ear transformation, they become non-overlapping.
It turns out that the multi-parameter optimization
for non-overlapping features is reduced to the indi-
vidual single parameter problems on the component
features. That is, with non-overlapping features the
problem decouples into many simple problems. We
recover the solution to the problem with overlapping
features from the easily computed closed-form solu-
tion to that with non-overlapping features.
Since importance of a feature is given not by its
IDF but by the improvement in likelihood from that
feature, we can examine when a bigram becomes im-
portant when the prior already has the two compo-
nent word features. The bigram feature is important
only when it improves the retrieval likelihood over
the component word-features. We show a ranking
of bigrams on Reuters-21578 corpus with respect to
their generalized likelihood improvement.
</bodyText>
<sectionHeader confidence="0.823734" genericHeader="method">
2 Retrieval as Classification
</sectionHeader>
<bodyText confidence="0.999943048780488">
We treat information retrieval as a classification
problem with each document in the collection as a
class. To build a classifier, we are given a set of
labeled training data as (query, document) pairs.
A traditional method of classification employs a
probability distribution on the classes conditioned
on the observation (query), for example, using a de-
cision tree. For any observation, we assign nonnega-
tive scores (probabilities) to the classes based on the
trained model. Of course, some applications like in-
formation retrieval do not require that the model be
a true probability distribution, i.e., that the scores
add up to one; a probability distribution on the doc-
uments is not necessary.
Our framework is similar to the exponential mod-
els constructed in terms of features and their weights
in the familiar conditional maximum entropy
(minimum Kullback-Leibler &amp;quot;distance&amp;quot;) framework
(Della Pietra et al., 1997). We discuss this frame-
work below.
The standard exponential model has three com-
ponents: a prior conditional probability distribu-
tion Po (c x); a vector 0(x, c) of feature functions
(questions or rules) on observation and the candi-
date class; and a weights vector A corresponding
component-wise to the feature vector. The prior
distribution embodies prior knowledge about the do-
main, if any. If the modeler does not have any prior
knowledge, Po (clx) can simply be the uniform dis-
tribution. The features are usually, but not required
to be, binary. The features are questions on the ob-
servation as well as the candidate class, in contrast
to traditional decision tree questions. An example is
a binary-valued feature that examines if a particu-
lar word occurs in both the query and the candidate
document. Another example is a binary feature that
examines if the query contains a particular word and
the document contains a synonym or a hyponym of
the word.
The three components are combined to form an
exponential model as below:
</bodyText>
<equation confidence="0.999329">
Po (clx)ex-0(x,c)
P(clx) := zx
</equation>
<bodyText confidence="0.756821">
where
</bodyText>
<equation confidence="0.9707005">
Po (clx)ex4(x
cEC
</equation>
<bodyText confidence="0.949753">
is the normalization term needed to make P a prob-
ability distribution. C above is the set all of classes,
assumed to be finite and fixed a priori. Recall that
A and 0 are vectors and therefore
A 0(x, c) = E Aioi (x,
With ai := exi , we have
</bodyText>
<equation confidence="0.9973505">
Poccix)ri. cb(x&apos;c)
P(cix):= z:
</equation>
<bodyText confidence="0.999974594594594">
Therefore, the model P combines the information
from the training data and the prior model. We can
view the evaluation of the probabilities as a voting
scheme by experts. Each feature is an &amp;quot;expert&amp;quot; with
a &amp;quot;vote&amp;quot; (ai). An expert votes only when he agrees
with the observation and the candidate class. The
votes are multiplied and are followed by a normal-
ization.
The model is specified by the functional form,
training data to tune the parameters on, and an
objective function to tune the parameters. Maxi-
mum likelihood is a classic objective function and is
equivalent to maximum entropy in the special case
of uniform prior distribution. In the general prior
case, maximum likelihood is equivalent to minimum
Kullback-Leibler &amp;quot;distance&amp;quot; subject to expectation
constraints on the features and is globally convex.
The standard exponential model above contains
more components than we need. As we argued be-
fore, the normalization by Zx is not necessary in
many applications like information retrieval. If we
drop the normalization factor, we must formulate a
well-defined optimization problem on the new un-
normalized exponential models. Traditional likeli-
hood is not well-defined on nonnegative functions.
It turns out that there is an appropriate general-
ization to entropy maximization in the unnormal-
ized case that also results in a globally convex ob-
jective function. An added benefit is that the gen-
eralization results in a closed form solution to the
single-parameter binary feature problem. Remark-
ably, that closed-form solution is nothing other than
the IDF of a word for a feature that examines the
occurrence of the word both in the query and the
document.
First, we establish the parallel notation for non-
negative functions. We denote the scoring functions
</bodyText>
<equation confidence="0.705858">
Q : (x, c) [0, oo) as Q(clx) rather than as Q(x, c)
</equation>
<bodyText confidence="0.99975025">
to make the parallel explicit. Suppose the training
data is given as {(xi, ci)}, i = 1,2,•, N.
For any function g on the training data, we define
an analog of expectation:
</bodyText>
<equation confidence="0.991371">
:= E E weixi)9(xi,
i=1 ,Ec
</equation>
<bodyText confidence="0.999928">
We start with a prior nonnegative function
Q0(clx) that is not necessarily a probability distri-
bution. We fix a feature vector 0(x, c). We define a
parametric unnormalized convex exponential family
of functions as below:
</bodyText>
<equation confidence="0.539259">
:= {Q(clx):= Q0(cix)eA1
</equation>
<bodyText confidence="0.9995552">
The models are log-linear in the parameters. In or-
der to define the new objective function, we recall
the notion of log likelihood of training data accord-
ing to a conditional probability distribution P(clx) as
below:
</bodyText>
<equation confidence="0.934903">
E log P(ci lxi) .
</equation>
<bodyText confidence="0.999594">
In extending the log likelihood to a nonnegative
function Q, it is not enough to simply replace P
by Q in the above summation: for then it would
be possible to increase the objective function indefi-
nitely by simply scaling any arbitrary model without
exploring much of the model space. It is therefore
necessary to balance such scaling effects in the ob-
jective function. We use a simple balancing for our
generalization, below.
We now define a generalized likelihood of training
data according to Q as below, with 1 denoting the
constant function:
</bodyText>
<equation confidence="0.957608">
G(Q) := N — Q[1] + E log Q(cilxi) .
</equation>
<bodyText confidence="0.971561583333333">
When Q is a probability distribution, G(Q) clearly
coincides with the traditional likelihood. The gen-
eralized likelihood is well-behaved with respect to
scaling: Suppose we scale Q &amp;quot;row-by-row&amp;quot; for each
xi. Simple differentiation shows that optimal scal-
ing results in a probability distribution. Thus it is
not possible to increase the objective function indef-
initely by simply scaling a model.
We now have a family of models and an objective
function on them. It remains to optimize the ob-
jective function on the family. We first denote the
empirical feature counts by d as below.
</bodyText>
<figure confidence="0.469390636363636">
d :=
It is then routine to show that
G(Q)= C(Q0) + Qo[1]—N+N+A-d— Qo[eA].
Defining
L(A) := N+A-d—EE Qo(cixi)e&apos;e),
we see that
sup G(Q)
QEQ
is equivalent to
sup L (A)
A
</figure>
<bodyText confidence="0.967231">
The smooth function L(-) is particularly tractable:
Observation 1. L(A) is globally strictly convex.
Analogous to the regular maximum entropy frame-
work, the feature expectation with respect to the
optimal solution matches the empirical counts:
Observation 2. The following is necessary and suf-
ficient for a A* E TO to be optimal:
</bodyText>
<construct confidence="0.6701465">
d = E E Q0(cixi),A..0(xi,00(xi, c)
C
</construct>
<bodyText confidence="0.999969636363636">
In the next section, we present the problem of
maximizing L as the dual of a generalized Kullback-
Leibler distance minimization problem. In section
4, we solve the single-parameter optimization prob-
lem explicitly when 0 is a scalar binary feature. In
Section 5, we present a closed-form solution to the
multi-parameter problem with non-overlapping fea-
tures and use it compute the IDF&apos;s for a bigram and
unigram that share a word. In Section 6, we present
an analog of the TF-IDF formula used in informa-
tion retrieval.
</bodyText>
<sectionHeader confidence="0.9908195" genericHeader="method">
3 A Generalization of
Kullback-Leibler Distance
</sectionHeader>
<bodyText confidence="0.9999671875">
Observation 2 shows that the G-optimal solution
from the unnormalized exponential model class
matches the empirical counts in expectation. In-
stead of searching over the exponential model class,
can we search over all nonnegative functions that
match the empirical counts in expectation optimiz-
ing some objective function and arrive at the same
solution? That is the question we examine in this
section. Given the training data as in Section 2,
the following modification of Kullback-Leibler &amp;quot;dis-
tance&amp;quot; between two conditional probability distribu-
tions is common in statistical natural language pro-
cessing, e.g. (Lau et al., 1993). Assume Pi (clx) = 0
whenever P2(clx) = 0.
Therefore, the primal problem is equivalent to the
following dual:
</bodyText>
<equation confidence="0.969105333333333">
sup L(\)
XE R&apos;
which was shown in Section 2 to be equivalent to
DKL(p,, p2) := E E Pi(clxi) log Pi (cixi)
i1 cEC P2(Clxi)
=
I\T
sup G(Q).
C2EQ
</equation>
<bodyText confidence="0.772087">
We consider the following generalization to nonneg-
ative functions, Qi, Q2 0.
</bodyText>
<equation confidence="0.994840166666667">
DG(Qi,(22) := N — Qi [1] ±
Qi(eixi)
E E Q, (eixi) log (22(clxi)
i=1 cec
Clearly, if Pi is a probability distribution, then
DG (Pi, Q2) = DKL, (Pi, Q2).
</equation>
<bodyText confidence="0.985916454545455">
This is one of many possible generalizations of
the Kullback-Leibler distance. The Kullback-Leibler
distance is itself a Bregman distance (Lafferty et
al., 1997). However, our generalization is not a
Bregman distance and hence differs fundamentally
from the seemingly similar I-divergence considered
in (Csiszar, 1991) (the same as the unnormalized
relative entropy in (Collins et al., 2000)) even as it
is identical to the latter when Q2 is a probability
distribution.
Recall the empirical feature counts given by
</bodyText>
<equation confidence="0.7394615">
d := E0(xi,ci).
A priori, fix a Qo(clx) &gt; 0, called the prior model.
Define a family I?, as below:
:= {Q &gt; 0 : Q[0] =
</equation>
<bodyText confidence="0.767394">
We now pose an optimization problem:
</bodyText>
<equation confidence="0.596487">
inf DG(Q, (2o)
C2ER.
Formulating the Lagrangian
L(Q, A) := DG(Q, (2o) + A (d — QM) ,
</equation>
<bodyText confidence="0.9752145">
setting its differential with respect to Q(I) to 0, we
conclude that Q(clx) is of the form:
</bodyText>
<equation confidence="0.92547">
Q(clx) = Qo(clx)eA&apos;°(x&apos;e).
</equation>
<bodyText confidence="0.955866615384615">
Substituting this structural form into L(Q, )), we
get a function of A alone:
L(A) =N+A-d—EE Q0(clxi)eA.
Analogous to the regular maximum entropy
framework (Della Pietra et al., 1997), it turns out
that I?, n Q is a singleton containing the optimal
solution. However, unlike in the regular maximum
entropy framework, the optimal solution does not
satisfy the Pythagorean property. The optimal vec-
tor solution is obtained by a variant of the improved
iterative scaling algorithm (Papineni, 2000). How-
ever, in some special cases, the optimal solution has
a closed-form solution to which we now turn.
</bodyText>
<sectionHeader confidence="0.938749" genericHeader="method">
4 The Scalar Binary Feature
Problem
</sectionHeader>
<bodyText confidence="0.999232705882353">
The scalar feature problem is important since we se-
lect good features from a pool of possibly millions of
features. Feature selection involves solving a scalar
feature problem for every feature in the pool for the
optimal gain associated with the feature relative to
the current model. Then the features are ranked by
their gains and the top k features are selected and
added to the current model. The model is retrained
in a multi-parameter setting and the process is re-
peated until the optimal gain of the best feature in
the remaining pool falls below a threshold. With
the generalized Kullback-Leibler distance, the solu-
tion to the scalar binary feature problem turns out
to be closed-form, unlike that in the regular condi-
tional entropy maximization. Since feature is scalar,
the empirical count d is now scalar. We define a
scalar
</bodyText>
<equation confidence="0.9244525">
do := E E
cEc
</equation>
<bodyText confidence="0.96588025">
From Observation 2 and the fact that eq = eA
for a scalar binary 0, we get the following.
Proposition 1. The binary scalar feature solution
is given by
</bodyText>
<equation confidence="0.4920444">
= log —d&apos;
do
with optimal gain
do do
L(A.) —L(0) = d(—d — 1 — log —d).
</equation>
<bodyText confidence="0.99596625">
There is no such closed-form solution for a general
prior and a general binary feature in the regular
(normalized) conditional maximum entropy frame-
work.
</bodyText>
<figure confidence="0.999536821428571">
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
document frequency
Gan vs document frequency
idf
Gain
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0 1 2 3 4 5 6 7 8 9 10
Gain vs IOF
</figure>
<figureCaption confidence="0.999354">
Figure 3: Utility of a bigram
</figureCaption>
<table confidence="0.980928151515152">
Gain (milli-bits) IDF
59 2.23
53 2.72
42 3.18
39 1.63
38 3.35
37 3.21
37 1.89
35 3.46
35 3.49
34 3.36
33 3.42
33 1.84
32 0.81
31 3.53
30 2.52
29 2.94
29 3.78
28 0.84
27 3.07
23 2.40
22 2.22
20 4.13
19 3.38
19 3.14
0.68 7.13
0.68 7.13
0.68 7.13
0.68 7.13
0.68 7.13
0.68 7.13
0.68 7.13
Proposition 3. For non-overlapping binary fea-
</table>
<bodyText confidence="0.967544826086956">
tures, the multi-parameter optimal gain is the sum
of the single parameter optimal gains.
However, the features 0v and Ovw are not non-
overlapping. By transforming the features as below,
[ &apos;v Ii -1i ll :vvw
we get new features that are non-overlapping whose
solution can be obtained in closed-form (assuming
V)v is not identically zero) as
[ = [ log NN,th log ,
where Nvw is the number of documents that con-
tain v but not w. It remains to recover the optimal
weights for 0v and Ovw from the weights above.
We note that a linear-invertible transformation of
the feature vector results in an equivalent optimiza-
tion problem. Given a 0, let A. be the optimal
solution with optimal value 00. We consider that
A E R1&amp;quot; . We denote the feature, its unique opti-
mal solution, and the optimal value of the objective
function by the triple (0, A., Oct).
Observation 3. Let A be an invertible matrix.
Then, (0, A., oo) &lt;=&gt; (AO, A.A, oo).
From the above lemma, it follows that
[ A`t = [ log N log NN.u: .
</bodyText>
<subsubsectionHeader confidence="0.373122">
N,th
</subsubsectionHeader>
<bodyText confidence="0.999888222222222">
In the above derivations, it is assumed that there
are only two features in the model. This is much
like the assignment of IDF of a unigram that ig-
nores co-occurrences of words. To take care of all
correlations, one can solve the full multi-parameter
problem with all unigram, bigram features using the
improved iterative scaling algorithm. Indeed, term
frequencies can also be taken into account in this
framework, which is the subject of the next section.
</bodyText>
<sectionHeader confidence="0.989104" genericHeader="method">
6 Term Frequencies
</sectionHeader>
<bodyText confidence="0.929272777777778">
This framework also allows for counting the number
of occurrences of a given word in the document. We
simply define an integer-valued feature for a word w
as below:
tfw, if w is in both x and c
0 else
where tfwv is the number of times the word w ap-
pears in document c, that is, the term frequency of
w in c. We assume there is one such feature for every
word in the vocabulary. This results in the following
score for a document given a query:
Q(cix) = Qo(clx)ex °(x&apos;c)
Taking logarithms, we have
log Q (cix) = log Q0(clx) + E tfwvAw.
wExrc
Recall that Aw turned out to be IDF of w when
there is a single binary-valued feature in the model.
Now we have many features and they are not binary-
valued. Therefore the above scoring function is only
analogous but not identical to the familiar TF-IDF
scoring seen in the information retrieval literature.
But, we can obtain the weights for term-frequency
features in a principled manner by minimizing the
generalized likelihood proposed in this paper. An-
other variation is to use document-length normalized
term frequency in the definition of the feature, which
will be real-valued.
</bodyText>
<figure confidence="0.92856878125">
Bigram
year shr
pct year
pct last
cts share
pct interest
year net
billion dlr
pct billion
pct stock
year pct
year billion
stock exchange
last year
year company
net sales
u.s trade
pct company
cts net
u.s government
shares pct
due april
company stock
billion year
stock market
trade march
trade shares
trade year
interest government
interest shares
interest today
interest international
</figure>
<sectionHeader confidence="0.863353" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.993286826086956">
The IDF of a word is optimal for document self-
retrieval with respect to a generalized Kullback-
Leibler distance. This conclusion is the result of
viewing words independently and does not take into
account that words co-occur. While IDF is cheap to
compute, it must be modified when co-occurrence of
words is taken into account. There is a general opti-
mization framework where word co-occurrences can
be taken into account and which produces the famil-
iar IDF in the special case of a single feature. The
framework is that of a relaxed maximum entropy
or equivalently a relaxed log likelihood or Kullback-
Leibler distance. The general framework opens up
the possibility of assigning weights to more sophis-
ticated lexical questions that is consistent with the
popular notion of IDF.
S. K. M. Wong and Y. Y. Yao. 1992. An
information-theoretic measure of term specificity.
Journal of the American Society for Information
Science, 43:54-61.
Acknowledgments I thank R. T. Ward and J.
S. McCarley for many valuable discussions. I also
thank the anonymous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.99334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998808911764706">
Michael Collins, Robert E. Schapire, and Yoram
Singer. 2000. Logistic regression, Adaboost, and
Bregman distances. Proceedings of the Thirteenth
Annual Conference on Computational Learning
Theory.
I. Csiszar. 1991. Why least squares and maxi-
mum entropy? an axiomatic approach to inference
for linear inverse problems. Annals of Statistics,
19:2032-2066.
Stephen Della Pietra, Vincent Della Pietra, and
John Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(4):380-393.
John Lafferty, Stephen Della Pietra, and Vincent
Della Pietra. 1997. Statistical learning algorithms
based on Bregman distances. Canadian Workshop
on Information Theory, pages 77-80.
R. Lau, R. Rosenfeld, and S. Roukos. 1993. Adap-
tive language modeling using the maximum en-
tropy principle. Proceedings of the ARPA Human
Language Technology Workshop &apos;93, pages 108-
113.
Kishore Papineni. 2000. A generalized Kullback-
Leibler distance and its minimization. IBM Re-
search Report RC21815, August. Also available at
www.research.ibm.com/resources/paper_search.html.
S. E. Robertson and K. Sparck Jones. 1976. Rel-
evance weighting of search terms. Journal of the
American Society for Information Science, pages
129-146, May-June.
G. Salton and M. J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
K. Sparck Jones. 1973. Index term weighting. In-
formation Storage and Retrieval, 9:619-633.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370408">
<title confidence="0.998458">Why Inverse Document Frequency?</title>
<author confidence="0.623456">Kishore</author>
<affiliation confidence="0.7909355">IBM T.J. Watson Research Yorktown Heights, NY 10598,</affiliation>
<email confidence="0.99926">papineniAus.ibm.com</email>
<abstract confidence="0.996518705882353">Inverse Document Frequency (IDF) is a popular measure of a word&apos;s importance. The IDF invariably appears in a host of heuristic measures used in information retrieval. However, so far the IDF has itself been a heuristic. In this paper, we show IDF to be optimal in a principled sense. We show that IDF is the optimal weight of a word with respect to minimization of a Kullback-Leibler distance suitably generalized to nonnegative functions which need not be probability distributions. This optimization problem is closely related to maximum entropy problem. We show that the IDF is the optimal weight associated with a word-feature in an information retrieval setting where we treat each document as the query that retrieves itself. That is, IDF is optimal for document self-retrieval.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Logistic regression, Adaboost, and Bregman distances.</title>
<date>2000</date>
<booktitle>Proceedings of the Thirteenth Annual Conference on Computational Learning Theory.</booktitle>
<contexts>
<context position="14007" citStr="Collins et al., 2000" startWordPosition="2312" endWordPosition="2315"> We consider the following generalization to nonnegative functions, Qi, Q2 0. DG(Qi,(22) := N — Qi [1] ± Qi(eixi) E E Q, (eixi) log (22(clxi) i=1 cec Clearly, if Pi is a probability distribution, then DG (Pi, Q2) = DKL, (Pi, Q2). This is one of many possible generalizations of the Kullback-Leibler distance. The Kullback-Leibler distance is itself a Bregman distance (Lafferty et al., 1997). However, our generalization is not a Bregman distance and hence differs fundamentally from the seemingly similar I-divergence considered in (Csiszar, 1991) (the same as the unnormalized relative entropy in (Collins et al., 2000)) even as it is identical to the latter when Q2 is a probability distribution. Recall the empirical feature counts given by d := E0(xi,ci). A priori, fix a Qo(clx) &gt; 0, called the prior model. Define a family I?, as below: := {Q &gt; 0 : Q[0] = We now pose an optimization problem: inf DG(Q, (2o) C2ER. Formulating the Lagrangian L(Q, A) := DG(Q, (2o) + A (d — QM) , setting its differential with respect to Q(I) to 0, we conclude that Q(clx) is of the form: Q(clx) = Qo(clx)eA&apos;°(x&apos;e). Substituting this structural form into L(Q, )), we get a function of A alone: L(A) =N+A-d—EE Q0(clxi)eA. Analogous to</context>
</contexts>
<marker>Collins, Schapire, Singer, 2000</marker>
<rawString>Michael Collins, Robert E. Schapire, and Yoram Singer. 2000. Logistic regression, Adaboost, and Bregman distances. Proceedings of the Thirteenth Annual Conference on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Csiszar</author>
</authors>
<title>Why least squares and maximum entropy? an axiomatic approach to inference for linear inverse problems.</title>
<date>1991</date>
<journal>Annals of Statistics,</journal>
<pages>19--2032</pages>
<contexts>
<context position="13934" citStr="Csiszar, 1991" startWordPosition="2302" endWordPosition="2303">:= E E Pi(clxi) log Pi (cixi) i1 cEC P2(Clxi) = I\T sup G(Q). C2EQ We consider the following generalization to nonnegative functions, Qi, Q2 0. DG(Qi,(22) := N — Qi [1] ± Qi(eixi) E E Q, (eixi) log (22(clxi) i=1 cec Clearly, if Pi is a probability distribution, then DG (Pi, Q2) = DKL, (Pi, Q2). This is one of many possible generalizations of the Kullback-Leibler distance. The Kullback-Leibler distance is itself a Bregman distance (Lafferty et al., 1997). However, our generalization is not a Bregman distance and hence differs fundamentally from the seemingly similar I-divergence considered in (Csiszar, 1991) (the same as the unnormalized relative entropy in (Collins et al., 2000)) even as it is identical to the latter when Q2 is a probability distribution. Recall the empirical feature counts given by d := E0(xi,ci). A priori, fix a Qo(clx) &gt; 0, called the prior model. Define a family I?, as below: := {Q &gt; 0 : Q[0] = We now pose an optimization problem: inf DG(Q, (2o) C2ER. Formulating the Lagrangian L(Q, A) := DG(Q, (2o) + A (d — QM) , setting its differential with respect to Q(I) to 0, we conclude that Q(clx) is of the form: Q(clx) = Qo(clx)eA&apos;°(x&apos;e). Substituting this structural form into L(Q, </context>
</contexts>
<marker>Csiszar, 1991</marker>
<rawString>I. Csiszar. 1991. Why least squares and maximum entropy? an axiomatic approach to inference for linear inverse problems. Annals of Statistics, 19:2032-2066.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--4</pages>
<contexts>
<context position="6385" citStr="Pietra et al., 1997" startWordPosition="1034" endWordPosition="1037">oned on the observation (query), for example, using a decision tree. For any observation, we assign nonnegative scores (probabilities) to the classes based on the trained model. Of course, some applications like information retrieval do not require that the model be a true probability distribution, i.e., that the scores add up to one; a probability distribution on the documents is not necessary. Our framework is similar to the exponential models constructed in terms of features and their weights in the familiar conditional maximum entropy (minimum Kullback-Leibler &amp;quot;distance&amp;quot;) framework (Della Pietra et al., 1997). We discuss this framework below. The standard exponential model has three components: a prior conditional probability distribution Po (c x); a vector 0(x, c) of feature functions (questions or rules) on observation and the candidate class; and a weights vector A corresponding component-wise to the feature vector. The prior distribution embodies prior knowledge about the domain, if any. If the modeler does not have any prior knowledge, Po (clx) can simply be the uniform distribution. The features are usually, but not required to be, binary. The features are questions on the observation as wel</context>
<context position="14673" citStr="Pietra et al., 1997" startWordPosition="2433" endWordPosition="2436">2 is a probability distribution. Recall the empirical feature counts given by d := E0(xi,ci). A priori, fix a Qo(clx) &gt; 0, called the prior model. Define a family I?, as below: := {Q &gt; 0 : Q[0] = We now pose an optimization problem: inf DG(Q, (2o) C2ER. Formulating the Lagrangian L(Q, A) := DG(Q, (2o) + A (d — QM) , setting its differential with respect to Q(I) to 0, we conclude that Q(clx) is of the form: Q(clx) = Qo(clx)eA&apos;°(x&apos;e). Substituting this structural form into L(Q, )), we get a function of A alone: L(A) =N+A-d—EE Q0(clxi)eA. Analogous to the regular maximum entropy framework (Della Pietra et al., 1997), it turns out that I?, n Q is a singleton containing the optimal solution. However, unlike in the regular maximum entropy framework, the optimal solution does not satisfy the Pythagorean property. The optimal vector solution is obtained by a variant of the improved iterative scaling algorithm (Papineni, 2000). However, in some special cases, the optimal solution has a closed-form solution to which we now turn. 4 The Scalar Binary Feature Problem The scalar feature problem is important since we select good features from a pool of possibly millions of features. Feature selection involves solvin</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>Statistical learning algorithms based on Bregman distances. Canadian Workshop on Information Theory,</title>
<date>1997</date>
<pages>77--80</pages>
<contexts>
<context position="13777" citStr="Lafferty et al., 1997" startWordPosition="2279" endWordPosition="2282">whenever P2(clx) = 0. Therefore, the primal problem is equivalent to the following dual: sup L(\) XE R&apos; which was shown in Section 2 to be equivalent to DKL(p,, p2) := E E Pi(clxi) log Pi (cixi) i1 cEC P2(Clxi) = I\T sup G(Q). C2EQ We consider the following generalization to nonnegative functions, Qi, Q2 0. DG(Qi,(22) := N — Qi [1] ± Qi(eixi) E E Q, (eixi) log (22(clxi) i=1 cec Clearly, if Pi is a probability distribution, then DG (Pi, Q2) = DKL, (Pi, Q2). This is one of many possible generalizations of the Kullback-Leibler distance. The Kullback-Leibler distance is itself a Bregman distance (Lafferty et al., 1997). However, our generalization is not a Bregman distance and hence differs fundamentally from the seemingly similar I-divergence considered in (Csiszar, 1991) (the same as the unnormalized relative entropy in (Collins et al., 2000)) even as it is identical to the latter when Q2 is a probability distribution. Recall the empirical feature counts given by d := E0(xi,ci). A priori, fix a Qo(clx) &gt; 0, called the prior model. Define a family I?, as below: := {Q &gt; 0 : Q[0] = We now pose an optimization problem: inf DG(Q, (2o) C2ER. Formulating the Lagrangian L(Q, A) := DG(Q, (2o) + A (d — QM) , settin</context>
</contexts>
<marker>Lafferty, Pietra, Pietra, 1997</marker>
<rawString>John Lafferty, Stephen Della Pietra, and Vincent Della Pietra. 1997. Statistical learning algorithms based on Bregman distances. Canadian Workshop on Information Theory, pages 77-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
<author>R Rosenfeld</author>
<author>S Roukos</author>
</authors>
<title>Adaptive language modeling using the maximum entropy principle.</title>
<date>1993</date>
<booktitle>Proceedings of the ARPA Human Language Technology Workshop &apos;93,</booktitle>
<pages>108--113</pages>
<contexts>
<context position="13133" citStr="Lau et al., 1993" startWordPosition="2163" endWordPosition="2166">2 shows that the G-optimal solution from the unnormalized exponential model class matches the empirical counts in expectation. Instead of searching over the exponential model class, can we search over all nonnegative functions that match the empirical counts in expectation optimizing some objective function and arrive at the same solution? That is the question we examine in this section. Given the training data as in Section 2, the following modification of Kullback-Leibler &amp;quot;distance&amp;quot; between two conditional probability distributions is common in statistical natural language processing, e.g. (Lau et al., 1993). Assume Pi (clx) = 0 whenever P2(clx) = 0. Therefore, the primal problem is equivalent to the following dual: sup L(\) XE R&apos; which was shown in Section 2 to be equivalent to DKL(p,, p2) := E E Pi(clxi) log Pi (cixi) i1 cEC P2(Clxi) = I\T sup G(Q). C2EQ We consider the following generalization to nonnegative functions, Qi, Q2 0. DG(Qi,(22) := N — Qi [1] ± Qi(eixi) E E Q, (eixi) log (22(clxi) i=1 cec Clearly, if Pi is a probability distribution, then DG (Pi, Q2) = DKL, (Pi, Q2). This is one of many possible generalizations of the Kullback-Leibler distance. The Kullback-Leibler distance is itsel</context>
</contexts>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>R. Lau, R. Rosenfeld, and S. Roukos. 1993. Adaptive language modeling using the maximum entropy principle. Proceedings of the ARPA Human Language Technology Workshop &apos;93, pages 108-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
</authors>
<title>A generalized KullbackLeibler distance and its minimization.</title>
<date>2000</date>
<booktitle>IBM Research Report RC21815,</booktitle>
<note>Also available at www.research.ibm.com/resources/paper_search.html.</note>
<contexts>
<context position="14984" citStr="Papineni, 2000" startWordPosition="2484" endWordPosition="2485">, setting its differential with respect to Q(I) to 0, we conclude that Q(clx) is of the form: Q(clx) = Qo(clx)eA&apos;°(x&apos;e). Substituting this structural form into L(Q, )), we get a function of A alone: L(A) =N+A-d—EE Q0(clxi)eA. Analogous to the regular maximum entropy framework (Della Pietra et al., 1997), it turns out that I?, n Q is a singleton containing the optimal solution. However, unlike in the regular maximum entropy framework, the optimal solution does not satisfy the Pythagorean property. The optimal vector solution is obtained by a variant of the improved iterative scaling algorithm (Papineni, 2000). However, in some special cases, the optimal solution has a closed-form solution to which we now turn. 4 The Scalar Binary Feature Problem The scalar feature problem is important since we select good features from a pool of possibly millions of features. Feature selection involves solving a scalar feature problem for every feature in the pool for the optimal gain associated with the feature relative to the current model. Then the features are ranked by their gains and the top k features are selected and added to the current model. The model is retrained in a multi-parameter setting and the pr</context>
</contexts>
<marker>Papineni, 2000</marker>
<rawString>Kishore Papineni. 2000. A generalized KullbackLeibler distance and its minimization. IBM Research Report RC21815, August. Also available at www.research.ibm.com/resources/paper_search.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K Sparck Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>129--146</pages>
<location>May-June.</location>
<marker>Robertson, Jones, 1976</marker>
<rawString>S. E. Robertson and K. Sparck Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information Science, pages 129-146, May-June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="1455" citStr="Salton and McGill, 1983" startWordPosition="237" endWordPosition="240">ocument as the query that retrieves itself. That is, IDF is optimal for document self-retrieval. 1 Introduction Inverse Document Frequency (IDF) is a popular measure of a word&apos;s importance. It is defined as the logarithm of the ratio of number of documents in a collection to the number of documents containing the given word (Sparck Jones, 1973). This means rare words have high IDF and common function words like &amp;quot;the&amp;quot; have low IDF. IDF is believed to measure a word&apos;s ability to discriminate between documents. IDF invariably appears in a host of heuristic measures used in information retrieval (Salton and McGill, 1983). However, so far IDF has itself been a heuristic, although a good one. In this paper, we show IDF to be optimal in a precise sense. To show this, we first view information retrieval as a classification problem with each document in the collection being a class. We then build a classifier that scores the documents given a query. To train this classifier, we treat each document as a query that retrieves itself. Our classifier is an exponential model similar to the one in the maximum entropy framework, but without the usual normalization. Therefore, our classifier does not produce a conditional </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
</authors>
<title>Index term weighting.</title>
<date>1973</date>
<journal>Information Storage and Retrieval,</journal>
<pages>9--619</pages>
<contexts>
<context position="1177" citStr="Jones, 1973" startWordPosition="191" endWordPosition="192">gative functions which need not be probability distributions. This optimization problem is closely related to maximum entropy problem. We show that the IDF is the optimal weight associated with a word-feature in an information retrieval setting where we treat each document as the query that retrieves itself. That is, IDF is optimal for document self-retrieval. 1 Introduction Inverse Document Frequency (IDF) is a popular measure of a word&apos;s importance. It is defined as the logarithm of the ratio of number of documents in a collection to the number of documents containing the given word (Sparck Jones, 1973). This means rare words have high IDF and common function words like &amp;quot;the&amp;quot; have low IDF. IDF is believed to measure a word&apos;s ability to discriminate between documents. IDF invariably appears in a host of heuristic measures used in information retrieval (Salton and McGill, 1983). However, so far IDF has itself been a heuristic, although a good one. In this paper, we show IDF to be optimal in a precise sense. To show this, we first view information retrieval as a classification problem with each document in the collection being a class. We then build a classifier that scores the documents given </context>
</contexts>
<marker>Jones, 1973</marker>
<rawString>K. Sparck Jones. 1973. Index term weighting. Information Storage and Retrieval, 9:619-633.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>