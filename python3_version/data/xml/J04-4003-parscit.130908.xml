<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9886605">
Fast Approximate Search in Large
Dictionaries
</title>
<author confidence="0.965743">
Stoyan Mihov∗ Klaus U. Schulz†
</author>
<affiliation confidence="0.750101">
Bulgarian Academy of Sciences University of Munich
</affiliation>
<bodyText confidence="0.997596083333333">
The need to correct garbled strings arises in many areas of natural language processing. If a
dictionary is available that covers all possible input tokens, a natural set of candidatesfor correcting
an erroneous input P is the set ofall words in the dictionaryfor which the Levenshtein distance to P
does not exceed a given (small) bound k. In this article we describe methods for efficiently selecting
such candidate sets. After introducing as a starting point a basic correction method based on the
concept of a “universal Levenshtein automaton,” we show how two filtering methods known from
the field of approximate text search can be used to improve the basic procedure in a significant
way. The first method, which uses standard dictionaries plus dictionaries with reversed words,
leads to very short correction times for most classes of input strings. Our evaluation results
demonstrate that correction times for fixed-distance bounds depend on the expected number of
correction candidates, which decreases for longer input words. Similarly the choice of an optimal
filtering method depends on the length of the input words.
</bodyText>
<sectionHeader confidence="0.997806" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999941666666667">
In this article, we face a situation in which we receive some input in the form of strings
that may be garbled. A dictionary that is assumed to contain all possible correct input
strings is at our disposal. The dictionary is used to check whether a given input is
correct. If it is not, we would like to select the most plausible correction candidates
from the dictionary. We are primarily interested in applications in the area of natural
language processing in which the background dictionary is very large and fast selection
of an appropriate set of correction candidates is important. By a “dictionary,” we
mean any regular (finite or infinite) set of strings. Some possible concrete application
scenarios are the following:
</bodyText>
<listItem confidence="0.890682571428571">
• The dictionary describes the set of words of a highly inflectional or
agglutinating language (e.g., Russian, German, Turkish, Finnish,
Hungarian) or a language with compound nouns (German). The
dictionary is used by an automated or interactive spelling checker.
• The dictionary is multilingual and describes the set of all words of a
family of languages. It is used in a system for postcorrection of results of
OCR in which scanned texts have a multilingual vocabulary.
</listItem>
<note confidence="0.980484142857143">
∗ Linguistic Modelling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences,
25A, Akad. G. Bonchev Str., 1113 Sofia, Bulgaria. E-mail: stoyan@lml.bas.bg
† Centrum f¨ur Informations-und Sprachverarbeitung, Ludwig-Maximilians-Universit¨at-M¨unchen,
Oettingenstr. 67, 80538 Munchen, Germany. E-mail: schulz@cis.uni-muenchen.de
Submission received: 12 July 2003; Revised submission received: 28 February 2004; Accepted for
publication: 25 March 2004
© 2004 Association for Computational Linguistics
</note>
<title confidence="0.300699">
Computational Linguistics Volume 30, Number 4
</title>
<listItem confidence="0.958346">
• The dictionary describes the set of all indexed words and phrases of an
Internet search engine. It is used to determine the plausibility that a new
query is correct and to suggest “repaired” queries when the answer set
returned is empty.
• The input is a query to some bibliographic search engine. The dictionary
</listItem>
<bodyText confidence="0.991883880952381">
contains titles of articles, books, etc.
The selection of an appropriate set of correction candidates for a garbled input P is
often based on two steps. First, all entries W of the dictionary are selected for which the
distance between P and W does not exceed a given bound k. Popular distance measures
are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi
and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar,
and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and
Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992,
1994) Second, statistical data, such as frequency information, may be used to compute
a ranking of the correction candidates. In this article, we ignore the ranking problem
and concentrate on the first step. For selection of correction candidates we use the
standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned
applications, the number of correction candidates becomes huge for large values of k.
Hence small bounds are more realistic.
In light of this background, the algorithmic problem discussed in the article can
be described as follows:
Given a pattern P, a dictionary D, and a small bound k, efficiently
compute the set of all entries W in D such that the Levenshtein distance
between P and W does not exceed k.
We describe a basic method and two refinements for solving this problem. The basic
method depends on the new concept of a universal deterministic Levenshtein au-
tomaton of fixed degree k. The automaton of degree k may be used to decide, for
arbitrary words U and V, whether the Levenshtein distance between U and V does
not exceed k. The automaton is “universal” in the sense that it does not depend on
U and V. The input of the automaton is a sequence of bitvectors computed from U
and V. Though universal Levenshtein automata have not been discussed previously
in the literature, determining Levenshtein neighborhood using universal Levenshtein
automata is closely related to a more complex table-based method described by the
authors Schulz and Mihov (2002). Hence the main advantage of the new notion is its
conceptual simplicity. In order to use the automaton for solving the above problem,
we assume that the dictionary is given as a determininistic finite-state automaton. The
basic method may then be described as a parallel backtracking traversal of the uni-
versal Levenshtein automaton and the dictionary automaton. Backtracking procedures
of this form are well-known and have been used previously: for example, by Oflazer
(1996) and the authors Schulz and Mihov (2002).
For the first refinement of the basic method, a filtering method used in the field
of approximate text search is adapted to the problem of approximate search in a dic-
tionary. In this approach, an additional “backwards” dictionary D−R (representing the
set of all reverses of the words of a given dictionary D) is used to reduce approximate
search in D with a given bound k ≥ 1 to related search problems for smaller bounds
k&apos; &lt; k in D and D−R. As for the basic method, universal Levenshtein automata are used
to control the search. Ignoring very short input words and correction bound k = 1,
</bodyText>
<page confidence="0.545988">
452
</page>
<note confidence="0.956759">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<bodyText confidence="0.999845352941176">
this approach leads to a drastic increase in speed. Hence the “backwards dictionary
method” can be considered the central contribution of this article.
The second refinement, which is only interesting for bound k = 1 and short input
words, also uses a filtering method from the field of approximate text search (Muth
and Manber 1996; Mor and Fraenkel 1981). In this approach, “dictionaries with single
deletions” are used to reduce approximate search in a dictionary D with bound k = 1
to a conventional lookup technique for finite-state transducers. Dictionaries with single
deletions are constructed by deleting the symbol at a fixed position n in all words of
a given dictionary.
For the basic method and the two refinements, detailed evaluation results are given
for three dictionaries that differ in terms of the number and average length of entries:
a dictionary of the Bulgarian language with 965,339 entries (average length 10.23 sym-
bols), a dictionary of German with 3,871,605 entries (dominated by compound nouns,
average length 18.74 symbols), and a dictionary representing a collection of 1,200,073
book titles (average length 47.64 symbols). Tests were restricted to distance bounds
k = 1, 2, 3. For the approach based on backwards dictionaries, the average correction
time for a given input word—including the displaying of all correction suggestions—
is between a few microseconds and a few milliseconds, depending on the dictionary,
the length of the input word, and the bound k. Correction times over one millisecond
occur only in a few cases for bound k = 3 and short input words. For bound k = 1,
which is important for practical applications, average correction times did not exceed
40 microseconds.
As a matter of fact, correction times are a joint result of hardware improvements
and algorithmic solutions. In order to judge the quality of the correction procedure in
absolute terms, we introduce an “idealized” correction algorithm in which any kind
of blind search and superfluous backtracking is eliminated. Based on an analysis of
this algorithm, we believe that using purely algorithmic improvements, our correction
times can be improved only by a factor of 50–250, depending on the kind of dictionary
used. This factor represents a theoretical limit in the sense that the idealized algorithm
probably cannot be realized in practice.
This article is structured as follows. In Section 2, we collect some formal prelimi-
naries. In Section 3, we briefly summarize some known techniques from approximate
string search in a text. In Section 4, we introduce universal deterministic Levenshtein
automata of degree k and describe how the problem of deciding whether the Lev-
enshtein distance between two strings P and W does not exceed k can be efficiently
solved using this automaton. Since the method is closely related to a table-based
approach introduced by the authors (Schulz and Mihov 2002), most of the formal de-
tails have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method,
the refined approach based on backwards dictionaries, and the approach based on
dictionaries with single deletions. Evaluation results are given for the three dictio-
naries mentioned above. In Section 8 we briefly comment on the difficulties that
we encountered when trying to combine dictionary automata and similarity keys
(Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha
1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de
Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in
Section 9.
The problem considered in this article is well-studied. Since the number of contri-
butions is enormous, a complete review of related work cannot be given here. Relevant
references with an emphasis on spell-checking and OCR correction are Blair (1960),
Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari,
Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel
</bodyText>
<page confidence="0.847307">
453
</page>
<note confidence="0.711683">
Computational Linguistics Volume 30, Number 4
</note>
<bodyText confidence="0.99613825">
and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary
is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer
(1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate
search in texts is described in Section 3.
</bodyText>
<sectionHeader confidence="0.998658" genericHeader="keywords">
2. Formal Preliminaries
</sectionHeader>
<bodyText confidence="0.998929714285714">
We assume that the reader is familiar with the basic notions of formal language theory
as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual,
finite-state automata (FSA) are treated as tuples of the form A = (Σ, Q, q0, F, ∆), where
Σ is the input alphabet, Q is the set of states, q0 E Q is the initial state, F is the set
of final states, and ∆ C_ Q x Σε x Q is the transition relation. Here E denotes the
empty string and Σε := Σ U {E}. The generalized transition relation ∆ˆ is defined as the
smallest subset of Q x Σ* x Q with the following closure properties:
</bodyText>
<listItem confidence="0.977066666666667">
• For all q E Q we have (q, E, q) E ˆ∆.
• For all q1, q2, q3 E Q and W1, W2 E Σ*, if (q1, W1, q2) E ∆ˆ and
(q2, W2, q3) E ∆, then also (q1, W1W2, q3) E ˆ∆.
</listItem>
<bodyText confidence="0.996930857142857">
We write L(A) for the language accepted by A. We have L(A) = {W E Σ*  |Elq E
F : (q0, W, q) E ˆ∆}. Given A as above, the set of active states for input W E Σ* is
{q E Q  |(q0, W, q) E ˆ∆}.
A finite-state automaton A is deterministic if the transition relation is a function
6 : Q x Σ → Q. Let A = (Σ, Q, q0, F, 6) be a deterministic FSA, and let 6* : Q x Σ* → Q
denote the generalized transition function, which is defined in the usual way. For
q E Q, we write LA(q) := {U E Σ*  |6*(q,U) E F} for the language of all words that
lead from q to a final state.
The length of a word W is denoted by |W|. Regular languages over Σ are defined
in the usual way. With L1 · L2 we denote the concatenation of the languages L1 and
L2. It is well-known that for any regular language L, there exists a deterministic FSA
AL such that L(A) = L and AL is minimal (with respect to number of states) among
all deterministic FSA accepting L. AL is unique up to renaming of states.
A p-subsequential transducer is a tuple T = (Σ, Π, Q, q0, F, 6, A, Ψ), where
</bodyText>
<listItem confidence="0.9915606">
• (Σ, Q, q0, F, 6) is a deterministic finite-state automaton;
• Π is a finite output alphabet;
• A : Q x Σ → Π* is a function called the transition output function;
• the final function Ψ : F → 2Π∗ assigns to each f E F a set of strings over
Π, where |Ψ(f ) |&lt; p.
</listItem>
<bodyText confidence="0.716452">
The function A is extended to the domain Q x Σ* by the following definition of A*:
</bodyText>
<equation confidence="0.8685765">
dq E Q (A*(q,E) = E)
dq E QdUE Σ* da E Σ (A*(q,Ua) = A*(q, U)A(6*(q,U),a))
</equation>
<bodyText confidence="0.999201">
The input language of the transducer is L(T) := {U E Σ*  |6*(q0,U) E F}. The
subsequential transducer maps each word from the input language to a set of at most
</bodyText>
<page confidence="0.798056">
454
</page>
<note confidence="0.951767">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<bodyText confidence="0.9616135">
p output words. The output function OT : L(T) → 2Π∗ of the transducer is defined as
follows:
</bodyText>
<equation confidence="0.605243">
VU E L(T) (OT(U) = A*(q0,U) - Ψ(b*(q0,U)))
</equation>
<bodyText confidence="0.999672777777778">
By a dictionary, we mean a regular (finite or infinite) set of strings over a given
alphabet E. Using the algorithm described by Daciuk et al. (2000), the minimal deter-
ministic FSA AD accepting a finite dictionary D can be effectively computed.
By a dictionary with output sets, we mean a regular (finite or infinite) set of
input strings over a given alphabet together with a function that maps each of the
input strings to a finite set of output strings. Given a finite dictionary with output
sets, we can effectively compute, using the algorithm described by Mihov and Maurel
(2001), the minimal subsequential transducer that maps each input string to its set of
output strings.
</bodyText>
<sectionHeader confidence="0.975448" genericHeader="introduction">
3. Background
</sectionHeader>
<bodyText confidence="0.999971">
In this section, we describe some established work that is of help in understanding
the remainder of the article from a nontechnical, conceptual point of view. After in-
troducing the Levenshtein distance, we describe methods for computing the distance,
for checking whether the distance between two words exceeds a given bound, and for
approximate search for a pattern in a text. The similarities and differences described
below between approximate search in a text, on the one hand, and approximate search
in a dictionary, on the other hand, should help the reader understand the contents of
the following sections from a broader perspective.
</bodyText>
<subsectionHeader confidence="0.999982">
3.1 Computation of Levenshtein Distance
</subsectionHeader>
<bodyText confidence="0.999984">
The most prominent metric for comparing strings is the Levenshtein distance, which
is based on the notion of a primitive edit operation. In this article, we consider the
standard Levenshtein distance. Here the primitive operations are the substitution of
one symbol for another symbol, the deletion of a symbol, and the insertion of a
symbol. Obviously, given two words W and V in the alphabet E, it is always possible
to rewrite W into V using primitive edit operations.
</bodyText>
<subsectionHeader confidence="0.626442">
Definition 1
</subsectionHeader>
<bodyText confidence="0.999732833333333">
Let P, W be words in the alphabet E. The (standard) Levenshtein distance between
P and W, denoted dL(P, W), is the minimal number of primitive edit operations (sub-
stitutions, deletions, insertions) that are needed to transform P into W.
The Levenshtein distance between two words P and W can be computed using the
following simple dynamic programming scheme, described, for example, by Wagner
and Fischer (1974):
</bodyText>
<equation confidence="0.99897425">
dL(e, W) = |W|
dL(P,e) = |P|
dL(Pa, Wb) = �dL(P,W) if a = b
1 + min(dL(P, W), dL(Pa, W), dL(P, Wb)) if a =� b
</equation>
<bodyText confidence="0.9957445">
for P, W E E* and a, b E E. Given P = p1 ... pm and W = w1 ... wn (m, n &gt; 0), a standard
way to apply the scheme is as follows: Proceeding top-down and from left to right,
the cells of an (m + 1) x (n + 1) table TL(P,W) are filled, where entry (i, j) of T(P,W)
is dL(p1 ... pi, w1 ... wj) (0 &lt; i &lt; m, 0 &lt; j &lt; n) (Wagner and Fischer 1974). The first two
</bodyText>
<page confidence="0.669908">
455
</page>
<figure confidence="0.636793875">
Computational Linguistics Volume 30, Number 4
h c h o l d
0 1 2 3 4 5 6
c 1 1 1 2 3 4 5
h 2 1 2 1 2 3 4
o 3 2 2 2 1 2 3
l 4 3 3 3 2 1 2
d 5 4 4 4 3 2 1
</figure>
<figureCaption confidence="0.917308">
Figure 1
</figureCaption>
<bodyText confidence="0.99149">
Computation of the Levenshtein distance using dynamic programming and filling table
TL(chold, hchold). Shaded regions represent diagonals in Ukkonen’s approach (cf. Section 3.2).
clauses above are used for initialization and yield, respectively, the first column and
the first row. The third clause is used to compute the remaining entries. The table for
the strings chold and hchold is shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.99996">
3.2 Testing Levenshtein Neighborhood
</subsectionHeader>
<bodyText confidence="0.9999893">
The algorithm of Wagner and Fischer, which has time complexity O(m · n), has been
improved and generalized in many aspects. (See, for example, Stephen [1994] for a
survey). We briefly sketch a more efficient variant that can be used for the restricted
problem of deciding whether the Levenshtein distance between two words P and W
exceeds a fixed bound, k. Ukkonen (1985a) shows that in this case only the values of
2k + 1 “diagonals” of TL(P, W) are essential for a test to make such a determination.
Figure 1 illustrates the situation in which k = 2. Ukkonen obtained an algorithm
with time complexity O(k · min(m,n)). He used the test for determining whether the
Levenshtein distance between two words exceeds a given bound to derive an algorithm
for computing the edit distance with complexity O(min(m,n) · dL(P,W)).
</bodyText>
<subsectionHeader confidence="0.999433">
3.3 Approximate Search for a Pattern in a Text
</subsectionHeader>
<bodyText confidence="0.999450833333333">
A problem closely related to approximate search in a dictionary is approximate search
for a pattern in a text (AST): Given two strings P and T (called, respectively, the pattern
and the text), find all occurrences T&apos; of substrings of T that are within a given distance
of P. Each occurrence T&apos; is called a hit. In the following discussion, we consider the
case in which a fixed bound k for the Levenshtein distance between P and potential
hits is specified.
</bodyText>
<listItem confidence="0.983163307692308">
3.3.1 Adapting the Dynamic Programming Scheme. A simple adaptation of the
Wagner-Fischer algorithm may be used for approximate search for a pattern P =
p1 · · · pm in a text T = t1 · · · tn. As before, we compute an (m+1)x(n+1) table TAST(P,T).
Entry (i,j) of TAST(P,T) has value h if h is the minimal Levenshtein distance between
p1 · · · pi and a substring of T with last symbol (position) tj (j). From the definition, we
see that all cells in line 0 have to be initialized with 0. The remaining computation
proceeds as above. For a given bound k, the output consists of all positions j such that
entry (m, j) does not exceed k. Note that the positions j in the output are the end points
in T of approximate matches of P. Figure 2 illustrates a search employing dynamic
programming.
3.3.2 Automaton Approach. Several more-efficient methods for approximate search
of a pattern P in a text T take as their starting point a simple nondeterministic finite-
state automaton, AAST(P, k), which accepts the language of all words with Levenshtein
</listItem>
<page confidence="0.753643">
456
</page>
<bodyText confidence="0.61497225">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
t h i s _ c h i l d
0 0 0 0 0 0 0 0 0 0 0
c 1 1 1 1 1 1 0 1 1 1 1
h 2 2 1 2 2 2 1 0 1 2 2
o 3 3 2 2 3 3 2 1 1 2 3
l 4 4 3 3 3 4 3 2 2 1 2
d 5 5 4 4 4 4 4 3 3 2 1
</bodyText>
<figureCaption confidence="0.616265">
Figure 2
</figureCaption>
<bodyText confidence="0.983158888888889">
Approximate search of pattern chold in a text using dynamic programming.
distance &lt; k to some word in E∗·P (Ukkonen 1985b; Wu and Manber 1992; Baeza-Yates
and Navarro 1999). The automaton for pattern chold and distance bound k = 2 is shown
in Figure 3. States are numbered in the form be. The “base number” b determines the
position of the state in the pattern. The “exponent” e indicates the error level, that
is, the number of edit errors that have been observed. Horizontal transitions encode
“normal” transitions in which the text symbol matches the expected next symbol of
the pattern. Vertical transitions represent insertions, nonempty (respectively, empty)
diagonal transitions represent substitutions (respectively, deletions). In the example
shown in Figure 3, final states are 50, 51, and 52. It is obvious that when using a given
text T as input, we reach a final state of AAST(P, 2) exactly at those positions where
a substring T&apos; of T ends such that dL(P,T&apos;) &lt; 2. For other bounds k, we just have to
vary the number of levels. Note that a string can be accepted in AAST(P, k) at several
final states. In order to determine the optimal distance between P and a substring T&apos;
of T ending at a certain position, it is necessary to determine the final state that can
be reached that has the smallest exponent.
In the remainder of the article, the set of all states with base number i is called
the ith column of AAST(P, k).
</bodyText>
<sectionHeader confidence="0.464804" genericHeader="method">
Remark 1
</sectionHeader>
<bodyText confidence="0.997870714285714">
There is a direct relationship between the entries in column j of the dynamic program-
ming table TAST(P,T) and the set of active states of AAST(P,k) that are reached with
input t1 · · · tj. Entry (i,j) of TAST(P,T) has the value h &lt; k iff h is the exponent of the
bottom-most active state in the ith column of AAST(P, k). For example, in Figure 3, the
set of active states of AAST(chold, k) reached after reading the two symbols t and h
is highlighted. The bottom-most elements 00, 11, 21, and 32 correspond to the entries
0,1,1, and 2 shaded in the upper part of the third column of Figure 2.
</bodyText>
<figure confidence="0.99361832">
02 c 12 h 22 o 32 l 42 d 52
ε
ε
ε
ε
ε
Σ
Σ
Σ
Σ
Σ
Σ Σ
Σ Σ Σ
Σ
c h o l d
01 11 21 31 41 51
ε
ε
ε
ε
ε
Σ
Σ Σ
Σ Σ
Σ Σ
Σ Σ Σ
Σ
c l
00 10 h 20 o 30 40 d 50
Σ
2
1
0
Figure 3
Nondeterministic automaton AAST(chold, 2) for approximate search with pattern chold and
distance bound k = 2. Active states after symbols t and h have been read are highlighted.
457
Computational Linguistics Volume 30, Number 4
Σ Σ
Σ
c h o l d
02 12 22 32 42
c h o l d
01 11 21 31 41 51
40
c l
00 10 h 20 o 30
d 50
ε
ε
Σ Σ
Σ
ε
ΣΣ
Σ
Σ
ε
ΣΣ
Σ
Σ
ε
ε
Σ Σ
Σ
Σ
ε
ε
Σ
Σ
Σ
Σ
52
2
0
1
</figure>
<bodyText confidence="0.894816285714286">
pattern chold. Triangular areas are highlighted. Dark states are active after symbols h an
d c
have been read.
k) might be difficult or impossible. In practice, simulation of deter-
minism via bit-parallel computation of sets of active states gives rise to efficient and
flexible algorithms. See Navarro (2001) and Navarro and Raffinot (2002) for surveys
of algorithms along this li
</bodyText>
<figure confidence="0.7676445">
AAST(P,
ne.
ε
ε
</figure>
<figureCaption confidence="0.918841">
Figure 4
</figureCaption>
<bodyText confidence="0.910905">
Nondeterministic automaton A(chold,2) for testing Levenshtein distance with bound k = 2 for
The direct use of the nondeterministic automaton AAST(P,k) for conducting ap-
proximate searches is inefficient. Furthermore, depending on the length m of the pat-
tern and the error bound k, the explicit construction and storage of a deterministic
version of
</bodyText>
<figure confidence="0.683237571428571">
4. Testing Levenshtein Neighborhood with Universal Deterministic
Levenshtein Automata
In our approach, approximate search of a pattern P in a dictionary D is traced back
to the problem of deciding whether the Levenshtein distance between P and an entry
W of D exceeds a given bound k. A well-known method for solving this problem is
based on a nondeterministic automaton
similar to AAST(P,k). A string W is
accepted by A(P, k) iff
W)
k. The automaton A(P, k) does not have the initial E
loop that is needed in AAST(P,k) to traverse the text. The automaton for pattern chold
and distance bound k = 2 is shown in Figure 4. Columns of
with numbers
... , m =
</figure>
<bodyText confidence="0.961922857142857">
are defined as for AAST(P, k). In A(P, k), we use as final states all states
q from which we can reach one of the states in column m using a (possibly empty)
sequence of e-transitions. The reason for this modification—which obviously does not
change the set of accepted words—will become apparent later.
We now show that for fixed small error bounds k, the explicit computation of
A(P, k), in a deterministic or nondeterministic variant, can be completely avoided. In
our approach, pattern P and entry W =
are compared to produce a sequence
of n bitvectors
This sequence is used as input for a fixed automaton Ae(k).
The automaton Ae(k) is deterministic and
in the sense that it does not
depend on a given pattern P. For each bound k, there is just one fixed automaton
which is precomputed once and used for arbitrary patterns P and words W.
</bodyText>
<equation confidence="0.722049">
accepts input
iff
W) &lt;_ k. The efficiency of this method relies
on the fact that given
we need only one operation for each transition of the
</equation>
<bodyText confidence="0.9473438">
recognition phase after the initial computation of the bitvectors
It is worth mentioning that the possibility of using a fixed universal automaton
Ae(k) instead of a specific automaton
for each pattern P is based on special
features of the automata A(P, k) (cf. Remark 2); a similar technique for AAST(P,k)
</bodyText>
<equation confidence="0.975893777777778">
A(P,k)
dL(P,
&lt;_
A(P,k)
0,
|P|
w1···wn
�χ1, . . . , �χn.
“universal”
Ae(k),
Ae(k)
�χ1, . . . , �χn
dL(P,
Ae(k),
�χ1, . . . , �χn.
A(P,k)
458
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</equation>
<bodyText confidence="0.9992695">
appears to be impossible. For defining states, input vectors and transitions of A∀(k),
the following two definitions are essential:
</bodyText>
<subsectionHeader confidence="0.568535">
Definition 2
</subsectionHeader>
<bodyText confidence="0.99936">
The characteristic vector ;&amp;, V) of a symbol w E E in a word V = v1 · · · vn E E∗ is
the bitvector of length n where the ith bit is set to 1 iff w = vi.
</bodyText>
<subsectionHeader confidence="0.573023">
Definition 3
</subsectionHeader>
<bodyText confidence="0.999835294117647">
Let P denote a pattern of length m. The triangular area of a state p of A(P, k) consists of
all states q of A(P, k) that can be reached from p using a (potentially empty) sequence
of u upward transitions and, in addition, h &lt; u horizontal or reverse (i.e., leftward)
horizontal transitions. Let 0 &lt; i &lt; m. By triangular area i, we mean the triangular area
of state i0. For j = 1,. . . , k, by triangular area m+j, we mean the triangular area of the
state mj.
For example, in Figure 4, triangular areas 0,. . . , 7 of A(chold, 2) are shown.
In Remark 1, we pointed to the relationship between the entries in column i of table
TAST(P, T) and the set of active states of AAST(P, k) that are reached with input w1 · · · wi.
A similar relationship holds between the entries in column i of table TL(P, T) and the
set of active states of the automaton A(P, k) that are reached with input w1 · · · wi.
Triangular area i corresponds to the ith column of the subregion of TL(P,T) given by
the 2k + 1 diagonals used in Ukkonen’s (1985a) approach. The left-to-right orientation
in A(P, k) corresponds to a top-down orientation in TL(P, T). As an illustration, the
active states of A(chold,2) after symbols h and c have been consumed are marked in
Figure 4. The exponents 2, 1, 2, and 2 of the bottom-most active states in columns 1,
2, 3, and 4 are found in the shaded region of the third column of Figure 1.
</bodyText>
<sectionHeader confidence="0.393467" genericHeader="method">
Remark 2
</sectionHeader>
<bodyText confidence="0.999394">
It is simple to see that for any input string W = w1 ... wn, the set of active states of
A(P, k) reached after reading the ith symbol wi of W is a subset of triangular area i
(0 &lt; i &lt; minfn, m + k}). For i &gt; m + k, the set is empty. Furthermore, the set of active
states that is reached after reading symbol wi depends only
</bodyText>
<listItem confidence="0.96501875">
1. on the previous set of active states (the set reached after reading
w1 ... wi−1, a subset of the triangular area i − 1);
2. on the characteristic vector ;&amp;i, pl · · · pi · · · pr) where l = maxf1, i − k}
and r = minfm, i + k}.
</listItem>
<bodyText confidence="0.964224333333333">
The following description of A∀(k) proceeds in three steps that introduce, in order,
input vectors, states, and the transition function. States and transition function are
described informally.
</bodyText>
<listItem confidence="0.897965">
1. Input vectors. Basically we want to use the vectors x(wi,pl · · · pi · · · pr), which
are of length &lt; 2k + 1, as input for A∀(k). For technical reasons, we introduce two
modifications. First, in order to standardize the length of the characteristic vectors that
</listItem>
<bodyText confidence="0.9446758">
are obtained for the initial symbols w1, w2, ..., we define p0 = p−1 = ... = p−k+1 := $.
In other words, we attach to P a new prefix with k symbols $. Here $ is a new symbol
that does not occur in W. Second imagine that we get to triangular area i after reading
the ith letter wi (cf. Remark 2). As long as i &lt; m − k − 1, we know that we cannot
reach a triangular area containing final states after reading wi. In order to encode
</bodyText>
<figure confidence="0.2742525">
459
Computational Linguistics Volume 30, Number 4
</figure>
<bodyText confidence="0.999447333333333">
this information in the input vectors, we enlarge the relevant subword of P for input
wi and consider one additional position i + k + 1 on the right-hand side (whenever
i + k + 1 &lt; m). This means that we use the vectors ;Ci := x(wi,pi−k · · · pi · · · pr), where
r = min{m, i+k+1}, as input for A∀(k), for 1 &lt; i &lt; min{n, m+k}. Consequently, for 0 &lt;
i &lt; m−k−1, the length of ;i is 2k+2; for i = m−k (respectively, m−k+1, ... , m, ... , m+k),
the length of ;Ci is 2k + 1 (respectively, 2k, ... , k + 1,... ,1).
</bodyText>
<subsectionHeader confidence="0.594354">
Example 1
</subsectionHeader>
<bodyText confidence="0.967889">
Consider Figure 4 where P is chold and k = 2. Input hchold is translated into the vectors
</bodyText>
<equation confidence="0.999904333333333">
Z = ;(h, $$chol) = 000100
h = ;&amp;, $chold) = 010000
b = ;(h,chold) = 01000
�χ4 = ;&amp;, hold) = 0100
X5 = ;(l, old) = 010
�χ6 = ;(d, ld) = 01
</equation>
<bodyText confidence="0.999834">
The computation of the vectors xi for input W = w1 ... wn is based on a prelimi-
nary step in which we compute for each σ E E the vector 9(σ) := ;&amp;, $... $p1 ... pm)
(using k copies of $). The latter vectors are initialized in the form 9(w) := 0k+m. We
then compute for i = 1, ... , n the value 9(wi) := 9(wi)  |0k+i−110m−i. Here the sym-
bol  |denotes bitwise OR. Once we have obtained the values 9(wi), which are repre-
sented as arrays, the vectors ;Ci := ;&amp;i,pi−k ... pi ... pr) can be accessed in constant
time.
</bodyText>
<listItem confidence="0.6202785">
2. States. Henceforth, states of automata A(P,k) will be called positions. Recall
that a position is given by a base number and an exponent e, 0 &lt; e &lt; k represent-
</listItem>
<bodyText confidence="0.99863425">
ing the error count. By a symbolic triangular area, we mean a triangular area in
which “explicit” base numbers (like 1,2,...) in positions are replaced by “symbolic”
base numbers of a form described below. Two kinds of symbolic triangular areas
are used. A unique “I-area” represents all triangular areas of automata A(P,k) that
do not contain final positions. The “integer variable” I is used to abstract from pos-
sible base numbers i, 0 &lt; i &lt; m − k − 1. Furthermore, k + 1 “M-areas” are used
to represent triangular areas of automata A(P,k) that contain final positions. Vari-
able M is meant to abstract from concrete values of m, which differ for distinct P.
Symbolic base numbers are expressions of the form I, I + 1, I − 1, I + 2, I − 2... (I-
areas) or M, M − 1, M − 2,... (M-areas). The elements of the symbolic areas, which
are called symbolic positions, are symbolic base numbers together with exponents
indicating an error count. Details should become clear in Example 2. The use of ex-
pressions such as (I + 2)2 simply enables a convenient labeling of states of A∀(k)
(cf. Figure 6). Using this kind of labeling, it is easy to formulate a correspondence be-
tween derivations in automata A(P, k) and in A∀(k) (cf. properties C1 and C2 discussed
below).
</bodyText>
<subsectionHeader confidence="0.567846">
Example 2
</subsectionHeader>
<bodyText confidence="0.900899">
The symbolic triangular areas for bound k = 1 are
</bodyText>
<equation confidence="0.607050666666667">
(I-area) {I0, (I − 1)1,I1, (I + 1)1}
(First M-area) {M0, (M − 1)1,M1}
(Second M-area) {(M − 1)0, (M − 2)1, (M − 1)1,M1}
</equation>
<page confidence="0.338777">
460
</page>
<figure confidence="0.982056166666667">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
(I-2)2 (I-1)2
(I-1)1
I2
I1
I0
(I+1)2 (I+2)2
(I+1)1
(M-4)2 (M-3)2
(M-3)1
(M-2)2 (M-1)2
(M-2)1
(M-2)0
(M-1)1
(M-1)0
M2
M1
M0
</figure>
<figureCaption confidence="0.938139">
Figure 5
</figureCaption>
<bodyText confidence="0.975517357142857">
Symbolic triangular areas and symbolic final positions for bound k = 2 (cf. Example 2).
Symbolic final positions for k = 1 are M1, M0, and (M − 1)0. The symbolic triangular
areas for k = 2 are indicated in Figure 5, in which the ellipses around symbolic final
positions are rendered in boldface.
States of A∀(k) are merely subsets of symbolic triangular areas. Subsets containing
symbolic final positions are final states of A∀(k), and {I0} is the start state. A special
technique is used to reduce the number of states. Returning to automata of the form
A(P, k), it is simple to see that triangular areas often contain positions p = ge and
q = hf where p “subsumes” q in the following sense: If, for some fixed input rest U, it
is possible to reach a final position of A(P, k) starting from q and consuming U, then
we may also reach a final position starting from p using U. A corresponding notion of
subsumption can be defined for symbolic positions. States of A∀(k) are then defined
as subsets of symbolic triangular areas that are free of subsumption in the sense that a
symbolic position of a state is never subsumed by another position of the same state.
</bodyText>
<subsectionHeader confidence="0.444488">
Example 3
</subsectionHeader>
<bodyText confidence="0.988952818181818">
The states of automaton A∀(1) are shown in Figure 6. As a result of the above reduction
technique, the only state containing the symbolic position I0 is {I0}, the start state. Each
of the symbolic positions (I − 1)1,I1, (I + 1)1 is subsumed by I0.
3. Transition function. It remains to define the transition function δ∀ of A∀(k). We
describe only the basic idea. Imagine an automaton A(P, k), where the pattern P has
length m. Let W = w1 ... wn denote an input word. Let SPi denote the set of active
positions of A(P, k) that are reached after reading the ith symbol wi (1 &lt; i &lt; n). For
simplicity, we assume that in each set, all subsumed positions are erased. In A∀(k) we
have a parallel acceptance procedure in which we reach, say, state S∀i after reading
Z := ;&amp;i, pi−k · · · pi · · · pr), where r = min{m, i + k + 1}, as above, for 1 &lt; i &lt; n.
Transitions are defined in such a way that C1 and C2 hold:
</bodyText>
<equation confidence="0.730827">
C1. For all parallel sets SPi and S∀i of the two sequences
SP0 SP1 ... SPi ... SPn
S∀0 S∀1 ... S∀i ... S∀n
</equation>
<bodyText confidence="0.9422052">
the set SPi is obtained from S∀i by instantiating the letter I by i whenever
S∀i uses variable I and instantiating M by m in the other cases.
C2. Whenever SPi contains a final position, then S∀i is final.
Given properties C1 and C2, it follows immediately that A(P,k) accepts w1 ... wn iff
A∀(k) accepts x1, ... , ;Cn.
</bodyText>
<page confidence="0.5423">
461
</page>
<figure confidence="0.851489">
Computational Linguistics Volume 30, Number 4
</figure>
<figureCaption confidence="0.803348">
Figure 6
</figureCaption>
<bodyText confidence="0.676859">
The universal deterministic Levenshtein automaton A∀(1). See Example 4 for notation.
</bodyText>
<subsectionHeader confidence="0.370721">
Example 4
</subsectionHeader>
<bodyText confidence="0.9993554">
The universal deterministic automaton A∀(1) is shown in Figure 6. (Some redundant
transitions departing from nonfinal states S =~ {I0} and using vectors of length ≤ 3
have been omitted.) The symbol stands for either 1 or 0. Moreover, x( ) is shorthand
for χ~ or ;C. In order to illustrate the use of A∀(1), consider the pattern P of the form
chold. Input child is translated into the sequence
</bodyText>
<equation confidence="0.999689">
X1 = ;&amp;, $cho) = 0100
;2 = ;(h, chol) = 0100
�χ3 = ;(i, hold) = 0000
�χ4 = ;(l, old) = 010
;5 = ;(d, ld) = 01
</equation>
<bodyText confidence="0.999927875">
Starting from state {I0}, we successively reach {I0}, {I0}, {(I−1)1,I1}, {I1}, {M1}. Hence
child is accepted. In a similar way the input word cold is translated into the sequence
0100–0010–0010–001. Starting from {I0}, we successively reach {I0}, {(I−1)1,I1, (I+1)1},
{(I + 1)1}, {M1}. Hence cold is also accepted. Third, input hchold is translated into
the sequence 0010–1000–1000–100–10–1. We reach successively {(I − 1)1,I1, (I + 1)1},
{(I − 1)1}, {(I − 1)1}, {(I − 1)1}, {(I − 1)1}, {M1}. Hence hchold is accepted as well.
For larger values of k, the number of states of A∀(k) grows rapidly. A∀(2) has 50
nonfinal states and 40 final states. The automaton A∀(3) has 563 states. When we tried
</bodyText>
<figure confidence="0.99968984375">
1_(_)(_)
I-11
1
M1
_1
1_0(_)
_1_(_)
01
10(_)(_)
I1
1
_ _
0_1
11_(_)
01_(_)
_ _1_
001
I-11,I1
010(_)
_10_
I+11
_01
0_1_
1_
100(_)
1_1_
_10
01
1
_
I-11,I+11
001_
_01_
10
11
_1
101_
_11
M-11,M1
110(_)
011_
I1,I+11
_11_
011
_00(_)
I-11,I1,I+11
101
1_1
M-21,M1
11
111_
0
111
_01_
01
0
M-21,M-11,M1
_1
M0
_1_
M-10
I0
1
1
</figure>
<page confidence="0.516138">
462
</page>
<note confidence="0.525406">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<bodyText confidence="0.998230333333333">
to minimize the automata A∀(1), A∀(2), and A∀(3), we found that these three automata
are already minimal. However, we do not have a general proof that our construction
always leads to minimal automata.
</bodyText>
<sectionHeader confidence="0.949147" genericHeader="method">
5. Approximate Search in Dictionaries Using Universal Levenshtein Automata
</sectionHeader>
<bodyText confidence="0.9999785">
We now describe how to use the universal deterministic Levenshtein automaton A∀(k)
for approximate search for a pattern in a dictionary.
</bodyText>
<subsectionHeader confidence="0.996149">
5.1 Basic Correction Algorithm
</subsectionHeader>
<bodyText confidence="0.917590294117647">
Let D denote the background dictionary, and let P = p1 ... pm denote a given pattern.
Recall that we want to compute for some fixed bound k the set of all entries W E D
such that dL(P,W) &lt; k. We assume that D is implemented in the form of a determin-
istic finite-state automaton AD = (E, QD, qD0 , FD, δD), the dictionary automaton. Hence
L(AD) represents the set of all correct words.
Let A∀(k) = (P, Q∀, q∀0, F∀, δ∀) denote the universal deterministic Levenshtein au-
tomaton for bound k. We assume that we can access, for each symbol σ E E and
each index 1 &lt; i &lt; m + k, the characteristic vector ;&amp;, pi−k · · · pi · · · pr), where r =
min{m, i + k + 11, in constant time (cf. Section 4). We traverse the two automata A∀(k)
and AD in parallel, using a standard backtracking procedure. At each step, a symbol
σ read in AD representing the ith symbol of the current dictionary path is translated
into the bitvector ;&amp;, pi−k · · · pi · · · pr), r = min{m, i + k + 11, which is used as input for
A∀(k).
push (&lt;0,ε,qD0 ,q∀0&gt;);
while not empty(stack) do begin
pop (&lt;i, W, qD, q∀&gt;);
for σ in E do begin
</bodyText>
<equation confidence="0.9847284">
χ� := ;&amp;, pi−k ··· pi ··· pr);
qD1 := δD(qD,σ);
q∀1 := δ∀(q∀, �χ);
if (qD1 &lt;&gt; NIL) and (q∀1 &lt;&gt; NIL) then begin
W1 := concat(W,σ);
push(&lt;i + 1, W1, qD1 , q∀1 &gt;);
if (qD 1 E FD) and (q∀1 E F∀) then output(W1);
end;
end;
end;
</equation>
<bodyText confidence="0.999207888888889">
Starting with the pair of initial states (qD0 , q∀0 ), position i = 0, and the empty word
ε, each step of the traversal adds a new symbol σ E E to the actual word W and
leads from a pair of states (qD,q∀) E QD x Q∀ to (δD(qD,σ),δ∀(q∀, ;)). We proceed as
long as both components are distinct from the empty failure state,1 NIL. Whenever
a final state is reached in both automata, the actual word W is added to the output.
It is trivial to show that the list of all output words represents exactly the set of all
dictionary entries W such that dL(W, P) &lt; k.
The computational cost of the above algorithm is bounded by the size of the
dictionary automaton AD and depends on the bound k used. If k reaches the length of
</bodyText>
<table confidence="0.307445">
1 A failure state is a state q whose language LA(q) is empty.
463
Computational Linguistics Volume 30, Number 4
</table>
<bodyText confidence="0.997594">
the longest word in the dictionary, then in general (e.g., for the empty input word), the
algorithm will result in a complete traversal of AD. In practice, small bounds are used,
and only a small portion of AD will be visited. For bound 0, the algorithm validates
in time O(|P|) if the input pattern P is in the dictionary.
</bodyText>
<subsectionHeader confidence="0.999029">
5.2 Evaluation Results for Basic Correction Algorithm
</subsectionHeader>
<bodyText confidence="0.997734666666667">
Experimental results were obtained using a Bulgarian lexicon (BL) with 965,339 word
entries (average length 10.23 symbols), a German dictionary (GL) with 3,871,605
entries (dominated by compound nouns, average length 18.74 symbols), and a “lex-
icon” (TL) containing 1,200,073 bibliographic titles from the Bavarian National
Library (average length 47.64 symbols). The German dictionary and the title
dictionary are nonpublic. They were provided to us by Franz Guenthner and
the Bavarian National Library, respectively, for the tests we conducted. The
following table summarizes the dictionary automaton statistics for the three
dictionaries:
</bodyText>
<table confidence="0.946066285714286">
BL GL TL
Number of words 956,339 3,871,605 1,200,073
Automaton states 39,339 4,068,189 29,103,779
Automaton transitions 102,585 6,954,377 30,252,173
Size (bytes) 1,191,548 90,206,665 475,615,320
The basic correction algorithm was implemented in C and tested on a 1.6 GHz
Pentium IV machine under Linux.
</table>
<listItem confidence="0.851089333333333">
5.2.1 A Baseline. Before we present our evaluation results, we give a simplified base-
line. Let a garbled word W be given. In order to find all words from the dictionary
within Levenshtein distance k, we can use two simple methods:
1. For each dictionary word V, check whether dL(V, W) &lt; k.
2. For each string V such that dL(V,W) &lt; k, check whether V is in the
dictionary.
</listItem>
<bodyText confidence="0.95598175">
We consider input words W of length 10. Visiting a state in an automaton takes
about 0.1 µs. Using Method 1, the time needed to check whether dL(V, W) &lt; k for a
dictionary word V using the universal Levenshtein automaton can be estimated as 1
µs (a crude approximation). When using Method 2, we need about 1 µs for the dic-
tionary lookup of a word with 10 symbols. Assume that the alphabet has 30 symbols.
Given the input W, we have 639 strings within Levenshtein distance 1, about 400,000
strings within distance 2, and about 260,000,000 strings within distance 3. Assum-
ing that the dictionary has 1,000,000 words, we get the following table of correction
times:
Distance 1 Distance 2 Distance 3
Method 1 1,000 ms 1,000 ms 1,000 ms
Method 2 0.639 ms 400 ms 260,000 ms
</bodyText>
<subsubsectionHeader confidence="0.535888">
5.2.2 Correction with BL. To test the basic correction algorithm with the Bulgarian
</subsubsectionHeader>
<bodyText confidence="0.996557">
lexicon, we used a Bulgarian word list containing randomly introduced errors. In each
word, we introduced between zero and four randomly selected symbol substitutions,
</bodyText>
<page confidence="0.909258">
464
</page>
<note confidence="0.807655">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<bodyText confidence="0.936572">
insertions, or deletions. The number of test words created for each length is shown in
the following table:
</bodyText>
<figure confidence="0.3566695">
Length 3 4 5 6 7 8
0 words 3,563 11,066 27,196 53,763 90,202 128,620
Length 9 10 11 12 13 14
0 words 155,888 163,318 148,879 117,783 81,481 50,291
Length 15 16 17 18 19 20
0 words 28,481 15,079 8,048 4,350 2,526 1,422
</figure>
<bodyText confidence="0.983547166666667">
Table 1 lists the results of the basic correction algorithm using BL and standard
Levenshtein distance with bounds k = 1, 2, 3. Column 1 shows the length of the input
words. Column 2 (CT1) describes the average time needed for the parallel traversal of
the dictionary automaton and the universal Levenshtein automaton using Levenshtein
distance 1. The time needed to output the correction candidates is always included;
hence the column represents the total correction time. Column 3 (NC1) shows the
average number of correction candidates (dictionary words within the given distance
bound) per input word. (For k = 1, there are cases in which this number is below 1.
This shows that for some of the test words, no candidates were returned: These words
were too seriously corrupted for correction suggestions to be found within the given
distance bound.) Similarly Columns 4 (CT2) and 6 (CT3) yield, respectively, the total
correction times per word (averages) for distance bounds 2 and 3, and Columns 5
(NC2) and 7 (NC3) yield, respectively, the average number of correction candidates
per word for distance bounds 2 and 3. Again, the time needed to output all corrections
is included.
5.2.3 Correction with GL. To test the correction times when using the German lexicon,
we again created a word list with randomly introduced errors. The number of test
words of each particular length is shown in the following table:
Length 1–14 15–24 25–34 35–44 45–54 55–64
0 words 100,000 100,000 100,000 9,776 995 514
The average correction times and number of correction candidates for GL are sum-
marized in Table 2, which has the same arrangement of columns (with corresponding
interpretations) as Table 1.
5.2.4 Correction with TL. To test the correction times when using the title “lexicon,”
we again created a word list with randomly introduced errors. The number of test
words of each length is presented in the following table:
Length 1–14 15–24 25–34 35–44 45–54 55–64
0 words 91,767 244,449 215,094 163,425 121,665 80,765
Table 3 lists the results for correction with TL and standard Levenshtein distance
with bounds k = 1, 2, 3. The arrangement of columns is the same as for Table 1, with
corresponding interpretations.
5.2.5 Summary. For each of the three dictionaries, evaluation times strongly depend on
the tolerated number of edit operations. When fixing a distance bound, the length of
the input word does not have a significant influence. In many cases, correction works
faster for long input words, because the number of correction candidates decreases.
The large number of entries in GL leads to increased correction times.
</bodyText>
<page confidence="0.910953">
465
</page>
<table confidence="0.490619">
Computational Linguistics Volume 30, Number 4
</table>
<tableCaption confidence="0.996966">
Table 1
</tableCaption>
<table confidence="0.966374428571429">
Evaluation results for the basic correction algorithm, Bulgarian dictionary, standard
Levenshtein distance, and distance bounds k = 1, 2, 3. Times in milliseconds.
Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)
3 0.107 12.03 0.974 285.4 4.589 2983.2
4 0.098 8.326 1.048 192.1 5.087 2426.6
5 0.085 5.187 1.086 105.0 5.424 1466.5
6 0.079 4.087 0.964 63.29 5.454 822.77
7 0.079 3.408 0.853 40.95 5.426 466.86
8 0.081 3.099 0.809 30.35 5.101 294.84
9 0.083 2.707 0.824 22.36 4.631 187.12
10 0.088 2.330 0.794 16.83 4.410 121.73
11 0.088 1.981 0.821 12.74 4.311 81.090
12 0.088 1.633 0.831 9.252 4.277 51.591
13 0.089 1.337 0.824 6.593 4.262 31.405
14 0.089 1.129 0.844 4.824 4.251 19.187
15 0.089 0.970 0.816 3.748 4.205 12.337
16 0.087 0.848 0.829 3.094 4.191 9.1752
17 0.086 0.880 0.805 2.970 4.138 8.1250
18 0.087 0.809 0.786 2.717 4.117 7.1701
19 0.087 0.810 0.792 2.646 4.078 6.6544
20 0.091 0.765 0.795 2.364 4.107 5.7686
</table>
<tableCaption confidence="0.99192">
Table 2
</tableCaption>
<table confidence="0.895111111111111">
Evaluation results for the basic correction algorithm, German dictionary, standard Levenshtein
distance, and distance bounds k = 1, 2, 3. Times in milliseconds.
Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)
1–14 0.225 0.201 4.140 0.686 23.59 2.345
15–24 0.170 0.605 3.210 1.407 19.66 3.824
25–34 0.249 0.492 4.334 0.938 24.58 1.558
35–44 0.264 0.449 4.316 0.781 24.06 1.187
45–54 0.241 0.518 3.577 0.969 20.18 1.563
55–64 0.233 0.444 3.463 0.644 19.03 0.737
</table>
<tableCaption confidence="0.991997">
Table 3
</tableCaption>
<table confidence="0.952629111111111">
Evaluation results for the basic correction algorithm, title “lexicon,” standard Levenshtein
distance, and distance bounds k = 1, 2, 3. Times in milliseconds.
Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)
1–14 0.294 0.537 3.885 2.731 19.31 24.67
15–24 0.308 0.451 4.024 0.872 19.50 1.703
25–34 0.321 0.416 4.160 0.644 19.98 0.884
35–44 0.330 0.412 4.225 0.628 20.20 0.844
45–54 0.338 0.414 4.300 0.636 20.44 0.857
55–64 0.344 0.347 4.340 0.433 20.61 0.449
</table>
<page confidence="0.83523">
466
</page>
<note confidence="0.812052">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<sectionHeader confidence="0.932004" genericHeader="method">
6. Using Backwards Dictionaries for Filtering
</sectionHeader>
<bodyText confidence="0.9993544375">
In the related area of pattern matching in strings, various filtering methods have been
introduced that help to find portions of a given text in which an approximate match of
a given pattern P is not possible. (See Navarro [2001] and Navarro and Raffinot [2002]
for surveys). In this section, we show how one general method of this form (Wu
and Manber 1992; Myers 1994; Baeza-Yates and Navarro 1999; Navarro and Baeza-
Yates 1999) can be adapted to approximate search in a dictionary, improving the basic
correction algorithm.
For approximate text search, the crucial observation is the following: If the Leven-
shtein distance between a pattern P and a portion of text T&apos; does not exceed a given
bound k, and if we cut P into k + 1 disjoint pieces P1,. . . , Pk+1, then T&apos; must contain
at least one piece. Hence the search in text T can be started with an exact multipat-
tern search for {P1, ... , Pk+1}, which is much faster than approximate search for P.
When finding one of the pieces Pi in the text, the full pattern P is searched for (return-
ing now to approximate search) within a small neighborhood around the occurrence.
Generalizations of this idea rely on the following lemma (Myers 1994; Baeza-Yates and
Navarro 1999; Navarro and Raffinot 2002):
</bodyText>
<subsectionHeader confidence="0.462683">
Lemma 1
</subsectionHeader>
<bodyText confidence="0.991119666666667">
Let T&apos; match P with &lt; k errors. Let P be represented as the concatenation of j words
P1,. . . , Pj. Let a1,. . . , aj denote arbitrary integers, and define A = �ji=1 ai. Then, for
some i E {1, ... , j}, Pi matches a substring of T&apos; with &lt; Laik/AJ errors.2
In our experiments, which were limited to distance bounds k = 1, 2, 3, we used the
following three instances of the general idea. Let P denote an input pattern, and let W
denote an entry of the dictionary D. Assume we cut P into two pieces, representing it
in the form P = P1P2:
1. If dL(P, W) &lt; 3, then W can be represented in the form W = W1W2,
where we have the following mutually exclusive cases:
</bodyText>
<equation confidence="0.92146075">
(a) dL(P1, W1) = 0 and dL(P2, W2) &lt; 3
(b) 1 &lt; dL(P1, W1) &lt; 3 and dL(P2, W2) = 0
(c) dL(P1, W1) = 1 and 1 &lt; dL(P2, W2) &lt; 2
(d) dL(P1, W1) = 2 and dL(P2, W2) = 1
</equation>
<bodyText confidence="0.977104">
2. If dL(P, W) &lt; 2, then W can be represented in the form W = W1W2,
where we have the following mutually exclusive cases:
</bodyText>
<equation confidence="0.989603333333333">
(a) dL(P1, W1) = 0 and dL(P2, W2) &lt; 2
(b) dL(P2, W2) = 0 and 1 &lt; dL(P1, W1) &lt; 2
(c) dL(P1, W1) = 1 = dL(P2, W2)
</equation>
<bodyText confidence="0.980467">
3. If dL(P, W) &lt; 1, then W can be represented in the form W = W1W2,
where we have the following mutually exclusive cases:
</bodyText>
<figure confidence="0.276034">
(a) dL(P1, W1) = 0 and dL(P2, W2) &lt; 1
(b) dL(P1, W1) = 1 and dL(P2, W2) = 0
2 As usual, LrJ denotes the largest integer ≤ r.
467
Computational Linguistics Volume 30, Number 4
</figure>
<bodyText confidence="0.972195018181818">
In order to make use of these observations, we compute, given dictionary D, the back-
wards dictionary D−R := {W−R  |W ∈ D}.3 Dictionary D and backwards dictionary
D−R are compiled into deterministic finite-state automata AD and AD−R, respectively. If
the dictionary is infinite and directly given as a finite-state automaton AD, the automa-
ton AD−R may be computed using standard techniques from formal language theory.
Further steps depend on the bound k. We will describe only approximate search with
bound k = 3; the methods for bounds k = 1, 2 are similar.
Let P denote the given pattern. P is cut into two pieces P1, P2 of approximately the
same length. We compute P−R
2 and P−R
1 . We then start four subsearches, corresponding
to Cases (1a)–(1d) specified above.
For subsearch (1a), we first traverse AD using input P1. Let q denote the state that is
reached. Starting from q and the initial state {I0} of A∀(3), we continue with a parallel
traversal of AD and A∀(3). Transition symbols in AD are translated into input bitvectors
for A∀(3) by matching them against appropriate subwords of $$$P2, as described in
Section 5. The sequence of all transition labels of the actual paths in AD is stored as
usual. Whenever we reach a pair of final states, the current sequence—which includes
the prefix P1—is passed to the output. Clearly, each output sequence has the form
P1P2&apos;, where dL(P2,PZ) &lt; 3. Conversely, any dictionary word of this form is found
using subsearch (1a).
For subsearch (1b), we first traverse AD−R using P−R
2 . Let q denote the state that
is reached. Starting from q and the initial state {I0} of A∀(3), we continue with a
parallel traversal of AD−R and A∀(3). Transition symbols in AD−R are translated into
input bitvectors for A∀(3) by matching them against appropriate subwords of $$$P−R
1 .
Whenever we reach a pair of final states, the inversed sequence is passed to the output.
Clearly, each output sequence has the form P&apos;P2, where dL(P1, Pi) &lt; 3. Conversely, any
dictionary word of this form is found using a search of this form. For a given output,
a closer look at the final state S that is reached in A∀(3) may be used to exclude cases
in which P1 = Pi. (Simple details are omitted).
For subsearch (1c), we start with a parallel traversal of AD and A∀(1). Transition
symbols in AD are translated into input bitvectors for A∀(1) by matching them against
appropriate subwords of $P1. For each pair of states (q,S) that are reached, where S
represents a final state of A∀(1), we start a parallel traversal of AD and A∀(2), departing
from q and the initial state {I0} of A∀(2). Transition symbols in AD are translated
into input bitvectors for A∀(2) by matching them against appropriate subwords of
$$P2. Whenever we reach a pair of final states, the current sequence is passed to the
output. Clearly, each output sequence has the form P&apos;PZ, where dL(P1, Pi) &lt; 1 and
dL(P2, PZ) &lt; 2. Conversely, any dictionary word of this form is found using a search
of this form. A closer look at the final states that are respectively reached in A∀(1)
and A∀(2) may be used to exclude cases in which P1 = Pi or P2 = PZ. (Again, simple
details are omitted).
For subsearch (1d), we start with a parallel traversal of AD−R and A∀(1). Tran-
sition symbols in AD−R are translated into input bitvectors for A∀(1) by matching
them against appropriate subwords of $P−R
2 . For each pair of states (q, S) that are
reached, where S represents a final state of A∀(1), we start a parallel traversal of AD−R
and A∀(2), departing from q and the initial state {I0} of A∀(2). Transition symbols
in AD−R are translated into input bitvectors for A∀(2) by matching them against ap-
propriate subwords of $$P−R
1 . Whenever we reach a pair of final states, the inversed
sequence is passed to the output. Clearly, each output sequence has the form P&apos;PZ,
3 W−R denotes the reverse of W.
</bodyText>
<page confidence="0.881975">
468
</page>
<note confidence="0.872288">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<bodyText confidence="0.999933142857143">
where dL(P1, Pi) &lt; 2 and dL(P2, PZ) &lt; 1. Conversely, any word in the dictionary of
this form is found using a search of this form. A closer look at the final states that
are reached in A∀(1) and A∀(2) may be used to exclude cases where P2 = PZ or
dL(P1, Pi) &lt; 1, respectively. (Again, simple details are omitted).
It should be noted that the output sets obtained from the four subsearches (1a)–
(1d) are not necessarily disjoint, because a dictionary entry W may have more than
one partition W = W1W2 of the form described in cases (1a)–(1d).
</bodyText>
<subsectionHeader confidence="0.999447">
6.1 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.963999">
The following table summarizes the statistics of the automata for the three backwards
dictionaries:
</bodyText>
<table confidence="0.998375">
BL GL TL
Number of words 956,339 3,871,605 1,200,073
Automaton states 54,125 4,006,357 29,121,084
Automaton transitions 183,956 7,351,973 30,287,053
Size (bytes) 2,073,739 92,922,493 475,831,001
</table>
<bodyText confidence="0.999832090909091">
Note that the size of the backwards-dictionary automata is approximately the same
as the size of the dictionary automata.
Tables 4, 5, and 6 present the evaluation results for the backwards dictionary
filtering method using dictionaries BL, GL, and TL, respectively. We have constructed
additional automata for the backwards dictionaries.
For the tests, we used the same lists of input words as in Section 5.2 in order
to allow a direct comparison to the basic correction method. Dashes indicate that the
correction times were too small to be measured with sufficient confidence in their level
of precision. In columns 3, 5, and 7, we quantify the speedup factor, that is, the ratio
of the time taken by the basic algorithm to that taken by the backwards-dictionary
filtering method.
</bodyText>
<subsectionHeader confidence="0.998167">
6.2 Backwards-Dictionary Method for Levenshtein Distance with Transpositions
</subsectionHeader>
<bodyText confidence="0.998499529411765">
Universal Levenshtein automata can also be constructed for the modified Levenshtein
distance, in which character transpositions count as a primitive edit operation, along
with insertions, deletions, and substitutions. This kind of distance is preferable when
correcting typing errors. A generalization of the techniques presented by the authors
(Schulz and Mihov 2002) for modified Levenshtein distances—using either transpo-
sitions or merges and splits as additional edit operations—has been described in
Schulz and Mihov (2001). It is assumed that all edit operations are applied in par-
allel, which implies, for example, that insertions between transposed letters are not
possible.
If we want to apply the filtering method using backwards dictionaries for the
modified Levenshtein distance d�L(P, W) with transpositions, we are faced with the
following problem: Assume that the pattern P = a1a2 ... amam+1... an is split into P1 =
a1a2 ... am and P2 = am+1am+2 . . . an. When we apply the above procedure, the case in
which am and am+1 are transposed is not covered. In order to overcome this problem,
we can draw on the following observation:
If d�L(P, W) &lt; 3, then W can be represented in the form W = W1W2, where there
are seven alternatives, inlcuding the following four:
</bodyText>
<listItem confidence="0.7392895">
1. d�L(P1, W1) = 0 and d�L(P2, W2) &lt; 3
2. 1 &lt; d&apos;L(P1, W1) &lt; 3 and d&apos;L(P2,W2) = 0
</listItem>
<page confidence="0.817801">
469
</page>
<table confidence="0.506103">
Computational Linguistics Volume 30, Number 4
</table>
<tableCaption confidence="0.997999">
Table 4
</tableCaption>
<table confidence="0.955356181818182">
Evaluation results using the backwards-dictionary filtering method, Bulgarian dictionary, and
distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with
respect to basic algorithm.
Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 3
3 0.031 3.45 0.876 1.11 6.466 0.71
4 0.027 3.63 0.477 2.20 4.398 1.16
5 0.018 4.72 0.450 2.41 2.629 2.06
6 0.016 4.94 0.269 3.58 2.058 2.65
7 0.011 7.18 0.251 3.40 1.327 4.09
8 0.012 6.75 0.196 4.13 1.239 4.12
9 0.009 9.22 0.177 4.66 0.828 5.59
10 0.010 8.80 0.159 4.99 0.827 5.33
11 0.008 11.0 0.147 5.59 0.603 7.15
12 0.008 11.0 0.142 5.85 0.658 6.50
13 0.006 14.8 0.128 6.44 0.457 9.33
14 0.006 14.8 0.123 6.86 0.458 9.28
15 0.005 17.8 0.112 7.29 0.321 13.1
16 0.005 17.4 0.111 7.47 0.320 13.1
17 0.005 17.2 0.108 7.45 0.283 14.6
18 0.005 17.4 0.108 7.28 0.280 14.7
19 0.004 21.8 0.103 7.69 0.269 15.2
20 — — 0.105 7.57 0.274 15.0
</table>
<tableCaption confidence="0.99469">
Table 5
</tableCaption>
<table confidence="0.8908649">
Evaluation results using the backwards-dictionary filtering method, German dictionary, and
distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with
respect to basic algorithm.
Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 3
1–14 0.007 32.1 0.220 18.8 0.665 35.5
15–24 0.010 17.0 0.175 18.3 0.601 32.7
25–34 0.009 27.7 0.221 19.6 0.657 37.4
35–44 0.007 37.7 0.220 19.6 0.590 40.8
45–54 — — 0.201 17.8 0.452 44.6
55–64 — — 0.195 17.8 0.390 48.8
</table>
<equation confidence="0.6671795">
3. d�L(P1, W1) = 1 and 1 &lt; d�L(P2, W2) &lt; 2
4. d�L(P1, W1) = 2 and d�L(P2, W2) = 1
</equation>
<bodyText confidence="0.90883">
In the remaining three alternatives, W1 = Wiam+1 ends with the symbol am+1, and
W2 = amWZ starts with am. For P1 := a1a2 ... am−1 and PZ := am+2am+3 . . . an, we have
the following three alternatives:
</bodyText>
<listItem confidence="0.954431333333333">
5. d�L(Pi, Wi) = 0 and dL(PZ, WZ) &lt; 2
6. d&apos;L(P&apos;,Wi) = 1 and d&apos;L(PZ,WZ) &lt; 1
7. d&apos;L(P&apos;, Wi) = 2 and d�L(PZ, WZ) = 0
</listItem>
<bodyText confidence="0.986044">
The cases for distance bounds k = 1, 2 are solved using similar extensions of the
original subcase analysis. In each case, it is straightforward to realize a search proce-
dure with subsearches corresponding to the new subcase analysis, using an ordinary
dictionary and a backwards dictionary, generalizing the above ideas.
</bodyText>
<page confidence="0.836723">
470
</page>
<note confidence="0.949052">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<tableCaption confidence="0.993202">
Table 6
</tableCaption>
<table confidence="0.9015576">
Evaluation results using the backwards-dictionary filtering method, title “lexicon,” and
distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with
respect to basic algorithm.
Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 3
1–14 0.032 9.19 0.391 9.94 1.543 12.5
15–24 0.019 16.2 0.247 16.3 0.636 30.7
25–34 0.028 11.5 0.260 16.0 0.660 30.3
35–44 0.029 11.4 0.295 14.3 0.704 28.7
45–54 0.037 9.14 0.332 13.0 0.759 26.9
55–64 0.038 9.05 0.343 12.7 0.814 25.3
</table>
<tableCaption confidence="0.992125">
Table 7
</tableCaption>
<table confidence="0.821474818181818">
Evaluation results using the backwards-dictionary filtering method for the modified
Levenshtein distance di with transpositions, for German dictionary and title “lexicon,”
distance bound k = 3. Times in milliseconds and speedup factors (ratio of times) with respect
to basic algorithm.
Length (CT3 GL) Speedup GL (CT3 TL) Speedup TL
1–14 1.154 23.0 2.822 7.7
15–24 1.021 21.3 1.235 17.5
25–34 1.148 23.7 1.261 17.7
35–44 1.096 24.3 1.283 17.6
45–54 0.874 25.5 1.326 17.3
55–64 0.817 25.7 1.332 17.4
</table>
<bodyText confidence="0.978908454545455">
We have tested the new search procedure for the modified Levenshtein distance
d&apos; L. In Table 7 we present the experimental results with the German dictionary and the
title “lexicon” for distance bound k = 3.
6.2.1 Summary. The filtering method using backwards dictionaries drastically im-
proves correction times. The increase in speed depends both on the length of the
input word and on the error bound. The method works particularly well for long
input words. For GL, a drastic improvement can be observed for all subclasses. In
contrast, for very short words of BL, only a modest improvement is obtained. When
using BL and the modified Levenshtein distance d&apos; L with transpositions, the backwards-
dictionary method improved the basic search method only for words of length ≥ 9.
For short words, a large number of repetitions of the same correction candidates was
observed. The analysis of this problem is a point of future work.
Variants of the backwards-dictionary method also can be used for the Levenshtein
distance d&apos;i, in which insertions, deletions, substitutions, merges, and splits are treated
as primitive edit operations. Here, the idea is to split the pattern at two neighboring
positions, which doubles the number of subsearches. We did not evaluate this variant.
7. Using Dictionaries with Single Deletions for Filtering
The final technique that we describe here is again an adaptation of a filtering method
from pattern matching in strings (Muth and Manber 1996; Navarro and Raffinot 2002).
When restricted to the error bound k = 1, this method is very efficient. It can be used
only for finite dictionaries. Assume that the pattern P = p1 ... pm matches a portion of
text, T&apos;, with one error. Then m − 1 letters of P are found in T&apos; in the correct order.
</bodyText>
<page confidence="0.692301">
471
</page>
<note confidence="0.401761">
Computational Linguistics Volume 30, Number 4
</note>
<bodyText confidence="0.999865153846154">
This fact can be used to compute m+1 derivatives of P that are compared with similar
derivatives of a window T&apos; of length m that is slid over the text. A derivative of a word
V can be V or a word that is obtained by deleting exactly one letter of V. Coincidence
between derivatives of P and T&apos; can be used to detect approximate matches of P of
the above form. For details we refer to Navarro and Raffinot (2002). In what follows
we describe an adaptation of the method to approximate search of a pattern P in a
dictionary D.
Let i be an integer. With V[i] we denote the word that is obtained from a word
V by deleting the ith symbol of V. For |V |&lt; i, we define V[i] = V. By a dictionary
with output sets, we mean a list of strings W, each of which is associated with a set
of output strings O(W). Each string W is called a key. Starting from the conventional
dictionary D, we compute the following dictionaries with output sets Dall, D1, D2, ...,
Dn0, where n0 is the maximal length of an entry in D:
</bodyText>
<listItem confidence="0.9336138">
• The set of keys of Dall is D ∪ {V  |∃i ≥ 1, W ∈ D such that V = W[i]}. The
output set for key V is
Oall(V) := {W ∈ D  |W = V ∨ V = W[i] for some i ≥ 1}.
• The set of keys for Di is D ∪ {V  |∃W ∈ D such that V = W[i]}. The
output set for a key V is Oi(V) := {W ∈ D  |W = V ∨ V = W[i]}.
</listItem>
<subsectionHeader confidence="0.480704">
Lemma 2
</subsectionHeader>
<bodyText confidence="0.999972230769231">
Let P denote a pattern, and let W ∈ D. Then dL(P, W) ≤ 1 iff either W ∈ Oall(P) or
there exists i, 1 ≤ i ≤ |P|, such that W ∈ Oi(P[i]).
The proof of the lemma is simple and has therefore been omitted.
In our approach, the dictionaries with output sets Dall, D1, D2, ..., Dn0 are com-
piled, respectively, into minimal subsequential transducers Aall, A1, A2, ...,An0. Given a
pattern P, we compute the union of the output sets Oall(P),O1(P[1]),...,O|P|(P[|P|]) us-
ing these transducers. It follows from Lemma 2 that we obtain as result the set of all
entries W of D such that dL(P, W) ≤ 1. It should be noted that the output sets are not
necessarily disjoint. For example, if P itself is a dictionary entry, then P ∈ Oi(P[i]) for
all 1 ≤ i ≤ |P|.
After we implemented the above procedure for approximate search, we found that
a similar approach based on hashing had been described as early as 1981 in a technical
report by Mor and Fraenkel (1981).
</bodyText>
<subsectionHeader confidence="0.99399">
7.1 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999934714285714">
Table 8 presents the evaluation results for edit distance 1 using dictionaries with single
deletions obtained from BL. The total size of the constructed single-deletion dictionary
automata is 34.691 megabytes. The word lists used for tests are those described in
Section 5.2. GL and TL are not considered here, since the complete system of subdic-
tionaries needed turned out to be too large. For a small range of input words of length
3–6, filtering using dictionaries with single deletions behaves better than filtering using
the backwards-dictionary method.
</bodyText>
<sectionHeader confidence="0.919084" genericHeader="method">
8. Similarity Keys
</sectionHeader>
<bodyText confidence="0.999828">
A well-known technique for improving lexical search not mentioned so far is the use
of similarity keys. A similarity key is a mapping κ that assigns to each word W a sim-
plified representation κ(W). Similarity keys are used to group dictionaries into classes
</bodyText>
<page confidence="0.594639">
472
</page>
<note confidence="0.815748">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<bodyText confidence="0.999949980392157">
of “similar” entries. Many concrete notions of “similarity” have been considered, de-
pending on the application domain. Examples are phonetic similarity (e.g., SOUNDEX
system; cf. Odell and Russell [1918, 1922] and Davidson [1962]), similarity in terms of
word shape and geometric form (e.g., “envelope representation” [Sinha 1990; Anig-
bogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, and
Willett 1983; Owolabi and McGregor 1988). In order to search for a pattern P in the
dictionary, the “code” κ(P) is computed. The dictionary is organized in such a way
that we may efficiently retrieve all regions containing entries with code (similar to)
κ(P). As a result, only small parts of the dictionary must be visited, which speeds up
search. Many variants of this basic idea have been discussed in the literature (Kukich
1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995).
In our own experiments we first considered the following simple idea. Given a
similarity key κ, each entry W of dictionary D is equipped with an additional pre-
fix of the form κ(W)&amp;. Here &amp; is a special symbol that marks the border between
codes and original words. The enhanced dictionary Dˆ with all entries of the form
κ(W)&amp;W is compiled into a deterministic finite-state automaton AˆD. Approximate
search for pattern P in D is then reorganized in the following way. The enhanced
pattern κ(P)&amp;P is used for search in AˆD. We distinguish two phases in the backtrack-
ing process. In Phase 1, which ends when the special symbol &amp; is read, we compute
an initial path of AˆD in which the corresponding sequence of transition labels rep-
resents a code α such that dL(κ(P),α) &lt; k. All paths of this form are visited. Each
label sequence α&amp; of the above form defines a unique state q of the automaton AˆD
such that LAˆD(q) = {W E D I κ(W) = α}. In Phase 2, starting from q, we compute all
entries W with code κ(W) = α such that dL(W,P) &lt; k. In both phases the automa-
ton A∀(k) is used to control the search, and transition labels of AˆD are translated into
characteristic vectors. In order to guarantee completeness of the method, the distance
between codes of a pair of words should not exceed the distance between the words
themselves.
It is simple to see that in this method, the backtracking search is automatically
restricted to the subset of all dictionary entries V such that dL(κ(V), κ(P)) &lt; k. Unfor-
tunately, despite this, the approach does not lead to reduced search times. A closer
look at the structure of (conventional) dictionary automata AD for large dictionaries D
shows that there exists an enormous number of distinct initial paths of AD of length
3–5. During the controlled traversal of AD, most of the search time is spent visiting
paths of this initial “wall.” Clearly, most of these paths do not lead to any correction
candidate. Unfortunately, however, these “blind” paths are recognized too late. Using
the basic method described in Section 5, we have to overcome one single wall in AD for
the whole dictionary. In contrast, when integrating similarity keys in the above form,
we have to traverse a similar wall for the subdictionary Dκ(W) := {V E D I κ(V) =
κ(W)I for each code κ(W) found in Phase 1. Even if the sets Dκ(W) are usually much
smaller than D, the larger number of walls that are visited leads to increased traversal
times.
As an alternative, we tested a method in which we attached to each entry W of D all
prefixes of the form α&amp;, where α represents a possible code such that dL(κ(W), α) &lt; k.
Using a procedure similar to the one described above, we have to traverse only one
wall in Phase 2. With this method, we obtained a reduction in search time. However,
with this approach, enhanced dictionaries Dˆ are typically much larger than original
dictionaries D. Hence the method can be used only if both dictionary D and bound
k are not too large and if the key is not too fine. Since the method is not more effi-
cient than filtering using backwards dictionaries, evaluation results are not presented
here.
</bodyText>
<page confidence="0.796038">
473
</page>
<table confidence="0.48532">
Computational Linguistics Volume 30, Number 4
</table>
<tableCaption confidence="0.993284">
Table 8
</tableCaption>
<table confidence="0.978780666666667">
Results for BL, using dictionaries with single deletions for filtering, distance bound k = 1.
Times in milliseconds and speedup factors (ratio of times) with respect to basic algorithm.
Length (CT1) Speedup 1 Length (CT1) Speedup 1
3 0.011 9.73 12 0.026 3.38
4 0.011 8.91 13 0.028 3.18
5 0.010 8.50 14 0.033 2.70
6 0.011 7.18 15 0.035 2.54
7 0.013 6.08 16 0.039 2.23
8 0.015 5.40 17 0.044 1.95
9 0.017 4.88 18 0.052 1.67
10 0.020 4.40 19 0.055 1.58
11 0.022 4.00 20 0.063 1.44
</table>
<sectionHeader confidence="0.930139" genericHeader="conclusions">
9. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999768521739131">
In this article, we have shown how filtering methods can be used to improve finite-
state techniques for approximate search in large dictionaries. As a central contribution
we introduced a new correction method, filtering based on backwards dictionaries
and partitioned input patterns. Though this method generally leads to very short
correction times, we believe that correction times could possibly be improved further
using refinements and variants of the method or introducing other filtering methods.
There are, however, reasons to assume that we are not too far from a situation in
which further algorithmic improvements become impossible for fundamental reasons.
The following considerations show how an “optimal” correction time can be estimated
that cannot be improved upon without altering the hardware or using faster access
methods for automata.
We used a simple backtracking procedure to realize a complete traversal of the
dictionary automaton AD. During a first traversal, we counted the total number of
visits to any state. Since AD is not a tree, states may be passed through several times
during the complete traversal. Each such event counts as one visit to a state. The ratio
of the number of visits to the total number of symbols in the list of words D gives the
average number of visits per symbol, denoted v0. In practice, the value of v0 depends
on the compression rate that is achieved when compiling D into the automaton AD. It is
smaller than 1 because of numerous prefixes of dictionary words that are shared in AD.
We then used a second traversal of AD—not counting visits to states—to compute the
total traversal time. The ratio of the total traversal time to the number of visits yields
the time t0 that is needed for a single visit. For the three dictionaries, the following
values were obtained:
</bodyText>
<table confidence="0.923136666666667">
BL GL TL
Average number v0 of visits per symbol 0.1433 0.3618 0.7335
Average time t0 for one visit (in µs) 0.0918 0.1078 0.0865
</table>
<bodyText confidence="0.999459777777778">
Given an input V, we may consider the total number nV of symbols in the list of
correction candidates. Then nV·v0·t0 can be used to estimate the optimal correction time
for V. In fact, in order to achieve this correction time, we need an oracle that knows
how to avoid any kind of useless backtracking. Each situation in which we proceeded
on a dictionary path that does not lead to a correction candidate for V would require
some extra time that is not included in the above calculation. From another point of
view, the above idealized algorithm essentially just copies the correction candidates
into a resulting destination. The time that is consumed is proportional to the sum of
the length of the correction candidates.
</bodyText>
<page confidence="0.765203">
474
</page>
<note confidence="0.810536">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<bodyText confidence="0.9939632">
For each of the three dictionaries, we estimated the optimal correction time for
one class of input words. For BL we looked at input words of length 10. The average
number of correction candidates for Levenshtein distance 3 is 121.73 (cf. Table 1).
Assuming that the average length of correction candidates is 10, we obtain a total of
1,217.3 symbols in the complete set of all correction candidates. Hence the optimal
correction time is approximately
1217.3 · 0.0000918 ms · 0.1433 = 0.016 ms
The actual correction time using filtering with the backwards-dictionary method is
0.827 ms, which is 52 times slower.
For GL, we considered input words of length 15–24 and distance bound 3. We have
on average 3.824 correction candidates of length 20, that is, 76.48 symbols. Hence the
optimal correction time is approximately
76.48 · 0.0001078 ms · 0.3618 = 0.003 ms
The actual correction time using filtering with the backwards-dictionary method is
0.601 ms, which is 200 times slower.
For TL, we used input sequences of length 45–54 and again distance bound 3. We
have on average 0.857 correction candidates of length 50, that is, 42.85 symbols. Hence
the optimal correction time is approximately
42.85 · 0.0000865 ms · 0.7335 = 0.003 ms
The actual correction time using filtering with the backwards-dictionary method is
0.759 ms, which is 253 times slower.
These numbers coincide with our basic intuition that further algorithmic improve-
ments are simpler for dictionaries with long entries. For example, variants of the
backwards-dictionary method could be considered in which a finer subcase analysis
is used to improve filtering.
</bodyText>
<sectionHeader confidence="0.979059" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99986125">
This work was funded by a grant from
VolkswagenStiftung. The authors thank the
anonymous referees for many suggestions
that helped to improve the presentation.
</bodyText>
<sectionHeader confidence="0.962706" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99418040625">
Angell, Richard C., George E. Freund, and
Peter Willett. 1983. Automatic spelling
correction using a trigram similarity
measure. Information Processing and
Management, 19:255–261.
Anigbogu, Julain C. and Abdel Belaid. 1995.
Hidden Markov models in text
recognition. International Journal of Pattern
Recognition and Artificial Intelligence,
9(6):925–958.
Baeza-Yates, Ricardo A. and Gonzalo
Navarro. 1998. Fast approximative string
matching in a dictionary. In R. Werner,
editor, Proceedings SPIRE’98, pages 14–22.
IEEE Computer Science.
Baeza-Yates, Ricardo A. and Gonzalo
Navarro. 1999. Faster approximate string
matching. Algorithmica, 23(2):127–158.
Blair, Charles R. 1960. A program for
correcting spelling errors. Information and
Control, 3:60–67.
Bunke, Horst. 1993. A fast algorithm for
finding the nearest neighbor of a word in
a dictionary. In Proceedings of the Second
International Conference on Document
Analysis and Recognition (ICDAR’93), pages
632–637, Tsukuba, Japan.
Daciuk, Jan, Stoyan Mihov, Bruce W.
Watson, and Richard E. Watson. 2000.
Incremental construction of minimal
acyclic finite state automata.
Computational Linguistics, 26(1):3–16.
Davidson, Leon. 1962. Retrieval of
misspelled names in an airline passenger
record system. Communications of the ACM,
5(3):169–171.
475
Computational Linguistics Volume 30, Number 4
de Bertrand de Beuvron, Francois and
Philippe Trigano. 1995. Hierarchically
coded lexicon with variants. International
Journal of Pattern Recognition and Artificial
Intelligence, 9(1):145–165.
Dengel, Andreas, Rainer Hoch, Frank
H¨ones, Thorsten J¨ager, Michael Malburg,
and Achim Weigel. 1997. Techniques for
improving OCR results. In Horst Bunke
and Patrick S. P. Wang, editors, Handbook
of Character Recognition and Document Image
Analysis, 227–258. World Scientific.
Hopcroft, John E. and Jeffrey D. Ullman.
1979. Introduction to Automata Theory,
Languages, and Computation.
Addison-Wesley, Reading, MA.
Kim, Jong Yong and John Shawe-Taylor.
1992. An approximate string-matching
algorithm. Theoretical Computer Science,
92:107–117.
Kim, Jong Yong and John Shawe-Taylor.
1994. Fast string matching using an
n-gram algorithm. Software—Practice and
Experience, 94(1):79–88.
Kozen, Dexter C. 1997. Automata and
Computability. Springer.
Kukich, Karen. 1992. Techniques for
automatically correcting words in texts.
ACM Computing Surveys, 24:377–439.
Levenshtein, Vladimir I. 1966. Binary codes
capable of correcting deletions, insertions,
and reversals. Soviet Physics-Doklady,
10:707–710.
Mihov, Stoyan and Denis Maurel. 2001.
Direct construction of minimal acyclic
subsequential transducers. In S. Yu and
A. Pun, editors, Implementation and
Application of Automata: Fifth International
Conference (CIAA’2000) (Lecture Notes in
Computer Science no. 2088), pages
217–229. Springer.
Mor, Moshe and Aviezri S. Fraenkel. 1981.
A hash code method for detecting and
correcting spelling errors. Technical
Report CS81-03, Department of Applied
Mathematics, Weizmann Institute of
Science, Rehovot, Israel.
Muth, Robert and Udi Manber. 1996.
Approximate multiple string search. In
Proceedings of the Seventh Annual
Symposium on Combinatorical Pattern
Matching (Lecture Notes in Computer
Science, no. 1075) pages 75–86. Springer.
Myers, Eugene W. 1994. A sublinear
algorithm for approximate keyword
searching. Algorithmica, 12(4/5):345–374.
Navarro, Gonzalo. 2001. A guided tour to
approximate string matching. ACM
Computing Surveys, 33(1):31–88.
Navarro, Gonzalo and Ricardo A.
Baeza-Yates. 1999. Very fast and simple
approximate string matching. Information
Processing Letters, 72:65–70.
Navarro, Gonzalo and Mathieu Raffinot.
2002. Flexible Pattern Matching in Strings.
Cambridge University Press,
Cambridge.
Odell, Margaret K. and Robert C. Russell.
1918. U.S. Patent Number 1, 261, 167. U.S.
Patent Office, Washington, DC.
Odell, Margaret K. and Robert C. Russell.
1992. U.S. Patent Number 1,435,663. U.S.
Patent Office, Washington, DC.
Oflazer, Kemal. 1996. Error-tolerant
finite-state recognition with applications
to morphological analysis and spelling
correction. Computational Linguistics,
22(1):73–89.
Oommen, B. John and Richard K. S. Loke.
1997. Pattern recognition of strings with
substitutions, insertions, deletions, and
generalized transpositions. Pattern
Recognition, 30(5):789–800.
Owolabi, Olumide and D. R. McGregor.
1988. Fast approximate string matching.
Software—Practice and Experience,
18(4):387–393.
Riseman, Edward M. and Roger W. Ehrich.
1971. Contextual word recognition using
binary digrams. IEEE Transactions on
Computers, C-20(4):397–403.
Schulz, Klaus U. and Stoyan Mihov. 2001.
Fast string correction with
Levenshtein-automata. Technical Report
01-127, Centrum f¨ur Informations= und
sprachverarbeitung, University of
Munich.
Schulz, Klaus U. and Stoyan Mihov. 2002.
Fast string correction with
Levenshtein-automata. International
Journal of Document Analysis and
Recognition, 5(1):67–85.
Seni, Giovanni, V. Kripasundar, and
Rohini K. Srihari. 1996. Generalizing edit
distance to incorporate domain
information: Handwritten text recognition
as a case study. Pattern Recognition,
29(3):405–414.
Sinha, R. M. K. 1990. On partitioning a
dictionary for visual text recognition.
Pattern Recognition, 23(5):497–500.
Srihari, Sargur N. 1985. Computer Text
Recognition and Error Correction. Tutorial.
IEEE Computer Society Press, Silver
Spring, MD.
Srihari, Sargur N., Jonathan J. Hull, and
Ramesh Choudhari. 1983. Integrating
diverse knowledge sources in text
recognition. ACM Transactions on Office
Information Systems, 1(1):68–87.
Stephen, Graham A. 1994. String Searching
Algorithms. World Scientific, Singapore.
</reference>
<page confidence="0.826588">
476
</page>
<note confidence="0.726354">
Mihov and Schulz Fast Approximate Search in Large Dictionaries
</note>
<reference confidence="0.999735536585366">
Takahashi, Hiroyasu, Nobuyasu Itoh, Tomio
Amano, and Akio Yamashita. 1990. A
spelling correction method and its
application to an OCR system. Pattern
Recognition, 23(3/4):363–377.
Ukkonen, Esko. 1985a. Algorithms for
approximate string matching. Information
and Control, 64:100–118.
Ukkonen, Esko. 1985b. Finding approximate
patterns in strings. Journal of Algorithms,
6(1–3):132–137.
Ukkonen, Esko. 1992. Approximate
string-matching with q-grams and
maximal matches. Theoretical Computer
Science, 92:191–211.
Ullman, Jeffrey R. 1977. A binary n-gram
technique for automatic correction of
substitution, deletion, insertion and
reversal errors. Computer Journal,
20(2):141–147.
Wagner, Robert A. and Michael J. Fischer.
1974. The string-to-string correction
problem. Journal of the ACM, 21(1):168–173.
Weigel, Achim, Stephan Baumann, and
J. Rohrschneider. 1995. Lexical
postprocessing by heuristic search and
automatic determination of the edit costs.
In Proceedings of the Third International
Conference on Document Analysis and
Recognition (ICDAR 95), pages 857–860.
Wells, C. J., L. J. Evett, Paul E. Whitby, and
R.-J. Withrow. 1990. Fast dictionary
look-up for contextual word recognition.
Pattern Recognition, 23(5):501–508.
Wu, Sun and Udi Manber. 1992. Fast text
searching allowing errors. Communications
of the ACM, 35(10):83–91.
Zobel, Justin and Philip Dart. 1995. Finding
approximate matches in large lexicons.
Software—Practice and Experience,
25(3):331–345.
</reference>
<page confidence="0.935221">
477
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.720181">
<title confidence="0.9991335">Fast Approximate Search in Large Dictionaries</title>
<author confidence="0.981675">U Klaus</author>
<affiliation confidence="0.834517">Bulgarian Academy of Sciences University of Munich</affiliation>
<abstract confidence="0.989761916666667">The need to correct garbled strings arises in many areas of natural language processing. If a dictionary is available that covers all possible input tokens, a natural set of candidatesfor correcting erroneous input the set ofall words in the dictionaryfor which the Levenshtein distance to not exceed a given (small) bound In this article we describe methods for efficiently selecting such candidate sets. After introducing as a starting point a basic correction method based on the concept of a “universal Levenshtein automaton,” we show how two filtering methods known from the field of approximate text search can be used to improve the basic procedure in a significant way. The first method, which uses standard dictionaries plus dictionaries with reversed words, leads to very short correction times for most classes of input strings. Our evaluation results demonstrate that correction times for fixed-distance bounds depend on the expected number of correction candidates, which decreases for longer input words. Similarly the choice of an optimal filtering method depends on the length of the input words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard C Angell</author>
<author>George E Freund</author>
<author>Peter Willett</author>
</authors>
<title>Automatic spelling correction using a trigram similarity measure. Information Processing and Management,</title>
<date>1983</date>
<marker>Angell, Freund, Willett, 1983</marker>
<rawString>Angell, Richard C., George E. Freund, and Peter Willett. 1983. Automatic spelling correction using a trigram similarity measure. Information Processing and Management, 19:255–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julain C Anigbogu</author>
<author>Abdel Belaid</author>
</authors>
<title>Hidden Markov models in text recognition.</title>
<date>1995</date>
<journal>International Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>9</volume>
<issue>6</issue>
<contexts>
<context position="10143" citStr="Anigbogu and Belaid 1995" startWordPosition="1608" endWordPosition="1611">ased approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30</context>
<context position="64146" citStr="Anigbogu and Belaid 1995" startWordPosition="11558" endWordPosition="11562">mentioned so far is the use of similarity keys. A similarity key is a mapping κ that assigns to each word W a simplified representation κ(W). Similarity keys are used to group dictionaries into classes 472 Mihov and Schulz Fast Approximate Search in Large Dictionaries of “similar” entries. Many concrete notions of “similarity” have been considered, depending on the application domain. Examples are phonetic similarity (e.g., SOUNDEX system; cf. Odell and Russell [1918, 1922] and Davidson [1962]), similarity in terms of word shape and geometric form (e.g., “envelope representation” [Sinha 1990; Anigbogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988). In order to search for a pattern P in the dictionary, the “code” κ(P) is computed. The dictionary is organized in such a way that we may efficiently retrieve all regions containing entries with code (similar to) κ(P). As a result, only small parts of the dictionary must be visited, which speeds up search. Many variants of this basic idea have been discussed in the literature (Kukich 1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). In our own experiments we first considered</context>
</contexts>
<marker>Anigbogu, Belaid, 1995</marker>
<rawString>Anigbogu, Julain C. and Abdel Belaid. 1995. Hidden Markov models in text recognition. International Journal of Pattern Recognition and Artificial Intelligence, 9(6):925–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
<author>Gonzalo Navarro</author>
</authors>
<title>Fast approximative string matching in a dictionary. In</title>
<date>1998</date>
<booktitle>Proceedings SPIRE’98,</booktitle>
<pages>14--22</pages>
<editor>R. Werner, editor,</editor>
<contexts>
<context position="10970" citStr="Baeza-Yates and Navarro (1998)" startWordPosition="1735" endWordPosition="1738">e number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-state automata (FSA) are treated as tuples of the form A = (Σ, Q, q0, F, ∆), where Σ is the input alphabet, Q is the set of states, q0 E Q is the initial state, F is the set of final states, and ∆ C_ Q x Σε x Q is the transition relation. Here E denotes the empty string and Σε := Σ U {E}. The generalized transition </context>
</contexts>
<marker>Baeza-Yates, Navarro, 1998</marker>
<rawString>Baeza-Yates, Ricardo A. and Gonzalo Navarro. 1998. Fast approximative string matching in a dictionary. In R. Werner, editor, Proceedings SPIRE’98, pages 14–22.</rawString>
</citation>
<citation valid="false">
<institution>IEEE Computer Science.</institution>
<marker></marker>
<rawString>IEEE Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo A Baeza-Yates</author>
<author>Gonzalo Navarro</author>
</authors>
<title>Faster approximate string matching.</title>
<date>1999</date>
<journal>Algorithmica,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="19769" citStr="Baeza-Yates and Navarro 1999" startWordPosition="3455" endWordPosition="3458">ore-efficient methods for approximate search of a pattern P in a text T take as their starting point a simple nondeterministic finitestate automaton, AAST(P, k), which accepts the language of all words with Levenshtein 456 Mihov and Schulz Fast Approximate Search in Large Dictionaries t h i s _ c h i l d 0 0 0 0 0 0 0 0 0 0 0 c 1 1 1 1 1 1 0 1 1 1 1 h 2 2 1 2 2 2 1 0 1 2 2 o 3 3 2 2 3 3 2 1 1 2 3 l 4 4 3 3 3 4 3 2 2 1 2 d 5 5 4 4 4 4 4 3 3 2 1 Figure 2 Approximate search of pattern chold in a text using dynamic programming. distance &lt; k to some word in E∗·P (Ukkonen 1985b; Wu and Manber 1992; Baeza-Yates and Navarro 1999). The automaton for pattern chold and distance bound k = 2 is shown in Figure 3. States are numbered in the form be. The “base number” b determines the position of the state in the pattern. The “exponent” e indicates the error level, that is, the number of edit errors that have been observed. Horizontal transitions encode “normal” transitions in which the text symbol matches the expected next symbol of the pattern. Vertical transitions represent insertions, nonempty (respectively, empty) diagonal transitions represent substitutions (respectively, deletions). In the example shown in Figure 3, f</context>
<context position="46320" citStr="Baeza-Yates and Navarro 1999" startWordPosition="8360" endWordPosition="8363">30 0.412 4.225 0.628 20.20 0.844 45–54 0.338 0.414 4.300 0.636 20.44 0.857 55–64 0.344 0.347 4.340 0.433 20.61 0.449 466 Mihov and Schulz Fast Approximate Search in Large Dictionaries 6. Using Backwards Dictionaries for Filtering In the related area of pattern matching in strings, various filtering methods have been introduced that help to find portions of a given text in which an approximate match of a given pattern P is not possible. (See Navarro [2001] and Navarro and Raffinot [2002] for surveys). In this section, we show how one general method of this form (Wu and Manber 1992; Myers 1994; Baeza-Yates and Navarro 1999; Navarro and BaezaYates 1999) can be adapted to approximate search in a dictionary, improving the basic correction algorithm. For approximate text search, the crucial observation is the following: If the Levenshtein distance between a pattern P and a portion of text T&apos; does not exceed a given bound k, and if we cut P into k + 1 disjoint pieces P1,. . . , Pk+1, then T&apos; must contain at least one piece. Hence the search in text T can be started with an exact multipattern search for {P1, ... , Pk+1}, which is much faster than approximate search for P. When finding one of the pieces Pi in the text</context>
</contexts>
<marker>Baeza-Yates, Navarro, 1999</marker>
<rawString>Baeza-Yates, Ricardo A. and Gonzalo Navarro. 1999. Faster approximate string matching. Algorithmica, 23(2):127–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles R Blair</author>
</authors>
<title>A program for correcting spelling errors.</title>
<date>1960</date>
<journal>Information and Control,</journal>
<pages>3--60</pages>
<contexts>
<context position="10525" citStr="Blair (1960)" startWordPosition="1670" endWordPosition="1671"> on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the bas</context>
</contexts>
<marker>Blair, 1960</marker>
<rawString>Blair, Charles R. 1960. A program for correcting spelling errors. Information and Control, 3:60–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horst Bunke</author>
</authors>
<title>A fast algorithm for finding the nearest neighbor of a word in a dictionary.</title>
<date>1993</date>
<booktitle>In Proceedings of the Second International Conference on Document Analysis and Recognition (ICDAR’93),</booktitle>
<pages>632--637</pages>
<location>Tsukuba, Japan.</location>
<contexts>
<context position="10918" citStr="Bunke (1993)" startWordPosition="1730" endWordPosition="1731"> article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-state automata (FSA) are treated as tuples of the form A = (Σ, Q, q0, F, ∆), where Σ is the input alphabet, Q is the set of states, q0 E Q is the initial state, F is the set of final states, and ∆ C_ Q x Σε x Q is the transition relation. Here E denotes the empty s</context>
</contexts>
<marker>Bunke, 1993</marker>
<rawString>Bunke, Horst. 1993. A fast algorithm for finding the nearest neighbor of a word in a dictionary. In Proceedings of the Second International Conference on Document Analysis and Recognition (ICDAR’93), pages 632–637, Tsukuba, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Daciuk</author>
<author>Stoyan Mihov</author>
<author>Bruce W Watson</author>
<author>Richard E Watson</author>
</authors>
<title>Incremental construction of minimal acyclic finite state automata.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="13795" citStr="Daciuk et al. (2000)" startWordPosition="2326" endWordPosition="2329"> the following definition of A*: dq E Q (A*(q,E) = E) dq E QdUE Σ* da E Σ (A*(q,Ua) = A*(q, U)A(6*(q,U),a)) The input language of the transducer is L(T) := {U E Σ* |6*(q0,U) E F}. The subsequential transducer maps each word from the input language to a set of at most 454 Mihov and Schulz Fast Approximate Search in Large Dictionaries p output words. The output function OT : L(T) → 2Π∗ of the transducer is defined as follows: VU E L(T) (OT(U) = A*(q0,U) - Ψ(b*(q0,U))) By a dictionary, we mean a regular (finite or infinite) set of strings over a given alphabet E. Using the algorithm described by Daciuk et al. (2000), the minimal deterministic FSA AD accepting a finite dictionary D can be effectively computed. By a dictionary with output sets, we mean a regular (finite or infinite) set of input strings over a given alphabet together with a function that maps each of the input strings to a finite set of output strings. Given a finite dictionary with output sets, we can effectively compute, using the algorithm described by Mihov and Maurel (2001), the minimal subsequential transducer that maps each input string to its set of output strings. 3. Background In this section, we describe some established work th</context>
</contexts>
<marker>Daciuk, Mihov, Watson, Watson, 2000</marker>
<rawString>Daciuk, Jan, Stoyan Mihov, Bruce W. Watson, and Richard E. Watson. 2000. Incremental construction of minimal acyclic finite state automata. Computational Linguistics, 26(1):3–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Davidson</author>
</authors>
<title>Retrieval of misspelled names in an airline passenger record system.</title>
<date>1962</date>
<journal>Communications of the ACM,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="10031" citStr="Davidson 1962" startWordPosition="1593" endWordPosition="1594">ed k can be efficiently solved using this automaton. Since the method is closely related to a table-based approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudha</context>
</contexts>
<marker>Davidson, 1962</marker>
<rawString>Davidson, Leon. 1962. Retrieval of misspelled names in an airline passenger record system. Communications of the ACM, 5(3):169–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de Bertrand de Beuvron</author>
<author>Francois</author>
<author>Philippe Trigano</author>
</authors>
<title>Hierarchically coded lexicon with variants.</title>
<date>1995</date>
<journal>International Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>9</volume>
<issue>1</issue>
<marker>de Beuvron, Francois, Trigano, 1995</marker>
<rawString>de Bertrand de Beuvron, Francois and Philippe Trigano. 1995. Hierarchically coded lexicon with variants. International Journal of Pattern Recognition and Artificial Intelligence, 9(1):145–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Dengel</author>
<author>Rainer Hoch</author>
<author>Frank H¨ones</author>
<author>Thorsten J¨ager</author>
<author>Michael Malburg</author>
<author>Achim Weigel</author>
</authors>
<title>Techniques for improving OCR results.</title>
<date>1997</date>
<booktitle>In Horst Bunke and</booktitle>
<pages>227--258</pages>
<editor>Patrick S. P. Wang, editors,</editor>
<publisher>World Scientific.</publisher>
<marker>Dengel, Hoch, H¨ones, J¨ager, Malburg, Weigel, 1997</marker>
<rawString>Dengel, Andreas, Rainer Hoch, Frank H¨ones, Thorsten J¨ager, Michael Malburg, and Achim Weigel. 1997. Techniques for improving OCR results. In Horst Bunke and Patrick S. P. Wang, editors, Handbook of Character Recognition and Document Image Analysis, 227–258. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="11218" citStr="Hopcroft and Ullman (1979)" startWordPosition="1775" endWordPosition="1778">lett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-state automata (FSA) are treated as tuples of the form A = (Σ, Q, q0, F, ∆), where Σ is the input alphabet, Q is the set of states, q0 E Q is the initial state, F is the set of final states, and ∆ C_ Q x Σε x Q is the transition relation. Here E denotes the empty string and Σε := Σ U {E}. The generalized transition relation ∆ˆ is defined as the smallest subset of Q x Σ* x Q with the following closure properties: • For all q E Q we have (q, E, q) E ˆ∆. • For all q1, q2, q3 E Q and W1, W2 E Σ*, if (q1, W1, q2) E ∆ˆ and (q2, W2, q3) E ∆, then also (q1, W1W2, q3)</context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>Hopcroft, John E. and Jeffrey D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong Yong Kim</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An approximate string-matching algorithm.</title>
<date>1992</date>
<journal>Theoretical Computer Science,</journal>
<pages>92--107</pages>
<contexts>
<context position="3981" citStr="Kim and Shawe-Taylor 1992" startWordPosition="609" endWordPosition="612"> contains titles of articles, books, etc. The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the first step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned applications, the number of correction candidates becomes huge for large values of k. Hence small bounds are more realistic. In light of this background, the algorithmic problem discussed in the article can be described as follows: Given a pattern P, a dictio</context>
</contexts>
<marker>Kim, Shawe-Taylor, 1992</marker>
<rawString>Kim, Jong Yong and John Shawe-Taylor. 1992. An approximate string-matching algorithm. Theoretical Computer Science, 92:107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong Yong Kim</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Fast string matching using an n-gram algorithm.</title>
<date>1994</date>
<journal>Software—Practice and Experience,</journal>
<volume>94</volume>
<issue>1</issue>
<marker>Kim, Shawe-Taylor, 1994</marker>
<rawString>Kim, Jong Yong and John Shawe-Taylor. 1994. Fast string matching using an n-gram algorithm. Software—Practice and Experience, 94(1):79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dexter C Kozen</author>
</authors>
<title>Automata and Computability.</title>
<date>1997</date>
<publisher>Springer.</publisher>
<contexts>
<context position="11234" citStr="Kozen (1997)" startWordPosition="1780" endWordPosition="1781">d Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-state automata (FSA) are treated as tuples of the form A = (Σ, Q, q0, F, ∆), where Σ is the input alphabet, Q is the set of states, q0 E Q is the initial state, F is the set of final states, and ∆ C_ Q x Σε x Q is the transition relation. Here E denotes the empty string and Σε := Σ U {E}. The generalized transition relation ∆ˆ is defined as the smallest subset of Q x Σ* x Q with the following closure properties: • For all q E Q we have (q, E, q) E ˆ∆. • For all q1, q2, q3 E Q and W1, W2 E Σ*, if (q1, W1, q2) E ∆ˆ and (q2, W2, q3) E ∆, then also (q1, W1W2, q3) E ˆ∆. We write </context>
</contexts>
<marker>Kozen, 1997</marker>
<rawString>Kozen, Dexter C. 1997. Automata and Computability. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in texts.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<pages>24--377</pages>
<contexts>
<context position="10117" citStr="Kukich 1992" startWordPosition="1606" endWordPosition="1607"> to a table-based approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computati</context>
<context position="64639" citStr="Kukich 1992" startWordPosition="11644" endWordPosition="11645">arity in terms of word shape and geometric form (e.g., “envelope representation” [Sinha 1990; Anigbogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988). In order to search for a pattern P in the dictionary, the “code” κ(P) is computed. The dictionary is organized in such a way that we may efficiently retrieve all regions containing entries with code (similar to) κ(P). As a result, only small parts of the dictionary must be visited, which speeds up search. Many variants of this basic idea have been discussed in the literature (Kukich 1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). In our own experiments we first considered the following simple idea. Given a similarity key κ, each entry W of dictionary D is equipped with an additional prefix of the form κ(W)&amp;. Here &amp; is a special symbol that marks the border between codes and original words. The enhanced dictionary Dˆ with all entries of the form κ(W)&amp;W is compiled into a deterministic finite-state automaton AˆD. Approximate search for pattern P in D is then reorganized in the following way. The enhanced pattern κ(P)&amp;P is used for search in AˆD. We distingu</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Kukich, Karen. 1992. Techniques for automatically correcting words in texts. ACM Computing Surveys, 24:377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics-Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="3706" citStr="Levenshtein 1966" startWordPosition="571" endWordPosition="572">d words and phrases of an Internet search engine. It is used to determine the plausibility that a new query is correct and to suggest “repaired” queries when the answer set returned is empty. • The input is a query to some bibliographic search engine. The dictionary contains titles of articles, books, etc. The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the first step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the </context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Levenshtein, Vladimir I. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics-Doklady, 10:707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stoyan Mihov</author>
<author>Denis Maurel</author>
</authors>
<title>Direct construction of minimal acyclic subsequential transducers.</title>
<date>2001</date>
<booktitle>Implementation and Application of Automata: Fifth International Conference (CIAA’2000) (Lecture Notes in Computer Science no. 2088),</booktitle>
<pages>217--229</pages>
<editor>In S. Yu and A. Pun, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="14231" citStr="Mihov and Maurel (2001)" startWordPosition="2400" endWordPosition="2403">L(T) (OT(U) = A*(q0,U) - Ψ(b*(q0,U))) By a dictionary, we mean a regular (finite or infinite) set of strings over a given alphabet E. Using the algorithm described by Daciuk et al. (2000), the minimal deterministic FSA AD accepting a finite dictionary D can be effectively computed. By a dictionary with output sets, we mean a regular (finite or infinite) set of input strings over a given alphabet together with a function that maps each of the input strings to a finite set of output strings. Given a finite dictionary with output sets, we can effectively compute, using the algorithm described by Mihov and Maurel (2001), the minimal subsequential transducer that maps each input string to its set of output strings. 3. Background In this section, we describe some established work that is of help in understanding the remainder of the article from a nontechnical, conceptual point of view. After introducing the Levenshtein distance, we describe methods for computing the distance, for checking whether the distance between two words exceeds a given bound, and for approximate search for a pattern in a text. The similarities and differences described below between approximate search in a text, on the one hand, and ap</context>
</contexts>
<marker>Mihov, Maurel, 2001</marker>
<rawString>Mihov, Stoyan and Denis Maurel. 2001. Direct construction of minimal acyclic subsequential transducers. In S. Yu and A. Pun, editors, Implementation and Application of Automata: Fifth International Conference (CIAA’2000) (Lecture Notes in Computer Science no. 2088), pages 217–229. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Mor</author>
<author>Aviezri S Fraenkel</author>
</authors>
<title>A hash code method for detecting and correcting spelling errors.</title>
<date>1981</date>
<tech>Technical Report CS81-03,</tech>
<institution>Department of Applied Mathematics, Weizmann Institute of Science,</institution>
<location>Rehovot,</location>
<contexts>
<context position="7024" citStr="Mor and Fraenkel 1981" startWordPosition="1114" endWordPosition="1117">h problems for smaller bounds k&apos; &lt; k in D and D−R. As for the basic method, universal Levenshtein automata are used to control the search. Ignoring very short input words and correction bound k = 1, 452 Mihov and Schulz Fast Approximate Search in Large Dictionaries this approach leads to a drastic increase in speed. Hence the “backwards dictionary method” can be considered the central contribution of this article. The second refinement, which is only interesting for bound k = 1 and short input words, also uses a filtering method from the field of approximate text search (Muth and Manber 1996; Mor and Fraenkel 1981). In this approach, “dictionaries with single deletions” are used to reduce approximate search in a dictionary D with bound k = 1 to a conventional lookup technique for finite-state transducers. Dictionaries with single deletions are constructed by deleting the symbol at a fixed position n in all words of a given dictionary. For the basic method and the two refinements, detailed evaluation results are given for three dictionaries that differ in terms of the number and average length of entries: a dictionary of the Bulgarian language with 965,339 entries (average length 10.23 symbols), a dictio</context>
<context position="62863" citStr="Mor and Fraenkel (1981)" startWordPosition="11360" endWordPosition="11363">sequential transducers Aall, A1, A2, ...,An0. Given a pattern P, we compute the union of the output sets Oall(P),O1(P[1]),...,O|P|(P[|P|]) using these transducers. It follows from Lemma 2 that we obtain as result the set of all entries W of D such that dL(P, W) ≤ 1. It should be noted that the output sets are not necessarily disjoint. For example, if P itself is a dictionary entry, then P ∈ Oi(P[i]) for all 1 ≤ i ≤ |P|. After we implemented the above procedure for approximate search, we found that a similar approach based on hashing had been described as early as 1981 in a technical report by Mor and Fraenkel (1981). 7.1 Evaluation Results Table 8 presents the evaluation results for edit distance 1 using dictionaries with single deletions obtained from BL. The total size of the constructed single-deletion dictionary automata is 34.691 megabytes. The word lists used for tests are those described in Section 5.2. GL and TL are not considered here, since the complete system of subdictionaries needed turned out to be too large. For a small range of input words of length 3–6, filtering using dictionaries with single deletions behaves better than filtering using the backwards-dictionary method. 8. Similarity Ke</context>
</contexts>
<marker>Mor, Fraenkel, 1981</marker>
<rawString>Mor, Moshe and Aviezri S. Fraenkel. 1981. A hash code method for detecting and correcting spelling errors. Technical Report CS81-03, Department of Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Muth</author>
<author>Udi Manber</author>
</authors>
<title>Approximate multiple string search.</title>
<date>1996</date>
<booktitle>In Proceedings of the Seventh Annual Symposium on Combinatorical Pattern Matching (Lecture Notes in Computer Science,</booktitle>
<volume>1075</volume>
<pages>75--86</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7000" citStr="Muth and Manber 1996" startWordPosition="1110" endWordPosition="1113">k ≥ 1 to related search problems for smaller bounds k&apos; &lt; k in D and D−R. As for the basic method, universal Levenshtein automata are used to control the search. Ignoring very short input words and correction bound k = 1, 452 Mihov and Schulz Fast Approximate Search in Large Dictionaries this approach leads to a drastic increase in speed. Hence the “backwards dictionary method” can be considered the central contribution of this article. The second refinement, which is only interesting for bound k = 1 and short input words, also uses a filtering method from the field of approximate text search (Muth and Manber 1996; Mor and Fraenkel 1981). In this approach, “dictionaries with single deletions” are used to reduce approximate search in a dictionary D with bound k = 1 to a conventional lookup technique for finite-state transducers. Dictionaries with single deletions are constructed by deleting the symbol at a fixed position n in all words of a given dictionary. For the basic method and the two refinements, detailed evaluation results are given for three dictionaries that differ in terms of the number and average length of entries: a dictionary of the Bulgarian language with 965,339 entries (average length </context>
<context position="60283" citStr="Muth and Manber 1996" startWordPosition="10831" endWordPosition="10834">tes was observed. The analysis of this problem is a point of future work. Variants of the backwards-dictionary method also can be used for the Levenshtein distance d&apos;i, in which insertions, deletions, substitutions, merges, and splits are treated as primitive edit operations. Here, the idea is to split the pattern at two neighboring positions, which doubles the number of subsearches. We did not evaluate this variant. 7. Using Dictionaries with Single Deletions for Filtering The final technique that we describe here is again an adaptation of a filtering method from pattern matching in strings (Muth and Manber 1996; Navarro and Raffinot 2002). When restricted to the error bound k = 1, this method is very efficient. It can be used only for finite dictionaries. Assume that the pattern P = p1 ... pm matches a portion of text, T&apos;, with one error. Then m − 1 letters of P are found in T&apos; in the correct order. 471 Computational Linguistics Volume 30, Number 4 This fact can be used to compute m+1 derivatives of P that are compared with similar derivatives of a window T&apos; of length m that is slid over the text. A derivative of a word V can be V or a word that is obtained by deleting exactly one letter of V. Coinc</context>
</contexts>
<marker>Muth, Manber, 1996</marker>
<rawString>Muth, Robert and Udi Manber. 1996. Approximate multiple string search. In Proceedings of the Seventh Annual Symposium on Combinatorical Pattern Matching (Lecture Notes in Computer Science, no. 1075) pages 75–86. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene W Myers</author>
</authors>
<title>A sublinear algorithm for approximate keyword searching.</title>
<date>1994</date>
<journal>Algorithmica,</journal>
<pages>12--4</pages>
<contexts>
<context position="46290" citStr="Myers 1994" startWordPosition="8358" endWordPosition="8359">84 35–44 0.330 0.412 4.225 0.628 20.20 0.844 45–54 0.338 0.414 4.300 0.636 20.44 0.857 55–64 0.344 0.347 4.340 0.433 20.61 0.449 466 Mihov and Schulz Fast Approximate Search in Large Dictionaries 6. Using Backwards Dictionaries for Filtering In the related area of pattern matching in strings, various filtering methods have been introduced that help to find portions of a given text in which an approximate match of a given pattern P is not possible. (See Navarro [2001] and Navarro and Raffinot [2002] for surveys). In this section, we show how one general method of this form (Wu and Manber 1992; Myers 1994; Baeza-Yates and Navarro 1999; Navarro and BaezaYates 1999) can be adapted to approximate search in a dictionary, improving the basic correction algorithm. For approximate text search, the crucial observation is the following: If the Levenshtein distance between a pattern P and a portion of text T&apos; does not exceed a given bound k, and if we cut P into k + 1 disjoint pieces P1,. . . , Pk+1, then T&apos; must contain at least one piece. Hence the search in text T can be started with an exact multipattern search for {P1, ... , Pk+1}, which is much faster than approximate search for P. When finding on</context>
</contexts>
<marker>Myers, 1994</marker>
<rawString>Myers, Eugene W. 1994. A sublinear algorithm for approximate keyword searching. Algorithmica, 12(4/5):345–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Computing Surveys,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="22481" citStr="Navarro (2001)" startWordPosition="4014" endWordPosition="4015">pattern chold and distance bound k = 2. Active states after symbols t and h have been read are highlighted. 457 Computational Linguistics Volume 30, Number 4 Σ Σ Σ c h o l d 02 12 22 32 42 c h o l d 01 11 21 31 41 51 40 c l 00 10 h 20 o 30 d 50 ε ε Σ Σ Σ ε ΣΣ Σ Σ ε ΣΣ Σ Σ ε ε Σ Σ Σ Σ ε ε Σ Σ Σ Σ 52 2 0 1 pattern chold. Triangular areas are highlighted. Dark states are active after symbols h an d c have been read. k) might be difficult or impossible. In practice, simulation of determinism via bit-parallel computation of sets of active states gives rise to efficient and flexible algorithms. See Navarro (2001) and Navarro and Raffinot (2002) for surveys of algorithms along this li AAST(P, ne. ε ε Figure 4 Nondeterministic automaton A(chold,2) for testing Levenshtein distance with bound k = 2 for The direct use of the nondeterministic automaton AAST(P,k) for conducting approximate searches is inefficient. Furthermore, depending on the length m of the pattern and the error bound k, the explicit construction and storage of a deterministic version of 4. Testing Levenshtein Neighborhood with Universal Deterministic Levenshtein Automata In our approach, approximate search of a pattern P in a dictionary D</context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Navarro, Gonzalo. 2001. A guided tour to approximate string matching. ACM Computing Surveys, 33(1):31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
<author>Ricardo A Baeza-Yates</author>
</authors>
<title>Very fast and simple approximate string matching. Information Processing Letters,</title>
<date>1999</date>
<pages>72--65</pages>
<marker>Navarro, Baeza-Yates, 1999</marker>
<rawString>Navarro, Gonzalo and Ricardo A. Baeza-Yates. 1999. Very fast and simple approximate string matching. Information Processing Letters, 72:65–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
<author>Mathieu Raffinot</author>
</authors>
<title>Flexible Pattern Matching in Strings.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="22513" citStr="Navarro and Raffinot (2002)" startWordPosition="4017" endWordPosition="4020">istance bound k = 2. Active states after symbols t and h have been read are highlighted. 457 Computational Linguistics Volume 30, Number 4 Σ Σ Σ c h o l d 02 12 22 32 42 c h o l d 01 11 21 31 41 51 40 c l 00 10 h 20 o 30 d 50 ε ε Σ Σ Σ ε ΣΣ Σ Σ ε ΣΣ Σ Σ ε ε Σ Σ Σ Σ ε ε Σ Σ Σ Σ 52 2 0 1 pattern chold. Triangular areas are highlighted. Dark states are active after symbols h an d c have been read. k) might be difficult or impossible. In practice, simulation of determinism via bit-parallel computation of sets of active states gives rise to efficient and flexible algorithms. See Navarro (2001) and Navarro and Raffinot (2002) for surveys of algorithms along this li AAST(P, ne. ε ε Figure 4 Nondeterministic automaton A(chold,2) for testing Levenshtein distance with bound k = 2 for The direct use of the nondeterministic automaton AAST(P,k) for conducting approximate searches is inefficient. Furthermore, depending on the length m of the pattern and the error bound k, the explicit construction and storage of a deterministic version of 4. Testing Levenshtein Neighborhood with Universal Deterministic Levenshtein Automata In our approach, approximate search of a pattern P in a dictionary D is traced back to the problem o</context>
<context position="47172" citStr="Navarro and Raffinot 2002" startWordPosition="8513" endWordPosition="8516">between a pattern P and a portion of text T&apos; does not exceed a given bound k, and if we cut P into k + 1 disjoint pieces P1,. . . , Pk+1, then T&apos; must contain at least one piece. Hence the search in text T can be started with an exact multipattern search for {P1, ... , Pk+1}, which is much faster than approximate search for P. When finding one of the pieces Pi in the text, the full pattern P is searched for (returning now to approximate search) within a small neighborhood around the occurrence. Generalizations of this idea rely on the following lemma (Myers 1994; Baeza-Yates and Navarro 1999; Navarro and Raffinot 2002): Lemma 1 Let T&apos; match P with &lt; k errors. Let P be represented as the concatenation of j words P1,. . . , Pj. Let a1,. . . , aj denote arbitrary integers, and define A = �ji=1 ai. Then, for some i E {1, ... , j}, Pi matches a substring of T&apos; with &lt; Laik/AJ errors.2 In our experiments, which were limited to distance bounds k = 1, 2, 3, we used the following three instances of the general idea. Let P denote an input pattern, and let W denote an entry of the dictionary D. Assume we cut P into two pieces, representing it in the form P = P1P2: 1. If dL(P, W) &lt; 3, then W can be represented in the fo</context>
<context position="60311" citStr="Navarro and Raffinot 2002" startWordPosition="10835" endWordPosition="10838">analysis of this problem is a point of future work. Variants of the backwards-dictionary method also can be used for the Levenshtein distance d&apos;i, in which insertions, deletions, substitutions, merges, and splits are treated as primitive edit operations. Here, the idea is to split the pattern at two neighboring positions, which doubles the number of subsearches. We did not evaluate this variant. 7. Using Dictionaries with Single Deletions for Filtering The final technique that we describe here is again an adaptation of a filtering method from pattern matching in strings (Muth and Manber 1996; Navarro and Raffinot 2002). When restricted to the error bound k = 1, this method is very efficient. It can be used only for finite dictionaries. Assume that the pattern P = p1 ... pm matches a portion of text, T&apos;, with one error. Then m − 1 letters of P are found in T&apos; in the correct order. 471 Computational Linguistics Volume 30, Number 4 This fact can be used to compute m+1 derivatives of P that are compared with similar derivatives of a window T&apos; of length m that is slid over the text. A derivative of a word V can be V or a word that is obtained by deleting exactly one letter of V. Coincidence between derivatives o</context>
</contexts>
<marker>Navarro, Raffinot, 2002</marker>
<rawString>Navarro, Gonzalo and Mathieu Raffinot. 2002. Flexible Pattern Matching in Strings. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret K Odell</author>
<author>Robert C Russell</author>
</authors>
<date>1918</date>
<journal>U.S. Patent Number</journal>
<volume>1</volume>
<pages>261--167</pages>
<institution>U.S. Patent Office,</institution>
<location>Washington, DC.</location>
<marker>Odell, Russell, 1918</marker>
<rawString>Odell, Margaret K. and Robert C. Russell. 1918. U.S. Patent Number 1, 261, 167. U.S. Patent Office, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret K Odell</author>
<author>Robert C Russell</author>
</authors>
<date>1992</date>
<booktitle>U.S. Patent Number 1,435,663. U.S. Patent Office,</booktitle>
<location>Washington, DC.</location>
<marker>Odell, Russell, 1992</marker>
<rawString>Odell, Margaret K. and Robert C. Russell. 1992. U.S. Patent Number 1,435,663. U.S. Patent Office, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5967" citStr="Oflazer (1996)" startWordPosition="933" endWordPosition="934"> universal Levenshtein automata is closely related to a more complex table-based method described by the authors Schulz and Mihov (2002). Hence the main advantage of the new notion is its conceptual simplicity. In order to use the automaton for solving the above problem, we assume that the dictionary is given as a determininistic finite-state automaton. The basic method may then be described as a parallel backtracking traversal of the universal Levenshtein automaton and the dictionary automaton. Backtracking procedures of this form are well-known and have been used previously: for example, by Oflazer (1996) and the authors Schulz and Mihov (2002). For the first refinement of the basic method, a filtering method used in the field of approximate text search is adapted to the problem of approximate search in a dictionary. In this approach, an additional “backwards” dictionary D−R (representing the set of all reverses of the words of a given dictionary D) is used to reduce approximate search in D with a given bound k ≥ 1 to related search problems for smaller bounds k&apos; &lt; k in D and D−R. As for the basic method, universal Levenshtein automata are used to control the search. Ignoring very short input </context>
<context position="10934" citStr="Oflazer (1996)" startWordPosition="1732" endWordPosition="1733">ll-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-state automata (FSA) are treated as tuples of the form A = (Σ, Q, q0, F, ∆), where Σ is the input alphabet, Q is the set of states, q0 E Q is the initial state, F is the set of final states, and ∆ C_ Q x Σε x Q is the transition relation. Here E denotes the empty string and Σε := </context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Oflazer, Kemal. 1996. Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1):73–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B John Oommen</author>
<author>Richard K S Loke</author>
</authors>
<title>Pattern recognition of strings with substitutions, insertions, deletions, and generalized transpositions.</title>
<date>1997</date>
<journal>Pattern Recognition,</journal>
<volume>30</volume>
<issue>5</issue>
<contexts>
<context position="3859" citStr="Oommen and Loke 1997" startWordPosition="591" endWordPosition="594">ries when the answer set returned is empty. • The input is a query to some bibliographic search engine. The dictionary contains titles of articles, books, etc. The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the first step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned applications, the number of correction candidates becomes huge for large values of k. Hence small bounds are more realistic. In light of </context>
</contexts>
<marker>Oommen, Loke, 1997</marker>
<rawString>Oommen, B. John and Richard K. S. Loke. 1997. Pattern recognition of strings with substitutions, insertions, deletions, and generalized transpositions. Pattern Recognition, 30(5):789–800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olumide Owolabi</author>
<author>D R McGregor</author>
</authors>
<title>Fast approximate string matching.</title>
<date>1988</date>
<journal>Software—Practice and Experience,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="3758" citStr="Owolabi and McGregor 1988" startWordPosition="577" endWordPosition="580">ngine. It is used to determine the plausibility that a new query is correct and to suggest “repaired” queries when the answer set returned is empty. • The input is a query to some bibliographic search engine. The dictionary contains titles of articles, books, etc. The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the first step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned applications, the number of correcti</context>
<context position="10092" citStr="Owolabi and McGregor 1988" startWordPosition="1600" endWordPosition="1603">on. Since the method is closely related to a table-based approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1</context>
<context position="64247" citStr="Owolabi and McGregor 1988" startWordPosition="11574" endWordPosition="11577">h word W a simplified representation κ(W). Similarity keys are used to group dictionaries into classes 472 Mihov and Schulz Fast Approximate Search in Large Dictionaries of “similar” entries. Many concrete notions of “similarity” have been considered, depending on the application domain. Examples are phonetic similarity (e.g., SOUNDEX system; cf. Odell and Russell [1918, 1922] and Davidson [1962]), similarity in terms of word shape and geometric form (e.g., “envelope representation” [Sinha 1990; Anigbogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988). In order to search for a pattern P in the dictionary, the “code” κ(P) is computed. The dictionary is organized in such a way that we may efficiently retrieve all regions containing entries with code (similar to) κ(P). As a result, only small parts of the dictionary must be visited, which speeds up search. Many variants of this basic idea have been discussed in the literature (Kukich 1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). In our own experiments we first considered the following simple idea. Given a similarity key κ, each entry W of dictionary D is equipped with a</context>
</contexts>
<marker>Owolabi, McGregor, 1988</marker>
<rawString>Owolabi, Olumide and D. R. McGregor. 1988. Fast approximate string matching. Software—Practice and Experience, 18(4):387–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward M Riseman</author>
<author>Roger W Ehrich</author>
</authors>
<title>Contextual word recognition using binary digrams.</title>
<date>1971</date>
<journal>IEEE Transactions on Computers,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="10552" citStr="Riseman and Ehrich (1971)" startWordPosition="1672" endWordPosition="1675">ulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal langua</context>
</contexts>
<marker>Riseman, Ehrich, 1971</marker>
<rawString>Riseman, Edward M. and Roger W. Ehrich. 1971. Contextual word recognition using binary digrams. IEEE Transactions on Computers, C-20(4):397–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus U Schulz</author>
<author>Stoyan Mihov</author>
</authors>
<title>Fast string correction with Levenshtein-automata.</title>
<date>2001</date>
<tech>Technical Report 01-127,</tech>
<institution>University of Munich.</institution>
<contexts>
<context position="54723" citStr="Schulz and Mihov (2001)" startWordPosition="9860" endWordPosition="9863">filtering method. 6.2 Backwards-Dictionary Method for Levenshtein Distance with Transpositions Universal Levenshtein automata can also be constructed for the modified Levenshtein distance, in which character transpositions count as a primitive edit operation, along with insertions, deletions, and substitutions. This kind of distance is preferable when correcting typing errors. A generalization of the techniques presented by the authors (Schulz and Mihov 2002) for modified Levenshtein distances—using either transpositions or merges and splits as additional edit operations—has been described in Schulz and Mihov (2001). It is assumed that all edit operations are applied in parallel, which implies, for example, that insertions between transposed letters are not possible. If we want to apply the filtering method using backwards dictionaries for the modified Levenshtein distance d�L(P, W) with transpositions, we are faced with the following problem: Assume that the pattern P = a1a2 ... amam+1... an is split into P1 = a1a2 ... am and P2 = am+1am+2 . . . an. When we apply the above procedure, the case in which am and am+1 are transposed is not covered. In order to overcome this problem, we can draw on the follow</context>
</contexts>
<marker>Schulz, Mihov, 2001</marker>
<rawString>Schulz, Klaus U. and Stoyan Mihov. 2001. Fast string correction with Levenshtein-automata. Technical Report 01-127, Centrum f¨ur Informations= und sprachverarbeitung, University of Munich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus U Schulz</author>
<author>Stoyan Mihov</author>
</authors>
<title>Fast string correction with Levenshtein-automata.</title>
<date>2002</date>
<journal>International Journal of Document Analysis and Recognition,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="5489" citStr="Schulz and Mihov (2002)" startWordPosition="857" endWordPosition="860">rministic Levenshtein automaton of fixed degree k. The automaton of degree k may be used to decide, for arbitrary words U and V, whether the Levenshtein distance between U and V does not exceed k. The automaton is “universal” in the sense that it does not depend on U and V. The input of the automaton is a sequence of bitvectors computed from U and V. Though universal Levenshtein automata have not been discussed previously in the literature, determining Levenshtein neighborhood using universal Levenshtein automata is closely related to a more complex table-based method described by the authors Schulz and Mihov (2002). Hence the main advantage of the new notion is its conceptual simplicity. In order to use the automaton for solving the above problem, we assume that the dictionary is given as a determininistic finite-state automaton. The basic method may then be described as a parallel backtracking traversal of the universal Levenshtein automaton and the dictionary automaton. Backtracking procedures of this form are well-known and have been used previously: for example, by Oflazer (1996) and the authors Schulz and Mihov (2002). For the first refinement of the basic method, a filtering method used in the fie</context>
<context position="9582" citStr="Schulz and Mihov 2002" startWordPosition="1522" endWordPosition="1525">hat the idealized algorithm probably cannot be realized in practice. This article is structured as follows. In Section 2, we collect some formal preliminaries. In Section 3, we briefly summarize some known techniques from approximate string search in a text. In Section 4, we introduce universal deterministic Levenshtein automata of degree k and describe how the problem of deciding whether the Levenshtein distance between two strings P and W does not exceed k can be efficiently solved using this automaton. Since the method is closely related to a table-based approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de B</context>
<context position="54563" citStr="Schulz and Mihov 2002" startWordPosition="9838" endWordPosition="9841"> In columns 3, 5, and 7, we quantify the speedup factor, that is, the ratio of the time taken by the basic algorithm to that taken by the backwards-dictionary filtering method. 6.2 Backwards-Dictionary Method for Levenshtein Distance with Transpositions Universal Levenshtein automata can also be constructed for the modified Levenshtein distance, in which character transpositions count as a primitive edit operation, along with insertions, deletions, and substitutions. This kind of distance is preferable when correcting typing errors. A generalization of the techniques presented by the authors (Schulz and Mihov 2002) for modified Levenshtein distances—using either transpositions or merges and splits as additional edit operations—has been described in Schulz and Mihov (2001). It is assumed that all edit operations are applied in parallel, which implies, for example, that insertions between transposed letters are not possible. If we want to apply the filtering method using backwards dictionaries for the modified Levenshtein distance d�L(P, W) with transpositions, we are faced with the following problem: Assume that the pattern P = a1a2 ... amam+1... an is split into P1 = a1a2 ... am and P2 = am+1am+2 . . . </context>
</contexts>
<marker>Schulz, Mihov, 2002</marker>
<rawString>Schulz, Klaus U. and Stoyan Mihov. 2002. Fast string correction with Levenshtein-automata. International Journal of Document Analysis and Recognition, 5(1):67–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giovanni Seni</author>
<author>V Kripasundar</author>
<author>Rohini K Srihari</author>
</authors>
<title>Generalizing edit distance to incorporate domain information: Handwritten text recognition as a case study.</title>
<date>1996</date>
<journal>Pattern Recognition,</journal>
<volume>29</volume>
<issue>3</issue>
<marker>Seni, Kripasundar, Srihari, 1996</marker>
<rawString>Seni, Giovanni, V. Kripasundar, and Rohini K. Srihari. 1996. Generalizing edit distance to incorporate domain information: Handwritten text recognition as a case study. Pattern Recognition, 29(3):405–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M K Sinha</author>
</authors>
<title>On partitioning a dictionary for visual text recognition.</title>
<date>1990</date>
<journal>Pattern Recognition,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="10104" citStr="Sinha 1990" startWordPosition="1604" endWordPosition="1605">sely related to a table-based approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel </context>
<context position="64120" citStr="Sinha 1990" startWordPosition="11556" endWordPosition="11557"> search not mentioned so far is the use of similarity keys. A similarity key is a mapping κ that assigns to each word W a simplified representation κ(W). Similarity keys are used to group dictionaries into classes 472 Mihov and Schulz Fast Approximate Search in Large Dictionaries of “similar” entries. Many concrete notions of “similarity” have been considered, depending on the application domain. Examples are phonetic similarity (e.g., SOUNDEX system; cf. Odell and Russell [1918, 1922] and Davidson [1962]), similarity in terms of word shape and geometric form (e.g., “envelope representation” [Sinha 1990; Anigbogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988). In order to search for a pattern P in the dictionary, the “code” κ(P) is computed. The dictionary is organized in such a way that we may efficiently retrieve all regions containing entries with code (similar to) κ(P). As a result, only small parts of the dictionary must be visited, which speeds up search. Many variants of this basic idea have been discussed in the literature (Kukich 1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). In our own exper</context>
</contexts>
<marker>Sinha, 1990</marker>
<rawString>Sinha, R. M. K. 1990. On partitioning a dictionary for visual text recognition. Pattern Recognition, 23(5):497–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sargur N Srihari</author>
</authors>
<title>Computer Text Recognition and Error Correction.</title>
<date>1985</date>
<publisher>Tutorial. IEEE Computer Society Press, Silver Spring, MD.</publisher>
<contexts>
<context position="10656" citStr="Srihari (1985)" startWordPosition="1688" endWordPosition="1689">reund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-sta</context>
</contexts>
<marker>Srihari, 1985</marker>
<rawString>Srihari, Sargur N. 1985. Computer Text Recognition and Error Correction. Tutorial. IEEE Computer Society Press, Silver Spring, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sargur N Srihari</author>
<author>Jonathan J Hull</author>
<author>Ramesh Choudhari</author>
</authors>
<title>Integrating diverse knowledge sources in text recognition.</title>
<date>1983</date>
<journal>ACM Transactions on Office Information Systems,</journal>
<volume>1</volume>
<issue>1</issue>
<marker>Srihari, Hull, Choudhari, 1983</marker>
<rawString>Srihari, Sargur N., Jonathan J. Hull, and Ramesh Choudhari. 1983. Integrating diverse knowledge sources in text recognition. ACM Transactions on Office Information Systems, 1(1):68–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham A Stephen</author>
</authors>
<title>String Searching Algorithms. World Scientific,</title>
<date>1994</date>
<marker>Stephen, 1994</marker>
<rawString>Stephen, Graham A. 1994. String Searching Algorithms. World Scientific, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Takahashi</author>
<author>Nobuyasu Itoh</author>
<author>Tomio Amano</author>
<author>Akio Yamashita</author>
</authors>
<title>A spelling correction method and its application to an OCR system. Pattern Recognition,</title>
<date>1990</date>
<pages>23--3</pages>
<contexts>
<context position="10681" citStr="Takahashi et al. (1990)" startWordPosition="1690" endWordPosition="1693">tt 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-state automata (FSA) are tre</context>
</contexts>
<marker>Takahashi, Itoh, Amano, Yamashita, 1990</marker>
<rawString>Takahashi, Hiroyasu, Nobuyasu Itoh, Tomio Amano, and Akio Yamashita. 1990. A spelling correction method and its application to an OCR system. Pattern Recognition, 23(3/4):363–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esko Ukkonen</author>
</authors>
<title>Algorithms for approximate string matching.</title>
<date>1985</date>
<journal>Information and Control,</journal>
<pages>64--100</pages>
<contexts>
<context position="17364" citStr="Ukkonen (1985" startWordPosition="2980" endWordPosition="2981">d for initialization and yield, respectively, the first column and the first row. The third clause is used to compute the remaining entries. The table for the strings chold and hchold is shown in Figure 1. 3.2 Testing Levenshtein Neighborhood The algorithm of Wagner and Fischer, which has time complexity O(m · n), has been improved and generalized in many aspects. (See, for example, Stephen [1994] for a survey). We briefly sketch a more efficient variant that can be used for the restricted problem of deciding whether the Levenshtein distance between two words P and W exceeds a fixed bound, k. Ukkonen (1985a) shows that in this case only the values of 2k + 1 “diagonals” of TL(P, W) are essential for a test to make such a determination. Figure 1 illustrates the situation in which k = 2. Ukkonen obtained an algorithm with time complexity O(k · min(m,n)). He used the test for determining whether the Levenshtein distance between two words exceeds a given bound to derive an algorithm for computing the edit distance with complexity O(min(m,n) · dL(P,W)). 3.3 Approximate Search for a Pattern in a Text A problem closely related to approximate search in a dictionary is approximate search for a pattern in</context>
<context position="19717" citStr="Ukkonen 1985" startWordPosition="3449" endWordPosition="3450">3.3.2 Automaton Approach. Several more-efficient methods for approximate search of a pattern P in a text T take as their starting point a simple nondeterministic finitestate automaton, AAST(P, k), which accepts the language of all words with Levenshtein 456 Mihov and Schulz Fast Approximate Search in Large Dictionaries t h i s _ c h i l d 0 0 0 0 0 0 0 0 0 0 0 c 1 1 1 1 1 1 0 1 1 1 1 h 2 2 1 2 2 2 1 0 1 2 2 o 3 3 2 2 3 3 2 1 1 2 3 l 4 4 3 3 3 4 3 2 2 1 2 d 5 5 4 4 4 4 4 3 3 2 1 Figure 2 Approximate search of pattern chold in a text using dynamic programming. distance &lt; k to some word in E∗·P (Ukkonen 1985b; Wu and Manber 1992; Baeza-Yates and Navarro 1999). The automaton for pattern chold and distance bound k = 2 is shown in Figure 3. States are numbered in the form be. The “base number” b determines the position of the state in the pattern. The “exponent” e indicates the error level, that is, the number of edit errors that have been observed. Horizontal transitions encode “normal” transitions in which the text symbol matches the expected next symbol of the pattern. Vertical transitions represent insertions, nonempty (respectively, empty) diagonal transitions represent substitutions (respectiv</context>
</contexts>
<marker>Ukkonen, 1985</marker>
<rawString>Ukkonen, Esko. 1985a. Algorithms for approximate string matching. Information and Control, 64:100–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esko Ukkonen</author>
</authors>
<title>Finding approximate patterns in strings.</title>
<date>1985</date>
<journal>Journal of Algorithms,</journal>
<pages>6--1</pages>
<contexts>
<context position="17364" citStr="Ukkonen (1985" startWordPosition="2980" endWordPosition="2981">d for initialization and yield, respectively, the first column and the first row. The third clause is used to compute the remaining entries. The table for the strings chold and hchold is shown in Figure 1. 3.2 Testing Levenshtein Neighborhood The algorithm of Wagner and Fischer, which has time complexity O(m · n), has been improved and generalized in many aspects. (See, for example, Stephen [1994] for a survey). We briefly sketch a more efficient variant that can be used for the restricted problem of deciding whether the Levenshtein distance between two words P and W exceeds a fixed bound, k. Ukkonen (1985a) shows that in this case only the values of 2k + 1 “diagonals” of TL(P, W) are essential for a test to make such a determination. Figure 1 illustrates the situation in which k = 2. Ukkonen obtained an algorithm with time complexity O(k · min(m,n)). He used the test for determining whether the Levenshtein distance between two words exceeds a given bound to derive an algorithm for computing the edit distance with complexity O(min(m,n) · dL(P,W)). 3.3 Approximate Search for a Pattern in a Text A problem closely related to approximate search in a dictionary is approximate search for a pattern in</context>
<context position="19717" citStr="Ukkonen 1985" startWordPosition="3449" endWordPosition="3450">3.3.2 Automaton Approach. Several more-efficient methods for approximate search of a pattern P in a text T take as their starting point a simple nondeterministic finitestate automaton, AAST(P, k), which accepts the language of all words with Levenshtein 456 Mihov and Schulz Fast Approximate Search in Large Dictionaries t h i s _ c h i l d 0 0 0 0 0 0 0 0 0 0 0 c 1 1 1 1 1 1 0 1 1 1 1 h 2 2 1 2 2 2 1 0 1 2 2 o 3 3 2 2 3 3 2 1 1 2 3 l 4 4 3 3 3 4 3 2 2 1 2 d 5 5 4 4 4 4 4 3 3 2 1 Figure 2 Approximate search of pattern chold in a text using dynamic programming. distance &lt; k to some word in E∗·P (Ukkonen 1985b; Wu and Manber 1992; Baeza-Yates and Navarro 1999). The automaton for pattern chold and distance bound k = 2 is shown in Figure 3. States are numbered in the form be. The “base number” b determines the position of the state in the pattern. The “exponent” e indicates the error level, that is, the number of edit errors that have been observed. Horizontal transitions encode “normal” transitions in which the text symbol matches the expected next symbol of the pattern. Vertical transitions represent insertions, nonempty (respectively, empty) diagonal transitions represent substitutions (respectiv</context>
</contexts>
<marker>Ukkonen, 1985</marker>
<rawString>Ukkonen, Esko. 1985b. Finding approximate patterns in strings. Journal of Algorithms, 6(1–3):132–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esko Ukkonen</author>
</authors>
<title>Approximate string-matching with q-grams and maximal matches.</title>
<date>1992</date>
<journal>Theoretical Computer Science,</journal>
<pages>92--191</pages>
<contexts>
<context position="3954" citStr="Ukkonen 1992" startWordPosition="607" endWordPosition="608">The dictionary contains titles of articles, books, etc. The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the first step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned applications, the number of correction candidates becomes huge for large values of k. Hence small bounds are more realistic. In light of this background, the algorithmic problem discussed in the article can be described as follows: </context>
</contexts>
<marker>Ukkonen, 1992</marker>
<rawString>Ukkonen, Esko. 1992. Approximate string-matching with q-grams and maximal matches. Theoretical Computer Science, 92:191–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey R Ullman</author>
</authors>
<title>A binary n-gram technique for automatic correction of substitution, deletion, insertion and reversal errors.</title>
<date>1977</date>
<journal>Computer Journal,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="10567" citStr="Ullman (1977)" startWordPosition="1676" endWordPosition="1677">when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as de</context>
</contexts>
<marker>Ullman, 1977</marker>
<rawString>Ullman, Jeffrey R. 1977. A binary n-gram technique for automatic correction of substitution, deletion, insertion and reversal errors. Computer Journal, 20(2):141–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The string-to-string correction problem.</title>
<date>1974</date>
<journal>Journal of the ACM,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="3731" citStr="Wagner and Fischer 1974" startWordPosition="573" endWordPosition="576">s of an Internet search engine. It is used to determine the plausibility that a new query is correct and to suggest “repaired” queries when the answer set returned is empty. • The input is a query to some bibliographic search engine. The dictionary contains titles of articles, books, etc. The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the first step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned applicati</context>
<context position="15921" citStr="Wagner and Fischer (1974)" startWordPosition="2672" endWordPosition="2675">nother symbol, the deletion of a symbol, and the insertion of a symbol. Obviously, given two words W and V in the alphabet E, it is always possible to rewrite W into V using primitive edit operations. Definition 1 Let P, W be words in the alphabet E. The (standard) Levenshtein distance between P and W, denoted dL(P, W), is the minimal number of primitive edit operations (substitutions, deletions, insertions) that are needed to transform P into W. The Levenshtein distance between two words P and W can be computed using the following simple dynamic programming scheme, described, for example, by Wagner and Fischer (1974): dL(e, W) = |W| dL(P,e) = |P| dL(Pa, Wb) = �dL(P,W) if a = b 1 + min(dL(P, W), dL(Pa, W), dL(P, Wb)) if a =� b for P, W E E* and a, b E E. Given P = p1 ... pm and W = w1 ... wn (m, n &gt; 0), a standard way to apply the scheme is as follows: Proceeding top-down and from left to right, the cells of an (m + 1) x (n + 1) table TL(P,W) are filled, where entry (i, j) of T(P,W) is dL(p1 ... pi, w1 ... wj) (0 &lt; i &lt; m, 0 &lt; j &lt; n) (Wagner and Fischer 1974). The first two 455 Computational Linguistics Volume 30, Number 4 h c h o l d 0 1 2 3 4 5 6 c 1 1 1 2 3 4 5 h 2 1 2 1 2 3 4 o 3 2 2 2 1 2 3 l 4 3 3 3 2</context>
</contexts>
<marker>Wagner, Fischer, 1974</marker>
<rawString>Wagner, Robert A. and Michael J. Fischer. 1974. The string-to-string correction problem. Journal of the ACM, 21(1):168–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Achim Weigel</author>
<author>Stephan Baumann</author>
<author>J Rohrschneider</author>
</authors>
<title>Lexical postprocessing by heuristic search and automatic determination of the edit costs.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third International Conference on Document Analysis and Recognition (ICDAR 95),</booktitle>
<pages>857--860</pages>
<marker>Weigel, Baumann, Rohrschneider, 1995</marker>
<rawString>Weigel, Achim, Stephan Baumann, and J. Rohrschneider. 1995. Lexical postprocessing by heuristic search and automatic determination of the edit costs. In Proceedings of the Third International Conference on Document Analysis and Recognition (ICDAR 95), pages 857–860.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Wells</author>
<author>L J Evett</author>
<author>Paul E Whitby</author>
<author>R-J Withrow</author>
</authors>
<title>Fast dictionary look-up for contextual word recognition.</title>
<date>1990</date>
<journal>Pattern Recognition,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="10890" citStr="Wells et al. (1990)" startWordPosition="1724" endWordPosition="1727">n 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3. 2. Formal Preliminaries We assume that the reader is familiar with the basic notions of formal language theory as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual, finite-state automata (FSA) are treated as tuples of the form A = (Σ, Q, q0, F, ∆), where Σ is the input alphabet, Q is the set of states, q0 E Q is the initial state, F is the set of final states, and ∆ C_ Q x Σε x Q is the transition relation</context>
</contexts>
<marker>Wells, Evett, Whitby, Withrow, 1990</marker>
<rawString>Wells, C. J., L. J. Evett, Paul E. Whitby, and R.-J. Withrow. 1990. Fast dictionary look-up for contextual word recognition. Pattern Recognition, 23(5):501–508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sun Wu</author>
<author>Udi Manber</author>
</authors>
<title>Fast text searching allowing errors.</title>
<date>1992</date>
<journal>Communications of the ACM,</journal>
<volume>35</volume>
<issue>10</issue>
<contexts>
<context position="19738" citStr="Wu and Manber 1992" startWordPosition="3451" endWordPosition="3454"> Approach. Several more-efficient methods for approximate search of a pattern P in a text T take as their starting point a simple nondeterministic finitestate automaton, AAST(P, k), which accepts the language of all words with Levenshtein 456 Mihov and Schulz Fast Approximate Search in Large Dictionaries t h i s _ c h i l d 0 0 0 0 0 0 0 0 0 0 0 c 1 1 1 1 1 1 0 1 1 1 1 h 2 2 1 2 2 2 1 0 1 2 2 o 3 3 2 2 3 3 2 1 1 2 3 l 4 4 3 3 3 4 3 2 2 1 2 d 5 5 4 4 4 4 4 3 3 2 1 Figure 2 Approximate search of pattern chold in a text using dynamic programming. distance &lt; k to some word in E∗·P (Ukkonen 1985b; Wu and Manber 1992; Baeza-Yates and Navarro 1999). The automaton for pattern chold and distance bound k = 2 is shown in Figure 3. States are numbered in the form be. The “base number” b determines the position of the state in the pattern. The “exponent” e indicates the error level, that is, the number of edit errors that have been observed. Horizontal transitions encode “normal” transitions in which the text symbol matches the expected next symbol of the pattern. Vertical transitions represent insertions, nonempty (respectively, empty) diagonal transitions represent substitutions (respectively, deletions). In t</context>
<context position="46278" citStr="Wu and Manber 1992" startWordPosition="8354" endWordPosition="8357">.160 0.644 19.98 0.884 35–44 0.330 0.412 4.225 0.628 20.20 0.844 45–54 0.338 0.414 4.300 0.636 20.44 0.857 55–64 0.344 0.347 4.340 0.433 20.61 0.449 466 Mihov and Schulz Fast Approximate Search in Large Dictionaries 6. Using Backwards Dictionaries for Filtering In the related area of pattern matching in strings, various filtering methods have been introduced that help to find portions of a given text in which an approximate match of a given pattern P is not possible. (See Navarro [2001] and Navarro and Raffinot [2002] for surveys). In this section, we show how one general method of this form (Wu and Manber 1992; Myers 1994; Baeza-Yates and Navarro 1999; Navarro and BaezaYates 1999) can be adapted to approximate search in a dictionary, improving the basic correction algorithm. For approximate text search, the crucial observation is the following: If the Levenshtein distance between a pattern P and a portion of text T&apos; does not exceed a given bound k, and if we cut P into k + 1 disjoint pieces P1,. . . , Pk+1, then T&apos; must contain at least one piece. Hence the search in text T can be started with an exact multipattern search for {P1, ... , Pk+1}, which is much faster than approximate search for P. Whe</context>
</contexts>
<marker>Wu, Manber, 1992</marker>
<rawString>Wu, Sun and Udi Manber. 1992. Fast text searching allowing errors. Communications of the ACM, 35(10):83–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Zobel</author>
<author>Philip Dart</author>
</authors>
<title>Finding approximate matches in large lexicons.</title>
<date>1995</date>
<journal>Software—Practice and Experience,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="10164" citStr="Zobel and Dart 1995" startWordPosition="1612" endWordPosition="1615">y the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9. The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel 453 Computational Linguistics Volume 30, Number 4 and Dart (</context>
<context position="64660" citStr="Zobel and Dart 1995" startWordPosition="11646" endWordPosition="11649">s of word shape and geometric form (e.g., “envelope representation” [Sinha 1990; Anigbogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988). In order to search for a pattern P in the dictionary, the “code” κ(P) is computed. The dictionary is organized in such a way that we may efficiently retrieve all regions containing entries with code (similar to) κ(P). As a result, only small parts of the dictionary must be visited, which speeds up search. Many variants of this basic idea have been discussed in the literature (Kukich 1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). In our own experiments we first considered the following simple idea. Given a similarity key κ, each entry W of dictionary D is equipped with an additional prefix of the form κ(W)&amp;. Here &amp; is a special symbol that marks the border between codes and original words. The enhanced dictionary Dˆ with all entries of the form κ(W)&amp;W is compiled into a deterministic finite-state automaton AˆD. Approximate search for pattern P in D is then reorganized in the following way. The enhanced pattern κ(P)&amp;P is used for search in AˆD. We distinguish two phases in the</context>
</contexts>
<marker>Zobel, Dart, 1995</marker>
<rawString>Zobel, Justin and Philip Dart. 1995. Finding approximate matches in large lexicons. Software—Practice and Experience, 25(3):331–345.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>