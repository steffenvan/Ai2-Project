<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985626">
Modeling Topic Dependencies in Hierarchical Text Categorization
</title>
<author confidence="0.998387">
Alessandro Moschitti and Qi Ju Richard Johansson
</author>
<affiliation confidence="0.999954">
University of Trento University of Gothenburg
</affiliation>
<address confidence="0.53432">
38123 Povo (TN), Italy SE-405 30 Gothenburg, Sweden
</address>
<email confidence="0.996905">
{moschitti,qi}@disi.unitn.it richard.johansson@gu.se
</email>
<sectionHeader confidence="0.995592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998435">
In this paper, we encode topic dependencies
in hierarchical multi-label Text Categoriza-
tion (TC) by means of rerankers. We rep-
resent reranking hypotheses with several in-
novative kernels considering both the struc-
ture of the hierarchy and the probability of
nodes. Additionally, to better investigate the
role of category relationships, we consider two
interesting cases: (i) traditional schemes in
which node-fathers include all the documents
of their child-categories; and (ii) more gen-
eral schemes, in which children can include
documents not belonging to their fathers. The
extensive experimentation on Reuters Corpus
Volume 1 shows that our rerankers inject ef-
fective structural semantic dependencies in
multi-classifiers and significantly outperform
the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.998961" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993307">
Automated Text Categorization (TC) algorithms for
hierarchical taxonomies are typically based on flat
schemes, e.g., one-vs.-all, which do not take topic
relationships into account. This is due to two major
problems: (i) complexity in introducing them in the
learning algorithm and (ii) the small or no advan-
tage that they seem to provide (Rifkin and Klautau,
2004).
We speculate that the failure of using hierarchi-
cal approaches is caused by the inherent complexity
of modeling all possible topic dependencies rather
than the uselessness of such relationships. More pre-
cisely, although hierarchical multi-label classifiers
can exploit machine learning algorithms for struc-
tural output, e.g., (Tsochantaridis et al., 2005; Rie-
zler and Vasserman, 2010; Lavergne et al., 2010),
they often impose a number of simplifying restric-
tions on some category assignments. Typically, the
probability of a document d to belong to a subcate-
gory Ci of a category C is assumed to depend only
on d and C, but not on other subcategories of C,
or any other categories in the hierarchy. Indeed, the
introduction of these long-range dependencies lead
to computational intractability or more in general to
the problem of how to select an effective subset of
them. It is important to stress that (i) there is no
theory that can suggest which are the dependencies
to be included in the model and (ii) their exhaustive
explicit generation (i.e., the generation of all hierar-
chy subparts) is computationally infeasible. In this
perspective, kernel methods are a viable approach
to implicitly and easily explore feature spaces en-
coding dependencies. Unfortunately, structural ker-
nels, e.g., tree kernels, cannot be applied in struc-
tured output algorithms such as (Tsochantaridis et
al., 2005), again for the lack of a suitable theory.
In this paper, we propose to use the combination
of reranking with kernel methods as a way to han-
dle the computational and feature design issues. We
first use a basic hierarchical classifier to generate a
hypothesis set of limited size, and then apply rerank-
ing models. Since our rerankers are simple binary
classifiers of hypothesis pairs, they can encode com-
plex dependencies thanks to kernel methods. In par-
ticular, we used tree, sequence and linear kernels ap-
plied to structural and feature-vector representations
describing hierarchical dependencies.
Additionally, to better investigate the role of topi-
cal relationships, we consider two interesting cases:
(i) traditional categorization schemes in which node-
</bodyText>
<page confidence="0.979897">
759
</page>
<note confidence="0.9857775">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 759–767,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999790931034483">
fathers include all the documents of their child-
categories; and (ii) more general schemes, in which
children can include documents not belonging to
their fathers. The intuition under the above setting
is that shared documents between categories create
semantic links between them. Thus, if we remove
common documents between father and children, we
reduce the dependencies that can be captured with
traditional bag-of-words representation.
We carried out experiments on two entire hierar-
chies TOPICS (103 nodes organized in 5 levels) and
INDUSTRIAL (365 nodes organized in 6 levels) of
the well-known Reuters Corpus Volume 1 (RCV1).
We first evaluate the accuracy as well as the ef-
ficiency of several reranking models. The results
show that all our rerankers consistently and signif-
icantly improve on the traditional approaches to TC
up to 10 absolute percent points. Very interestingly,
the combination of structural kernels with the lin-
ear kernel applied to vectors of category probabil-
ities further improves on reranking: such a vector
provides a more effective information than the joint
global probability of the reranking hypothesis.
In the rest of the paper, Section 2 describes the hy-
pothesis generation algorithm, Section 3 illustrates
our reranking approach based on tree kernels, Sec-
tion 4 reports on our experiments, Section 5 illus-
trates the related work and finally Section 6 derives
the conclusions.
</bodyText>
<sectionHeader confidence="0.8850415" genericHeader="method">
2 Hierarchy classification hypotheses from
binary decisions
</sectionHeader>
<bodyText confidence="0.999992">
The idea of the paper is to build efficient models
for hierarchical classification using global depen-
dencies. For this purpose, we use reranking mod-
els, which encode global information. This neces-
sitates of a set of initial hypotheses, which are typ-
ically generated by local classifiers. In our study,
we used n one-vs.-all binary classifiers, associated
with the n different nodes of the hierarchy. In the
following sections, we describe a simple framework
for hypothesis generation.
</bodyText>
<subsectionHeader confidence="0.996155">
2.1 Top k hypothesis generation
</subsectionHeader>
<bodyText confidence="0.99975025">
Given n categories, C1, ... , Cn, we can define
p1 Ci(d) and p0Ci(d) as the probabilities that the clas-
sifier i assigns the document d to Ci or not, respec-
tively. For example, phCi(d) can be computed from
</bodyText>
<equation confidence="0.974345">
MCAT
M11 M12 M13 M14
M131 M132 M141 M142 M143
</equation>
<figureCaption confidence="0.873977">
Figure 1: A subhierarchy of Reuters.
</figureCaption>
<figure confidence="0.474067333333333">
MCAT
M11 -M12 M13 M14
-M131 -M132 -M141 -M142 M143
</figure>
<figureCaption confidence="0.980956">
Figure 2: A tree representing a category assignment hy-
pothesis for the subhierarchy in Fig. 1.
</figureCaption>
<bodyText confidence="0.999094">
the SVM classification output (i.e., the example mar-
gin). Typically, a large margin corresponds to high
probability for d to be in the category whereas small
margin indicates low probability1. Let us indicate
with h = {h1,.., hn} E {0,1}n a classification hy-
pothesis, i.e., the set of n binary decisions for a doc-
ument d. If we assume independence between the
SVM scores, the most probable hypothesis on d is
</bodyText>
<equation confidence="0.966495">
( )n
(d) = argmax ph i (d) i=1.
</equation>
<bodyText confidence="0.999203166666667">
Given h, the second best hypothesis can be ob-
tained by changing the label on the least probable
classification, i.e., associated with the index j =
(d). By storing the probability of the
k − 1 most probable configurations, the next k best
hypotheses can be efficiently generated.
</bodyText>
<sectionHeader confidence="0.973305" genericHeader="method">
3 Structural Kernels for Reranking
</sectionHeader>
<subsectionHeader confidence="0.537877">
Hierarchical Classification
</subsectionHeader>
<bodyText confidence="0.999863375">
In this section we describe our hypothesis reranker.
The main idea is to represent the hypotheses as a
tree structure, naturally derived from the hierarchy
and then to use tree kernels to encode such a struc-
tural description in a learning algorithm. For this
purpose, we describe our hypothesis representation,
kernel methods and the kernel-based approach to
preference reranking.
</bodyText>
<subsectionHeader confidence="0.999769">
3.1 Encoding hypotheses in a tree
</subsectionHeader>
<bodyText confidence="0.9989375">
Once hypotheses are generated, we need a represen-
tation from which the dependencies between the dif-
</bodyText>
<footnote confidence="0.9915505">
1We used the conversion of margin into probability provided
by LIBSVM.
</footnote>
<equation confidence="0.9700013125">
h = argmax
hE{0,1}n
hi
p
i
n
i=1
hE{0 ,1}
hi
pi
argmin
i=1,..,n
760
MCAT
M11 M13 M14
M143
</equation>
<figureCaption confidence="0.999988">
Figure 3: A compact representation of the hypothesis in
Fig. 2.
</figureCaption>
<bodyText confidence="0.998876588235294">
ferent nodes of the hierarchy can be learned. Since
we do not know in advance which are the important
dependencies and not even the scope of the interac-
tion between the different structure subparts, we rely
on automatic feature engineering via structural ker-
nels. For this paper, we consider tree-shaped hier-
archies so that tree kernels, e.g. (Collins and Duffy,
2002; Moschitti, 2006a), can be applied.
In more detail, we focus on the Reuters catego-
rization scheme. For example, Figure 1 shows a sub-
hierarchy of the Markets (MCAT) category and its
subcategories: Equity Markets (M11), Bond Mar-
kets (M12), Money Markets (M13) and Commod-
ity Markets (M14). These also have subcategories:
Interbank Markets (M131), Forex Markets (M132),
Soft Commodities (M141), Metals Trading (M142)
and Energy Markets (M143).
As the input of our reranker, we can simply use
a tree representing the hierarchy above, marking the
negative assignments of the current hypothesis in the
node labels with “-”, e.g., -M142 means that the doc-
ument was not classified in Metals Trading. For ex-
ample, Figure 2 shows the representation of a classi-
fication hypothesis consisting in assigning the target
document to the categories MCAT, M11, M13, M14
and M143.
Another more compact representation is the hier-
archy tree from which all the nodes associated with
a negative classification decision are removed. As
only a small subset of nodes of the full hierarchy will
be positively classified the tree will be much smaller.
Figure 3 shows the compact representation of the hy-
pothesis in Fig. 2. The next sections describe how to
exploit these kinds of representations.
</bodyText>
<subsectionHeader confidence="0.999372">
3.2 Structural Kernels
</subsectionHeader>
<bodyText confidence="0.9999166">
In kernel-based machines, both learning and classi-
fication algorithms only depend on the inner prod-
uct between instances. In several cases, this can be
efficiently and implicitly computed by kernel func-
tions by exploiting the following dual formulation:
</bodyText>
<equation confidence="0.6210355">
�
i=1..lyiαiO(oi)O(o) + b = 0, where oi and o are
</equation>
<bodyText confidence="0.981103333333333">
two objects, 0 is a mapping from the objects to fea-
ture vectors xi and 0(oi)0(o) = K(oi, o) is a ker-
nel function implicitly defining such a mapping. In
case of structural kernels, K determines the shape of
the substructures describing the objects above. The
most general kind of kernels used in NLP are string
kernels, e.g. (Shawe-Taylor and Cristianini, 2004),
the Syntactic Tree Kernels (Collins and Duffy, 2002)
and the Partial Tree Kernels (Moschitti, 2006a).
</bodyText>
<subsectionHeader confidence="0.963326">
3.2.1 String Kernels
</subsectionHeader>
<bodyText confidence="0.99994">
The String Kernels (SK) that we consider count
the number of subsequences shared by two strings
of symbols, s1 and s2. Some symbols during the
matching process can be skipped. This modifies
the weight associated with the target substrings as
shown by the following SK equation:
</bodyText>
<equation confidence="0.999302">
ESK(s1, s2) = Ou(s1) - Ou(s2) =
E E
uEE* Ad(~I1)+d(~I2)
~I1:u=s1[~I1] ~I2:u=s2[~I2]
</equation>
<bodyText confidence="0.956775730769231">
where, E* = U n=0 En is the set of all strings, h and
I2 are two sequences of indexes I = (i1, ..., i|u|),
with 1 &lt; i1 &lt; ... &lt; i|u |&lt; |s|, such that u = si,si|u|,
d( I� = i|u |− i1 + 1 (distance between the first and
last character) and A E [0, 1] is a decay factor.
It is worth noting that: (a) longer subsequences
receive lower weights; (b) some characters can be
omitted, i.e. gaps; (c) gaps determine a weight since
the exponent of A is the number of characters and
gaps between the first and last character; and (c)
the complexity of the SK computation is O(mnp)
(Shawe-Taylor and Cristianini, 2004), where m and
n are the lengths of the two strings, respectively and
p is the length of the largest subsequence we want to
consider.
In our case, given a hypothesis represented as
a tree like in Figure 2, we can visit it and derive
a linearization of the tree. SK applied to such
a node sequence can derive useful dependencies
between category nodes. For example, using the
Breadth First Search on the compact representa-
tion, we get the sequence [MCAT, M11, M13,
M14, M143], which generates the subsequences,
[MCAT, M11], [MCAT, M11, M13, M14],
[M11, M13, M143], [M11, M13, M143]
and so on.
</bodyText>
<equation confidence="0.715998">
E
uEE*
</equation>
<page confidence="0.89253">
761
</page>
<figureCaption confidence="0.998805">
Figure 4: The tree fragments of the hypothesis in Fig. 2
generated by STK
Figure 5: Some tree fragments of the hypothesis in Fig. 2
generated by PTK
</figureCaption>
<subsectionHeader confidence="0.515556">
3.2.2 Tree Kernels
</subsectionHeader>
<bodyText confidence="0.9910332">
Convolution Tree Kernels compute the number
of common substructures between two trees T1
and T2 without explicitly considering the whole
fragment space. For this purpose, let the set
T = If1, f2, ... , f|F|} be a tree fragment space and
χi(n) be an indicator function, equal to 1 if the
target fi is rooted at node n and equal to 0 oth-
erwise. A tree-kernel function over T1 and T2 is
TK(T1,T2) = En1∈NT1 En2∈NT2 O(n1,n2), NT1
and NT2 are the sets of the T1’s and T2’s nodes,
</bodyText>
<equation confidence="0.8833345">
respectively and O(n1, n2) = �|F|
i=1 χi(n1)χi(n2).
</equation>
<bodyText confidence="0.999664533333333">
The latter is equal to the number of common frag-
ments rooted in the n1 and n2 nodes. The O func-
tion determines the richness of the kernel space and
thus different tree kernels. Hereafter, we consider
the equation to evaluate STK and PTK.2
Syntactic Tree Kernels (STK) To compute STK,
it is enough to compute OSTK(n1, n2) as follows
(recalling that since it is a syntactic tree kernels, each
node can be associated with a production rule): (i)
if the productions at n1 and n2 are different then
OSTK(n1, n2) = 0; (ii) if the productions at n1
and n2 are the same, and n1 and n2 have only
leaf children then OSTK(n1, n2) = λ; and (iii) if
the productions at n1 and n2 are the same, and n1
and n2 are not pre-terminals then OSTK(n1, n2) =
</bodyText>
<equation confidence="0.941526">
λ jjl(n1)
j=1 (1 + OSTK(cjn1,cjn2)), where l(n1) is the
</equation>
<bodyText confidence="0.5372855">
2To have a similarity score between 0 and 1, a normalization
in the kernel space, i.e. TK(T1,T2) is applied.
</bodyText>
<equation confidence="0.397783">
NIT K(T1, T1) X TK(T2, T2)
</equation>
<bodyText confidence="0.995841142857143">
number of children of n1 and cjn is the j-th child
of the node n. Note that, since the productions
are the same, l(n1) = l(n2) and the computational
complexity of STK is O(|NT1||NT2|) but the aver-
age running time tends to be linear, i.e. O(|NT1 |+
|NT2|), for natural language syntactic trees (Mos-
chitti, 2006a; Moschitti, 2006b).
Figure 4 shows the five fragments of the hypothe-
sis in Figure 2. Such fragments satisfy the constraint
that each of their nodes includes all or none of its
children. For example, [M13 [-M131 -M132]] is an
STF, which has two non-terminal symbols, -M131
and -M132, as leaves while [M13 [-M131]] is not an
STF.
The Partial Tree Kernel (PTK) The compu-
tation of PTK is carried out by the following
OPTK function: if the labels of n1 and n2 are dif-
ferent then OPTK(n1, n2) = 0; else OPTK(n1, n2) =
where d( ~I1) = ~I1l( ~I1) ~I11 and d( ~I2) = ~I2l( ~I2) �
~I21. This way, we penalize both larger trees and
child subsequences with gaps. PTK is more gen-
eral than STK as if we only consider the contribu-
tion of shared subsequences containing all children
of nodes, we implement STK. The computational
complexity of PTK is O(pρ2|NT1||NT2|) (Moschitti,
2006a), where p is the largest subsequence of chil-
dren that we want consider and ρ is the maximal out-
degree observed in the two trees. However the aver-
age running time again tends to be linear for natural
language syntactic trees (Moschitti, 2006a).
Given a target T, PTK can generate any subset of
connected nodes of T, whose edges are in T. For
example, Fig. 5 shows the tree fragments from the
hypothesis of Fig. 2. Note that each fragment cap-
tures dependencies between different categories.
</bodyText>
<subsectionHeader confidence="0.999715">
3.3 Preference reranker
</subsectionHeader>
<bodyText confidence="0.999587875">
When training a reranker model, the task of the ma-
chine learning algorithm is to learn to select the best
candidate from a given set of hypotheses. To use
SVMs for training a reranker, we applied Preference
Kernel Method (Shen et al., 2003). The reduction
method from ranking tasks to binary classification is
an active research area; see for instance (Balcan et
al., 2008) and (Ailon and Mohri, 2010).
</bodyText>
<figure confidence="0.973809088235294">
MCAT
MCAT
MCAT
M11 -M12 M13 M14
M11 -M12 M13 M14
-M131
-M132
-M141
-M142
M143
-M131
M13
-M132
M14
M11 -M12 M13 M14
M143
-M142
-M141
MCAT MCAT
MCAT
MCAT
M11 -M12 M13 M14 M11 -M12 M13
M13
M13 M14
-M141 -M142
M14
MCAT
M13
MCAT
-M141 -M142 -M143 -M131 -M132 M11 M13
-M131
-M131
-M132
-M132
</figure>
<equation confidence="0.924479375">
( �
µ λ2 +
f1,�I2,l(�I1)=l(�I2)
)O � �(cn1(~I1j), cn2(~I2j))
l(i)
H
j=1
λd(F1)+d(F2)
</equation>
<page confidence="0.970139">
762
</page>
<table confidence="0.999854928571429">
Category Child-free Child-full
Train Train1 Train2 TEST Train Train1 Train2 TEST
C152 837 370 467 438 837 370 467 438
GPOL 723 357 366 380 723 357 366 380
M11 604 309 205 311 604 309 205 311
.. .. .. .. .. .. .. .. ..
C31 313 163 150 179 531 274 257 284
E41 191 89 95 102 223 121 102 118
GCAT 345 177 168 173 3293 1687 1506 1600
.. .. .. .. .. .. .. .. ..
E31 11 4 7 6 32 21 11 19
M14 96 49 47 58 1175 594 581 604
G15 5 4 1 0 290 137 153 146
Total: 103 10,000 5,000 5,000 5,000 10,000 5,000 5,000 5,000
</table>
<tableCaption confidence="0.924776333333333">
Table 1: Instance distributions of RCV1: the most populated categories are on the top, the medium sized ones follow
and the smallest ones are at the bottom. There are some difference between child-free and child-full setting since for
the former, from each node, we removed all the documents in its children.
</tableCaption>
<bodyText confidence="0.9995721875">
In the Preference Kernel approach, the reranking
problem – learning to pick the correct candidate h1
from a candidate set {h1, ... , hkI – is reduced to a
binary classification problem by creating pairs: pos-
itive training instances (h1, h2), ... , (h1, hk) and
negative instances (h2, h1), ... , (hk, h1). This train-
ing set can then be used to train a binary classifier.
At classification time, pairs are not formed (since the
correct candidate is not known); instead, the stan-
dard one-versus-all binarization method is still ap-
plied.
The kernels are then engineered to implicitly
represent the differences between the objects in
the pairs. If we have a valid kernel K over the
candidate space T , we can construct a preference
kernel PK over the space of pairs T x T as follows:
</bodyText>
<equation confidence="0.999447333333333">
PK(x, y) =//
PK((x1, x2), (y1, y2)) = K(x1, y1)+ (1)
K(x2, y2) − K(x1, y2) − K(x2, y1),
</equation>
<bodyText confidence="0.997194444444444">
where x, y E T x T . It is easy to show (Shen et al.,
2003) that PK is also a valid Mercer’s kernel. This
makes it possible to use kernel methods to train the
reranker.
We explore innovative kernels K to be used in
Eq. 1:
KJ = p(x1) x p(y1) + S, where p(·) is the global
joint probability of a target hypothesis and S is
a structural kernel, i.e., SK, STK and PTK.
</bodyText>
<equation confidence="0.963382">
KP = ~x1 · ~y1 + S, where ~x1={p(x1,j)IjEx1,
~y1 = {p(y1, j)IjEl1, p(t, n) is the classifica-
</equation>
<bodyText confidence="0.585009">
tion probability of the node (category) n in the
</bodyText>
<table confidence="0.992689333333333">
F1 BL BOL SK STK PTK
Micro-F1 0.769 0.771 0.786 0.790 0.790
Macro-F1 0.539 0.541 0.542 0.547 0.560
</table>
<tableCaption confidence="0.9949525">
Table 2: Comparison of rerankers using different kernels,
child-full setting (Kj model).
</tableCaption>
<table confidence="0.998674666666667">
F1 BL BOL SK STK PTK
Micro-F1 0.640 0.649 0.653 0.677 0.682
Macro-F1 0.408 0.417 0.431 0.447 0.447
</table>
<tableCaption confidence="0.99165">
Table 3: Comparison of rerankers using different kernels,
child-free setting (Kj model).
</tableCaption>
<bodyText confidence="0.979989166666667">
tree t E T and S is again a structural kernel,
i.e., SK, STK and PTK.
For comparative purposes, we also use for S a lin-
ear kernel over the bag-of-labels (BOL). This is
supposed to capture non-structural dependencies be-
tween the category labels.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999903777777778">
The aim of the experiments is to demonstrate that
our reranking approach can introduce semantic de-
pendencies in the hierarchical classification model,
which can improve accuracy. For this purpose, we
show that several reranking models based on tree
kernels improve the classification based on the fiat
one-vs.-all approach. Then, we analyze the effi-
ciency of our models, demonstrating their applica-
bility.
</bodyText>
<subsectionHeader confidence="0.982002">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999149">
We used two full hierarchies, TOPICS and INDUS-
TRY of Reuters Corpus Volume 1 (RCV1)3 TC cor-
</bodyText>
<page confidence="0.7407375">
3trec.nist.gov/data/reuters/reuters.html
763
</page>
<bodyText confidence="0.9896145">
pus. For most experiments, we randomly selected
two subsets of 10k and 5k of documents for train-
ing and testing from the total 804,414 Reuters news
from TOPICS by still using all the 103 categories
organized in 5 levels (hereafter SAM). The distri-
bution of the data instances of some of the dif-
ferent categories in such samples can be observed
in Table 1. The training set is used for learning
the binary classifiers needed to build the multiclass-
classifier (MCC). To compare with previous work
we also considered the Lewis’ split (Lewis et al.,
2004), which includes 23,149 news for training and
781,265 for testing.
Additionally, we carried out some experiments on
INDUSTRY data from RCV1. This contains 352,361
news assigned to 365 categories, which are orga-
nized in 6 levels. The Lewis’ split for INDUSTRY in-
cludes 9,644 news for training and 342,117 for test-
ing. We used the above datasets with two different
settings: the child-free setting, where we removed
all the document belonging to the child nodes from
the parent nodes, and the normal setting which we
refer to as child-full.
To implement the baseline model, we applied the
state-of-the-art method used by (Lewis et al., 2004)
for RCV1, i.e.,: SVMs with the default parameters
(trade-off and cost factor = 1), linear kernel, normal-
ized vectors, stemmed bag-of-words representation,
log(TF + 1) x IDF weighting scheme and stop
list4. We used the LIBSVM5 implementation, which
provides a probabilistic outcome for the classifica-
tion function. The classifiers are combined using the
one-vs.-all approach, which is also state-of-the-art
as argued in (Rifkin and Klautau, 2004). Since the
task requires us to assign multiple labels, we simply
collect the decisions of the n classifiers: this consti-
tutes our MCC baseline.
Regarding the reranker, we divided the training
set in two chunks of data: Train1 and Train2. The
binary classifiers are trained on Train1 and tested on
Train2 (and vice versa) to generate the hypotheses
on Train2 (Train1). The union of the two sets con-
stitutes the training data for the reranker. We imple-
4We have just a small difference in the number of tokens,
i.e., 51,002 vs. 47,219 but this is both not critical and rarely
achievable because of the diverse stop lists or tokenizers.
</bodyText>
<footnote confidence="0.988113">
5http://www.csie.ntu.edu.tw/˜cjlin/
libsvm/
</footnote>
<table confidence="0.965686">
BL (Child free)
RR (Child-free)
FRR (Child-free)
2 7 12 17 22 27 32
Training Data Size (thousands of instances)
</table>
<figureCaption confidence="0.976944666666667">
Figure 6: Learning curves of the reranking models using
STK in terms of MicroAverage-F1, according to increas-
ing training set (child-free setting).
</figureCaption>
<figure confidence="0.7310518">
BL (Child-free)
RR (Child-free)
FRR (Child-free)
2 7 12 17 22 27 32
Training Data Size (thousands of instances)
</figure>
<figureCaption confidence="0.999012666666667">
Figure 7: Learning curves of the reranking models using
STK in terms of MacroAverage-F1, according to increas-
ing training set (child-free setting).
</figureCaption>
<bodyText confidence="0.999511411764706">
mented two rerankers: RR, which use the represen-
tation of hypotheses described in Fig. 2; and FRR,
i.e., fast RR, which uses the compact representation
described in Fig. 3.
The rerankers are based on SVMs and the Prefer-
ence Kernel (PK) described in Sec. 1 built on top of
SK, STK or PTK (see Section 3.2.2). These are ap-
plied to the tree-structured hypotheses. We trained
the rerankers using SVM-light-TK6, which enables
the use of structural kernels in SVM-light (Joachims,
1999). This allows for applying kernels to pairs of
trees and combining them with vector-based kernels.
Again we use default parameters to facilitate replica-
bility and preserve generality. The rerankers always
use 8 best hypotheses.
All the performance values are provided by means
of Micro- and Macro-Average F1, evaluated on test
</bodyText>
<figure confidence="0.777290875">
6disi.unitn.it/moschitti/Tree-Kernel.htm
Micro-F1 0.676
0.666
0.656
0.646
0.636
0.626
Macro-F1 0.445
0.435
0.425
0.415
0.405
0.395
0.385
0.375
0.365
</figure>
<page confidence="0.840096">
764
</page>
<table confidence="0.9985116875">
Cat. Child-free Child-full
BL Kj Kp BL Kj Kp
C152 0.671 0.700 0.771 0.671 0.729 0.745
GPOL 0.660 0.695 0.743 0.660 0.680 0.734
M11 0.851 0.891 0.901 0.851 0.886 0.898
.. .. .. .. .. .. ..
C31 0.225 0.311 0.446 0.356 0.421 0.526
E41 0.643 0.714 0.719 0.776 0.791 0.806
GCAT 0.896 0.908 0.917 0.908 0.916 0.926
.. .. .. .. .. .. ..
E31 0.444 0.600 0.600 0.667 0.765 0.688
M14 0.591 0.600 0.575 0.887 0.897 0.904
G15 0.250 0.222 0.250 0.823 0.806 0.826
103 cat.
Mi-F1 0.640 0.677 0.731 0.769 0.794 0.815
Ma-F1 0.408 0.447 0.507 0.539 0.567 0.590
</table>
<tableCaption confidence="0.91897075">
Table 4: F1 of some binary classifiers along with the
Micro and Macro-Average F1 over all 103 categories
of RCV1, 8 hypotheses and 32k of training data for
rerankers using STK.
</tableCaption>
<bodyText confidence="0.6398615">
data over all categories (103 or 363). Additionally,
the F1 of some binary classifiers are reported.
</bodyText>
<subsectionHeader confidence="0.998582">
4.2 Classification Accuracy
</subsectionHeader>
<bodyText confidence="0.999987666666667">
In the first experiments, we compared the different
kernels using the KJ combination (which exploits
the joint hypothesis probability, see Sec. 3.3) on
SAM. Tab. 2 shows that the baseline (state-of-the-
art flat model) is largely improved by all rerankers.
BOL cannot capture the same dependencies as the
structural kernels. In contrast, when we remove the
dependencies generated by shared documents be-
tween a node and its descendants (child-free setting)
BOL improves on BL. Very interestingly, TK and
PTK in this setting significantly improves on SK
suggesting that the hierarchical structure is more im-
portant than the sequential one.
To study how much data is needed for the
reranker, the figures 6 and 7 report the Micro and
Macro Average F1 of our rerankers over 103 cate-
gories, according to different sets of training data.
This time, KJ is applied to only STK. We note that
(i) a few thousands of training examples are enough
to deliver most of the RR improvement; and (ii) the
FRR produces similar results as standard RR. This is
very interesting since, as it will be shown in the next
section, the compact representation produces much
faster models.
Table 4 reports the F1 of some individual cate-
gories as well as global performance. In these exper-
iments we used STK in KJ and KP. We note that
</bodyText>
<figureCaption confidence="0.993356">
Figure 8: Training and test time of the rerankers trained
on data of increasing size.
</figureCaption>
<bodyText confidence="0.99998904">
KP highly improves on the baseline on child-free
setting by about 7.1 and 9.9 absolute percent points
in Micro-and Macro-F1, respectively. Also the im-
provement on child-full is meaningful, i.e., 4.6 per-
cent points. This is rather interesting as BOL (not
reported in the table) achieved a Micro-average of
80.4% and a Macro-average of 57.2% when used in
KP, i.e., up to 2 points below STK. This means that
the use of probability vectors and combination with
structural kernels is a very promising direction for
reranker design.
To definitely assess the benefit of our rerankers
we tested them on the Lewis’ split of two different
datasets of RCV1, i.e., TOPIC and INDUSTRY. Ta-
ble 5 shows impressive results, e.g., for INDUSTRY,
the improvement is up to 5.2 percent points. We car-
ried out statistical significance tests, which certified
the significance at 99%. This was expected as the
size of the Lewis’ test sets is in the order of several
hundreds thousands.
Finally, to better understand the potential of
reranking, Table 6 shows the oracle performance
with respect to the increasing number of hypothe-
ses. The outcome clearly demonstrates that there is
large margin of improvement for the rerankers.
</bodyText>
<subsectionHeader confidence="0.999678">
4.3 Running Time
</subsectionHeader>
<bodyText confidence="0.999852857142857">
To study the applicability of our rerankers, we have
analyzed both the training and classification time.
Figure 8 shows the minutes required to train the dif-
ferent models as well as to classify the test set ac-
cording to data of increasing size.
It can be noted that the models using the compact
hypothesis representation are much faster than those
</bodyText>
<figure confidence="0.997428647058824">
Time (min)
450
400
350
300
250
200
150
100
50
0
2 12 22 32 42 52 62
Training Data Size (thousands of instances)
RR trainingTime
RR testTime
FRR trainingTime
FRR testTime
</figure>
<page confidence="0.980409">
765
</page>
<table confidence="0.995102">
F1 Topic Industry
BL (Lewis) BL (Ours) KJ (BOL) KJ KP BL (Lewis) BL (Ours) KJ (BOL) KJ KP
Micro-F1 0.816 0.815 0.818 0.827 0.849 0.512 0.562 0.566 0.576 0.628
Macro-F1 0.567 0.566 0.571 0.590 0.615 0.263 0.289 0.243 0.314 0.341
</table>
<tableCaption confidence="0.745161">
Table 5: Comparison between rankers using STK or BOL (when indicated) with the Kj and Kp schema. 32k
examples are used for training the rerankers with child-full setting.
</tableCaption>
<table confidence="0.999793">
k Micro-Fl Macro-Fl
1 0.640 0.408
2 0.758 0.504
4 0.821 0.566
8 0.858 0.610
16 0.898 0.658
</table>
<tableCaption confidence="0.98768">
Table 6: Oracle performance according to the number of
hypotheses (child-free setting).
</tableCaption>
<bodyText confidence="0.999894538461539">
using the complete hierarchy as representation, i.e.,
up to five times in training and eight time in test-
ing. This is not surprising as, in the latter case,
each kernel evaluation requires to perform tree ker-
nel evaluation on trees of 103 nodes. When using
the compact representation the number of nodes is
upper-bounded by the maximum number of labels
per documents, i.e., 6, times the depth of the hierar-
chy, i.e., 5 (the positive classification on the leaves
is the worst case). Thus, the largest tree would con-
tain 30 nodes. However, we only have 1.82 labels
per document on average, therefore the trees have
an average size of only about 9 nodes.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999977769230769">
Tree and sequence kernels have been successfully
used in many NLP applications, e.g.: parse rerank-
ing and adaptation (Collins and Duffy, 2002; Shen
et al., 2003; Toutanova et al., 2004; Kudo et al.,
2005; Titov and Henderson, 2006), chunking and
dependency parsing (Kudo and Matsumoto, 2003;
Daum´e III and Marcu, 2004), named entity recog-
nition (Cumby and Roth, 2003), text categorization
(Cancedda et al., 2003; Gliozzo et al., 2005) and re-
lation extraction (Zelenko et al., 2002; Bunescu and
Mooney, 2005; Zhang et al., 2006).
To our knowledge, ours is the first work explor-
ing structural kernels for reranking hierarchical text
categorization hypotheses. Additionally, there is a
substantial lack of work exploring reranking for hi-
erarchical text categorization. The work mostly re-
lated to ours is (Rousu et al., 2006) as they directly
encoded global dependencies in a gradient descen-
dent learning approach. This kind of algorithm is
less efficient than ours so they could experiment
with only the CCAT subhierarchy of RCV1, which
only contains 34 nodes. Other relevant work such
as (McCallum et al., 1998) and (Dumais and Chen,
2000) uses a rather different datasets and a different
idea of dependencies based on feature distributions
over the linked categories. An interesting method is
SVM-struct (Tsochantaridis et al., 2005), which has
been applied to model dependencies expressed as
category label subsets of flat categorization schemes
but no solution has been attempted for hierarchical
settings. The approaches in (Finley and Joachims,
2007; Riezler and Vasserman, 2010; Lavergne et al.,
2010) can surely be applied to model dependencies
in a tree, however, they need that feature templates
are specified in advance, thus the meaningful depen-
dencies must be already known. In contrast, kernel
methods allow for automatically generating all pos-
sible dependencies and reranking can efficiently en-
code them.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999896625">
In this paper, we have described several models for
reranking the output of an MCC based on SVMs
and structural kernels, i.e., SK, STK and PTK.
We have proposed a simple and efficient algorithm
for hypothesis generation and their kernel-based
representations. The latter are exploited by SVMs
using preference kernels to automatically derive
features from the hypotheses. When using tree
kernels such features are tree fragments, which can
encode complex semantic dependencies between
categories. We tested our rerankers on the entire
well-known RCV1. The results show impressive
improvement on the state-of-the-art flat TC models,
i.e., 3.3 absolute percent points on the Lewis’ split
(same setting) and up to 10 absolute points on
samples using child-free setting.
</bodyText>
<footnote confidence="0.51474">
Acknowledgements This research is partially sup-
ported by the EC FP7/2007-2013 under the grants:
247758 (ETERNALS), 288024 (LIMOSINE) and 231126
(LIVINGKNOWLEDGE). Many thanks to the reviewers
for their valuable suggestions.
</footnote>
<page confidence="0.997023">
766
</page>
<sectionHeader confidence="0.995802" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822885416667">
Nir Ailon and Mehryar Mohri. 2010. Preference-based
learning to rank. Machine Learning.
Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer,
Don Coppersmith, John Langford, and Gregory B.
Sorkin. 2008. Robust reductions from ranking to clas-
sification. Machine Learning, 72(1-2):139–153.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT and EMNLP, pages 724–731,
Vancouver, British Columbia, Canada, October.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059–1082.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of ACL’02, pages 263–270.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In Proceedings of ICML 2003.
Hal Daum´e III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP’04.
Susan T. Dumais and Hao Chen. 2000. Hierarchical clas-
sification of web content. In Nicholas J. Belkin, Peter
Ingwersen, and Mun-Kew Leong, editors, Proceedings
of SIGIR-00, 23rd ACM International Conference on
Research and Development in Information Retrieval,
pages 256–263, Athens, GR. ACM Press, New York,
US.
T. Finley and T. Joachims. 2007. Parameter learning
for loopy markov random fields with structural support
vector machines. In ICML Workshop on Constrained
Optimization and Structured Output Spaces.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava.
2005. Domain kernels for word sense disambiguation.
In Proceedings ofACL’05, pages 403–410.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in Kernel Methods – Sup-
port Vector Learning, 13.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL’03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings ofACL’05.
T. Lavergne, O. Capp´e, and F. Yvon. 2010. Practical very
large scale CRFs. In Proc. of ACL, pages 504–513.
D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. Rcv1: A
new benchmark collection for text categorization re-
search. The Journal of Machine Learning Research,
(5):361–397.
Andrew McCallum, Ronald Rosenfeld, Tom M. Mitchell,
and Andrew Y. Ng. 1998. Improving text classifica-
tion by shrinkage in a hierarchy of classes. In ICML,
pages 359–367.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML’06.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL’06.
S. Riezler and A. Vasserman. 2010. Incremental feature
selection and l1 regularization for relaxed maximum-
entropy modeling. In EMNLP.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101–
141, December.
Juho Rousu, Craig Saunders, Sandor Szedmak, and John
Shawe-Taylor. 2006. Kernel-based learning of hierar-
chical multilabel classification models. The Journal of
Machine Learning Research, (7):1601–1626.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Empirical Methods for Natural Language Processing
(EMNLP), pages 89–96, Sapporo, Japan.
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output vari-
ables. J. Machine Learning Reserach., 6:1453–1484,
December.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181–201.
Min Zhang, Jie Zhang, and Jian Su. 2006. Explor-
ing Syntactic Features for Relation Extraction using a
Convolution tree kernel. In Proceedings of NAACL.
</reference>
<page confidence="0.996753">
767
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948156">
<title confidence="0.999989">Modeling Topic Dependencies in Hierarchical Text Categorization</title>
<author confidence="0.999969">Moschitti Ju Richard Johansson</author>
<affiliation confidence="0.999948">University of Trento University of Gothenburg</affiliation>
<address confidence="0.999432">38123 Povo (TN), Italy SE-405 30 Gothenburg, Sweden</address>
<email confidence="0.991678">richard.johansson@gu.se</email>
<abstract confidence="0.997653157894737">In this paper, we encode topic dependencies in hierarchical multi-label Text Categorization (TC) by means of rerankers. We represent reranking hypotheses with several innovative kernels considering both the structure of the hierarchy and the probability of nodes. Additionally, to better investigate the role of category relationships, we consider two interesting cases: (i) traditional schemes in which node-fathers include all the documents of their child-categories; and (ii) more general schemes, in which children can include documents not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nir Ailon</author>
<author>Mehryar Mohri</author>
</authors>
<title>Preference-based learning to rank.</title>
<date>2010</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="15467" citStr="Ailon and Mohri, 2010" startWordPosition="2566" endWordPosition="2569"> connected nodes of T, whose edges are in T. For example, Fig. 5 shows the tree fragments from the hypothesis of Fig. 2. Note that each fragment captures dependencies between different categories. 3.3 Preference reranker When training a reranker model, the task of the machine learning algorithm is to learn to select the best candidate from a given set of hypotheses. To use SVMs for training a reranker, we applied Preference Kernel Method (Shen et al., 2003). The reduction method from ranking tasks to binary classification is an active research area; see for instance (Balcan et al., 2008) and (Ailon and Mohri, 2010). MCAT MCAT MCAT M11 -M12 M13 M14 M11 -M12 M13 M14 -M131 -M132 -M141 -M142 M143 -M131 M13 -M132 M14 M11 -M12 M13 M14 M143 -M142 -M141 MCAT MCAT MCAT MCAT M11 -M12 M13 M14 M11 -M12 M13 M13 M13 M14 -M141 -M142 M14 MCAT M13 MCAT -M141 -M142 -M143 -M131 -M132 M11 M13 -M131 -M131 -M132 -M132 ( � µ λ2 + f1,�I2,l(�I1)=l(�I2) )O � �(cn1(~I1j), cn2(~I2j)) l(i) H j=1 λd(F1)+d(F2) 762 Category Child-free Child-full Train Train1 Train2 TEST Train Train1 Train2 TEST C152 837 370 467 438 837 370 467 438 GPOL 723 357 366 380 723 357 366 380 M11 604 309 205 311 604 309 205 311 .. .. .. .. .. .. .. .. .. C31 3</context>
</contexts>
<marker>Ailon, Mohri, 2010</marker>
<rawString>Nir Ailon and Mehryar Mohri. 2010. Preference-based learning to rank. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria-Florina Balcan</author>
<author>Nikhil Bansal</author>
<author>Alina Beygelzimer</author>
<author>Don Coppersmith</author>
<author>John Langford</author>
<author>Gregory B Sorkin</author>
</authors>
<title>Robust reductions from ranking to classification.</title>
<date>2008</date>
<booktitle>Machine Learning,</booktitle>
<pages>72--1</pages>
<contexts>
<context position="15439" citStr="Balcan et al., 2008" startWordPosition="2561" endWordPosition="2564">can generate any subset of connected nodes of T, whose edges are in T. For example, Fig. 5 shows the tree fragments from the hypothesis of Fig. 2. Note that each fragment captures dependencies between different categories. 3.3 Preference reranker When training a reranker model, the task of the machine learning algorithm is to learn to select the best candidate from a given set of hypotheses. To use SVMs for training a reranker, we applied Preference Kernel Method (Shen et al., 2003). The reduction method from ranking tasks to binary classification is an active research area; see for instance (Balcan et al., 2008) and (Ailon and Mohri, 2010). MCAT MCAT MCAT M11 -M12 M13 M14 M11 -M12 M13 M14 -M131 -M132 -M141 -M142 M143 -M131 M13 -M132 M14 M11 -M12 M13 M14 M143 -M142 -M141 MCAT MCAT MCAT MCAT M11 -M12 M13 M14 M11 -M12 M13 M13 M13 M14 -M141 -M142 M14 MCAT M13 MCAT -M141 -M142 -M143 -M131 -M132 M11 M13 -M131 -M131 -M132 -M132 ( � µ λ2 + f1,�I2,l(�I1)=l(�I2) )O � �(cn1(~I1j), cn2(~I2j)) l(i) H j=1 λd(F1)+d(F2) 762 Category Child-free Child-full Train Train1 Train2 TEST Train Train1 Train2 TEST C152 837 370 467 438 837 370 467 438 GPOL 723 357 366 380 723 357 366 380 M11 604 309 205 311 604 309 205 311 .. .</context>
</contexts>
<marker>Balcan, Bansal, Beygelzimer, Coppersmith, Langford, Sorkin, 2008</marker>
<rawString>Maria-Florina Balcan, Nikhil Bansal, Alina Beygelzimer, Don Coppersmith, John Langford, and Gregory B. Sorkin. 2008. Robust reductions from ranking to classification. Machine Learning, 72(1-2):139–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT and EMNLP,</booktitle>
<pages>724--731</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="28742" citStr="Bunescu and Mooney, 2005" startWordPosition="4851" endWordPosition="4854"> 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 19</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of HLT and EMNLP, pages 724–731, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Jean Michel Renders</author>
</authors>
<title>Word sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<contexts>
<context position="28647" citStr="Cancedda et al., 2003" startWordPosition="4835" endWordPosition="4838">ves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhi</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="8101" citStr="Collins and Duffy, 2002" startWordPosition="1270" endWordPosition="1273">he dependencies between the dif1We used the conversion of margin into probability provided by LIBSVM. h = argmax hE{0,1}n hi p i n i=1 hE{0 ,1} hi pi argmin i=1,..,n 760 MCAT M11 M13 M14 M143 Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels. For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. (Collins and Duffy, 2002; Moschitti, 2006a), can be applied. In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143). As the input of our reranker, we can simply use a tree representing the hierarchy above, marking the negative assignments of the current hypothesis in the node</context>
<context position="10111" citStr="Collins and Duffy, 2002" startWordPosition="1597" endWordPosition="1600"> product between instances. In several cases, this can be efficiently and implicitly computed by kernel functions by exploiting the following dual formulation: � i=1..lyiαiO(oi)O(o) + b = 0, where oi and o are two objects, 0 is a mapping from the objects to feature vectors xi and 0(oi)0(o) = K(oi, o) is a kernel function implicitly defining such a mapping. In case of structural kernels, K determines the shape of the substructures describing the objects above. The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the Syntactic Tree Kernels (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a). 3.2.1 String Kernels The String Kernels (SK) that we consider count the number of subsequences shared by two strings of symbols, s1 and s2. Some symbols during the matching process can be skipped. This modifies the weight associated with the target substrings as shown by the following SK equation: ESK(s1, s2) = Ou(s1) - Ou(s2) = E E uEE* Ad(~I1)+d(~I2) ~I1:u=s1[~I1] ~I2:u=s2[~I2] where, E* = U n=0 En is the set of all strings, h and I2 are two sequences of indexes I = (i1, ..., i|u|), with 1 &lt; i1 &lt; ... &lt; i|u |&lt; |s|, such that u = si,si|u|, d( I</context>
<context position="28376" citStr="Collins and Duffy, 2002" startWordPosition="4792" endWordPosition="4795">res to perform tree kernel evaluation on trees of 103 nodes. When using the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarch</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL’02, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>On kernel methods for relational learning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="28603" citStr="Cumby and Roth, 2003" startWordPosition="4829" endWordPosition="4832">., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so th</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. On kernel methods for relational learning. In Proceedings of ICML 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Np bracketing by maximum entropy tagging and SVM reranking.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04.</booktitle>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2004. Np bracketing by maximum entropy tagging and SVM reranking. In Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Hao Chen</author>
</authors>
<title>Hierarchical classification of web content.</title>
<date>2000</date>
<booktitle>Proceedings of SIGIR-00, 23rd ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>256--263</pages>
<editor>In Nicholas J. Belkin, Peter Ingwersen, and Mun-Kew Leong, editors,</editor>
<publisher>ACM Press,</publisher>
<location>Athens, GR.</location>
<contexts>
<context position="29373" citStr="Dumais and Chen, 2000" startWordPosition="4952" endWordPosition="4955">t al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais and Chen, 2000) uses a rather different datasets and a different idea of dependencies based on feature distributions over the linked categories. An interesting method is SVM-struct (Tsochantaridis et al., 2005), which has been applied to model dependencies expressed as category label subsets of flat categorization schemes but no solution has been attempted for hierarchical settings. The approaches in (Finley and Joachims, 2007; Riezler and Vasserman, 2010; Lavergne et al., 2010) can surely be applied to model dependencies in a tree, however, they need that feature templates are specified in advance, thus the</context>
</contexts>
<marker>Dumais, Chen, 2000</marker>
<rawString>Susan T. Dumais and Hao Chen. 2000. Hierarchical classification of web content. In Nicholas J. Belkin, Peter Ingwersen, and Mun-Kew Leong, editors, Proceedings of SIGIR-00, 23rd ACM International Conference on Research and Development in Information Retrieval, pages 256–263, Athens, GR. ACM Press, New York, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley</author>
<author>T Joachims</author>
</authors>
<title>Parameter learning for loopy markov random fields with structural support vector machines.</title>
<date>2007</date>
<booktitle>In ICML Workshop on Constrained Optimization and Structured Output Spaces.</booktitle>
<contexts>
<context position="29788" citStr="Finley and Joachims, 2007" startWordPosition="5012" endWordPosition="5015">orithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais and Chen, 2000) uses a rather different datasets and a different idea of dependencies based on feature distributions over the linked categories. An interesting method is SVM-struct (Tsochantaridis et al., 2005), which has been applied to model dependencies expressed as category label subsets of flat categorization schemes but no solution has been attempted for hierarchical settings. The approaches in (Finley and Joachims, 2007; Riezler and Vasserman, 2010; Lavergne et al., 2010) can surely be applied to model dependencies in a tree, however, they need that feature templates are specified in advance, thus the meaningful dependencies must be already known. In contrast, kernel methods allow for automatically generating all possible dependencies and reranking can efficiently encode them. 6 Conclusions In this paper, we have described several models for reranking the output of an MCC based on SVMs and structural kernels, i.e., SK, STK and PTK. We have proposed a simple and efficient algorithm for hypothesis generation a</context>
</contexts>
<marker>Finley, Joachims, 2007</marker>
<rawString>T. Finley and T. Joachims. 2007. Parameter learning for loopy markov random fields with structural support vector machines. In ICML Workshop on Constrained Optimization and Structured Output Spaces.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Gliozzo</author>
<author>Claudio Giuliano</author>
<author>Carlo Strapparava</author>
</authors>
<title>Domain kernels for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL’05,</booktitle>
<pages>403--410</pages>
<contexts>
<context position="28670" citStr="Gliozzo et al., 2005" startWordPosition="4839" endWordPosition="4842"> Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which </context>
</contexts>
<marker>Gliozzo, Giuliano, Strapparava, 2005</marker>
<rawString>Alfio Gliozzo, Claudio Giuliano, and Carlo Strapparava. 2005. Domain kernels for word sense disambiguation. In Proceedings ofACL’05, pages 403–410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods – Support Vector Learning,</booktitle>
<volume>13</volume>
<contexts>
<context position="22531" citStr="Joachims, 1999" startWordPosition="3803" endWordPosition="3804">rning curves of the reranking models using STK in terms of MacroAverage-F1, according to increasing training set (child-free setting). mented two rerankers: RR, which use the representation of hypotheses described in Fig. 2; and FRR, i.e., fast RR, which uses the compact representation described in Fig. 3. The rerankers are based on SVMs and the Preference Kernel (PK) described in Sec. 1 built on top of SK, STK or PTK (see Section 3.2.2). These are applied to the tree-structured hypotheses. We trained the rerankers using SVM-light-TK6, which enables the use of structural kernels in SVM-light (Joachims, 1999). This allows for applying kernels to pairs of trees and combining them with vector-based kernels. Again we use default parameters to facilitate replicability and preserve generality. The rerankers always use 8 best hypotheses. All the performance values are provided by means of Micro- and Macro-Average F1, evaluated on test 6disi.unitn.it/moschitti/Tree-Kernel.htm Micro-F1 0.676 0.666 0.656 0.646 0.636 0.626 Macro-F1 0.445 0.435 0.425 0.415 0.405 0.395 0.385 0.375 0.365 764 Cat. Child-free Child-full BL Kj Kp BL Kj Kp C152 0.671 0.700 0.771 0.671 0.729 0.745 GPOL 0.660 0.695 0.743 0.660 0.680</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. Advances in Kernel Methods – Support Vector Learning, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="28525" citStr="Kudo and Matsumoto, 2003" startWordPosition="4816" endWordPosition="4819">um number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descende</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL’05.</booktitle>
<contexts>
<context position="28438" citStr="Kudo et al., 2005" startWordPosition="4804" endWordPosition="4807">ing the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings ofACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lavergne</author>
<author>O Capp´e</author>
<author>F Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>504--513</pages>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>T. Lavergne, O. Capp´e, and F. Yvon. 2010. Practical very large scale CRFs. In Proc. of ACL, pages 504–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T Rose</author>
<author>F Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="19768" citStr="Lewis et al., 2004" startWordPosition="3355" endWordPosition="3358">Volume 1 (RCV1)3 TC cor3trec.nist.gov/data/reuters/reuters.html 763 pus. For most experiments, we randomly selected two subsets of 10k and 5k of documents for training and testing from the total 804,414 Reuters news from TOPICS by still using all the 103 categories organized in 5 levels (hereafter SAM). The distribution of the data instances of some of the different categories in such samples can be observed in Table 1. The training set is used for learning the binary classifiers needed to build the multiclassclassifier (MCC). To compare with previous work we also considered the Lewis’ split (Lewis et al., 2004), which includes 23,149 news for training and 781,265 for testing. Additionally, we carried out some experiments on INDUSTRY data from RCV1. This contains 352,361 news assigned to 365 categories, which are organized in 6 levels. The Lewis’ split for INDUSTRY includes 9,644 news for training and 342,117 for testing. We used the above datasets with two different settings: the child-free setting, where we removed all the document belonging to the child nodes from the parent nodes, and the normal setting which we refer to as child-full. To implement the baseline model, we applied the state-of-the-</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. Rcv1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, (5):361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ronald Rosenfeld</author>
<author>Tom M Mitchell</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving text classification by shrinkage in a hierarchy of classes. In</title>
<date>1998</date>
<booktitle>ICML,</booktitle>
<pages>359--367</pages>
<contexts>
<context position="29345" citStr="McCallum et al., 1998" startWordPosition="4947" endWordPosition="4950">cu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais and Chen, 2000) uses a rather different datasets and a different idea of dependencies based on feature distributions over the linked categories. An interesting method is SVM-struct (Tsochantaridis et al., 2005), which has been applied to model dependencies expressed as category label subsets of flat categorization schemes but no solution has been attempted for hierarchical settings. The approaches in (Finley and Joachims, 2007; Riezler and Vasserman, 2010; Lavergne et al., 2010) can surely be applied to model dependencies in a tree, however, they need that feature templates are sp</context>
</contexts>
<marker>McCallum, Rosenfeld, Mitchell, Ng, 1998</marker>
<rawString>Andrew McCallum, Ronald Rosenfeld, Tom M. Mitchell, and Andrew Y. Ng. 1998. Improving text classification by shrinkage in a hierarchy of classes. In ICML, pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ECML’06.</booktitle>
<contexts>
<context position="8118" citStr="Moschitti, 2006" startWordPosition="1274" endWordPosition="1275">he dif1We used the conversion of margin into probability provided by LIBSVM. h = argmax hE{0,1}n hi p i n i=1 hE{0 ,1} hi pi argmin i=1,..,n 760 MCAT M11 M13 M14 M143 Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels. For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. (Collins and Duffy, 2002; Moschitti, 2006a), can be applied. In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143). As the input of our reranker, we can simply use a tree representing the hierarchy above, marking the negative assignments of the current hypothesis in the node labels with “-”,</context>
<context position="10157" citStr="Moschitti, 2006" startWordPosition="1606" endWordPosition="1607">be efficiently and implicitly computed by kernel functions by exploiting the following dual formulation: � i=1..lyiαiO(oi)O(o) + b = 0, where oi and o are two objects, 0 is a mapping from the objects to feature vectors xi and 0(oi)0(o) = K(oi, o) is a kernel function implicitly defining such a mapping. In case of structural kernels, K determines the shape of the substructures describing the objects above. The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the Syntactic Tree Kernels (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a). 3.2.1 String Kernels The String Kernels (SK) that we consider count the number of subsequences shared by two strings of symbols, s1 and s2. Some symbols during the matching process can be skipped. This modifies the weight associated with the target substrings as shown by the following SK equation: ESK(s1, s2) = Ou(s1) - Ou(s2) = E E uEE* Ad(~I1)+d(~I2) ~I1:u=s1[~I1] ~I2:u=s2[~I2] where, E* = U n=0 En is the set of all strings, h and I2 are two sequences of indexes I = (i1, ..., i|u|), with 1 &lt; i1 &lt; ... &lt; i|u |&lt; |s|, such that u = si,si|u|, d( I� = i|u |− i1 + 1 (distance between the first </context>
<context position="13682" citStr="Moschitti, 2006" startWordPosition="2253" endWordPosition="2255">; and (iii) if the productions at n1 and n2 are the same, and n1 and n2 are not pre-terminals then OSTK(n1, n2) = λ jjl(n1) j=1 (1 + OSTK(cjn1,cjn2)), where l(n1) is the 2To have a similarity score between 0 and 1, a normalization in the kernel space, i.e. TK(T1,T2) is applied. NIT K(T1, T1) X TK(T2, T2) number of children of n1 and cjn is the j-th child of the node n. Note that, since the productions are the same, l(n1) = l(n2) and the computational complexity of STK is O(|NT1||NT2|) but the average running time tends to be linear, i.e. O(|NT1 |+ |NT2|), for natural language syntactic trees (Moschitti, 2006a; Moschitti, 2006b). Figure 4 shows the five fragments of the hypothesis in Figure 2. Such fragments satisfy the constraint that each of their nodes includes all or none of its children. For example, [M13 [-M131 -M132]] is an STF, which has two non-terminal symbols, -M131 and -M132, as leaves while [M13 [-M131]] is not an STF. The Partial Tree Kernel (PTK) The computation of PTK is carried out by the following OPTK function: if the labels of n1 and n2 are different then OPTK(n1, n2) = 0; else OPTK(n1, n2) = where d( ~I1) = ~I1l( ~I1) ~I11 and d( ~I2) = ~I2l( ~I2) � ~I21. This way, we penalize</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006a. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of ECML’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proccedings of EACL’06.</booktitle>
<contexts>
<context position="8118" citStr="Moschitti, 2006" startWordPosition="1274" endWordPosition="1275">he dif1We used the conversion of margin into probability provided by LIBSVM. h = argmax hE{0,1}n hi p i n i=1 hE{0 ,1} hi pi argmin i=1,..,n 760 MCAT M11 M13 M14 M143 Figure 3: A compact representation of the hypothesis in Fig. 2. ferent nodes of the hierarchy can be learned. Since we do not know in advance which are the important dependencies and not even the scope of the interaction between the different structure subparts, we rely on automatic feature engineering via structural kernels. For this paper, we consider tree-shaped hierarchies so that tree kernels, e.g. (Collins and Duffy, 2002; Moschitti, 2006a), can be applied. In more detail, we focus on the Reuters categorization scheme. For example, Figure 1 shows a subhierarchy of the Markets (MCAT) category and its subcategories: Equity Markets (M11), Bond Markets (M12), Money Markets (M13) and Commodity Markets (M14). These also have subcategories: Interbank Markets (M131), Forex Markets (M132), Soft Commodities (M141), Metals Trading (M142) and Energy Markets (M143). As the input of our reranker, we can simply use a tree representing the hierarchy above, marking the negative assignments of the current hypothesis in the node labels with “-”,</context>
<context position="10157" citStr="Moschitti, 2006" startWordPosition="1606" endWordPosition="1607">be efficiently and implicitly computed by kernel functions by exploiting the following dual formulation: � i=1..lyiαiO(oi)O(o) + b = 0, where oi and o are two objects, 0 is a mapping from the objects to feature vectors xi and 0(oi)0(o) = K(oi, o) is a kernel function implicitly defining such a mapping. In case of structural kernels, K determines the shape of the substructures describing the objects above. The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the Syntactic Tree Kernels (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a). 3.2.1 String Kernels The String Kernels (SK) that we consider count the number of subsequences shared by two strings of symbols, s1 and s2. Some symbols during the matching process can be skipped. This modifies the weight associated with the target substrings as shown by the following SK equation: ESK(s1, s2) = Ou(s1) - Ou(s2) = E E uEE* Ad(~I1)+d(~I2) ~I1:u=s1[~I1] ~I2:u=s2[~I2] where, E* = U n=0 En is the set of all strings, h and I2 are two sequences of indexes I = (i1, ..., i|u|), with 1 &lt; i1 &lt; ... &lt; i|u |&lt; |s|, such that u = si,si|u|, d( I� = i|u |− i1 + 1 (distance between the first </context>
<context position="13682" citStr="Moschitti, 2006" startWordPosition="2253" endWordPosition="2255">; and (iii) if the productions at n1 and n2 are the same, and n1 and n2 are not pre-terminals then OSTK(n1, n2) = λ jjl(n1) j=1 (1 + OSTK(cjn1,cjn2)), where l(n1) is the 2To have a similarity score between 0 and 1, a normalization in the kernel space, i.e. TK(T1,T2) is applied. NIT K(T1, T1) X TK(T2, T2) number of children of n1 and cjn is the j-th child of the node n. Note that, since the productions are the same, l(n1) = l(n2) and the computational complexity of STK is O(|NT1||NT2|) but the average running time tends to be linear, i.e. O(|NT1 |+ |NT2|), for natural language syntactic trees (Moschitti, 2006a; Moschitti, 2006b). Figure 4 shows the five fragments of the hypothesis in Figure 2. Such fragments satisfy the constraint that each of their nodes includes all or none of its children. For example, [M13 [-M131 -M132]] is an STF, which has two non-terminal symbols, -M131 and -M132, as leaves while [M13 [-M131]] is not an STF. The Partial Tree Kernel (PTK) The computation of PTK is carried out by the following OPTK function: if the labels of n1 and n2 are different then OPTK(n1, n2) = 0; else OPTK(n1, n2) = where d( ~I1) = ~I1l( ~I1) ~I11 and d( ~I2) = ~I2l( ~I2) � ~I21. This way, we penalize</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006b. Making tree kernels practical for natural language learning. In Proccedings of EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>A Vasserman</author>
</authors>
<title>Incremental feature selection and l1 regularization for relaxed maximumentropy modeling.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1815" citStr="Riezler and Vasserman, 2010" startWordPosition="250" endWordPosition="254">.-all, which do not take topic relationships into account. This is due to two major problems: (i) complexity in introducing them in the learning algorithm and (ii) the small or no advantage that they seem to provide (Rifkin and Klautau, 2004). We speculate that the failure of using hierarchical approaches is caused by the inherent complexity of modeling all possible topic dependencies rather than the uselessness of such relationships. More precisely, although hierarchical multi-label classifiers can exploit machine learning algorithms for structural output, e.g., (Tsochantaridis et al., 2005; Riezler and Vasserman, 2010; Lavergne et al., 2010), they often impose a number of simplifying restrictions on some category assignments. Typically, the probability of a document d to belong to a subcategory Ci of a category C is assumed to depend only on d and C, but not on other subcategories of C, or any other categories in the hierarchy. Indeed, the introduction of these long-range dependencies lead to computational intractability or more in general to the problem of how to select an effective subset of them. It is important to stress that (i) there is no theory that can suggest which are the dependencies to be incl</context>
<context position="29817" citStr="Riezler and Vasserman, 2010" startWordPosition="5016" endWordPosition="5019">an ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais and Chen, 2000) uses a rather different datasets and a different idea of dependencies based on feature distributions over the linked categories. An interesting method is SVM-struct (Tsochantaridis et al., 2005), which has been applied to model dependencies expressed as category label subsets of flat categorization schemes but no solution has been attempted for hierarchical settings. The approaches in (Finley and Joachims, 2007; Riezler and Vasserman, 2010; Lavergne et al., 2010) can surely be applied to model dependencies in a tree, however, they need that feature templates are specified in advance, thus the meaningful dependencies must be already known. In contrast, kernel methods allow for automatically generating all possible dependencies and reranking can efficiently encode them. 6 Conclusions In this paper, we have described several models for reranking the output of an MCC based on SVMs and structural kernels, i.e., SK, STK and PTK. We have proposed a simple and efficient algorithm for hypothesis generation and their kernel-based represe</context>
</contexts>
<marker>Riezler, Vasserman, 2010</marker>
<rawString>S. Riezler and A. Vasserman. 2010. Incremental feature selection and l1 regularization for relaxed maximumentropy modeling. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
<author>Aldebaro Klautau</author>
</authors>
<title>In defense of one-vs-all classification.</title>
<date>2004</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>5</volume>
<pages>141</pages>
<contexts>
<context position="1430" citStr="Rifkin and Klautau, 2004" startWordPosition="196" endWordPosition="199">ocuments not belonging to their fathers. The extensive experimentation on Reuters Corpus Volume 1 shows that our rerankers inject effective structural semantic dependencies in multi-classifiers and significantly outperform the state-of-the-art. 1 Introduction Automated Text Categorization (TC) algorithms for hierarchical taxonomies are typically based on flat schemes, e.g., one-vs.-all, which do not take topic relationships into account. This is due to two major problems: (i) complexity in introducing them in the learning algorithm and (ii) the small or no advantage that they seem to provide (Rifkin and Klautau, 2004). We speculate that the failure of using hierarchical approaches is caused by the inherent complexity of modeling all possible topic dependencies rather than the uselessness of such relationships. More precisely, although hierarchical multi-label classifiers can exploit machine learning algorithms for structural output, e.g., (Tsochantaridis et al., 2005; Riezler and Vasserman, 2010; Lavergne et al., 2010), they often impose a number of simplifying restrictions on some category assignments. Typically, the probability of a document d to belong to a subcategory Ci of a category C is assumed to d</context>
<context position="20853" citStr="Rifkin and Klautau, 2004" startWordPosition="3526" endWordPosition="3529">from the parent nodes, and the normal setting which we refer to as child-full. To implement the baseline model, we applied the state-of-the-art method used by (Lewis et al., 2004) for RCV1, i.e.,: SVMs with the default parameters (trade-off and cost factor = 1), linear kernel, normalized vectors, stemmed bag-of-words representation, log(TF + 1) x IDF weighting scheme and stop list4. We used the LIBSVM5 implementation, which provides a probabilistic outcome for the classification function. The classifiers are combined using the one-vs.-all approach, which is also state-of-the-art as argued in (Rifkin and Klautau, 2004). Since the task requires us to assign multiple labels, we simply collect the decisions of the n classifiers: this constitutes our MCC baseline. Regarding the reranker, we divided the training set in two chunks of data: Train1 and Train2. The binary classifiers are trained on Train1 and tested on Train2 (and vice versa) to generate the hypotheses on Train2 (Train1). The union of the two sets constitutes the training data for the reranker. We imple4We have just a small difference in the number of tokens, i.e., 51,002 vs. 47,219 but this is both not critical and rarely achievable because of the </context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-vs-all classification. J. Mach. Learn. Res., 5:101– 141, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juho Rousu</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Kernel-based learning of hierarchical multilabel classification models.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--1601</pages>
<contexts>
<context position="29057" citStr="Rousu et al., 2006" startWordPosition="4900" endWordPosition="4903">; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais and Chen, 2000) uses a rather different datasets and a different idea of dependencies based on feature distributions over the linked categories. An interesting method is SVM-struct (Tsochantaridis et al., 2005), which has been applied to model dependencies expressed as category label subsets of fla</context>
</contexts>
<marker>Rousu, Saunders, Szedmak, Shawe-Taylor, 2006</marker>
<rawString>Juho Rousu, Craig Saunders, Sandor Szedmak, and John Shawe-Taylor. 2006. Kernel-based learning of hierarchical multilabel classification models. The Journal of Machine Learning Research, (7):1601–1626.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10057" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1589" endWordPosition="1592">h learning and classification algorithms only depend on the inner product between instances. In several cases, this can be efficiently and implicitly computed by kernel functions by exploiting the following dual formulation: � i=1..lyiαiO(oi)O(o) + b = 0, where oi and o are two objects, 0 is a mapping from the objects to feature vectors xi and 0(oi)0(o) = K(oi, o) is a kernel function implicitly defining such a mapping. In case of structural kernels, K determines the shape of the substructures describing the objects above. The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the Syntactic Tree Kernels (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a). 3.2.1 String Kernels The String Kernels (SK) that we consider count the number of subsequences shared by two strings of symbols, s1 and s2. Some symbols during the matching process can be skipped. This modifies the weight associated with the target substrings as shown by the following SK equation: ESK(s1, s2) = Ou(s1) - Ou(s2) = E E uEE* Ad(~I1)+d(~I2) ~I1:u=s1[~I1] ~I2:u=s2[~I2] where, E* = U n=0 En is the set of all strings, h and I2 are two sequences of indexes I = (i1, ..., i|u|), with 1</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind k Joshi</author>
</authors>
<title>Using LTAG Based Features in Parse Reranking.</title>
<date>2003</date>
<booktitle>In Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>89--96</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="15306" citStr="Shen et al., 2003" startWordPosition="2540" endWordPosition="2543">r the average running time again tends to be linear for natural language syntactic trees (Moschitti, 2006a). Given a target T, PTK can generate any subset of connected nodes of T, whose edges are in T. For example, Fig. 5 shows the tree fragments from the hypothesis of Fig. 2. Note that each fragment captures dependencies between different categories. 3.3 Preference reranker When training a reranker model, the task of the machine learning algorithm is to learn to select the best candidate from a given set of hypotheses. To use SVMs for training a reranker, we applied Preference Kernel Method (Shen et al., 2003). The reduction method from ranking tasks to binary classification is an active research area; see for instance (Balcan et al., 2008) and (Ailon and Mohri, 2010). MCAT MCAT MCAT M11 -M12 M13 M14 M11 -M12 M13 M14 -M131 -M132 -M141 -M142 M143 -M131 M13 -M132 M14 M11 -M12 M13 M14 M143 -M142 -M141 MCAT MCAT MCAT MCAT M11 -M12 M13 M14 M11 -M12 M13 M13 M13 M14 -M141 -M142 M14 MCAT M13 MCAT -M141 -M142 -M143 -M131 -M132 M11 M13 -M131 -M131 -M132 -M132 ( � µ λ2 + f1,�I2,l(�I1)=l(�I2) )O � �(cn1(~I1j), cn2(~I2j)) l(i) H j=1 λd(F1)+d(F2) 762 Category Child-free Child-full Train Train1 Train2 TEST Train </context>
<context position="17583" citStr="Shen et al., 2003" startWordPosition="2984" endWordPosition="2987">s training set can then be used to train a binary classifier. At classification time, pairs are not formed (since the correct candidate is not known); instead, the standard one-versus-all binarization method is still applied. The kernels are then engineered to implicitly represent the differences between the objects in the pairs. If we have a valid kernel K over the candidate space T , we can construct a preference kernel PK over the space of pairs T x T as follows: PK(x, y) =// PK((x1, x2), (y1, y2)) = K(x1, y1)+ (1) K(x2, y2) − K(x1, y2) − K(x2, y1), where x, y E T x T . It is easy to show (Shen et al., 2003) that PK is also a valid Mercer’s kernel. This makes it possible to use kernel methods to train the reranker. We explore innovative kernels K to be used in Eq. 1: KJ = p(x1) x p(y1) + S, where p(·) is the global joint probability of a target hypothesis and S is a structural kernel, i.e., SK, STK and PTK. KP = ~x1 · ~y1 + S, where ~x1={p(x1,j)IjEx1, ~y1 = {p(y1, j)IjEl1, p(t, n) is the classification probability of the node (category) n in the F1 BL BOL SK STK PTK Micro-F1 0.769 0.771 0.786 0.790 0.790 Macro-F1 0.539 0.541 0.542 0.547 0.560 Table 2: Comparison of rerankers using different kerne</context>
<context position="28395" citStr="Shen et al., 2003" startWordPosition="4796" endWordPosition="4799">l evaluation on trees of 103 nodes. When using the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categoriz</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In Empirical Methods for Natural Language Processing (EMNLP), pages 89–96, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Porting statistical parsers with data-defined kernels.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="28466" citStr="Titov and Henderson, 2006" startWordPosition="4808" endWordPosition="4811">resentation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they </context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Porting statistical parsers with data-defined kernels. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher Manning</author>
</authors>
<title>The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="28419" citStr="Toutanova et al., 2004" startWordPosition="4800" endWordPosition="4803">es of 103 nodes. When using the compact representation the number of nodes is upper-bounded by the maximum number of labels per documents, i.e., 6, times the depth of the hierarchy, i.e., 5 (the positive classification on the leaves is the worst case). Thus, the largest tree would contain 30 nodes. However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly r</context>
</contexts>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Kristina Toutanova, Penka Markova, and Christopher Manning. 2004. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>J. Machine Learning Reserach.,</journal>
<pages>6--1453</pages>
<contexts>
<context position="1786" citStr="Tsochantaridis et al., 2005" startWordPosition="246" endWordPosition="249">on flat schemes, e.g., one-vs.-all, which do not take topic relationships into account. This is due to two major problems: (i) complexity in introducing them in the learning algorithm and (ii) the small or no advantage that they seem to provide (Rifkin and Klautau, 2004). We speculate that the failure of using hierarchical approaches is caused by the inherent complexity of modeling all possible topic dependencies rather than the uselessness of such relationships. More precisely, although hierarchical multi-label classifiers can exploit machine learning algorithms for structural output, e.g., (Tsochantaridis et al., 2005; Riezler and Vasserman, 2010; Lavergne et al., 2010), they often impose a number of simplifying restrictions on some category assignments. Typically, the probability of a document d to belong to a subcategory Ci of a category C is assumed to depend only on d and C, but not on other subcategories of C, or any other categories in the hierarchy. Indeed, the introduction of these long-range dependencies lead to computational intractability or more in general to the problem of how to select an effective subset of them. It is important to stress that (i) there is no theory that can suggest which ar</context>
<context position="29568" citStr="Tsochantaridis et al., 2005" startWordPosition="4980" endWordPosition="4983">ork exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais and Chen, 2000) uses a rather different datasets and a different idea of dependencies based on feature distributions over the linked categories. An interesting method is SVM-struct (Tsochantaridis et al., 2005), which has been applied to model dependencies expressed as category label subsets of flat categorization schemes but no solution has been attempted for hierarchical settings. The approaches in (Finley and Joachims, 2007; Riezler and Vasserman, 2010; Lavergne et al., 2010) can surely be applied to model dependencies in a tree, however, they need that feature templates are specified in advance, thus the meaningful dependencies must be already known. In contrast, kernel methods allow for automatically generating all possible dependencies and reranking can efficiently encode them. 6 Conclusions I</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. J. Machine Learning Reserach., 6:1453–1484, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-ACL,</booktitle>
<pages>181--201</pages>
<contexts>
<context position="28716" citStr="Zelenko et al., 2002" startWordPosition="4847" endWordPosition="4850"> However, we only have 1.82 labels per document on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work su</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2002</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2002. Kernel methods for relation extraction. In Proceedings of EMNLP-ACL, pages 181–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jie Zhang</author>
<author>Jian Su</author>
</authors>
<title>Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="28763" citStr="Zhang et al., 2006" startWordPosition="4855" endWordPosition="4858">on average, therefore the trees have an average size of only about 9 nodes. 5 Related Work Tree and sequence kernels have been successfully used in many NLP applications, e.g.: parse reranking and adaptation (Collins and Duffy, 2002; Shen et al., 2003; Toutanova et al., 2004; Kudo et al., 2005; Titov and Henderson, 2006), chunking and dependency parsing (Kudo and Matsumoto, 2003; Daum´e III and Marcu, 2004), named entity recognition (Cumby and Roth, 2003), text categorization (Cancedda et al., 2003; Gliozzo et al., 2005) and relation extraction (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhang et al., 2006). To our knowledge, ours is the first work exploring structural kernels for reranking hierarchical text categorization hypotheses. Additionally, there is a substantial lack of work exploring reranking for hierarchical text categorization. The work mostly related to ours is (Rousu et al., 2006) as they directly encoded global dependencies in a gradient descendent learning approach. This kind of algorithm is less efficient than ours so they could experiment with only the CCAT subhierarchy of RCV1, which only contains 34 nodes. Other relevant work such as (McCallum et al., 1998) and (Dumais and C</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel. In Proceedings of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>