<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025721">
<title confidence="0.979255">
Segmentation for English-to-Arabic Statistical Machine Translation
</title>
<author confidence="0.986362">
Ibrahim Badr Rabih Zbib James Glass
</author>
<affiliation confidence="0.996608">
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.64559">
Cambridge, MA 02139, USA
</address>
<email confidence="0.996476">
{iab02, rabih, glass}@csail.mit.edu
</email>
<sectionHeader confidence="0.995594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997266555555556">
In this paper, we report on a set of ini-
tial results for English-to-Arabic Statistical
Machine Translation (SMT). We show that
morphological decomposition of the Arabic
source is beneficial, especially for smaller-size
corpora, and investigate different recombina-
tion techniques. We also report on the use
of Factored Translation Models for English-
to-Arabic translation.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.974671192307692">
Arabic has a complex morphology compared to
English. Words are inflected for gender, number,
and sometimes grammatical case, and various cli-
tics can attach to word stems. An Arabic corpus
will therefore have more surface forms than an En-
glish corpus of the same size, and will also be more
sparsely populated. These factors adversely affect
the performance of ArabicHEnglish Statistical Ma-
chine Translation (SMT). In prior work (Lee, 2004;
Habash and Sadat, 2006), it has been shown that
morphological segmentation of the Arabic source
benefits the performance of Arabic-to-English SMT.
The use of similar techniques for English-to-Arabic
SMT requires recombination of the target side into
valid surface forms, which is not a trivial task.
In this paper, we present an initial set of experi-
ments on English-to-Arabic SMT. We report results
from two domains: text news, trained on a large cor-
pus, and spoken travel conversation, trained on a sig-
nificantly smaller corpus. We show that segmenting
the Arabic target in training and decoding improves
performance. We propose various schemes for re-
combining the segmented Arabic, and compare their
effect on translation. We also report on applying
Factored Translation Models (Koehn and Hoang,
2007) for English-to-Arabic translation.
</bodyText>
<sectionHeader confidence="0.994698" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999077037037037">
The only previous work on English-to-Arabic SMT
that we are aware of is by Sarikaya and Deng (2007).
It uses shallow segmentation, and does not make
use of contextual information. The emphasis of that
work is on using Joint Morphological-Lexical Lan-
guage Models to rerank the output.
Most of the related work, though, is on Arabic-to-
English SMT. Lee (2004) uses a trigram language
model to segment Arabic words. She then pro-
ceeds to deleting or merging some of the segmented
morphemes in order to make the segmented Arabic
source align better with the English target. Habash
and Sadat (2006) use the Arabic morphological an-
alyzer MADA (Habash and Rambow, 2005) to seg-
ment the Arabic source; they propose various seg-
mentation schemes. Both works show that the im-
provements obtained from segmentation decrease as
the corpus size increases. As will be shown later, we
observe the same trend, which is due to the fact that
the model becomes less sparse with more training
data.
There has been work on translating from En-
glish to other morphologically complex languages.
Koehn and Hoang (2007) present Factored Transla-
tion Models as an extension to phrase-based statisti-
cal machine translation models. Factored models al-
low the integration of additional morphological fea-
</bodyText>
<page confidence="0.992255">
153
</page>
<reference confidence="0.223352">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 153–156,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.999557142857143">
tures, such as POS, gender, number, etc. at the word
level on both source and target sides. The tighter in-
tegration of such features was claimed to allow more
explicit modeling of the morphology, and is better
than using pre-processing and post-processing tech-
niques. Factored Models demonstrate improvements
when used to translate English to German or Czech.
</bodyText>
<sectionHeader confidence="0.966377" genericHeader="method">
3 Arabic Segmentation and
Recombination
</sectionHeader>
<bodyText confidence="0.999901555555556">
As mentioned in Section 1, Arabic has a relatively
rich morphology. In addition to being inflected for
gender, number, voice and case, words attach to var-
ious clitics for conjunction (w+ ’and’)1, the definite
article (Al+ ’the’), prepositions (e.g. b+ ’by/with’,
l+ ’for’, k+ ’as’), possessive pronouns and object
pronouns (e.g. +ny ’me/my’, +hm ’their/them’). For
example, the verbal form wsnsAEdhm and the nomi-
nal form wbsyAratnA can be decomposed as follows:
</bodyText>
<equation confidence="0.9023645">
(1) a. w+
and+
b. w+
and+
</equation>
<bodyText confidence="0.993089">
Also, Arabic is usually written without the diacritics
that denote the short vowels, and different sources
write a few characters inconsistently. These issues
create word-level ambiguity.
</bodyText>
<subsectionHeader confidence="0.997339">
3.1 Arabic Pre-processing
</subsectionHeader>
<bodyText confidence="0.999974266666667">
Due to the word-level ambiguity mentioned above,
but more generally, because a certain string of char-
acters can, in principle, be either an affixed mor-
pheme or part of the base word, morphological
decomposition requires both word-level linguistic
information and context analysis; simple pattern
matching is not sufficient to detect affixed mor-
phemes. To perform pre-translation morphologi-
cal decomposition of the Arabic source, we use the
morphological analyzer MADA. MADA uses SVM-
based classifiers for features (such as POS, number
and gender, etc.) to choose among the different anal-
yses of a given word in context.
We first normalize the Arabic by changing final
’Y’ to ’y’ and the various forms of Alif hamza to bare
</bodyText>
<footnote confidence="0.8801025">
1In this paper, Arabic text is written using Buckwalter
transliteration
</footnote>
<bodyText confidence="0.999310333333333">
Alif. We also remove diacritics wherever they occur.
We then apply one of two morphological decompo-
sition schemes before aligning the training data:
</bodyText>
<listItem confidence="0.971035375">
1. S1: Decliticization by splitting off each con-
junction clitic, particle, definite article and
pronominal clitic separately. Note that plural
and subject pronoun morphemes are not split.
2. S2: Same as S1, except that the split clitics are
glued into one prefix and one suffix, such that
any given word is split into at most three parts:
prefix+ stem +suffix.
</listItem>
<bodyText confidence="0.972948666666667">
For example the word wlAwlAdh (’and for his kids’)
is segmented to w+ l+ AwlAd +P:3MS according to
S1, and to wl+ AwlAd +P:3MS according to S2.
</bodyText>
<subsectionHeader confidence="0.998537">
3.2 Arabic Post-processing
</subsectionHeader>
<bodyText confidence="0.9981138">
As mentioned above, both training and decoding use
segmented Arabic. The final output of the decoder
must therefore be recombined into a surface form.
This proves to be a non-trivial challenge for a num-
ber of reasons:
</bodyText>
<listItem confidence="0.989988222222222">
1. Morpho-phonological Rules: For example, the
feminine marker ’p’ at the end of a word
changes to ’t’ when a suffix is attached to the
word. So syArp +P:1S recombines to syArty
(’my car’)
2. Letter Ambiguity: The character ’Y’ (Alf
mqSwrp) is normalized to ’y’. In the recom-
bination step we need to be able to decide
whether a final ’y’ was originally a ’Y’. For
example, mdy +P:3MS recombines to mdAh
’its extent’, since the ’y’ is actually a Y; but fy
+P:3MS recombines to fyh ’in it’.
3. Word Ambiguity: In some cases, a word can
recombine into 2 grammatically correct forms.
One example is the optional insertion of nwn
AlwqAyp (protective ’n’), so the segmented
word lkn +O:1S can recombine to either lknny
or lkny, both grammatically correct.
</listItem>
<bodyText confidence="0.832619">
To address these issues, we propose two recombina-
tion techniques:
</bodyText>
<listItem confidence="0.955682666666667">
1. R: Recombination rules defined manually. To
resolve word ambiguity we pick the grammat-
ical form that appears more frequently in the
</listItem>
<figure confidence="0.998970875">
n+
we+
+hm
+them
s+
will+
sAEd
help
+At
+PL
b+
with+
syAr
car
+nA
+our
</figure>
<page confidence="0.999558">
154
</page>
<bodyText confidence="0.960970266666667">
training data. To resolve letter ambiguity we
use a unigram language model trained on data
where the character ’Y’ had not been normal-
ized. We decide on the non-normalized from of
the ’y’ by comparing the unigram probability of
the word with ’y’ to its probability with ’Y’.
2. T: Uses a table derived from the training set
that maps the segmented form of the word to its
original form. If a segmented word has more
than one original form, one of them is picked
at random. The table is useful in recombin-
ing words that are split erroneously. For ex-
ample, qrDAy, a proper noun, gets incorrectly
segmented to qrDAn +P:1S which makes its re-
combination without the table difficult.
</bodyText>
<subsectionHeader confidence="0.975926">
3.3 Factored Models
</subsectionHeader>
<bodyText confidence="0.999601733333333">
For the Factored Translation Models experiment, the
factors on the English side are the POS tags and the
surface word. On the Arabic side, we use the sur-
face word, the stem and the POS tag concatenated
to the segmented clitics. For example, for the word
wlAwlAdh (’and for his kids’), the factored words are
AwlAd and w+l+N+P:3MS. We use two language
models: a trigram for surface words and a 7-gram
for the POS+clitic factor. We also use a genera-
tion model to generate the surface form from the
stem and POS+clitic, a translation table from POS
to POS+clitics and from the English surface word to
the Arabic stem. If the Arabic surface word cannot
be generated from the stem and POS+clitic, we back
off to translating it from the English surface word.
</bodyText>
<sectionHeader confidence="0.999642" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999992692307692">
The English source is aligned to the segmented Ara-
bic target using GIZA++ (Och and Ney, 2000), and
the decoding is done using the phrase-based SMT
system MOSES (MOSES, 2007). We use a max-
imum phrase length of 15 to account for the in-
crease in length of the segmented Arabic. Tuning
is done using Och’s algorithm (Och, 2003) to op-
timize weights for the distortion model, language
model, phrase translation model and word penalty
over the BLEU metric (Papineni et al., 2001). For
our baseline system the tuning reference was non-
segmented Arabic. For the segmented Arabic exper-
iments we experiment with 2 tuning schemes: T1
</bodyText>
<table confidence="0.9968932">
Scheme Training Set Tuning Set
Baseline 34.6% 36.8%
R 4.04% 4.65%
T N/A 22.1%
T + R N/A 1.9%
</table>
<tableCaption confidence="0.9945955">
Table 1: Recombination Results. Percentage of sentences
with mis-combined words.
</tableCaption>
<bodyText confidence="0.977327333333333">
uses segmented Arabic for reference, and T2 tunes
on non-segmented Arabic. The Factored Translation
Models experiments uses the MOSES system.
</bodyText>
<subsectionHeader confidence="0.997551">
4.1 Data Used
</subsectionHeader>
<bodyText confidence="0.999714607142857">
We experiment with two domains: text news and
spoken dialogue from the travel domain. For the
news training data we used corpora from LDC2. Af-
ter filtering out sentences that were too long to be
processed by GIZA (&gt; 85 words) and duplicate sen-
tences, we randomly picked 2000 development sen-
tences for tuning and 2000 sentences for testing. In
addition to training on the full set of 3 million words,
we also experimented with subsets of 1.6 million
and 600K words. For the language model, we used
20 million words from the LDC Arabic Gigaword
corpus plus 3 million words from the training data.
After experimenting with different language model
orders, we used 4-grams for the baseline system and
6-grams for the segmented Arabic. The English
source is downcased and the punctuations are sepa-
rated. The average sentence length is 33 for English,
25 for non-segmented Arabic and 36 for segmented
Arabic.
For the spoken language domain, we use the
IWSLT 2007 Arabic-English (Fordyce, 2007) cor-
pus which consists of a 200,000 word training set, a
500 sentence tuning set and a 500 sentence test set.
We use the Arabic side of the training data to train
the language model and use trigrams for the baseline
system and a 4-grams for segmented Arabic. The av-
erage sentence length is 9 for English, 8 for Arabic,
and 10 for segmented Arabic.
</bodyText>
<footnote confidence="0.988244666666667">
2Since most of the data was originally intended for Arabic-
to-English translation our test and tuning sets have only one
reference
</footnote>
<page confidence="0.996022">
155
</page>
<subsectionHeader confidence="0.998023">
4.2 Recombination Results
</subsectionHeader>
<bodyText confidence="0.999996">
To test the different recombination schemes de-
scribed in Section 3.2, we run these schemes on
the training and development sets of the news data,
and calculate the percentage of sentences with re-
combination errors (Note that, on average, there
is one mis-combined word per mis-combined sen-
tence). The scores are presented in Table 1. The
baseline approach consists of gluing the prefix and
suffix without processing the stem. T + R means that
the words seen in the training set were recombined
using scheme T and the remainder were recombined
using scheme R. In the remaining experiments we
use the scheme T + R.
</bodyText>
<subsectionHeader confidence="0.996447">
4.3 Translation Results
</subsectionHeader>
<bodyText confidence="0.99998947826087">
The 1-reference BLEU score results for the news
corpus are presented in Table 2; those for IWSLT are
in Table 3. We first note that the scores are generally
lower than those of comparable Arabic-to-English
systems. This is expected, since only one refer-
ence was used to evaluate translation quality and
since translating to a more morphologically com-
plex language is a more difficult task, where there
is a higher chance of translating word inflections in-
correctly. For the news corpus, the segmentation of
Arabic helps but the gain diminishes as the training
data size increases, since the model becomes less
sparse. This is consistent with the larger gain ob-
tained from segmentation for IWSLT. The segmen-
tation scheme S2 performs slightly better than S1.
The tuning scheme T2 performs better for the news
corpus, while T1 is better for the IWSLT corpus.
It is worth noting that tuning without segmentation
hurts the score for IWSLT, possibly because of the
small size of the training data. Factored models per-
form better than our approach with the large train-
ing corpus, although at a significantly higher cost in
terms of time and required resources.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998998666666667">
In this paper, we showed that making the Arabic
match better to the English through segmentation,
or by using additional translation model factors that
model grammatical information is beneficial, espe-
cially for smaller domains. We also presented sev-
eral methods for recombining the segmented Arabic
</bodyText>
<table confidence="0.999250375">
Training Size Large Medium Small
3M 1.6M 0.6M
Baseline 26.44 20.51 17.93
S1 + T1 tuning 26.46 21.94 20.59
S1 + T2 tuning 26.81 21.93 20.87
S2 + T1 tuning 26.86 21.99 20.44
S2 + T2 tuning 27.02 22.21 20.98
Factored Models + tuning 27.30 21.55 19.80
</table>
<tableCaption confidence="0.974453">
Table 2: BLEU (1-reference) scores for the News data.
</tableCaption>
<table confidence="0.9988435">
No Tuning T1 T2
Baseline 26.39 24.67
S1 29.07 29.82
S2 29.11 30.10 28.94
</table>
<tableCaption confidence="0.999869">
Table 3: BLEU (1-reference) scores for the IWSLT data.
</tableCaption>
<bodyText confidence="0.959622333333333">
target. Our results suggest that more sophisticated
techniques, such as syntactic reordering, should be
attempted.
</bodyText>
<sectionHeader confidence="0.998188" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9993665">
We would like to thank Ali Mohammad, Michael Collins and
Stephanie Seneff for their valuable comments.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997568">
Cameron S. Fordyce 2007. Overview of the 2007 IWSLT Eval-
uation Campaign. In Proc. of IWSLT 2007.
Nizar Habash and Owen Rambow, 2005. Arabic Tokenization,
Part-of-Speech Tagging and Morphological Disambiguation
in One Fell Swoop. In Proc. of ACL.
Nizar Habash and Fatiha Sadat, 2006. Arabic Preprocessing
Schemes for Statistical Machine Translation. In Proc. of
HLT.
Philipp Koehn and Hieu Hoang, 2007. Factored Translation
Models. In Proc. of EMNLP/CNLL.
Young-Suk Lee, 2004. Morphological Analysis for Statistical
Machine Translation. In Proc. of EMNLP.
MOSES, 2007. A Factored Phrase-based Beam-
search Decoder for Machine Translation. URL:
http://www.statmt.org/moses/.
Franz Och, 2003. Minimum Error Rate Training in Statistical
Machine Translation. In Proc. of ACL.
Franz Och and Hermann Ney, 2000. Improved Statistical
Alignment Models. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu, 2001. Bleu: a Method for Automatic Evaluation of
Machine Translation. In Proc. of ACL.
Ruhi Sarikaya and Yonggang Deng 2007. Joint
Morphological-Lexical Language Modeling for Machine
Translation. In Proc. of NAACL HLT.
</reference>
<page confidence="0.998788">
156
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871581">
<title confidence="0.999955">Segmentation for English-to-Arabic Statistical Machine Translation</title>
<author confidence="0.999847">Ibrahim Badr Rabih Zbib James Glass</author>
<affiliation confidence="0.999958">Computer Science and Artificial Intelligence Lab Massachusetts Institute of Technology</affiliation>
<address confidence="0.999997">Cambridge, MA 02139, USA</address>
<email confidence="0.993126">rabih,</email>
<abstract confidence="0.9875097">In this paper, we report on a set of initial results for English-to-Arabic Statistical Machine Translation (SMT). We show that morphological decomposition of the Arabic source is beneficial, especially for smaller-size corpora, and investigate different recombination techniques. We also report on the use of Factored Translation Models for Englishto-Arabic translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>153--156</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 153–156,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron S Fordyce</author>
</authors>
<title>Overview of the 2007 IWSLT Evaluation Campaign.</title>
<date>2007</date>
<booktitle>In Proc. of IWSLT</booktitle>
<marker>Fordyce, 2007</marker>
<rawString>Cameron S. Fordyce 2007. Overview of the 2007 IWSLT Evaluation Campaign. In Proc. of IWSLT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2602" citStr="Habash and Rambow, 2005" startWordPosition="393" endWordPosition="396">-Arabic SMT that we are aware of is by Sarikaya and Deng (2007). It uses shallow segmentation, and does not make use of contextual information. The emphasis of that work is on using Joint Morphological-Lexical Language Models to rerank the output. Most of the related work, though, is on Arabic-toEnglish SMT. Lee (2004) uses a trigram language model to segment Arabic words. She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with the English target. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. Both works show that the improvements obtained from segmentation decrease as the corpus size increases. As will be shown later, we observe the same trend, which is due to the fact that the model becomes less sparse with more training data. There has been work on translating from English to other morphologically complex languages. Koehn and Hoang (2007) present Factored Translation Models as an extension to phrase-based statistical machine translation models. Factored models allow the integration of additional morphologic</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Nizar Habash and Owen Rambow, 2005. Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Fatiha Sadat</author>
</authors>
<title>Arabic Preprocessing Schemes for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT.</booktitle>
<contexts>
<context position="1109" citStr="Habash and Sadat, 2006" startWordPosition="155" endWordPosition="158">, and investigate different recombination techniques. We also report on the use of Factored Translation Models for Englishto-Arabic translation. 1 Introduction Arabic has a complex morphology compared to English. Words are inflected for gender, number, and sometimes grammatical case, and various clitics can attach to word stems. An Arabic corpus will therefore have more surface forms than an English corpus of the same size, and will also be more sparsely populated. These factors adversely affect the performance of ArabicHEnglish Statistical Machine Translation (SMT). In prior work (Lee, 2004; Habash and Sadat, 2006), it has been shown that morphological segmentation of the Arabic source benefits the performance of Arabic-to-English SMT. The use of similar techniques for English-to-Arabic SMT requires recombination of the target side into valid surface forms, which is not a trivial task. In this paper, we present an initial set of experiments on English-to-Arabic SMT. We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus. We show that segmenting the Arabic target in training and decoding improves performance. We </context>
<context position="2533" citStr="Habash and Sadat (2006)" startWordPosition="382" endWordPosition="385">ic translation. 2 Previous Work The only previous work on English-to-Arabic SMT that we are aware of is by Sarikaya and Deng (2007). It uses shallow segmentation, and does not make use of contextual information. The emphasis of that work is on using Joint Morphological-Lexical Language Models to rerank the output. Most of the related work, though, is on Arabic-toEnglish SMT. Lee (2004) uses a trigram language model to segment Arabic words. She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with the English target. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. Both works show that the improvements obtained from segmentation decrease as the corpus size increases. As will be shown later, we observe the same trend, which is due to the fact that the model becomes less sparse with more training data. There has been work on translating from English to other morphologically complex languages. Koehn and Hoang (2007) present Factored Translation Models as an extension to phrase-based statistical machine translation mo</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>Nizar Habash and Fatiha Sadat, 2006. Arabic Preprocessing Schemes for Statistical Machine Translation. In Proc. of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP/CNLL.</booktitle>
<contexts>
<context position="1890" citStr="Koehn and Hoang, 2007" startWordPosition="276" endWordPosition="279">nglish-to-Arabic SMT requires recombination of the target side into valid surface forms, which is not a trivial task. In this paper, we present an initial set of experiments on English-to-Arabic SMT. We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus. We show that segmenting the Arabic target in training and decoding improves performance. We propose various schemes for recombining the segmented Arabic, and compare their effect on translation. We also report on applying Factored Translation Models (Koehn and Hoang, 2007) for English-to-Arabic translation. 2 Previous Work The only previous work on English-to-Arabic SMT that we are aware of is by Sarikaya and Deng (2007). It uses shallow segmentation, and does not make use of contextual information. The emphasis of that work is on using Joint Morphological-Lexical Language Models to rerank the output. Most of the related work, though, is on Arabic-toEnglish SMT. Lee (2004) uses a trigram language model to segment Arabic words. She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang, 2007. Factored Translation Models. In Proc. of EMNLP/CNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological Analysis for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="1084" citStr="Lee, 2004" startWordPosition="153" endWordPosition="154">ize corpora, and investigate different recombination techniques. We also report on the use of Factored Translation Models for Englishto-Arabic translation. 1 Introduction Arabic has a complex morphology compared to English. Words are inflected for gender, number, and sometimes grammatical case, and various clitics can attach to word stems. An Arabic corpus will therefore have more surface forms than an English corpus of the same size, and will also be more sparsely populated. These factors adversely affect the performance of ArabicHEnglish Statistical Machine Translation (SMT). In prior work (Lee, 2004; Habash and Sadat, 2006), it has been shown that morphological segmentation of the Arabic source benefits the performance of Arabic-to-English SMT. The use of similar techniques for English-to-Arabic SMT requires recombination of the target side into valid surface forms, which is not a trivial task. In this paper, we present an initial set of experiments on English-to-Arabic SMT. We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus. We show that segmenting the Arabic target in training and decoding </context>
<context position="2298" citStr="Lee (2004)" startWordPosition="344" endWordPosition="345">oves performance. We propose various schemes for recombining the segmented Arabic, and compare their effect on translation. We also report on applying Factored Translation Models (Koehn and Hoang, 2007) for English-to-Arabic translation. 2 Previous Work The only previous work on English-to-Arabic SMT that we are aware of is by Sarikaya and Deng (2007). It uses shallow segmentation, and does not make use of contextual information. The emphasis of that work is on using Joint Morphological-Lexical Language Models to rerank the output. Most of the related work, though, is on Arabic-toEnglish SMT. Lee (2004) uses a trigram language model to segment Arabic words. She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with the English target. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. Both works show that the improvements obtained from segmentation decrease as the corpus size increases. As will be shown later, we observe the same trend, which is due to the fact that the model becomes less sparse with mo</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee, 2004. Morphological Analysis for Statistical Machine Translation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MOSES</author>
</authors>
<title>A Factored Phrase-based Beamsearch Decoder for Machine Translation.</title>
<date>2007</date>
<location>URL: http://www.statmt.org/moses/.</location>
<marker>MOSES, 2007</marker>
<rawString>MOSES, 2007. A Factored Phrase-based Beamsearch Decoder for Machine Translation. URL: http://www.statmt.org/moses/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Och, 2003</marker>
<rawString>Franz Och, 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Och and Hermann Ney, 2000. Improved Statistical Alignment Models. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu, 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>Yonggang Deng</author>
</authors>
<title>Joint Morphological-Lexical Language Modeling for Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL HLT.</booktitle>
<contexts>
<context position="2041" citStr="Sarikaya and Deng (2007)" startWordPosition="300" endWordPosition="303">itial set of experiments on English-to-Arabic SMT. We report results from two domains: text news, trained on a large corpus, and spoken travel conversation, trained on a significantly smaller corpus. We show that segmenting the Arabic target in training and decoding improves performance. We propose various schemes for recombining the segmented Arabic, and compare their effect on translation. We also report on applying Factored Translation Models (Koehn and Hoang, 2007) for English-to-Arabic translation. 2 Previous Work The only previous work on English-to-Arabic SMT that we are aware of is by Sarikaya and Deng (2007). It uses shallow segmentation, and does not make use of contextual information. The emphasis of that work is on using Joint Morphological-Lexical Language Models to rerank the output. Most of the related work, though, is on Arabic-toEnglish SMT. Lee (2004) uses a trigram language model to segment Arabic words. She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with the English target. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they pro</context>
</contexts>
<marker>Sarikaya, Deng, 2007</marker>
<rawString>Ruhi Sarikaya and Yonggang Deng 2007. Joint Morphological-Lexical Language Modeling for Machine Translation. In Proc. of NAACL HLT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>