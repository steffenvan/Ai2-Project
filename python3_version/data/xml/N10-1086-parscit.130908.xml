<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000611">
<title confidence="0.969105">
Good Question! Statistical Ranking for Question Generation
</title>
<author confidence="0.993962">
Michael Heilman Noah A. Smith
</author>
<affiliation confidence="0.884069333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998761">
{mheilman,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9817313125">
We address the challenge of automatically
generating questions from reading materials
for educational practice and assessment. Our
approach is to overgenerate questions, then
rank them. We use manually written rules to
perform a sequence of general purpose syn-
tactic transformations (e.g., subject-auxiliary
inversion) to turn declarative sentences into
questions. These questions are then ranked by
a logistic regression model trained on a small,
tailored dataset consisting of labeled output
from our system. Experimental results show
that ranking nearly doubles the percentage of
questions rated as acceptable by annotators,
from 27% of all questions to 52% of the top
ranked 20% of questions.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.973950705882353">
In this paper, we focus on question generation (QG)
for the creation of educational materials for read-
ing practice and assessment. Our goal is to gener-
ate fact-based questions about the content of a given
article. The top-ranked questions could be filtered
and revised by educators, or given directly to stu-
dents for practice. Here we restrict our investigation
to questions about factual information in texts.
We begin with a motivating example. Consider
the following sentence from the Wikipedia article on
the history of Los Angeles:1 During the Gold Rush
years in northern California, Los Angeles became
known as the “Queen of the Cow Counties” for its
role in supplying beef and other foodstuffs to hungry
miners in the north.
Consider generating the following question from
that sentence: What did Los Angeles become known
</bodyText>
<note confidence="0.546106333333333">
1“History of Los Angeles.” Wikipedia. 2009. Wikimedia
Foundation, Inc. Retrieved Nov. 17, 2009 from: http://en.
wikipedia.org/wiki/History_of_Los_Angeles.
</note>
<bodyText confidence="0.996213142857143">
as the “Queen of the Cow Counties” for?
We observe that the QG process can be viewed
as a two-step process that essentially “factors” the
problem into simpler components.2 Rather than si-
multaneously trying to remove extraneous informa-
tion and transform a declarative sentence into an in-
terrogative one, we first transform the input sentence
into a simpler sentence such as Los Angeles become
known as the “Queen of the Cow Counties” for its
role in supplying beef and other foodstuffs to hungry
miners in the north, which we then can then trans-
form into a more succinct question.
Question transformation involves complex long
distance dependencies. For example, in the ques-
tion about Los Angeles, the word what at the begin-
ning of the sentence is a semantic argument of the
verb phrase known as ... at the end of the ques-
tion. The characteristics of such phenomena are (ar-
guably) difficult to learn from corpora, but they have
been studied extensively in linguistics (Ross, 1967;
Chomsky, 1973). We take a rule-based approach in
order to leverage this linguistic knowledge.
However, since many phenomena pertaining to
question generation are not so easily encoded with
rules, we include statistical ranking as an integral
component. Thus, we employ an overgenerate-and-
rank approach, which has been applied successfully
in areas such as generation (Walker et al., 2001;
Langkilde and Knight, 1998) and syntactic parsing
(Collins, 2000). Since large datasets of the appro-
priate domain, style, and form of questions are not
available to train our ranking model, we learn to rank
from a relatively small, tailored dataset of human-
labeled output from our rule-based system.
The remainder of the paper is organized as fol-
</bodyText>
<footnote confidence="0.99670625">
2The motivating example does not exhibit lexical semantic
variations such as synonymy. In this work, we do not model
complex paraphrasing, but believe that paraphrase generation
techniques could be incorporated into our approach.
</footnote>
<page confidence="0.945373">
609
</page>
<note confidence="0.7562955">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 609–617,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999788555555556">
lows. §2 clarifies connections to prior work and enu-
merates our contributions. §3 discusses particular
terms and conventions we will employ. §4 discusses
rule-based question transformation. §5 describes the
data used to learn and to evaluate our question rank-
ing model, and §6 then follows with details on the
ranking approach itself. We then present and dis-
cuss results from an evaluation of ranked question
output in §7 and conclude in §8.
</bodyText>
<sectionHeader confidence="0.828975" genericHeader="introduction">
2 Connections with Prior Work
</sectionHeader>
<bodyText confidence="0.999742927272727">
The generation of questions by humans has long mo-
tivated theoretical work in linguistics (e.g., Ross,
1967), particularly work that portrays questions as
transformations of canonical declarative sentences
(Chomsky, 1973).
Questions have also been a major topic of study
in computational linguistics, but primarily with the
goal of answering questions (Dang et al., 2008).
While much of the question answering research has
focused on retrieval or extraction (e.g., Ravichan-
dran and Hovy, 2001; Hovy et al., 2001), mod-
els of the transformation from answers to questions
have also been developed (Echihabi and Marcu,
2003) with the goal of finding correct answers given
a question (e.g., in a source-channel framework).
Also, Harabagiu et al. (2005) present a system that
automatically generates questions from texts to pre-
dict which user-generated questions the text might
answer. In such work on question answering, ques-
tion generation models are typically not evaluated
for their intrinsic quality, but rather with respect to
their utility as an intermediate step in the question
answering process.
QG is very different from many natural language
generation problems because the input is natural lan-
guage rather than a formal representation (cf. Reiter
and Dale, 1997). It is also different from some other
tasks related to generation: unlike machine transla-
tion (e.g., Brown et al., 1990), the input and output
for QG are in the same language, and their length
ratio is often far from one to one; and unlike sen-
tence compression (e.g., Knight and Marcu, 2000),
QG may involve substantial changes to words and
their ordering, beyond simple removal of words.
Some previous research has directly approached
the topic of generating questions for educational
purposes (Mitkov and Ha, 2003; Kunichika et al.,
2004; Gates, 2008; Rus and Graessar, 2009; Rus and
Lester, 2009), but to our knowledge, none has in-
volved statistical models for choosing among output
candidates. Mitkov et al. (2006) demonstrated that
automatic generation and manual correction of ques-
tions can be more time-efficient than manual author-
ing alone. Much of the prior QG research has evalu-
ated systems in specific domains (e.g., introductory
linguistics, English as a Second Language), and thus
we do not attempt empirical comparisons. Exist-
ing QG systems model their transformations from
source text to questions with many complex rules
for specific question types (e.g., a rule for creating
a question Who did the Subject Verb? from a
sentence with SVO word order and an object refer-
ring to a person), rather than with sets of general
rules.
This paper’s contributions are as follows:
</bodyText>
<listItem confidence="0.993665066666667">
• We apply statistical ranking to the task of gen-
erating natural language questions. In doing so,
we show that question rankings are improved by
considering features beyond surface characteris-
tics such as sentence lengths.
• We model QG as a two-step process of first
simplifying declarative input sentences and then
transforming them into questions, the latter step
being achieved by a sequence of general rules.
• We incorporate linguistic knowledge to explic-
itly model well-studied phenomena related to long
distance dependencies in WH questions, such as
noun phrase island constraints.
• We develop a QG evaluation methodology, in-
cluding the use of broad-domain corpora.
</listItem>
<sectionHeader confidence="0.952114" genericHeader="method">
3 Definitions and Conventions
</sectionHeader>
<bodyText confidence="0.999853">
The term “source sentence” refers to a sentence
taken directly from the input document, from which
a question will be generated (e.g., Kenya is located
in Africa.). The term “answer phrase” refers to
phrases in declarative sentences which may serve
as targets for WH-movement, and therefore as possi-
ble answers to generated questions (e.g., in Africa).
The term “question phrase” refers to the phrase con-
taining the WH word that replaces an answer phrase
(e.g., Where in Where is Kenya located?).
</bodyText>
<page confidence="0.995027">
610
</page>
<bodyText confidence="0.9999164375">
To represent the syntactic structure of sentences,
we use simplified Penn Treebank-style phrase struc-
ture trees, including POS and category labels, as
produced by the Stanford Parser (Klein and Man-
ning, 2003). Noun phrase heads are selected using
Collins’ rules (Collins, 1999).
To implement the rules for transforming source
sentences into questions, we use Tregex, a tree
query language, and Tsurgeon, a tree manipula-
tion language built on top of Tregex (Levy and An-
drew, 2006). The Tregex language includes vari-
ous relational operators based on the primitive re-
lations of immediate dominance (denoted “&lt;”) and
immediate precedence (denoted “.”). Tsurgeon
adds the ability to modify trees by relabeling, delet-
ing, moving, and inserting nodes.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="method">
4 Rule-based Overgeneration
</sectionHeader>
<bodyText confidence="0.9998475">
Many useful questions can be viewed as lexical, syn-
tactic, or semantic transformations of the declarative
sentences in a text. We describe how to model this
process in two steps, as proposed in §1.3
</bodyText>
<subsectionHeader confidence="0.998628">
4.1 Sentence Simplification
</subsectionHeader>
<bodyText confidence="0.999951857142857">
In the first step for transforming sentences into ques-
tions, each of the sentences from the source text is
expanded into a set of derived declarative sentences
(which also includes the original sentence) by al-
tering lexical items, syntactic structure, and seman-
tics. Many existing NLP transformations could po-
tentially be exploited in this step, including sentence
compression, paraphrase generation, or lexical se-
mantics for word substitution.
In our implementation, a set of transformations
derive a simpler form of the source sentence by
removing phrase types such as leading conjunc-
tions, sentence-level modifying phrases, and apposi-
tives. Tregex expressions identify the constituents
to move, alter, or delete. Similar transformations
have been utilized in previous work on headline gen-
eration (Dorr and Zajic, 2003) and summarization
(Toutanova et al., 2007).
To enable questions about syntactically embedded
content, our implementation also extracts a set of
declarative sentences from any finite clauses, rela-
</bodyText>
<footnote confidence="0.4811075">
3See Heilman and Smith (2009) for details on the rule-based
component.
</footnote>
<bodyText confidence="0.999586444444444">
tive clauses, appositives, and participial phrases that
appear in the source sentence. For example, it trans-
forms the sentence Selling snowballed because of
waves of automatic stop-loss orders, which are trig-
gered by computer when prices fall to certain lev-
els into Automatic stop-loss orders are triggered by
computer when prices fall to certain levels, from
which the next step will produce What are triggered
by computer when prices fall to certain levels?.
</bodyText>
<subsectionHeader confidence="0.998014">
4.2 Question Transformation
</subsectionHeader>
<bodyText confidence="0.999560911764706">
In the second step, the declarative sentences de-
rived in step 1 are transformed into sets of ques-
tions by a sequence of well-defined syntactic and
lexical transformations (subject-auxiliary inversion,
WH-movement, etc.). It identifies the answer phrases
which may be targets for WH-movement and con-
verts them into question phrases.4
In the current implementation, answer phrases can
be noun phrases or prepositional phrases, which en-
ables who, what, where, when, and how much ques-
tions. The system could be extended to transform
other types of phrases into other types of questions
(e.g., how, why, and what kind of). It should be
noted that the transformation from answer to ques-
tion is achieved by applying a series of general-
purpose rules. This would allow, for example, the
addition of a rule to generate why questions that
builds off of the existing rules for subject-auxiliary
inversion, verb decomposition, etc. In contrast, pre-
vious QG approaches have employed separate rules
for specific sentence types (e.g., Mitkov and Ha,
2003; Gates, 2008).
For each sentence, many questions may be pro-
duced: there are often multiple possible answer
phrases, and multiple question phrases for each an-
swer phrase. Hence many candidates may result
from the transformations.
These rules encode a substantial amount of lin-
guistic knowledge about the long distance depen-
dencies prevalent in questions, which would be chal-
lenging to learn from existing corpora of questions
and answers consisting typically of only thousands
of examples (e.g., Voorhees, 2003).
Specifically, the following sequence of transfor-
</bodyText>
<footnote confidence="0.945635">
4We leave the generation of correct answers and distractors
to future work.
</footnote>
<page confidence="0.987976">
611
</page>
<figure confidence="0.999620490566038">
WHNP
What did Los Angeles become known as the &amp;quot;QotCC&amp;quot; for?
Los Angeles became known as the &amp;quot;QotCC&amp;quot; for
Answer Phrase: its role...
WP VBD
During the Gold Rush years in northern California,
Los Angeles became known as the &amp;quot;Queen of the
Cow Counties&amp;quot; for its role in supplying beef and
other foodstuffs to hungry miners in the north.
NP
SBARQ
NP
VBD
Source Sentence
S
VBN
VB
SQ
VP
VP
PP
IN NP
VBN
VP
VP
PP
IN NP
PP
IN
Movement and Insertion of Question Phrase
PP
IN
Main Verb Decomposition
Answer Phrase Selection
(other possibilities)
Sentence Simplification
Statistical Ranking
(other possibilities)
1. What became known as ...?
2. What did Los Angeles become known...for?
3. What did Los Angeles become known...as?
4. During the Gold Rush years... ?
5. Whose role in supplying beef...?
...
Los Angeles did become known as the &amp;quot;QotCC&amp;quot; for
did Los Angeles become known as the &amp;quot;QotCC&amp;quot; for
Los Angeles became known as the &amp;quot;Queen of the
Cow Counties&amp;quot; for its role in supplying beef and
other foodstuffs to hungry miners in the north.
Subject-Auxiliary
Inversion
(other possibilities)
(other possibilities)
</figure>
<figureCaption confidence="0.985201">
Figure 1: An illustration of the sequence of steps for generating questions. For clarity, trees are not shown for all steps.
Also, while many questions may be generated from a single source sentence, only one path is shown.
</figureCaption>
<bodyText confidence="0.9974082">
mations is performed, as illustrated in Figure 1:
mark phrases that cannot be answer phrases due to
constraints on WH movement (§4.2.1, not in figure);
select an answer phrase, remove it, and generate pos-
sible question phrases for it (§4.2.2); decompose the
main verb; invert the subject and auxiliary verb; and
insert one of the possible question phrases.
Some of these steps do not apply in all cases. For
example, no answer phrases are removed when gen-
erating yes-no questions.
</bodyText>
<subsectionHeader confidence="0.983829">
4.2.1 Marking Unmovable Phrases
</subsectionHeader>
<bodyText confidence="0.99057795">
In English, various constraints determine whether
phrases can be involved in WH-movement and other
phenomena involving long distance dependencies.
In a seminal dissertation, Ross (1967) described
many of these phenomena. Goldberg (2006) pro-
vides a concise summary of them.
For example, noun phrases are “islands” to
movement, meaning that constituents dominated
by a noun phrase typically cannot undergo WH-
movement. Thus, from John liked the book that I
gave him, we generate What did John like? but not
*Who did John like the book that gave him?.
We operationalize this linguistic knowledge to ap-
propriately restrict the set of questions produced.
Eight Tregex expressions mark phrases that cannot
be answer phrases due to WH-movement constraints.
For example, the following expression encodes
the noun phrase island constraint described above,
where unmv indicates unmovable noun phrases:
NP &lt;&lt; NP=unmv.
</bodyText>
<subsubsectionHeader confidence="0.606439">
4.2.2 Generating Possible Question Phrases
</subsubsectionHeader>
<bodyText confidence="0.9999275">
After marking unmovable phrases, we iteratively
remove each possible answer phrase and generate
possible question phrases from it. The system an-
notates the source sentence with a set of entity types
taken from the BBN Identifinder Text Suite (Bikel
et al., 1999) and then uses these entity labels along
with the syntactic structure of a given answer phrase
to generate zero or more question phrases, each of
which is used to generate a final question. (This step
is skipped for yes-no questions.)
</bodyText>
<sectionHeader confidence="0.8114425" genericHeader="method">
5 Rating Questions for Evaluation and
Learning to Rank
</sectionHeader>
<bodyText confidence="0.999986428571429">
Since different sentences from the input text, as well
as different transformations of those sentences, may
be more or less likely to lead to high-quality ques-
tions, each question is scored according to features
of the source sentence, the input sentence, the ques-
tion, and the transformations used in its generation.
The scores are used to rank the questions. This is
</bodyText>
<page confidence="0.992751">
612
</page>
<bodyText confidence="0.999665606060606">
an example of an “overgenerate-and-rank” strategy
(Walker et al., 2001; Langkilde and Knight, 1998).
This section describes the acquisition of a set
of rated questions produced by the steps described
above. Separate portions of these labeled data will
be used to develop a discriminative question ranker
(§6), and to evaluate ranked lists of questions (§7).
Fifteen native English-speaking university stu-
dents rated a set of questions produced from steps
1 and 2, indicating whether each question exhibited
any of the deficiencies listed in Table 1.5 If a ques-
tion exhibited no deficiencies, raters were asked to
label it “acceptable.” Annotators were asked to read
the text of a newswire or encyclopedia article (§5.1
describes the corpora used), and then rate approxi-
mately 100 questions generated from that text. They
were asked to consider each question independently,
such that similar questions about the same informa-
tion would receive similar ratings.
For a predefined training set, each question was
rated by a single annotator (not the same for each
question), leading to a large number of diverse ex-
amples. For the test set, each question was rated by
three people (again, not the same for each question)
to provide a more reliable gold standard. To assign
final labels to the test data, a question was labeled as
acceptable only if a majority of the three raters rated
it as acceptable (i.e., without deficiencies).6
An inter-rater agreement of Fleiss’s r. = 0.42
was computed from the test set’s acceptability rat-
ings. This value corresponds to “moderate agree-
ment” (Landis and Koch, 1977) and is somewhat
lower than for other rating schemes.7
</bodyText>
<subsectionHeader confidence="0.964527">
5.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999809">
The training and test datasets consisted of 2,807
and 428 questions, respectively. The questions were
</bodyText>
<footnote confidence="0.855754166666667">
5The ratings from one person were excluded due to an ex-
tremely high rate of accepting questions as error-free and other
irregularities.
6The percentages in Table 1 do not add up to 100% for two
reasons: first, questions are labeled acceptable in the test set
only if the majority of raters labeled them as having no defi-
ciencies, rather than the less strict criterion of requiring no de-
ficiencies to be identified by a majority of raters; second, the
categories are not mutually exclusive.
7E.g., Dolan and Brockett (2005) and Glickman et al. (2005)
report r, values around 0.6 for paraphrase identification and tex-
tual entailment, respectively.
</footnote>
<bodyText confidence="0.999424961538461">
generated from three corpora.
The first corpus was a random sample from the
featured articles in the English Wikipedia8 with be-
tween 250 and 2,000 word tokens. This English
Wikipedia corpus provides expository texts written
at an adult reading level from a variety of domains,
which roughly approximates the prose that a sec-
ondary or post-secondary student would encounter.
By choosing from the featured articles, we intended
to select well-edited articles on topics of general in-
terest. The training set included 1,328 questions
about 12 articles, and the test set included 120 ques-
tions about 2 articles from this corpus.
The second corpus was a random sample from the
articles in the Simple English Wikipedia of simi-
lar length. This corpus provides similar text but at
a reading level corresponding to elementary educa-
tion or intermediate second language learning.9 The
training set included 1,195 questions about 16 arti-
cles, and the test set included 118 questions about 2
articles from this corpus.
The third corpus was Section 23 of the Wall Street
Journal data in the Penn Treebank (Marcus et al.,
1993).10 The training set included 284 questions
about 8 articles, and the test set included 190 ques-
tions about 2 articles from this corpus.
</bodyText>
<sectionHeader confidence="0.996375" genericHeader="method">
6 Ranking
</sectionHeader>
<bodyText confidence="0.999979363636364">
We use a discriminative ranker to rank questions,
similar to the approach described by Collins (2000)
for ranking syntactic parses. Questions are ranked
by the predictions of a logistic regression model of
question acceptability. Given the question q and
source text t, the model defines a binomial distribu-
tion p(R  |q, t), with binary random variable R rang-
ing over a (“acceptable”) and u (“unacceptable”).
We estimate the parameters by optimizing the reg-
ularized log-likelihood of the training data (cf. §5.1)
with a variant of Newton’s method (le Cessie and
</bodyText>
<footnote confidence="0.990310555555555">
8The English and Simple English Wikipedia data were
downloaded on December 16, 2008 from http://en.
wikipedia.org and http://simple.wikipedia.
org, respectively.
9The subject matter of the articles in the two Wikipedia cor-
pora was not matched.
10In separate experiments with the Penn Treebank, gold-
standard parses led to an absolute increase of 15% in the per-
centage of acceptable questions (Heilman and Smith, 2009).
</footnote>
<page confidence="0.996694">
613
</page>
<bodyText confidence="0.8883199375">
Question Deficiency Description %
Ungrammatical The question is not a valid English sentence. (e.g., In what were nests excavated exposed to the 14.0
sun? from ... eggs are usually laid ..., in nests excavated in pockets of earth exposed to the
sun.. This error results from the incorrect attachment by the parser of exposed to the sun to the
verb phrase headed by excavated)
Does not make sense The question is grammatical but indecipherable. (e.g., Who was the investment?) 20.6
Vague The question is too vague to know exactly what it is asking about, even after reading the article 19.6
(e.g., What do modern cities also have? from ..., but modern cities also have many problems).
Obvious answer The correct answer would be obvious even to someone who has not read the article (e.g., a 0.9
question where the answer is obviously the subject of the article).
Missing answer The answer to the question is not in the article. 1.4
Wrong WH word The question would be acceptable if the WH phrase were different (e.g., a what question with a 4.9
person’s name as the answer).
Formatting There are minor formatting errors (e.g., with respect to capitalization, punctuation). 8.9
Other The question was unacceptable for other reasons. 1.2
None The question exhibits none of the above deficiencies and is thus acceptable. 27.3
</bodyText>
<tableCaption confidence="0.984286">
Table 1: Deficiencies a question may exhibit, and the percentages of test set questions labeled with them.
</tableCaption>
<bodyText confidence="0.998827610169492">
van Houwelingen, 1997). In our experiments, the
regularization constant was selected through cross-
validation on the training data.
The features used by the ranker can be organized
into several groups described in this section. This
feature set was developed by an analysis of ques-
tions generated from the training set. The num-
bers of distinct features for each type are denoted in
parentheses, with the second number, after the ad-
dition symbol, indicating the number of histogram
features (explained below) for that type.
Length Features (3 + 24) The set includes integer
features for the numbers of tokens in the question,
the source sentence, and the answer phrase from
which the WH phrase was generated. These num-
bers of tokens will also be used for computing the
histogram features discussed below.
WH Words (9 + 0) The set includes boolean fea-
tures for the presence of each possible WH word in
the question.
Negation (1 + 0) This is a boolean feature for the
presence of not, never, or no in the question.
N-Gram Language Model Features (6 + 0) The
set includes real valued features for the log like-
lihoods and length-normalized log likelihoods of
the question, the source sentence, and the answer
phrase. Separate likelihood features are included for
unigram and trigram language models. These lan-
guage models were estimated from the written por-
tion of the American National Corpus Second Re-
lease (Ide and Suderman, 2004), which consists of
approximately 20 million tokens, using Kneser and
Ney (1995) smoothing.
Grammatical Features (23 + 95) The set includes
integer features for the numbers of proper nouns,
pronouns, adjectives, adverbs, conjunctions, num-
bers, noun phrases, prepositional phrases, and sub-
ordinate clauses in the phrase structure parse trees
for the question and answer phrase. It also includes
one integer feature for the number of modifying
phrases at the start of the question (e.g., as in At
the end of the Civil War, who led the Union Army?);
three boolean features for whether the main verb is
in past, present, or future tense; and one boolean fea-
ture for whether the main verb is a form of be.
Transformations (8 + 0) The set includes bi-
nary features for the possible syntactic transforma-
tions (e.g., removal of appositives and parentheti-
cals, choosing the subject of source sentence as the
answer phrase).
Vagueness (3 + 15) The set includes integer fea-
tures for the numbers of noun phrases in the ques-
tion, source sentence, and answer phrase that are
potentially vague. We define this set to include pro-
nouns as well as common nouns that are not speci-
fied by a subordinate clause, prepositional phrase, or
possessive. In the training data, we observed many
vague questions resulting from such noun phrases
(e.g., What is the bridge named for?).
</bodyText>
<page confidence="0.997263">
614
</page>
<bodyText confidence="0.9996937">
Histograms In addition to the integer features for
lengths, counts of grammatical types, and counts of
vague noun phrases, the set includes binary “his-
togram” features for each length or count. These
features indicate whether a count or length exceeds
various thresholds: 0, 1, 2, 3, and 4 for counts; 0,
4, 8, 12, 16, 20, 24, and 28 for lengths. We aim to
account for potentially non-linear relationships be-
tween question quality and these values (e.g., most
good questions are neither very long nor very short).
</bodyText>
<sectionHeader confidence="0.949422" genericHeader="evaluation">
7 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999981">
This section describes the results of experiments to
evaluate the quality of generated questions before
and after ranking. Results are aggregated across the
3 corpora (§5.1). The evaluation metric we employ
is the percentage of test set questions labeled as ac-
ceptable. For rankings, our metric is the percentage
of the top N% labeled as acceptable, for various N.
</bodyText>
<sectionHeader confidence="0.644726" genericHeader="evaluation">
7.1 Results for Unranked Questions
</sectionHeader>
<bodyText confidence="0.999996714285714">
First, we present results for the unranked questions
produced by the rule-based overgenerator. As shown
in Table 1, 27.3% of test set questions were labeled
acceptable (i.e., having no deficiencies) by a major-
ity of raters.11
The most frequent deficiencies were ungrammati-
cality (14.0%), vagueness (19.6%), and semantic er-
rors labeled with the “Does not make sense” cate-
gory (20.6%). Formatting errors (8.9%) were due
to both straightforward issues with pre-processing
and more challenging issues such as failing to iden-
tifying named entities (e.g., Who was nixon’s second
vice president?).
While Table 1 provides data on how often bad
questions were generated, a measure of how often
good questions were not generated would require
knowing the number of possible valid questions. In-
stead, we provide a measure of productivity: the sys-
tem produced an average of 6.0 acceptable questions
per 250 words (i.e., the approximate average number
of words on a single page in a printed book).
</bodyText>
<subsectionHeader confidence="0.999447">
7.2 Configurations and Baselines
</subsectionHeader>
<bodyText confidence="0.9835915">
For ranking experiments, we present results for the
following configurations of features:
1112.1% of test set questions were unanimously acceptable.
All This configuration includes the entire set of
features described in §6.
Surface Features This configuration includes
only features that can be computed from the sur-
face form of the question, source sentence, and
answer phrase—that is, without hidden linguistic
structures such as parts of speech or syntactic struc-
tures. Specifically, it includes features for lengths,
length histograms, WH words, negation, and lan-
guage model likelihoods.
Question Only This configuration includes all
features of questions, but no features involving the
source sentence or answer phrase (e.g., it does not
include source sentence part of speech counts). It
does not include transformation features.
We also present two baselines for comparison:
Random The expectation of the performance if
questions were ranked randomly.
Oracle The expected performance if all questions
that were labeled acceptable were ranked higher
than all questions that were labeled unacceptable.
</bodyText>
<subsectionHeader confidence="0.999751">
7.3 Ranking Results
</subsectionHeader>
<bodyText confidence="0.983098681818182">
Figure 2 shows that the percentage of questions
rated as acceptable generally increases as the set
of questions is restricted from the full 428 ques-
tions in the test set to only the top ranked questions.
While 27.3% of all test set questions were accept-
able, 52.3% of the top 20% of ranked questions were
acceptable. Thus, the quality of the top fifth was
nearly doubled by ranking with all the features.
Ranking with surface features also improved
question quality, but to a lesser extent. Thus, unob-
served linguistic features such as parts of speech and
syntax appear to add value for ranking questions.12
The ranker seems to have focused on the “Does
not make sense” and “Vague” categories. The
percentage of nonsensical questions dropped from
20.6% to 4.7%, and vagueness dropped from 19.6%
12Ranking with all features was statistically significantly bet-
ter (p &lt; .05) in terms of the percentage of acceptable questions
in the top ranked 20% than ranking with the “question only”
or “surface” configurations, or the random baseline, as verified
by computing 95% confidence intervals with the BC, Bootstrap
(Efron and Tibshirani, 1993).
</bodyText>
<page confidence="0.991565">
615
</page>
<figure confidence="0.982779">
0
</figure>
<figureCaption confidence="0.999072">
Figure 2: A graph of the percentage of acceptable ques-
</figureCaption>
<bodyText confidence="0.871294428571429">
tions in the top-N questions in the test set, using various
rankings, for N varying from 0 to the size of the test set.
The percentages become increasingly unstable when re-
stricted to very few questions (e.g., &lt; 50).
to 7.0%, while ungrammaticality dropped from
14.0% to 10.5%, and the other, less prevalent, cat-
egories changed very little.13
</bodyText>
<subsectionHeader confidence="0.997743">
7.4 Ablation Study
</subsectionHeader>
<bodyText confidence="0.999949952380952">
Ablation experiments were also conducted to study
the effects of removing each of the different types of
features. Table 2 presents the percentages of accept-
able test set questions in the top 20% and top 40%
when they are scored by rankers trained with vari-
ous feature sets that are defined by removing various
feature types from the set of all possible features.
Grammatical features appear to be the most im-
portant: removing them from the feature set resulted
in a 9.0% absolute drop in acceptability in the top
20% of questions, from 52.3% to 43.3%.
Some of the features did not appear to be partic-
ularly helpful, notably the N-gram language model
features. We speculate that they might improve re-
sults when used with a larger, less noisy training set.
Performance did not drop precipitously upon the
removal of any particular feature type, indicating a
high amount of shared variance among the features.
However, removing several types of features at once
led to somewhat larger drops in performance. For
example, using only surface features led to a 12.8%
</bodyText>
<footnote confidence="0.923812">
13We speculate that improvements in syntactic parsing and
entity recognition would reduce the proportion of ungrammati-
cal questions and incorrect WH words, respectively.
</footnote>
<table confidence="0.9994255">
Features # Top 20% Top 40%
All 187 52.3 40.8
All – Length 160 52.3 42.1
All – WH 178 50.6 39.8
All – Negation 186 51.7 39.3
All – Lang. Model 181 51.2 39.9
All – Grammatical 69 43.2 38.7
All – Transforms 179 46.5 39.0
All – Vagueness 169 48.3 41.5
All – Histograms 53 49.4 39.8
Surface 43 39.5 37.6
Question Only 91 41.9 39.5
Random - 27.3 27.3
Oracle - 100.0 87.3
</table>
<tableCaption confidence="0.995117">
Table 2: The total numbers of features (#) and the per-
</tableCaption>
<bodyText confidence="0.911113833333333">
centages of the top 20% and 40% of ranked test set ques-
tions labeled acceptable, for rankers built from variations
of the complete set of features (“All”). E.g., “All – WH”
is the set of all features except WH word features.
drop in acceptability in the top 20%, and using only
features of questions led to a 10.4% drop.
</bodyText>
<sectionHeader confidence="0.99727" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999989166666667">
By ranking the output of rule-based natural lan-
guage generation system, existing knowledge about
WH-movement from linguistics can be leveraged to
model the complex transformations and long dis-
tance dependencies present in questions. Also, in
this overgenerate-and-rank framework, a statistical
ranker trained from a small set of annotated ques-
tions can capture trends related to question quality
that are not easily encoded with rules. In our exper-
iments, we found that ranking approximately dou-
bled the acceptability of the top-ranked questions
generated by our approach.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99949975">
We acknowledge partial support from the Institute
of Education Sciences, U.S. Department of Educa-
tion, through Grant R305B040063 to Carnegie Mel-
lon University; and from the National Science Foun-
dation through a Graduate Research Fellowship for
the first author and grant IIS-0915187 to the second
author. We thank the anonymous reviewers for their
comments.
</bodyText>
<figure confidence="0.99745">
70%
60%
50%
40%
30%
20%
</figure>
<page confidence="0.993605">
616
</page>
<sectionHeader confidence="0.989529" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999702961904762">
D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999.
An algorithm that learns what’s in a name. Machine
Learning, 34(1-3).
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2).
N. Chomsky. 1973. Conditions on transformations. A
Festschrift for Morris Halle.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
H. T. Dang, D. Kelly, and J. Lin. 2008. Overview of
the TREC 2007 question answering track. In Proc. of
TREC.
W. B. Dolan and C. Brockett. 2005. Automatically con-
structing a corpus of sentential paraphrases. In Proc.
of IWP.
B. Dorr and D. Zajic. 2003. Hedge Trimmer: A parse-
and-trim approach to headline generation. In Proc. of
Workshop on Automatic Summarization.
A. Echihabi and D. Marcu. 2003. A noisy-channel ap-
proach to question answering. In Proc. of ACL.
B. Efron and R. Tibshirani. 1993. An Introduction to the
Bootstrap. Chapman &amp; Hall/CRC.
D. M. Gates. 2008. Generating reading comprehension
look-back strategy questions from expository texts.
Master’s thesis, Carnegie Mellon University.
O. Glickman, I. Dagan, and M. Koppel. 2005. A prob-
abilistic classification approach for lexical textual en-
tailment. In Proc. of AAAI.
A. Goldberg. 2006. Constructions at Work: The Na-
ture of Generalization in Language. Oxford Univer-
sity Press, New York.
S. Harabagiu, A. Hickl, J. Lehmann, and D. Moldovan.
2005. Experiments with interactive question-
answering. In Proc. of ACL.
Michael Heilman and Noah A. Smith. 2009. Ques-
tion generation via overgenerating transformations and
ranking. Technical Report CMU-LTI-09-013, Lan-
guage Technologies Institute, Carnegie Mellon Uni-
versity.
E. Hovy, U. Hermjakob, and C. Lin. 2001. The use of
external knowledge in factoid QA. In Proc. of TREC.
N. Ide and K. Suderman. 2004. The american national
corpus first release. In Proc. of LREC.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. of IEEE Int.
Conf. Acoustics, Speech and Signal Processing.
K. Knight and D. Marcu. 2000. Statistics-based summa-
rization - step one: Sentence compression. In Proc. of
the Seventeenth National Conference on Artificial In-
telligence and Twelfth Conference on Innovative Ap-
plications of Artificial Intelligence.
H. Kunichika, T. Katayama, T. Hirashima, and
A. Takeuchi. 2004. Automated question generation
methods for intelligent English learning systems and
its evaluation. In Proc. of ICCE.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33.
I. Langkilde and Kevin Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Proc.
of ACL.
S. le Cessie and J. C. van Houwelingen. 1997. Ridge es-
timators in logistic regression. Applied Statistics, 41.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon:
tools for querying and manipulating tree data struc-
tures. In Proc. of LREC.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
R. Mitkov and L. A. Ha. 2003. Computer-aided gen-
eration of multiple-choice tests. In Proc. of the HLT-
NAACL 03 workshop on Building educational appli-
cations using natural language processing.
R. Mitkov, L. A. Ha, and N. Karamanis. 2006. A
computer-aided environment for generating multiple-
choice test items. Natural Language Engineering,
12(2).
D. Ravichandran and E. Hovy. 2001. Learning surface
text patterns for a question answering system. In Proc.
of ACL.
E. Reiter and R. Dale. 1997. Building applied natural
language generation systems. Nat. Lang. Eng., 3(1).
J. R. Ross. 1967. Constraints on Variables in Syntax.
Phd dissertation, MIT, Cambridge, MA.
V. Rus and A. Graessar, editors. 2009. The Question
Generation Shared Task and Evaluation Challenge.
http://www.questiongeneration.org.
V. Rus and J. Lester, editors. 2009. Proc. of the 2nd
Workshop on Question Generation. IOS Press.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
summarization system: Microsoft research at duc
2007. In Proc. of DUC.
E. M. Voorhees. 2004. Overview of the TREC 2003
question answering track. In Proc. of TREC 2003.
M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot:
a trainable sentence planner. In Proc. of NAACL.
</reference>
<page confidence="0.997259">
617
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.777445">
<title confidence="0.999458">Good Question! Statistical Ranking for Question Generation</title>
<author confidence="0.992979">Michael Heilman Noah A</author>
<affiliation confidence="0.8958135">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.992775">Pittsburgh, PA 15213,</address>
<abstract confidence="0.998993705882353">We address the challenge of automatically generating questions from reading materials for educational practice and assessment. Our approach is to overgenerate questions, then rank them. We use manually written rules to perform a sequence of general purpose syntactic transformations (e.g., subject-auxiliary inversion) to turn declarative sentences into questions. These questions are then ranked by a logistic regression model trained on a small, tailored dataset consisting of labeled output from our system. Experimental results show that ranking nearly doubles the percentage of questions rated as acceptable by annotators, from 27% of all questions to 52% of the top ranked 20% of questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="15694" citStr="Bikel et al., 1999" startWordPosition="2453" endWordPosition="2456">to appropriately restrict the set of questions produced. Eight Tregex expressions mark phrases that cannot be answer phrases due to WH-movement constraints. For example, the following expression encodes the noun phrase island constraint described above, where unmv indicates unmovable noun phrases: NP &lt;&lt; NP=unmv. 4.2.2 Generating Possible Question Phrases After marking unmovable phrases, we iteratively remove each possible answer phrase and generate possible question phrases from it. The system annotates the source sentence with a set of entity types taken from the BBN Identifinder Text Suite (Bikel et al., 1999) and then uses these entity labels along with the syntactic structure of a given answer phrase to generate zero or more question phrases, each of which is used to generate a final question. (This step is skipped for yes-no questions.) 5 Rating Questions for Evaluation and Learning to Rank Since different sentences from the input text, as well as different transformations of those sentences, may be more or less likely to lead to high-quality questions, each question is scored according to features of the source sentence, the input sentence, the question, and the transformations used in its gene</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. M. Bikel, R. Schwartz, and R. M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="5917" citStr="Brown et al., 1990" startWordPosition="907" endWordPosition="910">utomatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (2006) d</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Conditions on transformations. A Festschrift for Morris Halle.</title>
<date>1973</date>
<contexts>
<context position="2903" citStr="Chomsky, 1973" startWordPosition="444" endWordPosition="445">eles become known as the “Queen of the Cow Counties” for its role in supplying beef and other foodstuffs to hungry miners in the north, which we then can then transform into a more succinct question. Question transformation involves complex long distance dependencies. For example, in the question about Los Angeles, the word what at the beginning of the sentence is a semantic argument of the verb phrase known as ... at the end of the question. The characteristics of such phenomena are (arguably) difficult to learn from corpora, but they have been studied extensively in linguistics (Ross, 1967; Chomsky, 1973). We take a rule-based approach in order to leverage this linguistic knowledge. However, since many phenomena pertaining to question generation are not so easily encoded with rules, we include statistical ranking as an integral component. Thus, we employ an overgenerate-andrank approach, which has been applied successfully in areas such as generation (Walker et al., 2001; Langkilde and Knight, 1998) and syntactic parsing (Collins, 2000). Since large datasets of the appropriate domain, style, and form of questions are not available to train our ranking model, we learn to rank from a relatively </context>
<context position="4748" citStr="Chomsky, 1973" startWordPosition="725" endWordPosition="726"> §3 discusses particular terms and conventions we will employ. §4 discusses rule-based question transformation. §5 describes the data used to learn and to evaluate our question ranking model, and §6 then follows with details on the ranking approach itself. We then present and discuss results from an evaluation of ranked question output in §7 and conclude in §8. 2 Connections with Prior Work The generation of questions by humans has long motivated theoretical work in linguistics (e.g., Ross, 1967), particularly work that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to pre</context>
</contexts>
<marker>Chomsky, 1973</marker>
<rawString>N. Chomsky. 1973. Conditions on transformations. A Festschrift for Morris Halle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8666" citStr="Collins, 1999" startWordPosition="1348" endWordPosition="1349"> The term “answer phrase” refers to phrases in declarative sentences which may serve as targets for WH-movement, and therefore as possible answers to generated questions (e.g., in Africa). The term “question phrase” refers to the phrase containing the WH word that replaces an answer phrase (e.g., Where in Where is Kenya located?). 610 To represent the syntactic structure of sentences, we use simplified Penn Treebank-style phrase structure trees, including POS and category labels, as produced by the Stanford Parser (Klein and Manning, 2003). Noun phrase heads are selected using Collins’ rules (Collins, 1999). To implement the rules for transforming source sentences into questions, we use Tregex, a tree query language, and Tsurgeon, a tree manipulation language built on top of Tregex (Levy and Andrew, 2006). The Tregex language includes various relational operators based on the primitive relations of immediate dominance (denoted “&lt;”) and immediate precedence (denoted “.”). Tsurgeon adds the ability to modify trees by relabeling, deleting, moving, and inserting nodes. 4 Rule-based Overgeneration Many useful questions can be viewed as lexical, syntactic, or semantic transformations of the declarativ</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="3343" citStr="Collins, 2000" startWordPosition="509" endWordPosition="510">stion. The characteristics of such phenomena are (arguably) difficult to learn from corpora, but they have been studied extensively in linguistics (Ross, 1967; Chomsky, 1973). We take a rule-based approach in order to leverage this linguistic knowledge. However, since many phenomena pertaining to question generation are not so easily encoded with rules, we include statistical ranking as an integral component. Thus, we employ an overgenerate-andrank approach, which has been applied successfully in areas such as generation (Walker et al., 2001; Langkilde and Knight, 1998) and syntactic parsing (Collins, 2000). Since large datasets of the appropriate domain, style, and form of questions are not available to train our ranking model, we learn to rank from a relatively small, tailored dataset of humanlabeled output from our rule-based system. The remainder of the paper is organized as fol2The motivating example does not exhibit lexical semantic variations such as synonymy. In this work, we do not model complex paraphrasing, but believe that paraphrase generation techniques could be incorporated into our approach. 609 Human Language Technologies: The 2010 Annual Conference of the North American Chapter</context>
<context position="20132" citStr="Collins (2000)" startWordPosition="3181" endWordPosition="3182">ovides similar text but at a reading level corresponding to elementary education or intermediate second language learning.9 The training set included 1,195 questions about 16 articles, and the test set included 118 questions about 2 articles from this corpus. The third corpus was Section 23 of the Wall Street Journal data in the Penn Treebank (Marcus et al., 1993).10 The training set included 284 questions about 8 articles, and the test set included 190 questions about 2 articles from this corpus. 6 Ranking We use a discriminative ranker to rank questions, similar to the approach described by Collins (2000) for ranking syntactic parses. Questions are ranked by the predictions of a logistic regression model of question acceptability. Given the question q and source text t, the model defines a binomial distribution p(R |q, t), with binary random variable R ranging over a (“acceptable”) and u (“unacceptable”). We estimate the parameters by optimizing the regularized log-likelihood of the training data (cf. §5.1) with a variant of Newton’s method (le Cessie and 8The English and Simple English Wikipedia data were downloaded on December 16, 2008 from http://en. wikipedia.org and http://simple.wikipedi</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>M. Collins. 2000. Discriminative reranking for natural language parsing. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>D Kelly</author>
<author>J Lin</author>
</authors>
<title>question answering track.</title>
<date>2008</date>
<journal>Overview of the TREC</journal>
<booktitle>In Proc. of TREC.</booktitle>
<contexts>
<context position="4898" citStr="Dang et al., 2008" startWordPosition="747" endWordPosition="750">n and to evaluate our question ranking model, and §6 then follows with details on the ranking approach itself. We then present and discuss results from an evaluation of ranked question output in §7 and conclude in §8. 2 Connections with Prior Work The generation of questions by humans has long motivated theoretical work in linguistics (e.g., Ross, 1967), particularly work that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated </context>
</contexts>
<marker>Dang, Kelly, Lin, 2008</marker>
<rawString>H. T. Dang, D. Kelly, and J. Lin. 2008. Overview of the TREC 2007 question answering track. In Proc. of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Dolan</author>
<author>C Brockett</author>
</authors>
<title>Automatically constructing a corpus of sentential paraphrases.</title>
<date>2005</date>
<booktitle>In Proc. of IWP.</booktitle>
<contexts>
<context position="18649" citStr="Dolan and Brockett (2005)" startWordPosition="2938" endWordPosition="2941"> The training and test datasets consisted of 2,807 and 428 questions, respectively. The questions were 5The ratings from one person were excluded due to an extremely high rate of accepting questions as error-free and other irregularities. 6The percentages in Table 1 do not add up to 100% for two reasons: first, questions are labeled acceptable in the test set only if the majority of raters labeled them as having no deficiencies, rather than the less strict criterion of requiring no deficiencies to be identified by a majority of raters; second, the categories are not mutually exclusive. 7E.g., Dolan and Brockett (2005) and Glickman et al. (2005) report r, values around 0.6 for paraphrase identification and textual entailment, respectively. generated from three corpora. The first corpus was a random sample from the featured articles in the English Wikipedia8 with between 250 and 2,000 word tokens. This English Wikipedia corpus provides expository texts written at an adult reading level from a variety of domains, which roughly approximates the prose that a secondary or post-secondary student would encounter. By choosing from the featured articles, we intended to select well-edited articles on topics of genera</context>
</contexts>
<marker>Dolan, Brockett, 2005</marker>
<rawString>W. B. Dolan and C. Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proc. of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
<author>D Zajic</author>
</authors>
<title>Hedge Trimmer: A parseand-trim approach to headline generation.</title>
<date>2003</date>
<booktitle>In Proc. of Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="10210" citStr="Dorr and Zajic, 2003" startWordPosition="1582" endWordPosition="1585">ce) by altering lexical items, syntactic structure, and semantics. Many existing NLP transformations could potentially be exploited in this step, including sentence compression, paraphrase generation, or lexical semantics for word substitution. In our implementation, a set of transformations derive a simpler form of the source sentence by removing phrase types such as leading conjunctions, sentence-level modifying phrases, and appositives. Tregex expressions identify the constituents to move, alter, or delete. Similar transformations have been utilized in previous work on headline generation (Dorr and Zajic, 2003) and summarization (Toutanova et al., 2007). To enable questions about syntactically embedded content, our implementation also extracts a set of declarative sentences from any finite clauses, rela3See Heilman and Smith (2009) for details on the rule-based component. tive clauses, appositives, and participial phrases that appear in the source sentence. For example, it transforms the sentence Selling snowballed because of waves of automatic stop-loss orders, which are triggered by computer when prices fall to certain levels into Automatic stop-loss orders are triggered by computer when prices fa</context>
</contexts>
<marker>Dorr, Zajic, 2003</marker>
<rawString>B. Dorr and D. Zajic. 2003. Hedge Trimmer: A parseand-trim approach to headline generation. In Proc. of Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5147" citStr="Echihabi and Marcu, 2003" startWordPosition="787" endWordPosition="790"> Work The generation of questions by humans has long motivated theoretical work in linguistics (e.g., Ross, 1967), particularly work that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a </context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D. Marcu. 2003. A noisy-channel approach to question answering. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="29452" citStr="Efron and Tibshirani, 1993" startWordPosition="4690" endWordPosition="4693">nguistic features such as parts of speech and syntax appear to add value for ranking questions.12 The ranker seems to have focused on the “Does not make sense” and “Vague” categories. The percentage of nonsensical questions dropped from 20.6% to 4.7%, and vagueness dropped from 19.6% 12Ranking with all features was statistically significantly better (p &lt; .05) in terms of the percentage of acceptable questions in the top ranked 20% than ranking with the “question only” or “surface” configurations, or the random baseline, as verified by computing 95% confidence intervals with the BC, Bootstrap (Efron and Tibshirani, 1993). 615 0 Figure 2: A graph of the percentage of acceptable questions in the top-N questions in the test set, using various rankings, for N varying from 0 to the size of the test set. The percentages become increasingly unstable when restricted to very few questions (e.g., &lt; 50). to 7.0%, while ungrammaticality dropped from 14.0% to 10.5%, and the other, less prevalent, categories changed very little.13 7.4 Ablation Study Ablation experiments were also conducted to study the effects of removing each of the different types of features. Table 2 presents the percentages of acceptable test set quest</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Gates</author>
</authors>
<title>Generating reading comprehension look-back strategy questions from expository texts. Master’s thesis,</title>
<date>2008</date>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="6349" citStr="Gates, 2008" startWordPosition="980" endWordPosition="981">r than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (2006) demonstrated that automatic generation and manual correction of questions can be more time-efficient than manual authoring alone. Much of the prior QG research has evaluated systems in specific domains (e.g., introductory linguistics, English as a Second Language), and thus we do not attempt empirical comparisons. Existing QG systems model their transformations from source text to questions with many complex rules for specific qu</context>
<context position="12018" citStr="Gates, 2008" startWordPosition="1866" endWordPosition="1867">ho, what, where, when, and how much questions. The system could be extended to transform other types of phrases into other types of questions (e.g., how, why, and what kind of). It should be noted that the transformation from answer to question is achieved by applying a series of generalpurpose rules. This would allow, for example, the addition of a rule to generate why questions that builds off of the existing rules for subject-auxiliary inversion, verb decomposition, etc. In contrast, previous QG approaches have employed separate rules for specific sentence types (e.g., Mitkov and Ha, 2003; Gates, 2008). For each sentence, many questions may be produced: there are often multiple possible answer phrases, and multiple question phrases for each answer phrase. Hence many candidates may result from the transformations. These rules encode a substantial amount of linguistic knowledge about the long distance dependencies prevalent in questions, which would be challenging to learn from existing corpora of questions and answers consisting typically of only thousands of examples (e.g., Voorhees, 2003). Specifically, the following sequence of transfor4We leave the generation of correct answers and distr</context>
</contexts>
<marker>Gates, 2008</marker>
<rawString>D. M. Gates. 2008. Generating reading comprehension look-back strategy questions from expository texts. Master’s thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Glickman</author>
<author>I Dagan</author>
<author>M Koppel</author>
</authors>
<title>A probabilistic classification approach for lexical textual entailment.</title>
<date>2005</date>
<booktitle>In Proc. of AAAI.</booktitle>
<contexts>
<context position="18676" citStr="Glickman et al. (2005)" startWordPosition="2943" endWordPosition="2946">s consisted of 2,807 and 428 questions, respectively. The questions were 5The ratings from one person were excluded due to an extremely high rate of accepting questions as error-free and other irregularities. 6The percentages in Table 1 do not add up to 100% for two reasons: first, questions are labeled acceptable in the test set only if the majority of raters labeled them as having no deficiencies, rather than the less strict criterion of requiring no deficiencies to be identified by a majority of raters; second, the categories are not mutually exclusive. 7E.g., Dolan and Brockett (2005) and Glickman et al. (2005) report r, values around 0.6 for paraphrase identification and textual entailment, respectively. generated from three corpora. The first corpus was a random sample from the featured articles in the English Wikipedia8 with between 250 and 2,000 word tokens. This English Wikipedia corpus provides expository texts written at an adult reading level from a variety of domains, which roughly approximates the prose that a secondary or post-secondary student would encounter. By choosing from the featured articles, we intended to select well-edited articles on topics of general interest. The training se</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>O. Glickman, I. Dagan, and M. Koppel. 2005. A probabilistic classification approach for lexical textual entailment. In Proc. of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Goldberg</author>
</authors>
<title>Constructions at Work: The Nature of Generalization in Language.</title>
<date>2006</date>
<publisher>Oxford University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="14719" citStr="Goldberg (2006)" startWordPosition="2303" endWordPosition="2304">, not in figure); select an answer phrase, remove it, and generate possible question phrases for it (§4.2.2); decompose the main verb; invert the subject and auxiliary verb; and insert one of the possible question phrases. Some of these steps do not apply in all cases. For example, no answer phrases are removed when generating yes-no questions. 4.2.1 Marking Unmovable Phrases In English, various constraints determine whether phrases can be involved in WH-movement and other phenomena involving long distance dependencies. In a seminal dissertation, Ross (1967) described many of these phenomena. Goldberg (2006) provides a concise summary of them. For example, noun phrases are “islands” to movement, meaning that constituents dominated by a noun phrase typically cannot undergo WHmovement. Thus, from John liked the book that I gave him, we generate What did John like? but not *Who did John like the book that gave him?. We operationalize this linguistic knowledge to appropriately restrict the set of questions produced. Eight Tregex expressions mark phrases that cannot be answer phrases due to WH-movement constraints. For example, the following expression encodes the noun phrase island constraint describ</context>
</contexts>
<marker>Goldberg, 2006</marker>
<rawString>A. Goldberg. 2006. Constructions at Work: The Nature of Generalization in Language. Oxford University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
<author>J Lehmann</author>
<author>D Moldovan</author>
</authors>
<title>Experiments with interactive questionanswering.</title>
<date>2005</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5274" citStr="Harabagiu et al. (2005)" startWordPosition="807" endWordPosition="810">ork that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike mac</context>
</contexts>
<marker>Harabagiu, Hickl, Lehmann, Moldovan, 2005</marker>
<rawString>S. Harabagiu, A. Hickl, J. Lehmann, and D. Moldovan. 2005. Experiments with interactive questionanswering. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Question generation via overgenerating transformations and ranking.</title>
<date>2009</date>
<tech>Technical Report CMU-LTI-09-013,</tech>
<institution>Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="10435" citStr="Heilman and Smith (2009)" startWordPosition="1614" endWordPosition="1617">for word substitution. In our implementation, a set of transformations derive a simpler form of the source sentence by removing phrase types such as leading conjunctions, sentence-level modifying phrases, and appositives. Tregex expressions identify the constituents to move, alter, or delete. Similar transformations have been utilized in previous work on headline generation (Dorr and Zajic, 2003) and summarization (Toutanova et al., 2007). To enable questions about syntactically embedded content, our implementation also extracts a set of declarative sentences from any finite clauses, rela3See Heilman and Smith (2009) for details on the rule-based component. tive clauses, appositives, and participial phrases that appear in the source sentence. For example, it transforms the sentence Selling snowballed because of waves of automatic stop-loss orders, which are triggered by computer when prices fall to certain levels into Automatic stop-loss orders are triggered by computer when prices fall to certain levels, from which the next step will produce What are triggered by computer when prices fall to certain levels?. 4.2 Question Transformation In the second step, the declarative sentences derived in step 1 are t</context>
<context position="21008" citStr="Heilman and Smith, 2009" startWordPosition="3316" endWordPosition="3319"> ranging over a (“acceptable”) and u (“unacceptable”). We estimate the parameters by optimizing the regularized log-likelihood of the training data (cf. §5.1) with a variant of Newton’s method (le Cessie and 8The English and Simple English Wikipedia data were downloaded on December 16, 2008 from http://en. wikipedia.org and http://simple.wikipedia. org, respectively. 9The subject matter of the articles in the two Wikipedia corpora was not matched. 10In separate experiments with the Penn Treebank, goldstandard parses led to an absolute increase of 15% in the percentage of acceptable questions (Heilman and Smith, 2009). 613 Question Deficiency Description % Ungrammatical The question is not a valid English sentence. (e.g., In what were nests excavated exposed to the 14.0 sun? from ... eggs are usually laid ..., in nests excavated in pockets of earth exposed to the sun.. This error results from the incorrect attachment by the parser of exposed to the sun to the verb phrase headed by excavated) Does not make sense The question is grammatical but indecipherable. (e.g., Who was the investment?) 20.6 Vague The question is too vague to know exactly what it is asking about, even after reading the article 19.6 (e.g</context>
</contexts>
<marker>Heilman, Smith, 2009</marker>
<rawString>Michael Heilman and Noah A. Smith. 2009. Question generation via overgenerating transformations and ranking. Technical Report CMU-LTI-09-013, Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>C Lin</author>
</authors>
<title>The use of external knowledge in factoid QA.</title>
<date>2001</date>
<booktitle>In Proc. of TREC.</booktitle>
<contexts>
<context position="5039" citStr="Hovy et al., 2001" startWordPosition="770" endWordPosition="773">sults from an evaluation of ranked question output in §7 and conclude in §8. 2 Connections with Prior Work The generation of questions by humans has long motivated theoretical work in linguistics (e.g., Ross, 1967), particularly work that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very d</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2001</marker>
<rawString>E. Hovy, U. Hermjakob, and C. Lin. 2001. The use of external knowledge in factoid QA. In Proc. of TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>K Suderman</author>
</authors>
<title>The american national corpus first release.</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="23869" citStr="Ide and Suderman, 2004" startWordPosition="3795" endWordPosition="3798">H Words (9 + 0) The set includes boolean features for the presence of each possible WH word in the question. Negation (1 + 0) This is a boolean feature for the presence of not, never, or no in the question. N-Gram Language Model Features (6 + 0) The set includes real valued features for the log likelihoods and length-normalized log likelihoods of the question, the source sentence, and the answer phrase. Separate likelihood features are included for unigram and trigram language models. These language models were estimated from the written portion of the American National Corpus Second Release (Ide and Suderman, 2004), which consists of approximately 20 million tokens, using Kneser and Ney (1995) smoothing. Grammatical Features (23 + 95) The set includes integer features for the numbers of proper nouns, pronouns, adjectives, adverbs, conjunctions, numbers, noun phrases, prepositional phrases, and subordinate clauses in the phrase structure parse trees for the question and answer phrase. It also includes one integer feature for the number of modifying phrases at the start of the question (e.g., as in At the end of the Civil War, who led the Union Army?); three boolean features for whether the main verb is i</context>
</contexts>
<marker>Ide, Suderman, 2004</marker>
<rawString>N. Ide and K. Suderman. 2004. The american national corpus first release. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in NIPS 15.</booktitle>
<contexts>
<context position="8597" citStr="Klein and Manning, 2003" startWordPosition="1335" endWordPosition="1339">t, from which a question will be generated (e.g., Kenya is located in Africa.). The term “answer phrase” refers to phrases in declarative sentences which may serve as targets for WH-movement, and therefore as possible answers to generated questions (e.g., in Africa). The term “question phrase” refers to the phrase containing the WH word that replaces an answer phrase (e.g., Where in Where is Kenya located?). 610 To represent the syntactic structure of sentences, we use simplified Penn Treebank-style phrase structure trees, including POS and category labels, as produced by the Stanford Parser (Klein and Manning, 2003). Noun phrase heads are selected using Collins’ rules (Collins, 1999). To implement the rules for transforming source sentences into questions, we use Tregex, a tree query language, and Tsurgeon, a tree manipulation language built on top of Tregex (Levy and Andrew, 2006). The Tregex language includes various relational operators based on the primitive relations of immediate dominance (denoted “&lt;”) and immediate precedence (denoted “.”). Tsurgeon adds the ability to modify trees by relabeling, deleting, moving, and inserting nodes. 4 Rule-based Overgeneration Many useful questions can be viewed</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. of IEEE Int. Conf. Acoustics, Speech and Signal Processing.</booktitle>
<contexts>
<context position="23949" citStr="Kneser and Ney (1995)" startWordPosition="3807" endWordPosition="3810">e WH word in the question. Negation (1 + 0) This is a boolean feature for the presence of not, never, or no in the question. N-Gram Language Model Features (6 + 0) The set includes real valued features for the log likelihoods and length-normalized log likelihoods of the question, the source sentence, and the answer phrase. Separate likelihood features are included for unigram and trigram language models. These language models were estimated from the written portion of the American National Corpus Second Release (Ide and Suderman, 2004), which consists of approximately 20 million tokens, using Kneser and Ney (1995) smoothing. Grammatical Features (23 + 95) The set includes integer features for the numbers of proper nouns, pronouns, adjectives, adverbs, conjunctions, numbers, noun phrases, prepositional phrases, and subordinate clauses in the phrase structure parse trees for the question and answer phrase. It also includes one integer feature for the number of modifying phrases at the start of the question (e.g., as in At the end of the Civil War, who led the Union Army?); three boolean features for whether the main verb is in past, present, or future tense; and one boolean feature for whether the main v</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. of IEEE Int. Conf. Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence.</booktitle>
<contexts>
<context position="6088" citStr="Knight and Marcu, 2000" startWordPosition="939" endWordPosition="942">dels are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (2006) demonstrated that automatic generation and manual correction of questions can be more time-efficient than manual authoring alone. Much of the prior QG research has evaluate</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-based summarization - step one: Sentence compression. In Proc. of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kunichika</author>
<author>T Katayama</author>
<author>T Hirashima</author>
<author>A Takeuchi</author>
</authors>
<title>Automated question generation methods for intelligent English learning systems and its evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of ICCE.</booktitle>
<contexts>
<context position="6336" citStr="Kunichika et al., 2004" startWordPosition="976" endWordPosition="979">s natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (2006) demonstrated that automatic generation and manual correction of questions can be more time-efficient than manual authoring alone. Much of the prior QG research has evaluated systems in specific domains (e.g., introductory linguistics, English as a Second Language), and thus we do not attempt empirical comparisons. Existing QG systems model their transformations from source text to questions with many complex rules fo</context>
</contexts>
<marker>Kunichika, Katayama, Hirashima, Takeuchi, 2004</marker>
<rawString>H. Kunichika, T. Katayama, T. Hirashima, and A. Takeuchi. 2004. Automated question generation methods for intelligent English learning systems and its evaluation. In Proc. of ICCE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<contexts>
<context position="17958" citStr="Landis and Koch, 1977" startWordPosition="2823" endWordPosition="2826">ining set, each question was rated by a single annotator (not the same for each question), leading to a large number of diverse examples. For the test set, each question was rated by three people (again, not the same for each question) to provide a more reliable gold standard. To assign final labels to the test data, a question was labeled as acceptable only if a majority of the three raters rated it as acceptable (i.e., without deficiencies).6 An inter-rater agreement of Fleiss’s r. = 0.42 was computed from the test set’s acceptability ratings. This value corresponds to “moderate agreement” (Landis and Koch, 1977) and is somewhat lower than for other rating schemes.7 5.1 Corpora The training and test datasets consisted of 2,807 and 428 questions, respectively. The questions were 5The ratings from one person were excluded due to an extremely high rate of accepting questions as error-free and other irregularities. 6The percentages in Table 1 do not add up to 100% for two reasons: first, questions are labeled acceptable in the test set only if the majority of raters labeled them as having no deficiencies, rather than the less strict criterion of requiring no deficiencies to be identified by a majority of </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="3305" citStr="Langkilde and Knight, 1998" startWordPosition="502" endWordPosition="505"> the verb phrase known as ... at the end of the question. The characteristics of such phenomena are (arguably) difficult to learn from corpora, but they have been studied extensively in linguistics (Ross, 1967; Chomsky, 1973). We take a rule-based approach in order to leverage this linguistic knowledge. However, since many phenomena pertaining to question generation are not so easily encoded with rules, we include statistical ranking as an integral component. Thus, we employ an overgenerate-andrank approach, which has been applied successfully in areas such as generation (Walker et al., 2001; Langkilde and Knight, 1998) and syntactic parsing (Collins, 2000). Since large datasets of the appropriate domain, style, and form of questions are not available to train our ranking model, we learn to rank from a relatively small, tailored dataset of humanlabeled output from our rule-based system. The remainder of the paper is organized as fol2The motivating example does not exhibit lexical semantic variations such as synonymy. In this work, we do not model complex paraphrasing, but believe that paraphrase generation techniques could be incorporated into our approach. 609 Human Language Technologies: The 2010 Annual Co</context>
<context position="16456" citStr="Langkilde and Knight, 1998" startWordPosition="2578" endWordPosition="2581">s, each of which is used to generate a final question. (This step is skipped for yes-no questions.) 5 Rating Questions for Evaluation and Learning to Rank Since different sentences from the input text, as well as different transformations of those sentences, may be more or less likely to lead to high-quality questions, each question is scored according to features of the source sentence, the input sentence, the question, and the transformations used in its generation. The scores are used to rank the questions. This is 612 an example of an “overgenerate-and-rank” strategy (Walker et al., 2001; Langkilde and Knight, 1998). This section describes the acquisition of a set of rated questions produced by the steps described above. Separate portions of these labeled data will be used to develop a discriminative question ranker (§6), and to evaluate ranked lists of questions (§7). Fifteen native English-speaking university students rated a set of questions produced from steps 1 and 2, indicating whether each question exhibited any of the deficiencies listed in Table 1.5 If a question exhibited no deficiencies, raters were asked to label it “acceptable.” Annotators were asked to read the text of a newswire or encyclo</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>I. Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S le Cessie</author>
<author>J C van Houwelingen</author>
</authors>
<title>Ridge estimators in logistic regression.</title>
<date>1997</date>
<journal>Applied Statistics,</journal>
<volume>41</volume>
<marker>le Cessie, van Houwelingen, 1997</marker>
<rawString>S. le Cessie and J. C. van Houwelingen. 1997. Ridge estimators in logistic regression. Applied Statistics, 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="8868" citStr="Levy and Andrew, 2006" startWordPosition="1379" endWordPosition="1383">erm “question phrase” refers to the phrase containing the WH word that replaces an answer phrase (e.g., Where in Where is Kenya located?). 610 To represent the syntactic structure of sentences, we use simplified Penn Treebank-style phrase structure trees, including POS and category labels, as produced by the Stanford Parser (Klein and Manning, 2003). Noun phrase heads are selected using Collins’ rules (Collins, 1999). To implement the rules for transforming source sentences into questions, we use Tregex, a tree query language, and Tsurgeon, a tree manipulation language built on top of Tregex (Levy and Andrew, 2006). The Tregex language includes various relational operators based on the primitive relations of immediate dominance (denoted “&lt;”) and immediate precedence (denoted “.”). Tsurgeon adds the ability to modify trees by relabeling, deleting, moving, and inserting nodes. 4 Rule-based Overgeneration Many useful questions can be viewed as lexical, syntactic, or semantic transformations of the declarative sentences in a text. We describe how to model this process in two steps, as proposed in §1.3 4.1 Sentence Simplification In the first step for transforming sentences into questions, each of the senten</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>R. Levy and G. Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="19884" citStr="Marcus et al., 1993" startWordPosition="3138" endWordPosition="3141"> training set included 1,328 questions about 12 articles, and the test set included 120 questions about 2 articles from this corpus. The second corpus was a random sample from the articles in the Simple English Wikipedia of similar length. This corpus provides similar text but at a reading level corresponding to elementary education or intermediate second language learning.9 The training set included 1,195 questions about 16 articles, and the test set included 118 questions about 2 articles from this corpus. The third corpus was Section 23 of the Wall Street Journal data in the Penn Treebank (Marcus et al., 1993).10 The training set included 284 questions about 8 articles, and the test set included 190 questions about 2 articles from this corpus. 6 Ranking We use a discriminative ranker to rank questions, similar to the approach described by Collins (2000) for ranking syntactic parses. Questions are ranked by the predictions of a logistic regression model of question acceptability. Given the question q and source text t, the model defines a binomial distribution p(R |q, t), with binary random variable R ranging over a (“acceptable”) and u (“unacceptable”). We estimate the parameters by optimizing the </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
<author>L A Ha</author>
</authors>
<title>Computer-aided generation of multiple-choice tests.</title>
<date>2003</date>
<booktitle>In Proc. of the HLTNAACL 03 workshop on Building educational</booktitle>
<contexts>
<context position="6312" citStr="Mitkov and Ha, 2003" startWordPosition="972" endWordPosition="975">s because the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (2006) demonstrated that automatic generation and manual correction of questions can be more time-efficient than manual authoring alone. Much of the prior QG research has evaluated systems in specific domains (e.g., introductory linguistics, English as a Second Language), and thus we do not attempt empirical comparisons. Existing QG systems model their transformations from source text to questions wi</context>
<context position="12004" citStr="Mitkov and Ha, 2003" startWordPosition="1862" endWordPosition="1865">ases, which enables who, what, where, when, and how much questions. The system could be extended to transform other types of phrases into other types of questions (e.g., how, why, and what kind of). It should be noted that the transformation from answer to question is achieved by applying a series of generalpurpose rules. This would allow, for example, the addition of a rule to generate why questions that builds off of the existing rules for subject-auxiliary inversion, verb decomposition, etc. In contrast, previous QG approaches have employed separate rules for specific sentence types (e.g., Mitkov and Ha, 2003; Gates, 2008). For each sentence, many questions may be produced: there are often multiple possible answer phrases, and multiple question phrases for each answer phrase. Hence many candidates may result from the transformations. These rules encode a substantial amount of linguistic knowledge about the long distance dependencies prevalent in questions, which would be challenging to learn from existing corpora of questions and answers consisting typically of only thousands of examples (e.g., Voorhees, 2003). Specifically, the following sequence of transfor4We leave the generation of correct ans</context>
</contexts>
<marker>Mitkov, Ha, 2003</marker>
<rawString>R. Mitkov and L. A. Ha. 2003. Computer-aided generation of multiple-choice tests. In Proc. of the HLTNAACL 03 workshop on Building educational applications using natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
<author>L A Ha</author>
<author>N Karamanis</author>
</authors>
<title>A computer-aided environment for generating multiplechoice test items.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="6515" citStr="Mitkov et al. (2006)" startWordPosition="1005" endWordPosition="1008">., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (2006) demonstrated that automatic generation and manual correction of questions can be more time-efficient than manual authoring alone. Much of the prior QG research has evaluated systems in specific domains (e.g., introductory linguistics, English as a Second Language), and thus we do not attempt empirical comparisons. Existing QG systems model their transformations from source text to questions with many complex rules for specific question types (e.g., a rule for creating a question Who did the Subject Verb? from a sentence with SVO word order and an object referring to a person), rather than wit</context>
</contexts>
<marker>Mitkov, Ha, Karamanis, 2006</marker>
<rawString>R. Mitkov, L. A. Ha, and N. Karamanis. 2006. A computer-aided environment for generating multiplechoice test items. Natural Language Engineering, 12(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2001</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="5019" citStr="Ravichandran and Hovy, 2001" startWordPosition="765" endWordPosition="769">e then present and discuss results from an evaluation of ranked question output in §7 and conclude in §8. 2 Connections with Prior Work The generation of questions by humans has long motivated theoretical work in linguistics (e.g., Ross, 1967), particularly work that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering p</context>
</contexts>
<marker>Ravichandran, Hovy, 2001</marker>
<rawString>D. Ravichandran and E. Hovy. 2001. Learning surface text patterns for a question answering system. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building applied natural language generation systems.</title>
<date>1997</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="5796" citStr="Reiter and Dale, 1997" startWordPosition="887" endWordPosition="890">orrect answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009)</context>
</contexts>
<marker>Reiter, Dale, 1997</marker>
<rawString>E. Reiter and R. Dale. 1997. Building applied natural language generation systems. Nat. Lang. Eng., 3(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Ross</author>
</authors>
<date>1967</date>
<booktitle>Constraints on Variables in Syntax. Phd dissertation, MIT,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="2887" citStr="Ross, 1967" startWordPosition="442" endWordPosition="443">h as Los Angeles become known as the “Queen of the Cow Counties” for its role in supplying beef and other foodstuffs to hungry miners in the north, which we then can then transform into a more succinct question. Question transformation involves complex long distance dependencies. For example, in the question about Los Angeles, the word what at the beginning of the sentence is a semantic argument of the verb phrase known as ... at the end of the question. The characteristics of such phenomena are (arguably) difficult to learn from corpora, but they have been studied extensively in linguistics (Ross, 1967; Chomsky, 1973). We take a rule-based approach in order to leverage this linguistic knowledge. However, since many phenomena pertaining to question generation are not so easily encoded with rules, we include statistical ranking as an integral component. Thus, we employ an overgenerate-andrank approach, which has been applied successfully in areas such as generation (Walker et al., 2001; Langkilde and Knight, 1998) and syntactic parsing (Collins, 2000). Since large datasets of the appropriate domain, style, and form of questions are not available to train our ranking model, we learn to rank fr</context>
<context position="4635" citStr="Ross, 1967" startWordPosition="712" endWordPosition="713">n for Computational Linguistics lows. §2 clarifies connections to prior work and enumerates our contributions. §3 discusses particular terms and conventions we will employ. §4 discusses rule-based question transformation. §5 describes the data used to learn and to evaluate our question ranking model, and §6 then follows with details on the ranking approach itself. We then present and discuss results from an evaluation of ranked question output in §7 and conclude in §8. 2 Connections with Prior Work The generation of questions by humans has long motivated theoretical work in linguistics (e.g., Ross, 1967), particularly work that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel fr</context>
<context position="14668" citStr="Ross (1967)" startWordPosition="2296" endWordPosition="2297">rases due to constraints on WH movement (§4.2.1, not in figure); select an answer phrase, remove it, and generate possible question phrases for it (§4.2.2); decompose the main verb; invert the subject and auxiliary verb; and insert one of the possible question phrases. Some of these steps do not apply in all cases. For example, no answer phrases are removed when generating yes-no questions. 4.2.1 Marking Unmovable Phrases In English, various constraints determine whether phrases can be involved in WH-movement and other phenomena involving long distance dependencies. In a seminal dissertation, Ross (1967) described many of these phenomena. Goldberg (2006) provides a concise summary of them. For example, noun phrases are “islands” to movement, meaning that constituents dominated by a noun phrase typically cannot undergo WHmovement. Thus, from John liked the book that I gave him, we generate What did John like? but not *Who did John like the book that gave him?. We operationalize this linguistic knowledge to appropriately restrict the set of questions produced. Eight Tregex expressions mark phrases that cannot be answer phrases due to WH-movement constraints. For example, the following expressio</context>
</contexts>
<marker>Ross, 1967</marker>
<rawString>J. R. Ross. 1967. Constraints on Variables in Syntax. Phd dissertation, MIT, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>A Graessar</author>
<author>editors</author>
</authors>
<date>2009</date>
<booktitle>The Question Generation Shared Task and Evaluation Challenge. http://www.questiongeneration.org.</booktitle>
<marker>Rus, Graessar, editors, 2009</marker>
<rawString>V. Rus and A. Graessar, editors. 2009. The Question Generation Shared Task and Evaluation Challenge. http://www.questiongeneration.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>J Lester</author>
<author>editors</author>
</authors>
<date>2009</date>
<booktitle>Proc. of the 2nd Workshop on Question Generation.</booktitle>
<publisher>IOS Press.</publisher>
<marker>Rus, Lester, editors, 2009</marker>
<rawString>V. Rus and J. Lester, editors. 2009. Proc. of the 2nd Workshop on Question Generation. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Brockett</author>
<author>M Gamon</author>
<author>J Jagarlamudi</author>
<author>H Suzuki</author>
<author>L Vanderwende</author>
</authors>
<title>The PYTHY summarization system: Microsoft research at duc</title>
<date>2007</date>
<booktitle>In Proc. of DUC.</booktitle>
<contexts>
<context position="10253" citStr="Toutanova et al., 2007" startWordPosition="1588" endWordPosition="1591">structure, and semantics. Many existing NLP transformations could potentially be exploited in this step, including sentence compression, paraphrase generation, or lexical semantics for word substitution. In our implementation, a set of transformations derive a simpler form of the source sentence by removing phrase types such as leading conjunctions, sentence-level modifying phrases, and appositives. Tregex expressions identify the constituents to move, alter, or delete. Similar transformations have been utilized in previous work on headline generation (Dorr and Zajic, 2003) and summarization (Toutanova et al., 2007). To enable questions about syntactically embedded content, our implementation also extracts a set of declarative sentences from any finite clauses, rela3See Heilman and Smith (2009) for details on the rule-based component. tive clauses, appositives, and participial phrases that appear in the source sentence. For example, it transforms the sentence Selling snowballed because of waves of automatic stop-loss orders, which are triggered by computer when prices fall to certain levels into Automatic stop-loss orders are triggered by computer when prices fall to certain levels, from which the next s</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi, H. Suzuki, and L. Vanderwende. 2007. The PYTHY summarization system: Microsoft research at duc 2007. In Proc. of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2004</date>
<booktitle>In Proc. of TREC</booktitle>
<marker>Voorhees, 2004</marker>
<rawString>E. M. Voorhees. 2004. Overview of the TREC 2003 question answering track. In Proc. of TREC 2003. M. A. Walker, O. Rambow, and M. Rogati. 2001. Spot: a trainable sentence planner. In Proc. of NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>