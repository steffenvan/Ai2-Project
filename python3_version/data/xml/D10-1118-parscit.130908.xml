<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012798">
<title confidence="0.9995805">
It Depends on the Translation:
Unsupervised Dependency Parsing via Word Alignment
</title>
<author confidence="0.995694">
Samuel Brody
</author>
<affiliation confidence="0.9962155">
Dept. of Biomedical Informatics
Columbia University
</affiliation>
<email confidence="0.996253">
samuel.brody@dbmi.columbia.edu
</email>
<sectionHeader confidence="0.99736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938882352941">
We reveal a previously unnoticed connection
between dependency parsing and statistical
machine translation (SMT), by formulating
the dependency parsing task as a problem of
word alignment. Furthermore, we show that
two well known models for these respective
tasks (DMV and the IBM models) share com-
mon modeling assumptions. This motivates us
to develop an alignment-based framework for
unsupervised dependency parsing. The frame-
work (which will be made publicly available)
is flexible, modular and easy to extend. Us-
ing this framework, we implement several al-
gorithms based on the IBM alignment mod-
els, which prove surprisingly effective on the
dependency parsing task, and demonstrate the
potential of the alignment-based approach.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982698113208">
Both statistical machine translation (SMT) and un-
supervised dependency parsing have seen a surge of
interest in recent years, as the need for large scale
data processing has increased. The problems ad-
dressed by each of the fields seem quite different
at first glance. However, in this paper, we reveal a
strong connection between them and show that the
problem of dependency parsing can be formulated
as one of word alignment within the sentence. Fur-
thermore, we show that the two models that are ar-
guably the most influential in their respective fields,
the IBM models 1-3 (Brown et al., 1993) and Klein
and Manning’s (2004) Dependency Model with Va-
lence (DMV), share a common set of modeling as-
sumptions.
Based on this connection, we develop a frame-
work which uses an alignment-based approach for
unsupervised dependency parsing. The framework
is flexible and modular, and allows us to explore dif-
ferent modeling assumptions. We demonstrate these
properties and the merit of the alignment-based pars-
ing approach by implementing several dependency
parsing algorithms based on the IBM alignment
models and evaluating their performance on the task.
Although the algorithms are not competitive with
state-of-the-art systems, they outperform the right-
branching baseline and approach the performance of
DMV. This is especially surprising when we con-
sider that the IBM models were not originally de-
signed for the task. These results are encourag-
ing and indicate that the alignment-based approach
could serve as the basis for competitive dependency
parsing systems, much as DMV did.
This paper offers two main contributions. First,
by revealing the connection between the two tasks,
we introduce a new approach to dependency pars-
ing, and open the way for use of SMT alignment re-
sources and tools for parsing. Our experiments with
the IBM models demonstrate the potential of this ap-
proach and provide a strong motivation for further
development. The second contribution is a publicly-
available framework for exploring new alignment
models. The framework uses Gibbs sampling tech-
niques and includes our sampling-based implemen-
tations of the IBM models (see Section 3.4). The
sampling approach makes it easy to modify the ex-
isting models and add new ones. The framework
can be used both for dependency parsing and for bi-
lingual word alignment.
The rest of the paper is structured as follows. In
Section 2 we present a brief overview of those works
in the fields of dependency parsing and alignment
for statistical machine translation which are directly
</bodyText>
<page confidence="0.938862">
1214
</page>
<note confidence="0.816355">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1214–1222,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999812666666667">
relevant to this paper. Section 3 describes the con-
nection between the two problems, examines the
shared assumptions of the DMV and IBM models,
and describes our framework and algorithms. In
Section 4 we present our experiments and discuss
the results. We conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.973211" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<subsectionHeader confidence="0.96139">
2.1 Unsupervised Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999830166666666">
In recent years, the field of supervised parsing has
advanced tremendously, to the point where highly
accurate parsers are available for many languages.
However, supervised methods require the manual
annotation of training data with parse trees, a pro-
cess which is expensive and time consuming. There-
fore, for domains and languages with minimal re-
sources, unsupervised parsing is of great impor-
tance.
Early work in the field focused on models that
made use primarily of the co-occurrence informa-
tion of the head and its argument (Yuret, 1998;
Paskin, 2001). The introduction of DMV by Klein
and Manning (2004) represented a shift in the di-
rection of research in the field. DMV is based on
a linguistically motivated generative model, which
follows common practice in supervised parsing and
takes into consideration the distance between head
and argument, as well as the valence (the capac-
ity of a head word to attach arguments). Klein and
Manning (2004) also shifted from a lexical repre-
sentation of the sentences to representing them as
part-of-speech sequences. DMV strongly outper-
formed previous models and was the first unsuper-
vised dependency induction system to achieve accu-
racy above the right-branching baseline. Much sub-
sequent work in the field has focused on modifica-
tions and extensions of DMV, and it is the basis for
today’s state-of-the-art systems (Cohen and Smith,
2009; Headden III et al., 2009).
</bodyText>
<subsectionHeader confidence="0.989322">
2.2 Alignment for SMT
</subsectionHeader>
<bodyText confidence="0.9997925">
SMT treats translation as a machine learning prob-
lem. It attempts to learn a translation model from
a parallel corpus composed of sentences and their
translations. The IBM models (Brown et al., 1993)
represent the first generation of word-based SMT
models, and serve as a starting point for most cur-
</bodyText>
<figureCaption confidence="0.9948585">
Figure 1: An example of an alignment between an En-
glish sentence (top) and its French translation (bottom).
</figureCaption>
<bodyText confidence="0.9998294">
rent SMT systems (e.g., Moses, Koehn et al. 2007;
Hiero, Chiang 2005). The models employ the notion
of alignment between individual words in the source
and translation. An example of such an alignment is
given in Figure 1.
The IBM models all seek to maximize Pr(f le),
the probability of a French translation f of an En-
glish sentence e. This probability is broken down
by taking into account all possible alignments a be-
tween e and f, and their probabilities:
</bodyText>
<equation confidence="0.893053">
Pr(f le) _ Ya Pr(f,ale) (1)
</equation>
<bodyText confidence="0.9998755">
Each of the IBM models is based on the previous one
in the series, and adds another level of latent parame-
ters which take into account a specific characteristic
of the data.
</bodyText>
<sectionHeader confidence="0.926445" genericHeader="method">
3 Alignment-based Dependency Parsing
</sectionHeader>
<subsectionHeader confidence="0.995378">
3.1 The Connection
</subsectionHeader>
<bodyText confidence="0.999951833333333">
The task of dependency parsing requires finding a
parse tree for a sentence, where two words are con-
nected by an edge if they participate in a syntactic
dependency relation. When dealing with unlabeled
dependencies, the exact nature of the relationship is
not determined. An example of a dependency parse
of a sentence is given in Figure 2 (left).
Another possible formulation of the problem is as
follows. Find a set of pairwise relations (si,sj) con-
necting a dependent word sj with its head word si in
the sentence. This alternate formulation allows us to
view the problem as one of alignment of a sentence
to itself, as shown in Figure 2 (right).
Given this perspective on the problem, it makes
sense to examine existing alignment models, com-
pare them to dependency parsing models, and see
if they can be successfully employed for the depen-
dency parsing task.
</bodyText>
<page confidence="0.994591">
1215
</page>
<figureCaption confidence="0.95147">
Figure 2: Left: An example of an unlabeled dependency parse of a sentence. Right: The same parse, in the form of
</figureCaption>
<bodyText confidence="0.616536">
an alignment between a head words (top) and their dependents (bottom).
</bodyText>
<subsectionHeader confidence="0.999133">
3.2 Comparing IBM &amp; DMV Assumptions
</subsectionHeader>
<bodyText confidence="0.999848339622642">
Lexical Association The core assumption of IBM
Model 1 is that the lexical identities of the En-
glish and French words help determine whether they
should be aligned. The same assumption is made in
all the dependency models mentioned in Section 2
regarding a head and its dependent (although DMV
uses word classes instead of the actual words).
Location IBM Model 2 adds the consideration
of difference in location between the English and
French words when considering the likelihood of
alignment. One of the improvements contributing
to the success of DMV was the notion of distance,
which was absent from previous models (see Sec-
tion 3 in Klein and Manning 2004).
Fertility IBM Model 3 adds the notion of fertil-
ity, or the idea that different words in the source lan-
guage tend to generate different numbers of words in
the target language. This corresponds to the notion
of valence, used by Klein and Manning (2004), and
the other major contributor to the success of DMV
(ibid.).
Null Source The IBM models all make use of
an additional “null” word in every sentence, which
has special status. It is attached to words in the
translation that do not correspond to a word in the
source. It is treated separately when calculating
distance (since it has no location) and fertility. In
these characteristics, it is very similar to the “root”
node, which is artificially added to parse trees and
used to represent the head of words which are not
dependents of any other word in the sentence.
In examining the core assumptions of the IBM
models, we note that there is a strong resemblance
to those of DMV. The similarity is at an abstract
level since the nature of the relationship that each
model attempts to detect is quite different. The
IBM models look for an equivalence relationship be-
tween lexical items in two languages, whereas DMV
addresses functional relationships between two el-
ements with distinct meanings. However, both at-
tempt to model a similar set of factors, which they
posit will be important to their respective tasks1.
This similarity motivates the work presented in the
rest of the paper, i.e, exploring the use of the IBM
alignment models for dependency parsing. It is im-
portant to note that the IBM models do not address
many important factors relevant to the parsing task.
For instance, they have no notion of a parse tree, a
deficit which may lead to degenerate solutions and
malformed parses. However, they serve as a good
starting point for exploring the alignment approach
to parsing, as well as discovering additional factors
that need to be addressed under this approach.
</bodyText>
<subsectionHeader confidence="0.991916">
3.3 Experimental Framework
</subsectionHeader>
<bodyText confidence="0.9999155">
We developed a Gibbs sampling framework for
alignment-based dependency parsing2. The tradi-
tional approach to alignment uses Expectation Max-
imization (EM) to find the optimal values for the
latent variables. In each iteration, it considers all
possible alignments for each pair of sentences, and
</bodyText>
<footnote confidence="0.743380166666667">
1These abstract notions (lexical association, proximity, ten-
dencies towards few or many relations, and allowing for unasso-
ciated items) play an important role in many relation-detection
tasks (e.g., co-reference resolution, Haghighi and Klein 2010).
2Available for download at:
http://people.dbmi.columbia.edu/∼sab7012
</footnote>
<page confidence="0.986894">
1216
</page>
<bodyText confidence="0.999866333333333">
chooses the optimal one based on the current pa-
rameter estimates. The sampling method, on the
other hand, only considers a small change in each
step - that of re-aligning a previously aligned target
word to a new source. The reason for our choice is
the ease of modification of such sampling models.
They allow for easy introduction of further param-
eters and more complex probabilistic functions, as
well as Bayesian priors, all of which are likely to be
helpful in development3.
Under the sampling framework, the model pro-
vides the probability of changing the alignment A[i]
of a target word i from a previously aligned source
word j to a new one ˆj. In all the models we consider,
this probability is proportional to the ratio between
the scores of the old sentence alignment A and the
new one ˆA, which differs from the old only in the
realignment of i to ˆj.
</bodyText>
<equation confidence="0.9905275">
P(A[i] = j ⇒ A[i] = jˆ ) ∼ Pmodel(ˆA) (2)
Pmodel (A)
</equation>
<bodyText confidence="0.999955">
As a starting point for our dependency parsing
model, we re-implemented the first three IBM mod-
els 4 in the sampling framework.
</bodyText>
<subsectionHeader confidence="0.995391">
3.4 Reformulating the IBM models
</subsectionHeader>
<bodyText confidence="0.9998676">
IBM Model 1 According to this model, the prob-
ability of an alignment between target word i and
source word jˆ depends only on the lexical identities
of the two words wi and wjˆ respectively. This gives
us equation 3.
</bodyText>
<equation confidence="0.9958085">
P(wi,wˆj)
P(A[i] ⇒ jˆ ) ∼
(3)
P(wi,wj)
</equation>
<bodyText confidence="0.997379">
In our implementation we assume the alignment
follows a Chinese Restaurant Process (CRP), where
</bodyText>
<footnote confidence="0.9932429">
3Preliminary experiments using the EM approach via the
GIZA++ toolkit (Och and Ney, 2003) resulted in similar per-
formance to that of the sampling method for IBM Models 1 and
2. However, we were unable to explore the use of Model 3
under that framework, since the implementation of the model
was strongly coupled to other, SMT-specific, optimizations and
heuristics.
4Our implementation, as well as some core components in
our framework, are based on code kindly provided by Chris
Dyer.
</footnote>
<bodyText confidence="0.998771666666667">
the probability of wi aligning to wj is proportional
to the number of times they have been aligned in the
past (the rest of the data), as follows:
</bodyText>
<equation confidence="0.999745666666667">
#(wi,wj) + α1/V
P(wi,w ˆj) = (4)
#(∗,wj) + α1
</equation>
<bodyText confidence="0.999493466666667">
Here, #(wi,wj) represents the number of times the
target word wi was observed to be aligned to wj in
the rest of the data, and ∗ stands for any word, V
is the size of the vocabulary, and α1 is a hyperpa-
rameter of the CRP, which can also be viewed as a
smoothing factor.
IBM Model 2 The original IBM model 2 is a
distortion model that assumes that the probability
of an alignment between target word i and source
word jˆ depends only on the locations of the words,
i.e., the values i and ˆj, taking into account the dif-
ferent lengths l and m of the source and target sen-
tences, respectively. For dependency parsing, where
we align sentences to themselves, l = m. This gives
us equation 5.
</bodyText>
<equation confidence="0.999393">
P(A[i] ⇒jˆ) ∼ Pmodel(ˆA) = P(i, ˆj,l)
Pmodel(A) P(i, j,l)
P(i, ˆj,l) = #(i, ˆj,l) + α2/D (5)
#(i,∗,l) + α2
</equation>
<bodyText confidence="0.99995465">
Again, we assume a CRP when choosing a dis-
tortion value, where D is the expected number of
distance values (set to 10 in our experiments), α2
is the CRP hyperparameter, #(i, j,l) is the number
of times a target word in position i was aligned to
a source word in position j in sentences of length l,
and #(i,∗,l) is the number of times word in position
i was aligned (to any source position) in sentences
of length l.
Even without the need for handling different
lengths for source and target sentences, this model
is complex and requires estimating a separate prob-
ability for each triplet (i, j,l). In addition, the as-
sumption that the distance distribution depends only
on the sentence length and is similar for all to-
kens seems unreasonable, especially when dealing
with part-of-speech tokens and dependency rela-
tions. Such concerns have been mentioned in the
SMT literature and were shown to be justified in
our experiments (see Sec. 4). For this reason, we
</bodyText>
<equation confidence="0.999672666666667">
P(A[i] ⇒ I&apos;model(ˆA) = ∏kP(wk,wA[k])
J )�
Pmodel(A) ∏k0 P(wk0,wˆA[k0])
</equation>
<page confidence="0.899226">
1217
</page>
<bodyText confidence="0.999913">
also implemented an alternate distance model, based
loosely on Liang et al. (2006). Under the alternate
model, the probability of an alignment between tar-
get word i and source word jˆ depends on the distance
between them, their order, the sentence length, and
the word type of the head, according to equation 6.
</bodyText>
<equation confidence="0.9994255">
P(i,ˆj,l) = #[wi,(i- ˆj),l] + α3/D (6)
#(wi,∗,l) +α3
</equation>
<bodyText confidence="0.99795075">
IBM Model 3 This model handles the notion of
fertility (or valence). Under this model, the proba-
bility of an alignment depends on how many target
words are aligned to each of the source words. Each
source word type wˆj, has a distribution specifying
the probability of having n aligned target words. The
probability of an alignment is proportional to the
product of the probabilities of the fertilities in the
alignment and takes into account the special status of
the null word (represented by the index j = 0). This
probability is given in Equation 7, which is based on
Equation 32 in Brown et al. (1993)5.
</bodyText>
<equation confidence="0.9992125">
P(A) ∼ Cl − φ0 l−2φ0 φ0l #(wj, φj) + α4 /F
$0 )p0 p1 ��j #(wj,∗)+α4
j 1
(7)
</equation>
<bodyText confidence="0.9999575">
Here, φj denotes the number of target words aligned
to the j-th source word in alignment A. p1 and p0
sum to 1 and are used to derive the probability that
there will be φ0 null-aligned words in a sentence
containing l words6. #(wj,φj) represents the num-
ber of times source word wj was observed to have
φj dependent target words, #(wj,∗) is the number
of times wj appeared in the data, F is the expected
number of fertility values (5 in our experiments),
and α4 is the CRP hyperparameter.
Combining the Models The original IBM mod-
els work in an incremental fashion, with each model
using the output of the previous one as a starting
point and adding a new component to the probabil-
ity distribution. The dependency parsing framework
employs a similar approach. It uses the alignments
</bodyText>
<footnote confidence="0.744401">
5The transitional version of this equation depends on
whether either the old source word (j) or the new one ( ˆj) are
null, and is omitted for brevity. Further details can be found in
Brown et al. (1993) Section 4.4 and Equation 43.
6For details, see Brown et al. (1993) Equation 31.
</footnote>
<bodyText confidence="0.9989115">
learned by the previous model as the starting point of
the next and combines the probability distributions
of each component via a product model. This al-
lows for the easy introduction of new models which
consider different aspects of the alignment and com-
plement each other.
Preventing Self-Alignment When adapting the
alignment approach to dependency parsing, we view
the task as that of aligning a sentence to itself. One
issue we must address is preventing the degenerate
solution of aligning each word to itself. For this pur-
pose we introduce a simple model into the product
which gives zero probability to alignments which
contain a word aligned to itself, as in equation 8.
</bodyText>
<equation confidence="0.999856">
_ _ = jˆ
P(A[i] — j) — 0 if { l 11 otherwise (8)
</equation>
<sectionHeader confidence="0.999581" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.944463">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999945266666667">
We evaluated our model on several corpora. The first
of these was the Penn. Treebank portion of the Wall
Street Journal (WSJ). We used the Constituent-to-
Dependency Conversion Tool7 to convert the tree-
bank format into CoNLL format.
We also made use of the Danish and Dutch
datasets from the CoNLL 2006 shared task8. Since
we do not make use of annotation, we can induce a
dependency structure on the entire dataset provided
(disregarding the division into training and testing).
Following Klein and Manning (2004), we used
the gold-standard part-of-speech sequences rather
than the lexical forms and evaluated on sentences
containing 10 or fewer tokens after removal of punc-
tuation.
</bodyText>
<sectionHeader confidence="0.611431" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999852333333333">
Table 1 shows the results of the IBM Models on
the task of directed (unlabeled) dependency parsing.
We compare to the right-branching baseline used by
Klein and Manning (2004). For the WSJ10 corpus,
the authors reported 43.2% accuracy for DMV and
33.6% for the baseline. Although there are small
</bodyText>
<footnote confidence="0.9997915">
7nlp.cs.lth.se/software/treebank converter/
8http://nextens.uvt.nl/∼conll/
</footnote>
<page confidence="0.946498">
1218
</page>
<table confidence="0.99714975">
Corpus M 1 M2 M3 R-br
WSJ10 25.42 35.73 39.32 32.85
Dutch10 25.17 32.46 35.28 28.42
Danish10 23.12 25.96 41.94 16.05 *
</table>
<tableCaption confidence="0.8618925">
Table 1: Percent accuracy of IBM Models 1-3 (M1-3) and
the right-branching baseline (R-br) on several corpora.
</tableCaption>
<table confidence="0.999732166666667">
PoS attachment
NNS JJ
RB VBZ
VBD NN
VB TO
CC NNS
</table>
<tableCaption confidence="0.9944705">
Table 2: Most likely dependency attachment for the top
ten most common parts-of-speech, according to Model 1.
</tableCaption>
<bodyText confidence="0.99957925">
differences in evaluation, as evidenced by the dif-
ference between our baseline scores, IBM Models
2 and 3 outperform the baseline by a large margin
and Model 3 approaches the performance of DMV.
On the Dutch and Danish datasets, the trends are
similar. On the latter dataset, even Model 1 out-
performs the right-branching baseline. However, the
Danish dataset is unusual (see Buchholz and Marsi
2006) in that the alternate adjacency baseline of left-
branching (also mentioned by Klein and Manning
2004) is extremely strong and achieves 48.8% di-
rected accuracy.
</bodyText>
<subsectionHeader confidence="0.99841">
4.3 Analysis
</subsectionHeader>
<bodyText confidence="0.99994805">
In order to better understand what our alignment
model was learning, we looked at each component
element individually.
Lexical Association To explore what Model 1 was
learning, we analyzed the resulting probability ta-
bles for association between tokens. Table 2 shows
the most likely dependency attachment for the top
ten most common parts-of-speech. The model is
clearly learning meaningful connections between
parts of speech (determiners and adjectives to nouns,
adverbs to verbs, etc.), but there is little notion of
directionality, and cycles can exist. For instance,
the model learns the connection between determiner
and noun, but is unsure which is the head and which
the dependent. A similar connection is learned be-
tween to and verbs in the base form (VB). This in-
consistency is, to a large extent, the result of the
deficiencies of the model, stemming from the fact
that the IBM models were designed for a different
task and are not trying to learn a well-formed tree.
However, there is a strong linguistic basis to con-
sider the directionality of these relations difficult.
There is some debate among linguists as to whether
the head of a noun phrase is the noun or the deter-
miner9 (see Abney 1987). Each can be seen as a dif-
ferent kind of head element, performing a different
function, similarly to the multiple types of depen-
dency relations identified in Hudson’s (1990) Word
Grammar. A similar case can be made regarding the
head of an infinitive phrase. The infinitive form of
the verb may be considered the lexical head, deter-
mining the predicate, while to can be seen as the
functional head, encoding inflectional features, as in
Chomsky’s (1981) Government &amp; Binding model10.
Distance Models The original IBM distortion
model (Model 2), which does not differentiate be-
tween words types and looks only at positions, has
an accuracy of 33.43% on the WSJ10 corpus. In
addition, it tends to strongly favor left-branching at-
tachment (57.2% of target words were attached to
the word immediately to their right, 22.6% to their
left, as opposed to 31% and 25.8% in the gold stan-
dard). The alternative distance model we proposed,
which takes into account the identity of the head
word, achieves better accuracy and is closer to the
gold standard balance (43.5% right and 35.3% left).
Figure 3 shows the distribution of the location of
the dependent relative to the head word (at position
0) for several common parts-of-speech. It is inter-
esting to see that singular and plural nouns (NN,
NNS) behave similarly. They both have a strong
preference for local attachment and a tendency to-
wards a left-dependent (presumably the determiner,
see above Table 2). Pronouns (NNP), on the other
hand, are more likely to attach to the right since
they are not modified by determiners. Verbs in past
(VBZ) and present (VBD, VBP) forms have simi-
lar behavior, with a flatter distribution of dependent
locations, whereas the base form (VB) attaches al-
most exclusively to the preceding token, presumably
</bodyText>
<footnote confidence="0.995523333333333">
9In fact, the original DMV chose the determiner as the head
(see discussion in Klein and Manning 2004, Section 3).
10We thank an anonymous reviewer for elucidating this point.
</footnote>
<table confidence="0.963249833333333">
PoS attachment
NN DET
IN NN
NNP NNP
DET NN
JJ NN
</table>
<page confidence="0.976307">
1219
</page>
<figureCaption confidence="0.997195">
Figure 3: Distribution of head-to-dependent distance for several types of verbs (left) and nouns (right), as learned by
our alternate distance model.
</figureCaption>
<bodyText confidence="0.993711763157895">
to (see Table 2).
Fertility Figure 4 shows the distribution of fertil-
ity values for several common parts of speech. Verbs
have a relatively flat distribution with a longer tail
as compared to nouns, which means they are likely
to have a larger number of arguments. Once again,
the base form (VB) exhibits different behavior from
the other verbs forms, taking almost exclusively one
argument. This is likely an effect of the strong con-
nection between base form verbs and the preceding
word to.
Hyper-Parameters Each of our models requires a
value for its CRP hyperparameter (see Section 3.4).
In this work, since parameter estimation was not our
focus, we set the hyperparameters to be approxi-
mately K1, where K is the number of possible val-
ues, according to the rule of thumb common in the
literature. Specifically, we chose a1 = 0.01,a3 =
0.05,a4 = 0.1. We investigated the effect of these
choices on performance in a separate set of exper-
iments, which showed that small variations (up to
an order of magnitude) in these parameters had little
effect on the results.
In addition to the CRP parameters, Model 3 re-
quires a value for p1, the null fertility hyperparame-
ter. In our experiments, we found that this hyper-
parameter had a very strong effect on results if it
was above 0.1, creating many spurious null align-
ments. However, below that threshold, the effects
were small. In the experiments reported here, we set
p1 = 0.01.
Initialization One issue with DMV, which is of-
ten mentioned, is its sensitivity to initialization. We
tested our model with random initialization (uniform
alignment probabilities) and with an approximation
of the ad-hoc “harmonic” initialization described in
Klein and Manning (2004) and found no noticeable
difference in accuracy.
</bodyText>
<subsectionHeader confidence="0.870828">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999992210526316">
The accuracy achieved by the IBM models (Table 1)
is surprisingly high, given the fact that the IBM
models were not designed with dependency parsing
in mind. It is likely that customizing the models to
the task will result in even better performance. Our
findings in Section 4.3 support this hypothesis. The
analysis showed that the lack of tree structure in the
model impacted the learning, and therefore it is ex-
pected that a component which enforces tree struc-
ture (prevents cycles) will be beneficial.
Although it lacks an inherent notion of tree struc-
ture, the alignment-based approach has several ad-
vantages over the head-outward approach of DMV
and related models. It can consider the alignment as
a whole and take into account global sentence con-
straints, not just head-dependent relations. These
may also include tree-structure constraints common
to the head-outward approaches, but can be more
flexible in how they are addressed. For instance,
</bodyText>
<page confidence="0.986413">
1220
</page>
<figureCaption confidence="0.999962">
Figure 4: Distribution of fertility values for several types of verbs (left) and nouns (right), as learned by IBM Model 3.
</figureCaption>
<bodyText confidence="0.999856">
DMV’s method of modeling tree structure does
not allow non-projective dependencies, whereas an
alignment-based model may choose to allow or con-
strain non-projectivity, as learned from the data. An-
other advantage of our alignment-based models is
the fact that they are not strongly sensitive to ini-
tialization and can be started from a set of random
alignments.
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999981435897436">
We have described an alternative formulation of de-
pendency parsing as a problem of word alignment.
This connection motivated us to explore the possi-
bility of using alignment tools for the task of un-
supervised dependency parsing. We chose to ex-
periment with the well-known IBM alignment mod-
els which share a set of similar modeling assump-
tions with Klein and Manning’s (2004) Dependency
Model with Valence. Our experiments showed that
the IBM models are surprisingly effective at the
dependency parsing task, outperforming the right-
branching baseline and approaching the accuracy of
DMV. Our results demonstrate that the alignment
approach can be used as a foundation for depen-
dency parsing algorithms and motivates further re-
search in this area.
There are many interesting avenues for further re-
search. These include improving and extending the
existing IBM models, as well as introducing new
models that are specifically designed for the parsing
task and represent relevant linguistic considerations
(e.g., enforcing tree structure, handling crossing de-
pendencies, learning left- or right-branching tenden-
cies).
In Spitkovsky et al. (2010), the authors show that
a gradual increase in the complexity of the data can
aid the learning process. The IBM approach demon-
strated the benefit of a gradual increase of model
complexity. It would be interesting to see if the two
approaches could be successfully combined.
Finally, although we use our framework for de-
pendency parsing, the sampling approach and the
framework we developed can be used to explore new
models for bilingual word alignment. Furthermore,
an alignment-based parsing method is expected to
integrate well with SMT bi-lingual alignment mod-
els and may, therefore, be suitable for combined
models which use parse trees to improve word align-
ment (e.g., Burkett et al. 2010).
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999956666666667">
I would like to thank Chris Dyer for providing the
basis for the sampling implementation. I would
also like to thank Chris, Adam Lopez, Trevor Cohn,
Adam Faulkner and the anonymous reviewers for
their time and effort and their helpful comments and
suggestions.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9133685">
Abney, Steven. 1987. The English Noun Phrase in its
Sentential Aspect. Ph.D. thesis, Massachusetts Insti-
</reference>
<page confidence="0.798709">
1221
</page>
<reference confidence="0.999666379746835">
tute of Technology.
Brown, Peter F., Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: parameter
estimation. Comput. Linguist. 19(2):263–311.
Buchholz, Sabine and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In In Proc.
of CoNLL. pages 149–164.
Burkett, David, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In North American Association for Com-
putational Linguistics. Los Angeles.
Chiang, David. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL ’05: Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics. Association for Com-
putational Linguistics, Morristown, NJ, USA, pages
263–270.
Chomsky, Noam. 1981. Lectures on government and
binding: the Pisa lectures /Noam Chomsky.
Cohen, Shay B. and Noah A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In NAACL ’09: Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics. Associa-
tion for Computational Linguistics, Morristown, NJ,
USA, pages 74–82.
Haghighi, Aria and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics. Association for
Computational Linguistics, Los Angeles, California,
pages 385–393.
Headden III, William P., Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics, Boulder, Col-
orado, pages 101–109.
Hudson, R. 1990. English Word Grammar. Basil Black-
well, Oxford.
Klein, Dan and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ’04: Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics. Association for Computational
Linguistics, Morristown, NJ, USA, page 478.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In ACL ’07:
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions. As-
sociation for Computational Linguistics, Morristown,
NJ, USA, pages 177–180.
Liang, Percy, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference. Association for Computational Linguis-
tics, New York City, USA, pages 104–111.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics 29(1):19–51.
Paskin, Mark A. 2001. Grammatical bigrams. In
Thomas G. Dietterich, Suzanna Becker, and Zoubin
Ghahramani, editors, NIPS. MIT Press, pages 91–97.
Spitkovsky, Valentin I., Hiyan Alshawi, and Daniel Juraf-
sky. 2010. From Baby Steps to Leapfrog: How “Less
is More” in unsupervised dependency parsing. In Proc.
of NAACL-HLT.
Yuret, D. 1998. Discovery of linguistic relations using
lexical attraction. Ph.D. thesis, Department of Com-
puter Science and Electrical Engineering, MIT.
</reference>
<page confidence="0.99211">
1222
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.704559">
<title confidence="0.9936945">It Depends on the Unsupervised Dependency Parsing via Word Alignment</title>
<author confidence="0.990458">Samuel</author>
<affiliation confidence="0.875052">Dept. of Biomedical Columbia</affiliation>
<email confidence="0.999882">samuel.brody@dbmi.columbia.edu</email>
<abstract confidence="0.997492444444445">We reveal a previously unnoticed connection between dependency parsing and statistical machine translation (SMT), by formulating the dependency parsing task as a problem of word alignment. Furthermore, we show that two well known models for these respective tasks (DMV and the IBM models) share common modeling assumptions. This motivates us to develop an alignment-based framework for unsupervised dependency parsing. The framework (which will be made publicly available) is flexible, modular and easy to extend. Using this framework, we implement several algorithms based on the IBM alignment models, which prove surprisingly effective on the dependency parsing task, and demonstrate the potential of the alignment-based approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>The English Noun Phrase in its Sentential Aspect.</title>
<date>1987</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="20966" citStr="Abney 1987" startWordPosition="3508" endWordPosition="3509">ection between determiner and noun, but is unsure which is the head and which the dependent. A similar connection is learned between to and verbs in the base form (VB). This inconsistency is, to a large extent, the result of the deficiencies of the model, stemming from the fact that the IBM models were designed for a different task and are not trying to learn a well-formed tree. However, there is a strong linguistic basis to consider the directionality of these relations difficult. There is some debate among linguists as to whether the head of a noun phrase is the noun or the determiner9 (see Abney 1987). Each can be seen as a different kind of head element, performing a different function, similarly to the multiple types of dependency relations identified in Hudson’s (1990) Word Grammar. A similar case can be made regarding the head of an infinitive phrase. The infinitive form of the verb may be considered the lexical head, determining the predicate, while to can be seen as the functional head, encoding inflectional features, as in Chomsky’s (1981) Government &amp; Binding model10. Distance Models The original IBM distortion model (Model 2), which does not differentiate between words types and l</context>
</contexts>
<marker>Abney, 1987</marker>
<rawString>Abney, Steven. 1987. The English Noun Phrase in its Sentential Aspect. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Comput. Linguist.</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1529" citStr="Brown et al., 1993" startWordPosition="229" endWordPosition="232">approach. 1 Introduction Both statistical machine translation (SMT) and unsupervised dependency parsing have seen a surge of interest in recent years, as the need for large scale data processing has increased. The problems addressed by each of the fields seem quite different at first glance. However, in this paper, we reveal a strong connection between them and show that the problem of dependency parsing can be formulated as one of word alignment within the sentence. Furthermore, we show that the two models that are arguably the most influential in their respective fields, the IBM models 1-3 (Brown et al., 1993) and Klein and Manning’s (2004) Dependency Model with Valence (DMV), share a common set of modeling assumptions. Based on this connection, we develop a framework which uses an alignment-based approach for unsupervised dependency parsing. The framework is flexible and modular, and allows us to explore different modeling assumptions. We demonstrate these properties and the merit of the alignment-based parsing approach by implementing several dependency parsing algorithms based on the IBM alignment models and evaluating their performance on the task. Although the algorithms are not competitive wi</context>
<context position="5662" citStr="Brown et al., 1993" startWordPosition="884" endWordPosition="887">representing them as part-of-speech sequences. DMV strongly outperformed previous models and was the first unsupervised dependency induction system to achieve accuracy above the right-branching baseline. Much subsequent work in the field has focused on modifications and extensions of DMV, and it is the basis for today’s state-of-the-art systems (Cohen and Smith, 2009; Headden III et al., 2009). 2.2 Alignment for SMT SMT treats translation as a machine learning problem. It attempts to learn a translation model from a parallel corpus composed of sentences and their translations. The IBM models (Brown et al., 1993) represent the first generation of word-based SMT models, and serve as a starting point for most curFigure 1: An example of an alignment between an English sentence (top) and its French translation (bottom). rent SMT systems (e.g., Moses, Koehn et al. 2007; Hiero, Chiang 2005). The models employ the notion of alignment between individual words in the source and translation. An example of such an alignment is given in Figure 1. The IBM models all seek to maximize Pr(f le), the probability of a French translation f of an English sentence e. This probability is broken down by taking into account </context>
<context position="15829" citStr="Brown et al. (1993)" startWordPosition="2636" endWordPosition="2639">(wi,∗,l) +α3 IBM Model 3 This model handles the notion of fertility (or valence). Under this model, the probability of an alignment depends on how many target words are aligned to each of the source words. Each source word type wˆj, has a distribution specifying the probability of having n aligned target words. The probability of an alignment is proportional to the product of the probabilities of the fertilities in the alignment and takes into account the special status of the null word (represented by the index j = 0). This probability is given in Equation 7, which is based on Equation 32 in Brown et al. (1993)5. P(A) ∼ Cl − φ0 l−2φ0 φ0l #(wj, φj) + α4 /F $0 )p0 p1 ��j #(wj,∗)+α4 j 1 (7) Here, φj denotes the number of target words aligned to the j-th source word in alignment A. p1 and p0 sum to 1 and are used to derive the probability that there will be φ0 null-aligned words in a sentence containing l words6. #(wj,φj) represents the number of times source word wj was observed to have φj dependent target words, #(wj,∗) is the number of times wj appeared in the data, F is the expected number of fertility values (5 in our experiments), and α4 is the CRP hyperparameter. Combining the Models The original</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Comput. Linguist. 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing. In</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<pages>149--164</pages>
<contexts>
<context position="19584" citStr="Buchholz and Marsi 2006" startWordPosition="3278" endWordPosition="3281">M1-3) and the right-branching baseline (R-br) on several corpora. PoS attachment NNS JJ RB VBZ VBD NN VB TO CC NNS Table 2: Most likely dependency attachment for the top ten most common parts-of-speech, according to Model 1. differences in evaluation, as evidenced by the difference between our baseline scores, IBM Models 2 and 3 outperform the baseline by a large margin and Model 3 approaches the performance of DMV. On the Dutch and Danish datasets, the trends are similar. On the latter dataset, even Model 1 outperforms the right-branching baseline. However, the Danish dataset is unusual (see Buchholz and Marsi 2006) in that the alternate adjacency baseline of leftbranching (also mentioned by Klein and Manning 2004) is extremely strong and achieves 48.8% directed accuracy. 4.3 Analysis In order to better understand what our alignment model was learning, we looked at each component element individually. Lexical Association To explore what Model 1 was learning, we analyzed the resulting probability tables for association between tokens. Table 2 shows the most likely dependency attachment for the top ten most common parts-of-speech. The model is clearly learning meaningful connections between parts of speech</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Buchholz, Sabine and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In In Proc. of CoNLL. pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Burkett</author>
<author>John Blitzer</author>
<author>Dan Klein</author>
</authors>
<title>Joint parsing and alignment with weakly synchronized grammars. In North American Association for Computational Linguistics.</title>
<date>2010</date>
<location>Los Angeles.</location>
<marker>Burkett, Blitzer, Klein, 2010</marker>
<rawString>Burkett, David, John Blitzer, and Dan Klein. 2010. Joint parsing and alignment with weakly synchronized grammars. In North American Association for Computational Linguistics. Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="5939" citStr="Chiang 2005" startWordPosition="933" endWordPosition="934"> DMV, and it is the basis for today’s state-of-the-art systems (Cohen and Smith, 2009; Headden III et al., 2009). 2.2 Alignment for SMT SMT treats translation as a machine learning problem. It attempts to learn a translation model from a parallel corpus composed of sentences and their translations. The IBM models (Brown et al., 1993) represent the first generation of word-based SMT models, and serve as a starting point for most curFigure 1: An example of an alignment between an English sentence (top) and its French translation (bottom). rent SMT systems (e.g., Moses, Koehn et al. 2007; Hiero, Chiang 2005). The models employ the notion of alignment between individual words in the source and translation. An example of such an alignment is given in Figure 1. The IBM models all seek to maximize Pr(f le), the probability of a French translation f of an English sentence e. This probability is broken down by taking into account all possible alignments a between e and f, and their probabilities: Pr(f le) _ Ya Pr(f,ale) (1) Each of the IBM models is based on the previous one in the series, and adds another level of latent parameters which take into account a specific characteristic of the data. 3 Align</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>Chiang, David. 2005. A hierarchical phrase-based model for statistical machine translation. In ACL ’05: Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Morristown, NJ, USA, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Lectures on government and binding: the Pisa lectures /Noam Chomsky.</title>
<date>1981</date>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, Noam. 1981. Lectures on government and binding: the Pisa lectures /Noam Chomsky.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>74--82</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="5412" citStr="Cohen and Smith, 2009" startWordPosition="842" endWordPosition="845">n supervised parsing and takes into consideration the distance between head and argument, as well as the valence (the capacity of a head word to attach arguments). Klein and Manning (2004) also shifted from a lexical representation of the sentences to representing them as part-of-speech sequences. DMV strongly outperformed previous models and was the first unsupervised dependency induction system to achieve accuracy above the right-branching baseline. Much subsequent work in the field has focused on modifications and extensions of DMV, and it is the basis for today’s state-of-the-art systems (Cohen and Smith, 2009; Headden III et al., 2009). 2.2 Alignment for SMT SMT treats translation as a machine learning problem. It attempts to learn a translation model from a parallel corpus composed of sentences and their translations. The IBM models (Brown et al., 1993) represent the first generation of word-based SMT models, and serve as a starting point for most curFigure 1: An example of an alignment between an English sentence (top) and its French translation (bottom). rent SMT systems (e.g., Moses, Koehn et al. 2007; Hiero, Chiang 2005). The models employ the notion of alignment between individual words in t</context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Cohen, Shay B. and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Morristown, NJ, USA, pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>385--393</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="10842" citStr="Haghighi and Klein 2010" startWordPosition="1755" endWordPosition="1758">actors that need to be addressed under this approach. 3.3 Experimental Framework We developed a Gibbs sampling framework for alignment-based dependency parsing2. The traditional approach to alignment uses Expectation Maximization (EM) to find the optimal values for the latent variables. In each iteration, it considers all possible alignments for each pair of sentences, and 1These abstract notions (lexical association, proximity, tendencies towards few or many relations, and allowing for unassociated items) play an important role in many relation-detection tasks (e.g., co-reference resolution, Haghighi and Klein 2010). 2Available for download at: http://people.dbmi.columbia.edu/∼sab7012 1216 chooses the optimal one based on the current parameter estimates. The sampling method, on the other hand, only considers a small change in each step - that of re-aligning a previously aligned target word to a new source. The reason for our choice is the ease of modification of such sampling models. They allow for easy introduction of further parameters and more complex probabilistic functions, as well as Bayesian priors, all of which are likely to be helpful in development3. Under the sampling framework, the model prov</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Haghighi, Aria and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Los Angeles, California, pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Headden William P</author>
<author>Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>101--109</pages>
<location>Boulder, Colorado,</location>
<marker>P, Johnson, McClosky, 2009</marker>
<rawString>Headden III, William P., Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Boulder, Colorado, pages 101–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1990</date>
<publisher>Basil Blackwell,</publisher>
<location>Oxford.</location>
<marker>Hudson, 1990</marker>
<rawString>Hudson, R. 1990. English Word Grammar. Basil Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>478</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="4635" citStr="Klein and Manning (2004)" startWordPosition="717" endWordPosition="720">ndency Parsing In recent years, the field of supervised parsing has advanced tremendously, to the point where highly accurate parsers are available for many languages. However, supervised methods require the manual annotation of training data with parse trees, a process which is expensive and time consuming. Therefore, for domains and languages with minimal resources, unsupervised parsing is of great importance. Early work in the field focused on models that made use primarily of the co-occurrence information of the head and its argument (Yuret, 1998; Paskin, 2001). The introduction of DMV by Klein and Manning (2004) represented a shift in the direction of research in the field. DMV is based on a linguistically motivated generative model, which follows common practice in supervised parsing and takes into consideration the distance between head and argument, as well as the valence (the capacity of a head word to attach arguments). Klein and Manning (2004) also shifted from a lexical representation of the sentences to representing them as part-of-speech sequences. DMV strongly outperformed previous models and was the first unsupervised dependency induction system to achieve accuracy above the right-branchin</context>
<context position="8337" citStr="Klein and Manning 2004" startWordPosition="1343" endWordPosition="1346">n of IBM Model 1 is that the lexical identities of the English and French words help determine whether they should be aligned. The same assumption is made in all the dependency models mentioned in Section 2 regarding a head and its dependent (although DMV uses word classes instead of the actual words). Location IBM Model 2 adds the consideration of difference in location between the English and French words when considering the likelihood of alignment. One of the improvements contributing to the success of DMV was the notion of distance, which was absent from previous models (see Section 3 in Klein and Manning 2004). Fertility IBM Model 3 adds the notion of fertility, or the idea that different words in the source language tend to generate different numbers of words in the target language. This corresponds to the notion of valence, used by Klein and Manning (2004), and the other major contributor to the success of DMV (ibid.). Null Source The IBM models all make use of an additional “null” word in every sentence, which has special status. It is attached to words in the translation that do not correspond to a word in the source. It is treated separately when calculating distance (since it has no location)</context>
<context position="18238" citStr="Klein and Manning (2004)" startWordPosition="3064" endWordPosition="3067">rd aligned to itself, as in equation 8. _ _ = jˆ P(A[i] — j) — 0 if { l 11 otherwise (8) 4 Experiments 4.1 Data We evaluated our model on several corpora. The first of these was the Penn. Treebank portion of the Wall Street Journal (WSJ). We used the Constituent-toDependency Conversion Tool7 to convert the treebank format into CoNLL format. We also made use of the Danish and Dutch datasets from the CoNLL 2006 shared task8. Since we do not make use of annotation, we can induce a dependency structure on the entire dataset provided (disregarding the division into training and testing). Following Klein and Manning (2004), we used the gold-standard part-of-speech sequences rather than the lexical forms and evaluated on sentences containing 10 or fewer tokens after removal of punctuation. 4.2 Results Table 1 shows the results of the IBM Models on the task of directed (unlabeled) dependency parsing. We compare to the right-branching baseline used by Klein and Manning (2004). For the WSJ10 corpus, the authors reported 43.2% accuracy for DMV and 33.6% for the baseline. Although there are small 7nlp.cs.lth.se/software/treebank converter/ 8http://nextens.uvt.nl/∼conll/ 1218 Corpus M 1 M2 M3 R-br WSJ10 25.42 35.73 39</context>
<context position="19685" citStr="Klein and Manning 2004" startWordPosition="3294" endWordPosition="3297">VB TO CC NNS Table 2: Most likely dependency attachment for the top ten most common parts-of-speech, according to Model 1. differences in evaluation, as evidenced by the difference between our baseline scores, IBM Models 2 and 3 outperform the baseline by a large margin and Model 3 approaches the performance of DMV. On the Dutch and Danish datasets, the trends are similar. On the latter dataset, even Model 1 outperforms the right-branching baseline. However, the Danish dataset is unusual (see Buchholz and Marsi 2006) in that the alternate adjacency baseline of leftbranching (also mentioned by Klein and Manning 2004) is extremely strong and achieves 48.8% directed accuracy. 4.3 Analysis In order to better understand what our alignment model was learning, we looked at each component element individually. Lexical Association To explore what Model 1 was learning, we analyzed the resulting probability tables for association between tokens. Table 2 shows the most likely dependency attachment for the top ten most common parts-of-speech. The model is clearly learning meaningful connections between parts of speech (determiners and adjectives to nouns, adverbs to verbs, etc.), but there is little notion of directi</context>
<context position="22854" citStr="Klein and Manning 2004" startWordPosition="3821" endWordPosition="3824">and plural nouns (NN, NNS) behave similarly. They both have a strong preference for local attachment and a tendency towards a left-dependent (presumably the determiner, see above Table 2). Pronouns (NNP), on the other hand, are more likely to attach to the right since they are not modified by determiners. Verbs in past (VBZ) and present (VBD, VBP) forms have similar behavior, with a flatter distribution of dependent locations, whereas the base form (VB) attaches almost exclusively to the preceding token, presumably 9In fact, the original DMV chose the determiner as the head (see discussion in Klein and Manning 2004, Section 3). 10We thank an anonymous reviewer for elucidating this point. PoS attachment NN DET IN NN NNP NNP DET NN JJ NN 1219 Figure 3: Distribution of head-to-dependent distance for several types of verbs (left) and nouns (right), as learned by our alternate distance model. to (see Table 2). Fertility Figure 4 shows the distribution of fertility values for several common parts of speech. Verbs have a relatively flat distribution with a longer tail as compared to nouns, which means they are likely to have a larger number of arguments. Once again, the base form (VB) exhibits different behavi</context>
<context position="24844" citStr="Klein and Manning (2004)" startWordPosition="4154" endWordPosition="4157">RP parameters, Model 3 requires a value for p1, the null fertility hyperparameter. In our experiments, we found that this hyperparameter had a very strong effect on results if it was above 0.1, creating many spurious null alignments. However, below that threshold, the effects were small. In the experiments reported here, we set p1 = 0.01. Initialization One issue with DMV, which is often mentioned, is its sensitivity to initialization. We tested our model with random initialization (uniform alignment probabilities) and with an approximation of the ad-hoc “harmonic” initialization described in Klein and Manning (2004) and found no noticeable difference in accuracy. 4.4 Discussion The accuracy achieved by the IBM models (Table 1) is surprisingly high, given the fact that the IBM models were not designed with dependency parsing in mind. It is likely that customizing the models to the task will result in even better performance. Our findings in Section 4.3 support this hypothesis. The analysis showed that the lack of tree structure in the model impacted the learning, and therefore it is expected that a component which enforces tree structure (prevents cycles) will be beneficial. Although it lacks an inherent </context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Klein, Dan and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Morristown, NJ, USA, page 478.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics,</booktitle>
<pages>177--180</pages>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="5918" citStr="Koehn et al. 2007" startWordPosition="928" endWordPosition="931">ications and extensions of DMV, and it is the basis for today’s state-of-the-art systems (Cohen and Smith, 2009; Headden III et al., 2009). 2.2 Alignment for SMT SMT treats translation as a machine learning problem. It attempts to learn a translation model from a parallel corpus composed of sentences and their translations. The IBM models (Brown et al., 1993) represent the first generation of word-based SMT models, and serve as a starting point for most curFigure 1: An example of an alignment between an English sentence (top) and its French translation (bottom). rent SMT systems (e.g., Moses, Koehn et al. 2007; Hiero, Chiang 2005). The models employ the notion of alignment between individual words in the source and translation. An example of such an alignment is given in Figure 1. The IBM models all seek to maximize Pr(f le), the probability of a French translation f of an English sentence e. This probability is broken down by taking into account all possible alignments a between e and f, and their probabilities: Pr(f le) _ Ya Pr(f,ale) (1) Each of the IBM models is based on the previous one in the series, and adds another level of latent parameters which take into account a specific characteristic</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics, Morristown, NJ, USA, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>New York City, USA,</location>
<contexts>
<context position="14940" citStr="Liang et al. (2006)" startWordPosition="2480" endWordPosition="2483">s, this model is complex and requires estimating a separate probability for each triplet (i, j,l). In addition, the assumption that the distance distribution depends only on the sentence length and is similar for all tokens seems unreasonable, especially when dealing with part-of-speech tokens and dependency relations. Such concerns have been mentioned in the SMT literature and were shown to be justified in our experiments (see Sec. 4). For this reason, we P(A[i] ⇒ I&apos;model(ˆA) = ∏kP(wk,wA[k]) J )� Pmodel(A) ∏k0 P(wk0,wˆA[k0]) 1217 also implemented an alternate distance model, based loosely on Liang et al. (2006). Under the alternate model, the probability of an alignment between target word i and source word jˆ depends on the distance between them, their order, the sentence length, and the word type of the head, according to equation 6. P(i,ˆj,l) = #[wi,(i- ˆj),l] + α3/D (6) #(wi,∗,l) +α3 IBM Model 3 This model handles the notion of fertility (or valence). Under this model, the probability of an alignment depends on how many target words are aligned to each of the source words. Each source word type wˆj, has a distribution specifying the probability of having n aligned target words. The probability o</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Liang, Percy, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. Association for Computational Linguistics, New York City, USA, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12436" citStr="Och and Ney, 2003" startWordPosition="2031" endWordPosition="2034"> (2) Pmodel (A) As a starting point for our dependency parsing model, we re-implemented the first three IBM models 4 in the sampling framework. 3.4 Reformulating the IBM models IBM Model 1 According to this model, the probability of an alignment between target word i and source word jˆ depends only on the lexical identities of the two words wi and wjˆ respectively. This gives us equation 3. P(wi,wˆj) P(A[i] ⇒ jˆ ) ∼ (3) P(wi,wj) In our implementation we assume the alignment follows a Chinese Restaurant Process (CRP), where 3Preliminary experiments using the EM approach via the GIZA++ toolkit (Och and Ney, 2003) resulted in similar performance to that of the sampling method for IBM Models 1 and 2. However, we were unable to explore the use of Model 3 under that framework, since the implementation of the model was strongly coupled to other, SMT-specific, optimizations and heuristics. 4Our implementation, as well as some core components in our framework, are based on code kindly provided by Chris Dyer. the probability of wi aligning to wj is proportional to the number of times they have been aligned in the past (the rest of the data), as follows: #(wi,wj) + α1/V P(wi,w ˆj) = (4) #(∗,wj) + α1 Here, #(wi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Paskin</author>
</authors>
<title>Grammatical bigrams.</title>
<date>2001</date>
<pages>91--97</pages>
<editor>In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, NIPS.</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="4582" citStr="Paskin, 2001" startWordPosition="710" endWordPosition="711">und and Related Work 2.1 Unsupervised Dependency Parsing In recent years, the field of supervised parsing has advanced tremendously, to the point where highly accurate parsers are available for many languages. However, supervised methods require the manual annotation of training data with parse trees, a process which is expensive and time consuming. Therefore, for domains and languages with minimal resources, unsupervised parsing is of great importance. Early work in the field focused on models that made use primarily of the co-occurrence information of the head and its argument (Yuret, 1998; Paskin, 2001). The introduction of DMV by Klein and Manning (2004) represented a shift in the direction of research in the field. DMV is based on a linguistically motivated generative model, which follows common practice in supervised parsing and takes into consideration the distance between head and argument, as well as the valence (the capacity of a head word to attach arguments). Klein and Manning (2004) also shifted from a lexical representation of the sentences to representing them as part-of-speech sequences. DMV strongly outperformed previous models and was the first unsupervised dependency inductio</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>Paskin, Mark A. 2001. Grammatical bigrams. In Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani, editors, NIPS. MIT Press, pages 91–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="27521" citStr="Spitkovsky et al. (2010)" startWordPosition="4576" endWordPosition="4579">rming the rightbranching baseline and approaching the accuracy of DMV. Our results demonstrate that the alignment approach can be used as a foundation for dependency parsing algorithms and motivates further research in this area. There are many interesting avenues for further research. These include improving and extending the existing IBM models, as well as introducing new models that are specifically designed for the parsing task and represent relevant linguistic considerations (e.g., enforcing tree structure, handling crossing dependencies, learning left- or right-branching tendencies). In Spitkovsky et al. (2010), the authors show that a gradual increase in the complexity of the data can aid the learning process. The IBM approach demonstrated the benefit of a gradual increase of model complexity. It would be interesting to see if the two approaches could be successfully combined. Finally, although we use our framework for dependency parsing, the sampling approach and the framework we developed can be used to explore new models for bilingual word alignment. Furthermore, an alignment-based parsing method is expected to integrate well with SMT bi-lingual alignment models and may, therefore, be suitable f</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Spitkovsky, Valentin I., Hiyan Alshawi, and Daniel Jurafsky. 2010. From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yuret</author>
</authors>
<title>Discovery of linguistic relations using lexical attraction.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science and Electrical Engineering, MIT.</institution>
<contexts>
<context position="4567" citStr="Yuret, 1998" startWordPosition="708" endWordPosition="709"> 5. 2 Background and Related Work 2.1 Unsupervised Dependency Parsing In recent years, the field of supervised parsing has advanced tremendously, to the point where highly accurate parsers are available for many languages. However, supervised methods require the manual annotation of training data with parse trees, a process which is expensive and time consuming. Therefore, for domains and languages with minimal resources, unsupervised parsing is of great importance. Early work in the field focused on models that made use primarily of the co-occurrence information of the head and its argument (Yuret, 1998; Paskin, 2001). The introduction of DMV by Klein and Manning (2004) represented a shift in the direction of research in the field. DMV is based on a linguistically motivated generative model, which follows common practice in supervised parsing and takes into consideration the distance between head and argument, as well as the valence (the capacity of a head word to attach arguments). Klein and Manning (2004) also shifted from a lexical representation of the sentences to representing them as part-of-speech sequences. DMV strongly outperformed previous models and was the first unsupervised depe</context>
</contexts>
<marker>Yuret, 1998</marker>
<rawString>Yuret, D. 1998. Discovery of linguistic relations using lexical attraction. Ph.D. thesis, Department of Computer Science and Electrical Engineering, MIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>