<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.9991095">
Adapting a Polarity Lexicon using Integer Linear Programming
for Domain-Specific Sentiment Classification
</title>
<author confidence="0.938085">
Yejin Choi and Claire Cardie
</author>
<affiliation confidence="0.9268155">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.727201">
Ithaca, NY 14853
</address>
<email confidence="0.998296">
{ychoi,cardie}@cs.cornell.edu
</email>
<sectionHeader confidence="0.993899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994894">
Polarity lexicons have been a valuable re-
source for sentiment analysis and opinion
mining. There are a number of such lexi-
cal resources available, but it is often sub-
optimal to use them as is, because general
purpose lexical resources do not reflect
domain-specific lexical usage. In this pa-
per, we propose a novel method based on
integer linear programming that can adapt
an existing lexicon into a new one to re-
flect the characteristics of the data more
directly. In particular, our method collec-
tively considers the relations among words
and opinion expressions to derive the most
likely polarity of each lexical item (posi-
tive, neutral, negative, or negator) for the
given domain. Experimental results show
that our lexicon adaptation technique im-
proves the performance of fine-grained po-
larity classification.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817879310345">
Polarity lexicons have been a valuable resource for
sentiment analysis and opinion mining. In particu-
lar, they have been an essential ingredient for fine-
grained sentiment analysis (e.g., Kim and Hovy
(2004), Kennedy and Inkpen (2005), Wilson et al.
(2005)). Even though the polarity lexicon plays an
important role (Section 3.1), it has received rela-
tively less attention in previous research. In most
cases, polarity lexicon construction is discussed
only briefly as a preprocessing step for a sentiment
analysis task (e.g., Hu and Liu (2004), Moilanen
and Pulman (2007)), but the effect of different al-
ternative polarity lexicons is not explicitly inves-
tigated. Conversely, research efforts that focus
on constructing a general purpose polarity lexicon
(e.g., Takamura et al. (2005), Andreevskaia and
Bergler (2006), Esuli and Sebastiani (2006), Rao
and Ravichandran (2009)) generally evaluate the
lexicon in isolation from any potentially relevant
NLP task, and it is unclear how the new lexicon
might affect end-to-end performance of a concrete
NLP application.
It might even be unrealistic to expect that there
can be a general-purpose lexical resource that
can be effective across all relevant NLP applica-
tions, as general-purpose lexicons will not reflect
domain-specific lexical usage. Indeed, Blitzer
et al. (2007) note that the polarity of a particu-
lar word can carry opposite sentiment depending
on the domain (e.g., Andreevskaia and Bergler
(2008)).
In this paper, we propose a novel method based
on integer linear programming to adapt an existing
polarity lexicon into a new one to reflect the char-
acteristics of the data more directly. In particular,
our method considers the relations among words
and opinion expressions collectively to derive the
most likely polarity of each word for the given do-
main.
Figure 1 depicts the key insight of our approach
using a bipartite graph. On the left hand side, each
node represents a word, and on the right hand side,
each node represents an opinion expression. There
is an edge between a word wi and an opinion ex-
pression ej, if the word wi appears in the expres-
sion ej. We assume the possible polarity of each
expression is one of the following three values:
{positive, neutral, negative}, while the possible
polarity of each word is one of: {positive, neutral,
negative or negator}. Strictly speaking, negator is
not a value for polarity, but we include them in our
lexicon, because valence shifters or negators have
been shown to play an important role for sentiment
analysis (e.g., Polanyi and Zaenen (2004), Moila-
nen and Pulman (2007), Choi and Cardie (2008)).
Typically, the ultimate goal of the sentiment
analysis task is to determine the expression-level
(or sentiment/ document-level) polarities, rather
</bodyText>
<page confidence="0.959365">
590
</page>
<note confidence="0.9966485">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999383530612245">
than the correct word-level polarities with respect
to the domain. Therefore, word-level polarities
can be considered as latent information. In this pa-
per, we show how we can improve the word-level
polarities of a general-purpose polarity lexicon by
utilizing the expression-level polarities, and in re-
turn, how the adapted word-level polarities can
improve the expression-level polarities.
In Figure 1, there are two types of relations
we could exploit when adapting a general-purpose
polarity lexicon into a domain-specific one. The
first are word-to-word relations within each ex-
pression. That is, if we are not sure about the
polarity of a certain word, we can still make a
guess based on the polarities of other words within
the same expression and knowledge of the polar-
ity of the expression. The second type of relations
are word-to-expression relations: e.g., some words
appear in expressions that take on a variety of po-
larities, while other words are associated with ex-
pressions of one polarity class or another.
In relation to previous research, analyz-
ing word-to-word (intra-expression) relations
is most related to techniques that determine
expression-level polarity in context (e.g., Wilson
et al. (2005)), while exploring word-to-expression
(inter-expression) relations has connections to
techniques that employ more of a global-view of
corpus statistics (e.g., Kanayama and Nasukawa
(2006)).1
While most previous research exploits only one
or the other type of relation, we propose a unified
method that can exploit both types of semantic re-
lation, while adapting a general purpose polarity
lexicon into a domain specific one. We formulate
our lexicon adaptation task using integer linear
programming (ILP), which has been shown to be
very effective when solving problems with com-
plex constraints (e.g., Roth and Yih (2004), Denis
and Baldridge (2007)). And the word-to-word and
word-to-expression relations discussed above can
be encoded as soft and hard constraints in ILP. Un-
fortunately, one class of constraint that we would
like to encode (see Section 2) will require an
exponentially many number of constraints when
grounded into an actual ILP problem. We there-
fore propose an approximation scheme to make
the problem more practically solvable.
We evaluate the effect of the adapted lex-
</bodyText>
<footnote confidence="0.989853">
1In case of document-level polarity classification, word-
to-expression relations correspond to word-to-document re-
lations.
</footnote>
<figureCaption confidence="0.879130333333333">
Figure 1: The relations among words and expres-
sions. + indicates positive, - indicates negative, =
indicates neutral, and , indicates a negator.
</figureCaption>
<bodyText confidence="0.999544666666667">
icon in the context of a concrete NLP task:
expression-level polarity classification. Experi-
mental results show that our lexicon adaptation
technique improves the accuracy of two com-
petitive expression-level polarity classifiers from
64.2% - 70.4% to 67.0% - 71.2%..
</bodyText>
<sectionHeader confidence="0.979542" genericHeader="method">
2 An Integer Linear Programming
Approach
</sectionHeader>
<bodyText confidence="0.999872777777778">
In this section, we describe how we formulate the
lexicon adaptation task using integer linear pro-
gramming. Before we begin, we assume that we
have a general-purpose polarity lexicon L, and a
polarity classification algorithm f(el, L), that can
determine the polarity of the opinion expression el
based on the words in el and the initial lexicon L.
The polarity classification algorithm f(�) can be
either a heuristic-based one, or a machine-learning
based one – we consider it as a black box for now.
Constraints for word-level polarities: For
each word xi, we define four binary variables:
x+i , x=i , x−i , x¬i to represent positive, neutral, neg-
ative polarity, and negators respectively. If xδi = 1
for some S E {+, =, −, ,I, then the word xi has
the polarity S. The following inequality constraint
states that at least one polarity value must be cho-
sen for each word.
</bodyText>
<equation confidence="0.989793">
x+i + x=i + x−i + x¬i &gt;= 1 (1)
</equation>
<bodyText confidence="0.999861">
If we allow only one polarity per word, then the
above inequality constraint should be modified as
an equality constraint. Although most words tend
to associate with a single polarity, some can take
on more than one polarity. In order to capture this
observation, we introduce an auxiliary binary vari-
able αi for each word xi. Then the next inequality
</bodyText>
<figure confidence="0.9988668">
�
�
�
�
W
W
W
W
W
�
�
�
�
exp W W W
exp
exp
exp
W W W W
W W
W
</figure>
<page confidence="0.99617">
591
</page>
<bodyText confidence="0.9843355">
constraint states that at most two polarities can be
chosen for each word.
</bodyText>
<equation confidence="0.991982">
x+i + x=i + x−i + x¬i &lt;= 1 + αi (2)
</equation>
<bodyText confidence="0.999914818181818">
Next we introduce the initial part of our objec-
tive function.
For the auxiliary variable αi, we apply a con-
stant weight wα to discourage ILP from choosing
more than one polarity for each word. We can al-
low more than two polarities for each word, by
adding extra auxiliary variables and weights. For
each variable xδi, we define its weight wδi , which
indicates how likely it is that word xi carries the
polarity 6. We define the value of wδi using two
different types of information as follows:
</bodyText>
<equation confidence="0.75935">
wδi :=Lwδi + Cwδi
</equation>
<bodyText confidence="0.999567">
where Lwδi is the degree of polarity 6 for word xi
determined by the general-purpose polarity lexi-
con L, and Cwδi is the degree of polarity 6 deter-
mined by the corpus statistics as follows:2
</bodyText>
<equation confidence="0.926847">
C δ # of xi in expressions with polarity 6
wi :=
</equation>
<bodyText confidence="0.9917792">
# of xi in the corpus C
Note that the occurrence of word xi in an ex-
pression ej with a polarity 6 does not necessar-
ily mean that the polarity of xi should also be
6, as the interpretation of the polarity of an ex-
pression is more than just a linear sum of the
word-level polarities (e.g., Moilanen and Pulman
(2007)). Nonetheless, not all expressions require
a complicated inference procedure to determine
their polarity. Therefore, Cwδi still provides useful
information about the likely polarity of each word
based on the corpus statistics.
From the perspective of Chomskyan linguistics,
the weights Lwδi based on the prior polarity from
the lexicon can be considered as having a ”com-
petence” component , while Cwδi derived from
the corpus counts can be considered as a ”perfor-
mance” component (Noam Chomsky (1965)).
2If a word xi is in an expression that is not an opinion,
then we count it as an occurrence with neutral polarity.
Constraints for content-word negators: Next
we describe a constraint that exploits knowledge
of the typical distribution of content-word nega-
tors in natural language. Content-word negators
are words that are not function words, but act se-
mantically as negators (Choi and Cardie, 2008).3
Although it is possible to artificially construct a
very convoluted sentence with lots of negations, it
is unlikely for multiple layers of negations to ap-
pear very often in natural language (Pickett et al.
(1996)). Therefore, we allow at most one content-
word negator for each expression el. Because we
do not restrict the number of function-word nega-
tors, our constraint still gives room for multiple
layers of negations.
</bodyText>
<equation confidence="0.995195">
� x¬i &lt;= 1 (4)
i∈µ(el)
</equation>
<bodyText confidence="0.999937090909091">
In the above constraint, µ(el) indicates the set
of indices of content words appearing in el. For
instance, if i E µ(el), then xi appears in el. This
constraint can be polished further to accommodate
longer expressions where multiple content-word
negators are more likely to appear, by adding a
separate constraint with a sliding window.
Constraints for expression-level polarities:
Before we begin, we introduce 7r(el) that will be
used often in the remaining section. For each ex-
pression el, we define 7r(el) to be the set of con-
tent words appearing in el, together with the most
likely polarity proposed by a general-purpose po-
larity lexicon L. For instance, if x+i E 7r(el), then
the polarity of word xi is + according to L.
Next we encode constraints that consider
expression-level polarities. If the polarity classifi-
cation algorithm f(el, L) makes an incorrect pre-
diction for el using the original lexicon L, then we
need to encourage ILP to fix the error by suggest-
ing different word-level polarities. We capture this
idea by the following constraint:
</bodyText>
<equation confidence="0.976162">
� xδi &lt;= |7r(el) |− 1 + βl (5)
xδi ∈π(el)
</equation>
<bodyText confidence="0.860137">
The auxiliary binary variable βl is introduced
for each el so that the assignment 7r(el) does not
have to be changed if paying for the cost wβ in the
objective function. (See equation (10).) That is,
suppose the ILP solver assigns ‘1’ to all variables
3Examples of content-word negators are destroy, elimi-
nate, prevent etc.
</bodyText>
<figure confidence="0.97800475">
�maximize Cw+i x+i + w=i x=i
i + w−i x−i + w¬i x¬i
\
− wααi I + ··· (3)
</figure>
<page confidence="0.987676">
592
</page>
<bodyText confidence="0.999692933333334">
in φ(el), (which corresponds to keeping the orig-
inal lexicon as it is for all words in the given ex-
pression el), then the auxiliary variable βl must be
also set as ‘1’ in order to satisfy the constraint (5).
Because βl is associated with a negative weight
in the objective function, doing so will act against
maximizing the objective function. This way, we
discourage the ILP solver to preserve the original
lexicon as it is.
To verify the constraint (5) further, suppose that
the ILP solver assigns ‘1’ for all variables in φ(el)
except for one variable. (Notice that doing so cor-
responds to proposing a new polarity for one of
the words in the given expression el.) Then the
constraint (5) will hold regardless of whether the
ILP solver assigns ‘0’ or ‘1’ to βl. Because βl is
associated with a negative weight in the objective
function, the ILP solver will then assign ‘0’ to βl to
maximize the objective function. In other words,
we encourage the ILP solver to modify the original
lexicon for the given expression el.
We use this type of soft constraint in order to
cope with the following two noise factors: first, it
is possible that some annotations are noisy. Sec-
ond, f(el, L) is not perfect, and might not be able
to make a correct prediction even with the correct
word-level polarities.
Next we encode a constraint that is the oppo-
site of the previous one. That is, if the polarity
classification algorithm f(el, L) makes a correct
prediction on el using the original lexicon L, then
we encourage ILP to keep the original word-level
polarities for words in el.
constraints (4|el|) for each expression.4
To make the problem more practically solv-
able, we only consider changes to the lexicon that
are within edit-one distance with respect to π(el).
More formally, let us define π0(el) to be the set of
content words appearing in el, together with the
most likely polarity proposed by a modified polar-
ity lexicon L0. Then we need to consider all π0(el)
such that |π0(el) ∩ π(el) |= |π(el) |− 1. There are
(4−1)|el |number of different π0(el), and we index
them as π0k(el). We then add following constraints
similarly as equation (5) &amp; (6):
</bodyText>
<equation confidence="0.9739465">
� xδi &lt;= |π0 k(el) |− 1 + β(l,k) (7)
xδi ∈π&apos;(el)
</equation>
<bodyText confidence="0.9929645">
if the polarity classification algorithm f(·) makes
an incorrect prediction based on π0k(el). And,
</bodyText>
<equation confidence="0.944675">
� xδi &gt;= |π0k(el) |− |π0k(el)|β(l,k) (8)
xδi ∈π&apos;(el)
</equation>
<bodyText confidence="0.999838166666667">
if the polarity classification algorithm f(·) makes
a correct prediction based on π0k(el). Remember
that none of the constraints (5) - (8) enforces as-
signment π(el) or π0k(el) as a hard constraint. In
order to enforce at least one of them to be chosen,
we add the following constraint:
</bodyText>
<equation confidence="0.618895">
� xδi &gt;= |π(el) |− |π(el)|βl (6) � xδi &gt;= |π(el) |− 1 (9)
xδi ∈π(el) xδi ∈π(el)
</equation>
<bodyText confidence="0.993512181818182">
Interpretation of constraint (6) with the auxil-
iary binary variable βl is similar to that of con-
straint (5) elaborated above.
Notice that in equation (5), we encouraged ILP
to fix the current lexicon L for words in el, but
we have not specified the consequence of a mod-
ified lexicon (L0) in terms of expression-level po-
larity classification f(el, L0). Certain changes to
L might not fix the prediction error for el, and
those might even cause extra incorrect predictions
for other expressions. Then it would seem that we
need to replicate constraints (5) &amp; (6) for all per-
mutations of word-level polarities. However, do-
ing so would incur exponentially many number of
This constraint ensures that the modified lexi-
con L0 is not drastically different from L. Assum-
ing that the initial lexicon L is a reasonably good
one, constraining the search space for L0 will reg-
ulate that L0 does not turn into a degenerative one
that overfits to the current corpus C.
Objective function: Finally, we introduce our
full objective function.
</bodyText>
<footnote confidence="0.4890626">
4For certain simple polarity classification algorithm
f(el, G), it is possible to write polynomially many number of
constraints. However our approach intends to be more gen-
eral by treating f(el, G) as a black box, so that algorithms
that do not factor nicely can also be considered as an option.
</footnote>
<page confidence="0.954566">
593
</page>
<table confidence="0.998997166666667">
If π(el) correct pl ← 1.5
If π0k(el) correct p(l,k) ← 1.0
If π(el) very incorrect pl ← 1.0
If π(el) less incorrect pl ← 0.5
If π0k(el) very incorrect p(l,k) ← 1.5
If π0k(el) less incorrect p(l,k) ← 1.0
</table>
<tableCaption confidence="0.997583">
Table 1: The value of amplifiers pl and p(l,k).
</tableCaption>
<figure confidence="0.864694125">
maximize
i Cw+i x+i + w=i x=i
+ w−i x−i + w¬i x¬i
/− wααi
1: − wβplQl
l
1: − wβp(l,k)Q(l,k) (10)
l,k
</figure>
<bodyText confidence="0.994308">
We have already described the first part of the
objective function (equation (3)), thus we only de-
scribe the last two terms here. wβ is defined simi-
larly as wα; it is a constant weight that applies for
any auxiliary binary variable Ql and Q(l,k).
We further define pl and p(l,k) as secondary
weights, or amplifiers to adjust the constant weight
wβ. To enlighten the motivation behind the am-
plifiers pl and p(l,k), we bring out the following
observations:
</bodyText>
<listItem confidence="0.866431631578947">
1. Among the incorrect predictions for
expression-level polarity classification,
some are more incorrect than the other.
For instance, classifying positive class to
negative class is more wrong than classifying
positive class to neutral class. Therefore, the
cost of not fixing very incorrect predictions
should be higher than the cost of not fixing
less incorrect predictions. (See [R2] and
[R3] in Table 1.)
2. If the current assignment π(el) for expression
el yields a correct prediction using the classi-
fier y(el, L), then there is not much point in
changing L to L0, even if y(el, L0) also yields
a correct prediction. In this case, we would
like to assign slightly higher confidence in the
original lexicon L then the new one L0. (See
[R1] in Table 1.)
3. Likewise, if the current assignment π(el) for
</listItem>
<bodyText confidence="0.921009642857143">
expression el yields an incorrect prediction
using the classifier y(el, L), then there is not
much point in changing L to L0, if y(el, L0)
also yields an equally incorrect prediction.
Again we assign slightly higher confidence in
the original lexicon L than the new one L0 in
such cases. (Compare each row in [R2] with
a corresponding row in [R3] in Table 1.)
To summarize, for correct predictions, the de-
gree of p determines the degree of cost of (unde-
sirably) altering the current lexicon for el. For in-
correct predictions, the degree of p determines the
degree of cost of not fixing the current lexicon for
el.
</bodyText>
<sectionHeader confidence="0.999662" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999858967741936">
In the experiment section, we seek for answers for
the following questions:
Q1 What is the effect of a polarity lexicon on the
expression-level polarity classification task?
In particular, is it useful when using a ma-
chine learning technique that might be able to
learn the necessary polarity information just
based on the words in the training data, with-
out consulting a dictionary? (Section 3.1)
Q2 What is the effect of an adapted polarity lex-
icon on the expression-level polarity classifi-
cation task? (Section 3.2)
Notice that we include the neutral polarity in the
polarity classification. It makes our task much
harder (e.g., Wilson et al. (2009)) than those that
assume inputs are guaranteed to be either strongly
positive or negative (e.g., Pang et al. (2002), Choi
and Cardie (2008)). But in practice, one can-
not expect that a given input is strongly polar, as
automatically extracted opinions are bound to be
noisy. Furthermore, Wiebe et al. (2005) discuss
that some opinion expressions do carry a neutral
polarity.
We experiment with the Multi-Perspective
Question Answering (MPQA) corpus (Wiebe et
al., 2005) for evaluation. It contains 535 newswire
documents annotated with phrase-level subjectiv-
ity information. We evaluate on all opinion ex-
pressions that are known to have high level of
inter-annotator agreement. That is, we include
opinions with intensity marked as ‘medium’ or
</bodyText>
<page confidence="0.996678">
594
</page>
<bodyText confidence="0.999958821428572">
higher, and exclude those with annotation confi-
dence marked as ‘uncertain’. To focus our study
on the direct influence of the polarity lexicon upon
the sentiment classification task, we assume the
boundaries of the expressions are given. How-
ever, our approach can be readily used in tan-
dem with a system that extracts opinion expres-
sions (e.g., Kim and Hovy (2005), Breck et al.
(2007)). Performance is reported using 10-fold
cross-validation on 400 documents, and a separate
135 documents were used as a development set.
For the general-purpose polarity lexicon, we ex-
pand the polarity lexicon of Wilson et al. (2005)
with General Inquirer dictionary as suggested by
Choi and Cardie (2008).
We report the performance in two measures: ac-
curacy for 3-way classification, and average error
distance. The reason why we consider average er-
ror distance is because classifying a positive class
into a negative class is worse than classifying a
positive class into a neutral one. We define the er-
ror distance between ‘neutral’ class and any other
class as 1, while the error distance between ‘posi-
tive’ class and ‘negative’ class as 2. If a predicted
polarity is correct, then the error distance is 0. We
compute the error distance of each prediction and
take the average over all predictions in the test
data.
</bodyText>
<subsectionHeader confidence="0.696938">
3.1 Experiment-I: Effect of a Polarity
Lexicon
</subsectionHeader>
<bodyText confidence="0.980795882352941">
To verify the effect of a polarity lexicon on the
expression-level polarity classification task, we
experiment with simple classification-based ma-
chine learning technique. We use the Mallet
(McCallum, 2002) implementation of Conditional
Random Fields (CRFs) (Lafferty et al., 2001).5 To
highlight the influence of a polarity lexicon, we
compare the performance of CRFs with and with-
out features derived from polarity lexicons.
Features: We encode basic features as words
and lemmas for all content words in the given ex-
pression. The performance of CRFs using only the
basic features are given in the first row of the Ta-
ble 2. Next we encode features derived from po-
larity lexicons as follows.
• The output of Vote &amp; Flip algorithm. (Sec-
tion 3.2 &amp; Figure 2.)
</bodyText>
<footnote confidence="0.685903666666667">
5We use the CRF implementation of Mallet (McCallum,
2002) with Markov-order 0, which is equivalent to Maximum
Entropy models (Berger et al. (1996)).
</footnote>
<table confidence="0.997436666666667">
Accuracy Avg. Error Distance
Without Lexicon 63.9 0.440
With Lexicon 70.4 0.334
</table>
<tableCaption confidence="0.8323995">
Table 2: Effect of a polarity lexicon on expression-
level classification using CRFs
</tableCaption>
<listItem confidence="0.957360545454545">
• Number of positive, neutral, negative, and
negators in the given expression.
• Number of positive (or negative) words in
conjunction with number of negators.
• (boolean) Whether the number of positive
words dominates negative ones.
• (boolean) Whether the number of negative
words dominates positive ones.
• (boolean) None of the above two cases
• Each of the above three boolean values in
conjunction with the number of negators.
</listItem>
<bodyText confidence="0.998668333333333">
Results: Table 2 shows the performance of
CRFs with and without features that consult the
general-purpose lexicon. As expected, CRFs can
perform reasonably well (accuracy = 63.9%) even
without consulting the dictionary, by learning di-
rectly from the data. However, having the polarity
lexicon boosts the performance significantly (ac-
curacy = 70.4%), demonstrating that lexical re-
sources are very helpful for fine-grained sentiment
analysis. The difference in performance is statisti-
cally significant by paired t-test for both accuracy
(p &lt; 0.01) and average error distance (p &lt; 0.01).
</bodyText>
<subsectionHeader confidence="0.9628145">
3.2 Experiment-II: Adapting a Polarity
Lexicon
</subsectionHeader>
<bodyText confidence="0.999898923076923">
In this section, we assess the quality of the adapted
lexicon in the context of an expression-level polar-
ity classification task. In order to perform the lex-
icon adaptation via ILP, we need an expression-
level polarity classification algorithm f(el, L) as
described in Section 2. According to Choi and
Cardie (2008), voting algorithms that recognize
content-word negators achieve a competitive per-
formance, so we will use a variant of it for sim-
plicity. Because none of the algorithms proposed
by Choi and Cardie (2008) is designed to handle
the neutral polarity, we invent our own version as
shown in Figure 2.
</bodyText>
<page confidence="0.994755">
595
</page>
<bodyText confidence="0.5932964">
For each expression ei,
nPositive ← # of positive words in ei
nNeutral ← # of neutral words in ei
nNegative ← # of negative words in ei
nNegator ← # of negating words in ei
</bodyText>
<figure confidence="0.984285647058823">
if (nNegator % 2 = 0)
then fFlipPolarity ← false
else
then fFlipPolarity ← true
if (nPositive &gt; nNegative) &amp; ¬ fFlipPolarity
then Polarity(ei) ← positive
else if (nPositive &gt; nNegative) &amp; fFlipPolarity
then Polarity(ei) ← negative
else if (nPositive &lt; nNegative) &amp; ¬ fFlipPolarity
then Polarity(ei) ← negative
else if (nPositive &lt; nNegative) &amp; fFlipPolarity
then Polarity(ei) ← neutral
else if nNeutral &gt; 0
then Polarity(ei) ← neutral
else
then Polarity(ei) ← default polarity (the most
prominent polarity in the corpus)
</figure>
<figureCaption confidence="0.998483">
Figure 2: Vote &amp; Flip Algorithm
</figureCaption>
<bodyText confidence="0.999645">
It might look a bit complex at first glance,
but the intuition is simple. The variable
fFlipPolarity determines whether we need to
flip the overall majority polarity based on the num-
ber of negators in the given expression. If the
positive (or negative) polarity words dominate the
given expression, and if there is no need to flip
the majority polarity, then we take the positive (or
negative) polarity as the overall polarity. If the
positive (or negative) polarity words dominate the
given expression, and if we need to flip the major-
ity polarity, then we take the negative (or neutral)
polarity as the overall polarity.
Notice that the result of flipping the negative po-
larity is neutral, not positive. In our pilot study, we
found that this strategy works better than flipping
the negative polarity to positive.6 Finally, if the
number of positive words and the negative words
tie, and there is any neutral word, then we assign
the neutral polarity. In this case, we don’t worry if
</bodyText>
<subsectionHeader confidence="0.595431">
6This finding is not surprising. For instance, if we con-
</subsectionHeader>
<bodyText confidence="0.985416654545455">
sider the polarity of ”She did not get hurt much from the ac-
cident.”, it can be viewed as neutral; although it is good that
one did not hurt much, it is still bad that there was an acci-
dent. Hence it gives a mixed feeling, which corresponds to
the neutral polarity.
there is a negator, because flipping a neutral polar-
ity would still result in a neutral polarity. If none of
above condition is met, than we default to the most
prominent polarity of the data, which is the nega-
tive polarity in the MPQA corpus. We name this
simple algorithm as Vote &amp; Flip algorithm. The
performance is shown in the first row in Table 2.
Next we describe the implementation part of the
ILP. For 10 fold-cross validation, we formulate the
ILP problem using the training data (360 docu-
ments), and then test the effect of the adapted lex-
icon on the remaining 40 documents. We include
only those content words that appeared more than
3 times in the training data. From the pilot test us-
ing the development set, we picked the value of
wβ as 0.1. We found that having the auxiliary
variables αl which allow more than one polarity
per word does not necessarily help with the per-
formance, so we omitted them. We suspect it is
because the polarity classifiers we experimented
with is not highly capable of disambiguating dif-
ferent lexical usages and select the right polarity
for a given context. We use CPLEX integer pro-
gramming solver to solve our ILP problems. On a
machine with 4GHz CPU, it took several minutes
to solve each ILP problem.
In order to assess the effect of the adapted lex-
icon using CRFs, we need to first train the CRFs
model. Using the same training set used for the
lexicon adaptation would be suboptimal, because
the features generated from the adapted lexicon
will be unrealistically good in that particular data.
Therefore, we prepared a separate training data for
CRFs using 135 documents from the development
set.
Results: Table 3 shows the comparison of the
original lexicon and the adapted lexicon in terms
of polarity classification performance using the
Vote &amp; Flip algorithm. The adapted lexicon im-
proves the accuracy as well as reducing the aver-
age error distance. The difference in performance
is statistically significant by paired t-test for both
accuracy (p &lt; 0.01) and average error distance
(p &lt; 0.01).
Table 4 shows the comparison of the original
lexicon and the adapted lexicon using CRFs. The
improvement is not as substantial as that of Vote &amp;
Flip algorithm but the difference in performance is
also statistically significant for both accuracy (p =
0.03) and average error distance (p = 0.04).
</bodyText>
<page confidence="0.994452">
596
</page>
<table confidence="0.997231666666667">
Accuracy Avg. Error Distance
Original Lexicon 64.2 0.395
Adapted Lexicon 67.0 0.365
</table>
<tableCaption confidence="0.911132333333333">
Table 3: Effect of an adapted polarity lexicon on
expression-level classification using the Vote &amp;
Flip Algorithm
</tableCaption>
<table confidence="0.999489333333333">
Accuracy Avg. Error Distance
Original Lexicon 70.4 0.334
Adapted Lexicon 71.2 0.327
</table>
<tableCaption confidence="0.9906585">
Table 4: Effect of an adapted polarity lexicon on
expression-level classification using CRFs
</tableCaption>
<sectionHeader confidence="0.999879" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999954">
There are a number of previous work that focus
on building polarity lexicons (e.g., Takamura et
al. (2005), Kaji and Kitsuregawa (2007), Rao and
Ravichandran (2009)). But most of them evalu-
ated their lexicon in isolation from any potentially
relevant NLP task, and it is unclear how the new
lexicon might affect end-to-end performance of a
concrete NLP application. Our work differs in that
we try to draw a bridge between general purpose
lexical resources and a domain-specific NLP ap-
plication.
Kim and Hovy (2005) and Banea et al. (2008)
present bootstrapping methods to construct a sub-
jectivity lexicon and measure the effect of the new
lexicon for sentence-level subjectivity classifica-
tion. However, their lexicons only tell whether a
word is a subjective one, but not the polarity of the
sentiment. Furthermore, the construction of lexi-
con is still an isolated step from the classification
task. Our work on the other hand allows the classi-
fication task to directly influence the construction
of lexicon, enabling the lexicon to be adapted for
a concrete NLP application and for a specific do-
main.
Wilson et al. (2005) pioneered the expression-
level polarity classification task using the MPQA
corpus. The experimental results are not directly
comparable to ours, because Wilson et al. (2005)
limit the evaluation only for the words that ap-
peared in their polarity lexicon. Choi and Cardie
(2008) also focus on the expression-level polarity
classification, but their evaluation setting is not as
practical as ours in that they assume the inputs are
guaranteed to be either strongly positive or nega-
tive.
</bodyText>
<sectionHeader confidence="0.997698" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999880461538462">
In this paper, we present a novel lexicon adapta-
tion technique based on integer linear program-
ming to reflect the characteristics of the domain
more directly. In particular, our method collec-
tively considers the relations among words and
opinion expressions to derive the most likely po-
larity of each lexical item for the given domain.
We evaluate the effect of our lexicon adaptation
technique in the context of a concrete NLP ap-
plication: expression-level polarity classification.
The positive results from our experiments encour-
age further research for lexical resource adaptation
techniques.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982774">
This work was supported in part by National Sci-
ence Foundation Grant BCS-0624277 and by the
Department of Homeland Security under ONR
Grant N0014-07-1-0152. We also thank the
EMNLP reviewers for insightful comments.
</bodyText>
<sectionHeader confidence="0.998041" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994492">
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
ACL
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing WordNet For a Fuzzy Sentiment: Sentiment Tag
Extraction From WordNet Glosses. EACL
Carmen Banea, Rada Mihalcea, and JanyceWiebe.
2008. A Bootstrapping Method for Building Sub-
jectivity Lexicons for Languages with Scarce Re-
sources. LREC
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. In Computational Lin-
guistics, 22(1)
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes, and
Blenders: Domain Adaptation for Sentiment Classi-
fication. Association for Computational Linguistics
- ACL 2007
Eric Breck, Yejin Choi and Claire Cardie. 2007. Iden-
tifying Expressions of Opinion in Context. In IJCAI.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. EMNLP
Noam Chomsky. 1965. Aspects of the theory of syn-
tax. Cambridge, MA: MIT Press.
</reference>
<page confidence="0.97505">
597
</page>
<reference confidence="0.999881853333333">
Pascal Denis and Jason Baldridge. 2007. Joint deter-
mination of anaphoricity and coreference resolution
using integer programming. NAACL
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of 5th Con-
ference on Language Resources and Evaluation
(LREC),.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery &amp; Data Mining (KDD-2004).
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. In EMNLP-
CoNLL.
Hiroshi Kanayama Tetsuya Nasukawa. 2006. Fully
Automatic Lexicon Expansion for Domain-oriented
Sentiment Analysis. In ACL.
Alistair Kennedy and Diana Inkpen. 2005. Sentiment
Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Ne-
gotiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COL-
ING.
Soo-Min Kim and Eduard Hovy. 2005. Automatic De-
tection of Opinion Bearing Words and Sentences. In
Companion Volume to the Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05)
John Lafferty, Andrew Kachites McCallum and Fer-
nando Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In EMNLP.
Joseph Pickett et al. 1996. The American heritage
book of English usage: A practical and authoritative
guide to contemporary English. Houghton Mifflin
Company.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Exploring Attitude and
Affect in Text: Theories and Applications: Papers
from the 2004 Spring Symposium, AAAI.
Delip Rao and Deepak Ravichandran. 2009. Semi-
Supervised Polarity Lexicon Induction. In EACL.
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In CoNLL.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL.
Janyce Wiebe, Theresa Wilson and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. In Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2-3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of
HLT/EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an explo-
ration of features for phrase-level sentiment analy-
sis. In Computational Linguistics 35(3).
</reference>
<page confidence="0.99704">
598
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799175">
<title confidence="0.9996025">Adapting a Polarity Lexicon using Integer Linear for Domain-Specific Sentiment Classification</title>
<author confidence="0.995706">Yejin Choi</author>
<author confidence="0.995706">Claire</author>
<affiliation confidence="0.959403">Department of Computer Cornell</affiliation>
<address confidence="0.977688">Ithaca, NY</address>
<abstract confidence="0.994927380952381">Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. There are a number of such lexical resources available, but it is often suboptimal to use them as is, because general purpose lexical resources do not reflect domain-specific lexical usage. In this paper, we propose a novel method based on integer linear programming that can adapt an existing lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most polarity of each lexical item neutral, or for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging.</title>
<date>2008</date>
<publisher>ACL</publisher>
<contexts>
<context position="2530" citStr="Andreevskaia and Bergler (2008)" startWordPosition="379" endWordPosition="382"> Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and Bergler (2008)). In this paper, we propose a novel method based on integer linear programming to adapt an existing polarity lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method considers the relations among words and opinion expressions collectively to derive the most likely polarity of each word for the given domain. Figure 1 depicts the key insight of our approach using a bipartite graph. On the left hand side, each node represents a word, and on the right hand side, each node represents an opinion expression. There is an edge between a word wi and an </context>
</contexts>
<marker>Andreevskaia, Bergler, 2008</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2008. When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging. ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>Mining WordNet For a Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses.</title>
<date>2006</date>
<publisher>EACL</publisher>
<contexts>
<context position="1888" citStr="Andreevskaia and Bergler (2006)" startWordPosition="281" endWordPosition="284"> Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the dom</context>
</contexts>
<marker>Andreevskaia, Bergler, 2006</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2006. Mining WordNet For a Fuzzy Sentiment: Sentiment Tag Extraction From WordNet Glosses. EACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>JanyceWiebe</author>
</authors>
<title>A Bootstrapping Method for Building Subjectivity Lexicons for Languages with Scarce Resources.</title>
<date>2008</date>
<publisher>LREC</publisher>
<contexts>
<context position="29326" citStr="Banea et al. (2008)" startWordPosition="4912" endWordPosition="4915">d polarity lexicon on expression-level classification using CRFs 4 Related Work There are a number of previous work that focus on building polarity lexicons (e.g., Takamura et al. (2005), Kaji and Kitsuregawa (2007), Rao and Ravichandran (2009)). But most of them evaluated their lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. Our work differs in that we try to draw a bridge between general purpose lexical resources and a domain-specific NLP application. Kim and Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, their lexicons only tell whether a word is a subjective one, but not the polarity of the sentiment. Furthermore, the construction of lexicon is still an isolated step from the classification task. Our work on the other hand allows the classification task to directly influence the construction of lexicon, enabling the lexicon to be adapted for a concrete NLP application and for a specific domain. Wilson et al. (2005) pioneered the </context>
</contexts>
<marker>Banea, Mihalcea, JanyceWiebe, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, and JanyceWiebe. 2008. A Bootstrapping Method for Building Subjectivity Lexicons for Languages with Scarce Resources. LREC</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>In Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="22168" citStr="Berger et al. (1996)" startWordPosition="3713" endWordPosition="3716">To highlight the influence of a polarity lexicon, we compare the performance of CRFs with and without features derived from polarity lexicons. Features: We encode basic features as words and lemmas for all content words in the given expression. The performance of CRFs using only the basic features are given in the first row of the Table 2. Next we encode features derived from polarity lexicons as follows. • The output of Vote &amp; Flip algorithm. (Section 3.2 &amp; Figure 2.) 5We use the CRF implementation of Mallet (McCallum, 2002) with Markov-order 0, which is equivalent to Maximum Entropy models (Berger et al. (1996)). Accuracy Avg. Error Distance Without Lexicon 63.9 0.440 With Lexicon 70.4 0.334 Table 2: Effect of a polarity lexicon on expressionlevel classification using CRFs • Number of positive, neutral, negative, and negators in the given expression. • Number of positive (or negative) words in conjunction with number of negators. • (boolean) Whether the number of positive words dominates negative ones. • (boolean) Whether the number of negative words dominates positive ones. • (boolean) None of the above two cases • Each of the above three boolean values in conjunction with the number of negators. R</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. In Computational Linguistics, 22(1)</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes, and Blenders: Domain Adaptation for Sentiment Classification. Association for Computational Linguistics - ACL</title>
<date>2007</date>
<contexts>
<context position="2394" citStr="Blitzer et al. (2007)" startWordPosition="357" endWordPosition="360">s on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and Bergler (2008)). In this paper, we propose a novel method based on integer linear programming to adapt an existing polarity lexicon into a new one to reflect the characteristics of the data more directly. In particular, our method considers the relations among words and opinion expressions collectively to derive the most likely polarity of each word for the given domain. Figure 1 depicts the key insight of our approach using a bipartite graph. On the left hand side, each no</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes, and Blenders: Domain Adaptation for Sentiment Classification. Association for Computational Linguistics - ACL 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying Expressions of Opinion in Context.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="20298" citStr="Breck et al. (2007)" startWordPosition="3406" endWordPosition="3409">uments annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with intensity marked as ‘medium’ or 594 higher, and exclude those with annotation confidence marked as ‘uncertain’. To focus our study on the direct influence of the polarity lexicon upon the sentiment classification task, we assume the boundaries of the expressions are given. However, our approach can be readily used in tandem with a system that extracts opinion expressions (e.g., Kim and Hovy (2005), Breck et al. (2007)). Performance is reported using 10-fold cross-validation on 400 documents, and a separate 135 documents were used as a development set. For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al. (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). We report the performance in two measures: accuracy for 3-way classification, and average error distance. The reason why we consider average error distance is because classifying a positive class into a negative class is worse than classifying a positive class into a neutral one. We define th</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi and Claire Cardie. 2007. Identifying Expressions of Opinion in Context. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis.</title>
<date>2008</date>
<publisher>EMNLP</publisher>
<contexts>
<context position="3690" citStr="Choi and Cardie (2008)" startWordPosition="577" endWordPosition="580">opinion expression. There is an edge between a word wi and an opinion expression ej, if the word wi appears in the expression ej. We assume the possible polarity of each expression is one of the following three values: {positive, neutral, negative}, while the possible polarity of each word is one of: {positive, neutral, negative or negator}. Strictly speaking, negator is not a value for polarity, but we include them in our lexicon, because valence shifters or negators have been shown to play an important role for sentiment analysis (e.g., Polanyi and Zaenen (2004), Moilanen and Pulman (2007), Choi and Cardie (2008)). Typically, the ultimate goal of the sentiment analysis task is to determine the expression-level (or sentiment/ document-level) polarities, rather 590 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP than the correct word-level polarities with respect to the domain. Therefore, word-level polarities can be considered as latent information. In this paper, we show how we can improve the word-level polarities of a general-purpose polarity lexicon by utilizing the expression-level polarities, a</context>
<context position="10282" citStr="Choi and Cardie, 2008" startWordPosition="1677" endWordPosition="1680">ights Lwδi based on the prior polarity from the lexicon can be considered as having a ”competence” component , while Cwδi derived from the corpus counts can be considered as a ”performance” component (Noam Chomsky (1965)). 2If a word xi is in an expression that is not an opinion, then we count it as an occurrence with neutral polarity. Constraints for content-word negators: Next we describe a constraint that exploits knowledge of the typical distribution of content-word negators in natural language. Content-word negators are words that are not function words, but act semantically as negators (Choi and Cardie, 2008).3 Although it is possible to artificially construct a very convoluted sentence with lots of negations, it is unlikely for multiple layers of negations to appear very often in natural language (Pickett et al. (1996)). Therefore, we allow at most one contentword negator for each expression el. Because we do not restrict the number of function-word negators, our constraint still gives room for multiple layers of negations. � x¬i &lt;= 1 (4) i∈µ(el) In the above constraint, µ(el) indicates the set of indices of content words appearing in el. For instance, if i E µ(el), then xi appears in el. This co</context>
<context position="19305" citStr="Choi and Cardie (2008)" startWordPosition="3248" endWordPosition="3251">ask? In particular, is it useful when using a machine learning technique that might be able to learn the necessary polarity information just based on the words in the training data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with intensity marked as ‘medium’</context>
<context position="20603" citStr="Choi and Cardie (2008)" startWordPosition="3453" endWordPosition="3456">rtain’. To focus our study on the direct influence of the polarity lexicon upon the sentiment classification task, we assume the boundaries of the expressions are given. However, our approach can be readily used in tandem with a system that extracts opinion expressions (e.g., Kim and Hovy (2005), Breck et al. (2007)). Performance is reported using 10-fold cross-validation on 400 documents, and a separate 135 documents were used as a development set. For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al. (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). We report the performance in two measures: accuracy for 3-way classification, and average error distance. The reason why we consider average error distance is because classifying a positive class into a negative class is worse than classifying a positive class into a neutral one. We define the error distance between ‘neutral’ class and any other class as 1, while the error distance between ‘positive’ class and ‘negative’ class as 2. If a predicted polarity is correct, then the error distance is 0. We compute the error distance of each prediction and take the average over all predictions in t</context>
<context position="23713" citStr="Choi and Cardie (2008)" startWordPosition="3954" endWordPosition="3957">antly (accuracy = 70.4%), demonstrating that lexical resources are very helpful for fine-grained sentiment analysis. The difference in performance is statistically significant by paired t-test for both accuracy (p &lt; 0.01) and average error distance (p &lt; 0.01). 3.2 Experiment-II: Adapting a Polarity Lexicon In this section, we assess the quality of the adapted lexicon in the context of an expression-level polarity classification task. In order to perform the lexicon adaptation via ILP, we need an expressionlevel polarity classification algorithm f(el, L) as described in Section 2. According to Choi and Cardie (2008), voting algorithms that recognize content-word negators achieve a competitive performance, so we will use a variant of it for simplicity. Because none of the algorithms proposed by Choi and Cardie (2008) is designed to handle the neutral polarity, we invent our own version as shown in Figure 2. 595 For each expression ei, nPositive ← # of positive words in ei nNeutral ← # of neutral words in ei nNegative ← # of negative words in ei nNegator ← # of negating words in ei if (nNegator % 2 = 0) then fFlipPolarity ← false else then fFlipPolarity ← true if (nPositive &gt; nNegative) &amp; ¬ fFlipPolarity t</context>
<context position="30188" citStr="Choi and Cardie (2008)" startWordPosition="5050" endWordPosition="5053">olarity of the sentiment. Furthermore, the construction of lexicon is still an isolated step from the classification task. Our work on the other hand allows the classification task to directly influence the construction of lexicon, enabling the lexicon to be adapted for a concrete NLP application and for a specific domain. Wilson et al. (2005) pioneered the expressionlevel polarity classification task using the MPQA corpus. The experimental results are not directly comparable to ours, because Wilson et al. (2005) limit the evaluation only for the words that appeared in their polarity lexicon. Choi and Cardie (2008) also focus on the expression-level polarity classification, but their evaluation setting is not as practical as ours in that they assume the inputs are guaranteed to be either strongly positive or negative. 5 Conclusion In this paper, we present a novel lexicon adaptation technique based on integer linear programming to reflect the characteristics of the domain more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item for the given domain. We evaluate the effect of our lexicon adapt</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis. EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Aspects of the theory of syntax.</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="9880" citStr="Chomsky (1965)" startWordPosition="1614" endWordPosition="1615">ty of an expression is more than just a linear sum of the word-level polarities (e.g., Moilanen and Pulman (2007)). Nonetheless, not all expressions require a complicated inference procedure to determine their polarity. Therefore, Cwδi still provides useful information about the likely polarity of each word based on the corpus statistics. From the perspective of Chomskyan linguistics, the weights Lwδi based on the prior polarity from the lexicon can be considered as having a ”competence” component , while Cwδi derived from the corpus counts can be considered as a ”performance” component (Noam Chomsky (1965)). 2If a word xi is in an expression that is not an opinion, then we count it as an occurrence with neutral polarity. Constraints for content-word negators: Next we describe a constraint that exploits knowledge of the typical distribution of content-word negators in natural language. Content-word negators are words that are not function words, but act semantically as negators (Choi and Cardie, 2008).3 Although it is possible to artificially construct a very convoluted sentence with lots of negations, it is unlikely for multiple layers of negations to appear very often in natural language (Pick</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Noam Chomsky. 1965. Aspects of the theory of syntax. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<publisher>NAACL</publisher>
<contexts>
<context position="5863" citStr="Denis and Baldridge (2007)" startWordPosition="905" endWordPosition="908">-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately, one class of constraint that we would like to encode (see Section 2) will require an exponentially many number of constraints when grounded into an actual ILP problem. We therefore propose an approximation scheme to make the problem more practically solvable. We evaluate the effect of the adapted lex1In case of document-level polarity classification, wordto-expression relations correspond to word-to-document relations. Figure 1: The relations among words a</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. NAACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of 5th Conference on Language Resources and Evaluation (LREC),.</booktitle>
<contexts>
<context position="1917" citStr="Esuli and Sebastiani (2006)" startWordPosition="285" endWordPosition="288"> Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and B</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings of 5th Conference on Language Resources and Evaluation (LREC),.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004).</booktitle>
<contexts>
<context position="1614" citStr="Hu and Liu (2004)" startWordPosition="242" endWordPosition="245">s the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose le</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents.</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL.</booktitle>
<contexts>
<context position="28922" citStr="Kaji and Kitsuregawa (2007)" startWordPosition="4844" endWordPosition="4847">lso statistically significant for both accuracy (p = 0.03) and average error distance (p = 0.04). 596 Accuracy Avg. Error Distance Original Lexicon 64.2 0.395 Adapted Lexicon 67.0 0.365 Table 3: Effect of an adapted polarity lexicon on expression-level classification using the Vote &amp; Flip Algorithm Accuracy Avg. Error Distance Original Lexicon 70.4 0.334 Adapted Lexicon 71.2 0.327 Table 4: Effect of an adapted polarity lexicon on expression-level classification using CRFs 4 Related Work There are a number of previous work that focus on building polarity lexicons (e.g., Takamura et al. (2005), Kaji and Kitsuregawa (2007), Rao and Ravichandran (2009)). But most of them evaluated their lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. Our work differs in that we try to draw a bridge between general purpose lexical resources and a domain-specific NLP application. Kim and Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, their lexicons only tell wheth</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building Lexicon for Sentiment Analysis from Massive Collection of HTML Documents. In EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama Tetsuya Nasukawa</author>
</authors>
<title>Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="5401" citStr="Nasukawa (2006)" startWordPosition="833" endWordPosition="834">ression. The second type of relations are word-to-expression relations: e.g., some words appear in expressions that take on a variety of polarities, while other words are associated with expressions of one polarity class or another. In relation to previous research, analyzing word-to-word (intra-expression) relations is most related to techniques that determine expression-level polarity in context (e.g., Wilson et al. (2005)), while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately</context>
</contexts>
<marker>Nasukawa, 2006</marker>
<rawString>Hiroshi Kanayama Tetsuya Nasukawa. 2006. Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters.</title>
<date>2005</date>
<booktitle>In Proceedings of FINEXIN 2005, Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations.</booktitle>
<contexts>
<context position="1304" citStr="Kennedy and Inkpen (2005)" startWordPosition="193" endWordPosition="196"> of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Seba</context>
</contexts>
<marker>Kennedy, Inkpen, 2005</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2005. Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters. In Proceedings of FINEXIN 2005, Workshop on the Analysis of Informal and Formal Information Exchange during Negotiations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1277" citStr="Kim and Hovy (2004)" startWordPosition="189" endWordPosition="192">t the characteristics of the data more directly. In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Ber</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic Detection of Opinion Bearing Words and Sentences.</title>
<date>2005</date>
<booktitle>In Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP-05)</booktitle>
<contexts>
<context position="20277" citStr="Kim and Hovy (2005)" startWordPosition="3402" endWordPosition="3405">ains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with intensity marked as ‘medium’ or 594 higher, and exclude those with annotation confidence marked as ‘uncertain’. To focus our study on the direct influence of the polarity lexicon upon the sentiment classification task, we assume the boundaries of the expressions are given. However, our approach can be readily used in tandem with a system that extracts opinion expressions (e.g., Kim and Hovy (2005), Breck et al. (2007)). Performance is reported using 10-fold cross-validation on 400 documents, and a separate 135 documents were used as a development set. For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al. (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). We report the performance in two measures: accuracy for 3-way classification, and average error distance. The reason why we consider average error distance is because classifying a positive class into a negative class is worse than classifying a positive class into a neut</context>
<context position="29302" citStr="Kim and Hovy (2005)" startWordPosition="4907" endWordPosition="4910">e 4: Effect of an adapted polarity lexicon on expression-level classification using CRFs 4 Related Work There are a number of previous work that focus on building polarity lexicons (e.g., Takamura et al. (2005), Kaji and Kitsuregawa (2007), Rao and Ravichandran (2009)). But most of them evaluated their lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. Our work differs in that we try to draw a bridge between general purpose lexical resources and a domain-specific NLP application. Kim and Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, their lexicons only tell whether a word is a subjective one, but not the polarity of the sentiment. Furthermore, the construction of lexicon is still an isolated step from the classification task. Our work on the other hand allows the classification task to directly influence the construction of lexicon, enabling the lexicon to be adapted for a concrete NLP application and for a specific domain. Wilson et a</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Automatic Detection of Opinion Bearing Words and Sentences. In Companion Volume to the Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP-05)</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew Kachites McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="21545" citStr="Lafferty et al., 2001" startWordPosition="3604" endWordPosition="3607">‘neutral’ class and any other class as 1, while the error distance between ‘positive’ class and ‘negative’ class as 2. If a predicted polarity is correct, then the error distance is 0. We compute the error distance of each prediction and take the average over all predictions in the test data. 3.1 Experiment-I: Effect of a Polarity Lexicon To verify the effect of a polarity lexicon on the expression-level polarity classification task, we experiment with simple classification-based machine learning technique. We use the Mallet (McCallum, 2002) implementation of Conditional Random Fields (CRFs) (Lafferty et al., 2001).5 To highlight the influence of a polarity lexicon, we compare the performance of CRFs with and without features derived from polarity lexicons. Features: We encode basic features as words and lemmas for all content words in the given expression. The performance of CRFs using only the basic features are given in the first row of the Table 2. Next we encode features derived from polarity lexicons as follows. • The output of Vote &amp; Flip algorithm. (Section 3.2 &amp; Figure 2.) 5We use the CRF implementation of Mallet (McCallum, 2002) with Markov-order 0, which is equivalent to Maximum Entropy model</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew Kachites McCallum and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="21470" citStr="McCallum, 2002" startWordPosition="3596" endWordPosition="3597">tive class into a neutral one. We define the error distance between ‘neutral’ class and any other class as 1, while the error distance between ‘positive’ class and ‘negative’ class as 2. If a predicted polarity is correct, then the error distance is 0. We compute the error distance of each prediction and take the average over all predictions in the test data. 3.1 Experiment-I: Effect of a Polarity Lexicon To verify the effect of a polarity lexicon on the expression-level polarity classification task, we experiment with simple classification-based machine learning technique. We use the Mallet (McCallum, 2002) implementation of Conditional Random Fields (CRFs) (Lafferty et al., 2001).5 To highlight the influence of a polarity lexicon, we compare the performance of CRFs with and without features derived from polarity lexicons. Features: We encode basic features as words and lemmas for all content words in the given expression. The performance of CRFs using only the basic features are given in the first row of the Table 2. Next we encode features derived from polarity lexicons as follows. • The output of Vote &amp; Flip algorithm. (Section 3.2 &amp; Figure 2.) 5We use the CRF implementation of Mallet (McCall</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment Composition.</title>
<date>2007</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP</booktitle>
<contexts>
<context position="1642" citStr="Moilanen and Pulman (2007)" startWordPosition="246" endWordPosition="249">f fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be e</context>
<context position="3666" citStr="Moilanen and Pulman (2007)" startWordPosition="572" endWordPosition="576">de, each node represents an opinion expression. There is an edge between a word wi and an opinion expression ej, if the word wi appears in the expression ej. We assume the possible polarity of each expression is one of the following three values: {positive, neutral, negative}, while the possible polarity of each word is one of: {positive, neutral, negative or negator}. Strictly speaking, negator is not a value for polarity, but we include them in our lexicon, because valence shifters or negators have been shown to play an important role for sentiment analysis (e.g., Polanyi and Zaenen (2004), Moilanen and Pulman (2007), Choi and Cardie (2008)). Typically, the ultimate goal of the sentiment analysis task is to determine the expression-level (or sentiment/ document-level) polarities, rather 590 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP than the correct word-level polarities with respect to the domain. Therefore, word-level polarities can be considered as latent information. In this paper, we show how we can improve the word-level polarities of a general-purpose polarity lexicon by utilizing the expres</context>
<context position="9379" citStr="Moilanen and Pulman (2007)" startWordPosition="1536" endWordPosition="1539"> using two different types of information as follows: wδi :=Lwδi + Cwδi where Lwδi is the degree of polarity 6 for word xi determined by the general-purpose polarity lexicon L, and Cwδi is the degree of polarity 6 determined by the corpus statistics as follows:2 C δ # of xi in expressions with polarity 6 wi := # of xi in the corpus C Note that the occurrence of word xi in an expression ej with a polarity 6 does not necessarily mean that the polarity of xi should also be 6, as the interpretation of the polarity of an expression is more than just a linear sum of the word-level polarities (e.g., Moilanen and Pulman (2007)). Nonetheless, not all expressions require a complicated inference procedure to determine their polarity. Therefore, Cwδi still provides useful information about the likely polarity of each word based on the corpus statistics. From the perspective of Chomskyan linguistics, the weights Lwδi based on the prior polarity from the lexicon can be considered as having a ”competence” component , while Cwδi derived from the corpus counts can be considered as a ”performance” component (Noam Chomsky (1965)). 2If a word xi is in an expression that is not an opinion, then we count it as an occurrence with</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment Composition. In Proceedings of Recent Advances in Natural Language Processing (RANLP 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="19281" citStr="Pang et al. (2002)" startWordPosition="3244" endWordPosition="3247">ity classification task? In particular, is it useful when using a machine learning technique that might be able to learn the necessary polarity information just based on the words in the training data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with inte</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Pickett</author>
</authors>
<title>The American heritage book of English usage: A practical and authoritative guide to contemporary English.</title>
<date>1996</date>
<publisher>Houghton Mifflin Company.</publisher>
<marker>Pickett, 1996</marker>
<rawString>Joseph Pickett et al. 1996. The American heritage book of English usage: A practical and authoritative guide to contemporary English. Houghton Mifflin Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual lexical valence shifters.</title>
<date>2004</date>
<booktitle>In Exploring Attitude and Affect in Text: Theories and Applications: Papers from the 2004 Spring Symposium, AAAI.</booktitle>
<contexts>
<context position="3638" citStr="Polanyi and Zaenen (2004)" startWordPosition="568" endWordPosition="571">d, and on the right hand side, each node represents an opinion expression. There is an edge between a word wi and an opinion expression ej, if the word wi appears in the expression ej. We assume the possible polarity of each expression is one of the following three values: {positive, neutral, negative}, while the possible polarity of each word is one of: {positive, neutral, negative or negator}. Strictly speaking, negator is not a value for polarity, but we include them in our lexicon, because valence shifters or negators have been shown to play an important role for sentiment analysis (e.g., Polanyi and Zaenen (2004), Moilanen and Pulman (2007), Choi and Cardie (2008)). Typically, the ultimate goal of the sentiment analysis task is to determine the expression-level (or sentiment/ document-level) polarities, rather 590 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP than the correct word-level polarities with respect to the domain. Therefore, word-level polarities can be considered as latent information. In this paper, we show how we can improve the word-level polarities of a general-purpose polarity lex</context>
</contexts>
<marker>Polanyi, Zaenen, 2004</marker>
<rawString>Livia Polanyi and Annie Zaenen. 2004. Contextual lexical valence shifters. In Exploring Attitude and Affect in Text: Theories and Applications: Papers from the 2004 Spring Symposium, AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Deepak Ravichandran</author>
</authors>
<title>SemiSupervised Polarity Lexicon Induction.</title>
<date>2009</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="1946" citStr="Rao and Ravichandran (2009)" startWordPosition="289" endWordPosition="292"> (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposite sentiment depending on the domain (e.g., Andreevskaia and Bergler (2008)). In this paper</context>
<context position="28951" citStr="Rao and Ravichandran (2009)" startWordPosition="4848" endWordPosition="4851"> for both accuracy (p = 0.03) and average error distance (p = 0.04). 596 Accuracy Avg. Error Distance Original Lexicon 64.2 0.395 Adapted Lexicon 67.0 0.365 Table 3: Effect of an adapted polarity lexicon on expression-level classification using the Vote &amp; Flip Algorithm Accuracy Avg. Error Distance Original Lexicon 70.4 0.334 Adapted Lexicon 71.2 0.327 Table 4: Effect of an adapted polarity lexicon on expression-level classification using CRFs 4 Related Work There are a number of previous work that focus on building polarity lexicons (e.g., Takamura et al. (2005), Kaji and Kitsuregawa (2007), Rao and Ravichandran (2009)). But most of them evaluated their lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. Our work differs in that we try to draw a bridge between general purpose lexical resources and a domain-specific NLP application. Kim and Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, their lexicons only tell whether a word is a subjective one</context>
</contexts>
<marker>Rao, Ravichandran, 2009</marker>
<rawString>Delip Rao and Deepak Ravichandran. 2009. SemiSupervised Polarity Lexicon Induction. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A Linear Programming Formulation for Global Inference in Natural Language Tasks. In CoNLL.</title>
<date>2004</date>
<contexts>
<context position="5835" citStr="Roth and Yih (2004)" startWordPosition="901" endWordPosition="904"> while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). And the word-to-word and word-to-expression relations discussed above can be encoded as soft and hard constraints in ILP. Unfortunately, one class of constraint that we would like to encode (see Section 2) will require an exponentially many number of constraints when grounded into an actual ILP problem. We therefore propose an approximation scheme to make the problem more practically solvable. We evaluate the effect of the adapted lex1In case of document-level polarity classification, wordto-expression relations correspond to word-to-document relations. Figure 1:</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A Linear Programming Formulation for Global Inference in Natural Language Tasks. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1855" citStr="Takamura et al. (2005)" startWordPosition="277" endWordPosition="280">entiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and Ravichandran (2009)) generally evaluate the lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. It might even be unrealistic to expect that there can be a general-purpose lexical resource that can be effective across all relevant NLP applications, as general-purpose lexicons will not reflect domain-specific lexical usage. Indeed, Blitzer et al. (2007) note that the polarity of a particular word can carry opposi</context>
<context position="28893" citStr="Takamura et al. (2005)" startWordPosition="4840" endWordPosition="4843">ence in performance is also statistically significant for both accuracy (p = 0.03) and average error distance (p = 0.04). 596 Accuracy Avg. Error Distance Original Lexicon 64.2 0.395 Adapted Lexicon 67.0 0.365 Table 3: Effect of an adapted polarity lexicon on expression-level classification using the Vote &amp; Flip Algorithm Accuracy Avg. Error Distance Original Lexicon 70.4 0.334 Adapted Lexicon 71.2 0.327 Table 4: Effect of an adapted polarity lexicon on expression-level classification using CRFs 4 Related Work There are a number of previous work that focus on building polarity lexicons (e.g., Takamura et al. (2005), Kaji and Kitsuregawa (2007), Rao and Ravichandran (2009)). But most of them evaluated their lexicon in isolation from any potentially relevant NLP task, and it is unclear how the new lexicon might affect end-to-end performance of a concrete NLP application. Our work differs in that we try to draw a bridge between general purpose lexical resources and a domain-specific NLP application. Kim and Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, t</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<booktitle>In Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<pages>39--2</pages>
<contexts>
<context position="19472" citStr="Wiebe et al. (2005)" startWordPosition="3276" endWordPosition="3279">ining data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion expressions that are known to have high level of inter-annotator agreement. That is, we include opinions with intensity marked as ‘medium’ or 594 higher, and exclude those with annotation confidence marked as ‘uncertain’. To focus our study on the direct influence of the polarity lexicon upon the sentime</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. In Language Resources and Evaluation (formerly Computers and the Humanities), 39(2-3):165210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP.</booktitle>
<contexts>
<context position="1326" citStr="Wilson et al. (2005)" startWordPosition="197" endWordPosition="200"> In particular, our method collectively considers the relations among words and opinion expressions to derive the most likely polarity of each lexical item (positive, neutral, negative, or negator) for the given domain. Experimental results show that our lexicon adaptation technique improves the performance of fine-grained polarity classification. 1 Introduction Polarity lexicons have been a valuable resource for sentiment analysis and opinion mining. In particular, they have been an essential ingredient for finegrained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al. (2005)). Even though the polarity lexicon plays an important role (Section 3.1), it has received relatively less attention in previous research. In most cases, polarity lexicon construction is discussed only briefly as a preprocessing step for a sentiment analysis task (e.g., Hu and Liu (2004), Moilanen and Pulman (2007)), but the effect of different alternative polarity lexicons is not explicitly investigated. Conversely, research efforts that focus on constructing a general purpose polarity lexicon (e.g., Takamura et al. (2005), Andreevskaia and Bergler (2006), Esuli and Sebastiani (2006), Rao and</context>
<context position="5214" citStr="Wilson et al. (2005)" startWordPosition="808" endWordPosition="811">s, if we are not sure about the polarity of a certain word, we can still make a guess based on the polarities of other words within the same expression and knowledge of the polarity of the expression. The second type of relations are word-to-expression relations: e.g., some words appear in expressions that take on a variety of polarities, while other words are associated with expressions of one polarity class or another. In relation to previous research, analyzing word-to-word (intra-expression) relations is most related to techniques that determine expression-level polarity in context (e.g., Wilson et al. (2005)), while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)).1 While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to be very effective when solving problems with complex constraints (e.g.</context>
<context position="20531" citStr="Wilson et al. (2005)" startWordPosition="3442" endWordPosition="3445">4 higher, and exclude those with annotation confidence marked as ‘uncertain’. To focus our study on the direct influence of the polarity lexicon upon the sentiment classification task, we assume the boundaries of the expressions are given. However, our approach can be readily used in tandem with a system that extracts opinion expressions (e.g., Kim and Hovy (2005), Breck et al. (2007)). Performance is reported using 10-fold cross-validation on 400 documents, and a separate 135 documents were used as a development set. For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al. (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). We report the performance in two measures: accuracy for 3-way classification, and average error distance. The reason why we consider average error distance is because classifying a positive class into a negative class is worse than classifying a positive class into a neutral one. We define the error distance between ‘neutral’ class and any other class as 1, while the error distance between ‘positive’ class and ‘negative’ class as 2. If a predicted polarity is correct, then the error distance is 0. We compute the error di</context>
<context position="29911" citStr="Wilson et al. (2005)" startWordPosition="5007" endWordPosition="5010">Hovy (2005) and Banea et al. (2008) present bootstrapping methods to construct a subjectivity lexicon and measure the effect of the new lexicon for sentence-level subjectivity classification. However, their lexicons only tell whether a word is a subjective one, but not the polarity of the sentiment. Furthermore, the construction of lexicon is still an isolated step from the classification task. Our work on the other hand allows the classification task to directly influence the construction of lexicon, enabling the lexicon to be adapted for a concrete NLP application and for a specific domain. Wilson et al. (2005) pioneered the expressionlevel polarity classification task using the MPQA corpus. The experimental results are not directly comparable to ours, because Wilson et al. (2005) limit the evaluation only for the words that appeared in their polarity lexicon. Choi and Cardie (2008) also focus on the expression-level polarity classification, but their evaluation setting is not as practical as ours in that they assume the inputs are guaranteed to be either strongly positive or negative. 5 Conclusion In this paper, we present a novel lexicon adaptation technique based on integer linear programming to </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<booktitle>In Computational Linguistics 35(3).</booktitle>
<contexts>
<context position="19166" citStr="Wilson et al. (2009)" startWordPosition="3225" endWordPosition="3228">ek for answers for the following questions: Q1 What is the effect of a polarity lexicon on the expression-level polarity classification task? In particular, is it useful when using a machine learning technique that might be able to learn the necessary polarity information just based on the words in the training data, without consulting a dictionary? (Section 3.1) Q2 What is the effect of an adapted polarity lexicon on the expression-level polarity classification task? (Section 3.2) Notice that we include the neutral polarity in the polarity classification. It makes our task much harder (e.g., Wilson et al. (2009)) than those that assume inputs are guaranteed to be either strongly positive or negative (e.g., Pang et al. (2002), Choi and Cardie (2008)). But in practice, one cannot expect that a given input is strongly polar, as automatically extracted opinions are bound to be noisy. Furthermore, Wiebe et al. (2005) discuss that some opinion expressions do carry a neutral polarity. We experiment with the Multi-Perspective Question Answering (MPQA) corpus (Wiebe et al., 2005) for evaluation. It contains 535 newswire documents annotated with phrase-level subjectivity information. We evaluate on all opinion</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis. In Computational Linguistics 35(3).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>