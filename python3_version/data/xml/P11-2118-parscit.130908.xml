<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002707">
<title confidence="0.996831">
A Hierarchical Model of Web Summaries
</title>
<author confidence="0.99663">
Yves Petinot and Kathleen McKeown and Kapil Thadani
</author>
<affiliation confidence="0.9966115">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.996053">
New York, NY 10027
</address>
<email confidence="0.999764">
{ypetinot|kathy|kapil}@cs.columbia.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942285714286">
We investigate the relevance of hierarchical
topic models to represent the content of Web
gists. We focus our attention on DMOZ,
a popular Web directory, and propose two
algorithms to infer such a model from its
manually-curated hierarchy of categories. Our
first approach, based on information-theoretic
grounds, uses an algorithm similar to recur-
sive feature selection. Our second approach
is fully Bayesian and derived from the more
general model, hierarchical LDA. We evalu-
ate the performance of both models against a
flat 1-gram baseline and show improvements
in terms of perplexity over held-out data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968">
The work presented in this paper is aimed at lever-
aging a manually created document ontology to
model the content of an underlying document col-
lection. While the primary usage of ontologies is
as a means of organizing and navigating document
collections, they can also help in inferring a signif-
icant amount of information about the documents
attached to them, including path-level, statistical,
representations of content, and fine-grained views
on the level of specificity of the language used in
those documents. Our study focuses on the ontology
underlying DMOZ1, a popular Web directory. We
propose two methods for crystalizing a hierarchical
topic model against its hierarchy and show that the
resulting models outperform a flat unigram model in
its predictive power over held-out data.
</bodyText>
<footnote confidence="0.977175">
1http://www.dmoz.org
</footnote>
<bodyText confidence="0.9995388125">
To construct our hierarchical topic models, we
adopt the mixed membership formalism (Hofmann,
1999; Blei et al., 2010), where a document is rep-
resented as a mixture over a set of word multi-
nomials. We consider the document hierarchy H
(e.g. the DMOZ hierarchy) as a tree where internal
nodes (category nodes) and leaf nodes (documents),
as well as the edges connecting them, are known a
priori. Each node Ni in H is mapped to a multi-
nomial word distribution MultN,, and each path cd
to a leaf node D is associated with a mixture over
the multinonials (MultC0 ... MultCk, MultD) ap-
pearing along this path. The mixture components
are combined using a mixing proportion vector
(BC0 ... BCk), so that the likelihood of string w be-
ing produced by path cd is:
</bodyText>
<equation confidence="0.98311225">
Bjp(wi|cdj) (1)
where:
� ICdl Bj = 1, bd (2)
j=0
</equation>
<bodyText confidence="0.999947333333333">
In the following, we propose two models that fit
in this framework. We describe how they allow the
derivation of both p(wi|cd�j) and B and present early
experimental results showing that explicit hierarchi-
cal information of content can indeed be used as a
basis for content modeling purposes.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9982445">
While several efforts have focused on the DMOZ
corpus, often as a reference for Web summarization
</bodyText>
<equation confidence="0.997341">
� |w|
i=0
p(w|cd) =
|cd |
E
j=0
</equation>
<page confidence="0.899198">
670
</page>
<note confidence="0.5528735">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670–675,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999871558823529">
tasks (Berger and Mittal, 2000; Delort et al., 2003)
or Web clustering tasks (Ramage et al., 2009b), very
little research has attempted to make use of its hier-
archy as is. The work by Sun et al. (2005), where
the DMOZ hierarchy is used as a basis for a hierar-
chical lexicon, is closest to ours although their con-
tribution is not a full-fledged content model, but a
selection of highly salient vocabulary for every cat-
egory of the hierarchy. The problem considered in
this paper is connected to the area of Topic Modeling
(Blei and Lafferty, 2009) where the goal is to reduce
the surface complexity of text documents by mod-
eling them as mixtures over a finite set of topics2.
While the inferred models are usually flat, in that
no explicit relationship exists among topics, more
complex, non-parametric, representations have been
proposed to elicit the hierarchical structure of vari-
ous datasets (Hofmann, 1999; Blei et al., 2010; Li
et al., 2007). Our purpose here is more specialized
and similar to that of Labeled LDA (Ramage et al.,
2009a) or Fixed hLDA (Reisinger and Pa¸sca, 2009)
where the set of topics associated with a document is
known a priori. In both cases, document labels are
mapped to constraints on the set of topics on which
the - otherwise unaltered - topic inference algorithm
is to be applied. Lastly, while most recent develop-
ments have been based on unsupervised data, it is
also worth mentioning earlier approaches like Topic
Signatures (Lin and Hovy, 2000) where words (or
phrases) characteristic of a topic are identified using
a statistical test of dependence. Our first model ex-
tends this approach to the hierarchical setting, build-
ing actual topic models based on the selected vocab-
ulary.
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="method">
3 Information-Theoretic Approach
</sectionHeader>
<bodyText confidence="0.999857222222222">
The assumption that topics are known a-priori al-
lows us to extend the concept of Topic Signatures to
a hierarchical setting. Lin and Hovy (2000) describe
a Topic Signature as a list of words highly correlated
with a target concept, and use a χ2 estimator over
labeled data to decide as to the allocation of a word
to a topic. Here, the sub-categories of a node corre-
spond to the topics. However, since the hierarchy is
naturally organized in a generic-to-specific fashion,
</bodyText>
<footnote confidence="0.6868945">
2Here we use the term topic to describe a normalized distri-
bution over a fixed vocabulary V.
</footnote>
<bodyText confidence="0.999661">
for each node we select words that have the least dis-
criminative power between the node’s children. The
rationale is that, if a word can discriminate well be-
tween one child and all others, then it belongs in that
child’s node.
</bodyText>
<subsectionHeader confidence="0.99914">
3.1 Word Assignment
</subsectionHeader>
<bodyText confidence="0.999958294117647">
The algorithm proceeds in two phases. In the first
phase, the hierarchy tree is traversed in a bottom-up
fashion to compile word frequency information un-
der each node. In the second phase, the hierarchy
is traversed top-down and, at each step, words get
assigned to the current node based on whether they
can discriminate between the current node’s chil-
dren. Once a word has been assigned on a given
path, it can no longer be assigned to any other node
on this path. Thus, within a path, a word always
takes on the meaning of the one topic to which it has
been assigned.
The discriminative power of a term with respect
to node N is formalized based on one of the follow-
ing measures:
Entropy of the a posteriori children category dis-
tribution for a given w.
</bodyText>
<equation confidence="0.998118">
Ent(w) = − E p(C|w) log(p(C|w) (3)
CESub(N)
</equation>
<bodyText confidence="0.99209325">
Cross-Entropy between the a priori children cat-
egory distribution and the a posteriori children cate-
gories distribution conditioned on the appearance of
w.
</bodyText>
<equation confidence="0.9957725">
ECrossEnt(w) = − p(C) log(p(C|w)) (4)
CESub(N)
</equation>
<bodyText confidence="0.9164325">
χ2 score, similar to Lin and Hovy (2000) but ap-
plied to classification tasks that can involve an ar-
bitrary number of (sub-)categories. The number of
degrees of freedom of the χ2 distribution is a func-
tion of the number of children.
(nC(i) − p(C)p(i))2 (5)
p(C)p(i)
To identify words exhibiting an unusually low dis-
criminative power between the children categories,
we assume a gaussian distribution of the score used
and select those whose score is at least σ = 2 stan-
dard deviations away from the population mean3.
</bodyText>
<footnote confidence="0.533341">
3Although this makes the decision process less arbitrary
</footnote>
<equation confidence="0.9852515">
Eχ2(w) = E
iE{w,w} CESub(N)
</equation>
<page confidence="0.958637">
671
</page>
<bodyText confidence="0.713456">
Algorithm 1 Generative process for hLLDA
</bodyText>
<listItem confidence="0.8842782">
• For each topic t ∈ H
– Draw βt = (βt,1, ... , βt,V )T ∼ Dir(·|η)
• For each document, d ∈ {1, 2 ... K}
– Draw a random path assignment cd ∈ H
– Draw a distribution over levels along cd, θd ∼
</listItem>
<table confidence="0.4868384">
Dir(·|α)
– Draw a document length n ∼ φH
– For each word wd,i ∈ {wd,1, wd,2, ... wd,n},
∗ Draw level zd,i ∼ Mult(θd)
∗ Draw word wd,i ∼ Mult(βcd[zd,i])
</table>
<subsectionHeader confidence="0.96905">
3.2 Topic Definition &amp; Mixing Proportions
</subsectionHeader>
<bodyText confidence="0.998408">
Based on the final word assignments, we estimate
the probability of word wi in topic Tk, as:
</bodyText>
<equation confidence="0.9687">
nCk(wi)
P(wi|Tk) =
nCk
</equation>
<bodyText confidence="0.999972285714286">
with nCk(wi) the total number of occurrence of wi
in documents under Ck, and nCk the total number of
words in documents under Ck.
Given the individual word assignments we eval-
uate the mixing proportions using corpus-level esti-
mates, which are computed by averaging the mixing
proportions of all the training documents.
</bodyText>
<sectionHeader confidence="0.995974" genericHeader="method">
4 Hierarchical Bayesian Approach
</sectionHeader>
<bodyText confidence="0.964621">
The previous approach, while attractive in its sim-
plicity, makes a strong claim that a word can be
emitted by at most one node on any given path. A
more interesting model might stem from allowing
soft word-topic assignments, where any topic on the
document’s path may emit any word in the vocabu-
lary space.
We consider a modified version of hierarchical
LDA (Blei et al., 2010), where the underlying tree
structure is known a priori and does not have to
be inferred from data. The generative story for this
model, which we designate as hierarchical Labeled-
LDA (hLLDA), is shown in Algorithm 1. Just as
with Fixed Structure LDA4 (Reisinger and Pa¸sca,
than with a hand-selected threshold, this raises the issue of iden-
tifying the true distribution for the estimator used.
</bodyText>
<footnote confidence="0.837466666666667">
4Our implementation of hLLDA was partially
based on the UTML toolkit which is available at
https://github.com/joeraii/
</footnote>
<bodyText confidence="0.999858703703704">
2009), the topics used for inference are, for each
document, those found on the path from the hierar-
chy root to the document itself. Once the target path
cd ∈ H is known, the model reduces to LDA over
the set of topics comprising cd. Given that the joint
distribution p(θ, z, w|cd) is intractable (Blei et al.,
2003), we use collapsed Gibbs-sampling (Griffiths
and Steyvers, 2004) to obtain individual word-level
assignments. The probability of assigning wi, the
ith word in document d, to the jth topic on path cd,
conditioned on all other word assignments, is given
by:
where nd ij is the frequency of words from docu-
ment d assigned to topic j, nf&amp;quot; �i��is the frequency
of word wi in topic j, α and η are Dirichlet con-
centration parameters for the path-topic and topic-
word multinomials respectively, and V is the vocab-
ulary size. Equation 7 can be understood as defin-
ing the unormalized posterior word-level assignment
distribution as the product of the current level mix-
ing proportion θi and of the current estimate of the
word-topic conditional probability p(wi|zi). By re-
peatedly resampling from this distribution we ob-
tain individual word assignments which in turn al-
low us to estimate the topic multinomials and the
per-document mixing proportions. Specifically, the
topic multinomials are estimated as:
</bodyText>
<equation confidence="0.999155">
� (8)
n�zcd[j] + V η
</equation>
<bodyText confidence="0.9096675">
while the per-document mixing proportions θd can
be estimated as:
</bodyText>
<equation confidence="0.998722">
nd�,j + α
θd,j ≈ nd + |cd|α, ∀j ∈ 1, ...,cd (9)
</equation>
<bodyText confidence="0.999971833333333">
Although we experimented with hyper-parameter
learning (Dirichlet concentration parameter η), do-
ing so did not significantly impact the final model.
The results we report are therefore based on stan-
dard values for the hyper-parameters (α = 1 and
η = 0.1).
</bodyText>
<sectionHeader confidence="0.997857" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999725">
We compared the predictive power of our model to
that of several language models. In every case, we
</bodyText>
<equation confidence="0.999382545454546">
(6)
nd−i,j + α
|cd|(α + 1) ·
V (η + 1) (7)
p(zi = j|z−i, w, cd) ∝
nwi
−i,j + η
βcd[j],i = p(wi|zcd[j]) =
wi
n η
zcd[j] +
</equation>
<page confidence="0.995899">
672
</page>
<subsectionHeader confidence="0.903838">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9996574">
The perplexities obtained for the hierarchical and n-
gram models are reported in Table 1.
compute the perplexity of the model over the held-
out data W = {w1 ... wn} given the model M and
the observed (training) data, namely:
</bodyText>
<equation confidence="0.9904">
log pM(wi,j))
(10)
1
��
i=1
|wi|
� |wM|
j=1
1
perplM(W) = exp(−
n
</equation>
<subsectionHeader confidence="0.983201">
5.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.99997375">
Our experiments focused on the English portion of
the DMOZ dataset5 (about 2.1 million entries). The
raw dataset was randomized and divided according
to a 98% training (31M words), 1% development
(320k words), 1% testing (320k words) split. Gists
were tokenized using simple tokenization rules, with
no stemming, and were case-normalized. Akin to
Berger and Mittal (2000) we mapped numerical to-
kens to the NUM placeholder and selected the V =
65535 most frequent words as our vocabulary. Any
token outside of this set was mapped to the OOV to-
ken. We did not perform any stop-word filtering.
</bodyText>
<subsectionHeader confidence="0.990252">
5.2 Reference Models
</subsectionHeader>
<bodyText confidence="0.9999024">
Our reference models consists of several n-gram
(n E [1, 3]) language models, none of which makes
use of the hierarchical information available from
the corpus. Under these models, the probability of
a given string is given by:
</bodyText>
<equation confidence="0.765004">
p(wi|wi−1, ... , wi−(n−1)) (11)
</equation>
<bodyText confidence="0.999978384615385">
We used the SRILM toolkit (Stolcke, 2002), en-
abling Kneser-Ney smoothing with default param-
eters.
Note that an interesting model to include here
would have been one that jointly infers a hierarchy
of topics as well as the topics that comprise it, much
like the regular hierarchical LDA algorithm (Blei et
al., 2010). While we did not perform this experiment
as part of this work, this is definitely an avenue for
future work. We are especially interested in seeing
whether an automatically inferred hierarchy of top-
ics would fundamentally differ from the manually-
curated hierarchy used by DMOZ.
</bodyText>
<footnote confidence="0.974545">
5We discarded the Top/World portion of the hierarchy.
</footnote>
<table confidence="0.9996657">
reg all
# documents 1153000 2083949
avg. gist length 15.47 15.36
1-gram 1644.10 1414.98
2-gram 352.10 287.09
3-gram 239.08 179.71
entropy 812.91 1037.70
cross-entropy 1167.07 1869.90
x2 1639.29 1693.76
hLLDA 941.16 983.77
</table>
<tableCaption confidence="0.978538333333333">
Table 1: Perplexity of the hierarchical models and the
reference n-gram models over the entire DMOZ dataset
(all), and the non-Regional portion of the dataset (reg).
</tableCaption>
<bodyText confidence="0.999961807692308">
When taken on the entire hierarchy (all), the per-
formance of the Bayesian and entropy-based mod-
els significantly exceeds that of the 1-gram model
(significant under paired t-test, both with p-value &lt;
2.2 · 10−16) while remaining well below that of ei-
ther the 2 or 3 gram models. This suggests that, al-
though the hierarchy plays a key role in the appear-
ance of content in DMOZ gists, word context is also
a key factor that needs to be taken into account: the
two families of models we propose are based on the
bag-of-word assumption and, by design, assume that
words are drawn i.i.d. from an underlying distribu-
tion. While it is not clear how one could extend the
information-theoretic models to include such con-
text, we are currently investigating enhancements to
the hLLDA model along the lines of the approach
proposed in Wallach (2006).
A second area of analysis is to compare the per-
formance of the various models on the entire hier-
archy versus on the non-Regional portion of the tree
(reg). We can see that the perplexity of the proposed
models decreases while that of the flat n-grams mod-
els increase. Since the non-Regional portion of the
DMOZ hierarchy is organized more consistently in
a semantic fashion6, we believe this reflects the abil-
ity of the hierarchical models to take advantage of
</bodyText>
<footnote confidence="0.995037">
6The specificity of the Regional sub-tree has also been dis-
cussed by previous work (Ramage et al., 2009b), justifying a
special treatment for that part of the DMOZ dataset.
</footnote>
<equation confidence="0.991347">
� |s|
i=1
p(w) =
</equation>
<page confidence="0.998182">
673
</page>
<figureCaption confidence="0.996689">
Figure 1: Perplexity of the proposed algorithms against the 1-gram baseline for each of the 14 top level DMOZ cate-
gories: Arts, Business, Computer, Games, Health, Home, News, Recreation, Reference, Regional, Science, Shopping,
Society, Sports.
</figureCaption>
<bodyText confidence="0.999952633333334">
the corpus structure to represent the content of the
summaries. On the other hand, the Regional por-
tion of the dataset seems to contribute a significant
amount of noise to the hierarchy, leading to a loss in
performance for those models.
We can observe that while hLLDA outperforms
all information-theoretical models when applied to
the entire DMOZ corpus, it falls behind the entropy-
based model when restricted to the non-regional
section of the corpus. Also if the reduction in
perplexity remains limited for the entropy, x2 and
hLLDA models, the cross-entropy based model in-
curs a more significant boost in performance when
applied to the more semantically-organized portion
of the corpus. The reason behind such disparity in
behavior is not clear and we plan on investigating
this issue as part of our future work.
Further analyzing the impact of the respective
DMOZ sub-sections, we show in Figure 1 re-
sults for the hierarchical and 1-gram models when
trained and tested over the 14 main sub-trees of
the hierarchy. Our intuition is that differences
in the organization of those sub-trees might af-
fect the predictive power of the various mod-
els. Looking at sub-trees we can see that the
trend is the same for most of them, with the best
level of perplexity being achieved by the hierar-
chical Bayesian model, closely followed by the
information-theoretical model using entropy as its
selection criterion.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999411692307692">
In this paper we have demonstrated the creation of a
topic-model of Web summaries using the hierarchy
of a popular Web directory. This hierarchy provides
a backbone around which we crystalize hierarchical
topic models. Individual topics exhibit increasing
specificity as one goes down a path in the tree. While
we focused on Web summaries, this model can be
readily adapted to any Web-related content that can
be seen as a mixture of the component topics appear-
ing along a paths in the hierarchy. Such model can
become a key resource for the fine-grained distinc-
tion between generic and specific elements of lan-
guage in a large, heterogenous corpus.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999774166666667">
This material is based on research supported in part
by the U.S. National Science Foundation (NSF) un-
der IIS-05-34871. Any opinions, findings and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the NSF.
</bodyText>
<page confidence="0.998634">
674
</page>
<sectionHeader confidence="0.996304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999694640625">
A. Berger and V. Mittal. 2000. Ocelot: a system for
summarizing web pages. In Proceedings of the 23rd
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR’00), pages 144–151.
David M. Blei and J. Lafferty. 2009. Topic models. In A.
Srivastava and M. Sahami, editors, Text Mining: The-
ory and Applications. Taylor and Francis.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet allocation. JMLR, 3:993–1022.
David M. Blei, Thomas L. Griffiths, and Micheal I. Jor-
dan. 2010. The nested chinese restaurant process and
bayesian nonparametric inference of topic hierarchies.
In Journal of ACM, volume 57.
Jean-Yves Delort, Bernadette Bouchon-Meunier, and
Maria Rifqi. 2003. Enhanced web document sum-
marization using hyperlinks. In Hypertext 2003, pages
208–215.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding
scientific topics. PNAS, 101(suppl. 1):5228–5235.
Thomas Hofmann. 1999. The cluster-abstraction model:
Unsupervised learning of topic hierarchies from text
data. In Proceedings of IJCAI’99.
Wei Li, David Blei, and Andrew McCallum. 2007. Non-
parametric bayes pachinko allocation. In Proceedings
of the Proceedings of the Twenty-Third ConferenceAn-
nual Conference on Uncertainty in Artificial Intelli-
gence (UAI-07), pages 243–250, Corvallis, Oregon.
AUAI Press.
C.-Y. Lin and E. Hovy. 2000. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics, pages 495–501.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D. Manning. 2009a. Labeled lda: A
supervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2009), Singapore, pages 248–256.
Daniel Ramage, Paul Heymann, Christopher D. Man-
ning, and Hector Garcia-Molina. 2009b. Clustering
the tagged web. In Proceedings of the Second ACM In-
ternational Conference on Web Search and Data Min-
ing, WSDM ’09, pages 54–63, New York, NY, USA.
ACM.
Joseph Reisinger and Marius Pa¸sca. 2009. Latent vari-
able models of concept-attribute attachment. In ACL-
IJCNLP ’09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP: Volume 2, pages 620–628, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pages 901–904, September.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In SIGIR 2005,
pages 194–201.
Hanna M. Wallach. 2006. Topic modeling: Beyond bag-
of-words. In Proceedings of the 23rd International
Conference on Machine Learning, Pittsburgh, Penn-
sylvania, U.S., pages 977–984.
</reference>
<page confidence="0.998959">
675
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.661116">
<title confidence="0.999861">A Hierarchical Model of Web Summaries</title>
<author confidence="0.999712">Petinot McKeown</author>
<affiliation confidence="0.999971">Department of Computer</affiliation>
<address confidence="0.8290425">Columbia New York, NY</address>
<email confidence="0.999416">{ypetinot|kathy|kapil}@cs.columbia.edu</email>
<abstract confidence="0.9990094">We investigate the relevance of hierarchical topic models to represent the content of Web gists. We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. Our first approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA. We evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>V Mittal</author>
</authors>
<title>Ocelot: a system for summarizing web pages.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’00),</booktitle>
<pages>144--151</pages>
<contexts>
<context position="3121" citStr="Berger and Mittal, 2000" startWordPosition="494" endWordPosition="497">n this framework. We describe how they allow the derivation of both p(wi|cd�j) and B and present early experimental results showing that explicit hierarchical information of content can indeed be used as a basis for content modeling purposes. 2 Related Work While several efforts have focused on the DMOZ corpus, often as a reference for Web summarization � |w| i=0 p(w|cd) = |cd | E j=0 670 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670–675, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics tasks (Berger and Mittal, 2000; Delort et al., 2003) or Web clustering tasks (Ramage et al., 2009b), very little research has attempted to make use of its hierarchy as is. The work by Sun et al. (2005), where the DMOZ hierarchy is used as a basis for a hierarchical lexicon, is closest to ours although their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling th</context>
<context position="11708" citStr="Berger and Mittal (2000)" startWordPosition="1984" endWordPosition="1987">dels are reported in Table 1. compute the perplexity of the model over the heldout data W = {w1 ... wn} given the model M and the observed (training) data, namely: log pM(wi,j)) (10) 1 �� i=1 |wi| � |wM| j=1 1 perplM(W) = exp(− n 5.1 Data Preprocessing Our experiments focused on the English portion of the DMOZ dataset5 (about 2.1 million entries). The raw dataset was randomized and divided according to a 98% training (31M words), 1% development (320k words), 1% testing (320k words) split. Gists were tokenized using simple tokenization rules, with no stemming, and were case-normalized. Akin to Berger and Mittal (2000) we mapped numerical tokens to the NUM placeholder and selected the V = 65535 most frequent words as our vocabulary. Any token outside of this set was mapped to the OOV token. We did not perform any stop-word filtering. 5.2 Reference Models Our reference models consists of several n-gram (n E [1, 3]) language models, none of which makes use of the hierarchical information available from the corpus. Under these models, the probability of a given string is given by: p(wi|wi−1, ... , wi−(n−1)) (11) We used the SRILM toolkit (Stolcke, 2002), enabling Kneser-Ney smoothing with default parameters. N</context>
</contexts>
<marker>Berger, Mittal, 2000</marker>
<rawString>A. Berger and V. Mittal. 2000. Ocelot: a system for summarizing web pages. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’00), pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>J Lafferty</author>
</authors>
<title>Topic models.</title>
<date>2009</date>
<editor>In A. Srivastava and M. Sahami, editors, Text</editor>
<publisher>Taylor and Francis.</publisher>
<contexts>
<context position="3637" citStr="Blei and Lafferty, 2009" startWordPosition="587" endWordPosition="590">d, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics tasks (Berger and Mittal, 2000; Delort et al., 2003) or Web clustering tasks (Ramage et al., 2009b), very little research has attempted to make use of its hierarchy as is. The work by Sun et al. (2005), where the DMOZ hierarchy is used as a basis for a hierarchical lexicon, is closest to ours although their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2. While the inferred models are usually flat, in that no explicit relationship exists among topics, more complex, non-parametric, representations have been proposed to elicit the hierarchical structure of various datasets (Hofmann, 1999; Blei et al., 2010; Li et al., 2007). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al., 2009a) or Fixed hLDA (Reisinger and Pa¸sca, 2009) where the set of topics associated with a document is known </context>
</contexts>
<marker>Blei, Lafferty, 2009</marker>
<rawString>David M. Blei and J. Lafferty. 2009. Topic models. In A. Srivastava and M. Sahami, editors, Text Mining: Theory and Applications. Taylor and Francis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent dirichlet allocation. JMLR,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="9366" citStr="Blei et al., 2003" startWordPosition="1580" endWordPosition="1583">hm 1. Just as with Fixed Structure LDA4 (Reisinger and Pa¸sca, than with a hand-selected threshold, this raises the issue of identifying the true distribution for the estimator used. 4Our implementation of hLLDA was partially based on the UTML toolkit which is available at https://github.com/joeraii/ 2009), the topics used for inference are, for each document, those found on the path from the hierarchy root to the document itself. Once the target path cd ∈ H is known, the model reduces to LDA over the set of topics comprising cd. Given that the joint distribution p(θ, z, w|cd) is intractable (Blei et al., 2003), we use collapsed Gibbs-sampling (Griffiths and Steyvers, 2004) to obtain individual word-level assignments. The probability of assigning wi, the ith word in document d, to the jth topic on path cd, conditioned on all other word assignments, is given by: where nd ij is the frequency of words from document d assigned to topic j, nf&amp;quot; �i��is the frequency of word wi in topic j, α and η are Dirichlet concentration parameters for the path-topic and topicword multinomials respectively, and V is the vocabulary size. Equation 7 can be understood as defining the unormalized posterior word-level assign</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Ng, and Michael Jordan. 2003. Latent dirichlet allocation. JMLR, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Micheal I Jordan</author>
</authors>
<title>The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies.</title>
<date>2010</date>
<journal>In Journal of ACM,</journal>
<volume>57</volume>
<contexts>
<context position="1763" citStr="Blei et al., 2010" startWordPosition="261" endWordPosition="264">ormation about the documents attached to them, including path-level, statistical, representations of content, and fine-grained views on the level of specificity of the language used in those documents. Our study focuses on the ontology underlying DMOZ1, a popular Web directory. We propose two methods for crystalizing a hierarchical topic model against its hierarchy and show that the resulting models outperform a flat unigram model in its predictive power over held-out data. 1http://www.dmoz.org To construct our hierarchical topic models, we adopt the mixed membership formalism (Hofmann, 1999; Blei et al., 2010), where a document is represented as a mixture over a set of word multinomials. We consider the document hierarchy H (e.g. the DMOZ hierarchy) as a tree where internal nodes (category nodes) and leaf nodes (documents), as well as the edges connecting them, are known a priori. Each node Ni in H is mapped to a multinomial word distribution MultN,, and each path cd to a leaf node D is associated with a mixture over the multinonials (MultC0 ... MultCk, MultD) appearing along this path. The mixture components are combined using a mixing proportion vector (BC0 ... BCk), so that the likelihood of str</context>
<context position="4019" citStr="Blei et al., 2010" startWordPosition="648" endWordPosition="651">ntribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2. While the inferred models are usually flat, in that no explicit relationship exists among topics, more complex, non-parametric, representations have been proposed to elicit the hierarchical structure of various datasets (Hofmann, 1999; Blei et al., 2010; Li et al., 2007). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al., 2009a) or Fixed hLDA (Reisinger and Pa¸sca, 2009) where the set of topics associated with a document is known a priori. In both cases, document labels are mapped to constraints on the set of topics on which the - otherwise unaltered - topic inference algorithm is to be applied. Lastly, while most recent developments have been based on unsupervised data, it is also worth mentioning earlier approaches like Topic Signatures (Lin and Hovy, 2000) where words (or phrases) characteristic of a t</context>
<context position="8537" citStr="Blei et al., 2010" startWordPosition="1440" endWordPosition="1443">ocuments under Ck. Given the individual word assignments we evaluate the mixing proportions using corpus-level estimates, which are computed by averaging the mixing proportions of all the training documents. 4 Hierarchical Bayesian Approach The previous approach, while attractive in its simplicity, makes a strong claim that a word can be emitted by at most one node on any given path. A more interesting model might stem from allowing soft word-topic assignments, where any topic on the document’s path may emit any word in the vocabulary space. We consider a modified version of hierarchical LDA (Blei et al., 2010), where the underlying tree structure is known a priori and does not have to be inferred from data. The generative story for this model, which we designate as hierarchical LabeledLDA (hLLDA), is shown in Algorithm 1. Just as with Fixed Structure LDA4 (Reisinger and Pa¸sca, than with a hand-selected threshold, this raises the issue of identifying the true distribution for the estimator used. 4Our implementation of hLLDA was partially based on the UTML toolkit which is available at https://github.com/joeraii/ 2009), the topics used for inference are, for each document, those found on the path fr</context>
<context position="12524" citStr="Blei et al., 2010" startWordPosition="2124" endWordPosition="2127"> stop-word filtering. 5.2 Reference Models Our reference models consists of several n-gram (n E [1, 3]) language models, none of which makes use of the hierarchical information available from the corpus. Under these models, the probability of a given string is given by: p(wi|wi−1, ... , wi−(n−1)) (11) We used the SRILM toolkit (Stolcke, 2002), enabling Kneser-Ney smoothing with default parameters. Note that an interesting model to include here would have been one that jointly infers a hierarchy of topics as well as the topics that comprise it, much like the regular hierarchical LDA algorithm (Blei et al., 2010). While we did not perform this experiment as part of this work, this is definitely an avenue for future work. We are especially interested in seeing whether an automatically inferred hierarchy of topics would fundamentally differ from the manuallycurated hierarchy used by DMOZ. 5We discarded the Top/World portion of the hierarchy. reg all # documents 1153000 2083949 avg. gist length 15.47 15.36 1-gram 1644.10 1414.98 2-gram 352.10 287.09 3-gram 239.08 179.71 entropy 812.91 1037.70 cross-entropy 1167.07 1869.90 x2 1639.29 1693.76 hLLDA 941.16 983.77 Table 1: Perplexity of the hierarchical mode</context>
</contexts>
<marker>Blei, Griffiths, Jordan, 2010</marker>
<rawString>David M. Blei, Thomas L. Griffiths, and Micheal I. Jordan. 2010. The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies. In Journal of ACM, volume 57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Yves Delort</author>
<author>Bernadette Bouchon-Meunier</author>
<author>Maria Rifqi</author>
</authors>
<title>Enhanced web document summarization using hyperlinks. In Hypertext</title>
<date>2003</date>
<pages>208--215</pages>
<contexts>
<context position="3143" citStr="Delort et al., 2003" startWordPosition="498" endWordPosition="501">ribe how they allow the derivation of both p(wi|cd�j) and B and present early experimental results showing that explicit hierarchical information of content can indeed be used as a basis for content modeling purposes. 2 Related Work While several efforts have focused on the DMOZ corpus, often as a reference for Web summarization � |w| i=0 p(w|cd) = |cd | E j=0 670 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670–675, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics tasks (Berger and Mittal, 2000; Delort et al., 2003) or Web clustering tasks (Ramage et al., 2009b), very little research has attempted to make use of its hierarchy as is. The work by Sun et al. (2005), where the DMOZ hierarchy is used as a basis for a hierarchical lexicon, is closest to ours although their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a </context>
</contexts>
<marker>Delort, Bouchon-Meunier, Rifqi, 2003</marker>
<rawString>Jean-Yves Delort, Bernadette Bouchon-Meunier, and Maria Rifqi. 2003. Enhanced web document summarization using hyperlinks. In Hypertext 2003, pages 208–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="9430" citStr="Griffiths and Steyvers, 2004" startWordPosition="1588" endWordPosition="1591">d Pa¸sca, than with a hand-selected threshold, this raises the issue of identifying the true distribution for the estimator used. 4Our implementation of hLLDA was partially based on the UTML toolkit which is available at https://github.com/joeraii/ 2009), the topics used for inference are, for each document, those found on the path from the hierarchy root to the document itself. Once the target path cd ∈ H is known, the model reduces to LDA over the set of topics comprising cd. Given that the joint distribution p(θ, z, w|cd) is intractable (Blei et al., 2003), we use collapsed Gibbs-sampling (Griffiths and Steyvers, 2004) to obtain individual word-level assignments. The probability of assigning wi, the ith word in document d, to the jth topic on path cd, conditioned on all other word assignments, is given by: where nd ij is the frequency of words from document d assigned to topic j, nf&amp;quot; �i��is the frequency of word wi in topic j, α and η are Dirichlet concentration parameters for the path-topic and topicword multinomials respectively, and V is the vocabulary size. Equation 7 can be understood as defining the unormalized posterior word-level assignment distribution as the product of the current level mixing pro</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. PNAS, 101(suppl. 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI’99.</booktitle>
<contexts>
<context position="1743" citStr="Hofmann, 1999" startWordPosition="259" endWordPosition="260">t amount of information about the documents attached to them, including path-level, statistical, representations of content, and fine-grained views on the level of specificity of the language used in those documents. Our study focuses on the ontology underlying DMOZ1, a popular Web directory. We propose two methods for crystalizing a hierarchical topic model against its hierarchy and show that the resulting models outperform a flat unigram model in its predictive power over held-out data. 1http://www.dmoz.org To construct our hierarchical topic models, we adopt the mixed membership formalism (Hofmann, 1999; Blei et al., 2010), where a document is represented as a mixture over a set of word multinomials. We consider the document hierarchy H (e.g. the DMOZ hierarchy) as a tree where internal nodes (category nodes) and leaf nodes (documents), as well as the edges connecting them, are known a priori. Each node Ni in H is mapped to a multinomial word distribution MultN,, and each path cd to a leaf node D is associated with a mixture over the multinonials (MultC0 ... MultCk, MultD) appearing along this path. The mixture components are combined using a mixing proportion vector (BC0 ... BCk), so that t</context>
<context position="4000" citStr="Hofmann, 1999" startWordPosition="646" endWordPosition="647">though their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2. While the inferred models are usually flat, in that no explicit relationship exists among topics, more complex, non-parametric, representations have been proposed to elicit the hierarchical structure of various datasets (Hofmann, 1999; Blei et al., 2010; Li et al., 2007). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al., 2009a) or Fixed hLDA (Reisinger and Pa¸sca, 2009) where the set of topics associated with a document is known a priori. In both cases, document labels are mapped to constraints on the set of topics on which the - otherwise unaltered - topic inference algorithm is to be applied. Lastly, while most recent developments have been based on unsupervised data, it is also worth mentioning earlier approaches like Topic Signatures (Lin and Hovy, 2000) where words (or phrases) ch</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data. In Proceedings of IJCAI’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>David Blei</author>
<author>Andrew McCallum</author>
</authors>
<title>Nonparametric bayes pachinko allocation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Proceedings of the Twenty-Third ConferenceAnnual Conference on Uncertainty in Artificial Intelligence (UAI-07),</booktitle>
<pages>243--250</pages>
<publisher>AUAI Press.</publisher>
<location>Corvallis, Oregon.</location>
<contexts>
<context position="4037" citStr="Li et al., 2007" startWordPosition="652" endWordPosition="655"> full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2. While the inferred models are usually flat, in that no explicit relationship exists among topics, more complex, non-parametric, representations have been proposed to elicit the hierarchical structure of various datasets (Hofmann, 1999; Blei et al., 2010; Li et al., 2007). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al., 2009a) or Fixed hLDA (Reisinger and Pa¸sca, 2009) where the set of topics associated with a document is known a priori. In both cases, document labels are mapped to constraints on the set of topics on which the - otherwise unaltered - topic inference algorithm is to be applied. Lastly, while most recent developments have been based on unsupervised data, it is also worth mentioning earlier approaches like Topic Signatures (Lin and Hovy, 2000) where words (or phrases) characteristic of a topic are identifie</context>
</contexts>
<marker>Li, Blei, McCallum, 2007</marker>
<rawString>Wei Li, David Blei, and Andrew McCallum. 2007. Nonparametric bayes pachinko allocation. In Proceedings of the Proceedings of the Twenty-Third ConferenceAnnual Conference on Uncertainty in Artificial Intelligence (UAI-07), pages 243–250, Corvallis, Oregon. AUAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>495--501</pages>
<contexts>
<context position="4572" citStr="Lin and Hovy, 2000" startWordPosition="743" endWordPosition="746">al structure of various datasets (Hofmann, 1999; Blei et al., 2010; Li et al., 2007). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al., 2009a) or Fixed hLDA (Reisinger and Pa¸sca, 2009) where the set of topics associated with a document is known a priori. In both cases, document labels are mapped to constraints on the set of topics on which the - otherwise unaltered - topic inference algorithm is to be applied. Lastly, while most recent developments have been based on unsupervised data, it is also worth mentioning earlier approaches like Topic Signatures (Lin and Hovy, 2000) where words (or phrases) characteristic of a topic are identified using a statistical test of dependence. Our first model extends this approach to the hierarchical setting, building actual topic models based on the selected vocabulary. 3 Information-Theoretic Approach The assumption that topics are known a-priori allows us to extend the concept of Topic Signatures to a hierarchical setting. Lin and Hovy (2000) describe a Topic Signature as a list of words highly correlated with a target concept, and use a χ2 estimator over labeled data to decide as to the allocation of a word to a topic. Here</context>
<context position="6699" citStr="Lin and Hovy (2000)" startWordPosition="1111" endWordPosition="1114"> assigned to any other node on this path. Thus, within a path, a word always takes on the meaning of the one topic to which it has been assigned. The discriminative power of a term with respect to node N is formalized based on one of the following measures: Entropy of the a posteriori children category distribution for a given w. Ent(w) = − E p(C|w) log(p(C|w) (3) CESub(N) Cross-Entropy between the a priori children category distribution and the a posteriori children categories distribution conditioned on the appearance of w. ECrossEnt(w) = − p(C) log(p(C|w)) (4) CESub(N) χ2 score, similar to Lin and Hovy (2000) but applied to classification tasks that can involve an arbitrary number of (sub-)categories. The number of degrees of freedom of the χ2 distribution is a function of the number of children. (nC(i) − p(C)p(i))2 (5) p(C)p(i) To identify words exhibiting an unusually low discriminative power between the children categories, we assume a gaussian distribution of the score used and select those whose score is at least σ = 2 standard deviations away from the population mean3. 3Although this makes the decision process less arbitrary Eχ2(w) = E iE{w,w} CESub(N) 671 Algorithm 1 Generative process for </context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C.-Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th conference on Computational linguistics, pages 495–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled lda: A supervised topic model for credit attribution in multilabeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), Singapore,</booktitle>
<pages>248--256</pages>
<contexts>
<context position="3188" citStr="Ramage et al., 2009" startWordPosition="506" endWordPosition="509">i|cd�j) and B and present early experimental results showing that explicit hierarchical information of content can indeed be used as a basis for content modeling purposes. 2 Related Work While several efforts have focused on the DMOZ corpus, often as a reference for Web summarization � |w| i=0 p(w|cd) = |cd | E j=0 670 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670–675, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics tasks (Berger and Mittal, 2000; Delort et al., 2003) or Web clustering tasks (Ramage et al., 2009b), very little research has attempted to make use of its hierarchy as is. The work by Sun et al. (2005), where the DMOZ hierarchy is used as a basis for a hierarchical lexicon, is closest to ours although their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2. While the inferred mod</context>
<context position="14650" citStr="Ramage et al., 2009" startWordPosition="2479" endWordPosition="2482"> the lines of the approach proposed in Wallach (2006). A second area of analysis is to compare the performance of the various models on the entire hierarchy versus on the non-Regional portion of the tree (reg). We can see that the perplexity of the proposed models decreases while that of the flat n-grams models increase. Since the non-Regional portion of the DMOZ hierarchy is organized more consistently in a semantic fashion6, we believe this reflects the ability of the hierarchical models to take advantage of 6The specificity of the Regional sub-tree has also been discussed by previous work (Ramage et al., 2009b), justifying a special treatment for that part of the DMOZ dataset. � |s| i=1 p(w) = 673 Figure 1: Perplexity of the proposed algorithms against the 1-gram baseline for each of the 14 top level DMOZ categories: Arts, Business, Computer, Games, Health, Home, News, Recreation, Reference, Regional, Science, Shopping, Society, Sports. the corpus structure to represent the content of the summaries. On the other hand, the Regional portion of the dataset seems to contribute a significant amount of noise to the hierarchy, leading to a loss in performance for those models. We can observe that while h</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D. Manning. 2009a. Labeled lda: A supervised topic model for credit attribution in multilabeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), Singapore, pages 248–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Paul Heymann</author>
<author>Christopher D Manning</author>
<author>Hector Garcia-Molina</author>
</authors>
<title>Clustering the tagged web.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM ’09,</booktitle>
<pages>54--63</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3188" citStr="Ramage et al., 2009" startWordPosition="506" endWordPosition="509">i|cd�j) and B and present early experimental results showing that explicit hierarchical information of content can indeed be used as a basis for content modeling purposes. 2 Related Work While several efforts have focused on the DMOZ corpus, often as a reference for Web summarization � |w| i=0 p(w|cd) = |cd | E j=0 670 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670–675, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics tasks (Berger and Mittal, 2000; Delort et al., 2003) or Web clustering tasks (Ramage et al., 2009b), very little research has attempted to make use of its hierarchy as is. The work by Sun et al. (2005), where the DMOZ hierarchy is used as a basis for a hierarchical lexicon, is closest to ours although their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2. While the inferred mod</context>
<context position="14650" citStr="Ramage et al., 2009" startWordPosition="2479" endWordPosition="2482"> the lines of the approach proposed in Wallach (2006). A second area of analysis is to compare the performance of the various models on the entire hierarchy versus on the non-Regional portion of the tree (reg). We can see that the perplexity of the proposed models decreases while that of the flat n-grams models increase. Since the non-Regional portion of the DMOZ hierarchy is organized more consistently in a semantic fashion6, we believe this reflects the ability of the hierarchical models to take advantage of 6The specificity of the Regional sub-tree has also been discussed by previous work (Ramage et al., 2009b), justifying a special treatment for that part of the DMOZ dataset. � |s| i=1 p(w) = 673 Figure 1: Perplexity of the proposed algorithms against the 1-gram baseline for each of the 14 top level DMOZ categories: Arts, Business, Computer, Games, Health, Home, News, Recreation, Reference, Regional, Science, Shopping, Society, Sports. the corpus structure to represent the content of the summaries. On the other hand, the Regional portion of the dataset seems to contribute a significant amount of noise to the hierarchy, leading to a loss in performance for those models. We can observe that while h</context>
</contexts>
<marker>Ramage, Heymann, Manning, Garcia-Molina, 2009</marker>
<rawString>Daniel Ramage, Paul Heymann, Christopher D. Manning, and Hector Garcia-Molina. 2009b. Clustering the tagged web. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM ’09, pages 54–63, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Marius Pa¸sca</author>
</authors>
<title>Latent variable models of concept-attribute attachment.</title>
<date>2009</date>
<booktitle>In ACLIJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>2</volume>
<pages>620--628</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Reisinger, Pa¸sca, 2009</marker>
<rawString>Joseph Reisinger and Marius Pa¸sca. 2009. Latent variable models of concept-attribute attachment. In ACLIJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2, pages 620–628, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="12250" citStr="Stolcke, 2002" startWordPosition="2080" endWordPosition="2081"> no stemming, and were case-normalized. Akin to Berger and Mittal (2000) we mapped numerical tokens to the NUM placeholder and selected the V = 65535 most frequent words as our vocabulary. Any token outside of this set was mapped to the OOV token. We did not perform any stop-word filtering. 5.2 Reference Models Our reference models consists of several n-gram (n E [1, 3]) language models, none of which makes use of the hierarchical information available from the corpus. Under these models, the probability of a given string is given by: p(wi|wi−1, ... , wi−(n−1)) (11) We used the SRILM toolkit (Stolcke, 2002), enabling Kneser-Ney smoothing with default parameters. Note that an interesting model to include here would have been one that jointly infers a hierarchy of topics as well as the topics that comprise it, much like the regular hierarchical LDA algorithm (Blei et al., 2010). While we did not perform this experiment as part of this work, this is definitely an avenue for future work. We are especially interested in seeing whether an automatically inferred hierarchy of topics would fundamentally differ from the manuallycurated hierarchy used by DMOZ. 5We discarded the Top/World portion of the hie</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, vol. 2, pages 901–904, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Tao Sun</author>
<author>Dou Shen</author>
<author>Hua-Jun Zeng</author>
<author>Qiang Yang</author>
<author>Yuchang Lu</author>
<author>Zheng Chen</author>
</authors>
<title>Web-page summarization using clickthrough data. In SIGIR</title>
<date>2005</date>
<pages>194--201</pages>
<contexts>
<context position="3292" citStr="Sun et al. (2005)" startWordPosition="527" endWordPosition="530">tent can indeed be used as a basis for content modeling purposes. 2 Related Work While several efforts have focused on the DMOZ corpus, often as a reference for Web summarization � |w| i=0 p(w|cd) = |cd | E j=0 670 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670–675, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics tasks (Berger and Mittal, 2000; Delort et al., 2003) or Web clustering tasks (Ramage et al., 2009b), very little research has attempted to make use of its hierarchy as is. The work by Sun et al. (2005), where the DMOZ hierarchy is used as a basis for a hierarchical lexicon, is closest to ours although their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2. While the inferred models are usually flat, in that no explicit relationship exists among topics, more complex, non-parametric</context>
</contexts>
<marker>Sun, Shen, Zeng, Yang, Lu, Chen, 2005</marker>
<rawString>Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang, Yuchang Lu, and Zheng Chen. 2005. Web-page summarization using clickthrough data. In SIGIR 2005, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: Beyond bagof-words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning,</booktitle>
<pages>977--984</pages>
<location>Pittsburgh, Pennsylvania, U.S.,</location>
<contexts>
<context position="14084" citStr="Wallach (2006)" startWordPosition="2383" endWordPosition="2384">ning well below that of either the 2 or 3 gram models. This suggests that, although the hierarchy plays a key role in the appearance of content in DMOZ gists, word context is also a key factor that needs to be taken into account: the two families of models we propose are based on the bag-of-word assumption and, by design, assume that words are drawn i.i.d. from an underlying distribution. While it is not clear how one could extend the information-theoretic models to include such context, we are currently investigating enhancements to the hLLDA model along the lines of the approach proposed in Wallach (2006). A second area of analysis is to compare the performance of the various models on the entire hierarchy versus on the non-Regional portion of the tree (reg). We can see that the perplexity of the proposed models decreases while that of the flat n-grams models increase. Since the non-Regional portion of the DMOZ hierarchy is organized more consistently in a semantic fashion6, we believe this reflects the ability of the hierarchical models to take advantage of 6The specificity of the Regional sub-tree has also been discussed by previous work (Ramage et al., 2009b), justifying a special treatment</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M. Wallach. 2006. Topic modeling: Beyond bagof-words. In Proceedings of the 23rd International Conference on Machine Learning, Pittsburgh, Pennsylvania, U.S., pages 977–984.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>