<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001112">
<title confidence="0.9702895">
A Semi-Supervised Method to Learn and Construct Taxonomies using the
Web
</title>
<author confidence="0.89216">
Zornitsa Kozareva and Eduard Hovy
</author>
<affiliation confidence="0.867435">
USC Information Sciences Institute
</affiliation>
<address confidence="0.887534">
4676 Admiralty Way
Marina del Rey, CA 90292-6695
</address>
<email confidence="0.999772">
{kozareva,hovy}@isi.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998877380952381">
Although many algorithms have been devel-
oped to harvest lexical resources, few organize
the mined terms into taxonomies. We pro-
pose (1) a semi-supervised algorithm that uses
a root concept, a basic level concept, and re-
cursive surface patterns to learn automatically
from the Web hyponym-hypernym pairs sub-
ordinated to the root; (2) a Web based concept
positioning procedure to validate the learned
pairs’ is-a relations; and (3) a graph algorithm
that derives from scratch the integrated tax-
onomy structure of all the terms. Comparing
results with WordNet, we find that the algo-
rithm misses some concepts and links, but also
that it discovers many additional ones lacking
in WordNet. We evaluate the taxonomization
power of our method on reconstructing parts
of the WordNet taxonomy. Experiments show
that starting from scratch, the algorithm can
reconstruct 62% of the WordNet taxonomy for
the regions tested.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957066666667">
A variety of NLP tasks, including inference, tex-
tual entailment (Glickman et al., 2005; Szpektor
et al., 2008), and question answering (Moldovan
et al., 1999), rely on semantic knowledge derived
from term taxonomies and thesauri such as Word-
Net. However, the coverage of WordNet is still lim-
ited in many regions (even well-studied ones such as
the concepts and instances below Animals and Peo-
ple), as noted by researchers such as (Pennacchiotti
and Pantel, 2006) and (Hovy et al., 2009) who per-
form automated semantic class learning. This hap-
pens because WordNet and most other existing tax-
onomies are manually created, which makes them
difficult to maintain in rapidly changing domains,
and (in the face of taxonomic complexity) makes
them hard to build with consistency. To surmount
these problems, it would be advantageous to have
an automatic procedure that can not only augment
existing resources but can also produce taxonomies
for existing and new domains and tasks starting from
scratch.
The main stages of automatic taxonomy induc-
tion are term extraction and term organization. In
recent years there has been a substantial amount of
work on term extraction, including semantic class
learning (Hearst, 1992; Riloff and Shepherd, 1997;
Etzioni et al., 2005; Pasca, 2004; Kozareva et al.,
2008), relation acquisition between entities (Girju
et al., 2003; Pantel and Pennacchiotti, 2006; Davi-
dov et al., 2007), and creation of concept lists (Katz
and Lin, 2003). Various attempts have been made to
learn the taxonomic organization of concepts (Wid-
dows, 2003; Snow et al., 2006; Yang and Callan,
2009). Among the most common is to start with a
good ontology and then to try to position the miss-
ing concepts into it. (Snow et al., 2006) maximize
the conditional probability of hyponym-hypernym
relations given certain evidence, while (Yang and
Callan, 2009) combines heterogenous features like
context, co-occurrence, and surface patterns to pro-
duce a more-inclusive inclusion ranking formula.
The obtained results are promising, but the problem
of how to organize the gathered knowledge when
there is no initial taxonomy, or when the initial tax-
onomy is grossly impoverished, still remains.
</bodyText>
<page confidence="0.92876">
1110
</page>
<note confidence="0.815755">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1110–1118,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997229888888889">
The major problem in performing taxonomy con-
struction from scratch is that overall concept po-
sitioning is not trivial. It is difficult to discover
whether concepts are unrelated, subordinated, or
parallel to each other. In this paper, we address the
following question: How can one induce the taxo-
nomic organization of concepts in a given domain
starting from scratch?
The contributions of this paper are as follows:
</bodyText>
<listItem confidence="0.9876887">
• An automatic procedure for harvesting
hyponym-hypernym pairs given a domain of
interest.
• A ranking mechanism for validating the learned
is-a relations between the pairs.
• A graph-based approach for inducing the taxo-
nomic organization of the harvested terms start-
ing from scratch.
• An experiment on reconstructing WordNet’s
taxonomy for given domains.
</listItem>
<bodyText confidence="0.999845222222222">
Before focusing on the harvesting and taxonomy
induction algorithms, we are going to describe some
basic terminology following (Hovy et al., 2009). A
term is an English word (for our current purposes,
a noun or a proper name). A concept is an item in
the classification taxonomy we are building. A root
concept is a fairly general concept which is located
on the high level of the taxonomy. A basic-level
concept corresponds to the Basic Level categories
defined in Prototype Theory in Psychology (Rosch,
1978). For example, a dog, not a mammal or a col-
lie. An instance is an item in the classification tax-
onomy that is more specific than a concept. For ex-
ample, Lassie, not a dog or collie .
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work. Section 3 describes the
taxonomization framework. Section 4 discusses the
experiments. We conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999505">
The first stage of automatic taxonomy induction,
term and relation extraction, is relatively well-
understood. Methods have matured to the point of
achieving high accuracy (Girju et al., 2003; Pantel
and Pennacchiotti, 2006; Kozareva et al., 2008). The
produced output typically contains flat lists of terms
and/or ground instance facts (lion is-a mammal)
and general relation types (mammal is-a animal).
Most approaches use either clustering or patterns
to mine knowledge from structured and unstructured
text. Clustering approaches (Lin, 1998; Lin and Pan-
tel, 2002; Davidov and Rappoport, 2006) are fully
unsupervised and discover relations that are not di-
rectly expressed in text. Their main drawback is that
they may or may not produce the term types and
granularities useful to the user. In contrast, pattern-
based approaches harvest information with high ac-
curacy, but they require a set of seeds and surface
patterns to initiate the learning process. These meth-
ods are successfully used to collect semantic lex-
icons (Riloff and Shepherd, 1997; Etzioni et al.,
2005; Pasca, 2004; Kozareva et al., 2008), encyclo-
pedic knowledge (Suchanek et al., 2007), concept
lists (Katz and Lin, 2003), and relations between
terms, such as hypernyms (Ritter et al., 2009; Hovy
et al., 2009) and part-of (Girju et al., 2003; Pantel
and Pennacchiotti, 2006).
However, simple term lists are not enough to solve
many problems involving natural language. Terms
may be augmented with information that is required
for knowledge-intensive tasks such as textual entail-
ment (Glickman et al., 2005; Szpektor et al., 2008)
and question answering (Moldovan et al., 1999). To
support inference, (Ritter et al., 2010) learn the se-
lectional restrictions of semantic relations, and (Pen-
nacchiotti and Pantel, 2006) ontologize the learned
arguments using WordNet.
Taxonomizing the terms is a very powerful
method to leverage added information. Subordi-
nated terms (hyponyms) inherit information from
their superordinates (hypernyms), making it unnec-
essary to learn all relevant information over and over
for every term in the language. But despite many at-
tempts, no ‘correct’ taxonomization has ever been
constructed for the terms of, say, English. Typically,
people build term taxonomies (and/or richer struc-
tures like ontologies) for particular purposes, using
specific taxonomization criteria. Different tasks and
criteria produce different taxonomies, even when us-
ing the same basic level concepts. This is because
most basic level concepts admit to multiple perspec-
tives, while each task focuses on one, or at most two,
perspectives at a time. For example, a dolphin is a
Mammal (and not a Fish) to a biologist, but is a Fish
</bodyText>
<page confidence="0.988633">
1111
</page>
<bodyText confidence="0.999913617021276">
(and hence not a Mammal) to a fisherman or anyone
building or visiting an aquarium. More confusingly,
a tiger and a puppy are both Mammals and hence
belong close together in a typical taxonomy, but a
tiger is a WildAnimal (in the perspective of Animal-
Function) and a JungleDweller (in the perspective of
Habitat), while a puppy is a Pet (as function) and a
HouseAnimal (as habitat), which would place them
relatively far from one another. Attempts at pro-
ducing a single multi-perspective taxonomy fail due
to the complexity of interaction among perspectives,
and people are notoriously bad at constructing tax-
onomies adherent to a single perspective when given
terms from multiple perspectives. This issue and the
major alternative principles for taxonomization are
discussed in (Hovy, 2002).
It is therefore not surprising that the second
stage of automated taxonomy induction is harder to
achieve. As mentioned, most attempts to learn tax-
onomy structures start with a reasonably complete
taxonomy and then insert the newly learned terms
into it, one term at a time (Widdows, 2003; Pasca,
2004; Snow et al., 2006; Yang and Callan, 2009).
(Snow et al., 2006) guide the incremental approach
by maximizing the conditional probability over a
set of relations. (Yang and Callan, 2009) introduce
a taxonomy induction framework which combines
the power of surface patterns and clustering through
combining numerous heterogeneous features.
Still, one would like a procedure to organize the
harvested terms into a taxonomic structure starting
fresh (i.e., without using an initial taxonomic struc-
ture). We propose an approach that bridges the gap
between the term extraction algorithms that focus
mainly on harvesting but do not taxonomize, and
those that accept a new term and seek to enrich an al-
ready existing taxonomy. Our aim is to perform both
stages: to extract the terms of a given domain and to
induce their taxonomic organization without any ini-
tial taxonomic structure and information. This task
is challenging because it is not trivial to discover
both the hierarchically related and the parallel (per-
spectival) organizations of concepts. Achieving this
goal can provide the research community with the
ability to produce taxonomies for domains for which
currently there are no existing or manually created
ontologies.
</bodyText>
<sectionHeader confidence="0.965598" genericHeader="method">
3 Building Taxonomies from Scratch
</sectionHeader>
<subsectionHeader confidence="0.998916">
3.1 Problem Formulation
</subsectionHeader>
<bodyText confidence="0.997874125">
We define our task as:
Task Definition: Given a root concept, a basic level
concept or an instance, and recursive lexico-syntactic
patterns, (1) harvest in bootstrapping fashion hy-
ponyms and hypernyms subordinated to the root; (2)
filter out erroneous information (extracted concepts
and isa relations); (3) organize the harvested con-
cepts into a taxonomy structure.
</bodyText>
<figure confidence="0.446288">
animal
</figure>
<figureCaption confidence="0.999356">
Figure 1: Taxonomy Induction from Scratch.
</figureCaption>
<bodyText confidence="0.9924436">
Figure 1 shows an example of the task. Start-
ing with the root concept animal and the basic
level concept lion, the algorithm learns new
terms like tiger, puma, deer, donkey of class
animal. Next for each basic level concept, the
algorithm harvests hypernyms and learns that a
lion is-a vertebrate, chordate, feline and mammal.
Finally, the taxonomic structure of each basic
level concept and its hypernyms is induced: ani-
mal—*chordate—*vertebrate—*mammal—*feline—*lion.
</bodyText>
<subsectionHeader confidence="0.999783">
3.2 Knowledge Harvesting
</subsectionHeader>
<bodyText confidence="0.999796111111111">
The main objective of our work is not the creation
of a new harvesting algorithm, but rather the or-
ganization of the harvested information in a tax-
onomy structure starting from scratch. There are
many algorithms for hyponym and hypernym har-
vesting from the Web. In our experiments, we use
the doubly-anchored lexico-syntactic patterns and
bootstrapping algorithm introduced by (Kozareva et
al., 2008) and (Hovy et al., 2009).
</bodyText>
<figure confidence="0.9968145">
deer
li0n
tiger
elephant
felines
vertebrates ch0rdates
carniv0res mammals herbiv0res
d0g
turtle
d0nkey
puma
cheetah
</figure>
<page confidence="0.991689">
1112
</page>
<bodyText confidence="0.999879">
We are interested in using this approach, because
it is: (1) simple and easy to implement; (2) requires
minimal supervision using only one root concept
and a term to learn new hyponyms and hypernyms
associated to the root; (3) reports higher precision
than current semantic class algorithms (Etzioni et
al., 2005; Pasca, 2004); and (4) adapts easily to dif-
ferent domains.
The general framework of the knowledge harvest-
ing algorithm is shown in Figure 2.
</bodyText>
<figure confidence="0.928079428571429">
1. Given:
a hyponym pattern Pi={concept such as seed
and *}
a hypernym pattern P,={* such as term, and
term2}
a root concept root
a term called seed for Pi
</figure>
<listItem confidence="0.968856333333333">
2. build a query using Pi
3. submit Pi to Yahoo! or other search engine
4. extract terms occupying the * position
5. take terms from step 4 and go to step 2.
6. repeat steps 2–5 until no new terms are found
7. rank terms by outDegree
8. for d terms with outDegree&gt;0, build a query
using P,
9. submit P, to Yahoo! or other search engine
10. extract concepts (hypernyms) occupying the *
position
11. rank concepts by inDegree
</listItem>
<figureCaption confidence="0.996959">
Figure 2: Knowledge Harvesting Framework.
</figureCaption>
<bodyText confidence="0.999812285714286">
The algorithm starts with a root concept, seed
term1 of type root and a doubly-anchored pattern
(DAP) such as ‘&lt;root&gt; such as &lt;seed&gt; and *’
which learns on the * position new terms of type
root. The newly learned terms, which can be either
instances, basic level or intermediate concepts, are
placed into the position of the seed in the DAP pat-
tern, and the bootstrapping process is repeated. The
process ceases when no new terms are found.
To separate the true from incorrect terms, we use
a graph-based algorithm in which each vertex u is
a term, and an each edge (u, v) E E corresponds
to the direction in which the term u discovered the
term v. The graph is weighted w(u, v) according
</bodyText>
<footnote confidence="0.751260333333333">
1The input term can be an instance, a basic level or an in-
termediate concept. An intermediate concept is the one that is
located between the basic level and root concepts.
</footnote>
<bodyText confidence="0.989713">
to the number of times the term pair u-v is seen
in unique web snippets. The terms are ranked by
</bodyText>
<equation confidence="0.994103333333333">
/EV(u,v)EE w(u,v)
outDegree(u)= which counts the
|V |−1
</equation>
<bodyText confidence="0.9988275">
number of outgoing links of node u normalized by
the total number of nodes in the graph excluding the
current. The algorithm considers as true terms with
outDegree&gt;0.
All harvested terms are automatically fed into the
hypernym extraction phase. We use the natural or-
der in which the terms discovered each other and
place them into an inverse doubly-anchored pattern
(DAP−1) ‘* such as &lt;term1&gt; and &lt;term2&gt;’ to
learn hypernyms on the * position. Similarly we
build a graph with nodes h denoting the hypernyms
and nodes t1-t2 denoting the term pairs. The edges
(h, t1 − t2) E E&apos; show the direction in which the
term pair discovered the hypernym. The hypernyms
are ranked by inDegree(h)= EV(t1−t2,h)EE, w(t1−
t2, h) which rewards hypernyms that are frequently
discovered by various term pairs. The output of
the algorithm is a list of is-a relations between the
learned terms (instances, basic level or intermediate
concepts) and their corresponding hypernyms. For
example, deer is-a herbivore, deer is-a ruminant,
deer is-a mammal.
</bodyText>
<subsectionHeader confidence="0.99889">
3.3 Graph-Based Taxonomy Induction
</subsectionHeader>
<bodyText confidence="0.999852944444444">
In the final stage of our algorithm, we induce the
overall taxonomic structure using information about
the pairwise positioning of the terms. In the knowl-
edge harvesting and filtering phases, the algorithm
learned is-a relations between the root and the terms
(instances, basic level or intermediate concepts), as
well as the harvested hypernyms and the terms. The
only missing information is the positioning of the in-
termediate concepts located between the basic level
and the root such as mammals, vertibrates, felines,
chordates, among others.
We introduce a concept positioning (CP) proce-
dure that uses a set of surface patterns: “X such as
Y”, “X are Y that”, “X including Y”, “X like Y”,
“such X as Y” to learn the hierarchical relations for
all possible concept pairs. For each concept pair,
say chordates and vertebrates, we issue the two
following queries:
</bodyText>
<listItem confidence="0.99304">
(a) chordates such as vertebrates
(b) vertebrates such as chordates
</listItem>
<page confidence="0.968547">
1113
</page>
<bodyText confidence="0.9992322">
If (a) returns more web hits than (b), then chordates
subsumes (or is broader than) vertebrates, other-
wise vertebrates subsumes chordates. For this
pair the such as pattern returned 7 hits for (a) and
0 hits for (b), so that the overall magnitude of the
direction of the relation is weak. To accumulate
stronger evidence, we issue web queries with the
remaining patterns. For the same concept pair, the
overall magnitude of “X including Y” is 5820 hits
for (a) and 0 for (b).
As shown in Figure 3, the concept positioning pat-
terns cannot always determine the direct taxonomic
organization between two concepts as in the case
of felines and chordates, felines and vertebrates.
One reason is that the concepts are located on dis-
tant taxonomic levels. We humans typically exem-
plify concepts using more proximate ones. There-
fore, the concept positioning procedure can find ev-
idence for the relation “mammals—*felines”, but not
for “chordates—*felines”.
</bodyText>
<figureCaption confidence="0.975078">
Figure 3: Concept Positioning and Induced Taxonomy.
</figureCaption>
<bodyText confidence="0.999773142857143">
After the concept positioning procedure has ex-
plored all concept pairs, we encounter two phenom-
ena: (1) direct links between some concepts are
missing and (2) multiple paths can be taken to reach
from one concept to another.
To surmount these problems, we employ a
graph based algorithm that finds the longest
path in the graph G&amp;quot;=(V&amp;quot;, E&amp;quot;). The nodes
V &amp;quot;={it1, h1, h2, .., hn, r} represent the input term,
its hypernyms, and the root. An edge (tm, tn) E E&amp;quot;
indicates that there is a path between the terms tm
and tn. The direction tm — tn indicates the term
subordination discovered during the CP procedure.
The objective is to find the longest path in G&amp;quot; be-
tween the root and the input term. Intuitively, find-
ing the longest paths is equivalent to finding the tax-
onomic organization of all concepts.
First, if present, we eliminate all cycles from the
graph. Then, we find all nodes that have no prede-
cessor and those that have no successor. Intuitively,
a node with no predecessors p is likely to be posi-
tioned on the top of the taxonomy (e.g. animal),
while a node with no successor s is likely to be lo-
cated at the bottom (e.g. terms like lion, tiger, puma,
or concepts like krill predators that could not be re-
lated to an instance or a basic level concept during
the CP procedure). We represent the directed graph
as an adjacency matrix A = [am,n], where am,n is
1 if (tm, tn) is an edge of G&amp;quot;, and 0 otherwise. For
each (p, s) pair, we find the list of all paths connect-
ing p with s. In the end, from all discovered can-
didate paths, the algorithm returns the longest one.
The same graph-based taxonomization procedure is
repeated for the rest of the basic level concepts and
their hypernyms.
</bodyText>
<sectionHeader confidence="0.998653" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999980533333333">
To evaluate the performance of a taxonomy induc-
tion algorithm, one can compare against a simple
taxonomy composed of 2–3 levels. However, one
cannot guarantee that the algorithm can learn larger
hierarchies completely or correctly.
Animals provide a good example of the true com-
plexity of concept organization: there are many
types, they are of numerous kinds, people take nu-
merous perspectives over them, and they are rela-
tively well-known to human annotators. In addition,
WordNet has a very rich and deep taxonomic struc-
ture for animals that can be used for direct compar-
ison. We further evaluate our algorithm on the do-
mains of Plants and Vehicles, which share some of
these properties.
</bodyText>
<subsectionHeader confidence="0.995738">
4.1 Data Collection
</subsectionHeader>
<bodyText confidence="0.999969555555555">
We have run the knowledge harvesting algorithm on
the semantic classes Animals, Plants and Vehicles
starting with only one seed example such as lions,
cucumbers and cars respectively.
First, we formed and submitted the DAP pattern
as web queries to Yahoo!Boss. We retrieved the
top 1000 web snippets for each query. We kept
all unique terms and term pairs. Second, we used
the learned term pairs to form and submit new web
</bodyText>
<figure confidence="0.998787333333333">
vertebrates
li.n
felines
animal
ch.rdates
mammals
animal
ch.rdates
vertebrates
mammals
felines
li.n
</figure>
<page confidence="0.995078">
1114
</page>
<bodyText confidence="0.998412461538462">
queries DAP−1. In this step, the algorithm harvested
the hypernyms associated with each term. We kept
all unique triples composed of a hypernym and the
term pairs that extracted it. The algorithm ran until
complete exhaustion for 8 iterations for Animals, 10
iterations for Plants and 18 iterations of Vehicles.
Table 1 shows the total number of terms extracted
by the Web harvesting algorithm during the first
stage. In addition, we show the number of terms that
passed the outDegree threshold. We found that the
majority of the learned terms for Animals are basic
level concepts, while for Plants and Vehicles they are
a mixture of basic level and intermediate concepts.
</bodyText>
<table confidence="0.987602333333333">
Animals Plants Vehicles
#Extracted Terms 1855 2801 1425
#outDegree(Term)&gt; 0 858 1262 581
</table>
<tableCaption confidence="0.999653">
Table 1: Learned Terms.
</tableCaption>
<bodyText confidence="0.9994406">
Since human based evaluation of all harvested
terms is time consuming and costly, we have se-
lected 90 terms located at the beginning, in the mid-
dle and in the end of the outDegree ranking. Table
2 summarizes the results.
</bodyText>
<table confidence="0.999724375">
Plants #CorrectByHand #inWN PrecByHand
rank[1-30] 29 28 .97
rank[420-450] 29 21 .97
rank[1232-1262] 27 19 .90
Vehicles #CorrectByHand #inWN PrecByHand
rank[1-30] 29 27 .97
rank[193-223] 22 18 .73
rank[551-581] 25 19 .83
</table>
<tableCaption confidence="0.999422">
Table 2: Term Evaluation.
</tableCaption>
<bodyText confidence="0.998145666666667">
Independently, we can say that the precision of the
harvesting algorithm is from 73 to 90%. In the case
of Vehicles, we found that the learned terms in the
middle ranking do not refer to the meaning of vehi-
cle as a transportation devise, but to the meaning of
vehicle as media (i.e. seminar, newspapers), com-
munication and marketing. For the same category,
the algorithm learned many terms which are missing
from WordNet such as BMW, bakkies, two-wheeler,
all-terrain-vehicle among others.
The second stage of the harvesting algorithm con-
cerns hypernym extraction. Table 3 shows the total
number of hypernyms harvested for all term pairs.
The top 20 highly ranked concepts by inDegree are
the most descriptive terms for the domain. However,
if we are interested in learning a larger set of hy-
pernyms, we found that inDegree is not sufficient
by itself. For example, highly frequent but irrele-
vant hypernyms such as meats, others are ranked
at the top of the list, while low frequent but rele-
vant ones such as protochordates, hooved-mammals,
homeotherms are discarded. This shows that we
need to develop additional and more sensitive mea-
sures for hypernym ranking.
</bodyText>
<table confidence="0.997243666666667">
Animals Plants Vehicles
#Extracted Hypernyms 1904 8947 2554
#inDegree(Hypernyms)&gt; 10 110 294 100
</table>
<tableCaption confidence="0.999734">
Table 3: Learned Hypernyms.
</tableCaption>
<bodyText confidence="0.8227806">
Table 4 shows some examples of the learned an-
imal hypernyms which were annotated by humans
as: correct but not present in WordNet; borderline
which depending on the application could be valu-
able to have or exclude; and incorrect.
</bodyText>
<table confidence="0.999470583333333">
CorrectNotInWN {colony|social} insects, grazers, monogastrics
camelid, {mammalian|land|areal} predators
{australian|african} wildlife, filter feeders
hard shelled invertebrates, pelagics
bottom dwellers
Borderline prehistoric animals, large herbivores
pocket pets, farm raised fish, roaring cats
endangered mammals, mysterious hunters
top predators, modern-snakes, heavy game
Incorrect frozen foods, native mammals, red meats
furry predators, others, resources, sorts
products, items, protein
</table>
<tableCaption confidence="0.999595">
Table 4: Examples of Learned Animal Hypernyms.
</tableCaption>
<bodyText confidence="0.997879">
The annotators found that 9% of the harvested is-
a relations are missing from WordNet. For example,
cartilaginous fish —* shark; colony insects—* bees;
filter feeders —* tube anemones among others. This
shows that despite its completeness, WordNet has
still room for improvement.
</bodyText>
<subsectionHeader confidence="0.991498">
4.2 A Test: Reconstructing WordNet
</subsectionHeader>
<bodyText confidence="0.999529571428572">
As previously discussed in (Hovy et al., 2009), it is
extremely difficult even for expert to manually con-
struct and evaluate the correctness of the harvested
taxonomies. Therefore, we decided to evaluate the
performance of our taxonomization approach recon-
structing WordNet Animals, Plants and Vehicles tax-
onomies.
</bodyText>
<page confidence="0.984368">
1115
</page>
<bodyText confidence="0.9995456">
Given a domain, we select from 140 to 170 of
the harvested terms. For each term, we retrieve all
WordNet hypernyms located on the path between the
input term and the root that is animal, plant or ve-
hicle depending on the domain of interest. We have
found that 98% of the WordNet terms are also har-
vested by our knowledge acquisition algorithm. This
means that being able to reconstruct WordNet’s tax-
onomy is equivalent to evaluating the performance
of our taxonomy induction approach.
Table 5 summarizes the characteristics of the tax-
onomies for the regions tested. For each domain,
we show the total number of terms that must be or-
ganized, and the total number of is-a relations that
must be induced.
</bodyText>
<table confidence="0.998480833333333">
Animals Plants Vehicles
#terms 684 554 140
#is-a 4327 2294 412
average depth 6.23 4.12 3.91
max depth 12 8 7
min depth 1 1 1
</table>
<tableCaption confidence="0.991679">
Table 5: Data for WordNet reconstruction.
</tableCaption>
<bodyText confidence="0.99998725">
Among the three domains we have tested, An-
imals is the most complex and richest one. The
maximum number of levels our algorithm must in-
fer is 11, the minimum is 1 and the average taxo-
nomic depth is 6.2. In total there are three basic level
concepts (longhorns, gaur and bullock) with maxi-
mum depth, twenty terms (basic level and intermedi-
ate concepts) with minimum depth and ninety-eight
terms (wombat, viper, rat, limpkin) with depth 6.
Plants is also a very challenging domain, because
it contains a mixture of scientific and general terms
such as magnoliopsida and flowering plant.
</bodyText>
<subsectionHeader confidence="0.99901">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999682">
To evaluate the performance of our taxonomy induc-
tion approach, we use the following measures:
</bodyText>
<construct confidence="0.672687">
Precision = #is−a found in WordNet and by system
#is−a found by system
Recall = #is−a found in WordNet and by system
#is−a found in WordNet
</construct>
<bodyText confidence="0.975560833333333">
Table 6 shows results of the taxonomy induction
of the Vehicles domain using different concept po-
sitioning patterns. The most productive ones are:
“X are Y that” and “X including Y”. However, the
highest yield is obtained when we combine evidence
from all patterns.
</bodyText>
<table confidence="0.998828428571429">
Vehicles Precision Recall
X such as Y .99 (174/175) .42 (174/410)
X are Y that .99 (206/208) .50 (206/410)
X including Y .96 (165/171) .40 (165/410)
X like Y .96 (137/142) .33 (137/410)
such X as Y .98 (44/45) .11 (44/410)
AllPatterns .99 (246/249) .60 ( 246/410)
</table>
<tableCaption confidence="0.999521">
Table 6: Evaluation of the Induced Vehicle Taxonomy.
</tableCaption>
<bodyText confidence="0.945334888888889">
Table 7 shows results of the taxonomization of
the Animals and Plants domains. Overall, the ob-
tained results are very encouraging given the fact
that we started from scratch without the usage of
any taxonomic structure. Precision is robust, but we
must further improve recall. Our observation for the
lower recall is that some intermediate concepts re-
late mostly to the high level ones, but not to the basic
level concepts.
</bodyText>
<table confidence="0.997893666666667">
Precision Recall
Animals .98 (1643/1688) .38 (1643/4327)
Plants .97 (905/931) .39 (905/2294)
</table>
<tableCaption confidence="0.958489">
Table 7: Evaluation of the Induced Animal and Plant Tax-
onomies.
</tableCaption>
<figureCaption confidence="0.7428574">
Figure 4 shows an example of the taxonomy in-
duced by our algorithm for the vipers, rats, wom-
bats, ducks, emus, moths and penguins basic level
concepts and their WordNet hypernyms.
Figure 4: Induced Taxonomy for Animals.
</figureCaption>
<bodyText confidence="0.939282">
The biggest challenge of the taxonomization pro-
cess is the merging of independent taxonomic per-
</bodyText>
<figure confidence="0.996691538461539">
snakes
vipers
reptiles birds metatherians mammals
emus
animals
aquatic_vertebrates chordates invertebrates
vertebrates arthropods
penguins ducks
aquatic_birds
marsupials rodents placentals
wombats rats
insects
moths
</figure>
<page confidence="0.989244">
1116
</page>
<bodyText confidence="0.99891625">
spectives (a deer is a grazer in BehaviorByFeeding,
a wildlife in BehaviorByHabitat, a herd in Behavior-
SocialGroup and an even-toed ungulate in Morpho-
logicalType) into a single hierarchy.
</bodyText>
<sectionHeader confidence="0.997824" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999988375">
We are encouraged by the ability of the taxonomiza-
tion algorithm to reconstruct WordNet’s Animal hi-
erarchy, which is one of its most complete and elab-
orated. In addition, we have also evaluated the per-
formance of our algorithm with the Plant and Vehi-
cle WordNet hierarchies.
Currently, our automated taxonomization algo-
rithm is able to build some of the quasi-independent
perspectival taxonomies (Hovy et al., 2009). How-
ever, further research is required to develop methods
that reliably (a) identify the number of independent
perspectives a concept can take (or seems to take in
the domain text), and (b) classify any harvested term
into one or more of them. The result would greatly
simplify the task of the taxonomization stage.
We note that despite this richness, WordNet has
many concepts like camelid, filter feeder, mono-
gastrics among others which are missing, but the
harvesting algorithm can provide. Another promis-
ing line of research would investigate the combina-
tion of the two styles of taxonomization algorithms:
first, the one described here to produce an initial (set
of) taxonomies, and second, the term-insertion algo-
rithms developed in prior work.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999915">
We acknowledge the support of DARPA contract
number FA8750-09-C-3705. We thank Mark John-
son for the valuable discussions on taxonomy evalu-
ation. We thank the reviewers for their useful feed-
back and suggestions.
</bodyText>
<sectionHeader confidence="0.988721" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.963293938461539">
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
ACL, pages 297–304.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 232–239.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91–134,
June.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 1–8.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
A probabilistic classification approach for lexical tex-
tual entailment. In Proceedings, The Twentieth Na-
tional Conference on Artificial Intelligence and the
Seventeenth Innovative Applications ofArtificialIntel-
ligence Conference, pages 1050–1055.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics, pages 539–
545.
Eduard H. Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction and
classification. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2009, pages 948–957.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tions in ontologies. The Semantics of Relationships:
An Interdisciplinary Perspective, pages 91–110.
Boris Katz and Jimmy Lin. 2003. Selectively using rela-
tions to improve precision in question answering. In In
Proceedings of the EACL-2003 Workshop on Natural
Language Processing for Question Answering, pages
43–50.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048–1056.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proceedings of the 19th international
conference on Computational linguistics, pages 1–7.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics, pages
768–774.
Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana Girju, and
Vasile Rus. 1999. Lasso: A tool for surfing the answer
net. In TREC.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
</reference>
<page confidence="0.886632">
1117
</page>
<reference confidence="0.999621795918368">
44th Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2006.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137–145.
Marco Pennacchiotti and Patrick Pantel. 2006. Ontolo-
gizing semantic relations. In ACL-44: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 793–800.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 117–
124.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of AAAI Spring Symposium on Learn-
ing by Reading and Learning to Read.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional prefer-
ences. In to appear in Proceedings of the Association
for Computational Linguistics ACL2010.
Eleanor Rosch. 1978. Principles of categorization.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ’07: Proceedings of the 16th international
conference on World Wide Web, pages 697–706.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In ACL
2008, Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics, pages
683–691.
Dominic Widdows. 2003. Unsupervised methods for de-
veloping taxonomies by combining syntactic and sta-
tistical information. In Proceedings of HLT-NAACL.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
ACL-IJCNLP ’09: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1, pages 271–279.
</reference>
<page confidence="0.995509">
1118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636508">
<title confidence="0.9167355">A Semi-Supervised Method to Learn and Construct Taxonomies using the Web</title>
<author confidence="0.972866">Zornitsa Kozareva</author>
<author confidence="0.972866">Eduard</author>
<affiliation confidence="0.996486">USC Information Sciences</affiliation>
<address confidence="0.988538">4676 Admiralty</address>
<author confidence="0.795693">Marina del Rey</author>
<author confidence="0.795693">CA</author>
<abstract confidence="0.999069181818182">Although many algorithms have been developed to harvest lexical resources, few organize the mined terms into taxonomies. We propose (1) a semi-supervised algorithm that uses a root concept, a basic level concept, and recursive surface patterns to learn automatically from the Web hyponym-hypernym pairs subordinated to the root; (2) a Web based concept positioning procedure to validate the learned pairs’ is-a relations; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="5817" citStr="Davidov and Rappoport, 2006" startWordPosition="912" endWordPosition="915">lude in Section 5. 2 Related Work The first stage of automatic taxonomy induction, term and relation extraction, is relatively wellunderstood. Methods have matured to the point of achieving high accuracy (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Kozareva et al., 2008). The produced output typically contains flat lists of terms and/or ground instance facts (lion is-a mammal) and general relation types (mammal is-a animal). Most approaches use either clustering or patterns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshel Koppel</author>
</authors>
<title>Fully unsupervised discovery of conceptspecific relationships by web mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>232--239</pages>
<contexts>
<context position="2565" citStr="Davidov et al., 2007" startWordPosition="396" endWordPosition="400">oblems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like context, co-occurrence, and surface patterns to produce a more-inclusive inclusion ranking formula. The obtained resu</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport, and Moshel Koppel. 2007. Fully unsupervised discovery of conceptspecific relationships by web mining. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="2414" citStr="Etzioni et al., 2005" startWordPosition="374" endWordPosition="377">ult to maintain in rapidly changing domains, and (in the face of taxonomic complexity) makes them hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) co</context>
<context position="6291" citStr="Etzioni et al., 2005" startWordPosition="991" endWordPosition="994">erns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference,</context>
<context position="12081" citStr="Etzioni et al., 2005" startWordPosition="1892" endWordPosition="1895">rvesting from the Web. In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al., 2009). deer li0n tiger elephant felines vertebrates ch0rdates carniv0res mammals herbiv0res d0g turtle d0nkey puma cheetah 1112 We are interested in using this approach, because it is: (1) simple and easy to implement; (2) requires minimal supervision using only one root concept and a term to learn new hyponyms and hypernyms associated to the root; (3) reports higher precision than current semantic class algorithms (Etzioni et al., 2005; Pasca, 2004); and (4) adapts easily to different domains. The general framework of the knowledge harvesting algorithm is shown in Figure 2. 1. Given: a hyponym pattern Pi={concept such as seed and *} a hypernym pattern P,={* such as term, and term2} a root concept root a term called seed for Pi 2. build a query using Pi 3. submit Pi to Yahoo! or other search engine 4. extract terms occupying the * position 5. take terms from step 4 and go to step 2. 6. repeat steps 2–5 until no new terms are found 7. rank terms by outDegree 8. for d terms with outDegree&gt;0, build a query using P, 9. submit P,</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2510" citStr="Girju et al., 2003" startWordPosition="388" endWordPosition="391">hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like context, co-occurrence, and surface patterns to produce a more</context>
<context position="5412" citStr="Girju et al., 2003" startWordPosition="853" endWordPosition="856">type Theory in Psychology (Rosch, 1978). For example, a dog, not a mammal or a collie. An instance is an item in the classification taxonomy that is more specific than a concept. For example, Lassie, not a dog or collie . The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 describes the taxonomization framework. Section 4 discusses the experiments. We conclude in Section 5. 2 Related Work The first stage of automatic taxonomy induction, term and relation extraction, is relatively wellunderstood. Methods have matured to the point of achieving high accuracy (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Kozareva et al., 2008). The produced output typically contains flat lists of terms and/or ground instance facts (lion is-a mammal) and general relation types (mammal is-a animal). Most approaches use either clustering or patterns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>A probabilistic classification approach for lexical textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings, The Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative Applications ofArtificialIntelligence Conference,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1237" citStr="Glickman et al., 2005" startWordPosition="186" endWordPosition="189">earned pairs’ is-a relations; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested. 1 Introduction A variety of NLP tasks, including inference, textual entailment (Glickman et al., 2005; Szpektor et al., 2008), and question answering (Moldovan et al., 1999), rely on semantic knowledge derived from term taxonomies and thesauri such as WordNet. However, the coverage of WordNet is still limited in many regions (even well-studied ones such as the concepts and instances below Animals and People), as noted by researchers such as (Pennacchiotti and Pantel, 2006) and (Hovy et al., 2009) who perform automated semantic class learning. This happens because WordNet and most other existing taxonomies are manually created, which makes them difficult to maintain in rapidly changing domains</context>
<context position="6797" citStr="Glickman et al., 2005" startWordPosition="1070" endWordPosition="1073">. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Taxonomizing the terms is a very powerful method to leverage added information. Subordinated terms (hyponyms) inherit information from their superordinates (hypernyms), making it unnecessary to learn all relevant information over and over for every term in the language. But despite many attempts, no ‘correct’ taxonomization has ever been </context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. A probabilistic classification approach for lexical textual entailment. In Proceedings, The Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative Applications ofArtificialIntelligence Conference, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="2365" citStr="Hearst, 1992" startWordPosition="368" endWordPosition="369">manually created, which makes them difficult to maintain in rapidly changing domains, and (in the face of taxonomic complexity) makes them hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given c</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguistics, pages 539– 545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
</authors>
<title>Toward completeness in concept extraction and classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>948--957</pages>
<contexts>
<context position="1637" citStr="Hovy et al., 2009" startWordPosition="252" endWordPosition="255">s show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested. 1 Introduction A variety of NLP tasks, including inference, textual entailment (Glickman et al., 2005; Szpektor et al., 2008), and question answering (Moldovan et al., 1999), rely on semantic knowledge derived from term taxonomies and thesauri such as WordNet. However, the coverage of WordNet is still limited in many regions (even well-studied ones such as the concepts and instances below Animals and People), as noted by researchers such as (Pennacchiotti and Pantel, 2006) and (Hovy et al., 2009) who perform automated semantic class learning. This happens because WordNet and most other existing taxonomies are manually created, which makes them difficult to maintain in rapidly changing domains, and (in the face of taxonomic complexity) makes them hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization.</context>
<context position="4469" citStr="Hovy et al., 2009" startWordPosition="689" endWordPosition="692"> organization of concepts in a given domain starting from scratch? The contributions of this paper are as follows: • An automatic procedure for harvesting hyponym-hypernym pairs given a domain of interest. • A ranking mechanism for validating the learned is-a relations between the pairs. • A graph-based approach for inducing the taxonomic organization of the harvested terms starting from scratch. • An experiment on reconstructing WordNet’s taxonomy for given domains. Before focusing on the harvesting and taxonomy induction algorithms, we are going to describe some basic terminology following (Hovy et al., 2009). A term is an English word (for our current purposes, a noun or a proper name). A concept is an item in the classification taxonomy we are building. A root concept is a fairly general concept which is located on the high level of the taxonomy. A basic-level concept corresponds to the Basic Level categories defined in Prototype Theory in Psychology (Rosch, 1978). For example, a dog, not a mammal or a collie. An instance is an item in the classification taxonomy that is more specific than a concept. For example, Lassie, not a dog or collie . The rest of the paper is organized as follows. Sectio</context>
<context position="6501" citStr="Hovy et al., 2009" startWordPosition="1025" endWordPosition="1028">ly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Taxonomizing the terms is a very powerful me</context>
<context position="11646" citStr="Hovy et al., 2009" startWordPosition="1825" endWordPosition="1828">rdate, feline and mammal. Finally, the taxonomic structure of each basic level concept and its hypernyms is induced: animal—*chordate—*vertebrate—*mammal—*feline—*lion. 3.2 Knowledge Harvesting The main objective of our work is not the creation of a new harvesting algorithm, but rather the organization of the harvested information in a taxonomy structure starting from scratch. There are many algorithms for hyponym and hypernym harvesting from the Web. In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al., 2009). deer li0n tiger elephant felines vertebrates ch0rdates carniv0res mammals herbiv0res d0g turtle d0nkey puma cheetah 1112 We are interested in using this approach, because it is: (1) simple and easy to implement; (2) requires minimal supervision using only one root concept and a term to learn new hyponyms and hypernyms associated to the root; (3) reports higher precision than current semantic class algorithms (Etzioni et al., 2005; Pasca, 2004); and (4) adapts easily to different domains. The general framework of the knowledge harvesting algorithm is shown in Figure 2. 1. Given: a hyponym pat</context>
<context position="24321" citStr="Hovy et al., 2009" startWordPosition="3792" endWordPosition="3795"> roaring cats endangered mammals, mysterious hunters top predators, modern-snakes, heavy game Incorrect frozen foods, native mammals, red meats furry predators, others, resources, sorts products, items, protein Table 4: Examples of Learned Animal Hypernyms. The annotators found that 9% of the harvested isa relations are missing from WordNet. For example, cartilaginous fish —* shark; colony insects—* bees; filter feeders —* tube anemones among others. This shows that despite its completeness, WordNet has still room for improvement. 4.2 A Test: Reconstructing WordNet As previously discussed in (Hovy et al., 2009), it is extremely difficult even for expert to manually construct and evaluate the correctness of the harvested taxonomies. Therefore, we decided to evaluate the performance of our taxonomization approach reconstructing WordNet Animals, Plants and Vehicles taxonomies. 1115 Given a domain, we select from 140 to 170 of the harvested terms. For each term, we retrieve all WordNet hypernyms located on the path between the input term and the root that is animal, plant or vehicle depending on the domain of interest. We have found that 98% of the WordNet terms are also harvested by our knowledge acqui</context>
<context position="28632" citStr="Hovy et al., 2009" startWordPosition="4499" endWordPosition="4502">er is a grazer in BehaviorByFeeding, a wildlife in BehaviorByHabitat, a herd in BehaviorSocialGroup and an even-toed ungulate in MorphologicalType) into a single hierarchy. 5 Conclusions and Future Work We are encouraged by the ability of the taxonomization algorithm to reconstruct WordNet’s Animal hierarchy, which is one of its most complete and elaborated. In addition, we have also evaluated the performance of our algorithm with the Plant and Vehicle WordNet hierarchies. Currently, our automated taxonomization algorithm is able to build some of the quasi-independent perspectival taxonomies (Hovy et al., 2009). However, further research is required to develop methods that reliably (a) identify the number of independent perspectives a concept can take (or seems to take in the domain text), and (b) classify any harvested term into one or more of them. The result would greatly simplify the task of the taxonomization stage. We note that despite this richness, WordNet has many concepts like camelid, filter feeder, monogastrics among others which are missing, but the harvesting algorithm can provide. Another promising line of research would investigate the combination of the two styles of taxonomization </context>
</contexts>
<marker>Hovy, Kozareva, Riloff, 2009</marker>
<rawString>Eduard H. Hovy, Zornitsa Kozareva, and Ellen Riloff. 2009. Toward completeness in concept extraction and classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP 2009, pages 948–957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Comparing sets of semantic relations in ontologies. The Semantics of Relationships: An Interdisciplinary Perspective,</title>
<date>2002</date>
<pages>91--110</pages>
<contexts>
<context position="8719" citStr="Hovy, 2002" startWordPosition="1374" endWordPosition="1375">cal taxonomy, but a tiger is a WildAnimal (in the perspective of AnimalFunction) and a JungleDweller (in the perspective of Habitat), while a puppy is a Pet (as function) and a HouseAnimal (as habitat), which would place them relatively far from one another. Attempts at producing a single multi-perspective taxonomy fail due to the complexity of interaction among perspectives, and people are notoriously bad at constructing taxonomies adherent to a single perspective when given terms from multiple perspectives. This issue and the major alternative principles for taxonomization are discussed in (Hovy, 2002). It is therefore not surprising that the second stage of automated taxonomy induction is harder to achieve. As mentioned, most attempts to learn taxonomy structures start with a reasonably complete taxonomy and then insert the newly learned terms into it, one term at a time (Widdows, 2003; Pasca, 2004; Snow et al., 2006; Yang and Callan, 2009). (Snow et al., 2006) guide the incremental approach by maximizing the conditional probability over a set of relations. (Yang and Callan, 2009) introduce a taxonomy induction framework which combines the power of surface patterns and clustering through c</context>
</contexts>
<marker>Hovy, 2002</marker>
<rawString>Eduard Hovy. 2002. Comparing sets of semantic relations in ontologies. The Semantics of Relationships: An Interdisciplinary Perspective, pages 91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boris Katz</author>
<author>Jimmy Lin</author>
</authors>
<title>Selectively using relations to improve precision in question answering. In</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL-2003 Workshop on Natural Language Processing for Question Answering,</booktitle>
<pages>43--50</pages>
<contexts>
<context position="2617" citStr="Katz and Lin, 2003" startWordPosition="406" endWordPosition="409">procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like context, co-occurrence, and surface patterns to produce a more-inclusive inclusion ranking formula. The obtained results are promising, but the problem of how to organiz</context>
<context position="6412" citStr="Katz and Lin, 2003" startWordPosition="1010" endWordPosition="1013">dov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ont</context>
</contexts>
<marker>Katz, Lin, 2003</marker>
<rawString>Boris Katz and Jimmy Lin. 2003. Selectively using relations to improve precision in question answering. In In Proceedings of the EACL-2003 Workshop on Natural Language Processing for Question Answering, pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="2451" citStr="Kozareva et al., 2008" startWordPosition="380" endWordPosition="383"> domains, and (in the face of taxonomic complexity) makes them hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like con</context>
<context position="5468" citStr="Kozareva et al., 2008" startWordPosition="861" endWordPosition="864">, a dog, not a mammal or a collie. An instance is an item in the classification taxonomy that is more specific than a concept. For example, Lassie, not a dog or collie . The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 describes the taxonomization framework. Section 4 discusses the experiments. We conclude in Section 5. 2 Related Work The first stage of automatic taxonomy induction, term and relation extraction, is relatively wellunderstood. Methods have matured to the point of achieving high accuracy (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Kozareva et al., 2008). The produced output typically contains flat lists of terms and/or ground instance facts (lion is-a mammal) and general relation types (mammal is-a animal). Most approaches use either clustering or patterns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest informati</context>
<context position="11622" citStr="Kozareva et al., 2008" startWordPosition="1820" endWordPosition="1823"> a lion is-a vertebrate, chordate, feline and mammal. Finally, the taxonomic structure of each basic level concept and its hypernyms is induced: animal—*chordate—*vertebrate—*mammal—*feline—*lion. 3.2 Knowledge Harvesting The main objective of our work is not the creation of a new harvesting algorithm, but rather the organization of the harvested information in a taxonomy structure starting from scratch. There are many algorithms for hyponym and hypernym harvesting from the Web. In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al., 2009). deer li0n tiger elephant felines vertebrates ch0rdates carniv0res mammals herbiv0res d0g turtle d0nkey puma cheetah 1112 We are interested in using this approach, because it is: (1) simple and easy to implement; (2) requires minimal supervision using only one root concept and a term to learn new hyponyms and hypernyms associated to the root; (3) reports higher precision than current semantic class algorithms (Etzioni et al., 2005; Pasca, 2004); and (4) adapts easily to different domains. The general framework of the knowledge harvesting algorithm is shown in Figure 2.</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of ACL-08: HLT, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="5787" citStr="Lin and Pantel, 2002" startWordPosition="907" endWordPosition="911">e experiments. We conclude in Section 5. 2 Related Work The first stage of automatic taxonomy induction, term and relation extraction, is relatively wellunderstood. Methods have matured to the point of achieving high accuracy (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Kozareva et al., 2008). The produced output typically contains flat lists of terms and/or ground instance facts (lion is-a mammal) and general relation types (mammal is-a animal). Most approaches use either clustering or patterns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept l</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>Dekang Lin and Patrick Pantel. 2002. Concept discovery from text. In Proceedings of the 19th international conference on Computational linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="5765" citStr="Lin, 1998" startWordPosition="905" endWordPosition="906">iscusses the experiments. We conclude in Section 5. 2 Related Work The first stage of automatic taxonomy induction, term and relation extraction, is relatively wellunderstood. Methods have matured to the point of achieving high accuracy (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Kozareva et al., 2008). The produced output typically contains flat lists of terms and/or ground instance facts (lion is-a mammal) and general relation types (mammal is-a animal). Most approaches use either clustering or patterns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguistics, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan I Moldovan</author>
<author>Sanda M Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Richard Goodrum</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
</authors>
<title>Lasso: A tool for surfing the answer net.</title>
<date>1999</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="1309" citStr="Moldovan et al., 1999" startWordPosition="197" endWordPosition="200">om scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested. 1 Introduction A variety of NLP tasks, including inference, textual entailment (Glickman et al., 2005; Szpektor et al., 2008), and question answering (Moldovan et al., 1999), rely on semantic knowledge derived from term taxonomies and thesauri such as WordNet. However, the coverage of WordNet is still limited in many regions (even well-studied ones such as the concepts and instances below Animals and People), as noted by researchers such as (Pennacchiotti and Pantel, 2006) and (Hovy et al., 2009) who perform automated semantic class learning. This happens because WordNet and most other existing taxonomies are manually created, which makes them difficult to maintain in rapidly changing domains, and (in the face of taxonomic complexity) makes them hard to build wit</context>
<context position="6868" citStr="Moldovan et al., 1999" startWordPosition="1081" endWordPosition="1084">loff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Taxonomizing the terms is a very powerful method to leverage added information. Subordinated terms (hyponyms) inherit information from their superordinates (hypernyms), making it unnecessary to learn all relevant information over and over for every term in the language. But despite many attempts, no ‘correct’ taxonomization has ever been constructed for the terms of, say, English. Typically, people build ter</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Goodrum, Girju, Rus, 1999</marker>
<rawString>Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum, Roxana Girju, and Vasile Rus. 1999. Lasso: A tool for surfing the answer net. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<contexts>
<context position="2542" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="392" endWordPosition="395">onsistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like context, co-occurrence, and surface patterns to produce a more-inclusive inclusion ranking for</context>
<context position="5444" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="857" endWordPosition="860">ology (Rosch, 1978). For example, a dog, not a mammal or a collie. An instance is an item in the classification taxonomy that is more specific than a concept. For example, Lassie, not a dog or collie . The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 describes the taxonomization framework. Section 4 discusses the experiments. We conclude in Section 5. 2 Related Work The first stage of automatic taxonomy induction, term and relation extraction, is relatively wellunderstood. Methods have matured to the point of achieving high accuracy (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Kozareva et al., 2008). The produced output typically contains flat lists of terms and/or ground instance facts (lion is-a mammal) and general relation types (mammal is-a animal). Most approaches use either clustering or patterns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased appr</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pasca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>137--145</pages>
<contexts>
<context position="2427" citStr="Pasca, 2004" startWordPosition="378" endWordPosition="379">idly changing domains, and (in the face of taxonomic complexity) makes them hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines hetero</context>
<context position="6304" citStr="Pasca, 2004" startWordPosition="995" endWordPosition="996"> from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et a</context>
<context position="9022" citStr="Pasca, 2004" startWordPosition="1424" endWordPosition="1425">tive taxonomy fail due to the complexity of interaction among perspectives, and people are notoriously bad at constructing taxonomies adherent to a single perspective when given terms from multiple perspectives. This issue and the major alternative principles for taxonomization are discussed in (Hovy, 2002). It is therefore not surprising that the second stage of automated taxonomy induction is harder to achieve. As mentioned, most attempts to learn taxonomy structures start with a reasonably complete taxonomy and then insert the newly learned terms into it, one term at a time (Widdows, 2003; Pasca, 2004; Snow et al., 2006; Yang and Callan, 2009). (Snow et al., 2006) guide the incremental approach by maximizing the conditional probability over a set of relations. (Yang and Callan, 2009) introduce a taxonomy induction framework which combines the power of surface patterns and clustering through combining numerous heterogeneous features. Still, one would like a procedure to organize the harvested terms into a taxonomic structure starting fresh (i.e., without using an initial taxonomic structure). We propose an approach that bridges the gap between the term extraction algorithms that focus mainl</context>
<context position="12095" citStr="Pasca, 2004" startWordPosition="1896" endWordPosition="1897"> In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al., 2009). deer li0n tiger elephant felines vertebrates ch0rdates carniv0res mammals herbiv0res d0g turtle d0nkey puma cheetah 1112 We are interested in using this approach, because it is: (1) simple and easy to implement; (2) requires minimal supervision using only one root concept and a term to learn new hyponyms and hypernyms associated to the root; (3) reports higher precision than current semantic class algorithms (Etzioni et al., 2005; Pasca, 2004); and (4) adapts easily to different domains. The general framework of the knowledge harvesting algorithm is shown in Figure 2. 1. Given: a hyponym pattern Pi={concept such as seed and *} a hypernym pattern P,={* such as term, and term2} a root concept root a term called seed for Pi 2. build a query using Pi 3. submit Pi to Yahoo! or other search engine 4. extract terms occupying the * position 5. take terms from step 4 and go to step 2. 6. repeat steps 2–5 until no new terms are found 7. rank terms by outDegree 8. for d terms with outDegree&gt;0, build a query using P, 9. submit P, to Yahoo! or </context>
</contexts>
<marker>Pasca, 2004</marker>
<rawString>Marius Pasca. 2004. Acquisition of categorized named entities for web search. In Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Patrick Pantel</author>
</authors>
<title>Ontologizing semantic relations.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>793--800</pages>
<contexts>
<context position="1613" citStr="Pennacchiotti and Pantel, 2006" startWordPosition="247" endWordPosition="250">s of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested. 1 Introduction A variety of NLP tasks, including inference, textual entailment (Glickman et al., 2005; Szpektor et al., 2008), and question answering (Moldovan et al., 1999), rely on semantic knowledge derived from term taxonomies and thesauri such as WordNet. However, the coverage of WordNet is still limited in many regions (even well-studied ones such as the concepts and instances below Animals and People), as noted by researchers such as (Pennacchiotti and Pantel, 2006) and (Hovy et al., 2009) who perform automated semantic class learning. This happens because WordNet and most other existing taxonomies are manually created, which makes them difficult to maintain in rapidly changing domains, and (in the face of taxonomic complexity) makes them hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extractio</context>
<context position="7008" citStr="Pennacchiotti and Pantel, 2006" startWordPosition="1101" endWordPosition="1105"> concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Taxonomizing the terms is a very powerful method to leverage added information. Subordinated terms (hyponyms) inherit information from their superordinates (hypernyms), making it unnecessary to learn all relevant information over and over for every term in the language. But despite many attempts, no ‘correct’ taxonomization has ever been constructed for the terms of, say, English. Typically, people build term taxonomies (and/or richer structures like ontologies) for particular purposes, using specific taxonomization criteria. Different tasks and</context>
</contexts>
<marker>Pennacchiotti, Pantel, 2006</marker>
<rawString>Marco Pennacchiotti and Patrick Pantel. 2006. Ontologizing semantic relations. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 793–800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Jessica Shepherd</author>
</authors>
<title>A CorpusBased Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="2392" citStr="Riloff and Shepherd, 1997" startWordPosition="370" endWordPosition="373">ed, which makes them difficult to maintain in rapidly changing domains, and (in the face of taxonomic complexity) makes them hard to build with consistency. To surmount these problems, it would be advantageous to have an automatic procedure that can not only augment existing resources but can also produce taxonomies for existing and new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yan</context>
<context position="6269" citStr="Riloff and Shepherd, 1997" startWordPosition="987" endWordPosition="990">e either clustering or patterns to mine knowledge from structured and unstructured text. Clustering approaches (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999).</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>Ellen Riloff and Jessica Shepherd. 1997. A CorpusBased Approach for Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of AAAI Spring Symposium on Learning by Reading and Learning to Read.</booktitle>
<contexts>
<context position="6481" citStr="Ritter et al., 2009" startWordPosition="1021" endWordPosition="1024">s that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Taxonomizing the terms i</context>
</contexts>
<marker>Ritter, Soderland, Etzioni, 2009</marker>
<rawString>Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of AAAI Spring Symposium on Learning by Reading and Learning to Read.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In to appear in Proceedings of the Association for Computational Linguistics ACL2010.</booktitle>
<contexts>
<context position="6913" citStr="Ritter et al., 2010" startWordPosition="1088" endWordPosition="1091">Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Taxonomizing the terms is a very powerful method to leverage added information. Subordinated terms (hyponyms) inherit information from their superordinates (hypernyms), making it unnecessary to learn all relevant information over and over for every term in the language. But despite many attempts, no ‘correct’ taxonomization has ever been constructed for the terms of, say, English. Typically, people build term taxonomies (and/or richer structures like o</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In to appear in Proceedings of the Association for Computational Linguistics ACL2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Rosch</author>
</authors>
<title>Principles of categorization.</title>
<date>1978</date>
<contexts>
<context position="4833" citStr="Rosch, 1978" startWordPosition="754" endWordPosition="755"> terms starting from scratch. • An experiment on reconstructing WordNet’s taxonomy for given domains. Before focusing on the harvesting and taxonomy induction algorithms, we are going to describe some basic terminology following (Hovy et al., 2009). A term is an English word (for our current purposes, a noun or a proper name). A concept is an item in the classification taxonomy we are building. A root concept is a fairly general concept which is located on the high level of the taxonomy. A basic-level concept corresponds to the Basic Level categories defined in Prototype Theory in Psychology (Rosch, 1978). For example, a dog, not a mammal or a collie. An instance is an item in the classification taxonomy that is more specific than a concept. For example, Lassie, not a dog or collie . The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 describes the taxonomization framework. Section 4 discusses the experiments. We conclude in Section 5. 2 Related Work The first stage of automatic taxonomy induction, term and relation extraction, is relatively wellunderstood. Methods have matured to the point of achieving high accuracy (Girju et al., 2003; Pantel and Pennacch</context>
</contexts>
<marker>Rosch, 1978</marker>
<rawString>Eleanor Rosch. 1978. Principles of categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, ACL.</booktitle>
<contexts>
<context position="2732" citStr="Snow et al., 2006" startWordPosition="425" endWordPosition="428">nd tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like context, co-occurrence, and surface patterns to produce a more-inclusive inclusion ranking formula. The obtained results are promising, but the problem of how to organize the gathered knowledge when there is no initial taxonomy, or when the initial taxonomy is grossly impoverished, s</context>
<context position="9041" citStr="Snow et al., 2006" startWordPosition="1426" endWordPosition="1429"> fail due to the complexity of interaction among perspectives, and people are notoriously bad at constructing taxonomies adherent to a single perspective when given terms from multiple perspectives. This issue and the major alternative principles for taxonomization are discussed in (Hovy, 2002). It is therefore not surprising that the second stage of automated taxonomy induction is harder to achieve. As mentioned, most attempts to learn taxonomy structures start with a reasonably complete taxonomy and then insert the newly learned terms into it, one term at a time (Widdows, 2003; Pasca, 2004; Snow et al., 2006; Yang and Callan, 2009). (Snow et al., 2006) guide the incremental approach by maximizing the conditional probability over a set of relations. (Yang and Callan, 2009) introduce a taxonomy induction framework which combines the power of surface patterns and clustering through combining numerous heterogeneous features. Still, one would like a procedure to organize the harvested terms into a taxonomic structure starting fresh (i.e., without using an initial taxonomic structure). We propose an approach that bridges the gap between the term extraction algorithms that focus mainly on harvesting but</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In WWW ’07: Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<contexts>
<context position="6376" citStr="Suchanek et al., 2007" startWordPosition="1004" endWordPosition="1007"> (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006) are fully unsupervised and discover relations that are not directly expressed in text. Their main drawback is that they may or may not produce the term types and granularities useful to the user. In contrast, patternbased approaches harvest information with high accuracy, but they require a set of seeds and surface patterns to initiate the learning process. These methods are successfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and </context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
<author>Roy Bar-Haim</author>
<author>Jacob Goldberger</author>
</authors>
<title>Contextual preferences.</title>
<date>2008</date>
<booktitle>In ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>683--691</pages>
<contexts>
<context position="1261" citStr="Szpektor et al., 2008" startWordPosition="190" endWordPosition="193">tions; and (3) a graph algorithm that derives from scratch the integrated taxonomy structure of all the terms. Comparing results with WordNet, we find that the algorithm misses some concepts and links, but also that it discovers many additional ones lacking in WordNet. We evaluate the taxonomization power of our method on reconstructing parts of the WordNet taxonomy. Experiments show that starting from scratch, the algorithm can reconstruct 62% of the WordNet taxonomy for the regions tested. 1 Introduction A variety of NLP tasks, including inference, textual entailment (Glickman et al., 2005; Szpektor et al., 2008), and question answering (Moldovan et al., 1999), rely on semantic knowledge derived from term taxonomies and thesauri such as WordNet. However, the coverage of WordNet is still limited in many regions (even well-studied ones such as the concepts and instances below Animals and People), as noted by researchers such as (Pennacchiotti and Pantel, 2006) and (Hovy et al., 2009) who perform automated semantic class learning. This happens because WordNet and most other existing taxonomies are manually created, which makes them difficult to maintain in rapidly changing domains, and (in the face of ta</context>
<context position="6821" citStr="Szpektor et al., 2008" startWordPosition="1074" endWordPosition="1077">cessfully used to collect semantic lexicons (Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), encyclopedic knowledge (Suchanek et al., 2007), concept lists (Katz and Lin, 2003), and relations between terms, such as hypernyms (Ritter et al., 2009; Hovy et al., 2009) and part-of (Girju et al., 2003; Pantel and Pennacchiotti, 2006). However, simple term lists are not enough to solve many problems involving natural language. Terms may be augmented with information that is required for knowledge-intensive tasks such as textual entailment (Glickman et al., 2005; Szpektor et al., 2008) and question answering (Moldovan et al., 1999). To support inference, (Ritter et al., 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Taxonomizing the terms is a very powerful method to leverage added information. Subordinated terms (hyponyms) inherit information from their superordinates (hypernyms), making it unnecessary to learn all relevant information over and over for every term in the language. But despite many attempts, no ‘correct’ taxonomization has ever been constructed for the term</context>
</contexts>
<marker>Szpektor, Dagan, Bar-Haim, Goldberger, 2008</marker>
<rawString>Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob Goldberger. 2008. Contextual preferences. In ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 683–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="2713" citStr="Widdows, 2003" startWordPosition="422" endWordPosition="424">d new domains and tasks starting from scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like context, co-occurrence, and surface patterns to produce a more-inclusive inclusion ranking formula. The obtained results are promising, but the problem of how to organize the gathered knowledge when there is no initial taxonomy, or when the initial taxonomy is gros</context>
<context position="9009" citStr="Widdows, 2003" startWordPosition="1422" endWordPosition="1423">e multi-perspective taxonomy fail due to the complexity of interaction among perspectives, and people are notoriously bad at constructing taxonomies adherent to a single perspective when given terms from multiple perspectives. This issue and the major alternative principles for taxonomization are discussed in (Hovy, 2002). It is therefore not surprising that the second stage of automated taxonomy induction is harder to achieve. As mentioned, most attempts to learn taxonomy structures start with a reasonably complete taxonomy and then insert the newly learned terms into it, one term at a time (Widdows, 2003; Pasca, 2004; Snow et al., 2006; Yang and Callan, 2009). (Snow et al., 2006) guide the incremental approach by maximizing the conditional probability over a set of relations. (Yang and Callan, 2009) introduce a taxonomy induction framework which combines the power of surface patterns and clustering through combining numerous heterogeneous features. Still, one would like a procedure to organize the harvested terms into a taxonomic structure starting fresh (i.e., without using an initial taxonomic structure). We propose an approach that bridges the gap between the term extraction algorithms tha</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>A metric-based framework for automatic taxonomy induction.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>1</volume>
<pages>271--279</pages>
<contexts>
<context position="2756" citStr="Yang and Callan, 2009" startWordPosition="429" endWordPosition="432">rom scratch. The main stages of automatic taxonomy induction are term extraction and term organization. In recent years there has been a substantial amount of work on term extraction, including semantic class learning (Hearst, 1992; Riloff and Shepherd, 1997; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), relation acquisition between entities (Girju et al., 2003; Pantel and Pennacchiotti, 2006; Davidov et al., 2007), and creation of concept lists (Katz and Lin, 2003). Various attempts have been made to learn the taxonomic organization of concepts (Widdows, 2003; Snow et al., 2006; Yang and Callan, 2009). Among the most common is to start with a good ontology and then to try to position the missing concepts into it. (Snow et al., 2006) maximize the conditional probability of hyponym-hypernym relations given certain evidence, while (Yang and Callan, 2009) combines heterogenous features like context, co-occurrence, and surface patterns to produce a more-inclusive inclusion ranking formula. The obtained results are promising, but the problem of how to organize the gathered knowledge when there is no initial taxonomy, or when the initial taxonomy is grossly impoverished, still remains. 1110 Proce</context>
<context position="9065" citStr="Yang and Callan, 2009" startWordPosition="1430" endWordPosition="1433">mplexity of interaction among perspectives, and people are notoriously bad at constructing taxonomies adherent to a single perspective when given terms from multiple perspectives. This issue and the major alternative principles for taxonomization are discussed in (Hovy, 2002). It is therefore not surprising that the second stage of automated taxonomy induction is harder to achieve. As mentioned, most attempts to learn taxonomy structures start with a reasonably complete taxonomy and then insert the newly learned terms into it, one term at a time (Widdows, 2003; Pasca, 2004; Snow et al., 2006; Yang and Callan, 2009). (Snow et al., 2006) guide the incremental approach by maximizing the conditional probability over a set of relations. (Yang and Callan, 2009) introduce a taxonomy induction framework which combines the power of surface patterns and clustering through combining numerous heterogeneous features. Still, one would like a procedure to organize the harvested terms into a taxonomic structure starting fresh (i.e., without using an initial taxonomic structure). We propose an approach that bridges the gap between the term extraction algorithms that focus mainly on harvesting but do not taxonomize, and </context>
</contexts>
<marker>Yang, Callan, 2009</marker>
<rawString>Hui Yang and Jamie Callan. 2009. A metric-based framework for automatic taxonomy induction. In ACL-IJCNLP ’09: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1, pages 271–279.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>