<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9996145">
Source-Language Features and Maximum Correlation Training
for Machine Translation Evaluation
</title>
<author confidence="0.999464">
Ding Liu and Daniel Gildea
</author>
<affiliation confidence="0.9985535">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.338141">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.976682" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999767588235294">
We propose three new features for MT
evaluation: source-sentence constrained
n-gram precision, source-sentence re-
ordering metrics, and discriminative un-
igram precision, as well as a method of
learning linear feature weights to directly
maximize correlation with human judg-
ments. By aligning both the hypothe-
sis and the reference with the source-
language sentence, we achieve better cor-
relation with human judgments than pre-
viously proposed metrics. We further
improve performance by combining indi-
vidual evaluation metrics using maximum
correlation training, which is shown to be
better than the classification-based frame-
work.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998786">
Evaluation has long been a stumbling block in the
development of machine translation systems, due to
the simple fact that there are many correct trans-
lations for a given sentence. The most commonly
used metric, BLEU, correlates well over large test
sets with human judgments (Papineni et al., 2002),
but does not perform as well on sentence-level eval-
uation (Blatz et al., 2003). Later approaches to im-
prove sentence-level evaluation performance can be
summarized as falling into four types:
</bodyText>
<listItem confidence="0.882586413793103">
• Metrics based on common loose sequences of
MT outputs and references (Lin and Och, 2004;
Liu and Gildea, 2006). Such metrics were
shown to have better fluency evaluation per-
formance than metrics based on n-grams such
BLEU and NIST (Doddington, 2002).
• Metrics based on syntactic similarities such as
the head-word chain metric (HWCM) (Liu and
Gildea, 2005). Such metrics try to improve flu-
ency evaluation performance for MT, but they
heavily depend on automatic parsers, which are
designed for well-formed sentences and cannot
generate robust parse trees for MT outputs.
• Metrics based on word alignment between MT
outputs and the references (Banerjee and Lavie,
2005). Such metrics do well in adequacy evalu-
ation, but are not as good in fluency evaluation,
because of their unigram basis (Liu and Gildea,
2006).
• Combination of metrics based on machine
learning. Kulesza and Shieber (2004) used
SVMs to combine several metrics. Their
method is based on the assumption that
higher classification accuracy in discriminat-
ing human- from machine-generated transla-
tions will yield closer correlation with human
judgment. This assumption may not always
hold, particularly when classification is diffi-
cult. Lita et al. (2005) proposed a log-linear
</listItem>
<bodyText confidence="0.965856142857143">
model to combine features, but they only did
preliminary experiments based on 2 features.
Following the track of previous work, to improve
evaluation performance, one could either propose
new metrics, or find more effective ways to combine
the metrics. We explore both approaches. Much
work has been done on computing MT scores based
</bodyText>
<page confidence="0.994334">
41
</page>
<note confidence="0.797342">
Proceedings of NAACL HLT 2007, pages 41–48,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999646428571429">
on the pair of MT output/reference, and we aim to
investigate whether some other information could
be used in the MT evaluation, such as source sen-
tences. We propose two types of source-sentence
related features as well as a feature based on part of
speech. The three new types of feature can be sum-
marized as follows:
</bodyText>
<listItem confidence="0.682112">
• Source-sentence constrained n-gram precision.
</listItem>
<bodyText confidence="0.97164525">
Overlapping n-grams between an MT hypothe-
sis and its references do not necessarily indicate
correct translation segments, since they could
correspond to different parts of the source sen-
tence. Thus our constrained n-gram precision
counts only overlapping n-grams in MT hy-
pothesis and reference which are aligned to the
same words in the source sentences.
</bodyText>
<listItem confidence="0.988675888888889">
• Source-sentence reordering agreement. With
the alignment information, we can compare the
reorderings of the source sentence in the MT
hypothesis and in its references. Such compar-
ison only considers the aligned positions of the
source words in MT hypothesis and references,
and thus is oriented towards evaluating the sen-
tence structure.
• Discriminative unigram precision. We divide
</listItem>
<bodyText confidence="0.984119972222222">
the normal n-gram precision into many sub-
precisions according to their part of speech
(POS). The division gives us flexibility to train
the weights of each sub-precision in frame-
works such as SVM and Maximum Correla-
tion Training, which will be introduced later.
The motivation behind such differentiation is
that different sub-precisions should have dif-
ferent importance in MT evaluation, e.g., sub-
precision of nouns, verbs, and adjectives should
be important for evaluating adequacy, and
sub-precision in determiners and conjunctions
should mean more in evaluating fluency.
Along the direction of feature combination, since
indirect weight training using SVMs, based on re-
ducing classification error, cannot always yield good
performance, we train the weights by directly opti-
mizing the evaluation performance, i.e., maximizing
the correlation with the human judgment. This type
of direct optimization is known as Minimum Error
Rate Training (Och, 2003) in the MT community,
and is an essential component in building the state-
of-art MT systems. It would seem logical to apply
similar methods to MT evaluation. What is more,
Maximum Correlation Training (MCT) enables us
to train the weights based on human fluency judg-
ments and adequacy judgments respectively, and
thus makes it possible to make a fluency-oriented or
adequacy-oriented metric. It surpasses previous MT
metrics’ approach, where a a single metric evaluates
both fluency and adequacy. The rest of the paper is
organized as follows: Section 2 gives a brief recap of
n-gram precision-based metrics and introduces our
three extensions to them; Section 3 introduces MCT
for MT evaluation; Section 4 describes the experi-
mental results, and Section 5 gives our conclusion.
</bodyText>
<sectionHeader confidence="0.93608" genericHeader="method">
2 Three New Features for MT Evaluation
</sectionHeader>
<bodyText confidence="0.999684">
Since our source-sentence constrained n-gram preci-
sion and discriminative unigram precision are both
derived from the normal n-gram precision, it is
worth describing the original n-gram precision met-
ric, BLEU (Papineni et al., 2002). For every MT
hypothesis, BLEU computes the fraction of n-grams
which also appear in the reference sentences, as well
as a brevity penalty. The formula for computing
BLEU is shown below:
</bodyText>
<equation confidence="0.821229">
�C ngram∈C Count�lzp(ngram)
EC Engram′ ∈C Count(ngram′)
</equation>
<bodyText confidence="0.9999680625">
where C denotes the set of MT hypotheses.
Count,lip(ngram) denotes the clipped number of
n-grams in the candidates which also appear in the
references. BP in the above formula denotes the
brevity penalty, which is set to 1 if the accumulated
length of the MT outputs is longer than the arith-
metic mean of the accumulated length of the refer-
ences, and otherwise is set to the ratio of the two.
For sentence-level evaluation with BLEU, we com-
pute the score based on each pair of MT hypothe-
sis/reference. Later approaches, as described in Sec-
tion 1, use different ways to manipulate the morpho-
logical similarity between the MT hypothesis and its
references. Most of them, except NIST, consider the
words in MT hypothesis as the same, i.e., as long as
the words in MT hypothesis appear in the references,
</bodyText>
<equation confidence="0.987849">
B�
BLEU = �
N
E
n��
</equation>
<page confidence="0.985684">
42
</page>
<bodyText confidence="0.999933923076923">
they make no difference to the metrics.1 NIST com-
putes the n-grams weights as the logarithm of the ra-
tio of the n-gram frequency and its one word lower
n-gram frequency. From our experiments, NIST is
not generally better than BLEU, and the reason, we
conjecture, is that it differentiates the n-grams too
much and the frequency estimated upon the evalua-
tion corpus is not always reliable. In this section we
will describe two other strategies for differentiating
the n-grams, one of which uses the alignments with
the source sentence as a further constraint, while the
other differentiates the n-gram precisions according
to POS.
</bodyText>
<subsectionHeader confidence="0.7694395">
2.1 Source-sentence Constrained N-gram
Precision
</subsectionHeader>
<bodyText confidence="0.928548413793103">
The quality of an MT sentence should be indepen-
dent of the source sentence given the reference trans-
lation, but considering that current metrics are all
based on shallow morphological similarity of the
MT outputs and the reference, without really under-
standing the meaning in both sides, the source sen-
tences could have some useful information in dif-
ferentiating the MT outputs. Consider the Chinese-
English translation example below:
Source: wo bu neng zhe me zuo
Hypothesis: I must hardly not do this
Reference: I must not do this
It is clear that the word not in the MT output can-
not co-exist with the word hardly while maintain-
ing the meaning of the source sentence. None of
the metrics mentioned above can prevent not from
being counted in the evaluation, due to the simple
reason that they only compute shallow morphologi-
cal similarity. Then how could the source sentence
help in the example? If we reveal the alignment
of the source sentence with both the reference and
the MT output, the Chinese word bu neng would
be aligned to must not in the reference and must
hardly in the MT output respectively, leaving the
word not in the MT output not aligned to any word in
the source sentence. Therefore, if we can somehow
find the alignments between the source sentence and
the reference/MT output, we could be smarter in se-
lecting the overlapping words to be counted in the
</bodyText>
<footnote confidence="0.527264333333333">
1In metrics such as METEOR, ROUGE, SIA (Liu and
Gildea, 2006), the positions of words do make difference, but
it has nothing to do with the word itself.
</footnote>
<figure confidence="0.951967235294117">
for all n-grams wi, ..., wi+n_1 in MT hypothesis
do
max val = 0;
for all reference sentences do
for all n-grams rj, ..., rj+n_1 in current ref-
erence sentence do
val=0;
for k=0; k &lt; n-1; k ++ do
if wi+k equals rj+k AND MTaligni
equals REFalignj then
val += 1n;
if val &gt; max val then
max val = val;
hit count += max val;
return hit count x penalty;
length
MThypothesislength g
</figure>
<figureCaption confidence="0.9973895">
Figure 1: Algorithm for Computing Source-
sentence Constrained n-gram Precision
</figureCaption>
<bodyText confidence="0.881906433333333">
metric: only select the words which are aligned to
the same source words. Now the question comes
to how to find the alignment of source sentence and
MT hypothesis/references, since the evaluation data
set usually does not contain alignment information.
Our approach uses GIZA++2 to construct the many-
to-one alignments between source sentences and the
MT hypothesis/references respectively.3 GIZA++
could generate many-to-one alignments either from
source sentence to the MT hypothesis, in which case
every word in MT hypothesis is aligned to a set
of (or none) words in the source sentence, or from
the reverse direction, in which case every word in
MT hypothesis is aligned to exactly one word (or
none) word in the source sentence. In either case,
using MTaligni and REFaligni to denote the po-
sitions of the words in the source sentences which
are aligned to a word in the MT hypothesis and a
word in the reference respectively, the algorithm for
computing source-sentence constrained n-gram pre-
cision of length n is described in Figure 1.
Since source-sentence constrained n-gram preci-
sion (SSCN) is a precision-based metric, the vari-
2GIZA++ is available at
http://www.fjoch.com/GIZA++.html
3More refined alignments could be got for source-hypothesis
from the MT system, and for source-references by using manual
proof-reading after the automatic alignment. Doing so, how-
ever, requires the MT system’s cooperation and some costly hu-
man labor.
</bodyText>
<page confidence="0.999172">
43
</page>
<bodyText confidence="0.99994188">
able length penalty is used to avoid assigning a
short MT hypothesis a high score, and is computed
in the same way as BLEU. Note that in the algo-
rithm for computing the precision of n-grams longer
than one word, not all words in the n-grams should
satisfy the source-sentence constraint. The reason is
that the high order n-grams are already very sparse
in the sentence-level evaluation. To differentiate the
SSCNs based on the source-to-MT/Ref (many-to-
one) alignments and the MT/Ref-to-source (many-
to-one) alignments, we use SSCN1 and SSCN2 to
denote them respectively. Naturally, we could com-
bine the constraint in SSCN1 and SSCN2 by either
taking their union (the combined constrained is sat-
isfied if either one is satisfied) or intersecting them
(the combined constrained is satisfied if both con-
straints are satisfied). We use SSCN u and SSCN i
to denote the SSCN based on unioned constraints
and intersected constraints respectively. We could
also apply the stochastic word mapping proposed in
SIA (Liu and Gildea, 2006) to replace the hard word
matching in Figure 1, and the corresponding met-
rics are denoted as pSSCN1, pSSCN2, pSSCN u,
pSSCN i, with the suffixed number denoting differ-
ent constraints.
</bodyText>
<subsectionHeader confidence="0.998044">
2.2 Metrics Based on Source Word Reordering
</subsectionHeader>
<bodyText confidence="0.9897585">
Most previous MT metrics concentrate on the co-
occurrence of the MT hypothesis words in the ref-
erences. Our metrics based on source sentence re-
orderings, on the contrary, do not take words identi-
ties into account, but rather compute how similarly
the source words are reordered in the MT output and
the references. For simplicity, we only consider the
pairwise reordering similarity. That is, for the source
word pair wi and wj, if their aligned positions in the
MT hypothesis and a reference are in the same order,
we call it a consistent word pair. Our pairwise re-
ordering similarity (PRS) metric computes the frac-
tion of the consistent word pairs in the source sen-
tence. Figure 2 gives the formal description of PRS.
SrcMTi and SrcRefk,i denote the aligned position
of source word wi in the MT hypothesis and the kth
reference respectively, and N denotes the length of
the source sentence.
Another criterion for evaluating the reordering of
the source sentence in the MT hypothesis is how
well it maintains the original word order in the
for all word pair wi, wj in the source sentence
such that i &lt; j do
for all reference sentences rk do
</bodyText>
<equation confidence="0.8878">
if (SrcMTi == SrcMTj AND
SrcRefk,i == SrcRefk,j) OR
((SrcMTi − SrcMTj) x (SrcRefk,i −
SrcRefk,j) &gt; 0) then
count + +; break;
return 2xcount
Nx(N−1);
</equation>
<figureCaption confidence="0.997022">
Figure 2: Compute Pairwise Reordering Similarity
</figureCaption>
<bodyText confidence="0.9822255">
for all word pair wi, wj in the source sentence,
such that i &lt; j do
</bodyText>
<construct confidence="0.625814">
if SrcMTi − SrcMTj &lt; 0 then
</construct>
<equation confidence="0.830547">
count + +;
return 2xcount ;
Nx(N−1)
</equation>
<figureCaption confidence="0.9615195">
Figure 3: Compute Source Sentence Monotonic Re-
ordering Ratio
</figureCaption>
<bodyText confidence="0.998964571428571">
source sentence. We know that most of the time,
the alignment of the source sentence and the MT hy-
pothesis is monotonic. This idea leads to the metric
of monotonic pairwise ratio (MPR), which computes
the fraction of the source word pairs whose aligned
positions in the MT hypothesis are of the same order.
It is described in Figure 3.
</bodyText>
<subsectionHeader confidence="0.7535175">
2.3 Discriminative Unigram Precision Based
on POS
</subsectionHeader>
<bodyText confidence="0.955728636363636">
The Discriminative Unigram Precision Based on
POS (DUPP) decomposes the normal unigram pre-
cision into many sub-precisions according to their
POS. The algorithm is described in Figure 4.
These sub-precisions by themselves carry the
same information as standard unigram precision, but
they provide us the opportunity to make a better
combined metric than the normal unigram precision
with MCT, which will be introduced in next section.
for all unigram s in the MT hypothesis do
if s is found in any of the references then
</bodyText>
<equation confidence="0.9148785">
countpOS(s) += 1
isi count,
precision,, = mt hypothesis length
bx E POS
</equation>
<figureCaption confidence="0.993573">
Figure 4: Compute DUPP for N-gram with length n
</figureCaption>
<page confidence="0.994814">
44
</page>
<bodyText confidence="0.999966846153846">
Such division could in theory be generalized to work
with higher order n-grams, but doing so would make
the n-grams in each POS set much more sparse. The
preprocessing step for the metric is tagging both
the MT hypothesis and the references with POS. It
might elicit some worries about the robustness of the
POS tagger on the noise-containing MT hypothesis.
This should not be a problem for two reasons. First,
compared with other preprocessing steps like pars-
ing, POS tagging is easier and has higher accuracy.
Second, because the counts for each POS are accu-
mulated, the correctness of a single word’s POS will
not affect the result very much.
</bodyText>
<sectionHeader confidence="0.9693385" genericHeader="method">
3 Maximum Correlation Training for
Machine Translation Evaluation
</sectionHeader>
<bodyText confidence="0.998569533333333">
Maximum Correlation Training (MCT) is an in-
stance of the general approach of directly optimiz-
ing the objective function by which a model will
ultimately be evaluated. In our case, the model is
the linear combination of the component metrics, the
parameters are the weights for each component met-
ric, and the objective function is the Pearson’s corre-
lation of the combined metric and the human judg-
ments. The reason to use the linear combination of
the metrics is that the component metrics are usu-
ally of the same or similar order of magnitude, and it
makes the optimization problem easy to solve. Us-
ing w to denote the weights, and m to denote the
component metrics, the combined metric x is com-
puted as:
</bodyText>
<equation confidence="0.844332">
�x(w) = wjmj (1)
j
</equation>
<bodyText confidence="0.9999435">
Using hi and x(w)i denote the human judgment
and combined metric for a sentence respectively, and
N denote the number of sentences in the evaluation
set, the objective function is then computed as:
</bodyText>
<equation confidence="0.9995186">
Pearson(X(w), H) =
�N N N
i=1 x(w)i i=1 hi
i=1 x(w)ihi _N
�//[[&apos;&apos;��N x/w 2 _ (ENi=1 x(w)i)2 [� N h2 _ (EN1 hi)2
</equation>
<bodyText confidence="0.9456258">
lL�i=1 l )i N )(L�i=1 i N )
Now our task is to find the weights for each compo-
nent metric so that the correlation of the combined
metric with the human judgment is maximized. It
can be formulated as:
</bodyText>
<equation confidence="0.974604">
w = argmax Pearson(X(w), H) (2)
w
</equation>
<bodyText confidence="0.9990964375">
The function Pearson(X(w), H) is differentiable
with respect to the vector w, and we compute this
derivative analytically and perform gradient ascent.
Our objective function not always convex (one can
easily create a non-convex function by setting the
human judgments and individual metrics to some
particular value). Thus there is no guarantee that,
starting from a random w, we will get the glob-
ally optimal w using optimization techniques such
as gradient ascent. The easiest way to avoid ending
up with a bad local optimum to run gradient ascent
by starting from different random points. In our ex-
periments, the difference in each run is very small,
i.e., by starting from different random initial values
of w, we end up with, not the same, but very similar
values for Pearson’s correlation.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99999528">
Experiments were conducted to evaluate the perfor-
mance of the new metrics proposed in this paper,
as well as the MCT combination framework. The
data for the experiments are from the MT evalua-
tion workshop at ACL05. There are seven sets of
MT outputs (E09 E11 E12 E14 E15 E17 E22), each
of which contains 919 English sentences translated
from the same set of Chinese sentences. There are
four references (E01, E02, E03, E04) and two sets
of human scores for each MT hypothesis. Each hu-
man score set contains a fluency and an adequacy
score, both of which range from 1 to 5. We create a
set of overall human scores by averaging the human
fluency and adequacy scores. For evaluating the au-
tomatic metrics, we compute the Pearson’s correla-
tion of the automatic scores and the averaged human
scores (over the two sets of available human scores),
for overall score, fluency, and adequacy. The align-
ment between the source sentences and the MT hy-
pothesis/references is computed by GIZA++, which
is trained on the combined corpus of the evalua-
tion data and a parallel corpus of Chinese-English
newswire text. The parallel newswire corpus con-
tains around 75,000 sentence pairs, 2,600,000 En-
glish words and 2,200,000 Chinese words. The
</bodyText>
<page confidence="0.998431">
45
</page>
<bodyText confidence="0.9999548">
stochastic word mapping is trained on a French-
English parallel corpus containing 700,000 sentence
pairs, and, following Liu and Gildea (2005), we only
keep the top 100 most similar words for each En-
glish word.
</bodyText>
<subsectionHeader confidence="0.996655">
4.1 Performance of the Individual Metrics
</subsectionHeader>
<bodyText confidence="0.99983856097561">
To evaluate our source-sentence based metrics, they
are used to evaluate the 7 MT outputs, with the 4 sets
of human references. The sentence-level Pearson’s
correlation with human judgment is computed for
each MT output, and the averaged results are shown
in Table 1. As a comparison, we also show the re-
sults of BLEU, NIST, METEOR, ROUGE, WER,
and HWCM. For METEOR and ROUGE, WORD-
NET and PORTER-STEMMER are enabled, and for
SIA, the decay factor is set to 0.6. The number
in brackets, for BLEU, shows the n-gram length it
counts up to, and for SSCN, shows the length of the
n-gram it uses. In the table, the top 3 results in each
column are marked bold and the best result is also
underlined. The results show that the SSCN2 met-
rics are better than the SSCN1 metrics in adequacy
and overall score. This is understandable since what
SSCN metrics need is which words in the source
sentence are aligned to an n-gram in the MT hy-
pothesis/references. This is directly modeled in the
alignment used in SSCN2. Though we could also
get such information from the reverse alignment, as
in SSCN1, it is rather an indirect way and could con-
tain more noise. It is interesting that SSCN1 gets
better fluency evaluation results than SSCN2. The
SSCN metrics with the unioned constraint, SSCN u,
by combining the strength of SSCN1 and SSCN2,
get even better results in all three aspects. We can
see that SSCN metrics, even without stochastic word
mapping, get significantly better results than their
relatives, BLEU, which indicates the source sen-
tence constraints do make a difference. SSCN2 and
SSCN u are also competitive to the state-of-art MT
metrics such as METEOR and SIA. The best SSCN
metric, pSSCN u(2), achieves the best performance
among all the testing metrics in overall and ade-
quacy, and the second best performance in fluency,
which is just a little bit worse than the best fluency
metric SIA.
The two reordering based metrics, PRS and MPR,
are not as good as the other testing metrics, in terms
</bodyText>
<table confidence="0.999775517241379">
Fluency Adequacy Overall
ROUGE W 24.8 27.8 29.0
ROUGE S 19.7 30.9 28.5
METEOR 24.4 34.8 33.1
SIA 26.8 32.1 32.6
NIST 1 09.6 22.6 18.5
WER 22.5 27.5 27.7
PRS 14.2 19.4 18.7
MPR 11.0 18.2 16.5
BLEU(1) 18.4 29.6 27.0
BLEU(2) 20.4 31.1 28.9
BLEU(3) 20.7 30.4 28.6
HWCM(2) 22.1 30.3 29.2
SSCN1(1) 24.2 29.6 29.8
SSCN2(1) 22.9 33.0 31.3
SSCN u(1) 23.8 34.2 32.5
SSCN i(1) 23.4 28.0 28.5
pSSCN1(1) 24.9 30.2 30.6
pSSCN2(1) 23.8 34.0 32.4
pSSCN u(1) 24.5 34.6 33.1
pSSCN i(1) 24.1 28.8 29.3
SSCN1(2) 24.0 29.6 29.7
SSCN2(2) 23.3 31.5 31.8
SSCN u(2) 24.1 34.5 32.8
SSCN i(2) 23.1 27.8 28.2
pSSCN1(2) 24.9 30.2 30.6
pSSCN2(2) 24.3 34.4 32.8
pSSCN u(2) 25.2 35.4 33.9
pSSCN i(2) 23.9 28.7 29.1
</table>
<tableCaption confidence="0.999909">
Table 1: Performance of Component Metrics
</tableCaption>
<bodyText confidence="0.99987525">
of the individual performance. It should not be sur-
prising since they are totally different kind of met-
rics, which do not count the overlapping n-grams,
but the consistent/monotonic word pair reorderings.
As long as they capture some property of the MT
hypothesis, they might be able to boost the per-
formance of the combined metric under the MCT
framework.
</bodyText>
<subsectionHeader confidence="0.998861">
4.2 Performance of the Combined Metrics
</subsectionHeader>
<bodyText confidence="0.9999065">
To test how well MCT works, the following scheme
is used: each set of MT outputs is evaluated by MCT,
which is trained on the other 6 sets of MT outputs
and their corresponding human judgment; the aver-
aged correlation of the 7 sets of MT outputs with the
human judgment is taken as the final result.
</bodyText>
<sectionHeader confidence="0.80761" genericHeader="method">
4.2.1 Discriminative Unigram Precision based
on POS
</sectionHeader>
<bodyText confidence="0.9994345">
We first use MCT to combine the discriminative
unigram precisions. To reduce the sparseness of the
unigrams of each POS, we do not use the original
POS set, but use a generalized one by combining
</bodyText>
<page confidence="0.997692">
46
</page>
<bodyText confidence="0.9999575">
all POS tags with the same first letter (e.g., the dif-
ferent verb forms such as VBN, VBD, and VBZ are
transformed to V). The unified POS set contains 23
POS tags. To give a fair comparison of DUPP with
BLEU, the length penalty is also added into it as a
component. Results are shown in Table 2. DUPP f,
DUPP a and DUPP o denote DUPP trained on hu-
man fluency, adequacy and overall judgment respec-
tively. This shows that DUPP achieves obvious im-
provement over BLEU, with only the unigrams and
length penalty, and DUPP f/ a/ o gets the best re-
sult in fluency/adequacy/overall evaluation, showing
that MCT is able to make a fluency- or adequacy-
oriented metric.
</bodyText>
<subsubsectionHeader confidence="0.572782">
4.2.2 Putting It All Together
</subsubsectionHeader>
<bodyText confidence="0.999653258064516">
The most interesting question in this paper is, with
all these metrics, how well we can do in the MT
evaluation. To answer the question, we put all the
metrics described into the MCT framework and use
the combined metric to evaluate the 7 MT outputs.
Note that to speed up the training process, we do
not directly use 24 DUPP components, instead, we
use the 3 combined DUPP metrics. With the met-
rics shown in Table 1, we then have in total 31 met-
rics. Table 2 shows the results of the final combined
metric. We can see that MCT trained on fluency,
adequacy and overall human judgment get the best
results among all the testing metrics in fluency, ade-
quacy and overall evaluation respectively. We did a
t-test with Fisher’s z transform for the combined re-
sults and the individual results to see how significant
the difference is. The combined results in adequacy
and overall are significantly better at 99.5% confi-
dence than the best results of the individual metrics
(pSSCN u(2)), and the combined result in fluency
is significantly better at 96.9% confidence than the
best individual metric (SIA). We also give the upper
bound for each evaluation aspect by training MCT
on the testing MT outputs, e.g., we train MCT on
E09 and then use it to evaluate E09. The upper-
bound is the best we can do with the MCT based
on linear combination. Another linear framework,
Classification SVM (CSVM),4 is also used to com-
bine the testing metrics except DUPP. Since DUPP
is based on MCT, to make a neat comparison, we
rule out DUPP in the experiments with CSVM. The
</bodyText>
<footnote confidence="0.964652">
4http://svmlight.joachims.org/
</footnote>
<table confidence="0.999589166666667">
Fluency Adequacy Overall
DUPP f 23.6 30.1 30.1
DUPP a 22.1 32.9 30.9
DUPP o 23.2 32.8 31.3
MCT f(4) 30.3 36.7 37.2
MCT a(4) 28.0 38.9 37.4
MCT o(4) 29.4 38.8 38.0
Upper bound 35.3 43.4 42.2
MCT f(3) 29.2 34.7 35.3
MCT a(3) 27.4 38.4 36.8
MCT o(3) 28.8 38.0 37.2
CSVM(3) 27.3 36.9 35.5
</table>
<tableCaption confidence="0.999863">
Table 2: Combination of the Testing Metrics
</tableCaption>
<bodyText confidence="0.999978">
testing scheme is the same as MCT, except that we
only use 3 references for each MT hypothesis, and
the positive samples for training CSVM are com-
puted as the scores of one of the 4 references based
on the other 3 references. The slack parameter of
CSVM is chosen so as to maximize the classifica-
tion accuracy of a heldout set of 800 negative and
800 positive samples, which are randomly selected
from the training set. The results are shown in Ta-
ble 2. We can see that MCT, with the same number
of reference sentences, is better than CSVM. Note
that the resources required by MCT and CSVM are
different. MCT uses human judgments to adjust the
weights, while CSVM needs extra human references
to produce positive training samples.
To have a rough idea of how the component met-
rics contribute to the final performance of MCT, we
incrementally add metrics into the MCT in descend-
ing order of their overall evaluation performance,
with the results shown in Figure 5. We can see that
the performance improves as the number of metrics
increases, in a rough sense. The major improvement
happens in the 3rd, 4th, 9th, 14th, and 30th metrics,
which are METEOR, SIA, DUPP a, pSSCN1(1),
and PRS. It is interesting to note that these are not
the metrics with the highest individual performance.
Another interesting observation is that there are no
two metrics belonging to the same series in the most
beneficial metrics, indicating that to get better com-
bined metrics, individual metrics showing different
sentence properties are preferred.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9888765">
This paper first describes two types of new ap-
proaches to MT evaluation, which includes making
</bodyText>
<page confidence="0.994452">
47
</page>
<figure confidence="0.83528">
correlation with human fluency/overall/adequacy judgements
the number of metrics (o: adequacy, x: fluency, +: overall)
</figure>
<figureCaption confidence="0.992219">
Figure 5: Performance as a Function of the Number
of Interpolated Metrics
</figureCaption>
<bodyText confidence="0.999739352941176">
use of source sentences, and discriminating unigram
precisions based on POS. Among all the testing met-
rics including BLEU, NIST, METEOR, ROUGE,
and SIA, our new metric, pSSCN u(2), based on
source-sentence constrained bigrams, achieves the
best adequacy and overall evaluation results, and the
second best result in fluency evaluation. We fur-
ther improve the performance by combining the in-
dividual metrics under the MCT framework, which
is shown to be better than a classification based
framework such as SVM. By examining the contri-
bution of each component metric, we find that met-
rics showing different properties of a sentence are
more likely to make a good combined metric.
Acknowledgments This work was supported by
NSF grants IIS-0546554, IIS-0428020, and IIS-
0325646.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999679928571429">
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judegments. In Proceedings of
the ACL-04 workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization, Ann Arbor, Michigan.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation for
machine translation. Technical report, Center for Lan-
guage and Speech Processing, Johns Hopkins Univer-
sity, Baltimore. Summer Workshop Final Report.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In In HLT 2002, Human Language Technology
Conference, San Diego, CA.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), Baltimore, MD, October.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42th Annual Conference of the
Association for Computational Linguistics (ACL-04),
Barcelona, Spain.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
Blanc: Learning evaluation metrics for mt. Vancouver.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization.
Ding Liu and Daniel Gildea. 2006. Stochastic iterative
alignment for machine translation evaluation. Sydney.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings ofACL-
03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL-
02, Philadelphia, PA.
</reference>
<figure confidence="0.9968791">
0.4
0.38
0.36
0.34
0.32
0.3
0.28
0.26
0.24
0 5 10 15 20 25 30 35
</figure>
<page confidence="0.9878">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.855861">
<title confidence="0.999012">Source-Language Features and Maximum Correlation for Machine Translation Evaluation</title>
<author confidence="0.970615">Liu</author>
<affiliation confidence="0.999818">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.993527444444445">We propose three new features for MT evaluation: source-sentence constrained n-gram precision, source-sentence reordering metrics, and discriminative unigram precision, as well as a method of learning linear feature weights to directly maximize correlation with human judgments. By aligning both the hypothesis and the reference with the sourcelanguage sentence, we achieve better correlation with human judgments than previously proposed metrics. We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judegments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-04 workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2010" citStr="Banerjee and Lavie, 2005" startWordPosition="302" endWordPosition="305"> outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002). • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). • Combination of metrics based on machine learning. Kulesza and Shieber (2004) used SVMs to combine several metrics. Their method is based on the assumption that higher classification accuracy in discriminating human- from machine-generated translations will yield closer correlation with human judgment. This assumption may not always hold, particularly when classification is difficult. Lita et al. (2005) proposed a log-linear model to combine features</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judegments. In Proceedings of the ACL-04 workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University, Baltimore. Summer Workshop Final Report.</institution>
<contexts>
<context position="1224" citStr="Blatz et al., 2003" startWordPosition="178" endWordPosition="181">an judgments than previously proposed metrics. We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002). • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are de</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore. Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In In HLT 2002, Human Language Technology Conference,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="1589" citStr="Doddington, 2002" startWordPosition="238" endWordPosition="239">e are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002). • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). • Combination of metrics based on </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In In HLT 2002, Human Language Technology Conference, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI),</booktitle>
<location>Baltimore, MD,</location>
<contexts>
<context position="2233" citStr="Kulesza and Shieber (2004)" startWordPosition="339" endWordPosition="342">on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). • Combination of metrics based on machine learning. Kulesza and Shieber (2004) used SVMs to combine several metrics. Their method is based on the assumption that higher classification accuracy in discriminating human- from machine-generated translations will yield closer correlation with human judgment. This assumption may not always hold, particularly when classification is difficult. Lita et al. (2005) proposed a log-linear model to combine features, but they only did preliminary experiments based on 2 features. Following the track of previous work, to improve evaluation performance, one could either propose new metrics, or find more effective ways to combine the metr</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Annual Conference of the Association for Computational Linguistics (ACL-04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1427" citStr="Lin and Och, 2004" startWordPosition="210" endWordPosition="213">ion-based framework. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002). • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42th Annual Conference of the Association for Computational Linguistics (ACL-04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucian Vlad Lita</author>
<author>Monica Rogati</author>
<author>Alon Lavie</author>
</authors>
<title>Blanc: Learning evaluation metrics for mt.</title>
<date>2005</date>
<location>Vancouver.</location>
<contexts>
<context position="2562" citStr="Lita et al. (2005)" startWordPosition="387" endWordPosition="390">t between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). • Combination of metrics based on machine learning. Kulesza and Shieber (2004) used SVMs to combine several metrics. Their method is based on the assumption that higher classification accuracy in discriminating human- from machine-generated translations will yield closer correlation with human judgment. This assumption may not always hold, particularly when classification is difficult. Lita et al. (2005) proposed a log-linear model to combine features, but they only did preliminary experiments based on 2 features. Following the track of previous work, to improve evaluation performance, one could either propose new metrics, or find more effective ways to combine the metrics. We explore both approaches. Much work has been done on computing MT scores based 41 Proceedings of NAACL HLT 2007, pages 41–48, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics on the pair of MT output/reference, and we aim to investigate whether some other information could be used in the MT eva</context>
</contexts>
<marker>Lita, Rogati, Lavie, 2005</marker>
<rawString>Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005. Blanc: Learning evaluation metrics for mt. Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="1697" citStr="Liu and Gildea, 2005" startWordPosition="253" endWordPosition="256">ll over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002). • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evaluation, but are not as good in fluency evaluation, because of their unigram basis (Liu and Gildea, 2006). • Combination of metrics based on machine learning. Kulesza and Shieber (2004) used SVMs to combine several metrics. Their method is based on </context>
<context position="19308" citStr="Liu and Gildea (2005)" startWordPosition="3191" endWordPosition="3194">tomatic scores and the averaged human scores (over the two sets of available human scores), for overall score, fluency, and adequacy. The alignment between the source sentences and the MT hypothesis/references is computed by GIZA++, which is trained on the combined corpus of the evaluation data and a parallel corpus of Chinese-English newswire text. The parallel newswire corpus contains around 75,000 sentence pairs, 2,600,000 English words and 2,200,000 Chinese words. The 45 stochastic word mapping is trained on a FrenchEnglish parallel corpus containing 700,000 sentence pairs, and, following Liu and Gildea (2005), we only keep the top 100 most similar words for each English word. 4.1 Performance of the Individual Metrics To evaluate our source-sentence based metrics, they are used to evaluate the 7 MT outputs, with the 4 sets of human references. The sentence-level Pearson’s correlation with human judgment is computed for each MT output, and the averaged results are shown in Table 1. As a comparison, we also show the results of BLEU, NIST, METEOR, ROUGE, WER, and HWCM. For METEOR and ROUGE, WORDNET and PORTER-STEMMER are enabled, and for SIA, the decay factor is set to 0.6. The number in brackets, for</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic iterative alignment for machine translation evaluation.</title>
<date>2006</date>
<location>Sydney.</location>
<contexts>
<context position="1450" citStr="Liu and Gildea, 2006" startWordPosition="214" endWordPosition="217">. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002). • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluation performance for MT, but they heavily depend on automatic parsers, which are designed for well-formed sentences and cannot generate robust parse trees for MT outputs. • Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie, 2005). Such metrics do well in adequacy evalu</context>
<context position="9338" citStr="Liu and Gildea, 2006" startWordPosition="1489" endWordPosition="1492">orphological similarity. Then how could the source sentence help in the example? If we reveal the alignment of the source sentence with both the reference and the MT output, the Chinese word bu neng would be aligned to must not in the reference and must hardly in the MT output respectively, leaving the word not in the MT output not aligned to any word in the source sentence. Therefore, if we can somehow find the alignments between the source sentence and the reference/MT output, we could be smarter in selecting the overlapping words to be counted in the 1In metrics such as METEOR, ROUGE, SIA (Liu and Gildea, 2006), the positions of words do make difference, but it has nothing to do with the word itself. for all n-grams wi, ..., wi+n_1 in MT hypothesis do max val = 0; for all reference sentences do for all n-grams rj, ..., rj+n_1 in current reference sentence do val=0; for k=0; k &lt; n-1; k ++ do if wi+k equals rj+k AND MTaligni equals REFalignj then val += 1n; if val &gt; max val then max val = val; hit count += max val; return hit count x penalty; length MThypothesislength g Figure 1: Algorithm for Computing Sourcesentence Constrained n-gram Precision metric: only select the words which are aligned to the </context>
<context position="12358" citStr="Liu and Gildea, 2006" startWordPosition="1987" endWordPosition="1990">the SSCNs based on the source-to-MT/Ref (many-toone) alignments and the MT/Ref-to-source (manyto-one) alignments, we use SSCN1 and SSCN2 to denote them respectively. Naturally, we could combine the constraint in SSCN1 and SSCN2 by either taking their union (the combined constrained is satisfied if either one is satisfied) or intersecting them (the combined constrained is satisfied if both constraints are satisfied). We use SSCN u and SSCN i to denote the SSCN based on unioned constraints and intersected constraints respectively. We could also apply the stochastic word mapping proposed in SIA (Liu and Gildea, 2006) to replace the hard word matching in Figure 1, and the corresponding metrics are denoted as pSSCN1, pSSCN2, pSSCN u, pSSCN i, with the suffixed number denoting different constraints. 2.2 Metrics Based on Source Word Reordering Most previous MT metrics concentrate on the cooccurrence of the MT hypothesis words in the references. Our metrics based on source sentence reorderings, on the contrary, do not take words identities into account, but rather compute how similarly the source words are reordered in the MT output and the references. For simplicity, we only consider the pairwise reordering s</context>
</contexts>
<marker>Liu, Gildea, 2006</marker>
<rawString>Ding Liu and Daniel Gildea. 2006. Stochastic iterative alignment for machine translation evaluation. Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL03.</booktitle>
<contexts>
<context position="5104" citStr="Och, 2003" startWordPosition="779" endWordPosition="780"> have different importance in MT evaluation, e.g., subprecision of nouns, verbs, and adjectives should be important for evaluating adequacy, and sub-precision in determiners and conjunctions should mean more in evaluating fluency. Along the direction of feature combination, since indirect weight training using SVMs, based on reducing classification error, cannot always yield good performance, we train the weights by directly optimizing the evaluation performance, i.e., maximizing the correlation with the human judgment. This type of direct optimization is known as Minimum Error Rate Training (Och, 2003) in the MT community, and is an essential component in building the stateof-art MT systems. It would seem logical to apply similar methods to MT evaluation. What is more, Maximum Correlation Training (MCT) enables us to train the weights based on human fluency judgments and adequacy judgments respectively, and thus makes it possible to make a fluency-oriented or adequacy-oriented metric. It surpasses previous MT metrics’ approach, where a a single metric evaluates both fluency and adequacy. The rest of the paper is organized as follows: Section 2 gives a brief recap of n-gram precision-based m</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings ofACL03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1144" citStr="Papineni et al., 2002" startWordPosition="164" endWordPosition="167"> reference with the sourcelanguage sentence, we achieve better correlation with human judgments than previously proposed metrics. We further improve performance by combining individual evaluation metrics using maximum correlation training, which is shown to be better than the classification-based framework. 1 Introduction Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence. The most commonly used metric, BLEU, correlates well over large test sets with human judgments (Papineni et al., 2002), but does not perform as well on sentence-level evaluation (Blatz et al., 2003). Later approaches to improve sentence-level evaluation performance can be summarized as falling into four types: • Metrics based on common loose sequences of MT outputs and references (Lin and Och, 2004; Liu and Gildea, 2006). Such metrics were shown to have better fluency evaluation performance than metrics based on n-grams such BLEU and NIST (Doddington, 2002). • Metrics based on syntactic similarities such as the head-word chain metric (HWCM) (Liu and Gildea, 2005). Such metrics try to improve fluency evaluatio</context>
<context position="6153" citStr="Papineni et al., 2002" startWordPosition="942" endWordPosition="945">proach, where a a single metric evaluates both fluency and adequacy. The rest of the paper is organized as follows: Section 2 gives a brief recap of n-gram precision-based metrics and introduces our three extensions to them; Section 3 introduces MCT for MT evaluation; Section 4 describes the experimental results, and Section 5 gives our conclusion. 2 Three New Features for MT Evaluation Since our source-sentence constrained n-gram precision and discriminative unigram precision are both derived from the normal n-gram precision, it is worth describing the original n-gram precision metric, BLEU (Papineni et al., 2002). For every MT hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty. The formula for computing BLEU is shown below: �C ngram∈C Count�lzp(ngram) EC Engram′ ∈C Count(ngram′) where C denotes the set of MT hypotheses. Count,lip(ngram) denotes the clipped number of n-grams in the candidates which also appear in the references. BP in the above formula denotes the brevity penalty, which is set to 1 if the accumulated length of the MT outputs is longer than the arithmetic mean of the accumulated length of the references, and other</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL02, Philadelphia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>