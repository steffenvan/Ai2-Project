<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030578">
<title confidence="0.9946665">
Using Conditional Random Fields to Predict Pitch Accents in
Conversational Speech
</title>
<author confidence="0.781344">
Michelle L. Gregory
</author>
<affiliation confidence="0.538067">
Linguistics Department
University at Buffalo
</affiliation>
<address confidence="0.955593">
Buffalo, NY 14260
</address>
<email confidence="0.998803">
mgregory@buffalo.edu
</email>
<author confidence="0.98645">
Yasemin Altun
</author>
<affiliation confidence="0.98575">
Department of Computer Science
Brown University
</affiliation>
<address confidence="0.923557">
Providence, RI 02912
</address>
<email confidence="0.998897">
altun@cs.brown.edu
</email>
<sectionHeader confidence="0.994796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999419473684211">
The detection of prosodic characteristics is an im-
portant aspect of both speech synthesis and speech
recognition. Correct placement of pitch accents aids
in more natural sounding speech, while automatic
detection of accents can contribute to better word-
level recognition and better textual understanding.
In this paper we investigate probabilistic, contex-
tual, and phonological factors that influence pitch
accent placement in natural, conversational speech
in a sequence labeling setting. We introduce Con-
ditional Random Fields (CRFs) to pitch accent pre-
diction task in order to incorporate these factors ef-
ficiently in a sequence model. We demonstrate the
usefulness and the incremental effect of these fac-
tors in a sequence model by performing experiments
on hand labeled data from the Switchboard Corpus.
Our model outperforms the baseline and previous
models of pitch accent prediction on the Switch-
board Corpus.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999470327102804">
The suprasegmental features of speech relay critical
information in conversation. Yet, one of the ma-
jor roadblocks to natural sounding speech synthe-
sis has been the identification and implementation
of prosodic characteristics. The difficulty with this
task lies in the fact that prosodic cues are never ab-
solute; they are relative to individual speakers, gen-
der, dialect, discourse context, local context, phono-
logical environment, and many other factors. This is
especially true of pitch accent, the acoustic cues that
make one word more prominent than others in an
utterance. For example, a word with a fundamen-
tal frequency (f0) of 120 Hz would likely be quite
prominent in a male speaker, but not for a typical fe-
male speaker. Likewise, the accent on the utterance
”Jon’s leaving.” is critical in determining whether
it is the answer to the question ”Who is leaving?”
(”JON’s leaving.”) or ”What is Jon doing?” (”Jon’s
LEAVING.”). Accurate pitch accent prediction lies
in the successful combination of as many of the con-
textual variables as possible. Syntactic information
such as part of speech has proven to be a success-
ful predictor of accentuation (Hirschberg, 1993; Pan
and Hirschberg, 2001). In general, function words
are not accented, while content words are. Vari-
ous measures of a word’s informativeness, such as
the information content (IC) of a word (Pan and
McKeown, 1999) and its collocational strength in a
given context (Pan and Hirschberg, 2001) have also
proven to be useful models of pitch accent. How-
ever, in open topic conversational speech, accent is
very unpredictable. Part of speech and the infor-
mativeness of a word do not capture all aspects of
accentuation, as we see in this example taken from
Switchboard, where a function word gets accented
(accented words are in uppercase):
I, I have STRONG OBJECTIONS to THAT.
Accent is also influenced by aspects of rhythm
and timing. The length of words, in both number
of phones and normalized duration, affect its likeli-
hood of being accented. Additionally, whether the
immediately surrounding words bear pitch accent
also affect the likelihood of accentuation. In other
words, a word that might typically be accented may
be unaccented because the surrounding words also
bear pitch accent. Phrase boundaries seem to play
a role in accentuation as well. The first word of in-
tonational phrases (IP) is less likely to be accented
while the last word of an IP tends be accented. In
short, accented words within the same IP are not in-
dependent of each other.
Previous work on pitch accent prediction, how-
ever, neglected the dependency between labels. Dif-
ferent machine learning techniques, such as deci-
sion trees (Hirschberg, 1993), rule induction sys-
tems (Pan and McKeown, 1999), bagging (Sun,
2002), boosting (Sun, 2002) have been used in a
scenario where the accent of each word is pre-
dicted independently. One exception to this line
of research is the use of Hidden Markov Models
(HMM) for pitch accent prediction (Pan and McK-
eown, 1999; Conkie et al., 1999). Pan and McKe-
own (1999) demonstrate the effectiveness of a se-
quence model over a rule induction system, RIP-
PER, that treats each label independently by show-
ing that HMMs outperform RIPPER when the same
variables are used.
Until recently, HMMs were the predominant for-
malism to model label sequences. However, they
have two major shortcomings. They are trained
non-discriminatively using maximum likelihood es-
timation to model the joint probability of the ob-
servation and label sequences. Also, they require
questionable independence assumptions to achieve
efficient inference and learning. Therefore, vari-
ables used in Hidden Markov models of pitch ac-
cent prediction have been very limited, e.g. part of
speech and frequency (Pan and McKeown, 1999).
Discriminative learning methods, such as Maximum
Entropy Markov Models (McCallum et al., 2000),
Projection Based Markov Models (Punyakanok and
Roth, 2000), Conditional Random Fields (Lafferty
et al., 2001), Sequence AdaBoost (Altun et al.,
2003a), Sequence Perceptron (Collins, 2002), Hid-
den Markov Support Vector Machines (Altun et
al., 2003b) and Maximum-Margin Markov Net-
works (Taskar et al., 2004), overcome the limita-
tions of HMMs. Among these methods, CRFs is
the most common technique used in NLP and has
been successfully applied to Part-of-Speech Tag-
ging (Lafferty et al., 2001), Named-Entity Recog-
nition (Collins, 2002) and shallow parsing (Sha and
Pereira, 2003; McCallum, 2003).
The goal of this study is to better identify which
words in a string of text will bear pitch accent.
Our contribution is two-fold: employing new pre-
dictors and utilizing a discriminative model. We
combine the advantages of probabilistic, syntactic,
and phonological predictors with the advantages of
modeling pitch accent in a sequence labeling setting
using CRFs (Lafferty et al., 2001).
The rest of the paper is organized as follows: In
Section 2, we introduce CRFs. Then, we describe
our corpus and the variables in Section 3 and Sec-
tion 4. We present the experimental setup and report
results in Section 5. Finally, we discuss our results
(Section 6) and conclude (Section 7).
</bodyText>
<sectionHeader confidence="0.965267" genericHeader="method">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.997135833333333">
CRFs can be considered as a generalization of lo-
gistic regression to label sequences. They define
a conditional probability distribution of a label se-
quence y given an observation sequence x. In this
paper, x = (x1, x2,. .. , xn) denotes a sentence of
length n and y = (y1, y2, . . . , yn) denotes the la-
bel sequence corresponding to x. In pitch accent
prediction, xt is a word and yt is a binary label de-
noting whether xt is accented or not.
CRFs specify a linear discriminative function F
parameterized by A over a feature representation of
the observation and label sequence IF(x, y). The
model is assumed to be stationary, thus the feature
representation can be partitioned with respect to po-
sitions t in the sequence and linearly combined with
respect to the importance of each feature Ok, de-
noted by Ak. Then the discriminative function can
be stated as in Equation 1:
</bodyText>
<equation confidence="0.9233004">
F (x, y; A) = � (A, &apos;Pt(x, y)) (1)
t
Then, the conditional probability is given by
1
p(y|x; A) = Z(x, A)F(x, y; A) (2)
</equation>
<bodyText confidence="0.997870166666667">
where Z(x, A) = �y¯ F (x, ¯y; A) is a normaliza-
tion constant which is computed by summing over
all possible label sequences y¯ of the observation se-
quence x.
We extract two types of features from a sequence
pair:
</bodyText>
<listItem confidence="0.9895569">
1. Current label and information about the obser-
vation sequence, such as part-of-speech tag of
a word that is within a window centered at the
word currently labeled, e.g. Is the current word
pitch accented and the part-of-speech tag of
the previous word=Noun?
2. Current label and the neighbors of that label,
i.e. features that capture the inter-label depen-
dencies, e.g. Is the current word pitch accented
and the previous word not accented?
</listItem>
<bodyText confidence="0.999657217391304">
Since CRFs condition on the observation se-
quence, they can efficiently employ feature repre-
sentations that incorporate overlapping features, i.e.
multiple interacting features or long-range depen-
dencies of the observations, as opposed to HMMs
which generate observation sequences.
In this paper, we limit ourselves to 1-order
Markov model features to encode inter-label de-
pendencies. The information used to encode the
observation-label dependencies is explained in de-
tail in Section 4.
In CRFs, the objective function is the log-loss of
the model with A parameters with respect to a train-
ing set D. This function is defined as the negative
sum of the conditional probabilities of each training
label sequence yi, given the observation sequence
xi, where D - {(xi, yi) : i = 1, ... , m}. CRFs are
known to overfit, especially with noisy data if not
regularized. To overcome this problem, we penalize
the objective function by adding a Gaussian prior
(a term proportional to the squared norm ||A||2) as
suggested in (Johnson et al., 1999). Then the loss
function is given as:
</bodyText>
<equation confidence="0.998733">
log p(yi|xi; A) + 2c||A||2
1
F(xi, yi; A) + log Z(xi, A)
c||A||2 (3)
</equation>
<bodyText confidence="0.999116222222222">
where c is a constant.
Lafferty et al. (2001), proposed a modification
of improved iterative scaling for parameter estima-
tion in CRFs. However, gradient-based methods
have often found to be more efficient for minimizing
Equation 3 (Minka, 2001; Sha and Pereira, 2003).
In this paper, we use the conjugate gradient descent
method to optimize the above objective function.
The gradients are computed as in Equation 4:
</bodyText>
<equation confidence="0.996866">
Ep[1Ft(xi, y)] − IFt(xi, yi)
+ cA (4)
</equation>
<bodyText confidence="0.999778">
where the expectation is with respect to all possi-
ble label sequences of the observation sequence xi
and can be computed using the forward backward
algorithm.
Given an observation sequence x, the best label
sequence is given by:
</bodyText>
<equation confidence="0.997991">
F(x, y; ˆA) (5)
</equation>
<bodyText confidence="0.999937333333333">
where Aˆ is the parameter vector that minimizes
L(A; D). The best label sequence can be identified
by performing the Viterbi algorithm.
</bodyText>
<sectionHeader confidence="0.993912" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.9999805">
The data for this study were taken from the Switch-
board Corpus (Godfrey et al., 1992), which con-
sists of 2430 telephone conversations between adult
speakers (approximately 2.4 million words). Partic-
ipants were both male and female and represented
all major dialects of American English. We used a
portion of this corpus that was phonetically hand-
transcribed (Greenberg et al., 1996) and segmented
into speech boundaries at turn boundaries or pauses
of more than 500 ms on both sides. Fragments con-
tained seven words on average. Additionally, each
word was coded for probabilistic and contextual
information, such as word frequency, conditional
probabilities, the rate of speech, and the canonical
pronunciation (Fosler-Lussier and Morgan, 1999).
The dataset used in all analysis in this study con-
sists of only the first hour of the database, comprised
of 1,824 utterances with 13,190 words. These utter-
ances were hand coded for pitch accent and intona-
tional phrase brakes.
</bodyText>
<subsectionHeader confidence="0.999366">
3.1 Pitch Accent Coding
</subsectionHeader>
<bodyText confidence="0.999962315789474">
The utterances were hand labeled for accents and
boundaries according to the Tilt Intonational Model
(Taylor, 2000). This model is characterized by a
series of intonational events: accents and bound-
aries. Labelers were instructed to use duration, am-
plitude, pausing information, and changes in f0 to
identify events. In general, labelers followed the ba-
sic conventions of EToBI for coding (Taylor, 2000).
However, the Tilt coding scheme was simplified.
Accents were coded as either major or minor (and
some rare level accents) and breaks were either ris-
ing or falling. Agreement for the Tilt coding was
reported at 86%. The CU coding also used a simpli-
fied EToBI coding scheme, with accent types con-
flated and only major breaks coded. Accent and
break coding pair-wise agreement was between 85-
95% between coders, with a kappa r. of 71%-74%
where r. is the difference between expected agree-
ment and actual agreement.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="method">
4 Variables
</sectionHeader>
<bodyText confidence="0.999979285714286">
The label we were predicting was a binary distinc-
tion of accented or not. The variables we used for
prediction fall into three main categories: syntac-
tic, probabilistic variables, which include word fre-
quency and collocation measures, and phonological
variables, which capture aspects of rhythm and tim-
ing that affect accentuation.
</bodyText>
<subsectionHeader confidence="0.986833">
4.1 Syntactic variables
</subsectionHeader>
<bodyText confidence="0.992903777777778">
The only syntactic category we used was a four-
way classification for hand-generated part of speech
(POS): Function, Noun, Verb, Other, where Other
includes all adjectives and adverbs&apos;. Table 1 gives
the percentage of accented and unaccented items by
POS.
&apos;We also tested a categorization of 14 distinct part of speech
classes, but the results did not improve, so we only report on the
four-way classification.
</bodyText>
<equation confidence="0.989182538461538">
L(A; D) = − Em
i
Em
i
1
+ 2
Em
i
VAL =
E
t
yˆ = arg max
Y
</equation>
<table confidence="0.9990274">
Accented Unaccented
Function 21% 79%
Verb 59% 41%
Noun 30% 70%
Other 49% 51%
</table>
<tableCaption confidence="0.976571">
Table 1: Percentage of accented and unaccented
items by POS.
</tableCaption>
<table confidence="0.9996875">
Variable Definition Example
Unigram log p(wi) and, I
Bigram log p(wi|wi_1) roughing it
Rev Bigram log p(wi|wi+1) rid of
Joint log p(wi_1, wi) and I
Rev Joint log p(wi, wi+1) and I
</table>
<tableCaption confidence="0.99814">
Table 2: Definition of probabilistic variables.
</tableCaption>
<subsectionHeader confidence="0.9541">
4.2 Probabilistic variables
</subsectionHeader>
<bodyText confidence="0.999864851851852">
Following a line of research that incorporates the
information content of a word as well as collo-
cation measures (Pan and McKeown, 1999; Pan
and Hirschberg, 2001) we have included a number
of probabilistic variables. The probabilistic vari-
ables we used were the unigram frequency, the pre-
dictability of a word given the preceding word (bi-
gram), the predictability of a word given the follow-
ing word (reverse bigram), the joint probability of a
word with the preceding (joint), and the joint prob-
ability of a word with the following word (reverse
joint). Table 2 provides the definition for these,
as well as high probability examples from the cor-
pus (the emphasized word being the current target).
Note all probabilistic variables were in log scale.
The values for these probabilities were obtained
using the entire 2.4 million words of SWBD2. Table
3 presents the Spearman’s rank correlation coeffi-
cient between the probabilistic measures and accent
(Conover, 1980). These values indicate the strong
correlation of accents to the probabilistic variables.
As the probability increases, the chance of an accent
decreases. Note that all values are significant at the
p &lt; .001 level.
We also created a combined part of speech and
unigram frequency variable in order to have a vari-
able that corresponds to the variable used in (Pan
</bodyText>
<footnote confidence="0.983640285714286">
2Our current implementation of CRF only takes categorical
variables, thus for the experiments, all probabilistic variables
were binned into 5 equal categories. We also tried more bins
and produced similar results, so we only report on the 5-binned
categories. We computed correlations between pitch accent and
the original 5 variables as well as the binned variables and they
are very similar.
</footnote>
<table confidence="0.999268333333333">
Variables Spearman’s p
Unigram -.451
Bigram -.309
Reverse Bigram -.383
Joint -.207
Reverse joint -.265
</table>
<tableCaption confidence="0.9984785">
Table 3: Spearman’s correlation values for the prob-
abilistic measures.
</tableCaption>
<bodyText confidence="0.800779">
and McKeown, 1999).
</bodyText>
<subsectionHeader confidence="0.998411">
4.3 Phonological variables
</subsectionHeader>
<bodyText confidence="0.998674047619048">
The last category of predictors, phonological vari-
ables, concern aspects of rhythm and timing of an
utterance. We have two main sources for these vari-
ables: those that can be computed solely from a
string of text (textual), and those that require some
sort of acoustic information (acoustic). Sun (2002)
demonstrated that the number of phones in a syl-
lable, the number of syllables in a word, and the
position of a word in a sentence are useful predic-
tors of which syllables get accented. While Sun was
concerned with predicting accented syllables, some
of the same variables apply to word level targets as
well. For our textual phonological features, we in-
cluded the number of syllables in a word and the
number of phones (both in citation form as well as
transcribed form). Instead of position in a sentence,
we used the position of the word in an utterance
since the fragments do not necessarily correspond
to sentences in the database we used. We also made
use of the utterance length. Below is the list of our
textual features:
</bodyText>
<listItem confidence="0.999772">
• Number of canonical syllables
• Number of canonical phones
• Number of transcribed phones
• The length of the utterance in number of words
• The position of the word in the utterance
</listItem>
<bodyText confidence="0.931693555555556">
The main purpose of this study is to better pre-
dict which words in a string of text receive accent.
So far, all of our predictors are ones easily com-
puted from a string of text. However, we have in-
cluded a few variables that affect the likelihood of
a word being accented that require some acoustic
data. To the best of our knowledge, these features
have not been used in acoustic models of pitch ac-
cent prediction. These features include the duration
of the word, speech rate, and following intonational
phrase boundaries. Given the nature of the SWBD
corpus, there are many disfluencies. Thus, we also
Feature x2 Sig
canonical syllables 1636 p &lt; .001
canonical phones 2430 p &lt; .001
transcribed phones 2741 p &lt; .001
utt length 80 p &lt; .005
utt position 295 p &lt; .001
duration 3073 p &lt; .001
speech rate 101 p &lt; .001
following pause 27 p &lt; .001
foll filled pause 328 p &lt; .001
foll IP boundary 1047 p &lt; .001
Table 4: Significance of phonological features on
pitch accent prediction.
included following pauses and filled pauses as pre-
dictors. Below is the list of our acoustic features:
</bodyText>
<listItem confidence="0.987784642857143">
• Log of duration in milliseconds normalized
by number of canonical phones binned into 5
equal categories.
• Log Speech Rate; calculated on strings of
speech bounded on either side by pauses of
300 ms or greater and binned into 5 equal cat-
egories.
• Following pause; a binary distinction of
whether a word is followed by a period of si-
lence or not.
• Following filled pause; a binary distinction of
whether a word was followed by a filled pause
(uh, um) or not.
• Following IP boundary
</listItem>
<bodyText confidence="0.991906285714286">
Table 4 indicates that each of these features sig-
nificantly affect the presence of pitch accent. While
certainly all of these variables are not independent
of on another, using CRFs, one can incorporate all
of these variables into the pitch accent prediction
model with the advantage of making use of the de-
pendencies among the labels.
</bodyText>
<subsectionHeader confidence="0.997688">
4.4 Surrounding Information
</subsectionHeader>
<bodyText confidence="0.999991692307692">
Sun (2002) has shown that the values immediately
preceding and following the target are good predic-
tors for the value of the target. We also experi-
mented with the effects of the surrounding values
by varying the window size of the observation-label
feature extraction described in Section 2. When the
window size is 1, only values of the word that is la-
belled are incorporated in the model. When the win-
dow size is 3, the values of the previous and the fol-
lowing words as well as the current word are incor-
porated in the model. Window size 5 captures the
values of the current word, the two previous words
and the two following words.
</bodyText>
<sectionHeader confidence="0.973343" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.993085612244898">
All experiments were run using 10 fold cross-
validation. We used Viterbi decoding to find the
most likely sequence and report the performance in
terms of label accuracy. We ran all experiments with
varying window sizes (w E {1, 3, 5}). The baseline
which simply assigns the most common label, un-
accented, achieves 60.53 f 1.50%.
Previous research has demonstrated that part of
speech and frequency, or a combination of these
two, are very reliable predictors of pitch accent.
Thus, to test the worthiness of using a CRF model,
the first experiment we ran was a comparison of an
HMM to a CRF using just the combination of part of
speech and unigram. The HMM score (referred as
HMM.-POS, Unigram in Table 5) was 68.62 f 1.78,
while the CRF model (referred as CRF.-POS, Uni-
gram in Table 5) performed significantly better at
72.56 f 1.86. Note that Pan and McKeown (1999)
reported 74% accuracy with their HMM model.
The difference is due to the different corpora used
in each case. While they also used spontaneous
speech, it was a limited domain in the sense that
it was speech from discharge orders from doctors
at one medical facility. The SWDB corpus is open
domain conversational speech.
In order to capture some aspects of the IC and
collocational strength of a word, in the second ex-
periment we ran part of speech plus all of the prob-
abilistic variables (referred as CRF.-POS, Prob in
Table 5). The model accuracy was 73.94%, thus
improved over the model using POS and unigram
values by 1.38%.
In the third experiment we wanted to know if TTS
applications that made use of purely textual input
could be aided by the addition of timing and rhythm
variables that can be gleaned from a text string.
Thus, we included the textual features described in
Section 4.3 in addition to the probabilistic and syn-
tactic features (referred as CRF.-POS, Prob, Txt in
Table 5). The accuracy was improved by 1.73%.
For the final experiment, we added the acoustic
variable, resulting in the use of all the variables de-
scribed in Section 4 (referred as CRF.-All in Table
5). We get about 0.5% increase in accuracy, 76.1%
with a window of size w = 1.
Using larger windows resulted in minor increases
in the performance of the model, as summarized in
Table 5. Our best accuracy was 76.36% using all
features in a w = 5 window size.
</bodyText>
<table confidence="0.999350857142857">
Model:Variables w = 1 w = 3 w = 5
Baseline 60.53
HMM: POS,Unigram 68.62
CRF: POS, Unigram 72.56
CRF: POS, Prob 73.94 74.19 74.51
CRF: POS, Prob, Txt 75.67 75.74 75.89
CRF: All 76.1 76.23 76.36
</table>
<tableCaption confidence="0.9721445">
Table 5: Test accuracy of pitch accent prediction on
SWDB using various variables and window sizes.
</tableCaption>
<sectionHeader confidence="0.995766" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999005">
Pitch accent prediction is a difficult task, in that, the
number of different speakers, topics, utterance frag-
ments and disfluent production of the SWBD corpus
only increase this difficulty. The fact that 21% of
the function words are accented indicates that mod-
els of pitch accent that mostly rely on part of speech
and unigram frequency would not fair well with this
corpus. We have presented a model of pitch accent
that captures some of the other factors that influence
accentuation. In addition to adding more probabilis-
tic variables and phonological factors, we have used
a sequence model that captures the interdependence
of accents within a phrase.
Given the distinct natures of corpora used, it is
difficult to compare these results with earlier mod-
els. However, in experiment 1 (HMM: POS, Uni-
gram vs CRF: POS, Unigram) we have shown that
a CRF model achieves a better performance than an
HMM model using the same features. However,
the real strength of CRFs comes from their ability
to incorporate different sources of information effi-
ciently, as is demonstrated in our experiments.
We did not test directly the probabilistic measures
(or collocation measures) that have been used before
for this task, namely information content (IC) (Pan
and McKeown, 1999) and mutual information (Pan
and Hirschberg, 2001). However, the measures we
have used encompass similar information. For ex-
ample, IC is only the additive inverse of our unigram
measure:
</bodyText>
<equation confidence="0.997003">
IC(w) = − log p(w) (6)
</equation>
<bodyText confidence="0.999957659574468">
Rather than using mutual information as a measure
of collocational strength, we used unigram, bigram
and joint probabilities. A model that includes both
joint probability and the unigram probabilities of wz
and wi−1 is comparable to one that includes mutual
information.
Just as the likelihood of a word being accented
is influenced by a following silence or IP bound-
ary, the collocational strength of the target word
with the following word (captured by reverse bi-
gram and reverse joint) is also a factor. With the
use of POS, unigram, and all bigram and joint prob-
abilities, we have shown that (a) CRFs outperform
HMMs, and (b) our probabilistic variables increase
accuracy from a model that include POS + unigram
(73.94% compared to 72.56%).
For tasks in which pitch accent is predicted solely
based on a string of text, without the addition of
acoustic data, we have shown that adding aspects
of rhythm and timing aids in the identification of
accent targets. We used the number of words in
an utterance, where in the utterance a word falls,
how long in both number of syllables and number
of phones all affect accentuation. The addition of
these variables improved the model by nearly 2%.
These results suggest that Accent prediction models
that only make use of textual information could be
improved with the addition of these variables.
While not trying to provide a complete model
of accentuation from acoustic information, in this
study we tested a few acoustic variables that have
not yet been tested. The nature of the SWBD cor-
pus allowed us to investigate the role of disfluencies
and widely variable durations and speech rate on ac-
centuation. Especially speech rate, duration and sur-
rounding silence are good predictors of pitch accent.
The addition of these predictors only slightly im-
proved the model (about .5%). Acoustic features are
very sensitive to individual speakers. In the corpus,
there are many different speakers of varying ages
and dialects. These variables might become more
useful if one controls for individual speaker differ-
ences. To really test the usefulness of these vari-
ables, one would have to combine them with acous-
tic features that have been demonstrated to be good
predictors of pitch accent (Sun, 2002; Conkie et al.,
1999; Wightman et al., 2000).
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987">
We used CRFs with new measures of collocational
strength and new phonological factors that capture
aspects of rhythm and timing to model pitch accent
prediction. CRFs have the theoretical advantage of
incorporating all these factors in a principled and ef-
ficient way. We demonstrated that CRFs outperform
HMMs also experimentally. We also demonstrated
the usefulness of some new probabilistic variables
and phonological variables. Our results mainly have
implications for the textual prediction of accents in
TTS applications, but might also be useful in au-
tomatic speech recognition tasks such as automatic
transcription of multi-speaker meetings. In the near
future we would like to incorporate reliable acoustic
information, controlling for individual speaker dif-
ference and also apply different discriminative se-
quence labeling techniques to pitch accent predic-
tion task.
</bodyText>
<sectionHeader confidence="0.994918" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998385">
This work was partially funded by CAREER award
#IIS 9733067 IGERT. We would also like to thank
Mark Johnson for the idea of this project, Dan Ju-
rafsky, Alan Bell, Cynthia Girand, and Jason Bre-
nier for their helpful comments and help with the
database.
</bodyText>
<sectionHeader confidence="0.997745" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999344">
Y. Altun, T. Hofmann, and M. Johnson. 2003a.
Discriminative learning for label sequences via
boosting. In Proc. of Advances in Neural Infor-
mation Processing Systems.
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003b.
Hidden markov support vector machines. In
Proc. of 20th International Conference on Ma-
chine Learning.
M. Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and ex-
periments with perceptron algorithms. In Proc.
of Empirical Methods of Natural Language Pro-
cessing.
A. Conkie, G. Riccardi, and R. Rose. 1999.
Prosody recognition from speech utterances us-
ing acoustic and linguistic based models of
prosodic events. In Proc. ofEUROSPEECH’99.
W. J. Conover. 1980. Practical Nonparametric
Statistics. Wiley, New York, 2nd edition.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on conversa-
tional pronunci ations. In Speech Communica-
tion.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and develo pment. In Proc. of the Inter-
national Conference on Acoustics, Speech, and
Signal Processing.
S. Greenberg, D. Ellis, and J. Hollenback. 1996. In-
sights into spoken language gleaned from pho-
netic transcripti on of the Switchboard corpus.
In Proc. of International Conference on Spoken
Language Processsing.
J. Hirschberg. 1993. Pitch accent in context: Pre-
dicting intonational prominence from text. Artifi-
cialIntelligence, 63(1-2):305–340.
M. Johnson, S. Geman, S. Canon, Z. Chi, and
S. Riezler. 1999. Estimators for stochastic
unification-based grammars. In Proc. of ACL’99
Association for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proc. of 18th International Conference on Ma-
chine Learning.
A. McCallum, D. Freitag, and F. Pereira. 2000.
Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proc.
of 17th International Conference on Machine
Learning.
A. McCallum. 2003. Efficiently inducing features
of Conditional Random Fields. In Proc. of Un-
certainty in Articifical Intelligence.
T. Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Technical report,
CMU, Department of Statistics, TR 758.
S. Pan and J. Hirschberg. 2001. Modeling local
context for pitch accent prediction. In Proc. of
ACL’01, Association for Computational Linguis-
tics.
S. Pan and K. McKeown. 1999. Word informa-
tiveness and automatic pitch accent modeling.
In Proc. of the Joint SIGDAT Conference on
EMNLP and VLC.
V. Punyakanok and D. Roth. 2000. The use of
classifiers in sequential inference. In Proc. of
Advances in Neural Information Processing Sys-
tems.
F. Sha and F. Pereira. 2003. Shallow parsing with
conditional random fields. In Proc. of Human
Language Technology.
Xuejing Sun. 2002. Pitch accent prediction using
ensemble machine learning. In Proc. of the In-
ternational Conference on Spoken Language Pro-
cessing.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-
margin markov networks. In Proc. of Advances
in Neural Information Processing Systems.
P. Taylor. 2000. Analysis and synthesis of intona-
tion using the Tilt model. Journal of the Acousti-
cal Society ofAmerica.
C. W. Wightman, A. K. Syrdal, G. Stemmer,
A. Conkie, and M. Beutnagel. 2000. Percep-
tually Based Automatic Prosody Labeling and
Prosodically Enriched Unit Selection Improve
Concatenative Text-To-Speech Synthesis. vol-
ume 2, pages 71–74.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.465175">
<title confidence="0.9844855">Using Conditional Random Fields to Predict Pitch Accents in Conversational Speech</title>
<author confidence="0.999726">Michelle L Gregory</author>
<affiliation confidence="0.9956265">Linguistics Department University at Buffalo</affiliation>
<address confidence="0.994254">Buffalo, NY 14260</address>
<email confidence="0.998348">mgregory@buffalo.edu</email>
<author confidence="0.991244">Yasemin Altun</author>
<affiliation confidence="0.999922">Department of Computer Science Brown University</affiliation>
<address confidence="0.999091">Providence, RI 02912</address>
<email confidence="0.999682">altun@cs.brown.edu</email>
<abstract confidence="0.974308">The detection of prosodic characteristics is an important aspect of both speech synthesis and speech recognition. Correct placement of pitch accents aids in more natural sounding speech, while automatic detection of accents can contribute to better wordlevel recognition and better textual understanding. In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting. We introduce Conditional Random Fields (CRFs) to pitch accent prediction task in order to incorporate these factors efficiently in a sequence model. We demonstrate the usefulness and the incremental effect of these factors in a sequence model by performing experiments on hand labeled data from the Switchboard Corpus. Our model outperforms the baseline and previous models of pitch accent prediction on the Switchboard Corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>T Hofmann</author>
<author>M Johnson</author>
</authors>
<title>Discriminative learning for label sequences via boosting. In</title>
<date>2003</date>
<booktitle>Proc. of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5257" citStr="Altun et al., 2003" startWordPosition="821" endWordPosition="824">vely using maximum likelihood estimation to model the joint probability of the observation and label sequences. Also, they require questionable independence assumptions to achieve efficient inference and learning. Therefore, variables used in Hidden Markov models of pitch accent prediction have been very limited, e.g. part of speech and frequency (Pan and McKeown, 1999). Discriminative learning methods, such as Maximum Entropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a string of text will bear pitch accent. Our contribution is two-fold: employing new predicto</context>
</contexts>
<marker>Altun, Hofmann, Johnson, 2003</marker>
<rawString>Y. Altun, T. Hofmann, and M. Johnson. 2003a. Discriminative learning for label sequences via boosting. In Proc. of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
</authors>
<title>Hidden markov support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of 20th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5257" citStr="Altun et al., 2003" startWordPosition="821" endWordPosition="824">vely using maximum likelihood estimation to model the joint probability of the observation and label sequences. Also, they require questionable independence assumptions to achieve efficient inference and learning. Therefore, variables used in Hidden Markov models of pitch accent prediction have been very limited, e.g. part of speech and frequency (Pan and McKeown, 1999). Discriminative learning methods, such as Maximum Entropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a string of text will bear pitch accent. Our contribution is two-fold: employing new predicto</context>
</contexts>
<marker>Altun, Tsochantaridis, Hofmann, 2003</marker>
<rawString>Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003b. Hidden markov support vector machines. In Proc. of 20th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of Empirical Methods of Natural Language Processing.</booktitle>
<contexts>
<context position="5296" citStr="Collins, 2002" startWordPosition="827" endWordPosition="828"> model the joint probability of the observation and label sequences. Also, they require questionable independence assumptions to achieve efficient inference and learning. Therefore, variables used in Hidden Markov models of pitch accent prediction have been very limited, e.g. part of speech and frequency (Pan and McKeown, 1999). Discriminative learning methods, such as Maximum Entropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a string of text will bear pitch accent. Our contribution is two-fold: employing new predictors and utilizing a discriminative model</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proc. of Empirical Methods of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Conkie</author>
<author>G Riccardi</author>
<author>R Rose</author>
</authors>
<title>Prosody recognition from speech utterances using acoustic and linguistic based models of prosodic events.</title>
<date>1999</date>
<booktitle>In Proc. ofEUROSPEECH’99.</booktitle>
<contexts>
<context position="4262" citStr="Conkie et al., 1999" startWordPosition="671" endWordPosition="674">last word of an IP tends be accented. In short, accented words within the same IP are not independent of each other. Previous work on pitch accent prediction, however, neglected the dependency between labels. Different machine learning techniques, such as decision trees (Hirschberg, 1993), rule induction systems (Pan and McKeown, 1999), bagging (Sun, 2002), boosting (Sun, 2002) have been used in a scenario where the accent of each word is predicted independently. One exception to this line of research is the use of Hidden Markov Models (HMM) for pitch accent prediction (Pan and McKeown, 1999; Conkie et al., 1999). Pan and McKeown (1999) demonstrate the effectiveness of a sequence model over a rule induction system, RIPPER, that treats each label independently by showing that HMMs outperform RIPPER when the same variables are used. Until recently, HMMs were the predominant formalism to model label sequences. However, they have two major shortcomings. They are trained non-discriminatively using maximum likelihood estimation to model the joint probability of the observation and label sequences. Also, they require questionable independence assumptions to achieve efficient inference and learning. Therefore</context>
<context position="25310" citStr="Conkie et al., 1999" startWordPosition="4225" endWordPosition="4228">centuation. Especially speech rate, duration and surrounding silence are good predictors of pitch accent. The addition of these predictors only slightly improved the model (about .5%). Acoustic features are very sensitive to individual speakers. In the corpus, there are many different speakers of varying ages and dialects. These variables might become more useful if one controls for individual speaker differences. To really test the usefulness of these variables, one would have to combine them with acoustic features that have been demonstrated to be good predictors of pitch accent (Sun, 2002; Conkie et al., 1999; Wightman et al., 2000). 7 Conclusion We used CRFs with new measures of collocational strength and new phonological factors that capture aspects of rhythm and timing to model pitch accent prediction. CRFs have the theoretical advantage of incorporating all these factors in a principled and efficient way. We demonstrated that CRFs outperform HMMs also experimentally. We also demonstrated the usefulness of some new probabilistic variables and phonological variables. Our results mainly have implications for the textual prediction of accents in TTS applications, but might also be useful in automa</context>
</contexts>
<marker>Conkie, Riccardi, Rose, 1999</marker>
<rawString>A. Conkie, G. Riccardi, and R. Rose. 1999. Prosody recognition from speech utterances using acoustic and linguistic based models of prosodic events. In Proc. ofEUROSPEECH’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Conover</author>
</authors>
<title>Practical Nonparametric Statistics.</title>
<date>1980</date>
<publisher>Wiley,</publisher>
<location>New York,</location>
<note>2nd edition.</note>
<contexts>
<context position="14162" citStr="Conover, 1980" startWordPosition="2312" endWordPosition="2313">dictability of a word given the following word (reverse bigram), the joint probability of a word with the preceding (joint), and the joint probability of a word with the following word (reverse joint). Table 2 provides the definition for these, as well as high probability examples from the corpus (the emphasized word being the current target). Note all probabilistic variables were in log scale. The values for these probabilities were obtained using the entire 2.4 million words of SWBD2. Table 3 presents the Spearman’s rank correlation coefficient between the probabilistic measures and accent (Conover, 1980). These values indicate the strong correlation of accents to the probabilistic variables. As the probability increases, the chance of an accent decreases. Note that all values are significant at the p &lt; .001 level. We also created a combined part of speech and unigram frequency variable in order to have a variable that corresponds to the variable used in (Pan 2Our current implementation of CRF only takes categorical variables, thus for the experiments, all probabilistic variables were binned into 5 equal categories. We also tried more bins and produced similar results, so we only report on the</context>
</contexts>
<marker>Conover, 1980</marker>
<rawString>W. J. Conover. 1980. Practical Nonparametric Statistics. Wiley, New York, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fosler-Lussier</author>
<author>N Morgan</author>
</authors>
<title>Effects of speaking rate and word frequency on conversational pronunci ations. In Speech Communication.</title>
<date>1999</date>
<contexts>
<context position="10795" citStr="Fosler-Lussier and Morgan, 1999" startWordPosition="1750" endWordPosition="1753">lephone conversations between adult speakers (approximately 2.4 million words). Participants were both male and female and represented all major dialects of American English. We used a portion of this corpus that was phonetically handtranscribed (Greenberg et al., 1996) and segmented into speech boundaries at turn boundaries or pauses of more than 500 ms on both sides. Fragments contained seven words on average. Additionally, each word was coded for probabilistic and contextual information, such as word frequency, conditional probabilities, the rate of speech, and the canonical pronunciation (Fosler-Lussier and Morgan, 1999). The dataset used in all analysis in this study consists of only the first hour of the database, comprised of 1,824 utterances with 13,190 words. These utterances were hand coded for pitch accent and intonational phrase brakes. 3.1 Pitch Accent Coding The utterances were hand labeled for accents and boundaries according to the Tilt Intonational Model (Taylor, 2000). This model is characterized by a series of intonational events: accents and boundaries. Labelers were instructed to use duration, amplitude, pausing information, and changes in f0 to identify events. In general, labelers followed </context>
</contexts>
<marker>Fosler-Lussier, Morgan, 1999</marker>
<rawString>E. Fosler-Lussier and N. Morgan. 1999. Effects of speaking rate and word frequency on conversational pronunci ations. In Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and develo pment.</title>
<date>1992</date>
<booktitle>In Proc. of the International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="10136" citStr="Godfrey et al., 1992" startWordPosition="1652" endWordPosition="1655">dient descent method to optimize the above objective function. The gradients are computed as in Equation 4: Ep[1Ft(xi, y)] − IFt(xi, yi) + cA (4) where the expectation is with respect to all possible label sequences of the observation sequence xi and can be computed using the forward backward algorithm. Given an observation sequence x, the best label sequence is given by: F(x, y; ˆA) (5) where Aˆ is the parameter vector that minimizes L(A; D). The best label sequence can be identified by performing the Viterbi algorithm. 3 Corpus The data for this study were taken from the Switchboard Corpus (Godfrey et al., 1992), which consists of 2430 telephone conversations between adult speakers (approximately 2.4 million words). Participants were both male and female and represented all major dialects of American English. We used a portion of this corpus that was phonetically handtranscribed (Greenberg et al., 1996) and segmented into speech boundaries at turn boundaries or pauses of more than 500 ms on both sides. Fragments contained seven words on average. Additionally, each word was coded for probabilistic and contextual information, such as word frequency, conditional probabilities, the rate of speech, and th</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey, E. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and develo pment. In Proc. of the International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Greenberg</author>
<author>D Ellis</author>
<author>J Hollenback</author>
</authors>
<title>Insights into spoken language gleaned from phonetic transcripti on of the Switchboard corpus.</title>
<date>1996</date>
<booktitle>In Proc. of International Conference on Spoken Language Processsing.</booktitle>
<contexts>
<context position="10433" citStr="Greenberg et al., 1996" startWordPosition="1697" endWordPosition="1700">lgorithm. Given an observation sequence x, the best label sequence is given by: F(x, y; ˆA) (5) where Aˆ is the parameter vector that minimizes L(A; D). The best label sequence can be identified by performing the Viterbi algorithm. 3 Corpus The data for this study were taken from the Switchboard Corpus (Godfrey et al., 1992), which consists of 2430 telephone conversations between adult speakers (approximately 2.4 million words). Participants were both male and female and represented all major dialects of American English. We used a portion of this corpus that was phonetically handtranscribed (Greenberg et al., 1996) and segmented into speech boundaries at turn boundaries or pauses of more than 500 ms on both sides. Fragments contained seven words on average. Additionally, each word was coded for probabilistic and contextual information, such as word frequency, conditional probabilities, the rate of speech, and the canonical pronunciation (Fosler-Lussier and Morgan, 1999). The dataset used in all analysis in this study consists of only the first hour of the database, comprised of 1,824 utterances with 13,190 words. These utterances were hand coded for pitch accent and intonational phrase brakes. 3.1 Pitch</context>
</contexts>
<marker>Greenberg, Ellis, Hollenback, 1996</marker>
<rawString>S. Greenberg, D. Ellis, and J. Hollenback. 1996. Insights into spoken language gleaned from phonetic transcripti on of the Switchboard corpus. In Proc. of International Conference on Spoken Language Processsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
</authors>
<title>Pitch accent in context: Predicting intonational prominence from text.</title>
<date>1993</date>
<journal>ArtificialIntelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="2404" citStr="Hirschberg, 1993" startWordPosition="363" endWordPosition="364">ers in an utterance. For example, a word with a fundamental frequency (f0) of 120 Hz would likely be quite prominent in a male speaker, but not for a typical female speaker. Likewise, the accent on the utterance ”Jon’s leaving.” is critical in determining whether it is the answer to the question ”Who is leaving?” (”JON’s leaving.”) or ”What is Jon doing?” (”Jon’s LEAVING.”). Accurate pitch accent prediction lies in the successful combination of as many of the contextual variables as possible. Syntactic information such as part of speech has proven to be a successful predictor of accentuation (Hirschberg, 1993; Pan and Hirschberg, 2001). In general, function words are not accented, while content words are. Various measures of a word’s informativeness, such as the information content (IC) of a word (Pan and McKeown, 1999) and its collocational strength in a given context (Pan and Hirschberg, 2001) have also proven to be useful models of pitch accent. However, in open topic conversational speech, accent is very unpredictable. Part of speech and the informativeness of a word do not capture all aspects of accentuation, as we see in this example taken from Switchboard, where a function word gets accente</context>
<context position="3931" citStr="Hirschberg, 1993" startWordPosition="615" endWordPosition="616">h accent also affect the likelihood of accentuation. In other words, a word that might typically be accented may be unaccented because the surrounding words also bear pitch accent. Phrase boundaries seem to play a role in accentuation as well. The first word of intonational phrases (IP) is less likely to be accented while the last word of an IP tends be accented. In short, accented words within the same IP are not independent of each other. Previous work on pitch accent prediction, however, neglected the dependency between labels. Different machine learning techniques, such as decision trees (Hirschberg, 1993), rule induction systems (Pan and McKeown, 1999), bagging (Sun, 2002), boosting (Sun, 2002) have been used in a scenario where the accent of each word is predicted independently. One exception to this line of research is the use of Hidden Markov Models (HMM) for pitch accent prediction (Pan and McKeown, 1999; Conkie et al., 1999). Pan and McKeown (1999) demonstrate the effectiveness of a sequence model over a rule induction system, RIPPER, that treats each label independently by showing that HMMs outperform RIPPER when the same variables are used. Until recently, HMMs were the predominant form</context>
</contexts>
<marker>Hirschberg, 1993</marker>
<rawString>J. Hirschberg. 1993. Pitch accent in context: Predicting intonational prominence from text. ArtificialIntelligence, 63(1-2):305–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>S Geman</author>
<author>S Canon</author>
<author>Z Chi</author>
<author>S Riezler</author>
</authors>
<title>Estimators for stochastic unification-based grammars.</title>
<date>1999</date>
<booktitle>In Proc. of ACL’99 Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9100" citStr="Johnson et al., 1999" startWordPosition="1474" endWordPosition="1477">he observation-label dependencies is explained in detail in Section 4. In CRFs, the objective function is the log-loss of the model with A parameters with respect to a training set D. This function is defined as the negative sum of the conditional probabilities of each training label sequence yi, given the observation sequence xi, where D - {(xi, yi) : i = 1, ... , m}. CRFs are known to overfit, especially with noisy data if not regularized. To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm ||A||2) as suggested in (Johnson et al., 1999). Then the loss function is given as: log p(yi|xi; A) + 2c||A||2 1 F(xi, yi; A) + log Z(xi, A) c||A||2 (3) where c is a constant. Lafferty et al. (2001), proposed a modification of improved iterative scaling for parameter estimation in CRFs. However, gradient-based methods have often found to be more efficient for minimizing Equation 3 (Minka, 2001; Sha and Pereira, 2003). In this paper, we use the conjugate gradient descent method to optimize the above objective function. The gradients are computed as in Equation 4: Ep[1Ft(xi, y)] − IFt(xi, yi) + cA (4) where the expectation is with respect t</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999. Estimators for stochastic unification-based grammars. In Proc. of ACL’99 Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of 18th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5218" citStr="Lafferty et al., 2001" startWordPosition="815" endWordPosition="818">tcomings. They are trained non-discriminatively using maximum likelihood estimation to model the joint probability of the observation and label sequences. Also, they require questionable independence assumptions to achieve efficient inference and learning. Therefore, variables used in Hidden Markov models of pitch accent prediction have been very limited, e.g. part of speech and frequency (Pan and McKeown, 1999). Discriminative learning methods, such as Maximum Entropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a string of text will bear pitch accent. Our contribut</context>
<context position="9252" citStr="Lafferty et al. (2001)" startWordPosition="1505" endWordPosition="1508">ith respect to a training set D. This function is defined as the negative sum of the conditional probabilities of each training label sequence yi, given the observation sequence xi, where D - {(xi, yi) : i = 1, ... , m}. CRFs are known to overfit, especially with noisy data if not regularized. To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm ||A||2) as suggested in (Johnson et al., 1999). Then the loss function is given as: log p(yi|xi; A) + 2c||A||2 1 F(xi, yi; A) + log Z(xi, A) c||A||2 (3) where c is a constant. Lafferty et al. (2001), proposed a modification of improved iterative scaling for parameter estimation in CRFs. However, gradient-based methods have often found to be more efficient for minimizing Equation 3 (Minka, 2001; Sha and Pereira, 2003). In this paper, we use the conjugate gradient descent method to optimize the above objective function. The gradients are computed as in Equation 4: Ep[1Ft(xi, y)] − IFt(xi, yi) + cA (4) where the expectation is with respect to all possible label sequences of the observation sequence xi and can be computed using the forward backward algorithm. Given an observation sequence x,</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of 18th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Freitag</author>
<author>F Pereira</author>
</authors>
<title>Maximum Entropy Markov Models for Information Extraction and Segmentation.</title>
<date>2000</date>
<booktitle>In Proc. of 17th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5107" citStr="McCallum et al., 2000" startWordPosition="800" endWordPosition="803">Until recently, HMMs were the predominant formalism to model label sequences. However, they have two major shortcomings. They are trained non-discriminatively using maximum likelihood estimation to model the joint probability of the observation and label sequences. Also, they require questionable independence assumptions to achieve efficient inference and learning. Therefore, variables used in Hidden Markov models of pitch accent prediction have been very limited, e.g. part of speech and frequency (Pan and McKeown, 1999). Discriminative learning methods, such as Maximum Entropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>A. McCallum, D. Freitag, and F. Pereira. 2000. Maximum Entropy Markov Models for Information Extraction and Segmentation. In Proc. of 17th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Efficiently inducing features of Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proc. of Uncertainty in Articifical Intelligence.</booktitle>
<contexts>
<context position="5702" citStr="McCallum, 2003" startWordPosition="891" endWordPosition="892">Callum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a string of text will bear pitch accent. Our contribution is two-fold: employing new predictors and utilizing a discriminative model. We combine the advantages of probabilistic, syntactic, and phonological predictors with the advantages of modeling pitch accent in a sequence labeling setting using CRFs (Lafferty et al., 2001). The rest of the paper is organized as follows: In Section 2, we introduce CRFs. Then, we describe our corpus and the variables in Section 3 and Section 4. We present the experimental setup and report results i</context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>A. McCallum. 2003. Efficiently inducing features of Conditional Random Fields. In Proc. of Uncertainty in Articifical Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Minka</author>
</authors>
<title>Algorithms for maximumlikelihood logistic regression.</title>
<date>2001</date>
<tech>Technical report, CMU,</tech>
<institution>Department of Statistics,</institution>
<contexts>
<context position="9450" citStr="Minka, 2001" startWordPosition="1536" endWordPosition="1537">, ... , m}. CRFs are known to overfit, especially with noisy data if not regularized. To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm ||A||2) as suggested in (Johnson et al., 1999). Then the loss function is given as: log p(yi|xi; A) + 2c||A||2 1 F(xi, yi; A) + log Z(xi, A) c||A||2 (3) where c is a constant. Lafferty et al. (2001), proposed a modification of improved iterative scaling for parameter estimation in CRFs. However, gradient-based methods have often found to be more efficient for minimizing Equation 3 (Minka, 2001; Sha and Pereira, 2003). In this paper, we use the conjugate gradient descent method to optimize the above objective function. The gradients are computed as in Equation 4: Ep[1Ft(xi, y)] − IFt(xi, yi) + cA (4) where the expectation is with respect to all possible label sequences of the observation sequence xi and can be computed using the forward backward algorithm. Given an observation sequence x, the best label sequence is given by: F(x, y; ˆA) (5) where Aˆ is the parameter vector that minimizes L(A; D). The best label sequence can be identified by performing the Viterbi algorithm. 3 Corpus</context>
</contexts>
<marker>Minka, 2001</marker>
<rawString>T. Minka. 2001. Algorithms for maximumlikelihood logistic regression. Technical report, CMU, Department of Statistics, TR 758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>J Hirschberg</author>
</authors>
<title>Modeling local context for pitch accent prediction.</title>
<date>2001</date>
<booktitle>In Proc. of ACL’01, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2431" citStr="Pan and Hirschberg, 2001" startWordPosition="365" endWordPosition="368">e. For example, a word with a fundamental frequency (f0) of 120 Hz would likely be quite prominent in a male speaker, but not for a typical female speaker. Likewise, the accent on the utterance ”Jon’s leaving.” is critical in determining whether it is the answer to the question ”Who is leaving?” (”JON’s leaving.”) or ”What is Jon doing?” (”Jon’s LEAVING.”). Accurate pitch accent prediction lies in the successful combination of as many of the contextual variables as possible. Syntactic information such as part of speech has proven to be a successful predictor of accentuation (Hirschberg, 1993; Pan and Hirschberg, 2001). In general, function words are not accented, while content words are. Various measures of a word’s informativeness, such as the information content (IC) of a word (Pan and McKeown, 1999) and its collocational strength in a given context (Pan and Hirschberg, 2001) have also proven to be useful models of pitch accent. However, in open topic conversational speech, accent is very unpredictable. Part of speech and the informativeness of a word do not capture all aspects of accentuation, as we see in this example taken from Switchboard, where a function word gets accented (accented words are in up</context>
<context position="13358" citStr="Pan and Hirschberg, 2001" startWordPosition="2180" endWordPosition="2183">ification. L(A; D) = − Em i Em i 1 + 2 Em i VAL = E t yˆ = arg max Y Accented Unaccented Function 21% 79% Verb 59% 41% Noun 30% 70% Other 49% 51% Table 1: Percentage of accented and unaccented items by POS. Variable Definition Example Unigram log p(wi) and, I Bigram log p(wi|wi_1) roughing it Rev Bigram log p(wi|wi+1) rid of Joint log p(wi_1, wi) and I Rev Joint log p(wi, wi+1) and I Table 2: Definition of probabilistic variables. 4.2 Probabilistic variables Following a line of research that incorporates the information content of a word as well as collocation measures (Pan and McKeown, 1999; Pan and Hirschberg, 2001) we have included a number of probabilistic variables. The probabilistic variables we used were the unigram frequency, the predictability of a word given the preceding word (bigram), the predictability of a word given the following word (reverse bigram), the joint probability of a word with the preceding (joint), and the joint probability of a word with the following word (reverse joint). Table 2 provides the definition for these, as well as high probability examples from the corpus (the emphasized word being the current target). Note all probabilistic variables were in log scale. The values f</context>
<context position="22887" citStr="Pan and Hirschberg, 2001" startWordPosition="3821" endWordPosition="3824">a used, it is difficult to compare these results with earlier models. However, in experiment 1 (HMM: POS, Unigram vs CRF: POS, Unigram) we have shown that a CRF model achieves a better performance than an HMM model using the same features. However, the real strength of CRFs comes from their ability to incorporate different sources of information efficiently, as is demonstrated in our experiments. We did not test directly the probabilistic measures (or collocation measures) that have been used before for this task, namely information content (IC) (Pan and McKeown, 1999) and mutual information (Pan and Hirschberg, 2001). However, the measures we have used encompass similar information. For example, IC is only the additive inverse of our unigram measure: IC(w) = − log p(w) (6) Rather than using mutual information as a measure of collocational strength, we used unigram, bigram and joint probabilities. A model that includes both joint probability and the unigram probabilities of wz and wi−1 is comparable to one that includes mutual information. Just as the likelihood of a word being accented is influenced by a following silence or IP boundary, the collocational strength of the target word with the following wor</context>
</contexts>
<marker>Pan, Hirschberg, 2001</marker>
<rawString>S. Pan and J. Hirschberg. 2001. Modeling local context for pitch accent prediction. In Proc. of ACL’01, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pan</author>
<author>K McKeown</author>
</authors>
<title>Word informativeness and automatic pitch accent modeling.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conference on EMNLP and VLC.</booktitle>
<contexts>
<context position="2619" citStr="Pan and McKeown, 1999" startWordPosition="396" endWordPosition="399">ce ”Jon’s leaving.” is critical in determining whether it is the answer to the question ”Who is leaving?” (”JON’s leaving.”) or ”What is Jon doing?” (”Jon’s LEAVING.”). Accurate pitch accent prediction lies in the successful combination of as many of the contextual variables as possible. Syntactic information such as part of speech has proven to be a successful predictor of accentuation (Hirschberg, 1993; Pan and Hirschberg, 2001). In general, function words are not accented, while content words are. Various measures of a word’s informativeness, such as the information content (IC) of a word (Pan and McKeown, 1999) and its collocational strength in a given context (Pan and Hirschberg, 2001) have also proven to be useful models of pitch accent. However, in open topic conversational speech, accent is very unpredictable. Part of speech and the informativeness of a word do not capture all aspects of accentuation, as we see in this example taken from Switchboard, where a function word gets accented (accented words are in uppercase): I, I have STRONG OBJECTIONS to THAT. Accent is also influenced by aspects of rhythm and timing. The length of words, in both number of phones and normalized duration, affect its </context>
<context position="3979" citStr="Pan and McKeown, 1999" startWordPosition="621" endWordPosition="624">ntuation. In other words, a word that might typically be accented may be unaccented because the surrounding words also bear pitch accent. Phrase boundaries seem to play a role in accentuation as well. The first word of intonational phrases (IP) is less likely to be accented while the last word of an IP tends be accented. In short, accented words within the same IP are not independent of each other. Previous work on pitch accent prediction, however, neglected the dependency between labels. Different machine learning techniques, such as decision trees (Hirschberg, 1993), rule induction systems (Pan and McKeown, 1999), bagging (Sun, 2002), boosting (Sun, 2002) have been used in a scenario where the accent of each word is predicted independently. One exception to this line of research is the use of Hidden Markov Models (HMM) for pitch accent prediction (Pan and McKeown, 1999; Conkie et al., 1999). Pan and McKeown (1999) demonstrate the effectiveness of a sequence model over a rule induction system, RIPPER, that treats each label independently by showing that HMMs outperform RIPPER when the same variables are used. Until recently, HMMs were the predominant formalism to model label sequences. However, they ha</context>
<context position="13331" citStr="Pan and McKeown, 1999" startWordPosition="2176" endWordPosition="2179">t on the four-way classification. L(A; D) = − Em i Em i 1 + 2 Em i VAL = E t yˆ = arg max Y Accented Unaccented Function 21% 79% Verb 59% 41% Noun 30% 70% Other 49% 51% Table 1: Percentage of accented and unaccented items by POS. Variable Definition Example Unigram log p(wi) and, I Bigram log p(wi|wi_1) roughing it Rev Bigram log p(wi|wi+1) rid of Joint log p(wi_1, wi) and I Rev Joint log p(wi, wi+1) and I Table 2: Definition of probabilistic variables. 4.2 Probabilistic variables Following a line of research that incorporates the information content of a word as well as collocation measures (Pan and McKeown, 1999; Pan and Hirschberg, 2001) we have included a number of probabilistic variables. The probabilistic variables we used were the unigram frequency, the predictability of a word given the preceding word (bigram), the predictability of a word given the following word (reverse bigram), the joint probability of a word with the preceding (joint), and the joint probability of a word with the following word (reverse joint). Table 2 provides the definition for these, as well as high probability examples from the corpus (the emphasized word being the current target). Note all probabilistic variables were</context>
<context position="19816" citStr="Pan and McKeown (1999)" startWordPosition="3295" endWordPosition="3298">ine which simply assigns the most common label, unaccented, achieves 60.53 f 1.50%. Previous research has demonstrated that part of speech and frequency, or a combination of these two, are very reliable predictors of pitch accent. Thus, to test the worthiness of using a CRF model, the first experiment we ran was a comparison of an HMM to a CRF using just the combination of part of speech and unigram. The HMM score (referred as HMM.-POS, Unigram in Table 5) was 68.62 f 1.78, while the CRF model (referred as CRF.-POS, Unigram in Table 5) performed significantly better at 72.56 f 1.86. Note that Pan and McKeown (1999) reported 74% accuracy with their HMM model. The difference is due to the different corpora used in each case. While they also used spontaneous speech, it was a limited domain in the sense that it was speech from discharge orders from doctors at one medical facility. The SWDB corpus is open domain conversational speech. In order to capture some aspects of the IC and collocational strength of a word, in the second experiment we ran part of speech plus all of the probabilistic variables (referred as CRF.-POS, Prob in Table 5). The model accuracy was 73.94%, thus improved over the model using POS</context>
<context position="22837" citStr="Pan and McKeown, 1999" startWordPosition="3814" endWordPosition="3817"> a phrase. Given the distinct natures of corpora used, it is difficult to compare these results with earlier models. However, in experiment 1 (HMM: POS, Unigram vs CRF: POS, Unigram) we have shown that a CRF model achieves a better performance than an HMM model using the same features. However, the real strength of CRFs comes from their ability to incorporate different sources of information efficiently, as is demonstrated in our experiments. We did not test directly the probabilistic measures (or collocation measures) that have been used before for this task, namely information content (IC) (Pan and McKeown, 1999) and mutual information (Pan and Hirschberg, 2001). However, the measures we have used encompass similar information. For example, IC is only the additive inverse of our unigram measure: IC(w) = − log p(w) (6) Rather than using mutual information as a measure of collocational strength, we used unigram, bigram and joint probabilities. A model that includes both joint probability and the unigram probabilities of wz and wi−1 is comparable to one that includes mutual information. Just as the likelihood of a word being accented is influenced by a following silence or IP boundary, the collocational </context>
</contexts>
<marker>Pan, McKeown, 1999</marker>
<rawString>S. Pan and K. McKeown. 1999. Word informativeness and automatic pitch accent modeling. In Proc. of the Joint SIGDAT Conference on EMNLP and VLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2000</date>
<booktitle>In Proc. of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5167" citStr="Punyakanok and Roth, 2000" startWordPosition="808" endWordPosition="811">odel label sequences. However, they have two major shortcomings. They are trained non-discriminatively using maximum likelihood estimation to model the joint probability of the observation and label sequences. Also, they require questionable independence assumptions to achieve efficient inference and learning. Therefore, variables used in Hidden Markov models of pitch accent prediction have been very limited, e.g. part of speech and frequency (Pan and McKeown, 1999). Discriminative learning methods, such as Maximum Entropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a s</context>
</contexts>
<marker>Punyakanok, Roth, 2000</marker>
<rawString>V. Punyakanok and D. Roth. 2000. The use of classifiers in sequential inference. In Proc. of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of Human Language Technology.</booktitle>
<contexts>
<context position="5685" citStr="Sha and Pereira, 2003" startWordPosition="887" endWordPosition="890">tropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a string of text will bear pitch accent. Our contribution is two-fold: employing new predictors and utilizing a discriminative model. We combine the advantages of probabilistic, syntactic, and phonological predictors with the advantages of modeling pitch accent in a sequence labeling setting using CRFs (Lafferty et al., 2001). The rest of the paper is organized as follows: In Section 2, we introduce CRFs. Then, we describe our corpus and the variables in Section 3 and Section 4. We present the experimental setup and</context>
<context position="9474" citStr="Sha and Pereira, 2003" startWordPosition="1538" endWordPosition="1541">RFs are known to overfit, especially with noisy data if not regularized. To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm ||A||2) as suggested in (Johnson et al., 1999). Then the loss function is given as: log p(yi|xi; A) + 2c||A||2 1 F(xi, yi; A) + log Z(xi, A) c||A||2 (3) where c is a constant. Lafferty et al. (2001), proposed a modification of improved iterative scaling for parameter estimation in CRFs. However, gradient-based methods have often found to be more efficient for minimizing Equation 3 (Minka, 2001; Sha and Pereira, 2003). In this paper, we use the conjugate gradient descent method to optimize the above objective function. The gradients are computed as in Equation 4: Ep[1Ft(xi, y)] − IFt(xi, yi) + cA (4) where the expectation is with respect to all possible label sequences of the observation sequence xi and can be computed using the forward backward algorithm. Given an observation sequence x, the best label sequence is given by: F(x, y; ˆA) (5) where Aˆ is the parameter vector that minimizes L(A; D). The best label sequence can be identified by performing the Viterbi algorithm. 3 Corpus The data for this study</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuejing Sun</author>
</authors>
<title>Pitch accent prediction using ensemble machine learning.</title>
<date>2002</date>
<booktitle>In Proc. of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="4000" citStr="Sun, 2002" startWordPosition="626" endWordPosition="627">that might typically be accented may be unaccented because the surrounding words also bear pitch accent. Phrase boundaries seem to play a role in accentuation as well. The first word of intonational phrases (IP) is less likely to be accented while the last word of an IP tends be accented. In short, accented words within the same IP are not independent of each other. Previous work on pitch accent prediction, however, neglected the dependency between labels. Different machine learning techniques, such as decision trees (Hirschberg, 1993), rule induction systems (Pan and McKeown, 1999), bagging (Sun, 2002), boosting (Sun, 2002) have been used in a scenario where the accent of each word is predicted independently. One exception to this line of research is the use of Hidden Markov Models (HMM) for pitch accent prediction (Pan and McKeown, 1999; Conkie et al., 1999). Pan and McKeown (1999) demonstrate the effectiveness of a sequence model over a rule induction system, RIPPER, that treats each label independently by showing that HMMs outperform RIPPER when the same variables are used. Until recently, HMMs were the predominant formalism to model label sequences. However, they have two major shortcom</context>
<context position="15442" citStr="Sun (2002)" startWordPosition="2515" endWordPosition="2516">d the original 5 variables as well as the binned variables and they are very similar. Variables Spearman’s p Unigram -.451 Bigram -.309 Reverse Bigram -.383 Joint -.207 Reverse joint -.265 Table 3: Spearman’s correlation values for the probabilistic measures. and McKeown, 1999). 4.3 Phonological variables The last category of predictors, phonological variables, concern aspects of rhythm and timing of an utterance. We have two main sources for these variables: those that can be computed solely from a string of text (textual), and those that require some sort of acoustic information (acoustic). Sun (2002) demonstrated that the number of phones in a syllable, the number of syllables in a word, and the position of a word in a sentence are useful predictors of which syllables get accented. While Sun was concerned with predicting accented syllables, some of the same variables apply to word level targets as well. For our textual phonological features, we included the number of syllables in a word and the number of phones (both in citation form as well as transcribed form). Instead of position in a sentence, we used the position of the word in an utterance since the fragments do not necessarily corr</context>
<context position="18299" citStr="Sun (2002)" startWordPosition="3027" endWordPosition="3028">Following pause; a binary distinction of whether a word is followed by a period of silence or not. • Following filled pause; a binary distinction of whether a word was followed by a filled pause (uh, um) or not. • Following IP boundary Table 4 indicates that each of these features significantly affect the presence of pitch accent. While certainly all of these variables are not independent of on another, using CRFs, one can incorporate all of these variables into the pitch accent prediction model with the advantage of making use of the dependencies among the labels. 4.4 Surrounding Information Sun (2002) has shown that the values immediately preceding and following the target are good predictors for the value of the target. We also experimented with the effects of the surrounding values by varying the window size of the observation-label feature extraction described in Section 2. When the window size is 1, only values of the word that is labelled are incorporated in the model. When the window size is 3, the values of the previous and the following words as well as the current word are incorporated in the model. Window size 5 captures the values of the current word, the two previous words and </context>
<context position="25289" citStr="Sun, 2002" startWordPosition="4223" endWordPosition="4224"> rate on accentuation. Especially speech rate, duration and surrounding silence are good predictors of pitch accent. The addition of these predictors only slightly improved the model (about .5%). Acoustic features are very sensitive to individual speakers. In the corpus, there are many different speakers of varying ages and dialects. These variables might become more useful if one controls for individual speaker differences. To really test the usefulness of these variables, one would have to combine them with acoustic features that have been demonstrated to be good predictors of pitch accent (Sun, 2002; Conkie et al., 1999; Wightman et al., 2000). 7 Conclusion We used CRFs with new measures of collocational strength and new phonological factors that capture aspects of rhythm and timing to model pitch accent prediction. CRFs have the theoretical advantage of incorporating all these factors in a principled and efficient way. We demonstrated that CRFs outperform HMMs also experimentally. We also demonstrated the usefulness of some new probabilistic variables and phonological variables. Our results mainly have implications for the textual prediction of accents in TTS applications, but might als</context>
</contexts>
<marker>Sun, 2002</marker>
<rawString>Xuejing Sun. 2002. Pitch accent prediction using ensemble machine learning. In Proc. of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Maxmargin markov networks.</title>
<date>2004</date>
<booktitle>In Proc. of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="5414" citStr="Taskar et al., 2004" startWordPosition="844" endWordPosition="847">e assumptions to achieve efficient inference and learning. Therefore, variables used in Hidden Markov models of pitch accent prediction have been very limited, e.g. part of speech and frequency (Pan and McKeown, 1999). Discriminative learning methods, such as Maximum Entropy Markov Models (McCallum et al., 2000), Projection Based Markov Models (Punyakanok and Roth, 2000), Conditional Random Fields (Lafferty et al., 2001), Sequence AdaBoost (Altun et al., 2003a), Sequence Perceptron (Collins, 2002), Hidden Markov Support Vector Machines (Altun et al., 2003b) and Maximum-Margin Markov Networks (Taskar et al., 2004), overcome the limitations of HMMs. Among these methods, CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging (Lafferty et al., 2001), Named-Entity Recognition (Collins, 2002) and shallow parsing (Sha and Pereira, 2003; McCallum, 2003). The goal of this study is to better identify which words in a string of text will bear pitch accent. Our contribution is two-fold: employing new predictors and utilizing a discriminative model. We combine the advantages of probabilistic, syntactic, and phonological predictors with the advantages of modeling p</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2004. Maxmargin markov networks. In Proc. of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Taylor</author>
</authors>
<title>Analysis and synthesis of intonation using the Tilt model.</title>
<date>2000</date>
<journal>Journal of the Acoustical Society ofAmerica.</journal>
<contexts>
<context position="11163" citStr="Taylor, 2000" startWordPosition="1813" endWordPosition="1814">ined seven words on average. Additionally, each word was coded for probabilistic and contextual information, such as word frequency, conditional probabilities, the rate of speech, and the canonical pronunciation (Fosler-Lussier and Morgan, 1999). The dataset used in all analysis in this study consists of only the first hour of the database, comprised of 1,824 utterances with 13,190 words. These utterances were hand coded for pitch accent and intonational phrase brakes. 3.1 Pitch Accent Coding The utterances were hand labeled for accents and boundaries according to the Tilt Intonational Model (Taylor, 2000). This model is characterized by a series of intonational events: accents and boundaries. Labelers were instructed to use duration, amplitude, pausing information, and changes in f0 to identify events. In general, labelers followed the basic conventions of EToBI for coding (Taylor, 2000). However, the Tilt coding scheme was simplified. Accents were coded as either major or minor (and some rare level accents) and breaks were either rising or falling. Agreement for the Tilt coding was reported at 86%. The CU coding also used a simplified EToBI coding scheme, with accent types conflated and only </context>
</contexts>
<marker>Taylor, 2000</marker>
<rawString>P. Taylor. 2000. Analysis and synthesis of intonation using the Tilt model. Journal of the Acoustical Society ofAmerica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Wightman</author>
<author>A K Syrdal</author>
<author>G Stemmer</author>
<author>A Conkie</author>
<author>M Beutnagel</author>
</authors>
<title>Perceptually Based Automatic Prosody Labeling and Prosodically Enriched Unit Selection Improve Concatenative Text-To-Speech Synthesis.</title>
<date>2000</date>
<volume>2</volume>
<pages>71--74</pages>
<contexts>
<context position="25334" citStr="Wightman et al., 2000" startWordPosition="4229" endWordPosition="4232">y speech rate, duration and surrounding silence are good predictors of pitch accent. The addition of these predictors only slightly improved the model (about .5%). Acoustic features are very sensitive to individual speakers. In the corpus, there are many different speakers of varying ages and dialects. These variables might become more useful if one controls for individual speaker differences. To really test the usefulness of these variables, one would have to combine them with acoustic features that have been demonstrated to be good predictors of pitch accent (Sun, 2002; Conkie et al., 1999; Wightman et al., 2000). 7 Conclusion We used CRFs with new measures of collocational strength and new phonological factors that capture aspects of rhythm and timing to model pitch accent prediction. CRFs have the theoretical advantage of incorporating all these factors in a principled and efficient way. We demonstrated that CRFs outperform HMMs also experimentally. We also demonstrated the usefulness of some new probabilistic variables and phonological variables. Our results mainly have implications for the textual prediction of accents in TTS applications, but might also be useful in automatic speech recognition t</context>
</contexts>
<marker>Wightman, Syrdal, Stemmer, Conkie, Beutnagel, 2000</marker>
<rawString>C. W. Wightman, A. K. Syrdal, G. Stemmer, A. Conkie, and M. Beutnagel. 2000. Perceptually Based Automatic Prosody Labeling and Prosodically Enriched Unit Selection Improve Concatenative Text-To-Speech Synthesis. volume 2, pages 71–74.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>