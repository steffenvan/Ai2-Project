<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004321">
<title confidence="0.87007">
Sentiment Relevance
</title>
<author confidence="0.997504">
Christian Scheible Hinrich Sch¨utze
</author>
<affiliation confidence="0.994783">
Institute for Natural Language Processing Center for Information
University of Stuttgart, Germany and Language Processing
scheibcn@ims.uni-stuttgart.de University of Munich, Germany
</affiliation>
<sectionHeader confidence="0.97901" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999746608695652">
A number of different notions, including
subjectivity, have been proposed for dis-
tinguishing parts of documents that con-
vey sentiment from those that do not. We
propose a new concept, sentiment rele-
vance, to make this distinction and argue
that it better reflects the requirements of
sentiment analysis systems. We demon-
strate experimentally that sentiment rele-
vance and subjectivity are related, but dif-
ferent. Since no large amount of labeled
training data for our new notion of sen-
timent relevance is available, we investi-
gate two semi-supervised methods for cre-
ating sentiment relevance classifiers: a dis-
tant supervision approach that leverages
structured information about the domain
of the reviews; and transfer learning on
feature representations based on lexical
taxonomies that enables knowledge trans-
fer. We show that both methods learn sen-
timent relevance classifiers that perform
well.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951232142857">
It is generally recognized in sentiment analy-
sis that only a subset of the content of a doc-
ument contributes to the sentiment it conveys.
For this reason, some authors distinguish the
categories subjective and objective (Wilson and
Wiebe, 2003). Subjective statements refer to the
internal state of mind of a person, which cannot be
observed. In contrast, objective statements can be
verified by observing and checking reality. Some
sentiment analysis systems filter out objective lan-
guage and predict sentiment based on subjective
language only because objective statements do not
directly reveal sentiment.
Even though the categories subjective/objective
are well-established in philosophy, we argue that
they are not optimal for sentiment analysis. We in-
stead introduce the notion of sentiment relevance
(S-relevance or SR for short). A sentence or lin-
guistic expression is S-relevant if it contains infor-
mation about the sentiment the document conveys;
it is S-nonrelevant (SNR) otherwise.
Ideally, we would like to have at our disposal
a large annotated training set for our new con-
cept of sentiment relevance. However, such a
resource does not yet exist. For this reason,
we investigate two semi-supervised approaches to
S-relevance classification that do not require S-
relevance-labeled data. The first approach is dis-
tant supervision (DS). We create an initial label-
ing based on domain-specific metadata that we ex-
tract from a public database and show that this
improves performance by 5.8% F1 compared to a
baseline. The second approach is transfer learning
(TL) (Thrun, 1996). We show that TL improves
F1 by 12.6% for sentiment relevance classification
when we use a feature representation based on lex-
ical taxonomies that supports knowledge transfer.
In our approach, we classify sentences as S-
(non)relevant because this is the most fine-grained
level at which S-relevance manifests itself; at the
word or phrase level, S-relevance classification
is not possible because of scope and context ef-
fects. However, S-relevance is also a discourse
phenomenon: authors tend to structure documents
into S-relevant passages and S-nonrelevant pas-
sages. To impose this discourse constraint, we em-
ploy a sequence model. We represent each docu-
ment as a graph of sentences and apply a minimum
cut method.
The rest of the paper is structured as follows.
Section 2 introduces the concept of sentiment rel-
evance and relates it to subjectivity. In Section 3,
we review previous work related to sentiment rel-
evance. Next, we describe the methods applied in
this paper (Section 4) and the features we extract
(Section 5). Finally, we turn to the description and
</bodyText>
<page confidence="0.976557">
954
</page>
<note confidence="0.9142985">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 954–963,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.996078">
results of our experiments on distant supervision
(Section 6) and transfer learning (Section 7). We
end with a conclusion in Section 8.
</bodyText>
<sectionHeader confidence="0.934765" genericHeader="introduction">
2 Sentiment Relevance
</sectionHeader>
<bodyText confidence="0.999398125">
Sentiment Relevance is a concept to distinguish
content informative for determining the sentiment
of a document from uninformative content. This
is in contrast to the usual distinction between sub-
jective and objective content. Although there is
overlap between the two notions, they are differ-
ent. Consider the following examples for subjec-
tive and objective sentences:
</bodyText>
<listItem confidence="0.9566332">
(1) Subjective example: Bruce Banner, a genet-
ics researcher with a tragic past, suffers a horrible
accident.
(2) Objective example: The movie won a
Golden Globe for best foreign film and an Oscar.
</listItem>
<bodyText confidence="0.999219647058823">
Sentence (1) is subjective because assessments
like tragic past and horrible accident are subjec-
tive to the reader and writer. Sentence (2) is objec-
tive since we can check the truth of the statement.
However, even though sentence (1) has negative
subjective content, it is not S-relevant because it
is about the plot of the movie and can appear in
a glowingly positive review. Conversely, sentence
(2) contributes to the positive opinion expressed
by the author. Subjectivity and S-relevance are
two distinct concepts that do not imply each other:
Generally neutral and objective sentences can be
S-relevant while certain subjective content is S-
nonrelevant. Below, we first describe the annota-
tion procedure for the sentiment relevance corpus
and then demonstrate empirically that subjectivity
and S-relevance differ.
</bodyText>
<subsectionHeader confidence="0.998789">
2.1 Sentiment Relevance Corpus
</subsectionHeader>
<bodyText confidence="0.997130181818182">
For our initial experiments, we focus on senti-
ment relevance classification in the movie domain.
To create a sentiment-relevance-annotated corpus,
the SR corpus, we randomly selected 125 docu-
ments from the movie review data set (Pang et al.,
2002).1 Two annotators annotated the sentences
for S-relevance, using the labels SR and SNR. If no
decision can be made because a sentence contains
both S-relevant and S-nonrelevant linguistic ma-
terial, it is marked as uncertain. We excluded
360 sentences that were labeled uncertain from the
</bodyText>
<footnote confidence="0.988031">
1We used the texts from the raw HTML files since the
processed version does not have capitalization.
</footnote>
<bodyText confidence="0.999683842105263">
evaluation. In total, the SR corpus contains 2759
S-relevant and 728 S-nonrelevant sentences. Fig-
ure 1 shows an excerpt from the corpus. The full
corpus is available online.2
First, we study agreement between human an-
notators. We had 762 sentences annotated for S-
relevance by both annotators with an agreement
(Fleiss’ n) of .69. In addition, we obtained sub-
jectivity annotations for the same data on Amazon
Mechanical Turk, obtaining each label through a
vote of three, with an agreement of n = .61. How-
ever, the agreement of the subjectivity and rele-
vance labelings after voting, assuming that sub-
jectivity equals relevance, is only at n = .48.
This suggests that there is indeed a measurable
difference between subjectivity and relevance. An
annotator who we asked to examine the 225 ex-
amples where the annotations disagree found that
83.5% of these cases are true differences.
</bodyText>
<subsectionHeader confidence="0.975579">
2.2 Contrastive Classification Experiment
</subsectionHeader>
<bodyText confidence="0.999904785714286">
We will now examine the similarities of S-
relevance and an existing subjectivity dataset.
Pang and Lee (2004) introduced subjectivity data
(henceforth P&amp;L corpus) that consists of 5000
highly subjective (quote) review snippets from rot-
tentomatoes.com and 5000 objective (plot) sen-
tences from IMDb plot descriptions.
We now show that although the P&amp;L selection
criteria (quotes, plot) bear resemblance to the def-
inition of S-relevance, the two concepts are differ-
ent.
We use quote as S-relevant and plot as S-
nonrelevant data in TL. We divide both the SR
and P&amp;L corpora into training (50%) and test sets
(50%) and train a Maximum Entropy (MaxEnt)
classifier (Manning and Klein, 2003) with bag-of-
word features. Macro-averaged F1 for the four
possible training-test combinations is shown in Ta-
ble 1. The results clearly show that the classes
defined by the two labeled sets are different. A
classifier trained on P&amp;L performs worse by about
8% on SR than a classifier trained on SR (68.5 vs.
76.4). A classifier trained on SR performs worse
by more than 20% on P&amp;L than a classifier trained
on P&amp;L (67.4 vs. 89.7).
Note that the classes are not balanced in the
S-relevance data while they are balanced in the
subjectivity data. This can cause a misestimation
</bodyText>
<footnote confidence="0.997373333333333">
2http://www.ims.uni-stuttgart.
de/forschung/ressourcen/korpora/
sentimentrelevance/
</footnote>
<page confidence="0.99779">
955
</page>
<figure confidence="0.983856625">
O SNR Braxton is a gambling addict in deep to Mook (Ellen Burstyn), a local bookie.
S SNR Kennesaw is bitter about his marriage to a socialite (Rosanna Arquette), believing his wife
to be unfaithful.
S SR The plot is twisty and complex, with lots of lengthy flashbacks, and plenty of surprises.
S SR However, there are times when it is needlessly complex, and at least one instance the
storytelling turns so muddled that the answers to important plot points actually get lost.
S SR Take a look at L. A. Confidential, or the film’s more likely inspiration, The Usual Suspects
for how a complex plot can properly be handled.
</figure>
<figureCaption confidence="0.9775215">
Figure 1: Example data from the SR corpus with subjectivity (S/O) and S-relevance (SR/SNR) annota-
tions
</figureCaption>
<tableCaption confidence="0.781437333333333">
Table 1: TL/in-task F1 for P&amp;L and SR corpora
vocabulary fpSR
{actor, director, story}
{good, bad, great}
Table 2: % incorrect sentences containing specific
words
</tableCaption>
<bodyText confidence="0.999421176470588">
of class probabilities and lead to the experienced
performance drops. Indeed, if we either balance
the S-relevance data or unbalance the subjectivity
data, we can significantly increase F1 to 74.8%
and 77.9%, respectively, in the noisy label trans-
fer setting. Note however that this step is difficult
in practical applications if the actual label distri-
bution is unknown. Also, in a real practical ap-
plication the distribution of the data is what it is –
it cannot be adjusted to the training set. We will
show in Section 7 that using an unsupervised se-
quence model is superior to artificial manipulation
of class-imbalances.
An error analysis for the classifier trained on
P&amp;L shows that many sentences misclassified as
S-relevant (fpSR) contain polar words; for exam-
ple, Then, the situation turns bad. In contrast, sen-
tences misclassified as S-nonrelevant (fpSNR) con-
tain named entities or plot and movie business vo-
cabulary; for example, Tim Roth delivers the most
impressive acting job by getting the body language
right.
The word count statistics in Table 2 show this
for three polar words and for three plot/movie
business words. The P&amp;L-trained classifier seems
to have a strong bias to classify sentences with po-
lar words as S-relevant even if they are not, per-
haps because most training instances for the cat-
egory quote are highly subjective, so that there
is insufficient representation of less emphatic S-
relevant sentences. These snippets rarely con-
tain plot/movie-business words, so that the P&amp;L-
trained classifier assigns almost all sentences with
such words to the category S-nonrelevant.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999952551724138">
Many publications have addressed subjectivity in
sentiment analysis. Two important papers that are
based on the original philosophical definition of
the term (internal state of mind vs. external real-
ity) are (Wilson and Wiebe, 2003) and (Riloff and
Wiebe, 2003). As we argue above, if the goal is to
identify parts of a document that are useful/non-
useful for sentiment analysis, then S-relevance is
a better notion to use.
Researchers have implicitly deviated from the
philosophical definition because they were primar-
ily interested in satisfying the needs of a particular
task. For example, Pang and Lee (2004) use a min-
imum cut graph model for review summarization.
Because they do not directly evaluate the results
of subjectivity classification, it is not clear to what
extent their method is able to identify subjectivity
correctly.
In general, it is not possible to know what the
underlying concepts of a statistical classification
are if no detailed annotation guidelines exist and
no direct evaluation of manually labeled data is
performed.
Our work is most closely related to (Taboada
et al., 2009) who define a fine-grained classifica-
tion that is similar to sentiment relevance on the
highest level. However, unlike our study, they
fail to experimentally compare their classification
scheme to prior work in their experiments and
</bodyText>
<figure confidence="0.984895">
test
P&amp;L
P&amp;L 89.7 68.5
SR 67.4 76.4
SR
train
0
11.5
fpSNR
7.5
4.8
</figure>
<page confidence="0.994511">
956
</page>
<bodyText confidence="0.999947693333334">
to show that this scheme is different. In addi-
tion, they work on the paragraph level. How-
ever, paragraphs often contain a mix of S-relevant
and S-nonrelevant sentences. We use the mini-
mum cut method and are therefore able to incorpo-
rate discourse-level constraints in a more flexible
fashion, giving preference to “relevance-uniform”
paragraphs without mandating them.
T¨ackstr¨om and McDonald (2011) develop a
fine-grained annotation scheme that includes S-
nonrelevance as one of five categories. However,
they do not use the category S-nonrelevance di-
rectly in their experiments and do not evaluate
classification accuracy for it. We do not use their
data set as it would cause domain mismatch be-
tween the product reviews they use and the avail-
able movie review subjectivity data (Pang and Lee,
2004) in the TL approach. Changing both the do-
main (movies to products) and the task (subjectiv-
ity to S-relevance) would give rise to interactions
that we would like to avoid in our study.
The notion of annotator rationales (Zaidan et
al., 2007) has some overlap with our notion of
sentiment relevance. Yessenalina et al. (2010)
use rationales in a multi-level model to integrate
sentence-level information into a document classi-
fier. Neither paper presents a direct gold standard
evaluation of the accuracy of rationale detection.
In summary, no direct evaluation of sentiment
relevance has been performed previously. One
contribution in this paper is that we provide a
single-domain gold standard for sentiment rele-
vance, created based on clear annotation guide-
lines, and use it for direct evaluation.
Sentiment relevance is also related to review
mining (e.g., (Ding et al., 2008)) and sentiment
retrieval techniques (e.g., (Eguchi and Lavrenko,
2006)) in that they aim to find phrases, sentences
or snippets that are relevant for sentiment, either
with respect to certain features or with a focus on
high-precision retrieval (cf. (Liu, 2010)). How-
ever, finding a few S-relevant items with high pre-
cision is much easier than the task we address: ex-
haustive classification of all sentences.
Another contribution is that we show that gen-
eralization based on semantic classes improves S-
relevance classification. While previous work has
shown the utility of other types of feature gen-
eralization for sentiment and subjectivity analysis
(e.g., syntax and part-of-speech (Riloff and Wiebe,
2003)), semantic classes have so far not been ex-
ploited.
Named-entity features in movie reviews were
first used by Zhuang et al. (2006), in the form
of feature-opinion pairs (e.g., a positive opinion
about the acting). They show that recognizing plot
elements (e.g., script) and classes of people (e.g.,
actor) benefits review summarization. We follow
their approach by using IMDb to define named
entity features. We extend their work by intro-
ducing methods for labeling partial uses of names
and pronominal references. We address a different
problem (S-relevance vs. opinions) and use differ-
ent methods (graph-based and statistical vs. rule-
based).
T¨ackstr¨om and McDonald (2011) also solve a
similar sequence problem by applying a distantly
supervised classifier with an unsupervised hidden
sequence component. Their setup differs from
ours as our focus lies on pattern-based distant su-
pervision instead of distant supervision using doc-
uments for sentence classification.
Transfer learning has been applied previously in
sentiment analysis (Tan and Cheng, 2009), target-
ing polarity detection.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.9999478">
Due to the sequential properties of S-relevance (cf.
Taboada et al. (2009)), we impose the discourse
constraint that an S-relevant (resp. S-nonrelevant)
sentence tends to follow an S-relevant (resp. S-
nonrelevant) sentence. Following Pang and Lee
(2004), we use minimum cut (MinCut) to formal-
ize this discourse constraint.
For a document with n sentences, we create a
graph with n + 2 nodes: n sentence nodes and
source and sink nodes. We define source and
sink to represent the classes S-relevance and S-
nonrelevance, respectively, and refer to them as
SR and SNR.
The individual weight ind(s, x) between a sen-
tence s and the source/sink node x E {SR, SNR}
is weighted according to some confidence mea-
sure for assigning it to the corresponding class.
The weight on the edge from the document’s
ith sentence sZ to its jth sentence sj is set to
assoc(sZ, sj) = c/(j − i)2 where c is a parame-
ter (cf. (Pang and Lee, 2004)). The minimum cut
is a tradeoff between the confidence of the clas-
sification decisions and “discourse coherence”.
The discourse constraint often has the effect that
high-confidence labels are propagated over the se-
</bodyText>
<page confidence="0.988451">
957
</page>
<bodyText confidence="0.999833684210526">
quence. As a result, outliers with low confidence
are eliminated and we get a “smoother” label se-
quence.
To compute minimum cuts, we use the push-
relabel maximum flow method (Cherkassky and
Goldberg, 1995).3
We need to find values for multiple free param-
eters related to the sequence model. Supervised
optimization is impossible as we do not have any
labeled data. We therefore resort to a proxy mea-
sure, the run count. A run is a sequence of sen-
tences with the same label. We set each param-
eter p to the value that produces a median run
count that is closest to the true median run count
(or, in case of a tie, closest to the true mean run
count). We assume that the optimal median/mean
run count is known. In practice, it can be estimated
from a small number of documents. We find the
optimal value of p by grid search.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999610833333333">
Choosing features is crucial in situations where
no high-quality training data is available. We are
interested in features that are robust and support
generalization. We propose two linguistic feature
types for S-relevance classification that meet these
requirements.
</bodyText>
<subsectionHeader confidence="0.9242595">
5.1 Generalization through Semantic
Features
</subsectionHeader>
<bodyText confidence="0.999893736842105">
Distant supervision and transfer learning are set-
tings where exact training data is unavailable. We
therefore introduce generalization features which
are more likely to support knowledge transfer. To
generalize over concepts, we use knowledge from
taxonomies. A set of generalizations can be in-
duced by making a cut in the taxonomy and defin-
ing the concepts there as base classes. For nouns,
the taxonomy is WordNet (Miller, 1995) for which
CoreLex (Buitelaar, 1998) gives a set of basic
types. For verbs, VerbNet (Kipper et al., 2008)
already contains base classes.
We add for each verb in VerbNet and for each
noun in CoreLex its base class or basic type as
an additional feature where words tagged by the
mate tagger (Bohnet, 2010) as NN.* are treated as
nouns and words tagged as VB.* as verbs. For ex-
ample, the verb suggest occurs in the VerbNet base
class say, so we add a feature VN:say to the fea-
</bodyText>
<footnote confidence="0.6638345">
3using the HIPR tool (www.avglab.com/andrew/
soft.html)
</footnote>
<bodyText confidence="0.999050666666667">
ture representation. We refer to these feature sets
as CoreLex (CX) and VerbNet (VN) features and to
their combination as semantic features (SEM).
</bodyText>
<subsectionHeader confidence="0.993602">
5.2 Named Entities
</subsectionHeader>
<bodyText confidence="0.999989875">
As standard named entity recognition (NER) sys-
tems do not capture categories that are relevant to
the movie domain, we opt for a lexicon-based ap-
proach similar to (Zhuang et al., 2006). We use
the IMDb movie metadata database4 from which
we extract names for the categories &lt;ACTOR&gt;,
&lt;PERSONNEL&gt; (directors, screenwriters, and
composers), and &lt;CHARACTER&gt; (movie charac-
ters). Many entries are unsuitable for NER, e.g.,
dog is frequently listed as a character. We filter
out all words that also appear in lower case in a list
of English words extracted from the dict.cc dictio-
nary.5
A name n can be ambiguous between the cat-
egories (e.g., John Williams). We disambiguate
by calculating the maximum likelihood estimate
</bodyText>
<equation confidence="0.941376">
of p(cln) = f(n,c)
P c, f(n,c,) where c is one of the
</equation>
<bodyText confidence="0.999946777777778">
three categories and f(n, c) is the number of times
n occurs in the database as a member of cat-
egory c. We also calculate these probabilities
for all tokens that make up a name. While this
can cause false positives, it can help in many
cases where the name obviously belongs to a cat-
egory (e.g., Skywalker in Luke Skywalker is very
likely a character reference). We always inter-
pret a name preceding an actor in parentheses
as a character mention, e.g., Reese Witherspoon
in Tracy Flick (Reese Witherspoon) is an over-
achiever [... ] This way, we can recognize charac-
ter mentions for which IMDb provides insufficient
information.
In addition, we use a set of simple rules to prop-
agate annotations to related terms. If a capitalized
word occurs, we check whether it is part of an al-
ready recognized named entity. For example, if
we encounter Robin and we previously encoun-
tered Robin Hood, we assume that the two enti-
ties match. Personal pronouns will match the most
recently encountered named entity. This rule has
precedence over NER, so if a name matches a la-
beled entity, we do not attempt to label it through
NER.
The aforementioned features are encoded as bi-
nary presence indicators for each sentence. This
</bodyText>
<footnote confidence="0.9976155">
4www.imdb.com/interfaces/
5dict.cc
</footnote>
<page confidence="0.996261">
958
</page>
<bodyText confidence="0.994161">
feature set is referred to as named entities (NE).
</bodyText>
<subsectionHeader confidence="0.995046">
5.3 Sequential Features
</subsectionHeader>
<bodyText confidence="0.999986">
Following previous sequence classification work
with Maximum Entropy models (e.g., (Ratna-
parkhi, 1996)), we use selected features of adja-
cent sentences. If a sentence contains a feature F,
we add the feature F+1 to the following sentence.
For example, if a &lt;CHARACTER&gt; feature occurs
in a sentence, &lt;CHARACTER+1&gt; is added to the
following sentence. For S-relevance classification,
we perform this operation only for NE features as
they are restricted to a few classes and thus will
not enlarge the feature space notably. We refer to
this feature set as sequential features (SQ).
</bodyText>
<sectionHeader confidence="0.997552" genericHeader="method">
6 Distant Supervision
</sectionHeader>
<bodyText confidence="0.999972135593221">
Since a large labeled resource for sentiment rele-
vance classification is not yet available, we inves-
tigate semi-supervised methods for creating sen-
timent relevance classifiers. In this section, we
show how to bootstrap a sentiment relevance clas-
sifier by distant supervision (DS) .
Even though we do not have sentiment rele-
vance annotations, there are sources of metadata
about the movie domain that we can leverage for
distant supervision. Specifically, movie databases
like IMDb contain both metadata about the plot,
in particular the characters of a movie, and meta-
data about the “creators” who were involved in the
production of the movie: actors, writers, direc-
tors, and composers. On the one hand, statements
about characters usually describe the plot and are
not sentiment relevant and on the other hand, state-
ments about the creators tend to be evaluations of
their contributions – positive or negative – to the
movie. We formulate a classification rule based
on this observation: Count occurrences of NE fea-
tures and label sentences that contain a majority
of creators (and tied cases) as SR and sentences
that contain a majority of characters as SNR. This
simple labeling rule covers 1583 sentences with
an F1 score of 67.2% on the SR corpus. We call
these labels inferred from NE metadata distant su-
pervision (DS) labels. This is a form of distant
supervision in that we use the IMDb database as
described in Section 5 to automatically label sen-
tences based on which metadata from the database
they contain.
To increase coverage, we train a Maximum En-
tropy (MaxEnt) classifier (Manning and Klein,
2003) on the labels. The MaxEnt model achieves
an F1 of 61.2% on the SR corpus (Table 3, line 2).
As this classifier uses training data that is biased
towards a specialized case (sentences containing
the named entity types creators and characters),
it does not generalize well to other S-relevance
problems and thus yields lower performance on
the full dataset. This distant supervision setup suf-
fers from two issues. First, the classifier only sees
a subset of examples that contain named entities,
making generalization to other types of expres-
sions difficult. Second, there is no way to control
the quality of the input to the classifier, as we have
no confidence measure for our distant supervision
labeling rule. We will address these two issues by
introducing an intermediate step, the unsupervised
sequence model introduced in Section 4.
As described in Section 4, each document is
represented as a graph of sentences and weights
between sentences and source/sink nodes repre-
senting SR/SNR are set to the confidence values
obtained from the distantly trained MaxEnt clas-
sifier. We then apply MinCut as described in the
following paragraphs and select the most confident
examples as training material for a new classifier.
</bodyText>
<subsectionHeader confidence="0.997023">
6.1 MinCut Setup
</subsectionHeader>
<bodyText confidence="0.999997615384615">
We follow the general MinCut setup described in
Section 4. As explained above, we assume that
creators and directors indicate relevance and char-
acters indicate nonrelevance. Accordingly, we
define nSR to be the number of &lt;ACTOR&gt; and
&lt;PERSONNEL&gt; features occurring in a sentence,
and nSNR the number of &lt;CHARACTER&gt; features.
We then set the individual weight between a sen-
tence and the source/sink nodes to ind(s, x) = nx
where x ∈ {SR, SNR}. The MinCut parameter c
is set to 1; we wish to give the association scores
high weights as there might be long spans that
have individual weights with zero values.
</bodyText>
<subsectionHeader confidence="0.999934">
6.2 Confidence-based Data Selection
</subsectionHeader>
<bodyText confidence="0.9999703">
We use the output of the base classifier to train su-
pervised models. Since the MinCut model is based
on a weak assumption, it will make many false de-
cisions. To eliminate incorrect decisions, we only
use documents as training data that were labeled
with high confidence. As the confidence measure
for a document, we use the maximum flow value f
– the “amount of fluid” flowing through the docu-
ment. The max-flow min-cut theorem (Ford and
Fulkerson, 1956) implies that if the flow value
</bodyText>
<page confidence="0.995464">
959
</page>
<table confidence="0.999308222222222">
Model Features FSR FSNR Fm
1 Majority BL – 88.3 0.0 44.2
2 MaxEnt (DSlabels) NE 79.8 42.6 61.21
3 DSlabels+MinCut NE 79.6 48.2 63.912
4 DS MaxEnt NE 84.8 46.4 65.612
5 DS MaxEnt NE+SEM 85.2 48.0 66.6124
6 DS CRF NE 83.4 49.5 66.412
7 DS MaxEnt NE+SQ 84.8 49.2 67.01234
8 DS MaxEnt NE+SQ+SEM 84.5 49.1 66.81234
</table>
<tableCaption confidence="0.9736695">
Table 3: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averaged
F1). Superscript numbers indicate a significant improvement over the corresponding line.
</tableCaption>
<bodyText confidence="0.9997577">
is low, then the cut was found more quickly and
thus can be easier to calculate; this means that the
sentence is more likely to have been assigned to
the correct segment. Following this assumption,
we train MaxEnt and Conditional Random Field
(CRF, (McCallum, 2002)) classifiers on the k%
of documents that have the lowest maximum flow
values f, where k is a parameter which we op-
timize using the run count method introduced in
Section 4.
</bodyText>
<subsectionHeader confidence="0.996604">
6.3 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.99995925">
Table 3 shows S-relevant (FSR), S-nonrelevant
(FSNR) and macro average (Fm) F1 values for dif-
ferent setups with this parameter. We compare the
following setups: (1) The majority baseline (BL)
i.e., choosing the most frequent label (SR). (2) a
MaxEnt baseline trained on DS labels without ap-
plication of MinCut; (3) the base classifier using
MinCut (DSlabels+MinCut) as described above.
Conditions 4-8 train supervised classifiers based
on the labels from DSlabels+MinCut: (4) MaxEnt
with named entities (NE); (5) MaxEnt with NE
and semantic (SEM) features; (6) CRF with NE;
</bodyText>
<listItem confidence="0.9969605">
(7) MaxEnt with NE and sequential (SQ) features;
(8) MaxEnt with NE, SQ, and SEM.
</listItem>
<bodyText confidence="0.9999334">
We test statistical significance using the approx-
imate randomization test (Noreen, 1989) on doc-
uments with 10,000 iterations at p &lt; .05. We
achieve classification results above baseline using
the MinCut base classifier (line 3) and a consider-
able improvement through distant supervision. We
found that all classifiers using DS labels and Min-
cut are significantly better than MaxEnt trained on
purely rule-based DS labels (line 2). Also, the
MaxEnt models using SQ features (lines 7,8) are
significantly better than the MinCut base classi-
fier (line 3). For comparison to a chain-based se-
quence model, we train a CRF (line 6); however,
the improvement over MaxEnt (line 4) is not sig-
nificant.
We found that both semantic (lines 5,8) and se-
quential (lines 7,8) features help to improve the
classifier. The best model (line 7) performs bet-
ter than MinCut (3) by 3.1% and better than train-
ing on purely rule-generated DS labels (line 2) by
5.8%. However, we did not find a cumulative ef-
fect (line 8) of the two feature sets.
Generally, the quality of NER is crucial in this
task. While IMDb is in general a thoroughly com-
piled database, it is not perfect. For example, all
main characters in Groundhog Day are listed with
their first name only even though the full names
are given in the movie. Also, some entries are in-
tentionally incomplete to avoid spoiling the plot.
The data also contains ambiguities between char-
acters and titles (e.g., Forrest Gump) that are im-
possible to resolve with our maximum likelihood
method. In some types of movies, e.g., documen-
taries, the distinction between characters and ac-
tors makes little sense. Furthermore, ambiguities
like occurrences of common names such as John
are impossible to resolve if there is no earlier full
referring expression (e.g., John Williams).
Feature analysis for the best model using DS
labels (7) shows that NE features are dominant.
This correlation is not surprising as the seed la-
bels were induced based on NE features. Interest-
ingly, some subjective features, e.g., horrible have
high weights for S-nonrelevance, as they are asso-
ciated with non-relevant content such as plot de-
scriptions.
To summarize, the results of our experiments
using distant supervision show that a sentiment
relevance classifier can be trained successfully by
labeling data with a few simple feature rules, with
</bodyText>
<page confidence="0.989819">
960
</page>
<bodyText confidence="0.9994074">
MinCut-based input significantly outperforming
the baseline. Named entity recognition, accom-
plished with data extracted from a domain-specific
database, plays a significant rule in creating an ini-
tial labeling.
</bodyText>
<sectionHeader confidence="0.958512" genericHeader="method">
7 Transfer Learning
</sectionHeader>
<bodyText confidence="0.99996008">
To address the problem that we do not have
enough labeled SR data we now investigate a sec-
ond semi-supervised method for SR classification,
transfer learning (TL). We will use the P&amp;L data
(introduced in Section 2.2) for training. This data
set has labels that are intended to be subjectivity
labels. However, they were automatically created
using heuristics and the resulting labels can be ei-
ther viewed as noisy SR labels or noisy subjectiv-
ity labels. Compared to distant supervision, the
key advantage of training on P&amp;L is that the train-
ing set is much larger, containing around 7 times
as much data.
In TL, the key to success is to find a general-
ized feature representation that supports knowl-
edge transfer. We use a semantic feature gener-
alization method that relies on taxonomies to in-
troduce such features.
We again use MinCut to impose discourse con-
straints. This time, we first classify the data us-
ing a supervised classifier and then use MinCut to
smooth the sequences. The baseline (BL) uses a
simple bag-of-words representation of sentences
for classification which we then extend with se-
mantic features.
</bodyText>
<subsectionHeader confidence="0.99">
7.1 MinCut Setup
</subsectionHeader>
<bodyText confidence="0.999948333333333">
We again implement the basic MinCut setup from
Section 4. We set the individual weight ind(s, x)
on the edge between sentence s and class x to the
estimate p(xIs) returned by the supervised classi-
fier. The parameter c of the MinCut model is tuned
using the run count method described in Section 4.
</bodyText>
<subsectionHeader confidence="0.962231">
7.2 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999902222222222">
As we would expect, the baseline performance of
the supervised classifier on SR is low: 69.9% (Ta-
ble 4, line 1). MinCut significantly boosts the per-
formance by 7.9% to 77.5% (line 1), a result sim-
ilar to (Pang and Lee, 2004). Adding semantic
features improves supervised classification signif-
icantly by 5.7% (75.6% on line 4). When MinCut
and both types of semantic features are used to-
gether, these improvements are partially cumula-
</bodyText>
<figure confidence="0.453565">
c
</figure>
<figureCaption confidence="0.633457">
Figure 2: F1 measure for different values of c.
Horizontal line: optimal median run count. Cir-
cle: selected point.
</figureCaption>
<bodyText confidence="0.997137606060606">
tive: an improvement over the baseline by 12.6%
to 82.5% (line 4).
We also experiment with a training set where an
artificial class imbalance is introduced, matching
the 80:20 imbalance of SR:SNR in the S-relevance
corpus. After applying MinCut, we find that while
the results for BL with and without imbalances
does not differ significantly. However, models us-
ing CX and VN features and imbalances are ac-
tually significantly inferior to the respective bal-
anced versions. This result suggests that MinCut
is more effective at coping with class imbalances
than artificial balancing.
MinCut and semantic features are successful for
TL because both impose constraints that are more
useful in a setup where noise is a major problem.
MinCut can exploit test set information without
supervision as the MinCut graph is built directly
on each test set review. If high-confidence infor-
mation is “seeded” within a document and then
spread to neighbors, mistakes with low confidence
are corrected. This way, MinCut also leads to a
compensation of different class imbalances.
The results are evidence that semantic features
are robust to the differences between subjectivity
and S-relevance (cf. Section 2). In the CX+VN
model, meaningful feature classes receive high
weights, e.g., the human class from CoreLex
which contains professions that are frequently as-
sociated with non-relevant plot descriptions.
To illustrate the run-based parameter optimiza-
tion criterion, we show F1 and median/mean run
lengths for different values of c for the best TL
</bodyText>
<figure confidence="0.97101675">
0.2 0.4 0.6 0.8 1.0
2 4 6 8 10
median run count
mean run count
F1
run length
77 78 79 80 81 82
F1
</figure>
<page confidence="0.975836">
961
</page>
<table confidence="0.992617833333333">
Model base classifier MinCut
FSR FSNR Fm FSR FSNR Fm
1 BL 81.1 58.6 69.9 87.2 67.8 77.5B
2 CX 82.9 60.1 71.5B 89.0 70.3 79.7BM
3 VN 85.6 62.1 73.9B 91.4 73.6 82.5BM
4 CX+VN 88.3 62.9 75.6B 92.7 72.2 82.5BM
</table>
<tableCaption confidence="0.993215">
Table 4: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averaged
</tableCaption>
<bodyText confidence="0.975073125">
F1). B indicates a significant improvement over the BL base classifier (69.9), M over BL MinCut (77.5).
setting (line 4) in Figure 2. Due to differences in
the base classifier, the optimum of c may vary be-
tween the experiments. A weaker base classifier
may yield a higher weight on the sequence model,
resulting in a larger c. The circled point shows the
data point selected through optimization. The op-
timization criterion does not always correlate per-
fectly with F1. However, we find no statistically
significant difference between the selected result
and the highest F1 value.
These experiments demonstrate that S-
relevance classification improves considerably
through TL if semantic feature generalization
and unsupervised sequence classification through
MinCut are applied.
</bodyText>
<sectionHeader confidence="0.997288" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999921192307692">
A number of different notions, including subjec-
tivity, have been proposed for distinguishing parts
of documents that convey sentiment from those
that do not. We introduced sentiment relevance to
make this distinction and argued that it better re-
flects the requirements of sentiment analysis sys-
tems. Our experiments demonstrated that senti-
ment relevance and subjectivity are related, but
different. To enable other researchers to use this
new notion of S-relevance, we have published the
annotated S-relevance corpus used in this paper.
Since a large labeled sentiment relevance re-
source does not yet exist, we investigated semi-
supervised approaches to S-relevance classifica-
tion that do not require S-relevance-labeled data.
We showed that a combination of different tech-
niques gives us the best results: semantic gener-
alization features, imposing discourse constraints
implemented as the minimum cut graph-theoretic
method, automatic “distant” labeling based on a
domain-specific metadata database and transfer
learning to exploit existing labels for a related
classification problem.
In future work, we plan to use sentiment rele-
vance in a downstream task such as review sum-
marization.
</bodyText>
<sectionHeader confidence="0.998277" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999916">
This work was funded by the DFG through the
Sonderforschungsbereich 732. We thank Charles
Jochim, Wiltrud Kessler, and Khalid Al Khatib for
many helpful comments and discussions.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999645709677419">
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89–97, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
P. Buitelaar. 1998. CoreLex: systematic polysemy and
underspecification. Ph.D. thesis, Brandeis Univer-
sity.
B. Cherkassky and A. Goldberg. 1995. On imple-
menting push-relabel method for the maximum flow
problem. Integer Programming and Combinatorial
Optimization, pages 157–171.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In WSDM 2008,
pages 231–240.
K. Eguchi and V. Lavrenko. 2006. Sentiment retrieval
using generative models. In EMNLP 2006, pages
345–354.
L.R. Ford and D.R. Fulkerson. 1956. Maximal flow
through a network. Canadian Journal of Mathemat-
ics, 8(3):399–404.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer.
2008. A large-scale classification of English verbs.
Language Resources and Evaluation, 42(1):21–40.
B. Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, pages
978–1420085921.
C. Manning and D. Klein. 2003. Optimization, maxent
models, and conditional estimation without magic.
In NAACL-HLT 2003: Tutorials, page 8.
</reference>
<page confidence="0.990072">
962
</page>
<bodyText confidence="0.8724172">
A.K. McCallum. 2002. Mallet: A machine learning
for language toolkit.
L. Zhuang, F. Jing, and X. Zhu. 2006. Movie review
mining and summarization. In CIKM 2006, pages
43–50.
</bodyText>
<reference confidence="0.99933912244898">
G.A. Miller. 1995. Wordnet: a lexical database for
english. Communications of the ACM, 38(11):39–
41.
E.W. Noreen. 1989. Computer Intensive Methods for
Hypothesis Testing: An Introduction. Wiley.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In ACL 2004, pages 271–
278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In ACL-EMNLP 2002, pages 79–86.
A.M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 339–346. Association for Compu-
tational Linguistics.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the con-
ference on empirical methods in natural language
processing, volume 1, pages 133–142.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In EMNLP 2003,
pages 105–112.
M. Taboada, J. Brooke, and M. Stede. 2009. Genre-
based paragraph classification for sentiment analy-
sis. In SIGdial 2009, pages 62–70.
O. T¨ackstr¨om and R. McDonald. 2011. Discover-
ing fine-grained sentiment with latent variable struc-
tured prediction models. In ECIR 2011, pages 368–
374.
S. Tan and X. Cheng. 2009. Improving SCL model
for sentiment-transfer learning. In ACL 2009, pages
181–184.
S. Thrun. 1996. Is learning the n-th thing any easier
than learning the first? In NIPS 1996, pages 640–
646.
T. Wilson and J. Wiebe. 2003. Annotating opinions in
the world press. In 4th SIGdial Workshop on Dis-
course and Dialogue, pages 13–22.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level senti-
ment classification. In EMNLP 2010, pages 1046–
1056.
O. Zaidan, J. Eisner, and C. Piatko. 2007. Using anno-
tator rationales to improve machine learning for text
categorization. In NAACL-HLT 2007, pages 260–
267.
</reference>
<page confidence="0.999089">
963
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943298">
<title confidence="0.99949">Sentiment Relevance</title>
<author confidence="0.999476">Christian Scheible Hinrich Sch¨utze</author>
<affiliation confidence="0.998992">Institute for Natural Language Processing Center for Information University of Stuttgart, Germany and Language</affiliation>
<address confidence="0.98722">of Munich, Germany</address>
<abstract confidence="0.998221666666666">A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We a new concept, releto make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>89--97</pages>
<location>Beijing, China,</location>
<contexts>
<context position="18899" citStr="Bohnet, 2010" startWordPosition="3010" endWordPosition="3011">eneralization features which are more likely to support knowledge transfer. To generalize over concepts, we use knowledge from taxonomies. A set of generalizations can be induced by making a cut in the taxonomy and defining the concepts there as base classes. For nouns, the taxonomy is WordNet (Miller, 1995) for which CoreLex (Buitelaar, 1998) gives a set of basic types. For verbs, VerbNet (Kipper et al., 2008) already contains base classes. We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. For example, the verb suggest occurs in the VerbNet base class say, so we add a feature VN:say to the fea3using the HIPR tool (www.avglab.com/andrew/ soft.html) ture representation. We refer to these feature sets as CoreLex (CX) and VerbNet (VN) features and to their combination as semantic features (SEM). 5.2 Named Entities As standard named entity recognition (NER) systems do not capture categories that are relevant to the movie domain, we opt for a lexicon-based approach similar to (Zhuang et al., 2006). We use the IMDb movie </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89–97, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
</authors>
<title>CoreLex: systematic polysemy and underspecification.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Brandeis University.</institution>
<contexts>
<context position="18631" citStr="Buitelaar, 1998" startWordPosition="2961" endWordPosition="2962">on. We propose two linguistic feature types for S-relevance classification that meet these requirements. 5.1 Generalization through Semantic Features Distant supervision and transfer learning are settings where exact training data is unavailable. We therefore introduce generalization features which are more likely to support knowledge transfer. To generalize over concepts, we use knowledge from taxonomies. A set of generalizations can be induced by making a cut in the taxonomy and defining the concepts there as base classes. For nouns, the taxonomy is WordNet (Miller, 1995) for which CoreLex (Buitelaar, 1998) gives a set of basic types. For verbs, VerbNet (Kipper et al., 2008) already contains base classes. We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. For example, the verb suggest occurs in the VerbNet base class say, so we add a feature VN:say to the fea3using the HIPR tool (www.avglab.com/andrew/ soft.html) ture representation. We refer to these feature sets as CoreLex (CX) and VerbNet (VN) features and to their</context>
</contexts>
<marker>Buitelaar, 1998</marker>
<rawString>P. Buitelaar. 1998. CoreLex: systematic polysemy and underspecification. Ph.D. thesis, Brandeis University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Cherkassky</author>
<author>A Goldberg</author>
</authors>
<title>On implementing push-relabel method for the maximum flow problem. Integer Programming and Combinatorial Optimization,</title>
<date>1995</date>
<pages>157--171</pages>
<contexts>
<context position="17224" citStr="Cherkassky and Goldberg, 1995" startWordPosition="2723" endWordPosition="2726">ssigning it to the corresponding class. The weight on the edge from the document’s ith sentence sZ to its jth sentence sj is set to assoc(sZ, sj) = c/(j − i)2 where c is a parameter (cf. (Pang and Lee, 2004)). The minimum cut is a tradeoff between the confidence of the classification decisions and “discourse coherence”. The discourse constraint often has the effect that high-confidence labels are propagated over the se957 quence. As a result, outliers with low confidence are eliminated and we get a “smoother” label sequence. To compute minimum cuts, we use the pushrelabel maximum flow method (Cherkassky and Goldberg, 1995).3 We need to find values for multiple free parameters related to the sequence model. Supervised optimization is impossible as we do not have any labeled data. We therefore resort to a proxy measure, the run count. A run is a sequence of sentences with the same label. We set each parameter p to the value that produces a median run count that is closest to the true median run count (or, in case of a tie, closest to the true mean run count). We assume that the optimal median/mean run count is known. In practice, it can be estimated from a small number of documents. We find the optimal value of p</context>
</contexts>
<marker>Cherkassky, Goldberg, 1995</marker>
<rawString>B. Cherkassky and A. Goldberg. 1995. On implementing push-relabel method for the maximum flow problem. Integer Programming and Combinatorial Optimization, pages 157–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Ding</author>
<author>B Liu</author>
<author>P S Yu</author>
</authors>
<title>A holistic lexiconbased approach to opinion mining.</title>
<date>2008</date>
<booktitle>In WSDM</booktitle>
<pages>231--240</pages>
<contexts>
<context position="14075" citStr="Ding et al., 2008" startWordPosition="2220" endWordPosition="2223">th our notion of sentiment relevance. Yessenalina et al. (2010) use rationales in a multi-level model to integrate sentence-level information into a document classifier. Neither paper presents a direct gold standard evaluation of the accuracy of rationale detection. In summary, no direct evaluation of sentiment relevance has been performed previously. One contribution in this paper is that we provide a single-domain gold standard for sentiment relevance, created based on clear annotation guidelines, and use it for direct evaluation. Sentiment relevance is also related to review mining (e.g., (Ding et al., 2008)) and sentiment retrieval techniques (e.g., (Eguchi and Lavrenko, 2006)) in that they aim to find phrases, sentences or snippets that are relevant for sentiment, either with respect to certain features or with a focus on high-precision retrieval (cf. (Liu, 2010)). However, finding a few S-relevant items with high precision is much easier than the task we address: exhaustive classification of all sentences. Another contribution is that we show that generalization based on semantic classes improves Srelevance classification. While previous work has shown the utility of other types of feature gen</context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexiconbased approach to opinion mining. In WSDM 2008, pages 231–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Eguchi</author>
<author>V Lavrenko</author>
</authors>
<title>Sentiment retrieval using generative models.</title>
<date>2006</date>
<booktitle>In EMNLP</booktitle>
<pages>345--354</pages>
<contexts>
<context position="14146" citStr="Eguchi and Lavrenko, 2006" startWordPosition="2229" endWordPosition="2232"> use rationales in a multi-level model to integrate sentence-level information into a document classifier. Neither paper presents a direct gold standard evaluation of the accuracy of rationale detection. In summary, no direct evaluation of sentiment relevance has been performed previously. One contribution in this paper is that we provide a single-domain gold standard for sentiment relevance, created based on clear annotation guidelines, and use it for direct evaluation. Sentiment relevance is also related to review mining (e.g., (Ding et al., 2008)) and sentiment retrieval techniques (e.g., (Eguchi and Lavrenko, 2006)) in that they aim to find phrases, sentences or snippets that are relevant for sentiment, either with respect to certain features or with a focus on high-precision retrieval (cf. (Liu, 2010)). However, finding a few S-relevant items with high precision is much easier than the task we address: exhaustive classification of all sentences. Another contribution is that we show that generalization based on semantic classes improves Srelevance classification. While previous work has shown the utility of other types of feature generalization for sentiment and subjectivity analysis (e.g., syntax and p</context>
</contexts>
<marker>Eguchi, Lavrenko, 2006</marker>
<rawString>K. Eguchi and V. Lavrenko. 2006. Sentiment retrieval using generative models. In EMNLP 2006, pages 345–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Ford</author>
<author>D R Fulkerson</author>
</authors>
<title>Maximal flow through a network.</title>
<date>1956</date>
<journal>Canadian Journal of Mathematics,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="25936" citStr="Ford and Fulkerson, 1956" startWordPosition="4180" endWordPosition="4183"> to 1; we wish to give the association scores high weights as there might be long spans that have individual weights with zero values. 6.2 Confidence-based Data Selection We use the output of the base classifier to train supervised models. Since the MinCut model is based on a weak assumption, it will make many false decisions. To eliminate incorrect decisions, we only use documents as training data that were labeled with high confidence. As the confidence measure for a document, we use the maximum flow value f – the “amount of fluid” flowing through the document. The max-flow min-cut theorem (Ford and Fulkerson, 1956) implies that if the flow value 959 Model Features FSR FSNR Fm 1 Majority BL – 88.3 0.0 44.2 2 MaxEnt (DSlabels) NE 79.8 42.6 61.21 3 DSlabels+MinCut NE 79.6 48.2 63.912 4 DS MaxEnt NE 84.8 46.4 65.612 5 DS MaxEnt NE+SEM 85.2 48.0 66.6124 6 DS CRF NE 83.4 49.5 66.412 7 DS MaxEnt NE+SQ 84.8 49.2 67.01234 8 DS MaxEnt NE+SQ+SEM 84.5 49.1 66.81234 Table 3: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averaged F1). Superscript numbers indicate a significant improvement over the corresponding line. is low, then the cut was found more quickly and thus can be eas</context>
</contexts>
<marker>Ford, Fulkerson, 1956</marker>
<rawString>L.R. Ford and D.R. Fulkerson. 1956. Maximal flow through a network. Canadian Journal of Mathematics, 8(3):399–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>A Korhonen</author>
<author>N Ryant</author>
<author>M Palmer</author>
</authors>
<title>A large-scale classification of English verbs.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="18700" citStr="Kipper et al., 2008" startWordPosition="2972" endWordPosition="2975">ification that meet these requirements. 5.1 Generalization through Semantic Features Distant supervision and transfer learning are settings where exact training data is unavailable. We therefore introduce generalization features which are more likely to support knowledge transfer. To generalize over concepts, we use knowledge from taxonomies. A set of generalizations can be induced by making a cut in the taxonomy and defining the concepts there as base classes. For nouns, the taxonomy is WordNet (Miller, 1995) for which CoreLex (Buitelaar, 1998) gives a set of basic types. For verbs, VerbNet (Kipper et al., 2008) already contains base classes. We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. For example, the verb suggest occurs in the VerbNet base class say, so we add a feature VN:say to the fea3using the HIPR tool (www.avglab.com/andrew/ soft.html) ture representation. We refer to these feature sets as CoreLex (CX) and VerbNet (VN) features and to their combination as semantic features (SEM). 5.2 Named Entities As standa</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>K. Kipper, A. Korhonen, N. Ryant, and M. Palmer. 2008. A large-scale classification of English verbs. Language Resources and Evaluation, 42(1):21–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>Handbook of Natural Language Processing,</booktitle>
<pages>978--1420085921</pages>
<contexts>
<context position="14337" citStr="Liu, 2010" startWordPosition="2262" endWordPosition="2263">summary, no direct evaluation of sentiment relevance has been performed previously. One contribution in this paper is that we provide a single-domain gold standard for sentiment relevance, created based on clear annotation guidelines, and use it for direct evaluation. Sentiment relevance is also related to review mining (e.g., (Ding et al., 2008)) and sentiment retrieval techniques (e.g., (Eguchi and Lavrenko, 2006)) in that they aim to find phrases, sentences or snippets that are relevant for sentiment, either with respect to certain features or with a focus on high-precision retrieval (cf. (Liu, 2010)). However, finding a few S-relevant items with high precision is much easier than the task we address: exhaustive classification of all sentences. Another contribution is that we show that generalization based on semantic classes improves Srelevance classification. While previous work has shown the utility of other types of feature generalization for sentiment and subjectivity analysis (e.g., syntax and part-of-speech (Riloff and Wiebe, 2003)), semantic classes have so far not been exploited. Named-entity features in movie reviews were first used by Zhuang et al. (2006), in the form of featur</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>B. Liu. 2010. Sentiment analysis and subjectivity. Handbook of Natural Language Processing, pages 978–1420085921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>D Klein</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>In NAACL-HLT 2003: Tutorials,</booktitle>
<pages>8</pages>
<contexts>
<context position="7818" citStr="Manning and Klein, 2003" startWordPosition="1209" endWordPosition="1212"> existing subjectivity dataset. Pang and Lee (2004) introduced subjectivity data (henceforth P&amp;L corpus) that consists of 5000 highly subjective (quote) review snippets from rottentomatoes.com and 5000 objective (plot) sentences from IMDb plot descriptions. We now show that although the P&amp;L selection criteria (quotes, plot) bear resemblance to the definition of S-relevance, the two concepts are different. We use quote as S-relevant and plot as Snonrelevant data in TL. We divide both the SR and P&amp;L corpora into training (50%) and test sets (50%) and train a Maximum Entropy (MaxEnt) classifier (Manning and Klein, 2003) with bag-ofword features. Macro-averaged F1 for the four possible training-test combinations is shown in Table 1. The results clearly show that the classes defined by the two labeled sets are different. A classifier trained on P&amp;L performs worse by about 8% on SR than a classifier trained on SR (68.5 vs. 76.4). A classifier trained on SR performs worse by more than 20% on P&amp;L than a classifier trained on P&amp;L (67.4 vs. 89.7). Note that the classes are not balanced in the S-relevance data while they are balanced in the subjectivity data. This can cause a misestimation 2http://www.ims.uni-stuttg</context>
<context position="23599" citStr="Manning and Klein, 2003" startWordPosition="3793" endWordPosition="3796">Count occurrences of NE features and label sentences that contain a majority of creators (and tied cases) as SR and sentences that contain a majority of characters as SNR. This simple labeling rule covers 1583 sentences with an F1 score of 67.2% on the SR corpus. We call these labels inferred from NE metadata distant supervision (DS) labels. This is a form of distant supervision in that we use the IMDb database as described in Section 5 to automatically label sentences based on which metadata from the database they contain. To increase coverage, we train a Maximum Entropy (MaxEnt) classifier (Manning and Klein, 2003) on the labels. The MaxEnt model achieves an F1 of 61.2% on the SR corpus (Table 3, line 2). As this classifier uses training data that is biased towards a specialized case (sentences containing the named entity types creators and characters), it does not generalize well to other S-relevance problems and thus yields lower performance on the full dataset. This distant supervision setup suffers from two issues. First, the classifier only sees a subset of examples that contain named entities, making generalization to other types of expressions difficult. Second, there is no way to control the qua</context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>C. Manning and D. Klein. 2003. Optimization, maxent models, and conditional estimation without magic. In NAACL-HLT 2003: Tutorials, page 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>41</pages>
<contexts>
<context position="18595" citStr="Miller, 1995" startWordPosition="2956" endWordPosition="2957">e robust and support generalization. We propose two linguistic feature types for S-relevance classification that meet these requirements. 5.1 Generalization through Semantic Features Distant supervision and transfer learning are settings where exact training data is unavailable. We therefore introduce generalization features which are more likely to support knowledge transfer. To generalize over concepts, we use knowledge from taxonomies. A set of generalizations can be induced by making a cut in the taxonomy and defining the concepts there as base classes. For nouns, the taxonomy is WordNet (Miller, 1995) for which CoreLex (Buitelaar, 1998) gives a set of basic types. For verbs, VerbNet (Kipper et al., 2008) already contains base classes. We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. For example, the verb suggest occurs in the VerbNet base class say, so we add a feature VN:say to the fea3using the HIPR tool (www.avglab.com/andrew/ soft.html) ture representation. We refer to these feature sets as CoreLex (CX) an</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39– 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Noreen</author>
</authors>
<title>Computer Intensive Methods for Hypothesis Testing: An Introduction.</title>
<date>1989</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="27684" citStr="Noreen, 1989" startWordPosition="4470" endWordPosition="4471">compare the following setups: (1) The majority baseline (BL) i.e., choosing the most frequent label (SR). (2) a MaxEnt baseline trained on DS labels without application of MinCut; (3) the base classifier using MinCut (DSlabels+MinCut) as described above. Conditions 4-8 train supervised classifiers based on the labels from DSlabels+MinCut: (4) MaxEnt with named entities (NE); (5) MaxEnt with NE and semantic (SEM) features; (6) CRF with NE; (7) MaxEnt with NE and sequential (SQ) features; (8) MaxEnt with NE, SQ, and SEM. We test statistical significance using the approximate randomization test (Noreen, 1989) on documents with 10,000 iterations at p &lt; .05. We achieve classification results above baseline using the MinCut base classifier (line 3) and a considerable improvement through distant supervision. We found that all classifiers using DS labels and Mincut are significantly better than MaxEnt trained on purely rule-based DS labels (line 2). Also, the MaxEnt models using SQ features (lines 7,8) are significantly better than the MinCut base classifier (line 3). For comparison to a chain-based sequence model, we train a CRF (line 6); however, the improvement over MaxEnt (line 4) is not significan</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E.W. Noreen. 1989. Computer Intensive Methods for Hypothesis Testing: An Introduction. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In ACL 2004,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="7245" citStr="Pang and Lee (2004)" startWordPosition="1117" endWordPosition="1120">anical Turk, obtaining each label through a vote of three, with an agreement of n = .61. However, the agreement of the subjectivity and relevance labelings after voting, assuming that subjectivity equals relevance, is only at n = .48. This suggests that there is indeed a measurable difference between subjectivity and relevance. An annotator who we asked to examine the 225 examples where the annotations disagree found that 83.5% of these cases are true differences. 2.2 Contrastive Classification Experiment We will now examine the similarities of Srelevance and an existing subjectivity dataset. Pang and Lee (2004) introduced subjectivity data (henceforth P&amp;L corpus) that consists of 5000 highly subjective (quote) review snippets from rottentomatoes.com and 5000 objective (plot) sentences from IMDb plot descriptions. We now show that although the P&amp;L selection criteria (quotes, plot) bear resemblance to the definition of S-relevance, the two concepts are different. We use quote as S-relevant and plot as Snonrelevant data in TL. We divide both the SR and P&amp;L corpora into training (50%) and test sets (50%) and train a Maximum Entropy (MaxEnt) classifier (Manning and Klein, 2003) with bag-ofword features. </context>
<context position="11597" citStr="Pang and Lee (2004)" startWordPosition="1822" endWordPosition="1825">ed Work Many publications have addressed subjectivity in sentiment analysis. Two important papers that are based on the original philosophical definition of the term (internal state of mind vs. external reality) are (Wilson and Wiebe, 2003) and (Riloff and Wiebe, 2003). As we argue above, if the goal is to identify parts of a document that are useful/nonuseful for sentiment analysis, then S-relevance is a better notion to use. Researchers have implicitly deviated from the philosophical definition because they were primarily interested in satisfying the needs of a particular task. For example, Pang and Lee (2004) use a minimum cut graph model for review summarization. Because they do not directly evaluate the results of subjectivity classification, it is not clear to what extent their method is able to identify subjectivity correctly. In general, it is not possible to know what the underlying concepts of a statistical classification are if no detailed annotation guidelines exist and no direct evaluation of manually labeled data is performed. Our work is most closely related to (Taboada et al., 2009) who define a fine-grained classification that is similar to sentiment relevance on the highest level. H</context>
<context position="13197" citStr="Pang and Lee, 2004" startWordPosition="2081" endWordPosition="2084">nimum cut method and are therefore able to incorporate discourse-level constraints in a more flexible fashion, giving preference to “relevance-uniform” paragraphs without mandating them. T¨ackstr¨om and McDonald (2011) develop a fine-grained annotation scheme that includes Snonrelevance as one of five categories. However, they do not use the category S-nonrelevance directly in their experiments and do not evaluate classification accuracy for it. We do not use their data set as it would cause domain mismatch between the product reviews they use and the available movie review subjectivity data (Pang and Lee, 2004) in the TL approach. Changing both the domain (movies to products) and the task (subjectivity to S-relevance) would give rise to interactions that we would like to avoid in our study. The notion of annotator rationales (Zaidan et al., 2007) has some overlap with our notion of sentiment relevance. Yessenalina et al. (2010) use rationales in a multi-level model to integrate sentence-level information into a document classifier. Neither paper presents a direct gold standard evaluation of the accuracy of rationale detection. In summary, no direct evaluation of sentiment relevance has been performe</context>
<context position="16136" citStr="Pang and Lee (2004)" startWordPosition="2531" endWordPosition="2534">ntly supervised classifier with an unsupervised hidden sequence component. Their setup differs from ours as our focus lies on pattern-based distant supervision instead of distant supervision using documents for sentence classification. Transfer learning has been applied previously in sentiment analysis (Tan and Cheng, 2009), targeting polarity detection. 4 Methods Due to the sequential properties of S-relevance (cf. Taboada et al. (2009)), we impose the discourse constraint that an S-relevant (resp. S-nonrelevant) sentence tends to follow an S-relevant (resp. Snonrelevant) sentence. Following Pang and Lee (2004), we use minimum cut (MinCut) to formalize this discourse constraint. For a document with n sentences, we create a graph with n + 2 nodes: n sentence nodes and source and sink nodes. We define source and sink to represent the classes S-relevance and Snonrelevance, respectively, and refer to them as SR and SNR. The individual weight ind(s, x) between a sentence s and the source/sink node x E {SR, SNR} is weighted according to some confidence measure for assigning it to the corresponding class. The weight on the edge from the document’s ith sentence sZ to its jth sentence sj is set to assoc(sZ, </context>
<context position="31865" citStr="Pang and Lee, 2004" startWordPosition="5167" endWordPosition="5170">cation which we then extend with semantic features. 7.1 MinCut Setup We again implement the basic MinCut setup from Section 4. We set the individual weight ind(s, x) on the edge between sentence s and class x to the estimate p(xIs) returned by the supervised classifier. The parameter c of the MinCut model is tuned using the run count method described in Section 4. 7.2 Experiments and Results As we would expect, the baseline performance of the supervised classifier on SR is low: 69.9% (Table 4, line 1). MinCut significantly boosts the performance by 7.9% to 77.5% (line 1), a result similar to (Pang and Lee, 2004). Adding semantic features improves supervised classification significantly by 5.7% (75.6% on line 4). When MinCut and both types of semantic features are used together, these improvements are partially cumulac Figure 2: F1 measure for different values of c. Horizontal line: optimal median run count. Circle: selected point. tive: an improvement over the baseline by 12.6% to 82.5% (line 4). We also experiment with a training set where an artificial class imbalance is introduced, matching the 80:20 imbalance of SR:SNR in the S-relevance corpus. After applying MinCut, we find that while the resul</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In ACL 2004, pages 271– 278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In ACL-EMNLP</booktitle>
<pages>79--86</pages>
<contexts>
<context position="5825" citStr="Pang et al., 2002" startWordPosition="887" endWordPosition="890">d S-relevance are two distinct concepts that do not imply each other: Generally neutral and objective sentences can be S-relevant while certain subjective content is Snonrelevant. Below, we first describe the annotation procedure for the sentiment relevance corpus and then demonstrate empirically that subjectivity and S-relevance differ. 2.1 Sentiment Relevance Corpus For our initial experiments, we focus on sentiment relevance classification in the movie domain. To create a sentiment-relevance-annotated corpus, the SR corpus, we randomly selected 125 documents from the movie review data set (Pang et al., 2002).1 Two annotators annotated the sentences for S-relevance, using the labels SR and SNR. If no decision can be made because a sentence contains both S-relevant and S-nonrelevant linguistic material, it is marked as uncertain. We excluded 360 sentences that were labeled uncertain from the 1We used the texts from the raw HTML files since the processed version does not have capitalization. evaluation. In total, the SR corpus contains 2759 S-relevant and 728 S-nonrelevant sentences. Figure 1 shows an excerpt from the corpus. The full corpus is available online.2 First, we study agreement between hu</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In ACL-EMNLP 2002, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>339--346</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A.M. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 339–346. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<volume>1</volume>
<pages>133--142</pages>
<contexts>
<context position="21487" citStr="Ratnaparkhi, 1996" startWordPosition="3446" endWordPosition="3448">. For example, if we encounter Robin and we previously encountered Robin Hood, we assume that the two entities match. Personal pronouns will match the most recently encountered named entity. This rule has precedence over NER, so if a name matches a labeled entity, we do not attempt to label it through NER. The aforementioned features are encoded as binary presence indicators for each sentence. This 4www.imdb.com/interfaces/ 5dict.cc 958 feature set is referred to as named entities (NE). 5.3 Sequential Features Following previous sequence classification work with Maximum Entropy models (e.g., (Ratnaparkhi, 1996)), we use selected features of adjacent sentences. If a sentence contains a feature F, we add the feature F+1 to the following sentence. For example, if a &lt;CHARACTER&gt; feature occurs in a sentence, &lt;CHARACTER+1&gt; is added to the following sentence. For S-relevance classification, we perform this operation only for NE features as they are restricted to a few classes and thus will not enlarge the feature space notably. We refer to this feature set as sequential features (SQ). 6 Distant Supervision Since a large labeled resource for sentiment relevance classification is not yet available, we invest</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the conference on empirical methods in natural language processing, volume 1, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions. In EMNLP</title>
<date>2003</date>
<pages>105--112</pages>
<contexts>
<context position="11247" citStr="Riloff and Wiebe, 2003" startWordPosition="1765" endWordPosition="1768">y are not, perhaps because most training instances for the category quote are highly subjective, so that there is insufficient representation of less emphatic Srelevant sentences. These snippets rarely contain plot/movie-business words, so that the P&amp;Ltrained classifier assigns almost all sentences with such words to the category S-nonrelevant. 3 Related Work Many publications have addressed subjectivity in sentiment analysis. Two important papers that are based on the original philosophical definition of the term (internal state of mind vs. external reality) are (Wilson and Wiebe, 2003) and (Riloff and Wiebe, 2003). As we argue above, if the goal is to identify parts of a document that are useful/nonuseful for sentiment analysis, then S-relevance is a better notion to use. Researchers have implicitly deviated from the philosophical definition because they were primarily interested in satisfying the needs of a particular task. For example, Pang and Lee (2004) use a minimum cut graph model for review summarization. Because they do not directly evaluate the results of subjectivity classification, it is not clear to what extent their method is able to identify subjectivity correctly. In general, it is not p</context>
<context position="14784" citStr="Riloff and Wiebe, 2003" startWordPosition="2329" endWordPosition="2332">aim to find phrases, sentences or snippets that are relevant for sentiment, either with respect to certain features or with a focus on high-precision retrieval (cf. (Liu, 2010)). However, finding a few S-relevant items with high precision is much easier than the task we address: exhaustive classification of all sentences. Another contribution is that we show that generalization based on semantic classes improves Srelevance classification. While previous work has shown the utility of other types of feature generalization for sentiment and subjectivity analysis (e.g., syntax and part-of-speech (Riloff and Wiebe, 2003)), semantic classes have so far not been exploited. Named-entity features in movie reviews were first used by Zhuang et al. (2006), in the form of feature-opinion pairs (e.g., a positive opinion about the acting). They show that recognizing plot elements (e.g., script) and classes of people (e.g., actor) benefits review summarization. We follow their approach by using IMDb to define named entity features. We extend their work by introducing methods for labeling partial uses of names and pronominal references. We address a different problem (S-relevance vs. opinions) and use different methods (</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>E. Riloff and J. Wiebe. 2003. Learning extraction patterns for subjective expressions. In EMNLP 2003, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taboada</author>
<author>J Brooke</author>
<author>M Stede</author>
</authors>
<title>Genrebased paragraph classification for sentiment analysis. In SIGdial</title>
<date>2009</date>
<pages>62--70</pages>
<contexts>
<context position="12093" citStr="Taboada et al., 2009" startWordPosition="1902" endWordPosition="1905">inition because they were primarily interested in satisfying the needs of a particular task. For example, Pang and Lee (2004) use a minimum cut graph model for review summarization. Because they do not directly evaluate the results of subjectivity classification, it is not clear to what extent their method is able to identify subjectivity correctly. In general, it is not possible to know what the underlying concepts of a statistical classification are if no detailed annotation guidelines exist and no direct evaluation of manually labeled data is performed. Our work is most closely related to (Taboada et al., 2009) who define a fine-grained classification that is similar to sentiment relevance on the highest level. However, unlike our study, they fail to experimentally compare their classification scheme to prior work in their experiments and test P&amp;L P&amp;L 89.7 68.5 SR 67.4 76.4 SR train 0 11.5 fpSNR 7.5 4.8 956 to show that this scheme is different. In addition, they work on the paragraph level. However, paragraphs often contain a mix of S-relevant and S-nonrelevant sentences. We use the minimum cut method and are therefore able to incorporate discourse-level constraints in a more flexible fashion, givi</context>
<context position="15958" citStr="Taboada et al. (2009)" startWordPosition="2506" endWordPosition="2509">vance vs. opinions) and use different methods (graph-based and statistical vs. rulebased). T¨ackstr¨om and McDonald (2011) also solve a similar sequence problem by applying a distantly supervised classifier with an unsupervised hidden sequence component. Their setup differs from ours as our focus lies on pattern-based distant supervision instead of distant supervision using documents for sentence classification. Transfer learning has been applied previously in sentiment analysis (Tan and Cheng, 2009), targeting polarity detection. 4 Methods Due to the sequential properties of S-relevance (cf. Taboada et al. (2009)), we impose the discourse constraint that an S-relevant (resp. S-nonrelevant) sentence tends to follow an S-relevant (resp. Snonrelevant) sentence. Following Pang and Lee (2004), we use minimum cut (MinCut) to formalize this discourse constraint. For a document with n sentences, we create a graph with n + 2 nodes: n sentence nodes and source and sink nodes. We define source and sink to represent the classes S-relevance and Snonrelevance, respectively, and refer to them as SR and SNR. The individual weight ind(s, x) between a sentence s and the source/sink node x E {SR, SNR} is weighted accord</context>
</contexts>
<marker>Taboada, Brooke, Stede, 2009</marker>
<rawString>M. Taboada, J. Brooke, and M. Stede. 2009. Genrebased paragraph classification for sentiment analysis. In SIGdial 2009, pages 62–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O T¨ackstr¨om</author>
<author>R McDonald</author>
</authors>
<title>Discovering fine-grained sentiment with latent variable structured prediction models.</title>
<date>2011</date>
<booktitle>In ECIR 2011,</booktitle>
<pages>368--374</pages>
<marker>T¨ackstr¨om, McDonald, 2011</marker>
<rawString>O. T¨ackstr¨om and R. McDonald. 2011. Discovering fine-grained sentiment with latent variable structured prediction models. In ECIR 2011, pages 368– 374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tan</author>
<author>X Cheng</author>
</authors>
<title>Improving SCL model for sentiment-transfer learning.</title>
<date>2009</date>
<booktitle>In ACL</booktitle>
<pages>181--184</pages>
<contexts>
<context position="15842" citStr="Tan and Cheng, 2009" startWordPosition="2488" endWordPosition="2491">ducing methods for labeling partial uses of names and pronominal references. We address a different problem (S-relevance vs. opinions) and use different methods (graph-based and statistical vs. rulebased). T¨ackstr¨om and McDonald (2011) also solve a similar sequence problem by applying a distantly supervised classifier with an unsupervised hidden sequence component. Their setup differs from ours as our focus lies on pattern-based distant supervision instead of distant supervision using documents for sentence classification. Transfer learning has been applied previously in sentiment analysis (Tan and Cheng, 2009), targeting polarity detection. 4 Methods Due to the sequential properties of S-relevance (cf. Taboada et al. (2009)), we impose the discourse constraint that an S-relevant (resp. S-nonrelevant) sentence tends to follow an S-relevant (resp. Snonrelevant) sentence. Following Pang and Lee (2004), we use minimum cut (MinCut) to formalize this discourse constraint. For a document with n sentences, we create a graph with n + 2 nodes: n sentence nodes and source and sink nodes. We define source and sink to represent the classes S-relevance and Snonrelevance, respectively, and refer to them as SR and</context>
</contexts>
<marker>Tan, Cheng, 2009</marker>
<rawString>S. Tan and X. Cheng. 2009. Improving SCL model for sentiment-transfer learning. In ACL 2009, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thrun</author>
</authors>
<title>Is learning the n-th thing any easier than learning the first?</title>
<date>1996</date>
<booktitle>In NIPS</booktitle>
<pages>640--646</pages>
<contexts>
<context position="2745" citStr="Thrun, 1996" startWordPosition="410" endWordPosition="411">vant (SNR) otherwise. Ideally, we would like to have at our disposal a large annotated training set for our new concept of sentiment relevance. However, such a resource does not yet exist. For this reason, we investigate two semi-supervised approaches to S-relevance classification that do not require Srelevance-labeled data. The first approach is distant supervision (DS). We create an initial labeling based on domain-specific metadata that we extract from a public database and show that this improves performance by 5.8% F1 compared to a baseline. The second approach is transfer learning (TL) (Thrun, 1996). We show that TL improves F1 by 12.6% for sentiment relevance classification when we use a feature representation based on lexical taxonomies that supports knowledge transfer. In our approach, we classify sentences as S(non)relevant because this is the most fine-grained level at which S-relevance manifests itself; at the word or phrase level, S-relevance classification is not possible because of scope and context effects. However, S-relevance is also a discourse phenomenon: authors tend to structure documents into S-relevant passages and S-nonrelevant passages. To impose this discourse constr</context>
</contexts>
<marker>Thrun, 1996</marker>
<rawString>S. Thrun. 1996. Is learning the n-th thing any easier than learning the first? In NIPS 1996, pages 640– 646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
</authors>
<title>Annotating opinions in the world press.</title>
<date>2003</date>
<booktitle>In 4th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>13--22</pages>
<contexts>
<context position="1405" citStr="Wilson and Wiebe, 2003" startWordPosition="202" endWordPosition="205">emi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well. 1 Introduction It is generally recognized in sentiment analysis that only a subset of the content of a document contributes to the sentiment it conveys. For this reason, some authors distinguish the categories subjective and objective (Wilson and Wiebe, 2003). Subjective statements refer to the internal state of mind of a person, which cannot be observed. In contrast, objective statements can be verified by observing and checking reality. Some sentiment analysis systems filter out objective language and predict sentiment based on subjective language only because objective statements do not directly reveal sentiment. Even though the categories subjective/objective are well-established in philosophy, we argue that they are not optimal for sentiment analysis. We instead introduce the notion of sentiment relevance (S-relevance or SR for short). A sent</context>
<context position="11218" citStr="Wilson and Wiebe, 2003" startWordPosition="1760" endWordPosition="1763">rds as S-relevant even if they are not, perhaps because most training instances for the category quote are highly subjective, so that there is insufficient representation of less emphatic Srelevant sentences. These snippets rarely contain plot/movie-business words, so that the P&amp;Ltrained classifier assigns almost all sentences with such words to the category S-nonrelevant. 3 Related Work Many publications have addressed subjectivity in sentiment analysis. Two important papers that are based on the original philosophical definition of the term (internal state of mind vs. external reality) are (Wilson and Wiebe, 2003) and (Riloff and Wiebe, 2003). As we argue above, if the goal is to identify parts of a document that are useful/nonuseful for sentiment analysis, then S-relevance is a better notion to use. Researchers have implicitly deviated from the philosophical definition because they were primarily interested in satisfying the needs of a particular task. For example, Pang and Lee (2004) use a minimum cut graph model for review summarization. Because they do not directly evaluate the results of subjectivity classification, it is not clear to what extent their method is able to identify subjectivity corre</context>
</contexts>
<marker>Wilson, Wiebe, 2003</marker>
<rawString>T. Wilson and J. Wiebe. 2003. Annotating opinions in the world press. In 4th SIGdial Workshop on Discourse and Dialogue, pages 13–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yessenalina</author>
<author>Y Yue</author>
<author>C Cardie</author>
</authors>
<title>Multilevel structured models for document-level sentiment classification.</title>
<date>2010</date>
<booktitle>In EMNLP 2010,</booktitle>
<pages>1046--1056</pages>
<contexts>
<context position="13520" citStr="Yessenalina et al. (2010)" startWordPosition="2136" endWordPosition="2139">wever, they do not use the category S-nonrelevance directly in their experiments and do not evaluate classification accuracy for it. We do not use their data set as it would cause domain mismatch between the product reviews they use and the available movie review subjectivity data (Pang and Lee, 2004) in the TL approach. Changing both the domain (movies to products) and the task (subjectivity to S-relevance) would give rise to interactions that we would like to avoid in our study. The notion of annotator rationales (Zaidan et al., 2007) has some overlap with our notion of sentiment relevance. Yessenalina et al. (2010) use rationales in a multi-level model to integrate sentence-level information into a document classifier. Neither paper presents a direct gold standard evaluation of the accuracy of rationale detection. In summary, no direct evaluation of sentiment relevance has been performed previously. One contribution in this paper is that we provide a single-domain gold standard for sentiment relevance, created based on clear annotation guidelines, and use it for direct evaluation. Sentiment relevance is also related to review mining (e.g., (Ding et al., 2008)) and sentiment retrieval techniques (e.g., (</context>
</contexts>
<marker>Yessenalina, Yue, Cardie, 2010</marker>
<rawString>A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multilevel structured models for document-level sentiment classification. In EMNLP 2010, pages 1046– 1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Zaidan</author>
<author>J Eisner</author>
<author>C Piatko</author>
</authors>
<title>Using annotator rationales to improve machine learning for text categorization.</title>
<date>2007</date>
<booktitle>In NAACL-HLT 2007,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="13437" citStr="Zaidan et al., 2007" startWordPosition="2123" endWordPosition="2126">ed annotation scheme that includes Snonrelevance as one of five categories. However, they do not use the category S-nonrelevance directly in their experiments and do not evaluate classification accuracy for it. We do not use their data set as it would cause domain mismatch between the product reviews they use and the available movie review subjectivity data (Pang and Lee, 2004) in the TL approach. Changing both the domain (movies to products) and the task (subjectivity to S-relevance) would give rise to interactions that we would like to avoid in our study. The notion of annotator rationales (Zaidan et al., 2007) has some overlap with our notion of sentiment relevance. Yessenalina et al. (2010) use rationales in a multi-level model to integrate sentence-level information into a document classifier. Neither paper presents a direct gold standard evaluation of the accuracy of rationale detection. In summary, no direct evaluation of sentiment relevance has been performed previously. One contribution in this paper is that we provide a single-domain gold standard for sentiment relevance, created based on clear annotation guidelines, and use it for direct evaluation. Sentiment relevance is also related to re</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>O. Zaidan, J. Eisner, and C. Piatko. 2007. Using annotator rationales to improve machine learning for text categorization. In NAACL-HLT 2007, pages 260– 267.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>