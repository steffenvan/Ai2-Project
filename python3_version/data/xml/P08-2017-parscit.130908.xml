<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014473">
<title confidence="0.995657">
Assessing the Costs of Sampling Methods in Active Learning for Annotation
</title>
<author confidence="0.997244">
Robbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, Peter McClanahan
</author>
<affiliation confidence="0.9488485">
Department of Computer Science
Brigham Young University
</affiliation>
<address confidence="0.853953">
Provo, UT 84602, USA
</address>
<email confidence="0.924544">
robbie haertel@byu.edu, ringger@cs.byu.edu, kseppi@cs.byu.edu,
jlcarroll@gmail.com,petermcclanahan@gmail.com
</email>
<sectionHeader confidence="0.996399" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.977779470588235">
Traditional Active Learning (AL) techniques
assume that the annotation of each datum costs
the same. This is not the case when anno-
tating sequences; some sequences will take
longer than others. We show that the AL tech-
nique which performs best depends on how
cost is measured. Applying an hourly cost
model based on the results of an annotation
user study, we approximate the amount of time
necessary to annotate a given sentence. This
model allows us to evaluate the effectiveness
of AL sampling methods in terms of time
spent in annotation. We acheive a 77% re-
duction in hours from a random baseline to
achieve 96.5% tag accuracy on the Penn Tree-
bank. More significantly, we make the case
for measuring cost in assessing AL methods.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998444">
Obtaining human annotations for linguistic data is
labor intensive and typically the costliest part of the
acquisition of an annotated corpus. Hence, there is
strong motivation to reduce annotation costs, but not
at the expense of quality. Active learning (AL) can
be employed to reduce the costs of corpus annotation
(Engelson and Dagan, 1996; Ringger et al., 2007;
Tomanek et al., 2007). With the assistance of AL,
the role of the human oracle is either to label a da-
tum or simply to correct the label from an automatic
labeler. For the present work, we assume that cor-
rection is less costly than annotation from scratch;
testing this assumption is the subject of future work.
In AL, the learner leverages newly provided anno-
tations to select more informative sentences which
</bodyText>
<page confidence="0.997692">
65
</page>
<bodyText confidence="0.999863625">
in turn can be used by the automatic labeler to pro-
vide more accurate annotations in future iterations.
Ideally, this process yields accurate labels with less
human effort.
Annotation cost is project dependent. For in-
stance, annotators may be paid for the number of an-
notations they produce or by the hour. In the context
of parse tree annotation, Hwa (2004) estimates cost
using the number of constituents needing labeling
and Osborne &amp; Baldridge (2004) use a measure re-
lated to the number of possible parses. With few ex-
ceptions, previous work on AL has largely ignored
the question of actual labeling time. One excep-
tion is (Ngai and Yarowsky, 2000) (discussed later)
which compares the cost of manual rule writing with
AL-based annotation for noun phrase chunking. In
contrast, we focus on the performance of AL algo-
rithms using different estimates of cost (including
time) for part of speech (POS) tagging, although the
results are applicable to AL for sequential labeling
in general. We make the case for measuring cost in
assessing AL methods by showing that the choice of
a cost function significantly affects the choice of AL
algorithm.
</bodyText>
<sectionHeader confidence="0.726471" genericHeader="introduction">
2 Benefit and Cost in Active Learning
</sectionHeader>
<bodyText confidence="0.9989385">
Every annotation task begins with a set of un-
annotated items U. The ordered set A C U con-
sists of all annotated data after annotation is com-
plete or after available financial resources (or time)
have been exhausted. We expand the goal of AL
to produce the annotated set A such that the benefit
gained is maximized and cost is minimized.
In the case of POS tagging, tag accuracy is usu-
</bodyText>
<subsubsectionHeader confidence="0.353978">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 65–68,
</subsubsectionHeader>
<page confidence="0.48483">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.99996024">
ally used as the measure of benefit. Several heuristic
AL methods have been investigated for determining
which data will provide the most information and
hopefully the best accuracy. Perhaps the best known
are Query by Committee (QBC) (Seung et al., 1992)
and uncertainty sampling (or Query by Uncertainty,
QBU) (Thrun and Moeller, 1992). Unfortunately,
AL algorithms such as these ignore the cost term of
the maximization problem and thus assume a uni-
form cost of annotating each item. In this case, the
ordering of annotated data A will depend entirely on
the algorithm’s estimate of the expected benefit.
However, for AL in POS tagging, the cost term
may not be uniform. If annotators are required to
change only those automatically generated tags that
are incorrect, and depending on how annotators are
paid, the cost of tagging one sentence can depend
greatly on what is known from sentences already an-
notated. Thus, in POS tagging both the benefit (in-
crease in accuracy) and cost of annotating a sentence
depend not only on properties of the sentence but
also on the order in which the items are annotated.
Therefore, when evaluating the performance of an
AL technique, cost should be taken into account. To
illustrate this, consider some basic AL algorithms
evaluated using several simple cost metrics. The re-
sults are presented against a random baseline which
selects sentences at random; the learning curves rep-
resent the average of five runs starting from a ran-
dom initial sentence. If annotators are paid by the
sentence, Figure 1(a) presents a learning curve in-
dicating that the AL policy that selects the longest
sentence (LS) performs rather well. Figure 1(a) also
shows that given this cost model, QBU and QBC are
essentially tied, with QBU enjoying a slight advan-
tage. This indicates that if annotators are paid by
the sentence, QBU is the best solution, and LS is a
reasonable alternative. Next, Figure 1(b) illustrates
that the results differ substantially if annotators are
paid by the word. In this case, using LS as an AL
policy is worse than random selection. Furthermore,
QBC outperforms QBU. Finally, Figure 1(c) shows
what happens if annotators are paid by the number
of word labels corrected. Notice that in this case, the
random selector marginally outperforms the other
techniques. This is because QBU, QBC, and LS tend
to select data that require many corrections. Con-
sidered together, Figures 1(a)-Figure 1(c) show the
significant impact of choosing a cost model on the
relative performance of AL algorithms. This leads
us to conclude that AL techniques should be eval-
uated and compared with respect to a specific cost
function.
While not all of these cost functions are neces-
sarily used in real-life annotation, each can be re-
garded as an important component of a cost model
of payment by the hour. Since each of these func-
tions depends on factors having a significant effect
on the perceived performance of the various AL al-
gorithms, it is important to combine them in a way
that will accurately reflect the true performance of
the selection algorithms.
In prior work, we describe such a cost model for
POS annotation on the basis of the time required for
annotation (Ringger et al., 2008). We refer to this
model as the “hourly cost model”. This model is
computed from data obtained from a user study in-
volving a POS annotation task. In the study, tim-
ing information was gathered from many subjects
who annotated both sentences and individual words.
This study included tests in which words were pre-
labeled with a candidate labeling obtained from an
automatic tagger (with a known error rate) as would
occur in the context of AL. Linear regression on the
study data yielded a model of POS annotation cost:
</bodyText>
<equation confidence="0.98577">
h = (3.795 · l + 5.387 · c + 12.57)/3600 (1)
</equation>
<bodyText confidence="0.999993615384616">
where h is the time in hours spent on the sentence, l
is the number of tokens in the sentence, and c is the
number of words in the sentence needing correction.
For this model, the Relative Standard Error (RSE) is
89.5, and the adjusted correlation (R2) is 0.181. This
model reflects the abilities of the annotators in the
study and may not be representative of annotators in
other projects. However, the purpose of this paper is
to create a framework for accounting for cost in AL
algorithms. In contrast to the model presented by
Ngai and Yarowsky (2000), which predicts mone-
tary cost given time spent, this model estimates time
spent from characteristics of a sentence.
</bodyText>
<sectionHeader confidence="0.983119" genericHeader="method">
3 Evaluation Methodology and Results
</sectionHeader>
<bodyText confidence="0.994755333333333">
Our test data consists of English prose from the
POS-tagged Wall Street Journal text in the Penn
Treebank (PTB) version 3. We use sections 2-21 as
</bodyText>
<figure confidence="0.99145088372093">
66
Tag Accuracy
0.96
0.94
0.92
0.88
0.86
0.9
Random
LS
QBU
QBC
Tag Accuracy
0.96
0.94
0.92
0.88
0.86
0.9
Random
LS
QBU
QBC
Tag Accuracy
0.96
0.94
0.92
0.88
0.86
0.9
Random
LS
QBU
QBC
0 500 1000 1500 2000
Annotated Sentences
(a)
0 20000 40000 60000 80000 100000
Annotated Words
(b)
0 2000 4000 6000 8000 10000
Cumulative Tags Corrected
(c)
</figure>
<figureCaption confidence="0.9768745">
Figure 1: QBU, LS, QBC, and the random baseline plotted in terms of accuracy versus various cost functions: (a)
number of sentences annotated; (b) number of words annotated; and (c) number of tags corrected.
</figureCaption>
<bodyText confidence="0.998012951219512">
initially unannotated data. We employ section 24 as
the development test set on which tag accuracy is
computed at the end of every iteration of AL.
For tagging, we employ an order two Maximum
Entropy Markov Model (MEMM). For decoding, we
found that a beam of size five sped up the decoder
with almost no degradation in accuracy from Viterbi.
The features used in this work are typical for modern
MEMM POS tagging and are mostly based on work
by Toutanova and Manning (2000).
In our implementation, QBU employs a single
MEMM tagger. We approximate the entropy of the
per-sentence tag sequences by summing over per-
word entropy and have found that this approxima-
tion provides equivalent performance to the exact se-
quence entropy. We also consider another selection
algorithm introduced in (Ringger et al., 2007) that
eliminates the overhead of entropy computations al-
together by estimating per-sentence uncertainty with
1 − P(ˆt), where tˆ is the Viterbi (best) tag sequence.
We label this scheme QBUOMM (OMM = “One
Minus Max”).
Our implementation of QBC employs a commit-
tee of three MEMM taggers to balance computa-
tional cost and diversity, following Tomanek et al.
(2007). Each committee member’s training set is a
random bootstrap sample of the available annotated
data, but is otherwise as described above for QBU.
We follow Engelson &amp; Dagan (1996) in the imple-
mentation of vote entropy for sentence selection us-
ing these models.
When comparing the relative performance of AL
algorithms, learning curves can be challenging to in-
terpret. As curves proceed to the right, they can ap-
proach one another so closely that it may be difficult
to see the advantage of one curve over another. For
this reason, we introduce the “cost reduction curve”.
In such a curve, the accuracy is the independent vari-
able. We then compute the percent reduction in cost
(e.g., number of words or hours) over the cost of the
random baseline for the same accuracy a:
</bodyText>
<equation confidence="0.989703">
redux(a) = (costmd(a) − cost(a))/costmd(a)
</equation>
<bodyText confidence="0.999861333333333">
Consequently, the random baseline represents the
trajectory redux(a) = 0.0. Algorithms less costly
than the baseline appear above the baseline. For a
specific accuracy value on a learning curve, the cor-
responding value of the cost on the random baseline
is estimated by interpolation between neighboring
points on the baseline. Using hourly cost, Figure 2
shows the cost reduction curves of several AL al-
gorithms, including those already considered in the
learning curves of Figure 1 (except LS). Restricting
the discussion to the random baseline, QBC, and
QBU: for low accuracies, random selection is the
cheapest according to hourly cost; QBU begins to
be cost-effective at around 91%; and QBC begins to
outperform the baseline and QBU around 80%.
</bodyText>
<sectionHeader confidence="0.999844" genericHeader="method">
4 Normalized Methods
</sectionHeader>
<bodyText confidence="0.999777">
One approach to convert existing AL algorithms into
cost-conscious algorithms is to normalize the results
of the original algorithm by the estimated cost. It
should be somewhat obvious that many selection al-
gorithms are inherently length-biased for sequence
labeling tasks. For instance, since QBU is the sum
</bodyText>
<page confidence="0.996949">
67
</page>
<figure confidence="0.618476444444444">
Random
QBUOMM/N
QBC/N
QBU/N
QBUOMM
QBC
QBU
0.86 0.88 0.9 0.92 0.94 0.96
Tag Accuracy
</figure>
<figureCaption confidence="0.965573666666667">
Figure 2: Cost reduction curves for QBU, QBC,
QBUOMM, their normalized variants, and the random
baseline on the basis of hourly cost
</figureCaption>
<bodyText confidence="0.999964818181818">
of entropy over all words, longer sentences will tend
to have higher uncertainty. The easiest solution is
to normalize by sentence length, as has been done
previously (Engelson and Dagan, 1996; Tomanek et
al., 2007). This of course assumes that annotators
are paid by the word, which may or may not be true.
Nevertheless, this approach can be justified by the
hourly cost model. Replacing the number of words
needing correction, c, with the product of l (the sen-
tence length) and the accuracy p of the model, equa-
tion 1 can be re-written as the estimate:
</bodyText>
<equation confidence="0.948497">
h = ((3.795 + 5.387p) · l + 12.57)/3600
</equation>
<bodyText confidence="0.999888642857143">
Within a single iteration of AL, p is constant, so the
cost is approximately proportional to the length of
the sentence. Figure 2 shows that normalized AL al-
gorithms (suffixed with “/N”) generally outperform
the standard algorithms based on hourly cost (in
contrast to the cost models used in Figures 1(a) -
(c)). All algorithms shown have significant cost
savings over the random baseline for accuracy lev-
els above 92%. Furthermore, all algorithms except
QBU depict trends of further increasing the advan-
tage after 95%. According to the hourly cost model,
QBUOMM/N has an advantage over all other algo-
rithms for accuracies over 91%, achieving a signifi-
cant 77% reduction in cost at 96.5% accuracy.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999981153846154">
We have shown that annotation cost affects the as-
sessment of AL algorithms used in POS annotation
and advocate the use of a cost estimate that best es-
timates the true cost. For this reason, we employed
an hourly cost model to evaluate AL algorithms for
POS annotation. We have also introduced the cost
reduction plot in order to assess the cost savings pro-
vided by AL. Furthermore, inspired by the notion
of cost, we evaluated normalized variants of well-
known AL algorithms and showed that these vari-
ants out-perform the standard versions with respect
to the proposed hourly cost measure. In future work
we will build better cost-conscious AL algorithms.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898060606061">
S. Engelson and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora. In
Proc. of ACL, pages 319–326.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30:253–276.
G. Ngai and D. Yarowsky. 2000. Rule writing or an-
notation: cost-efficient resource usage for base noun
phrase chunking. In Proc. of ACL, pages 117–125.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proc. of HLT-
NAACL, pages 89–96.
E. Ringger, P. McClanahan, R. Haertel, G. Busby,
M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale.
2007. Active learning for part-of-speech tagging: Ac-
celerating corpus annotation. In Proc. of Linguistic
Annotation Workshop, pages 101–108.
E. Ringger, M. Carmen, R. Haertel, K. Seppi, D. Lond-
sale, P. McClanahan, J. Carroll, and N. Ellison. 2008.
Assessing the costs of machine-assisted corpus anno-
tation through a user study. In Proc. of LREC.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In Proc. of CoLT, pages 287–
294.
S. Thrun and K. Moeller. 1992. Active exploration in dy-
namic environments. In NIPS, volume 4, pages 531–
538.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts annota-
tion costs and maintains reusability of annotated data.
Proc. of EMNLP-CoNLL, pages 486–495.
K. Toutanova and C. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. of EMNLP, pages 63–70.
</reference>
<figure confidence="0.9980265">
Reduction in Hourly Cost 0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
-0.1
</figure>
<page confidence="0.977887">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.944359">
<title confidence="0.999971">Assessing the Costs of Sampling Methods in Active Learning for Annotation</title>
<author confidence="0.999992">Robbie Haertel</author>
<author confidence="0.999992">Eric Ringger</author>
<author confidence="0.999992">Kevin Seppi</author>
<author confidence="0.999992">James Carroll</author>
<author confidence="0.999992">Peter McClanahan</author>
<affiliation confidence="0.999356">Department of Computer Science Brigham Young University</affiliation>
<address confidence="0.999687">Provo, UT 84602, USA</address>
<abstract confidence="0.996038777777778">Traditional Active Learning (AL) techniques assume that the annotation of each datum costs the same. This is not the case when annotating sequences; some sequences will take longer than others. We show that the AL technique which performs best depends on how cost is measured. Applying an hourly cost model based on the results of an annotation user study, we approximate the amount of time necessary to annotate a given sentence. This model allows us to evaluate the effectiveness of AL sampling methods in terms of time spent in annotation. We acheive a 77% reduction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Treebank. More significantly, we make the case for measuring cost in assessing AL methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Engelson</author>
<author>I Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="1437" citStr="Engelson and Dagan, 1996" startWordPosition="217" endWordPosition="220">ffectiveness of AL sampling methods in terms of time spent in annotation. We acheive a 77% reduction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Treebank. More significantly, we make the case for measuring cost in assessing AL methods. 1 Introduction Obtaining human annotations for linguistic data is labor intensive and typically the costliest part of the acquisition of an annotated corpus. Hence, there is strong motivation to reduce annotation costs, but not at the expense of quality. Active learning (AL) can be employed to reduce the costs of corpus annotation (Engelson and Dagan, 1996; Ringger et al., 2007; Tomanek et al., 2007). With the assistance of AL, the role of the human oracle is either to label a datum or simply to correct the label from an automatic labeler. For the present work, we assume that correction is less costly than annotation from scratch; testing this assumption is the subject of future work. In AL, the learner leverages newly provided annotations to select more informative sentences which 65 in turn can be used by the automatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less hum</context>
<context position="12263" citStr="Engelson and Dagan, 1996" startWordPosition="2042" endWordPosition="2045">results of the original algorithm by the estimated cost. It should be somewhat obvious that many selection algorithms are inherently length-biased for sequence labeling tasks. For instance, since QBU is the sum 67 Random QBUOMM/N QBC/N QBU/N QBUOMM QBC QBU 0.86 0.88 0.9 0.92 0.94 0.96 Tag Accuracy Figure 2: Cost reduction curves for QBU, QBC, QBUOMM, their normalized variants, and the random baseline on the basis of hourly cost of entropy over all words, longer sentences will tend to have higher uncertainty. The easiest solution is to normalize by sentence length, as has been done previously (Engelson and Dagan, 1996; Tomanek et al., 2007). This of course assumes that annotators are paid by the word, which may or may not be true. Nevertheless, this approach can be justified by the hourly cost model. Replacing the number of words needing correction, c, with the product of l (the sentence length) and the accuracy p of the model, equation 1 can be re-written as the estimate: h = ((3.795 + 5.387p) · l + 12.57)/3600 Within a single iteration of AL, p is constant, so the cost is approximately proportional to the length of the sentence. Figure 2 shows that normalized AL algorithms (suffixed with “/N”) generally </context>
<context position="10134" citStr="Engelson &amp; Dagan (1996)" startWordPosition="1695" endWordPosition="1698"> We also consider another selection algorithm introduced in (Ringger et al., 2007) that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 − P(ˆt), where tˆ is the Viterbi (best) tag sequence. We label this scheme QBUOMM (OMM = “One Minus Max”). Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007). Each committee member’s training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. We follow Engelson &amp; Dagan (1996) in the implementation of vote entropy for sentence selection using these models. When comparing the relative performance of AL algorithms, learning curves can be challenging to interpret. As curves proceed to the right, they can approach one another so closely that it may be difficult to see the advantage of one curve over another. For this reason, we introduce the “cost reduction curve”. In such a curve, the accuracy is the independent variable. We then compute the percent reduction in cost (e.g., number of words or hours) over the cost of the random baseline for the same accuracy a: redux(a</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>S. Engelson and I. Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In Proc. of ACL, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Sample selection for statistical parsing.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--253</pages>
<contexts>
<context position="2233" citStr="Hwa (2004)" startWordPosition="357" endWordPosition="358">. For the present work, we assume that correction is less costly than annotation from scratch; testing this assumption is the subject of future work. In AL, the learner leverages newly provided annotations to select more informative sentences which 65 in turn can be used by the automatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation cost is project dependent. For instance, annotators may be paid for the number of annotations they produce or by the hour. In the context of parse tree annotation, Hwa (2004) estimates cost using the number of constituents needing labeling and Osborne &amp; Baldridge (2004) use a measure related to the number of possible parses. With few exceptions, previous work on AL has largely ignored the question of actual labeling time. One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast, we focus on the performance of AL algorithms using different estimates of cost (including time) for part of speech (POS) tagging, although the results are applicable to AL for </context>
</contexts>
<marker>Hwa, 2004</marker>
<rawString>R. Hwa. 2004. Sample selection for statistical parsing. Computational Linguistics, 30:253–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>D Yarowsky</author>
</authors>
<title>Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>117--125</pages>
<contexts>
<context position="2527" citStr="Ngai and Yarowsky, 2000" startWordPosition="405" endWordPosition="408">utomatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation cost is project dependent. For instance, annotators may be paid for the number of annotations they produce or by the hour. In the context of parse tree annotation, Hwa (2004) estimates cost using the number of constituents needing labeling and Osborne &amp; Baldridge (2004) use a measure related to the number of possible parses. With few exceptions, previous work on AL has largely ignored the question of actual labeling time. One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast, we focus on the performance of AL algorithms using different estimates of cost (including time) for part of speech (POS) tagging, although the results are applicable to AL for sequential labeling in general. We make the case for measuring cost in assessing AL methods by showing that the choice of a cost function significantly affects the choice of AL algorithm. 2 Benefit and Cost in Active Learning Every annotation task begins with a set of unannotated items U. The </context>
<context position="7937" citStr="Ngai and Yarowsky (2000)" startWordPosition="1324" endWordPosition="1327">of POS annotation cost: h = (3.795 · l + 5.387 · c + 12.57)/3600 (1) where h is the time in hours spent on the sentence, l is the number of tokens in the sentence, and c is the number of words in the sentence needing correction. For this model, the Relative Standard Error (RSE) is 89.5, and the adjusted correlation (R2) is 0.181. This model reflects the abilities of the annotators in the study and may not be representative of annotators in other projects. However, the purpose of this paper is to create a framework for accounting for cost in AL algorithms. In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. 3 Evaluation Methodology and Results Our test data consists of English prose from the POS-tagged Wall Street Journal text in the Penn Treebank (PTB) version 3. We use sections 2-21 as 66 Tag Accuracy 0.96 0.94 0.92 0.88 0.86 0.9 Random LS QBU QBC Tag Accuracy 0.96 0.94 0.92 0.88 0.86 0.9 Random LS QBU QBC Tag Accuracy 0.96 0.94 0.92 0.88 0.86 0.9 Random LS QBU QBC 0 500 1000 1500 2000 Annotated Sentences (a) 0 20000 40000 60000 80000 100000 Annotated Words (b) 0 2000 4000 6000 8</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>G. Ngai and D. Yarowsky. 2000. Rule writing or annotation: cost-efficient resource usage for base noun phrase chunking. In Proc. of ACL, pages 117–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
<author>J Baldridge</author>
</authors>
<title>Ensemble-based active learning for parse selection.</title>
<date>2004</date>
<booktitle>In Proc. of HLTNAACL,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="2329" citStr="Osborne &amp; Baldridge (2004)" startWordPosition="369" endWordPosition="372"> from scratch; testing this assumption is the subject of future work. In AL, the learner leverages newly provided annotations to select more informative sentences which 65 in turn can be used by the automatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation cost is project dependent. For instance, annotators may be paid for the number of annotations they produce or by the hour. In the context of parse tree annotation, Hwa (2004) estimates cost using the number of constituents needing labeling and Osborne &amp; Baldridge (2004) use a measure related to the number of possible parses. With few exceptions, previous work on AL has largely ignored the question of actual labeling time. One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast, we focus on the performance of AL algorithms using different estimates of cost (including time) for part of speech (POS) tagging, although the results are applicable to AL for sequential labeling in general. We make the case for measuring cost in assessing AL methods by s</context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>M. Osborne and J. Baldridge. 2004. Ensemble-based active learning for parse selection. In Proc. of HLTNAACL, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>P McClanahan</author>
<author>R Haertel</author>
<author>G Busby</author>
<author>M Carmen</author>
<author>J Carroll</author>
<author>K Seppi</author>
<author>D Lonsdale</author>
</authors>
<title>Active learning for part-of-speech tagging: Accelerating corpus annotation.</title>
<date>2007</date>
<booktitle>In Proc. of Linguistic Annotation Workshop,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="1459" citStr="Ringger et al., 2007" startWordPosition="221" endWordPosition="224">g methods in terms of time spent in annotation. We acheive a 77% reduction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Treebank. More significantly, we make the case for measuring cost in assessing AL methods. 1 Introduction Obtaining human annotations for linguistic data is labor intensive and typically the costliest part of the acquisition of an annotated corpus. Hence, there is strong motivation to reduce annotation costs, but not at the expense of quality. Active learning (AL) can be employed to reduce the costs of corpus annotation (Engelson and Dagan, 1996; Ringger et al., 2007; Tomanek et al., 2007). With the assistance of AL, the role of the human oracle is either to label a datum or simply to correct the label from an automatic labeler. For the present work, we assume that correction is less costly than annotation from scratch; testing this assumption is the subject of future work. In AL, the learner leverages newly provided annotations to select more informative sentences which 65 in turn can be used by the automatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation </context>
<context position="9593" citStr="Ringger et al., 2007" startWordPosition="1607" endWordPosition="1610">imum Entropy Markov Model (MEMM). For decoding, we found that a beam of size five sped up the decoder with almost no degradation in accuracy from Viterbi. The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). In our implementation, QBU employs a single MEMM tagger. We approximate the entropy of the per-sentence tag sequences by summing over perword entropy and have found that this approximation provides equivalent performance to the exact sequence entropy. We also consider another selection algorithm introduced in (Ringger et al., 2007) that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 − P(ˆt), where tˆ is the Viterbi (best) tag sequence. We label this scheme QBUOMM (OMM = “One Minus Max”). Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007). Each committee member’s training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. We follow Engelson &amp; Dagan (1996) in the implementation of vote entropy for sentence selecti</context>
</contexts>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, 2007</marker>
<rawString>E. Ringger, P. McClanahan, R. Haertel, G. Busby, M. Carmen, J. Carroll, K. Seppi, and D. Lonsdale. 2007. Active learning for part-of-speech tagging: Accelerating corpus annotation. In Proc. of Linguistic Annotation Workshop, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ringger</author>
<author>M Carmen</author>
<author>R Haertel</author>
<author>K Seppi</author>
<author>D Londsale</author>
<author>P McClanahan</author>
<author>J Carroll</author>
<author>N Ellison</author>
</authors>
<title>Assessing the costs of machine-assisted corpus annotation through a user study.</title>
<date>2008</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="6819" citStr="Ringger et al., 2008" startWordPosition="1123" endWordPosition="1126">ted and compared with respect to a specific cost function. While not all of these cost functions are necessarily used in real-life annotation, each can be regarded as an important component of a cost model of payment by the hour. Since each of these functions depends on factors having a significant effect on the perceived performance of the various AL algorithms, it is important to combine them in a way that will accurately reflect the true performance of the selection algorithms. In prior work, we describe such a cost model for POS annotation on the basis of the time required for annotation (Ringger et al., 2008). We refer to this model as the “hourly cost model”. This model is computed from data obtained from a user study involving a POS annotation task. In the study, timing information was gathered from many subjects who annotated both sentences and individual words. This study included tests in which words were prelabeled with a candidate labeling obtained from an automatic tagger (with a known error rate) as would occur in the context of AL. Linear regression on the study data yielded a model of POS annotation cost: h = (3.795 · l + 5.387 · c + 12.57)/3600 (1) where h is the time in hours spent on</context>
</contexts>
<marker>Ringger, Carmen, Haertel, Seppi, Londsale, McClanahan, Carroll, Ellison, 2008</marker>
<rawString>E. Ringger, M. Carmen, R. Haertel, K. Seppi, D. Londsale, P. McClanahan, J. Carroll, and N. Ellison. 2008. Assessing the costs of machine-assisted corpus annotation through a user study. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In Proc. of CoLT,</booktitle>
<pages>287--294</pages>
<contexts>
<context position="3853" citStr="Seung et al., 1992" startWordPosition="625" endWordPosition="628">resources (or time) have been exhausted. We expand the goal of AL to produce the annotated set A such that the benefit gained is maximized and cost is minimized. In the case of POS tagging, tag accuracy is usuProceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 65–68, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ally used as the measure of benefit. Several heuristic AL methods have been investigated for determining which data will provide the most information and hopefully the best accuracy. Perhaps the best known are Query by Committee (QBC) (Seung et al., 1992) and uncertainty sampling (or Query by Uncertainty, QBU) (Thrun and Moeller, 1992). Unfortunately, AL algorithms such as these ignore the cost term of the maximization problem and thus assume a uniform cost of annotating each item. In this case, the ordering of annotated data A will depend entirely on the algorithm’s estimate of the expected benefit. However, for AL in POS tagging, the cost term may not be uniform. If annotators are required to change only those automatically generated tags that are incorrect, and depending on how annotators are paid, the cost of tagging one sentence can depen</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. S. Seung, M. Opper, and H. Sompolinsky. 1992. Query by committee. In Proc. of CoLT, pages 287– 294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thrun</author>
<author>K Moeller</author>
</authors>
<title>Active exploration in dynamic environments.</title>
<date>1992</date>
<booktitle>In NIPS,</booktitle>
<volume>4</volume>
<pages>531--538</pages>
<contexts>
<context position="3935" citStr="Thrun and Moeller, 1992" startWordPosition="637" endWordPosition="640">the annotated set A such that the benefit gained is maximized and cost is minimized. In the case of POS tagging, tag accuracy is usuProceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 65–68, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ally used as the measure of benefit. Several heuristic AL methods have been investigated for determining which data will provide the most information and hopefully the best accuracy. Perhaps the best known are Query by Committee (QBC) (Seung et al., 1992) and uncertainty sampling (or Query by Uncertainty, QBU) (Thrun and Moeller, 1992). Unfortunately, AL algorithms such as these ignore the cost term of the maximization problem and thus assume a uniform cost of annotating each item. In this case, the ordering of annotated data A will depend entirely on the algorithm’s estimate of the expected benefit. However, for AL in POS tagging, the cost term may not be uniform. If annotators are required to change only those automatically generated tags that are incorrect, and depending on how annotators are paid, the cost of tagging one sentence can depend greatly on what is known from sentences already annotated. Thus, in POS tagging </context>
</contexts>
<marker>Thrun, Moeller, 1992</marker>
<rawString>S. Thrun and K. Moeller. 1992. Active exploration in dynamic environments. In NIPS, volume 4, pages 531– 538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tomanek</author>
<author>J Wermter</author>
<author>U Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data.</title>
<date>2007</date>
<booktitle>Proc. of EMNLP-CoNLL,</booktitle>
<pages>486--495</pages>
<contexts>
<context position="1482" citStr="Tomanek et al., 2007" startWordPosition="225" endWordPosition="228">time spent in annotation. We acheive a 77% reduction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Treebank. More significantly, we make the case for measuring cost in assessing AL methods. 1 Introduction Obtaining human annotations for linguistic data is labor intensive and typically the costliest part of the acquisition of an annotated corpus. Hence, there is strong motivation to reduce annotation costs, but not at the expense of quality. Active learning (AL) can be employed to reduce the costs of corpus annotation (Engelson and Dagan, 1996; Ringger et al., 2007; Tomanek et al., 2007). With the assistance of AL, the role of the human oracle is either to label a datum or simply to correct the label from an automatic labeler. For the present work, we assume that correction is less costly than annotation from scratch; testing this assumption is the subject of future work. In AL, the learner leverages newly provided annotations to select more informative sentences which 65 in turn can be used by the automatic labeler to provide more accurate annotations in future iterations. Ideally, this process yields accurate labels with less human effort. Annotation cost is project depende</context>
<context position="9955" citStr="Tomanek et al. (2007)" startWordPosition="1666" endWordPosition="1669">e entropy of the per-sentence tag sequences by summing over perword entropy and have found that this approximation provides equivalent performance to the exact sequence entropy. We also consider another selection algorithm introduced in (Ringger et al., 2007) that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 − P(ˆt), where tˆ is the Viterbi (best) tag sequence. We label this scheme QBUOMM (OMM = “One Minus Max”). Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al. (2007). Each committee member’s training set is a random bootstrap sample of the available annotated data, but is otherwise as described above for QBU. We follow Engelson &amp; Dagan (1996) in the implementation of vote entropy for sentence selection using these models. When comparing the relative performance of AL algorithms, learning curves can be challenging to interpret. As curves proceed to the right, they can approach one another so closely that it may be difficult to see the advantage of one curve over another. For this reason, we introduce the “cost reduction curve”. In such a curve, the accurac</context>
<context position="12286" citStr="Tomanek et al., 2007" startWordPosition="2046" endWordPosition="2049">gorithm by the estimated cost. It should be somewhat obvious that many selection algorithms are inherently length-biased for sequence labeling tasks. For instance, since QBU is the sum 67 Random QBUOMM/N QBC/N QBU/N QBUOMM QBC QBU 0.86 0.88 0.9 0.92 0.94 0.96 Tag Accuracy Figure 2: Cost reduction curves for QBU, QBC, QBUOMM, their normalized variants, and the random baseline on the basis of hourly cost of entropy over all words, longer sentences will tend to have higher uncertainty. The easiest solution is to normalize by sentence length, as has been done previously (Engelson and Dagan, 1996; Tomanek et al., 2007). This of course assumes that annotators are paid by the word, which may or may not be true. Nevertheless, this approach can be justified by the hourly cost model. Replacing the number of words needing correction, c, with the product of l (the sentence length) and the accuracy p of the model, equation 1 can be re-written as the estimate: h = ((3.795 + 5.387p) · l + 12.57)/3600 Within a single iteration of AL, p is constant, so the cost is approximately proportional to the length of the sentence. Figure 2 shows that normalized AL algorithms (suffixed with “/N”) generally outperform the standard</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>K. Tomanek, J. Wermter, and U. Hahn. 2007. An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data. Proc. of EMNLP-CoNLL, pages 486–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy partof-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="9258" citStr="Toutanova and Manning (2000)" startWordPosition="1555" endWordPosition="1558">ed in terms of accuracy versus various cost functions: (a) number of sentences annotated; (b) number of words annotated; and (c) number of tags corrected. initially unannotated data. We employ section 24 as the development test set on which tag accuracy is computed at the end of every iteration of AL. For tagging, we employ an order two Maximum Entropy Markov Model (MEMM). For decoding, we found that a beam of size five sped up the decoder with almost no degradation in accuracy from Viterbi. The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). In our implementation, QBU employs a single MEMM tagger. We approximate the entropy of the per-sentence tag sequences by summing over perword entropy and have found that this approximation provides equivalent performance to the exact sequence entropy. We also consider another selection algorithm introduced in (Ringger et al., 2007) that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 − P(ˆt), where tˆ is the Viterbi (best) tag sequence. We label this scheme QBUOMM (OMM = “One Minus Max”). Our implementation of QBC employs a committee o</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova and C. Manning. 2000. Enriching the knowledge sources used in a maximum entropy partof-speech tagger. In Proc. of EMNLP, pages 63–70.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>