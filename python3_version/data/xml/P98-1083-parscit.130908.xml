<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<note confidence="0.497031285714286">
Using Decision Trees to Construct a Practical Parser
Masahiko Haruno* Satoshi Shirait Yoshifumi Ooyamat
mharuno(qhip.atr.co.jp shirai©cslab.kecl.ntt.co.jp ooyarna*cslab.kecl.ntt.co.jp
*ATR Human Information Processing Research Laboratories
2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan.
tNTT Communication Science Laboratories
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan.
</note>
<sectionHeader confidence="0.949416" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980466666667">
This paper describes novel and practical Japanese
parsers that uses decision trees. First, we con-
struct a single decision tree to estimate modifica-
tion probabilities; how one phrase tends to modify
another. Next, we introduce a boosting algorithm
in which several decision trees are constructed and
then combined for probability estimation. The two
constructed parsers are evaluated by using the EDR
Japanese annotated corpus. The single-tree method
outperforms the conventional Japanese stochastic
methods by 4%. Moreover, the boosting version is
shown to have significant advantages; 1) better pars-
ing accuracy than its single-tree counterpart for any
amount of training data and 2) no over-fitting to
data for various iterations.
</bodyText>
<sectionHeader confidence="0.993971" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9995608">
Conventional parsers with practical levels of perfor-
mance require a number of sophisticated rules that
have to be hand-crafted by human linguists. It is
time-consuming and cumbersome to maintain these
rules for two reasons.
</bodyText>
<listItem confidence="0.97541025">
• The rules are specific to the application domain.
• Specific rules handling collocation al expressions
create side effects. Such rules often deteriorate
the overall performance of the parser.
</listItem>
<bodyText confidence="0.999889468085106">
The stochastic approach, on the other hand, has
the potential to overcome these difficulties. Because
it induces stochastic rules to maximize overall per-
formance against training data, it not only adapts
to any application domain but also may avoid over-
fitting to the data. In the late 80s and early 90s, the
induction and parameter estimation of probabilis-
tic context free grammars (PCFGs) from corpora
were intensively studied. Because these grammars
comprise only nonterminal and part-of-speech tag
symbols, their performances were not enough to be
used in practical applications (Charniak, 1993). A
broader range of information, in particular lexical in-
formation, was found to be essential in disambiguat-
ing the syntactic structures of real-world sentences.
SPATTER (Magerman, 1995) augmented the pure
PCFG by introducing a number of lexical attributes.
The parser controlled applications of each rule by us-
ing the lexical constraints induced by decision tree
algorithm (Quinlan, 1993). The SPATTER parser
attained 87% accuracy and first made stochastic
parsers a practical choice. The other type of high-
precision parser, which is based on dependency anal-
ysis was introduced by Collins (Collins, 1996). De-
pendency analysis first segments a sentence into syn-
tactically meaningful sequences of words and then
considers the modification of each segment. Collins&apos;
parser computes the likelihood that each segment
modifies the other (2 term relation) by using large
corpora. These modification probabilities are con-
ditioned by head words of two segments, distance
between the two segments and other syntactic fea-
tures. Although these two parsers have shown simi-
lar performance, the keys of their success are slightly
different. SPATTER parser performance greatly de-
pends on the feature selection ability of the decision
tree algorithm rather than its linguistic representa-
tion. On the other hand, dependency analysis plays
an essential role in Collins&apos; parser for efficiently ex-
tracting information from corpora.
In this paper, we describe practical Japanese de-
pendency parsers that uses decision trees. In the
Japanese language, dependency analysis has been
shown to be powerful because segment (bunsetsu)
order in a sentence is relatively free compared to
European languages. Japanese dependency parsers
generally proceed in three steps.
</bodyText>
<listItem confidence="0.994538833333333">
1. Segment a sentence into a sequence of bunsetsu.
2. Prepare a modification matrix, each value of
which represents how one bunsetsu is likely to
modify another.
3. Find optimal modifications in a sentence by a
dynamic programming technique.
</listItem>
<bodyText confidence="0.997832428571429">
The most difficult part is the second; how to con-
struct a sophisticated modification matrix. With
conventional Japanese parsers, the linguist must
classify the .bunsetsu and select appropriate features
to compute modification values. The parsers thus
suffer from application domain diversity and the side
effects of specific rules.
</bodyText>
<page confidence="0.996832">
505
</page>
<bodyText confidence="0.999988692307692">
Stochastic dependency parsers like Collins&apos;, on the
other hand, define a set of attributes for condition-
ing the modification probabilities. The parsers con-
sider all of the attributes regardless of bunsetsu type.
These methods can encompass only a small number
of features if the probabilities are to be precisely
evaluated from finite number of data. Our decision
tree method constructs a more sophisticated modi-
fication matrix. It automatically selects a sufficient
number of significant attributes according to bun-
setsu type. We can use arbitrary numbers of the
attributes which potentially increase parsing accu-
racy.
Natural languages are full of exceptional and collo-
cational expressions. It is difficult for machine learn-
ing algorithms, as well as human linguists, to judge
whether a specific rule is relevant in terms of over-
all performance. To tackle this problem, we test
the mixture of sequentially generated decision trees.
Specifically, we use the Ada-Boost algorithm (Fre-
und and Schapire, 1996) which iteratively performs
two procedures: 1. construct a decision tree based
on the current data distribution and 2. updating
the distribution by focusing on data that are not
well predicted by the constructed tree. The final
modification probabilities are computed by mixing
all the decision trees according to their performance.
The sequential decision trees gradually change from
broad coverage to specific exceptional trees that can-
not. be captured by a single general tree. In other
words, the method incorporates not only general ex-
pressions but also infrequent specific ones.
The rest of the paper is constructed as follows.
Section 2 summarizes dependency analysis for the
Japanese language. Section 3 explains our decision
tree models that compute modification probabili-
ties. Section 4 then presents experimental results
obtained by using EDR Japanese annotated corpora.
Finally, section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.89926" genericHeader="introduction">
2 Dependency Analysis in Japanese
Language
</sectionHeader>
<bodyText confidence="0.998294333333333">
This section overviews dependency analysis in the
Japanese language. The parser generally performs
the following three steps.
</bodyText>
<listItem confidence="0.994101333333333">
1. Segment a sentence into a sequence of bunsetsu.
2. Prepare modification matrix each value of which
represents how one bunsetsu is likely to modify
the other.
3. Find optimal modifications in a sentence by a
dynamic programming technique.
</listItem>
<bodyText confidence="0.996504875">
Because there are no explicit delimiters between
words in Japanese, input sentences are first word
segmented, part-of-speech tagged, and then chunked
into a sequence of bunsetsus. The first step yields,
for the following example, the sequence of bunsetsu
displayed below. The parenthesis in the Japanese
expressions represent the internal structures of the
bunsetsu (word segmentations).
</bodyText>
<equation confidence="0.992863571428572">
Example: H196D9 r_idfifi6D-1-e VI7 4
((fl t R)(Q))) ((idrifi)(0))
kinou-no yeittgata-ni kinjo-no
yesterday-NO evening-NI neighbor-NO
((f-ei))09)(r7-f2)(t-).) ((kk)(1:!)
kodomo-ga wain-wo nomu-Fta
children-GA wine-wo drink-4-PAST
</equation>
<bodyText confidence="0.9990872">
The second step of parsing is to construct a modifi-
cation matrix whose values represent the likelihood
that one bunsetsu modifies another in a sentence.
In the Japanese language, we usually make two as-
sumptions:
</bodyText>
<listItem confidence="0.99841575">
1. Every bunsetsu except the last one modifies
only one posterior bunsetsu.
2. No modification crosses to other modifications
in a sentence.
</listItem>
<bodyText confidence="0.99903525">
Table 1 illustrates a modification matrix for the
example sentence. In the matrix, columns and rows
represent anterior and posterior bunsetsus, respec-
tively. For example, the first bunsetsu &amp;quot;kinow- no
modifies the second &apos;yaugata-ni with score 0.70 and
the third &apos;kinjo-no&apos; with score 0.07. The aim of this
paper is to generate a modification matrix by using
decision trees.
</bodyText>
<table confidence="0.6617675">
y•vgaia•ni kino•-no yurgata-no kinyo-no dam a. 1.00
kinjo-no 0.70 0.10 0.70 0.05
kodorne-ga 0.07 0.10 0.20 0.95
nome•ta 0.10 0.10 0.10
0.10 0.70
0.03
</table>
<tableCaption confidence="0.997608">
Table 1: Modification Matrix for Sample Sentence
</tableCaption>
<bodyText confidence="0.9999816">
The final step of parsing optimizes the entire de-
pendency structure by using the values in the mod-
ification matrix.
Before going into our model, we introduce the no-
tations that will be used in the model. Let S be
the input sentence. S comprises a bunsetsu set B of
length in (l&lt; b1, f1 &gt;,- • , &lt; bm, f„, &gt;)) in which
bi and f represent the ith bunsetsu and its features,
respectively. We define D to be a modification set; D
= {mod(1), • • • ,mod(m — 1)) in which mod(i) indi-
cates the number of busetsu modified by the ith bun-
setsu. Because of the first assumption, the length of
D is always in — 1. Using these notations, the result
of the third step for the example can be given as D
= {2, 6, 4, 6, 6} as displayed in Figure 1.
</bodyText>
<sectionHeader confidence="0.991058" genericHeader="method">
3 Decision Trees for Dependency
Analysis
</sectionHeader>
<subsectionHeader confidence="0.992502">
3.1 Stochastic Model and Decision Trees
</subsectionHeader>
<bodyText confidence="0.999889">
The stochastic dependency parser assigns the most
plausible modification set Dbesi to a sentence S in
</bodyText>
<page confidence="0.977378">
506
</page>
<figure confidence="0.889889">
PDT(yesibi, fi • • fr.) (1)
I2 6
kiou-no pug;a-ni kinjo-no kodomo-ga wain-wo nomu-ta
Li Li
</figure>
<figureCaption confidence="0.999946">
Figure 1: Modification Set for Sample Sentence
</figureCaption>
<bodyText confidence="0.706492">
terms of the training data distribution.
</bodyText>
<equation confidence="0.95999">
Dbest = argmaxDP(DIS) argmaxDP(DIB)
</equation>
<bodyText confidence="0.999622111111111">
By assuming the independence of modifica-
tions, P(DIB) can be transformed as follows.
f , • • ,fm) means the probability that
a pair of bunsetsu b and bi have a modification rela-
tion. Note that each modification is constrained by
all features{ f „ • , f,} in a sentence despite of the
assumption of independence.We use decision trees
to dynamically select appropriate features for each
combination of bunsetsus from If „. ,
</bodyText>
<equation confidence="0.968024">
P(DIB) = bi , f,,- • • ,
</equation>
<bodyText confidence="0.982911833333333">
Let us first consider the single tree case. The
training data for the decision tree comprise any un-
ordered combination of two bunsetsu in a sentence.
Features used for learning are the linguistic informa-
tion associated with the two bunsetsu. The next sec-
tion will explain these features in detail. The class
set for learning has binary values yes and no which
delineate whether the data (the two bunstsu) has
a modification relation or not. In this setting, the
decision tree algorithm automatically and consecu-
tively selects the significant features for discriminat-
ing modify/non-modify relations.
We slightly changed C4.5 (Quinlan, 1993) pro-
grams to be able to extract class frequen-
cies at every node in the decision tree be-
cause our task is regression rather than classi-
fication. By using the class distribution, we
compute the probability P DT(yesibi, bi , 1, , • • , fm)
which is the Laplace estimate of empirical likeli-
hood that bi modifies bi in the constructed deci-
sion tree DT. Note that it is necessary to nor-
malize PDT(yesibi,bi , f „ • • ,fm) to approximate
f „ • • , fm). By considering all can-
didates posterior to b, P(yesibi,bi , f , • • • ,f,,) is
computed using a heulistic rule (1). It is of course
reasonable to normalize class frequencies instead of
the probability PDT (Yes b.; f&apos;, • • • , f .) • Equa-
tion (1) tends to emphasize long distance dependen-
cies more than is true for frequency-based normal-
ization.
</bodyText>
<equation confidence="0.9865995">
P(yesibi,bi, f , • , f,„„,)
E k&gt;imPDT(yeslbi,bi, fi, • • • f
</equation>
<bodyText confidence="0.997348785714286">
Let us extend the above to use a set of decision
trees. As briefly mentioned in Section 1, a number
of infrequent and exceptional expressions appear in
any natural language phenomena; they deteriorate
the overall performance of application systems. It
is also difficult for automated learning systems to
detect and handle these expressions because excep-
tional expressions are placed in the same class as
frequent ones. To tackle this difficulty, we gener-
ate a set of decision trees by adaboost (Freund and
Schapire, 1996) algorithm illustrated in Table 2. The
algorithm first sets the weights to 1 for all exam-
ples (2 in Table 2) and repeats the following two
procedures T times (3 in Table 2).
</bodyText>
<listItem confidence="0.997276666666666">
1. A decision tree is constructed by using the cur-
rent weight vector ((a) in Table 2)
2. Example data are then parsed by using the tree
and the weights of correctly handled examples
are reduced ((b),(c) in Table 2)
1. Input: sequence of N examples &lt; e., w. &gt;, &lt;
eN,UIN &gt; in which ei and iv, represent an example
and its weight, respectively.
2. Initialize the weight vector w, =1 for i 1, , N
3. Do fort =
(a) Call C4.5 providing it with the weight vector
tv,s and Construct a modification probability
set ht
(b) Let Error be a set of examples that are not
identified by ht
</listItem>
<bodyText confidence="0.8941495">
Compute the pseudo error rate of ht:
ft E iCErrortrii E
if et &gt; 1, then abort loop
=
</bodyText>
<listItem confidence="0.960186666666667">
(c) For examples correctly predicted by ht, update
the weights vector to be tvi =
4. Output a final probability set:
</listItem>
<construct confidence="0.67963425">
= Et.,T(log2--)ht/Et.=,T(logT1 )
Table 2: Combining Decision Trees by Ada-boost
Algorithm
The final probability set hf is then computed
by mixing T trees according to their perfor-
mance (4 in Table 2). Using hf instead of
PDT(yesibi,bi , f1, • • • , f,„), in equation (1) gener-
ates a boosting version of the dependency parser.
</construct>
<subsectionHeader confidence="0.9646455">
3.2 Linguistic Feature Types Used for
Learning
</subsectionHeader>
<bodyText confidence="0.990888">
This section explains the concrete feature setting we
used for learning. The feature set mainly focuses on
</bodyText>
<page confidence="0.970237">
507
</page>
<table confidence="0.973382666666667">
No. Two Bunsetsu No. Others
1 lexical information of bead word 6 distance between two bunsetsu
2 part-of-speech of -head word 7 particle &apos;wa&apos; between two bunsetsu
3 type of bunsetsu 8 punctuation between two bunsetsu
4 punctuation
5 _ parentheses
</table>
<tableCaption confidence="0.966971">
Table 3: Linguistic Feature Types Used for Learning
</tableCaption>
<table confidence="0.87664205">
IFe•tu re Type Value•
2 4&apos; )1:111,1 • !I:45 141 , :LI; , .U4 F, 14 , &apos;&amp;1; 01, 11,1 If 1+,i4, 171,111111+1,14, 41111+,14, 1,..Z, kol,
401414, CS. F414, ER014, 11,314, ItbMtlfilliAt, ta11414, Ifillfil11:1, 431/0111,
5i514, 11111i), ICILIEtIAT,14, algrOMA), ti4CtUtuT714, 5Rtt5a4phrtn,
r.11111Mallikt, 4;111V6t1M1:J. t1;14451f, 1,11/114, 141:114, ;1341:1. Eti.M4114
3 5.), &lt;.,,i.), Ite.,1110, Ifilei,, .L.i- Le,, ...i., I-, Lt., Lt., Itcht, t:D, T,
r, 0,,- -e, .f.-Li&apos;, ..!-ic.:, -F, f.,,...,, Fit, /!,r, rfo,,,it,&amp;quot;7...,-c, •,-7, t ,
,, ..-:6, ex, tr$,, te..., exoL,r.o.,LI: ,:eh&apos;:,, t.r.t•&apos;, t., i, te.,,lt.:, Cr % ) ,
l:.:A..t., tekT,::, la, 0,0A, I:, If.5. 4 , -s, 1&apos;1 r , It:, It::t, IT, 6,
6 ,.), 600), t., ,c,, X. 1) , 1 :. , Z., h, &apos;Cr &amp;quot; 4 &amp;quot; grAi), .t.i:, MMIT,I, !801.1.11, S*, k-5-,
ItZ&amp;quot;, A, EIC514, iBf1.64, 1bff1411/10i4fEdA1, ffilit14, Shti, 134Iltflanc ,IN1744,
Aft, IA, 414, rt14, 41,t111, *IS, 4., 14140,114, S14, DM, Ral9114, Affdfig,114,
$1110114, OA SIC IAIR, UafErif4), ifilliMt14..4W::, XI: *!..i, r,14/Warfur714,
*)S)4&amp;quot;)1),1514rEttIlfIbrdi&amp;quot;, t1111ttRIASP, air, Illit.N1P1,
14:0114. ;IN. it.1144, T.:11AM&apos;F214, i11111
4 non, PIA, 53A
5 non, &apos; . ■ . F. 1. 1, ,. 1. .1.1.1,1
6 A(0), B(1-4), C(&gt;5)
7 0.1
a 0.1
</table>
<tableCaption confidence="0.568468">
Table 4: Values for Each Feature Type
</tableCaption>
<figureCaption confidence="0.998236">
Figure 2: Learning Curve of Single-Tree Parser
</figureCaption>
<bodyText confidence="0.963931">
the two bunsetsu constituting each data. The class
set consists of binary values which delineate whether
a sample (the two bunsetsu) have a modification re-
lation or not. We use 13 features for the task, 10 di-
rectly from the 2 bunsetsu under consideration and
3 for other bunsetu information as summarized in
Table 3.
Each bunsetsu (anterior and posterior) has the 5
features: No.1 to No.5 in Table 3. Features No.6
to No.8 are related to bunsetsu pairs. Both No.1
and No.2 concern the head word of the bunsetsu.
No.1 takes values of frequent words or thesaurus cat-
egories (NLRI, 1964). No.2, on the other hand, takes
values of part-of-speech tags. No.3 deals with bun-
setsu types which consist of functional word chunks
or the part-of-speech tags that dominate the bun-
setsu&apos;s syntactic characteristics. No.4 and No.5 are
binary features and correspond to punctuation and
parentheses, respectively. No.6 represents how many
bunsetsus exist between the two bunsetsus. Possible
values are A(0), B(0-4) and C(&gt;5). No.7 deals with
the post-positional particle &apos;wa&apos; which greatly influ-
ences the long distance dependency of subject-verb
modifications. Finally, No.8 addresses the punctua-
tion between the two bunsetsu. The detailed values
of each feature type are summarized in Table 4.
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999898">
We evaluated the proposed parser using the EDR
Japanese annotated corpus (EDR, 1995). The ex-
periment consisted of two parts. One evaluated the
single-tree parser and the other the boosting coun-
terpart. In the rest of this section, parsing accuracy
refers only to precision; how many of the system&apos;s
output are correct in terms of the annotated corpus.
We do not show recall because we assume every bun-
setsu modifies only one posterior bunsetsu. The fea-
tures used for learning were non head-word features,
(i.e., type 2 to 8 in Table 3). Section 4.1.4 investi-
gates lexical information of head words such as fre-
quent words and thesaurus categories. Before going
into details of the experimental results, we summa-
rize here how training and test data were selected.
</bodyText>
<listItem confidence="0.995731166666667">
1. After all sentences in the EDR corpus
were word-segmented and part-of-speech
tagged (Matsumoto and others, 1996), they
were then chunked into a sequence of bunsetsu.
2. All bunsetsu pairs were compared with EDR
bracketing annotation (correct segmentations
</listItem>
<figure confidence="0.9195465">
5000 10000 ¶5000
20000 25000 30000 35000 40000 45000 50000
Number 01 Tnurong Da.
84
83
825
</figure>
<page confidence="0.987854">
508
</page>
<table confidence="0.9898405">
Confidence Level 25% 50% 75% 95%
Parsing Accuracy 82.01% 83.43% 83.52% 83.35%
</table>
<tableCaption confidence="0.977785">
Table 5: Number of Training Sentences v.s. Parsing Accuracy
</tableCaption>
<table confidence="0.9991435">
Number of Training Sentences 3000 6000 10000 20000 30000 50000
Parsing Accuracy 82.07% 82.70% 83.52% 84.07% 84.27% 84.33%
</table>
<tableCaption confidence="0.999572">
Table 6: Pruning Confidence Level v.s.Parsing Accuracy
</tableCaption>
<bodyText confidence="0.995457090909091">
and modifications). If a sentence contained a
pair inconsistent with the EDR annotation, the
sentence was removed from the data.
3. All data examined (total number of sen-
tences:207802, total number of bun-
setsu:1790920) were divided into 20 files.
The training data were same number of first
sentences of the 20 files according to the
training data size. Test data (10000 sentences)
were the 2501th to 3000th sentences of each
file.
</bodyText>
<subsectionHeader confidence="0.999167">
4.1 Single Tree Experiments
</subsectionHeader>
<bodyText confidence="0.9998085">
In the single tree experiments, we evaluated the fol-
lowing 4 properties of the new dependency parser.
</bodyText>
<listItem confidence="0.970216666666667">
• Tree pruning and parsing accuracy
• Number of training data and parsing accuracy
• Significance of features other than Head-word
Lexical Information
• Significance of Head-word Lexical Information
4.1.1 Pruning and Parsing Accuracy
</listItem>
<bodyText confidence="0.999938133333333">
Table 5 summarizes the parsing accuracy with var-
ious confidence levels of pruning. The number of
training sentences was 10000.
In C4.5 programs, a larger value of confidence
means weaker pruning and 25% is commonly used in
various domains (Quinlan, 1993). Our experimental
results show that 75% pruning attains the best per-
formance, i.e. weaker pruning than usual. In the
remaining single tree experiments, we used the 75%
confidence level. Although strong pruning treats in-
frequent data as noise, parsing involves many ex-
ceptional and infrequent modifications as mentioned
before. Our result means that only information in-
cluded in small numbers of samples are useful for
disambiguating the syntactic structure of sentences.
</bodyText>
<subsectionHeader confidence="0.7559025">
4.1.2 The amount of Training Data and
Parsing Accuracy
</subsectionHeader>
<bodyText confidence="0.98801875">
Table 6 and Figure 2 show how the number of train-
ing sentences influences parsing accuracy for the
same 10000 test sentences. They illustrate the fol-
lowing two characteristics of the learning curve.
</bodyText>
<listItem confidence="0.9861974">
1. The parsing accuracy rapidly rises up to 30000
sentences and converges at around 50000 sen-
tences.
2. The maximum parsing accuracy is 84.33% at
50000 training sentences.
</listItem>
<bodyText confidence="0.999968821428571">
We will discuss the maximum accuracy of 84.3&apos;3%.
Compared to recent stochastic English parsers that
yield 86 to 87% accuracy (Collins, 1996; Mager-
man, 1995), 84.33% seems unsatisfactory at the first
glance. The main reason behind this lies in the dif-
ference between the two corpora used: Penn Tree-
bank (Marcus et al., 1993) and EDR corpus (EDR,
1995). Penn Treebank(Niarcus et al., 1993) was also
used to induce part-of-speech (POS) taggers because
the corpus contains very precise and detailed POS
markers as well as bracket annotations. In addition,
English parsers incorporate the syntactic tags that
are contained in the corpus. The EDR corpus, on the
other hand, contains only coarse POS tags. We used
another Japanese POS tagger (Matsumoto and oth-
ers, 1996) to make use of well-grained information
for disambiguating syntactic structures. Only the
bracket information in the EDR corpus was consid-
ered. We conjecture that the difference between the
parsing accuracies is due to the difference of the cor-
pus information. (Fujio and Matsumoto, 1997) con-
structed an EDR-based dependency parser by using
a similar method to Collins&apos; (Collins, 1996). The
parser attained 80.48% accuracy. Although thier
training and test sentences are not exactly same as
ours, the result seems to support our conjecture on
the data difference between EDR and Penn Tree-
bank.
</bodyText>
<subsectionHeader confidence="0.9646755">
4.1.3 Significance of Non Head-Word
Features
</subsectionHeader>
<bodyText confidence="0.99947725">
We will now summarize the significance of each non
head-word feature introduced in Section 3. The in-
fluence of the lexical information of head words will
be discussed in the next section. Table 7 illustrates
how the parsing accuracy is reduced when each fea-
ture is removed. The number of training sentences
was 10000. In the table, ant and post represent the
anterior and the posterior bunsetsu, respectively.
</bodyText>
<tableCaption confidence="0.92108">
Table 7 clearly demonstrates that the most signifi-
</tableCaption>
<page confidence="0.952096">
509
</page>
<table confidence="0.785935">
Feature Accuracy Decrease Feature Accuracy Decrease
ant POS of head -0.07% post punctuation +1.62%
ant bunsetsu type +9.34% post parentheses ±0.00%
ant punctuation +1.15% distance between two bunsetsus +5.21%
ant parentheses ±0.00% punctuation between two bunsetsus +0.01%
post POS of head +2.13% &apos;wa&apos; between two bunsetsus +1.79%
post bunsetsu type +0.52%
</table>
<tableCaption confidence="0.999955">
Table 7: Decrease of Parsing Accuracy When Each Attribute Removed
Table 8: Head Word Information v.s. Parsing Accuracy
</tableCaption>
<table confidence="0.8610264">
Head Word Information
100 words 200 words Level 1 Level 2
83.34%
82.68% 82.51% 81.67%
Parsing Accuracy
</table>
<bodyText confidence="0.999718571428571">
cant features are anterior bunsetsu type and distance
between the two bunsetsu. This result may partially
support an often used heuristic; bunsetsu modifica-
tion should be as short range as possible, provided
the modification is syntactically possible. In partic-
ular, we need to concentrate on the types of bunsetsu
to attain a higher level of accuracy. Most features
contribute, to some extent, to the parsing perfor-
mance. In our experiment, information on paren-
theses has no effect on the performance. The reason
may be that. EDR contains only a small number of
parentheses. One exception in our features is an-
terior POS of head. We currently hypothesize that
this drop of accuracy arises from two reasons.
</bodyText>
<listItem confidence="0.99087125">
• In many cases, the POS of bead word can be
determined from bunsetsu type.
• Our POS tagger sometimes assigns verbs for
verb-derived nouns.
</listItem>
<sectionHeader confidence="0.441086" genericHeader="method">
4.1.4 Significance of Head-words Lexical
Information
</sectionHeader>
<bodyText confidence="0.999988714285714">
We focused on the head-word feature by testing the
following 4 lexical sources. The first and the second
are the 100 and 200 most frequent words, respec-
tively. The third and the fourth are derived from a
broadly used Japanese thesaurus, Word List, by Se-
mantic Principles (NLRI, 1964). Level 1 and Level 2
classify words into 15 and 67 categories, respectively.
</bodyText>
<listItem confidence="0.928066">
1. 100 most, Frequent words
2. 200 most Frequent words
3. Word List Level 1
4. Word List Level 2
</listItem>
<bodyText confidence="0.999736384615385">
Table 8 displays the parsing accuracy when each
head word information was used in addition to the
previous features. The number of training sentences
was 10000. In all cases, the performance was worse
than 83.52% which was attained without head word
lexical information. More surprisingly, more head
word information yielded worse performance. From
this result, it may be safely said, at least for the
Japanese language; that, we cannot expect lexical in-
formation to always improve the performance. Fur-
ther investigation of other thesaurus and cluster-
ing (Charniak, 1997) techniques is necessary to fully
understand the influence of lexical information.
</bodyText>
<subsectionHeader confidence="0.995406">
4.2 Boosting Experiments
</subsectionHeader>
<bodyText confidence="0.993636076923077">
This section reports experimental results on the
boosting version of our parser. In all experiments,
pruning confidence levels were set to 55%. Table 9
and Figure 3 show the parsing accuracy when the
number of training examples was increased. Because
the number of iterations in each data set changed be-
tween 5 and 8, we will show the accuracy by combin-
ing the first 5 decision trees. In Figure 3, the dotted
line plots the learning of the single tree case (identi-
cal to Figure 2) for reader&apos;s convenience. The char-
acteristics of the boosting version can be summa-
rized as follows compared to the single tree version.
• The learning curve rises more rapidly with a
small number of examples. It is surprising that
the boosting version with 10000 sentences per-
forms better than the single tree version with
50000 sentences.
• The boosting version significantly outperforms
the single tree counterpart for any number of
sentences although they use the same features
for learning.
Next, we discuss how the number of iterations in-
fluences the parsing accuracy. Table 10 shows the
parsing accuracy for various iteration numbers when
50000 sentences were used as training data. The re-
sults have two characteristics.
</bodyText>
<listItem confidence="0.99067">
• Parsing accuracy rose up rapidly at the second
iteration.
• No over-fitting to data was seen although the
performance of each generated tree fell around
30% at the final stage of iteration.
</listItem>
<page confidence="0.986843">
510
</page>
<table confidence="0.988058">
Number of Training Sentences 3000 6000 10000 20000 30000 50000
Parsing Accuracy 83.10% 84.03% 84.44% 84.74% 84.91% 85.03%
</table>
<tableCaption confidence="0.983299">
Table 9: Number of Training Sentences v.s. Parsing Accuracy
</tableCaption>
<table confidence="0.9977035">
Number of Iteration 1 2 3 4 5 6
Parsing Accuracy 84.32% 84.93% 84.89% 84.86% 85.03% 85.01%
</table>
<tableCaption confidence="0.997994">
Table 10: Number of Iteration v.s. Parsing Accuracy
</tableCaption>
<sectionHeader confidence="0.993425" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999941">
We have described a new Japanese dependency
parser that uses decision trees. First, we introduced
the single tree parser to clarify the basic character-
istics of our method. The experimental results show
that it outperforms conventional stochastic parsers
by 4%. Next, the boosting version of our parser was
introduced. The promising results of the boosting
parser can be summarized as follows.
</bodyText>
<listItem confidence="0.9606746">
• The boosting version outperforms the single-
tree counterpart regardless of training data
amount.
• No data over-fitting was seen when the number
of iterations changed.
</listItem>
<bodyText confidence="0.999477875">
We now plan to continue our research in two direc-
tions. One is to make our parser available to a broad
range of researchers and to use their feedback to re-
vise the features for learning. Second, we will apply
our method to other languages, say English. Al-
though we have focused on the Japanese language,
it is straightforward to modify our parser to work
with other languages.
</bodyText>
<figure confidence="0.9826772">
85.5
84
2 835
82.5
5000 10000 15000 2,0000 141,5.1 „55,0X„.„g3,0.0.0„0 35000 40000 45000 50000
</figure>
<figureCaption confidence="0.999367">
Figure 3: Learning Curve of Boosting Parser
</figureCaption>
<sectionHeader confidence="0.993595" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99964206060606">
Eugene Charniak. 1993. Statistical Language Learn-
ing. The MIT Press.
Eugene Charniak. 1997. Statistical Parsing with a
Context-free Grammar and Word Statistics. In
Proc. 15th National Conference on Artificial In-
telligence, pages 598-603.
Michael Collins. 1996. A New Statistical Parser
based on bigram lexical dependencies. In Proc.
3.4th Annual Meeting of Association for Compu-
tational Linguistics, pages 184-191.
Japan Electronic Dictionary Reseaech Institute Ltd.
EDR, 1995. the EDR Electronic Dictionary Tech-
nical Guide.
Yoav Freund and Robert Schapire, 1996. A
decision-theoretic generalization of on-line learn-
ing and an application to boosting.
M. Fujio and Y. Matsumoto. 1997. Japanese de-
pendency structure analysis based on statistics.
In SIGNL NL117-12, pages 83-90. (in Japanese).
David M. Magerman. 1995. Statistical Decision-
Tree Models for Parsing. In Proc.33rd Annual
Meeting of Association for Computational Lin-
guistics, pages 276-283.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Compu-
tational Linguistics, 19(2):313-330, June.
Y. Matsumoto et al. 1996. Japanese Morphological
Analyzer Chasen2.0 User&apos;s Manual.
NLRI. 1964. Word List by Semantic Principles.
Syuei Syuppan. (in Japanese).
J.Ross Quinlan. 1993. C4.5 Programs for Machine
Learning. Morgan Kaufmann Publishers.
</reference>
<page confidence="0.997707">
511
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999908">Using Decision Trees to Construct a Practical Parser</title>
<author confidence="0.717553">Masahiko Haruno Satoshi Shirait Yoshifumi Ooyamat mharuno</author>
<affiliation confidence="0.973727">ATR Human Information Processing Research Laboratories</affiliation>
<address confidence="0.98726">2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan.</address>
<affiliation confidence="0.991176">tNTT Communication Science Laboratories</affiliation>
<address confidence="0.989509">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan.</address>
<abstract confidence="0.9917912996633">This paper describes novel and practical Japanese parsers that uses decision trees. First, we construct a single decision tree to estimate modification probabilities; how one phrase tends to modify another. Next, we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation. The two constructed parsers are evaluated by using the EDR Japanese annotated corpus. The single-tree method outperforms the conventional Japanese stochastic methods by 4%. Moreover, the boosting version is shown to have significant advantages; 1) better parsing accuracy than its single-tree counterpart for any amount of training data and 2) no over-fitting to data for various iterations. Conventional parsers with practical levels of performance require a number of sophisticated rules that have to be hand-crafted by human linguists. It is time-consuming and cumbersome to maintain these rules for two reasons. • The rules are specific to the application domain. • Specific rules handling collocation al expressions create side effects. Such rules often deteriorate the overall performance of the parser. The stochastic approach, on the other hand, has the potential to overcome these difficulties. Because it induces stochastic rules to maximize overall performance against training data, it not only adapts to any application domain but also may avoid overfitting to the data. In the late 80s and early 90s, the induction and parameter estimation of probabilistic context free grammars (PCFGs) from corpora were intensively studied. Because these grammars comprise only nonterminal and part-of-speech tag symbols, their performances were not enough to be used in practical applications (Charniak, 1993). A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences. SPATTER (Magerman, 1995) augmented the pure PCFG by introducing a number of lexical attributes. The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm (Quinlan, 1993). The SPATTER parser attained 87% accuracy and first made stochastic parsers a practical choice. The other type of highprecision parser, which is based on dependency analysis was introduced by Collins (Collins, 1996). Dependency analysis first segments a sentence into syntactically meaningful sequences of words and then considers the modification of each segment. Collins&apos; parser computes the likelihood that each segment modifies the other (2 term relation) by using large corpora. These modification probabilities are conditioned by head words of two segments, distance between the two segments and other syntactic features. Although these two parsers have shown similar performance, the keys of their success are slightly different. SPATTER parser performance greatly depends on the feature selection ability of the decision tree algorithm rather than its linguistic representation. On the other hand, dependency analysis plays an essential role in Collins&apos; parser for efficiently extracting information from corpora. In this paper, we describe practical Japanese dependency parsers that uses decision trees. In the Japanese language, dependency analysis has been shown to be powerful because segment (bunsetsu) order in a sentence is relatively free compared to European languages. Japanese dependency parsers generally proceed in three steps. 1. Segment a sentence into a sequence of bunsetsu. 2. Prepare a modification matrix, each value of which represents how one bunsetsu is likely to modify another. 3. Find optimal modifications in a sentence by a dynamic programming technique. The most difficult part is the second; how to construct a sophisticated modification matrix. With conventional Japanese parsers, the linguist must classify the .bunsetsu and select appropriate features to compute modification values. The parsers thus suffer from application domain diversity and the side effects of specific rules. 505 Stochastic dependency parsers like Collins&apos;, on the other hand, define a set of attributes for conditioning the modification probabilities. The parsers consider all of the attributes regardless of bunsetsu type. These methods can encompass only a small number of features if the probabilities are to be precisely evaluated from finite number of data. Our decision tree method constructs a more sophisticated modification matrix. It automatically selects a sufficient number of significant attributes according to bunsetsu type. We can use arbitrary numbers of the attributes which potentially increase parsing accuracy. Natural languages are full of exceptional and collocational expressions. It is difficult for machine learning algorithms, as well as human linguists, to judge whether a specific rule is relevant in terms of overall performance. To tackle this problem, we test the mixture of sequentially generated decision trees. Specifically, we use the Ada-Boost algorithm (Freund and Schapire, 1996) which iteratively performs procedures: a decision tree based the current data distribution and the distribution by focusing on data that are not well predicted by the constructed tree. The final modification probabilities are computed by mixing all the decision trees according to their performance. The sequential decision trees gradually change from broad coverage to specific exceptional trees that cannot. be captured by a single general tree. In other words, the method incorporates not only general expressions but also infrequent specific ones. The rest of the paper is constructed as follows. Section 2 summarizes dependency analysis for the Japanese language. Section 3 explains our decision tree models that compute modification probabilities. Section 4 then presents experimental results obtained by using EDR Japanese annotated corpora. Finally, section 5 concludes the paper. 2 Dependency Analysis in Japanese Language This section overviews dependency analysis in the Japanese language. The parser generally performs the following three steps. 1. Segment a sentence into a sequence of bunsetsu. 2. Prepare modification matrix each value of which represents how one bunsetsu is likely to modify the other. 3. Find optimal modifications in a sentence by a dynamic programming technique. Because there are no explicit delimiters between words in Japanese, input sentences are first word segmented, part-of-speech tagged, and then chunked into a sequence of bunsetsus. The first step yields, for the following example, the sequence of bunsetsu displayed below. The parenthesis in the Japanese expressions represent the internal structures of the bunsetsu (word segmentations). Example: H196D9 r_idfifi6D-1-e VI7 4 t yeittgata-ni kinjo-no yesterday-NO evening-NI neighbor-NO kodomo-ga wain-wo nomu-Fta children-GA wine-wo drink-4-PAST The second step of parsing is to construct a modification matrix whose values represent the likelihood that one bunsetsu modifies another in a sentence. In the Japanese language, we usually make two assumptions: 1. Every bunsetsu except the last one modifies only one posterior bunsetsu. 2. No modification crosses to other modifications in a sentence. Table 1 illustrates a modification matrix for the example sentence. In the matrix, columns and rows represent anterior and posterior bunsetsus, respec- For example, the first bunsetsu the second score 0.70 and third score 0.07. The aim of this paper is to generate a modification matrix by using decision trees. y•vgaia•ni kinjo-no kodorne-ga kino•-no 0.70 0.07 0.10 0.10 0.03 yurgata-no 0.10 0.10 0.10 0.70 kinyo-no 0.70 0.20 0.10 dam a. 1.00 nome•ta 0.05 0.95 Table 1: Modification Matrix for Sample Sentence The final step of parsing optimizes the entire dependency structure by using the values in the modification matrix. Before going into our model, we introduce the nothat will be used in the model. Let input sentence. a bunsetsu set (l&lt; f1&gt;,- • , &lt; f„, &gt;)) which and the ith bunsetsu and its features, We define be a modification set; • • • ,mod(m — in which indicates the number of busetsu modified by the ith bunsetsu. Because of the first assumption, the length of always 1. Using these notations, the result the third step for the example can be given as = {2, 6, 4, 6, 6} as displayed in Figure 1. 3 Decision Trees for Dependency Analysis Model and Decision Trees The stochastic dependency parser assigns the most modification set to a sentence 506 • • fr.) I2 6 kiou-no pug;a-ni kinjo-no kodomo-ga wain-wo nomu-ta Li Li Figure 1: Modification Set for Sample Sentence terms of the training data distribution. = argmaxDP(DIB) By assuming the independence of modificabe transformed as follows. , • • ,fm) the probability that pair of bunsetsu b and a modification relation. Note that each modification is constrained by features{ „ • , f,}in a sentence despite of the assumption of independence.We use decision trees to dynamically select appropriate features for each of bunsetsus from If , P(DIB) = bi , f,,- • • , Let us first consider the single tree case. The training data for the decision tree comprise any unordered combination of two bunsetsu in a sentence. Features used for learning are the linguistic information associated with the two bunsetsu. The next section will explain these features in detail. The class set for learning has binary values yes and no which delineate whether the data (the two bunstsu) has a modification relation or not. In this setting, the decision tree algorithm automatically and consecutively selects the significant features for discriminating modify/non-modify relations. We slightly changed C4.5 (Quinlan, 1993) programs to be able to extract class frequencies at every node in the decision tree beour task is regression rather than classification. By using the class distribution, we the probability DT(yesibi, , 1, , • • , fm) which is the Laplace estimate of empirical likelithat bi modifies the constructed decitree that it is necessary to norf „ • • ,fm) approximate „ • • , considering all canposterior to , f , • • • ,f,,) computed using a heulistic rule (1). It is of course reasonable to normalize class frequencies instead of probability • • • , f .) • tion (1) tends to emphasize long distance dependencies more than is true for frequency-based normalization. P(yesibi,bi, f , • , f,„„,) • • • f Let us extend the above to use a set of decision trees. As briefly mentioned in Section 1, a number of infrequent and exceptional expressions appear in any natural language phenomena; they deteriorate the overall performance of application systems. It is also difficult for automated learning systems to detect and handle these expressions because exceptional expressions are placed in the same class as frequent ones. To tackle this difficulty, we generate a set of decision trees by adaboost (Freund and Schapire, 1996) algorithm illustrated in Table 2. The algorithm first sets the weights to 1 for all examples (2 in Table 2) and repeats the following two (3 in Table 2). 1. A decision tree is constructed by using the current weight vector ((a) in Table 2) 2. Example data are then parsed by using the tree and the weights of correctly handled examples are reduced ((b),(c) in Table 2) Input: of &lt; e., w. &gt;, &lt; which represent an example and its weight, respectively. weight vector w, =1 for i 1, , N fort = Call providing it with the weight vector and modification probability (b) Let Error be a set of examples that are not by the pseudo error rate of &gt; 1, then loop = For examples correctly predicted by update the weights vector to be tvi = a final set: Table 2: Combining Decision Trees by Ada-boost Algorithm final probability set then computed mixing according to their perfor- (4 in Table 2). Using instead of , f1, • • • , f,„), equation (1) generates a boosting version of the dependency parser. 3.2 Linguistic Feature Types Used for Learning This section explains the concrete feature setting we used for learning. The feature set mainly focuses on 507 No. Two Bunsetsu No. Others 1 lexical information of bead word 6 distance between two bunsetsu 2 of word 7 particle &apos;wa&apos; between two bunsetsu 3 type of bunsetsu 8 punctuation between two bunsetsu 4 punctuation 5 _ parentheses Table 3: Linguistic Feature Types Used for Learning IFe•tu re Type Value• 2 , :LI; , F, , 01, 11,1 If 41111+,14, kol, F414, ER014, 11,314, ItbMtlfilliAt, ta11414, Ifillfil11:1, 431/0111, t1;14451f, 1,11/114, 141:114, ;1341:1.</abstract>
<note confidence="0.722438352941176">3 Ite.,1110, Ifilei,, Le,, Lt., Lt., Itcht, -e, ..!-ic.:, -F, tr$,, t.r.t•&apos;, t., i, Cr ) , tekT,::, 0,0A, If.5. 4 , -s, 1&apos;1 It::t, 6, ,.), 600), t., X. 1) , 1 :. , Z., &apos;Cr 4&amp;quot; MMIT,I, A, iBf1.64, 1bff1411/10i4fEdA1, ffilit14, Shti, IA, 414, rt14, 41,t111, *IS, 4., S14, DM, Ral9114, OA SIC *!..i, r,14/Warfur714, t1111ttRIASP, air, it.1144, T.:11AM&apos;F214, 4 5 &apos; . ■ . F. 1. 1, ,. 1. 6 A(0), B(1-4), C(&gt;5) 7 a 0.1 Table 4: Values for Each Feature Type Figure 2: Learning Curve of Single-Tree Parser</note>
<abstract confidence="0.990795895833333">the two bunsetsu constituting each data. The class set consists of binary values which delineate whether a sample (the two bunsetsu) have a modification relation or not. We use 13 features for the task, 10 directly from the 2 bunsetsu under consideration and 3 for other bunsetu information as summarized in Table 3. Each bunsetsu (anterior and posterior) has the 5 features: No.1 to No.5 in Table 3. Features No.6 to No.8 are related to bunsetsu pairs. Both No.1 and No.2 concern the head word of the bunsetsu. No.1 takes values of frequent words or thesaurus categories (NLRI, 1964). No.2, on the other hand, takes values of part-of-speech tags. No.3 deals with bunsetsu types which consist of functional word chunks or the part-of-speech tags that dominate the bunsetsu&apos;s syntactic characteristics. No.4 and No.5 are binary features and correspond to punctuation and parentheses, respectively. No.6 represents how many bunsetsus exist between the two bunsetsus. Possible values are A(0), B(0-4) and C(&gt;5). No.7 deals with the post-positional particle &apos;wa&apos; which greatly influences the long distance dependency of subject-verb modifications. Finally, No.8 addresses the punctuation between the two bunsetsu. The detailed values of each feature type are summarized in Table 4. Results We evaluated the proposed parser using the EDR Japanese annotated corpus (EDR, 1995). The experiment consisted of two parts. One evaluated the single-tree parser and the other the boosting counterpart. In the rest of this section, parsing accuracy refers only to precision; how many of the system&apos;s output are correct in terms of the annotated corpus. We do not show recall because we assume every bunsetsu modifies only one posterior bunsetsu. The features used for learning were non head-word features, (i.e., type 2 to 8 in Table 3). Section 4.1.4 investigates lexical information of head words such as frequent words and thesaurus categories. Before going into details of the experimental results, we summarize here how training and test data were selected. 1. After all sentences in the EDR corpus were word-segmented and part-of-speech tagged (Matsumoto and others, 1996), they were then chunked into a sequence of bunsetsu. 2. All bunsetsu pairs were compared with EDR bracketing annotation (correct segmentations</abstract>
<address confidence="0.6096425">5000 10000 ¶5000 20000 25000 30000 35000 40000 45000 50000</address>
<note confidence="0.812834181818182">Number 01 Tnurong Da. 84 83 825 508 Confidence Level 25% 50% 75% 95% Parsing Accuracy 82.01% 83.43% 83.52% 83.35% Table 5: Number of Training Sentences v.s. Parsing Accuracy Number of Training Sentences 3000 6000 10000 20000 30000 50000 Parsing Accuracy 82.07% 82.70% 83.52% 84.07% 84.27% 84.33% Table 6: Pruning Confidence Level v.s.Parsing Accuracy</note>
<abstract confidence="0.965220051136363">and modifications). If a sentence contained a pair inconsistent with the EDR annotation, the sentence was removed from the data. 3. All data examined (total number of sentences:207802, total number of bunsetsu:1790920) were divided into 20 files. The training data were same number of first sentences of the 20 files according to the training data size. Test data (10000 sentences) were the 2501th to 3000th sentences of each file. Single Experiments single tree experiments, we evaluated the following 4 properties of the new dependency parser. • Tree pruning and parsing accuracy • Number of training data and parsing accuracy • Significance of features other than Head-word Lexical Information • Significance of Head-word Lexical Information and Parsing Accuracy Table 5 summarizes the parsing accuracy with various confidence levels of pruning. The number of training sentences was 10000. In C4.5 programs, a larger value of confidence means weaker pruning and 25% is commonly used in various domains (Quinlan, 1993). Our experimental results show that 75% pruning attains the best performance, i.e. weaker pruning than usual. In the remaining single tree experiments, we used the 75% confidence level. Although strong pruning treats infrequent data as noise, parsing involves many exceptional and infrequent modifications as mentioned before. Our result means that only information included in small numbers of samples are useful for disambiguating the syntactic structure of sentences. 4.1.2 The amount of Training Data and Parsing Accuracy Table 6 and Figure 2 show how the number of training sentences influences parsing accuracy for the same 10000 test sentences. They illustrate the following two characteristics of the learning curve. 1. The parsing accuracy rapidly rises up to 30000 sentences and converges at around 50000 sentences. 2. The maximum parsing accuracy is 84.33% at 50000 training sentences. We will discuss the maximum accuracy of 84.3&apos;3%. Compared to recent stochastic English parsers that yield 86 to 87% accuracy (Collins, 1996; Mager- 1995), unsatisfactory at the first glance. The main reason behind this lies in the difference between the two corpora used: Penn Treebank (Marcus et al., 1993) and EDR corpus (EDR, 1995). Penn Treebank(Niarcus et al., 1993) was also used to induce part-of-speech (POS) taggers because the corpus contains very precise and detailed POS markers as well as bracket annotations. In addition, English parsers incorporate the syntactic tags that are contained in the corpus. The EDR corpus, on the other hand, contains only coarse POS tags. We used another Japanese POS tagger (Matsumoto and others, 1996) to make use of well-grained information for disambiguating syntactic structures. Only the bracket information in the EDR corpus was considered. We conjecture that the difference between the parsing accuracies is due to the difference of the corpus information. (Fujio and Matsumoto, 1997) constructed an EDR-based dependency parser by using a similar method to Collins&apos; (Collins, 1996). The parser attained 80.48% accuracy. Although thier training and test sentences are not exactly same as ours, the result seems to support our conjecture on the data difference between EDR and Penn Treebank. Significance of Non Features We will now summarize the significance of each non head-word feature introduced in Section 3. The influence of the lexical information of head words will be discussed in the next section. Table 7 illustrates how the parsing accuracy is reduced when each feature is removed. The number of training sentences was 10000. In the table, ant and post represent the anterior and the posterior bunsetsu, respectively. 7 clearly demonstrates that the most signifi- 509 Feature Accuracy Decrease Feature Accuracy Decrease ant POS of head -0.07% post punctuation +1.62% ant bunsetsu type +9.34% post parentheses ±0.00% ant punctuation +1.15% distance between two bunsetsus +5.21% ant parentheses ±0.00% punctuation between two bunsetsus +0.01% post POS of head +2.13% &apos;wa&apos; between two bunsetsus +1.79% post bunsetsu type +0.52% Table 7: Decrease of Parsing Accuracy When Each Attribute Removed Table 8: Head Word Information v.s. Parsing Accuracy Head Word Information 100 words 200 words Level 1 Level 2 83.34% 82.68% 82.51% 81.67% Parsing Accuracy cant features are anterior bunsetsu type and distance between the two bunsetsu. This result may partially support an often used heuristic; bunsetsu modification should be as short range as possible, provided the modification is syntactically possible. In particular, we need to concentrate on the types of bunsetsu to attain a higher level of accuracy. Most features contribute, to some extent, to the parsing performance. In our experiment, information on parentheses has no effect on the performance. The reason may be that. EDR contains only a small number of parentheses. One exception in our features is anterior POS of head. We currently hypothesize that this drop of accuracy arises from two reasons. • In many cases, the POS of bead word can be determined from bunsetsu type. • Our POS tagger sometimes assigns verbs for verb-derived nouns. 4.1.4 Significance of Head-words Lexical Information We focused on the head-word feature by testing the following 4 lexical sources. The first and the second are the 100 and 200 most frequent words, respectively. The third and the fourth are derived from a broadly used Japanese thesaurus, Word List, by Semantic Principles (NLRI, 1964). Level 1 and Level 2 classify words into 15 and 67 categories, respectively. 1. 100 most, Frequent words 2. 200 most Frequent words 3. Word List Level 1 4. Word List Level 2 Table 8 displays the parsing accuracy when each word information was used in addition previous features. The number of training sentences was 10000. In all cases, the performance was worse than 83.52% which was attained without head word lexical information. More surprisingly, more head word information yielded worse performance. From this result, it may be safely said, at least for the Japanese language; that, we cannot expect lexical information to always improve the performance. Further investigation of other thesaurus and clustering (Charniak, 1997) techniques is necessary to fully understand the influence of lexical information. 4.2 Boosting Experiments This section reports experimental results on the boosting version of our parser. In all experiments, pruning confidence levels were set to 55%. Table 9 and Figure 3 show the parsing accuracy when the number of training examples was increased. Because the number of iterations in each data set changed between 5 and 8, we will show the accuracy by combining the first 5 decision trees. In Figure 3, the dotted line plots the learning of the single tree case (identical to Figure 2) for reader&apos;s convenience. The characteristics of the boosting version can be summarized as follows compared to the single tree version. • The learning curve rises more rapidly with a small number of examples. It is surprising that the boosting version with 10000 sentences performs better than the single tree version with 50000 sentences. • The boosting version significantly outperforms the single tree counterpart for any number of sentences although they use the same features for learning. Next, we discuss how the number of iterations influences the parsing accuracy. Table 10 shows the parsing accuracy for various iteration numbers when 50000 sentences were used as training data. The results have two characteristics. • Parsing accuracy rose up rapidly at the second iteration. • No over-fitting to data was seen although the performance of each generated tree fell around 30% at the final stage of iteration.</abstract>
<note confidence="0.820500714285714">510 Number of Training Sentences 3000 6000 10000 20000 30000 50000 Parsing Accuracy 83.10% 84.03% 84.44% 84.74% 84.91% 85.03% Table 9: Number of Training Sentences v.s. Parsing Accuracy Number of Iteration 1 2 3 4 5 6 Parsing Accuracy 84.32% 84.93% 84.89% 84.86% 85.03% 85.01% Table 10: Number of Iteration v.s. Parsing Accuracy</note>
<abstract confidence="0.982337">5 Conclusion We have described a new Japanese dependency parser that uses decision trees. First, we introduced the single tree parser to clarify the basic characteristics of our method. The experimental results show that it outperforms conventional stochastic parsers by 4%. Next, the boosting version of our parser was introduced. The promising results of the boosting parser can be summarized as follows. • The boosting version outperforms the singletree counterpart regardless of training data amount. • No data over-fitting was seen when the number of iterations changed. We now plan to continue our research in two directions. One is to make our parser available to a broad range of researchers and to use their feedback to revise the features for learning. Second, we will apply our method to other languages, say English. Although we have focused on the Japanese language, it is straightforward to modify our parser to work with other languages.</abstract>
<address confidence="0.651548">85.5 84 2835 82.5</address>
<phone confidence="0.57984">10000 15000 2,0000 141,5.135000 40000 45000 50000</phone>
<note confidence="0.840960583333333">Figure 3: Learning Curve of Boosting Parser References Charniak. 1993. Language Learn- MIT Press. Eugene Charniak. 1997. Statistical Parsing with a Context-free Grammar and Word Statistics. In Proc. 15th National Conference on Artificial In- 598-603. Michael Collins. 1996. A New Statistical Parser on bigram lexical dependencies. In 3.4th Annual Meeting of Association for Compu- Linguistics, 184-191.</note>
<affiliation confidence="0.522357">Japan Electronic Dictionary Reseaech Institute Ltd.</affiliation>
<address confidence="0.440279">1995. EDR Electronic Dictionary Tech-</address>
<abstract confidence="0.896009">nical Guide. Yoav Freund and Robert Schapire, 1996. A decision-theoretic generalization of on-line learning and an application to boosting. M. Fujio and Y. Matsumoto. 1997. Japanese dependency structure analysis based on statistics.</abstract>
<note confidence="0.7629028125">NL117-12, 83-90. (in Japanese). David M. Magerman. 1995. Statistical Decision- Models for Parsing. In Annual Meeting of Association for Computational Lin- 276-283. Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated of English: The Penn Treebank. Compu- Linguistics, June. Y. Matsumoto et al. 1996. Japanese Morphological Analyzer Chasen2.0 User&apos;s Manual. 1964. List by Semantic Principles. Syuei Syuppan. (in Japanese). Quinlan. 1993. Programs for Machine Kaufmann Publishers. 511</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2173" citStr="Charniak, 1993" startWordPosition="305" endWordPosition="306"> of the parser. The stochastic approach, on the other hand, has the potential to overcome these difficulties. Because it induces stochastic rules to maximize overall performance against training data, it not only adapts to any application domain but also may avoid overfitting to the data. In the late 80s and early 90s, the induction and parameter estimation of probabilistic context free grammars (PCFGs) from corpora were intensively studied. Because these grammars comprise only nonterminal and part-of-speech tag symbols, their performances were not enough to be used in practical applications (Charniak, 1993). A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences. SPATTER (Magerman, 1995) augmented the pure PCFG by introducing a number of lexical attributes. The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm (Quinlan, 1993). The SPATTER parser attained 87% accuracy and first made stochastic parsers a practical choice. The other type of highprecision parser, which is based on dependency analysis was introduced by Collins (Collins,</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>Eugene Charniak. 1993. Statistical Language Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Parsing with a Context-free Grammar and Word Statistics.</title>
<date>1997</date>
<booktitle>In Proc. 15th National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="24064" citStr="Charniak, 1997" startWordPosition="3850" endWordPosition="3851">s 3. Word List Level 1 4. Word List Level 2 Table 8 displays the parsing accuracy when each head word information was used in addition to the previous features. The number of training sentences was 10000. In all cases, the performance was worse than 83.52% which was attained without head word lexical information. More surprisingly, more head word information yielded worse performance. From this result, it may be safely said, at least for the Japanese language; that, we cannot expect lexical information to always improve the performance. Further investigation of other thesaurus and clustering (Charniak, 1997) techniques is necessary to fully understand the influence of lexical information. 4.2 Boosting Experiments This section reports experimental results on the boosting version of our parser. In all experiments, pruning confidence levels were set to 55%. Table 9 and Figure 3 show the parsing accuracy when the number of training examples was increased. Because the number of iterations in each data set changed between 5 and 8, we will show the accuracy by combining the first 5 decision trees. In Figure 3, the dotted line plots the learning of the single tree case (identical to Figure 2) for reader&apos;</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical Parsing with a Context-free Grammar and Word Statistics. In Proc. 15th National Conference on Artificial Intelligence, pages 598-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A New Statistical Parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. 3.4th Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="2779" citStr="Collins, 1996" startWordPosition="397" endWordPosition="398">k, 1993). A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences. SPATTER (Magerman, 1995) augmented the pure PCFG by introducing a number of lexical attributes. The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm (Quinlan, 1993). The SPATTER parser attained 87% accuracy and first made stochastic parsers a practical choice. The other type of highprecision parser, which is based on dependency analysis was introduced by Collins (Collins, 1996). Dependency analysis first segments a sentence into syntactically meaningful sequences of words and then considers the modification of each segment. Collins&apos; parser computes the likelihood that each segment modifies the other (2 term relation) by using large corpora. These modification probabilities are conditioned by head words of two segments, distance between the two segments and other syntactic features. Although these two parsers have shown similar performance, the keys of their success are slightly different. SPATTER parser performance greatly depends on the feature selection ability of</context>
<context position="19826" citStr="Collins, 1996" startWordPosition="3168" endWordPosition="3169">biguating the syntactic structure of sentences. 4.1.2 The amount of Training Data and Parsing Accuracy Table 6 and Figure 2 show how the number of training sentences influences parsing accuracy for the same 10000 test sentences. They illustrate the following two characteristics of the learning curve. 1. The parsing accuracy rapidly rises up to 30000 sentences and converges at around 50000 sentences. 2. The maximum parsing accuracy is 84.33% at 50000 training sentences. We will discuss the maximum accuracy of 84.3&apos;3%. Compared to recent stochastic English parsers that yield 86 to 87% accuracy (Collins, 1996; Magerman, 1995), 84.33% seems unsatisfactory at the first glance. The main reason behind this lies in the difference between the two corpora used: Penn Treebank (Marcus et al., 1993) and EDR corpus (EDR, 1995). Penn Treebank(Niarcus et al., 1993) was also used to induce part-of-speech (POS) taggers because the corpus contains very precise and detailed POS markers as well as bracket annotations. In addition, English parsers incorporate the syntactic tags that are contained in the corpus. The EDR corpus, on the other hand, contains only coarse POS tags. We used another Japanese POS tagger (Mat</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A New Statistical Parser based on bigram lexical dependencies. In Proc. 3.4th Annual Meeting of Association for Computational Linguistics, pages 184-191.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Japan Electronic Dictionary Reseaech Institute Ltd. EDR,</booktitle>
<marker>1995</marker>
<rawString>Japan Electronic Dictionary Reseaech Institute Ltd. EDR, 1995. the EDR Electronic Dictionary Technical Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and an application to boosting.</title>
<date>1996</date>
<contexts>
<context position="5497" citStr="Freund and Schapire, 1996" startWordPosition="806" endWordPosition="810">tructs a more sophisticated modification matrix. It automatically selects a sufficient number of significant attributes according to bunsetsu type. We can use arbitrary numbers of the attributes which potentially increase parsing accuracy. Natural languages are full of exceptional and collocational expressions. It is difficult for machine learning algorithms, as well as human linguists, to judge whether a specific rule is relevant in terms of overall performance. To tackle this problem, we test the mixture of sequentially generated decision trees. Specifically, we use the Ada-Boost algorithm (Freund and Schapire, 1996) which iteratively performs two procedures: 1. construct a decision tree based on the current data distribution and 2. updating the distribution by focusing on data that are not well predicted by the constructed tree. The final modification probabilities are computed by mixing all the decision trees according to their performance. The sequential decision trees gradually change from broad coverage to specific exceptional trees that cannot. be captured by a single general tree. In other words, the method incorporates not only general expressions but also infrequent specific ones. The rest of the</context>
<context position="11965" citStr="Freund and Schapire, 1996" startWordPosition="1877" endWordPosition="1880"> is true for frequency-based normalization. P(yesibi,bi, f , • , f,„„,) E k&gt;imPDT(yeslbi,bi, fi, • • • f Let us extend the above to use a set of decision trees. As briefly mentioned in Section 1, a number of infrequent and exceptional expressions appear in any natural language phenomena; they deteriorate the overall performance of application systems. It is also difficult for automated learning systems to detect and handle these expressions because exceptional expressions are placed in the same class as frequent ones. To tackle this difficulty, we generate a set of decision trees by adaboost (Freund and Schapire, 1996) algorithm illustrated in Table 2. The algorithm first sets the weights to 1 for all examples (2 in Table 2) and repeats the following two procedures T times (3 in Table 2). 1. A decision tree is constructed by using the current weight vector ((a) in Table 2) 2. Example data are then parsed by using the tree and the weights of correctly handled examples are reduced ((b),(c) in Table 2) 1. Input: sequence of N examples &lt; e., w. &gt;, &lt; eN,UIN &gt; in which ei and iv, represent an example and its weight, respectively. 2. Initialize the weight vector w, =1 for i 1, , N 3. Do fort = (a) Call C4.5 provid</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert Schapire, 1996. A decision-theoretic generalization of on-line learning and an application to boosting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fujio</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on statistics.</title>
<date>1997</date>
<booktitle>In SIGNL NL117-12,</booktitle>
<pages>83--90</pages>
<note>(in Japanese).</note>
<contexts>
<context position="20739" citStr="Fujio and Matsumoto, 1997" startWordPosition="3312" endWordPosition="3315"> (POS) taggers because the corpus contains very precise and detailed POS markers as well as bracket annotations. In addition, English parsers incorporate the syntactic tags that are contained in the corpus. The EDR corpus, on the other hand, contains only coarse POS tags. We used another Japanese POS tagger (Matsumoto and others, 1996) to make use of well-grained information for disambiguating syntactic structures. Only the bracket information in the EDR corpus was considered. We conjecture that the difference between the parsing accuracies is due to the difference of the corpus information. (Fujio and Matsumoto, 1997) constructed an EDR-based dependency parser by using a similar method to Collins&apos; (Collins, 1996). The parser attained 80.48% accuracy. Although thier training and test sentences are not exactly same as ours, the result seems to support our conjecture on the data difference between EDR and Penn Treebank. 4.1.3 Significance of Non Head-Word Features We will now summarize the significance of each non head-word feature introduced in Section 3. The influence of the lexical information of head words will be discussed in the next section. Table 7 illustrates how the parsing accuracy is reduced when </context>
</contexts>
<marker>Fujio, Matsumoto, 1997</marker>
<rawString>M. Fujio and Y. Matsumoto. 1997. Japanese dependency structure analysis based on statistics. In SIGNL NL117-12, pages 83-90. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical DecisionTree Models for Parsing. In</title>
<date>1995</date>
<booktitle>Proc.33rd Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="2360" citStr="Magerman, 1995" startWordPosition="332" endWordPosition="333">raining data, it not only adapts to any application domain but also may avoid overfitting to the data. In the late 80s and early 90s, the induction and parameter estimation of probabilistic context free grammars (PCFGs) from corpora were intensively studied. Because these grammars comprise only nonterminal and part-of-speech tag symbols, their performances were not enough to be used in practical applications (Charniak, 1993). A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences. SPATTER (Magerman, 1995) augmented the pure PCFG by introducing a number of lexical attributes. The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm (Quinlan, 1993). The SPATTER parser attained 87% accuracy and first made stochastic parsers a practical choice. The other type of highprecision parser, which is based on dependency analysis was introduced by Collins (Collins, 1996). Dependency analysis first segments a sentence into syntactically meaningful sequences of words and then considers the modification of each segment. Collins&apos; parser computes the li</context>
<context position="19843" citStr="Magerman, 1995" startWordPosition="3170" endWordPosition="3172">yntactic structure of sentences. 4.1.2 The amount of Training Data and Parsing Accuracy Table 6 and Figure 2 show how the number of training sentences influences parsing accuracy for the same 10000 test sentences. They illustrate the following two characteristics of the learning curve. 1. The parsing accuracy rapidly rises up to 30000 sentences and converges at around 50000 sentences. 2. The maximum parsing accuracy is 84.33% at 50000 training sentences. We will discuss the maximum accuracy of 84.3&apos;3%. Compared to recent stochastic English parsers that yield 86 to 87% accuracy (Collins, 1996; Magerman, 1995), 84.33% seems unsatisfactory at the first glance. The main reason behind this lies in the difference between the two corpora used: Penn Treebank (Marcus et al., 1993) and EDR corpus (EDR, 1995). Penn Treebank(Niarcus et al., 1993) was also used to induce part-of-speech (POS) taggers because the corpus contains very precise and detailed POS markers as well as bracket annotations. In addition, English parsers incorporate the syntactic tags that are contained in the corpus. The EDR corpus, on the other hand, contains only coarse POS tags. We used another Japanese POS tagger (Matsumoto and others</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical DecisionTree Models for Parsing. In Proc.33rd Annual Meeting of Association for Computational Linguistics, pages 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="20010" citStr="Marcus et al., 1993" startWordPosition="3198" endWordPosition="3201">parsing accuracy for the same 10000 test sentences. They illustrate the following two characteristics of the learning curve. 1. The parsing accuracy rapidly rises up to 30000 sentences and converges at around 50000 sentences. 2. The maximum parsing accuracy is 84.33% at 50000 training sentences. We will discuss the maximum accuracy of 84.3&apos;3%. Compared to recent stochastic English parsers that yield 86 to 87% accuracy (Collins, 1996; Magerman, 1995), 84.33% seems unsatisfactory at the first glance. The main reason behind this lies in the difference between the two corpora used: Penn Treebank (Marcus et al., 1993) and EDR corpus (EDR, 1995). Penn Treebank(Niarcus et al., 1993) was also used to induce part-of-speech (POS) taggers because the corpus contains very precise and detailed POS markers as well as bracket annotations. In addition, English parsers incorporate the syntactic tags that are contained in the corpus. The EDR corpus, on the other hand, contains only coarse POS tags. We used another Japanese POS tagger (Matsumoto and others, 1996) to make use of well-grained information for disambiguating syntactic structures. Only the bracket information in the EDR corpus was considered. We conjecture t</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
</authors>
<title>Word List by Semantic Principles. Syuei Syuppan.</title>
<date>1996</date>
<booktitle>Japanese Morphological Analyzer Chasen2.0 User&apos;s Manual. NLRI.</booktitle>
<note>(in Japanese).</note>
<marker>Matsumoto, 1996</marker>
<rawString>Y. Matsumoto et al. 1996. Japanese Morphological Analyzer Chasen2.0 User&apos;s Manual. NLRI. 1964. Word List by Semantic Principles. Syuei Syuppan. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<date>1993</date>
<booktitle>C4.5 Programs for Machine Learning.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="2563" citStr="Quinlan, 1993" startWordPosition="363" endWordPosition="364">rammars (PCFGs) from corpora were intensively studied. Because these grammars comprise only nonterminal and part-of-speech tag symbols, their performances were not enough to be used in practical applications (Charniak, 1993). A broader range of information, in particular lexical information, was found to be essential in disambiguating the syntactic structures of real-world sentences. SPATTER (Magerman, 1995) augmented the pure PCFG by introducing a number of lexical attributes. The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm (Quinlan, 1993). The SPATTER parser attained 87% accuracy and first made stochastic parsers a practical choice. The other type of highprecision parser, which is based on dependency analysis was introduced by Collins (Collins, 1996). Dependency analysis first segments a sentence into syntactically meaningful sequences of words and then considers the modification of each segment. Collins&apos; parser computes the likelihood that each segment modifies the other (2 term relation) by using large corpora. These modification probabilities are conditioned by head words of two segments, distance between the two segments a</context>
<context position="10592" citStr="Quinlan, 1993" startWordPosition="1628" endWordPosition="1629"> single tree case. The training data for the decision tree comprise any unordered combination of two bunsetsu in a sentence. Features used for learning are the linguistic information associated with the two bunsetsu. The next section will explain these features in detail. The class set for learning has binary values yes and no which delineate whether the data (the two bunstsu) has a modification relation or not. In this setting, the decision tree algorithm automatically and consecutively selects the significant features for discriminating modify/non-modify relations. We slightly changed C4.5 (Quinlan, 1993) programs to be able to extract class frequencies at every node in the decision tree because our task is regression rather than classification. By using the class distribution, we compute the probability P DT(yesibi, bi , 1, , • • , fm) which is the Laplace estimate of empirical likelihood that bi modifies bi in the constructed decision tree DT. Note that it is necessary to normalize PDT(yesibi,bi , f „ • • ,fm) to approximate f „ • • , fm). By considering all candidates posterior to b, P(yesibi,bi , f , • • • ,f,,) is computed using a heulistic rule (1). It is of course reasonable to normaliz</context>
<context position="18789" citStr="Quinlan, 1993" startWordPosition="3005" endWordPosition="3006">Single Tree Experiments In the single tree experiments, we evaluated the following 4 properties of the new dependency parser. • Tree pruning and parsing accuracy • Number of training data and parsing accuracy • Significance of features other than Head-word Lexical Information • Significance of Head-word Lexical Information 4.1.1 Pruning and Parsing Accuracy Table 5 summarizes the parsing accuracy with various confidence levels of pruning. The number of training sentences was 10000. In C4.5 programs, a larger value of confidence means weaker pruning and 25% is commonly used in various domains (Quinlan, 1993). Our experimental results show that 75% pruning attains the best performance, i.e. weaker pruning than usual. In the remaining single tree experiments, we used the 75% confidence level. Although strong pruning treats infrequent data as noise, parsing involves many exceptional and infrequent modifications as mentioned before. Our result means that only information included in small numbers of samples are useful for disambiguating the syntactic structure of sentences. 4.1.2 The amount of Training Data and Parsing Accuracy Table 6 and Figure 2 show how the number of training sentences influences</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.Ross Quinlan. 1993. C4.5 Programs for Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>