<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.992777">
Taxonomy Learning using Term Specificity and Similarity
</title>
<author confidence="0.966371">
Pum-Mo Ryu
</author>
<affiliation confidence="0.778124333333333">
Computer Science Division, KAIST
KORTERM/BOLA
Korea
</affiliation>
<email confidence="0.994751">
pmryu@world.kaist.ac.kr
</email>
<author confidence="0.870471">
Key-Sun Choi
</author>
<affiliation confidence="0.706116666666667">
Computer Science Division, KAIST
KORTERM/BOLA
Korea
</affiliation>
<email confidence="0.992476">
kschoi@cs.kaist.ac.kr
</email>
<sectionHeader confidence="0.995472" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917764705882">
Learning taxonomy for technical terms is
difficult and tedious task, especially
when new terms should be included. The
goal of this paper is to assign taxonomic
relations among technical terms. We pro-
pose new approach to the problem that
relies on term specificity and similarity
measures. Term specificity and similarity
are necessary conditions for taxonomy
learning, because highly specific terms
tend to locate in deep levels and semanti-
cally similar terms are close to each other
in taxonomy. We analyzed various fea-
tures used in previous researches in view
of term specificity and similarity, and ap-
plied optimal features for term specificity
and similarity to our method.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999489898305085">
Taxonomy is a collection of controlled vocabu-
lary terms organized into a hierarchical structure.
Each term in a taxonomy is one or more parent-
child relationships to other terms in the taxon-
omy. Taxonomies are useful artifacts for orga-
nizing many aspects of knowledge. As compo-
nents of ontologies, taxonomies can provide an
organizational model for a domain (domain on-
tology), or a model suitable for specific tasks
(task ontologies) (Burgun &amp; Bodenreider, 2001).
However their wide usage is still hindered by
time-consuming, cost-ineffective building proc-
esses.
The main paradigms of taxonomy learning are
on the one hand pattern based approaches and on
the other hand distributional hypothesis based
approaches. The former is approaches based on
matching lexico-syntactic patterns which convey
taxonomic relations in a corpus (Hearst, 1992;
Iwanska et al., 2000), and the latter is statistical
approaches based on the distribution of context
in corpus (Cimiano et al., 2005; Yamamoto et al.,
2005; Sanderson &amp; Croft, 1999). The former fea-
tures a high precision and low recall compared to
the latter. The quality of learned relations is
higher than those of statistical approaches, while
the patterns are rarely applied in real corpus. It is
also difficult to improve performance of pattern
based approaches because they are simple and
clear. So, many researches have been focused on
raising precision of statistical approaches.
We introduce new distributional hypothesis
based taxonomy learning method using term
specificity and term similarity. Term specificity
is a measure of information quantity of terms in
given domain. When a term has much domain
information, the term is highly specific to the
domain, and vice versa (Ryu &amp; Choi, 2005). Be-
cause highly specific terms tend to locate in low
level in domain taxonomy, term specificity can
be used as a necessary condition for taxonomy
learning. Term similarity is degree of semantic
overlap among terms. When two terms share
many common characteristics, they are semanti-
cally similar to each other. Term similarity can
be another necessary condition for taxonomy
learning, because semantically similar terms lo-
cate near by in given domain taxonomy. The two
conditions are generally valid for terms in a taxo-
nomic relation, while terms satisfying the condi-
tions do not always have taxonomic relation. So
they are necessary conditions for taxonomy
learning.
Based on these conditions, it is highly prob-
able that term t1 is an ancestor of term t2 in do-
main taxonomy TD, when t1 and t2 are semanti-
cally similar enough and the specificity of t1 is
lower than that of t2 in D as in Figure 1. However,
t1 is not an ancestor of t3 even though the speci-
</bodyText>
<page confidence="0.993767">
41
</page>
<note confidence="0.903327333333333">
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 41–48,
Sydney, July 2006. c�2006 Association for Computational Linguistics
ficity of t1 is lower than that of t3 because t1 is not
similar to t3 on the semantic level.
quantified to a positive real number as shown in
Eq. (1).
</note>
<figure confidence="0.982749888888889">
low
Similarity
Specificity
t1
t2 t3
high
low
Depth
high
</figure>
<figureCaption confidence="0.990099">
Figure 1. Term specificity and term similarity in
a domain taxonomy TD
</figureCaption>
<bodyText confidence="0.999936115384615">
The strength of this method lies in its ability to
adopt different optimal features for term specific-
ity and term similarity. Most of current re-
searches relied on single feature such as adjec-
tives of terms, verb-argument relation, or co-
occurrence ratio in documents according to their
methods. Firstly, we analyze characteristics of
features for taxonomy learning in view of term
specificity and term similarity to show that the
features embed characteristics of specificity and
similarity, and finally apply optimal features to
our method.
Additionally we tested inside information of
terms to measure term specificity and similarity.
As multiword terms cover the larger part of tech-
nical terms, lexical components are featuring
information representing semantics of terms
(Cerbah, 2000).
The remainder of this paper is organized fol-
lows. Characteristics of term specificity are de-
scribed in Section 2, while term similarity and its
features are addressed in Section 3. Our taxon-
omy learning method is discussed in Section 4.
Experiment and evaluation are discussed in Sec-
tion 5, and finally, conclusions are drawn in Sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.927548" genericHeader="method">
2 Term Specificity
</sectionHeader>
<bodyText confidence="0.999474">
Specificity is degree of detailed information of
an object about given target object. For example,
if an encyclopedia contains detailed information
about ‘IT domain’, then the encyclopedia is ‘IT
specific encyclopedia’. In this context, specificity
is a function of objects and target object to real
number. Traditionally term specificity is widely
used in information retrieval systems to weight
index terms in documents (S. Jones, 1972; Ai-
zawa, 2003; Wong &amp; Yao, 1992). In information
retrieval context, term specificity is function of
index terms and documents. On the other hand,
term specificity is the function of terms and tar-
get domains in taxonomy learning context (Ryu
&amp; Choi 2005). Term specificity to a domain is
</bodyText>
<equation confidence="0.992658">
Spec(t  |D) R+
∈ (1)
</equation>
<bodyText confidence="0.999795696969697">
where t is a term, and Spec(t|D) is the specificity
of t in a given domain D. We simply use Spec(t)
instead of Spec(t|D) assuming a particular do-
main D in this paper.
Understanding the relation between domain
concepts and their lexicalization methods is
needed, before we describe term specificity
measuring methods. Domain specific concepts
can be distinguished by a set of what we call
‘characteristics’. More specific concepts are cre-
ated by adding characteristics to the set of char-
acteristics of existing concepts. Let us consider
two concepts: C1 and C2. C1 is an existing con-
cept and C2 is a newly created concept by com-
bining new characteristics to the characteristic
set of C1. In this case, C1 is an ancestor of C2
(ISO, 2000). When domain specific concepts are
lexicalized as terms, the terms&apos; word-formation is
classified into two categories based on the com-
position of component words. In the first cate-
gory, new terms are created by adding modifiers
to existing terms. Figure 2 shows a subtree of
financial ontology. For example ‘current asset’
was created by adding the modifier ‘current’ to
its hypernym ‘asset’. In this case, inside informa-
tion is a good evidence to represent the charac-
teristics. In the second category, new terms are
created independently of existing terms. For ex-
ample, ‘cache’, ‘inventory’, and ‘receivable’
share no common words with their hypernyms
‘current asset’ and ‘asset’. In this case, outside
information is used to differentiate the character-
istics of the terms.
</bodyText>
<figure confidence="0.956788">
asset
current asset fixed asset
intangible
cache inventory receivable
asset
</figure>
<figureCaption confidence="0.999949">
Figure 2. Subtree of financial ontology
</figureCaption>
<bodyText confidence="0.999915857142857">
There are many kinds of inside and outside in-
formation to be used in measuring term specific-
ity. Distribution of adjective-term relation and
verb-argument dependency relation are colloca-
tion based statistics. Distribution of adjective-
term relation refers to the idea that specific nouns
are rarely modified, while general nouns are fre-
</bodyText>
<page confidence="0.998286">
42
</page>
<bodyText confidence="0.985973071428572">
quently modified in text. This feature has been
discussed to measure specificity of nouns in
(Caraballo, 1999; Ryu &amp; Choi, 2005) and to
build taxonomy of Japanese nouns (Yamamoto et
al., 2005). Inversed specificity of a term can be
measured by entropy of adjectives as shown Eq.
(2).
tion conditions, it is noticed that many are just
failing to be included because a few occurrences
of the subsumed term, tj, does not co-occur with
ti. Subsequently, the conditions are relaxed and
subsume function is defined as Eq. (5). In case of
P(ti|tj)&gt;P(tj|ti), subsume(ti,tj) returns 1, otherwise
returns 0.
</bodyText>
<equation confidence="0.989274666666667">
Specadj (t) −1 = −∑ P adj t P adj t subsume (i, tj) r. 1 if P(ti  |tj) &gt;P(tj |ti) (5)
(  |)log (  |) (2) 0 otherwise
adj
</equation>
<bodyText confidence="0.985904166666667">
where P(adj|t), the probability that adj modifies t,
is estimated as freq(adj,t)/freq(t). The entropy is
the average information quantity of all (adj,t)
pairs for term t. Specific terms have low entropy,
because their adjective distributions are simple.
For verb-argument distribution, we assume
that domain specific terms co-occur with selected
verbs which represent special characteristics of
terms while general terms are associated with
multiple verbs. Under this assumption, we make
use of syntactic dependencies between verbs ap-
pearing in the corpus and their arguments such as
subjects and objects. For example, ‘inventory’1,
in Figure 2, shows a tendency to be objects of
specific verbs like ‘increase’ and ‘reduce’. This
feature was used in (Cimiano et al., 2005) to
learn concept hierarchy. Inversed specificity of a
term can be measured by entropy of verb-
argument relations as Eq. (3).
Specvarg (t) −1 = −∑ P(t  |varg) log P(t  |varg) (3 )varg
where P(t|varg), the probability that t is argument
of varg, is estimated as freq(t,varg)/freq(varg). The
entropy is the average information quantity of all
(t,varg) pairs for term t.
Conditional probability of term co-occurrence
in documents was used in (Sanderson &amp; Croft,
1999) to build term taxonomy. This statistics is
based on the assumption that, for two terms, ti
and tj, ti is said to subsume tj if the following two
conditions hold,
</bodyText>
<equation confidence="0.920857">
P(ti|tj) = 1 and P(tj|ti)&lt;1 (4)
</equation>
<bodyText confidence="0.9999022">
In other words, ti subsumes tj if the documents
which tj occurs in are a subset of the documents
which ti occurs in, therefore ti can be parent of tj
in taxonomy. Although a good number of term
pairs are found that adhere to the two subsump-
</bodyText>
<footnote confidence="0.7408155">
1 ‘Inventory’ consists of a list of goods and materials held
available in stock (http://en.wikipedia.org/wiki/Inventory).
</footnote>
<bodyText confidence="0.999911">
We apply this function to calculate term speci-
ficity as shown Eq. (6) where a term is specific
when it is subsumed by most of other terms.
Specificity of t is determined by the ratio of
terms that subsume t over all co-occurring terms.
</bodyText>
<equation confidence="0.988034285714286">
= ∑ subsume t t
( , )
( ) j n
1 j
≤ ≤
t
n
</equation>
<bodyText confidence="0.999886521739131">
where n is number of terms co-occurring terms
with t.
Finally, inside-word information is important
to compute specificity for multiword terms. Con-
sider a term t that consists of two words like t =
w1w2. Two words, w1 and w2, have their unique
characteristics and the characteristics are
summed up to the characteristic of t. Mutual in-
formation is used to estimate the association be-
tween a term and its component words. Let
T={t1,...,tN} be a set of terms found in a corpus,
and W={w1,...,wM} be a set of component words
composing the terms in T. Assume a joint prob-
ability distribution P(ti,wj), probability of wj is a
component of ti, is given for ti and wj. Mutual
information between ti and wj compares the prob-
ability of observing ti and wj together and the
probability of observing ti and wj independently.
The mutual information represents the reduction
of uncertainty about ti when wj is observed. The
summed mutual information between ti and W, as
in Eq. (7), is total reduction of uncertainty about
ti when all component words are observed.
</bodyText>
<equation confidence="0.991599666666667">
P t w
( , )
Spec t
( ) = ∑ i j
log (7)
in i P(ti)P(wj )
</equation>
<bodyText confidence="0.9996746">
This equation indicates that wj which is highly
associated to ti contributes specificity of ti. For
example, ‘debenture bond’ is more specific con-
cept than ‘financial product’. Intuitively, ‘deben-
ture’ is highly associated to ‘debenture bond’
</bodyText>
<figure confidence="0.85344325">
Speccoldoc
(6)
w W
j ∈
</figure>
<page confidence="0.998337">
43
</page>
<bodyText confidence="0.9854025">
compared with ‘bond’ to ‘debenture bond’ or
‘financial’, ‘product’ to ‘financial product’.
</bodyText>
<sectionHeader confidence="0.978985" genericHeader="method">
3 Term Similarity
</sectionHeader>
<bodyText confidence="0.9997282">
We evaluate four statistical and lexical features,
related to taxonomy learning, in view of term
similarity. Three statistical features have been
used in existing taxonomy learning researches.
(Sanderson &amp; Croft, 1999) used conditional
probability of co-occurring terms in same docu-
ment in taxonomy learning process as shown in
Eq. (4). This feature can be used to measure
similarity of terms. If two terms co-occur in
common documents, they are semantically simi-
lar to each other. Based on this assumption, we
can calculate term similarity by comparing the
frequency of co-occurring ti and tj together and
the frequency of occurring ti and tj independently,
as Eq. (8).
</bodyText>
<equation confidence="0.9978682">
2* ( , )
df t t
i j
Sim t t
coldoc i j
( , ) =
df t df t
( ) ( )
+
i j
</equation>
<bodyText confidence="0.999922527777778">
where df(ti,tj) is number of documents in which
both ti and tj co-occur, df(ti) is number of docu-
ments in which ti occurs.
(Yamamoto et al., 2005) used adjective pat-
terns to make characteristics vectors for terms in
Complementary Similarity Measure (CSM). Al-
though CSM was initially designed to extract
superordinate-subordinate relations, it is a simi-
larity measure by itself. They proposed two CSM
measures; one is for binary images in which val-
ues in feature vectors are 0 or 1, and the other is
for gray-scale images in which values in feature
vectors are 0 through 1. We adapt gray-scale
measure in similarity calculation, because it
showed better performance in their research.
(Cimiano et al., 2005) applied Formal Concept
Analysis (FCA) to extract taxonomies from a
text corpus. They modeled the context of a term
as a vector representing syntactic dependencies.
Similarity based on verb-argument dependencies
is calculated using cosine measure as Eq. (9).
where P(t|varg), the probability that t is argument
of varg, is estimated as freq(t,varg)/freq(varg).
Above three similarity measures are valid when
terms, ti and tj, appear in corpus one or more
times.
The last similarity measure is based on inside
information of terms. Because many domain
terms are multiword terms, component words are
clues for term similarity. If two terms share
many common words, they share common char-
acteristics in given domain. For example, four
words ‘asset’, ‘current asset’, ‘fixed asset’ and
‘intangible asset’ share characteristics related to
‘asset’ as in Figure 2. This similarity measure is
shown in Eq. (10).
</bodyText>
<equation confidence="0.637871142857143">
2* ( , ) (10)
cwc t t
i j
Sim t t
in i j
( , ) = |
t + t i   ||j |
</equation>
<bodyText confidence="0.9998906">
where |t |is word count of t, and cwc(ti,tj) is
common word count in ti and tj. Simin(ti,tj) is
valid when cwc(ti,tj)&gt;0. Because cwc(ti,tj)=0 for
most of term pairs, it is difficult to catch reliable
results for all possible term pairs.
</bodyText>
<sectionHeader confidence="0.992988" genericHeader="method">
4 Taxonomy Learning Process
</sectionHeader>
<bodyText confidence="0.999391909090909">
We model taxonomy learning process as a se-
quential insertion of new terms to current taxon-
omy. New taxonomy starts with empty state, and
changes to rich taxonomic structure with the re-
peated insertion of terms as depicted in Figure 3.
Terms to be inserted are sorted by term specific-
ity values. Term insertion based on the increas-
ing order of term specificity is natural, because
the taxonomy grows from top to down with term
insertion process in increasing specificity se-
quence.
</bodyText>
<figureCaption confidence="0.5984275">
Figure 3. Terms are inserted to taxonomy in the
sequence of specificity
</figureCaption>
<bodyText confidence="0.988228333333333">
According to above assumption, our system
selects possible hypernyms of a new term, tnew in
current taxonomy as following steps:
</bodyText>
<listItem confidence="0.950437">
• Step 1: Select n-most similar terms to tnew
from current taxonomy
• Step 2: Select candidate hypernyms of tnew
from n-most similar terms. Specificity of
candidate hypernyms is less than that of tnew.
</listItem>
<equation confidence="0.893537333333333">
Simvarg (ti, tj) =
∑varg∈VP(ti  |varg)P(tj  |varg) (9)
P(ti  |varg)2 ∑ P(tj  |varg)2
</equation>
<figure confidence="0.9685965">
varg ∈ V varg∈ V
∑
Taxonomy
Low
Term sequence
...
tnew
Specificity
High Specificity Low
tnew
High
(8)
</figure>
<page confidence="0.983644">
44
</page>
<listItem confidence="0.973456">
• Step 3: Insert tnew as hyponyms of candidate
hypernyms
</listItem>
<bodyText confidence="0.7478582">
For example, suppose t2, t4, t5 and t6, are four
most similar terms to tnew in Figure 4. Two terms
t2 and t4 are selected as candidate hypernyms of
tnew, because specificity of the terms is less than
specificity of tnew.
</bodyText>
<equation confidence="0.6092095">
Spec(t1) = 1.0
High
</equation>
<figureCaption confidence="0.91967">
Figure 4. Selection of candidate hypernyms of
tnew from taxonomy using term specificity and
similarity
</figureCaption>
<sectionHeader confidence="0.984124" genericHeader="evaluation">
5 Experiment and Evaluation
</sectionHeader>
<bodyText confidence="0.946983075">
We applied our taxonomy learning method to set
of terms in existing taxonomy. We removed all
relations from the taxonomy, and made new
taxonomic relations among the terms. The
learned taxonomy was then compared to original
taxonomy. Our experiment is composed of four
steps. Firstly, we calculated term specificity us-
ing specificity measures discussed in chapter 2,
secondly, we calculated term similarity using
similarity measures described in chapter 3,
thirdly, we applied the best specificity and simi-
larity features to our taxonomy building process,
and finally, we evaluated our method and com-
pared with other taxonomy learning methods.
Finance ontology 2 which was developed
within the GETESS project (Staab et al., 1999)
was used in our experiment. We slightly modi-
fied original ontology. We unified different ex-
pressions of same concept to identical expression.
For example, &apos;cd-rom drive&apos; and &apos;cdrom drive&apos; are
unified as &apos;cd-rom drive&apos; because the former is
more usual expression than the latter. We also
removed terms that are not descendents of &apos;root&apos;
node to make the taxonomy have single root
node. The taxonomy consists of total 1,819
nodes and 1,130 distinct nodes. Maximum and
average depths are 15 and 5.5 respectively, and
2 The ontology can be downloaded at http://www.aifb.uni-
karlsruhe.de/WBS/pci/FinanceGoldStandard.isa. P. Cimiano
and his colleagues added English labels for the originally
German labeled nodes (Cimiano et al., 2005)
maximum and average children nodes are 32 and
3.5 respectively.
We considered Reuters215783 corpus, over 3.1
million words in title and body fields. We parsed
the corpus using Connexor functional depend-
ency parser4 and extracted various statistics: term
frequency, distribution of adjectives, distribution
of co-occurring frequency in documents, and
verb-argument distribution.
</bodyText>
<subsectionHeader confidence="0.995563">
5.1 Term Specificity
</subsectionHeader>
<bodyText confidence="0.999573222222222">
Term specificity was evaluated based on three
criteria: recall, precision and F-measure. Recall
is the fraction of the terms that have specificity
values by the given measuring method. Precision
is the fraction of relations with correct specificity
values. F-measure is a harmonic mean of preci-
sion and recall into a single measure of overall
performance. Precision (Pspec), recall (Rspec), F-
measure (Fspec) is defined as follows:
</bodyText>
<equation confidence="0.904151">
# of terms with specificity
# of all terms (11)
# ( , )
of R p c
valid with correct specificity
# ( , )
of R p c
valid
</equation>
<bodyText confidence="0.999810913043478">
where Rvalid(p,c) is a valid parent-child relation in
original taxonomy, and a relation is valid when
the specificity of two terms are measured by the
given method. If the specificity of child term, c,
is larger than that of parent term, p, then the rela-
tion is correct.
We tested four specificity measuring methods
discussed in section 2 and the result is shown in
Table 1. Specadj showed the highest precision as
we anticipated. Because domain specific terms
have sufficient information in themselves; they
are rarely modified by other words in real text.
However, Specadj showed the lowest recall for
data sparseness problem. As mentioned above, it
is hard to collect sufficient adjectives for domain
specific terms from text. Specvarg showed the
lowest precision. This result indicates that distri-
bution of verb-argument relation is less corre-
lated to term specificity. Specin showed the high-
est recall because it measures term specificity
using component words contrary to other meth-
ods. Speccoldoc showed comparable precision and
recall.
</bodyText>
<page confidence="0.476096">
3
</page>
<footnote confidence="0.861405">
http://www.daviddlewis.com/resources/testcollections/reute
rs21578/
4 http://www.connexor.com/
</footnote>
<equation confidence="0.990052736842105">
Spec(t4) = 2.0 Spec(t5) = 3.0
Spec(t2) = 1.5
t� t� to
Spec(t6) = 2.4
t, t� t�
t� t�
t,
Spec(t3) = 1.5
t1o
Spec(tnew) = 2.3
t..,
Low
Specificity
Spec(t7) = 4.0 Spec(t8) = 3.5 Spec(t9) = 2.5 Spec(t10) = 3.0
=
Rspec
=
P
spec
</equation>
<page confidence="0.984933">
45
</page>
<bodyText confidence="0.996073625">
We harmonized Specin and Specadj to Specin/adj
as described in (Ryu &amp; Choi, 2005) to take ad-
vantages of both inside and outside information.
Harmonic mean of two specificity values was
used in Specin/adj method. Specin/adj showed the
highest F-measure because precision was higher
than that of Specin and recall was equal to that of
Specin.
</bodyText>
<tableCaption confidence="0.937316">
Table 1. Precision, recall and F-measure for term
specificity
</tableCaption>
<table confidence="0.999518666666667">
Method Precision Recall F-measure
Specadj 0.795 0.609 0.689
Specvarg 0.663 0.702 0.682
Speccoldoc 0.717 0.702 0.709
Specin 0.728 0.907 0.808
Specin/adj 0.731 0.907 0.810
</table>
<subsectionHeader confidence="0.995888">
5.2 Term Similarity
</subsectionHeader>
<bodyText confidence="0.999940142857143">
We evaluated similarity measures by comparing
with taxonomy based similarity measure. (Bu-
danitsky &amp; Hirst, 2006) calculated correlation
coefficients (CC) between human similarity rat-
ings and the five WordNet based similarity
measures. Among the five computational meas-
ures, (Leacock &amp; Chodorow, 1998)’s method
showed the highest correlation coefficients, even
though all of the measures showed similar rang-
ing from 0.74 to 0.85. This result means that tax-
onomy based similarity is highly correlated to
human similarity ratings. We can indirectly
evaluate our similarity measures by comparing to
taxonomy based similarity measure, instead of
direct comparison to human rating. If applied
similarity measure is qualified, the calculated
similarity will be highly correlated to taxonomy
based similarity. Leacock and Chodorow pro-
posed following formula for computing the
scaled semantic similarity between terms t1 and t2
in taxonomy.
</bodyText>
<table confidence="0.963303428571429">
SimLC len t t (12)
( , )
1 2
(t„t2) = −log 2 max
x depth(t)
t Taxonomy
∈
</table>
<bodyText confidence="0.989076467741935">
where the denominator includes the maximum
depth of given taxonomy, and len(t1, t2) is num-
ber of edges in the shortest path between word t1
and t2 in the taxonomy.
Besides CC with ontology based similarity
measures, recall of a similarity measures is also
important evaluation factor. We defined recall of
similarity measure, RSim, as the fraction of the
term pairs that have similarity values by the
given measuring method as Eq. (13).
RSim # similarity measured term pairs
= (13)
# all possible term pairs
We also defined F-measure for a similarity
measure, Fsim, as harmonic means of CC and Rsim.
Because CC is a kind of precision, Fsim is overall
measure of precision and recall.
We calculated term similarity between all pos-
sible term pairs in finance ontology using the
measures described in section 3. Additionally we
introduced new similarity measure Simin/varg
which is combined similarity of Simvarg and Simin.
Simvarg and Simin between two terms are harmo-
nized to Simin/varg. We also calculated SimLC
based on finance ontology, and calculated CC
between SimLC and results of other measures.
Figure 5 shows variation of CC and recall as
threshold of similarity changes from 0.0 to 1.0
for five similarity measures. Threshold is directly
proportional to CC and inversely proportional to
recall in ideal case. We normalized all similarity
values to [0.0, 1.0] in each measure. CC grows as
threshold increases in Simcoldoc and Simvarg as we
expected. CC of CSM measure, Simcsm, increased
as threshold increased and decreased when
threshold is over 0.6. For example two terms ‘as-
set’ and ‘current asset’ are very similar to each
other based on SimLC measure, because edge
count between two terms is one in finance ontol-
ogy. The former can be modified many adjec-
tives such as ‘intangible’, ‘tangible’, ‘new’ and
‘estimated’, while the latter is rarely modified by
other adjectives in corpus because it was already
extended from ‘asset’ by adding adjective ‘cur-
rent’. Therefore, semantically similar terms do
not always have similar adjective distributions.
CC between Simin and SimLC showed high curve
in low threshold, but downed as threshold in-
creased. Similarity value above 0.6 is insignifi-
cant, because it is hard to be over 0.6 using Eq.
(10). For example, similarity between ‘executive
board meeting’ and ‘board meeting’ is 0.8, the
maximum similarity in our test set. The average
of inside-word similarity is 0.41.
Simvarg showed higher recall than other meas-
ures. This means that verb-argument relation is
more abundant than other features in corpus.
SimIn showed the lowest recall because we could
get valid similarity using Eq. (10). Simvarg
showed higher F-measure when threshold is over
0.2. This result illustrate that verb-argument rela-
tion is adequate feature to similarity calculation.
</bodyText>
<page confidence="0.998455">
46
</page>
<bodyText confidence="0.9999694">
The combined similarity measure, Simin/varg,
complement shortcomings of SimIn and Simvarg.
SimIn showed high CC but low recall. Contrarily
Simvarg showed low CC but high recall. Simin/varg
showed the highest F-measure.
</bodyText>
<subsectionHeader confidence="0.992784">
5.3 Taxonomy learning
</subsectionHeader>
<bodyText confidence="0.993089731707317">
In order to evaluate our approach we need to as-
sess how good the automatically learned tax-
onomies reflect a given domain. The goodness is
evaluated by the similarity of automatically
learned taxonomy to reference taxonomy. We
used (Cimiano et al., 2005)’s ontology evaluation
method in which lexical recall (LRTax), precision
(PTax) and F-measure (FTax) of learned taxonomy
are defined based on the notion of taxonomy
overlap. LRTax is defined as the ratio of number
of common terms in learned taxonomy and refer-
ence taxonomy over number of terms in refer-
ence taxonomy. PTax is defined as ratio of taxon-
omy overlap of learned taxonomy to reference
taxonomy. FTax is harmonic mean of LRTax and
PTax.
Threshold
We generated four taxonomies, Tcoldoc, Tcsm,
Tfca, Tspec/sim, using four taxonomy learning meth-
ods: term co-occurring method, CSM method,
FCA method and our method. We applied Spe-
cin/adj in specificity measuring and Simin/varg in
similarity calculation because they showed the
highest F-measure. In our method, the most
probable one term was selected as hypernym of
newly inserted term in each learning step.
Figure 6 shows variations of lexical recall,
precision and F-measure of four methods as
threshold changes. Threshold in each method
represent different information to each other.
Threshold in Tcsm is variation of CSM values.
Threshold in Tcoldoc is variation of probability of
two terms co-occur in a document. Threshold in
Tfca is normalized frequency of contexts. Thresh-
old in Tspec/sim, is variation of similarity.
Tspec/sim showed the highest lexical recall.
Lexical recall is tightly related to recall in simi-
larity measures. Simin/varg showed the highest re-
call in similarity measures. Tfca and Tcsm showed
higher precision than other taxonomies. It is as-
sumed that precision of taxonomy depends on
</bodyText>
<figure confidence="0.999617236363636">
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
CC
0.8
0.6
0.4
0.2
0.0
1.0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Lexical Recall
0.8
0.6
0.4
0.2
0
1
Threshold
Threshold
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
0.20
0.16
0.12
0.08
0.04
0.00
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
F measure Recall
0.24
0.20
0.16
0.12
0.08
0.04
0.00
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
0.8
0.6
0.4
0.2
0
1
F-Measure Precision
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold
Sim(coldoc) Sim(CSM) Sim(varg)
Sim(In) Sim(In/Varg)
Threshold
CSM COLDOC SPEC/SIM FCA
</figure>
<figureCaption confidence="0.9979024">
Figure 5 Correlation coefficient between SimLC
and other similarity measures. Recall and F-
measure of similarity measures
Figure 6. Lexical recall, precision and F-measure
of taxonomy learning methods
</figureCaption>
<page confidence="0.996996">
47
</page>
<bodyText confidence="0.999830230769231">
the precision of specificity measures and the CC
of similarity measures. In actual case, Simvarg
showed the most plausible curve in CC and Spe-
cadj showed the highest precision in specificity.
Verb-argument relation and adjective-term rela-
tion are used in FCA and CSM methods respec-
tively. Tspec/sim and Tcoldoc showed higher F-
measure curve than other two taxonomies due to
high lexical recall. Although our method showed
plausible F-measure, it showed the lowest preci-
sion. So other combination of similarity and
specificity measures are needed to improve pre-
cision of learned taxonomy.
</bodyText>
<sectionHeader confidence="0.99949" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999991807692308">
We have presented new taxonomy learning
method with term similarity and specificity taken
from domain-specific corpus. It can be applied to
different domains as it is; and, if we have a syn-
tactic parser available, to different languages. We
analyzed the features used in previous researches
in view of term specificity and similarity. In this
analysis, we found that the features embed the
characteristics of both conditions.
Compared to previous approaches, our method
has advantages in that we can use different fea-
tures for term specificity and similarity. It makes
easy to analyze errors in taxonomy learning step,
whether the wrong relations are caused by speci-
ficity errors or by similarity errors. The main
drawback of our method, as it is now, is that the
effect of wrong located terms in upper level
propagates to lower levels.
Until now, researches on automatic ontology
learning especially taxonomic relation showed
very low precision. Human experts’ intervention
is inevitable in automatic learning process to
make applicable taxonomy. Future work is to
make new model where human experts and sys-
tem work interactively in ontology learning
process in order to balance cost and precision.
</bodyText>
<sectionHeader confidence="0.994339" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999850630769231">
S. Caraballo, E. Charniak. 1999. Determining the
Specificity of Nouns from Text. Proceedings of the
1999 Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pp. 63-70
P. Cimiano, A. Hotho, S.Staab. 2005. Learning Con-
cept Hierarchies from Text Corpora using Formal
Concept Analysis. Journal of AI Research, Vol. 24,
pp. 305-339
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. Proceedings of the
14th International Conference on Computational
Linguistics
L. Iwanska, N. Mata and K. Kruger. 2000. Fully
automatic acquisition of taxonomic knowledge
from large corpora of texts. In Iwanska, L. &amp;
Shapiro, S. (Eds.), Natural Language Processing
and Knowledge Processing, pp. 335-345,
MIT/AAAI Press.
E. Yamamoto, K. Kanzaki and H. Isahara. 2005. Ex-
traction of Hierarchies Based on Inclusion of Co-
occurring Words with Frequency Information.
Proceedings of 9th International Joint Conference
on Artificial Intelligence, pp. 1160-1167
A. Burgun, O. Bodenreider. 2001. Aspects of the
Taxonomic Relation in the Biomedical Domain,
Proceedings of International Conference on For-
mal Ontology in Information Systems, pp. 222-233
Mark Sanderson and Bruce Croft. 1999. Deriving
concept hierarchies from text. Proceedings of the
22th Annual ACM S1GIR Conference on Research
and Development in Information Retrieval, pp.
206-213, 1999
Karen Sparck Jones. 1972. Exhausitivity and Speci-
ficity Journal of Documentation Vol. 28, Num. 1,
pp. 11-21
S.K.M. Wong, Y.Y. Yao. 1992. An Information-
Theoretic Measure of Term Specificity, Journal of
the American Society for Information Science, Vol.
43, Num. 1. pp.54-61
ISO 704. 2000. Terminology work-Principle and
methods. ISO 704 Second Edition
A. Aizawa. 2003. An information-theoretic perspec-
tive of tf-idf measures. Journal of Information
Processing and Management, vol. 39
Alexander Budanitsky, Graeme Hirst. 2006 Evaluat-
ing WordNet-based Measures of Lexical Semantic
Relatedness. Computational Linguistics. Vol. 32
NO. 1, pp. 13-47(35)
Claudia Leacock, Martin Chodorow. 1998. Combin-
ing local context and WordNet similarity for word
sense identification. In Christian Fellbaum, editor,
WordNet: An Electronic Lexical Database. The
MIT Press, pp. 265-283
Pum-Mo Ryu, Key-Sun Choi. 2005. An Information-
Theoretic Approach to Taxonomy Extraction for
Ontology Learning, In P. Buitelaar et al. (eds.), On-
tology Learning from Text: Methods, Evaluation
and Applications, Vol. 123, Frontiers in Artificial
Intelligence and Applications, IOS Press
Farid Cerbah. 2000. Exogeneous and Endogeneous
Approaches to Semantic Categorization of Un-
known Technical Terms. Proceedings of the 18th
International Conference on Computational Lin-
guistics, vol. 1, pp. 145-151
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.020075">
<title confidence="0.999291">Taxonomy Learning using Term Specificity and Similarity</title>
<author confidence="0.999299">Pum-Mo Ryu</author>
<affiliation confidence="0.7088025">Computer Science Division, KORTERM/BOLA</affiliation>
<address confidence="0.190711">Korea</address>
<email confidence="0.923716">pmryu@world.kaist.ac.kr</email>
<author confidence="0.988654">Key-Sun Choi</author>
<affiliation confidence="0.7186545">Computer Science Division, KORTERM/BOLA</affiliation>
<address confidence="0.222482">Korea</address>
<email confidence="0.972172">kschoi@cs.kaist.ac.kr</email>
<abstract confidence="0.999383222222222">Learning taxonomy for technical terms is difficult and tedious task, especially when new terms should be included. The goal of this paper is to assign taxonomic relations among technical terms. We propose new approach to the problem that relies on term specificity and similarity measures. Term specificity and similarity are necessary conditions for taxonomy learning, because highly specific terms tend to locate in deep levels and semantically similar terms are close to each other in taxonomy. We analyzed various features used in previous researches in view of term specificity and similarity, and applied optimal features for term specificity and similarity to our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Caraballo</author>
<author>E Charniak</author>
</authors>
<title>Determining the Specificity of Nouns from Text.</title>
<date>1999</date>
<booktitle>Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>63--70</pages>
<marker>Caraballo, Charniak, 1999</marker>
<rawString>S. Caraballo, E. Charniak. 1999. Determining the Specificity of Nouns from Text. Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp. 63-70</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>A Hotho</author>
<author>S Staab</author>
</authors>
<title>Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis.</title>
<date>2005</date>
<journal>Journal of AI Research,</journal>
<volume>24</volume>
<pages>305--339</pages>
<contexts>
<context position="1907" citStr="Cimiano et al., 2005" startWordPosition="281" endWordPosition="284">in (domain ontology), or a model suitable for specific tasks (task ontologies) (Burgun &amp; Bodenreider, 2001). However their wide usage is still hindered by time-consuming, cost-ineffective building processes. The main paradigms of taxonomy learning are on the one hand pattern based approaches and on the other hand distributional hypothesis based approaches. The former is approaches based on matching lexico-syntactic patterns which convey taxonomic relations in a corpus (Hearst, 1992; Iwanska et al., 2000), and the latter is statistical approaches based on the distribution of context in corpus (Cimiano et al., 2005; Yamamoto et al., 2005; Sanderson &amp; Croft, 1999). The former features a high precision and low recall compared to the latter. The quality of learned relations is higher than those of statistical approaches, while the patterns are rarely applied in real corpus. It is also difficult to improve performance of pattern based approaches because they are simple and clear. So, many researches have been focused on raising precision of statistical approaches. We introduce new distributional hypothesis based taxonomy learning method using term specificity and term similarity. Term specificity is a measu</context>
<context position="9411" citStr="Cimiano et al., 2005" startWordPosition="1503" endWordPosition="1506">) pairs for term t. Specific terms have low entropy, because their adjective distributions are simple. For verb-argument distribution, we assume that domain specific terms co-occur with selected verbs which represent special characteristics of terms while general terms are associated with multiple verbs. Under this assumption, we make use of syntactic dependencies between verbs appearing in the corpus and their arguments such as subjects and objects. For example, ‘inventory’1, in Figure 2, shows a tendency to be objects of specific verbs like ‘increase’ and ‘reduce’. This feature was used in (Cimiano et al., 2005) to learn concept hierarchy. Inversed specificity of a term can be measured by entropy of verbargument relations as Eq. (3). Specvarg (t) −1 = −∑ P(t |varg) log P(t |varg) (3 )varg where P(t|varg), the probability that t is argument of varg, is estimated as freq(t,varg)/freq(varg). The entropy is the average information quantity of all (t,varg) pairs for term t. Conditional probability of term co-occurrence in documents was used in (Sanderson &amp; Croft, 1999) to build term taxonomy. This statistics is based on the assumption that, for two terms, ti and tj, ti is said to subsume tj if the followi</context>
<context position="13641" citStr="Cimiano et al., 2005" startWordPosition="2250" endWordPosition="2253">mber of documents in which ti occurs. (Yamamoto et al., 2005) used adjective patterns to make characteristics vectors for terms in Complementary Similarity Measure (CSM). Although CSM was initially designed to extract superordinate-subordinate relations, it is a similarity measure by itself. They proposed two CSM measures; one is for binary images in which values in feature vectors are 0 or 1, and the other is for gray-scale images in which values in feature vectors are 0 through 1. We adapt gray-scale measure in similarity calculation, because it showed better performance in their research. (Cimiano et al., 2005) applied Formal Concept Analysis (FCA) to extract taxonomies from a text corpus. They modeled the context of a term as a vector representing syntactic dependencies. Similarity based on verb-argument dependencies is calculated using cosine measure as Eq. (9). where P(t|varg), the probability that t is argument of varg, is estimated as freq(t,varg)/freq(varg). Above three similarity measures are valid when terms, ti and tj, appear in corpus one or more times. The last similarity measure is based on inside information of terms. Because many domain terms are multiword terms, component words are cl</context>
<context position="17830" citStr="Cimiano et al., 2005" startWordPosition="2939" endWordPosition="2942">e concept to identical expression. For example, &apos;cd-rom drive&apos; and &apos;cdrom drive&apos; are unified as &apos;cd-rom drive&apos; because the former is more usual expression than the latter. We also removed terms that are not descendents of &apos;root&apos; node to make the taxonomy have single root node. The taxonomy consists of total 1,819 nodes and 1,130 distinct nodes. Maximum and average depths are 15 and 5.5 respectively, and 2 The ontology can be downloaded at http://www.aifb.unikarlsruhe.de/WBS/pci/FinanceGoldStandard.isa. P. Cimiano and his colleagues added English labels for the originally German labeled nodes (Cimiano et al., 2005) maximum and average children nodes are 32 and 3.5 respectively. We considered Reuters215783 corpus, over 3.1 million words in title and body fields. We parsed the corpus using Connexor functional dependency parser4 and extracted various statistics: term frequency, distribution of adjectives, distribution of co-occurring frequency in documents, and verb-argument distribution. 5.1 Term Specificity Term specificity was evaluated based on three criteria: recall, precision and F-measure. Recall is the fraction of the terms that have specificity values by the given measuring method. Precision is th</context>
<context position="25049" citStr="Cimiano et al., 2005" startWordPosition="4086" endWordPosition="4089">en threshold is over 0.2. This result illustrate that verb-argument relation is adequate feature to similarity calculation. 46 The combined similarity measure, Simin/varg, complement shortcomings of SimIn and Simvarg. SimIn showed high CC but low recall. Contrarily Simvarg showed low CC but high recall. Simin/varg showed the highest F-measure. 5.3 Taxonomy learning In order to evaluate our approach we need to assess how good the automatically learned taxonomies reflect a given domain. The goodness is evaluated by the similarity of automatically learned taxonomy to reference taxonomy. We used (Cimiano et al., 2005)’s ontology evaluation method in which lexical recall (LRTax), precision (PTax) and F-measure (FTax) of learned taxonomy are defined based on the notion of taxonomy overlap. LRTax is defined as the ratio of number of common terms in learned taxonomy and reference taxonomy over number of terms in reference taxonomy. PTax is defined as ratio of taxonomy overlap of learned taxonomy to reference taxonomy. FTax is harmonic mean of LRTax and PTax. Threshold We generated four taxonomies, Tcoldoc, Tcsm, Tfca, Tspec/sim, using four taxonomy learning methods: term co-occurring method, CSM method, FCA me</context>
</contexts>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>P. Cimiano, A. Hotho, S.Staab. 2005. Learning Concept Hierarchies from Text Corpora using Formal Concept Analysis. Journal of AI Research, Vol. 24, pp. 305-339</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<booktitle>Proceedings of the 14th International Conference on Computational Linguistics</booktitle>
<contexts>
<context position="1773" citStr="Hearst, 1992" startWordPosition="261" endWordPosition="262">r organizing many aspects of knowledge. As components of ontologies, taxonomies can provide an organizational model for a domain (domain ontology), or a model suitable for specific tasks (task ontologies) (Burgun &amp; Bodenreider, 2001). However their wide usage is still hindered by time-consuming, cost-ineffective building processes. The main paradigms of taxonomy learning are on the one hand pattern based approaches and on the other hand distributional hypothesis based approaches. The former is approaches based on matching lexico-syntactic patterns which convey taxonomic relations in a corpus (Hearst, 1992; Iwanska et al., 2000), and the latter is statistical approaches based on the distribution of context in corpus (Cimiano et al., 2005; Yamamoto et al., 2005; Sanderson &amp; Croft, 1999). The former features a high precision and low recall compared to the latter. The quality of learned relations is higher than those of statistical approaches, while the patterns are rarely applied in real corpus. It is also difficult to improve performance of pattern based approaches because they are simple and clear. So, many researches have been focused on raising precision of statistical approaches. We introduc</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. Proceedings of the 14th International Conference on Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Iwanska</author>
<author>N Mata</author>
<author>K Kruger</author>
</authors>
<title>Fully automatic acquisition of taxonomic knowledge from large corpora of texts.</title>
<date>2000</date>
<booktitle>In Iwanska, L. &amp; Shapiro, S. (Eds.), Natural Language Processing and Knowledge Processing,</booktitle>
<pages>335--345</pages>
<publisher>MIT/AAAI Press.</publisher>
<contexts>
<context position="1796" citStr="Iwanska et al., 2000" startWordPosition="263" endWordPosition="266">any aspects of knowledge. As components of ontologies, taxonomies can provide an organizational model for a domain (domain ontology), or a model suitable for specific tasks (task ontologies) (Burgun &amp; Bodenreider, 2001). However their wide usage is still hindered by time-consuming, cost-ineffective building processes. The main paradigms of taxonomy learning are on the one hand pattern based approaches and on the other hand distributional hypothesis based approaches. The former is approaches based on matching lexico-syntactic patterns which convey taxonomic relations in a corpus (Hearst, 1992; Iwanska et al., 2000), and the latter is statistical approaches based on the distribution of context in corpus (Cimiano et al., 2005; Yamamoto et al., 2005; Sanderson &amp; Croft, 1999). The former features a high precision and low recall compared to the latter. The quality of learned relations is higher than those of statistical approaches, while the patterns are rarely applied in real corpus. It is also difficult to improve performance of pattern based approaches because they are simple and clear. So, many researches have been focused on raising precision of statistical approaches. We introduce new distributional hy</context>
</contexts>
<marker>Iwanska, Mata, Kruger, 2000</marker>
<rawString>L. Iwanska, N. Mata and K. Kruger. 2000. Fully automatic acquisition of taxonomic knowledge from large corpora of texts. In Iwanska, L. &amp; Shapiro, S. (Eds.), Natural Language Processing and Knowledge Processing, pp. 335-345, MIT/AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Yamamoto</author>
<author>K Kanzaki</author>
<author>H Isahara</author>
</authors>
<title>Extraction of Hierarchies Based on Inclusion of Cooccurring Words with Frequency Information.</title>
<date>2005</date>
<booktitle>Proceedings of 9th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1160--1167</pages>
<contexts>
<context position="1930" citStr="Yamamoto et al., 2005" startWordPosition="285" endWordPosition="288">or a model suitable for specific tasks (task ontologies) (Burgun &amp; Bodenreider, 2001). However their wide usage is still hindered by time-consuming, cost-ineffective building processes. The main paradigms of taxonomy learning are on the one hand pattern based approaches and on the other hand distributional hypothesis based approaches. The former is approaches based on matching lexico-syntactic patterns which convey taxonomic relations in a corpus (Hearst, 1992; Iwanska et al., 2000), and the latter is statistical approaches based on the distribution of context in corpus (Cimiano et al., 2005; Yamamoto et al., 2005; Sanderson &amp; Croft, 1999). The former features a high precision and low recall compared to the latter. The quality of learned relations is higher than those of statistical approaches, while the patterns are rarely applied in real corpus. It is also difficult to improve performance of pattern based approaches because they are simple and clear. So, many researches have been focused on raising precision of statistical approaches. We introduce new distributional hypothesis based taxonomy learning method using term specificity and term similarity. Term specificity is a measure of information quant</context>
<context position="8115" citStr="Yamamoto et al., 2005" startWordPosition="1291" endWordPosition="1294">d asset intangible cache inventory receivable asset Figure 2. Subtree of financial ontology There are many kinds of inside and outside information to be used in measuring term specificity. Distribution of adjective-term relation and verb-argument dependency relation are collocation based statistics. Distribution of adjectiveterm relation refers to the idea that specific nouns are rarely modified, while general nouns are fre42 quently modified in text. This feature has been discussed to measure specificity of nouns in (Caraballo, 1999; Ryu &amp; Choi, 2005) and to build taxonomy of Japanese nouns (Yamamoto et al., 2005). Inversed specificity of a term can be measured by entropy of adjectives as shown Eq. (2). tion conditions, it is noticed that many are just failing to be included because a few occurrences of the subsumed term, tj, does not co-occur with ti. Subsequently, the conditions are relaxed and subsume function is defined as Eq. (5). In case of P(ti|tj)&gt;P(tj|ti), subsume(ti,tj) returns 1, otherwise returns 0. Specadj (t) −1 = −∑ P adj t P adj t subsume (i, tj) r. 1 if P(ti |tj) &gt;P(tj |ti) (5) ( |)log ( |) (2) 0 otherwise adj where P(adj|t), the probability that adj modifies t, is estimated as freq(ad</context>
<context position="13081" citStr="Yamamoto et al., 2005" startWordPosition="2160" endWordPosition="2163">rms in same document in taxonomy learning process as shown in Eq. (4). This feature can be used to measure similarity of terms. If two terms co-occur in common documents, they are semantically similar to each other. Based on this assumption, we can calculate term similarity by comparing the frequency of co-occurring ti and tj together and the frequency of occurring ti and tj independently, as Eq. (8). 2* ( , ) df t t i j Sim t t coldoc i j ( , ) = df t df t ( ) ( ) + i j where df(ti,tj) is number of documents in which both ti and tj co-occur, df(ti) is number of documents in which ti occurs. (Yamamoto et al., 2005) used adjective patterns to make characteristics vectors for terms in Complementary Similarity Measure (CSM). Although CSM was initially designed to extract superordinate-subordinate relations, it is a similarity measure by itself. They proposed two CSM measures; one is for binary images in which values in feature vectors are 0 or 1, and the other is for gray-scale images in which values in feature vectors are 0 through 1. We adapt gray-scale measure in similarity calculation, because it showed better performance in their research. (Cimiano et al., 2005) applied Formal Concept Analysis (FCA) t</context>
</contexts>
<marker>Yamamoto, Kanzaki, Isahara, 2005</marker>
<rawString>E. Yamamoto, K. Kanzaki and H. Isahara. 2005. Extraction of Hierarchies Based on Inclusion of Cooccurring Words with Frequency Information. Proceedings of 9th International Joint Conference on Artificial Intelligence, pp. 1160-1167</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Burgun</author>
<author>O Bodenreider</author>
</authors>
<title>Aspects of the Taxonomic Relation in the Biomedical Domain,</title>
<date>2001</date>
<booktitle>Proceedings of International Conference on Formal Ontology in Information Systems,</booktitle>
<pages>222--233</pages>
<contexts>
<context position="1394" citStr="Burgun &amp; Bodenreider, 2001" startWordPosition="205" endWordPosition="208">sed in previous researches in view of term specificity and similarity, and applied optimal features for term specificity and similarity to our method. 1 Introduction Taxonomy is a collection of controlled vocabulary terms organized into a hierarchical structure. Each term in a taxonomy is one or more parentchild relationships to other terms in the taxonomy. Taxonomies are useful artifacts for organizing many aspects of knowledge. As components of ontologies, taxonomies can provide an organizational model for a domain (domain ontology), or a model suitable for specific tasks (task ontologies) (Burgun &amp; Bodenreider, 2001). However their wide usage is still hindered by time-consuming, cost-ineffective building processes. The main paradigms of taxonomy learning are on the one hand pattern based approaches and on the other hand distributional hypothesis based approaches. The former is approaches based on matching lexico-syntactic patterns which convey taxonomic relations in a corpus (Hearst, 1992; Iwanska et al., 2000), and the latter is statistical approaches based on the distribution of context in corpus (Cimiano et al., 2005; Yamamoto et al., 2005; Sanderson &amp; Croft, 1999). The former features a high precision</context>
</contexts>
<marker>Burgun, Bodenreider, 2001</marker>
<rawString>A. Burgun, O. Bodenreider. 2001. Aspects of the Taxonomic Relation in the Biomedical Domain, Proceedings of International Conference on Formal Ontology in Information Systems, pp. 222-233</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sanderson</author>
<author>Bruce Croft</author>
</authors>
<title>Deriving concept hierarchies from text.</title>
<date>1999</date>
<booktitle>Proceedings of the 22th Annual ACM S1GIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>206--213</pages>
<contexts>
<context position="1956" citStr="Sanderson &amp; Croft, 1999" startWordPosition="289" endWordPosition="292"> specific tasks (task ontologies) (Burgun &amp; Bodenreider, 2001). However their wide usage is still hindered by time-consuming, cost-ineffective building processes. The main paradigms of taxonomy learning are on the one hand pattern based approaches and on the other hand distributional hypothesis based approaches. The former is approaches based on matching lexico-syntactic patterns which convey taxonomic relations in a corpus (Hearst, 1992; Iwanska et al., 2000), and the latter is statistical approaches based on the distribution of context in corpus (Cimiano et al., 2005; Yamamoto et al., 2005; Sanderson &amp; Croft, 1999). The former features a high precision and low recall compared to the latter. The quality of learned relations is higher than those of statistical approaches, while the patterns are rarely applied in real corpus. It is also difficult to improve performance of pattern based approaches because they are simple and clear. So, many researches have been focused on raising precision of statistical approaches. We introduce new distributional hypothesis based taxonomy learning method using term specificity and term similarity. Term specificity is a measure of information quantity of terms in given doma</context>
<context position="9872" citStr="Sanderson &amp; Croft, 1999" startWordPosition="1578" endWordPosition="1581"> example, ‘inventory’1, in Figure 2, shows a tendency to be objects of specific verbs like ‘increase’ and ‘reduce’. This feature was used in (Cimiano et al., 2005) to learn concept hierarchy. Inversed specificity of a term can be measured by entropy of verbargument relations as Eq. (3). Specvarg (t) −1 = −∑ P(t |varg) log P(t |varg) (3 )varg where P(t|varg), the probability that t is argument of varg, is estimated as freq(t,varg)/freq(varg). The entropy is the average information quantity of all (t,varg) pairs for term t. Conditional probability of term co-occurrence in documents was used in (Sanderson &amp; Croft, 1999) to build term taxonomy. This statistics is based on the assumption that, for two terms, ti and tj, ti is said to subsume tj if the following two conditions hold, P(ti|tj) = 1 and P(tj|ti)&lt;1 (4) In other words, ti subsumes tj if the documents which tj occurs in are a subset of the documents which ti occurs in, therefore ti can be parent of tj in taxonomy. Although a good number of term pairs are found that adhere to the two subsump1 ‘Inventory’ consists of a list of goods and materials held available in stock (http://en.wikipedia.org/wiki/Inventory). We apply this function to calculate term sp</context>
<context position="12411" citStr="Sanderson &amp; Croft, 1999" startWordPosition="2028" endWordPosition="2031"> j log (7) in i P(ti)P(wj ) This equation indicates that wj which is highly associated to ti contributes specificity of ti. For example, ‘debenture bond’ is more specific concept than ‘financial product’. Intuitively, ‘debenture’ is highly associated to ‘debenture bond’ Speccoldoc (6) w W j ∈ 43 compared with ‘bond’ to ‘debenture bond’ or ‘financial’, ‘product’ to ‘financial product’. 3 Term Similarity We evaluate four statistical and lexical features, related to taxonomy learning, in view of term similarity. Three statistical features have been used in existing taxonomy learning researches. (Sanderson &amp; Croft, 1999) used conditional probability of co-occurring terms in same document in taxonomy learning process as shown in Eq. (4). This feature can be used to measure similarity of terms. If two terms co-occur in common documents, they are semantically similar to each other. Based on this assumption, we can calculate term similarity by comparing the frequency of co-occurring ti and tj together and the frequency of occurring ti and tj independently, as Eq. (8). 2* ( , ) df t t i j Sim t t coldoc i j ( , ) = df t df t ( ) ( ) + i j where df(ti,tj) is number of documents in which both ti and tj co-occur, df(</context>
</contexts>
<marker>Sanderson, Croft, 1999</marker>
<rawString>Mark Sanderson and Bruce Croft. 1999. Deriving concept hierarchies from text. Proceedings of the 22th Annual ACM S1GIR Conference on Research and Development in Information Retrieval, pp. 206-213, 1999</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
</authors>
<date>1972</date>
<journal>Exhausitivity and Specificity Journal of Documentation</journal>
<volume>28</volume>
<pages>11--21</pages>
<contexts>
<context position="5650" citStr="Jones, 1972" startWordPosition="890" endWordPosition="891">taxonomy learning method is discussed in Section 4. Experiment and evaluation are discussed in Section 5, and finally, conclusions are drawn in Section 6. 2 Term Specificity Specificity is degree of detailed information of an object about given target object. For example, if an encyclopedia contains detailed information about ‘IT domain’, then the encyclopedia is ‘IT specific encyclopedia’. In this context, specificity is a function of objects and target object to real number. Traditionally term specificity is widely used in information retrieval systems to weight index terms in documents (S. Jones, 1972; Aizawa, 2003; Wong &amp; Yao, 1992). In information retrieval context, term specificity is function of index terms and documents. On the other hand, term specificity is the function of terms and target domains in taxonomy learning context (Ryu &amp; Choi 2005). Term specificity to a domain is Spec(t |D) R+ ∈ (1) where t is a term, and Spec(t|D) is the specificity of t in a given domain D. We simply use Spec(t) instead of Spec(t|D) assuming a particular domain D in this paper. Understanding the relation between domain concepts and their lexicalization methods is needed, before we describe term specif</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sparck Jones. 1972. Exhausitivity and Specificity Journal of Documentation Vol. 28, Num. 1, pp. 11-21</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>Y Y Yao</author>
</authors>
<title>An InformationTheoretic Measure of Term Specificity,</title>
<date>1992</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>43</volume>
<pages>54--61</pages>
<contexts>
<context position="5683" citStr="Wong &amp; Yao, 1992" startWordPosition="895" endWordPosition="898"> discussed in Section 4. Experiment and evaluation are discussed in Section 5, and finally, conclusions are drawn in Section 6. 2 Term Specificity Specificity is degree of detailed information of an object about given target object. For example, if an encyclopedia contains detailed information about ‘IT domain’, then the encyclopedia is ‘IT specific encyclopedia’. In this context, specificity is a function of objects and target object to real number. Traditionally term specificity is widely used in information retrieval systems to weight index terms in documents (S. Jones, 1972; Aizawa, 2003; Wong &amp; Yao, 1992). In information retrieval context, term specificity is function of index terms and documents. On the other hand, term specificity is the function of terms and target domains in taxonomy learning context (Ryu &amp; Choi 2005). Term specificity to a domain is Spec(t |D) R+ ∈ (1) where t is a term, and Spec(t|D) is the specificity of t in a given domain D. We simply use Spec(t) instead of Spec(t|D) assuming a particular domain D in this paper. Understanding the relation between domain concepts and their lexicalization methods is needed, before we describe term specificity measuring methods. Domain s</context>
</contexts>
<marker>Wong, Yao, 1992</marker>
<rawString>S.K.M. Wong, Y.Y. Yao. 1992. An InformationTheoretic Measure of Term Specificity, Journal of the American Society for Information Science, Vol. 43, Num. 1. pp.54-61</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISO</author>
</authors>
<title>Terminology work-Principle and methods.</title>
<date>2000</date>
<booktitle>ISO 704 Second Edition</booktitle>
<contexts>
<context position="6694" citStr="ISO, 2000" startWordPosition="1071" endWordPosition="1072">ing a particular domain D in this paper. Understanding the relation between domain concepts and their lexicalization methods is needed, before we describe term specificity measuring methods. Domain specific concepts can be distinguished by a set of what we call ‘characteristics’. More specific concepts are created by adding characteristics to the set of characteristics of existing concepts. Let us consider two concepts: C1 and C2. C1 is an existing concept and C2 is a newly created concept by combining new characteristics to the characteristic set of C1. In this case, C1 is an ancestor of C2 (ISO, 2000). When domain specific concepts are lexicalized as terms, the terms&apos; word-formation is classified into two categories based on the composition of component words. In the first category, new terms are created by adding modifiers to existing terms. Figure 2 shows a subtree of financial ontology. For example ‘current asset’ was created by adding the modifier ‘current’ to its hypernym ‘asset’. In this case, inside information is a good evidence to represent the characteristics. In the second category, new terms are created independently of existing terms. For example, ‘cache’, ‘inventory’, and ‘re</context>
</contexts>
<marker>ISO, 2000</marker>
<rawString>ISO 704. 2000. Terminology work-Principle and methods. ISO 704 Second Edition</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aizawa</author>
</authors>
<title>An information-theoretic perspective of tf-idf measures.</title>
<date>2003</date>
<journal>Journal of Information Processing and Management,</journal>
<volume>39</volume>
<contexts>
<context position="5664" citStr="Aizawa, 2003" startWordPosition="892" endWordPosition="894">ning method is discussed in Section 4. Experiment and evaluation are discussed in Section 5, and finally, conclusions are drawn in Section 6. 2 Term Specificity Specificity is degree of detailed information of an object about given target object. For example, if an encyclopedia contains detailed information about ‘IT domain’, then the encyclopedia is ‘IT specific encyclopedia’. In this context, specificity is a function of objects and target object to real number. Traditionally term specificity is widely used in information retrieval systems to weight index terms in documents (S. Jones, 1972; Aizawa, 2003; Wong &amp; Yao, 1992). In information retrieval context, term specificity is function of index terms and documents. On the other hand, term specificity is the function of terms and target domains in taxonomy learning context (Ryu &amp; Choi 2005). Term specificity to a domain is Spec(t |D) R+ ∈ (1) where t is a term, and Spec(t|D) is the specificity of t in a given domain D. We simply use Spec(t) instead of Spec(t|D) assuming a particular domain D in this paper. Understanding the relation between domain concepts and their lexicalization methods is needed, before we describe term specificity measurin</context>
</contexts>
<marker>Aizawa, 2003</marker>
<rawString>A. Aizawa. 2003. An information-theoretic perspective of tf-idf measures. Journal of Information Processing and Management, vol. 39</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based Measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics.</journal>
<volume>32</volume>
<pages>13--47</pages>
<contexts>
<context position="20851" citStr="Budanitsky &amp; Hirst, 2006" startWordPosition="3416" endWordPosition="3420">) to take advantages of both inside and outside information. Harmonic mean of two specificity values was used in Specin/adj method. Specin/adj showed the highest F-measure because precision was higher than that of Specin and recall was equal to that of Specin. Table 1. Precision, recall and F-measure for term specificity Method Precision Recall F-measure Specadj 0.795 0.609 0.689 Specvarg 0.663 0.702 0.682 Speccoldoc 0.717 0.702 0.709 Specin 0.728 0.907 0.808 Specin/adj 0.731 0.907 0.810 5.2 Term Similarity We evaluated similarity measures by comparing with taxonomy based similarity measure. (Budanitsky &amp; Hirst, 2006) calculated correlation coefficients (CC) between human similarity ratings and the five WordNet based similarity measures. Among the five computational measures, (Leacock &amp; Chodorow, 1998)’s method showed the highest correlation coefficients, even though all of the measures showed similar ranging from 0.74 to 0.85. This result means that taxonomy based similarity is highly correlated to human similarity ratings. We can indirectly evaluate our similarity measures by comparing to taxonomy based similarity measure, instead of direct comparison to human rating. If applied similarity measure is qua</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky, Graeme Hirst. 2006 Evaluating WordNet-based Measures of Lexical Semantic Relatedness. Computational Linguistics. Vol. 32 NO. 1, pp. 13-47(35)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database. The</booktitle>
<pages>265--283</pages>
<editor>In Christian Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="21039" citStr="Leacock &amp; Chodorow, 1998" startWordPosition="3443" endWordPosition="3446">n was higher than that of Specin and recall was equal to that of Specin. Table 1. Precision, recall and F-measure for term specificity Method Precision Recall F-measure Specadj 0.795 0.609 0.689 Specvarg 0.663 0.702 0.682 Speccoldoc 0.717 0.702 0.709 Specin 0.728 0.907 0.808 Specin/adj 0.731 0.907 0.810 5.2 Term Similarity We evaluated similarity measures by comparing with taxonomy based similarity measure. (Budanitsky &amp; Hirst, 2006) calculated correlation coefficients (CC) between human similarity ratings and the five WordNet based similarity measures. Among the five computational measures, (Leacock &amp; Chodorow, 1998)’s method showed the highest correlation coefficients, even though all of the measures showed similar ranging from 0.74 to 0.85. This result means that taxonomy based similarity is highly correlated to human similarity ratings. We can indirectly evaluate our similarity measures by comparing to taxonomy based similarity measure, instead of direct comparison to human rating. If applied similarity measure is qualified, the calculated similarity will be highly correlated to taxonomy based similarity. Leacock and Chodorow proposed following formula for computing the scaled semantic similarity betwe</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock, Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christian Fellbaum, editor, WordNet: An Electronic Lexical Database. The MIT Press, pp. 265-283</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pum-Mo Ryu</author>
<author>Key-Sun Choi</author>
</authors>
<title>An InformationTheoretic Approach to Taxonomy Extraction for Ontology Learning,</title>
<date>2005</date>
<booktitle>Ontology Learning from Text: Methods, Evaluation and Applications, Vol. 123, Frontiers in Artificial Intelligence and Applications, IOS</booktitle>
<editor>In P. Buitelaar et al. (eds.),</editor>
<publisher>Press</publisher>
<contexts>
<context position="2677" citStr="Ryu &amp; Choi, 2005" startWordPosition="403" endWordPosition="406">ations is higher than those of statistical approaches, while the patterns are rarely applied in real corpus. It is also difficult to improve performance of pattern based approaches because they are simple and clear. So, many researches have been focused on raising precision of statistical approaches. We introduce new distributional hypothesis based taxonomy learning method using term specificity and term similarity. Term specificity is a measure of information quantity of terms in given domain. When a term has much domain information, the term is highly specific to the domain, and vice versa (Ryu &amp; Choi, 2005). Because highly specific terms tend to locate in low level in domain taxonomy, term specificity can be used as a necessary condition for taxonomy learning. Term similarity is degree of semantic overlap among terms. When two terms share many common characteristics, they are semantically similar to each other. Term similarity can be another necessary condition for taxonomy learning, because semantically similar terms locate near by in given domain taxonomy. The two conditions are generally valid for terms in a taxonomic relation, while terms satisfying the conditions do not always have taxonomi</context>
<context position="5904" citStr="Ryu &amp; Choi 2005" startWordPosition="931" endWordPosition="934">et object. For example, if an encyclopedia contains detailed information about ‘IT domain’, then the encyclopedia is ‘IT specific encyclopedia’. In this context, specificity is a function of objects and target object to real number. Traditionally term specificity is widely used in information retrieval systems to weight index terms in documents (S. Jones, 1972; Aizawa, 2003; Wong &amp; Yao, 1992). In information retrieval context, term specificity is function of index terms and documents. On the other hand, term specificity is the function of terms and target domains in taxonomy learning context (Ryu &amp; Choi 2005). Term specificity to a domain is Spec(t |D) R+ ∈ (1) where t is a term, and Spec(t|D) is the specificity of t in a given domain D. We simply use Spec(t) instead of Spec(t|D) assuming a particular domain D in this paper. Understanding the relation between domain concepts and their lexicalization methods is needed, before we describe term specificity measuring methods. Domain specific concepts can be distinguished by a set of what we call ‘characteristics’. More specific concepts are created by adding characteristics to the set of characteristics of existing concepts. Let us consider two concep</context>
<context position="8051" citStr="Ryu &amp; Choi, 2005" startWordPosition="1280" endWordPosition="1283"> the characteristics of the terms. asset current asset fixed asset intangible cache inventory receivable asset Figure 2. Subtree of financial ontology There are many kinds of inside and outside information to be used in measuring term specificity. Distribution of adjective-term relation and verb-argument dependency relation are collocation based statistics. Distribution of adjectiveterm relation refers to the idea that specific nouns are rarely modified, while general nouns are fre42 quently modified in text. This feature has been discussed to measure specificity of nouns in (Caraballo, 1999; Ryu &amp; Choi, 2005) and to build taxonomy of Japanese nouns (Yamamoto et al., 2005). Inversed specificity of a term can be measured by entropy of adjectives as shown Eq. (2). tion conditions, it is noticed that many are just failing to be included because a few occurrences of the subsumed term, tj, does not co-occur with ti. Subsequently, the conditions are relaxed and subsume function is defined as Eq. (5). In case of P(ti|tj)&gt;P(tj|ti), subsume(ti,tj) returns 1, otherwise returns 0. Specadj (t) −1 = −∑ P adj t P adj t subsume (i, tj) r. 1 if P(ti |tj) &gt;P(tj |ti) (5) ( |)log ( |) (2) 0 otherwise adj where P(adj|</context>
<context position="20227" citStr="Ryu &amp; Choi, 2005" startWordPosition="3323" endWordPosition="3326">ion is less correlated to term specificity. Specin showed the highest recall because it measures term specificity using component words contrary to other methods. Speccoldoc showed comparable precision and recall. 3 http://www.daviddlewis.com/resources/testcollections/reute rs21578/ 4 http://www.connexor.com/ Spec(t4) = 2.0 Spec(t5) = 3.0 Spec(t2) = 1.5 t� t� to Spec(t6) = 2.4 t, t� t� t� t� t, Spec(t3) = 1.5 t1o Spec(tnew) = 2.3 t.., Low Specificity Spec(t7) = 4.0 Spec(t8) = 3.5 Spec(t9) = 2.5 Spec(t10) = 3.0 = Rspec = P spec 45 We harmonized Specin and Specadj to Specin/adj as described in (Ryu &amp; Choi, 2005) to take advantages of both inside and outside information. Harmonic mean of two specificity values was used in Specin/adj method. Specin/adj showed the highest F-measure because precision was higher than that of Specin and recall was equal to that of Specin. Table 1. Precision, recall and F-measure for term specificity Method Precision Recall F-measure Specadj 0.795 0.609 0.689 Specvarg 0.663 0.702 0.682 Speccoldoc 0.717 0.702 0.709 Specin 0.728 0.907 0.808 Specin/adj 0.731 0.907 0.810 5.2 Term Similarity We evaluated similarity measures by comparing with taxonomy based similarity measure. (B</context>
</contexts>
<marker>Ryu, Choi, 2005</marker>
<rawString>Pum-Mo Ryu, Key-Sun Choi. 2005. An InformationTheoretic Approach to Taxonomy Extraction for Ontology Learning, In P. Buitelaar et al. (eds.), Ontology Learning from Text: Methods, Evaluation and Applications, Vol. 123, Frontiers in Artificial Intelligence and Applications, IOS Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farid Cerbah</author>
</authors>
<title>Exogeneous and Endogeneous Approaches to Semantic Categorization of Unknown Technical Terms.</title>
<date>2000</date>
<booktitle>Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>145--151</pages>
<contexts>
<context position="4852" citStr="Cerbah, 2000" startWordPosition="764" endWordPosition="765">re such as adjectives of terms, verb-argument relation, or cooccurrence ratio in documents according to their methods. Firstly, we analyze characteristics of features for taxonomy learning in view of term specificity and term similarity to show that the features embed characteristics of specificity and similarity, and finally apply optimal features to our method. Additionally we tested inside information of terms to measure term specificity and similarity. As multiword terms cover the larger part of technical terms, lexical components are featuring information representing semantics of terms (Cerbah, 2000). The remainder of this paper is organized follows. Characteristics of term specificity are described in Section 2, while term similarity and its features are addressed in Section 3. Our taxonomy learning method is discussed in Section 4. Experiment and evaluation are discussed in Section 5, and finally, conclusions are drawn in Section 6. 2 Term Specificity Specificity is degree of detailed information of an object about given target object. For example, if an encyclopedia contains detailed information about ‘IT domain’, then the encyclopedia is ‘IT specific encyclopedia’. In this context, sp</context>
</contexts>
<marker>Cerbah, 2000</marker>
<rawString>Farid Cerbah. 2000. Exogeneous and Endogeneous Approaches to Semantic Categorization of Unknown Technical Terms. Proceedings of the 18th International Conference on Computational Linguistics, vol. 1, pp. 145-151</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>