<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008310">
<title confidence="0.984043">
A Scalable Probabilistic Classifier for Language Modeling
</title>
<author confidence="0.998838">
Joel Lang
</author>
<affiliation confidence="0.999677">
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.987912">
10 Crichton Street, Edinburgh EH8 9AB, UK
</address>
<email confidence="0.999061">
J.Lang-3@sms.ed.ac.uk
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998160588235294">
We present a novel probabilistic classifier,
which scales well to problems that involve a
large number of classes and require training on
large datasets. A prominent example of such a
problem is language modeling. Our classifier
is based on the assumption that each feature
is associated with a predictive strength, which
quantifies how well the feature can predict the
class by itself. The predictions of individual
features can then be combined according to
their predictive strength, resulting in a model,
whose parameters can be reliably and effi-
ciently estimated. We show that a generative
language model based on our classifier consis-
tently matches modified Kneser-Ney smooth-
ing and can outperform it if sufficiently rich
features are incorporated.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998265">
A Language Model (LM) is an important compo-
nent within many natural language applications in-
cluding speech recognition and machine translation.
The task of a generative LM is to assign a probabil-
ity p(w) to a sequence of words w = w1 ... wL. It
is common to factorize this probability as
</bodyText>
<equation confidence="0.995552">
L
p(w) = p(wiIwi−N+1 ... wi−1) (1)
i=1
</equation>
<bodyText confidence="0.999959105263158">
Thus, the central problem that arises from this
formulation consists of estimating the probability
p(wijwi−N+1 . . . wi−1). This can be viewed as a
classification problem in which the target word Wi
corresponds to the class that must be predicted,
based on features extracted from the conditioning
context, e.g. a word occurring in the context.
This paper describes a novel approach for mod-
eling such conditional probabilities. We propose a
classifier which is based on the assumption that each
feature has a predictive strength, quantifying how
well the feature can predict the class (target word)
by itself. Then the predictions made by individual
features can be combined into a mixture model, in
which the prediction of each feature is weighted ac-
cording to its predictive strength. This reflects the
fact that certain features (e.g. certain context words)
are much more predictive than others but the pre-
dictive strength for a particular feature often doesn’t
vary much across classes and can thus be assumed
constant. The main advantage of our model is that it
is straightforward to incorporate rich features with-
out sacrificing scalability or reliability of parame-
ter estimation. In addition, it is simple to imple-
ment and no feature selection is required. Section 3
shows that a generative1 LM built with our classi-
fier is competitive to modified Kneser-Ney smooth-
ing and can outperform it if sufficiently rich features
are incorporated.
The classification-based approach to language
modeling was introduced by Rosenfeld (1996) who
proposed an optimized variant of the maximum-
entropy classifier (Berger et al., 1996) for the task.
Unfortunately, data sparsity resulting from the large
number of classes makes it difficult to obtain reli-
able parameter estimates, even on large datasets and
the high computational costs make it difficult train
models on large datasets in the first place2. Scal-
</bodyText>
<footnote confidence="0.999382714285714">
1While the classifier itself is discriminative, i.e. condition-
ing on the contextual features, the resulting LM is generative.
See Roark et al. (2007) for work on discriminative LMs.
2For example, using a vocabulary of 20000 words Rosen-
feld (1994) trained his model on up to 40M words, however
employing heavy feature pruning and indicating that “the com-
putational load, was quite severe for a system this size”.
</footnote>
<page confidence="0.945811">
625
</page>
<note confidence="0.5977095">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 625–630,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999581086956522">
ability is however very important, since moving to
larger datasets is often the simplest way to obtain
a better model. Similarly, neural probabilistic LMs
(Bengio et al., 2003) don’t scale very well to large
datasets. Even the more scalable variant proposed
by Mnih and Hinton (2008) is trained on a dataset
consisting of only 14M words, also using a vocabu-
lary of around 20000 words. Van den Bosch (2005)
proposes a decision-tree classifier which has been
applied to training datasets with more than 100M
words. However, his model is non-probabilistic and
thus a standard comparison with probabilistic mod-
els in terms of perplexity isn’t possible.
N-Gram models (Goodman, 2001) obtain esti-
mates for p(wi|wi_N+1 ... wi_1) using counts of
N-Grams. Because directly using the maximum-
likelihood estimate would result in poor predictions,
smoothing techniques are applied. A modified inter-
polated form of Kneser-Ney smoothing (Kneser and
Ney, 1995) was shown to consistently outperform
a variety of other smoothing techniques (Chen and
Goodman, 1999) and currently constitutes a state-
of-the-art3 generative LM.
</bodyText>
<sectionHeader confidence="0.976876" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.996959909090909">
We are concerned with estimating a probability dis-
tribution p(Y |x) over a categorical class variable
Y with range Y, conditional on a feature vector
x = (x1, ... , xM), containing the feature values xi
of M features. While generalizations are conceiv-
able, we will restrict the features Xk to be binary,
i.e. xk E 10, 11. For language modeling the class
variable Y corresponds to the target word Wi which
is to be predicted and thus ranges over all possible
words of some vocabulary. The binary input fea-
tures x are extracted from the conditioning context
wi_N+1 ... wi_1. The specific features we use for
language modeling are given in Section 3.
We assume sparse features, such that typically
only a small number of the binary features take value
1. These features are referred to as the active fea-
tures and predictions are based on them. We in-
troduce a bias feature which is active for every in-
stance, in order to ensure that the set of active fea-
tures is non-empty for each instance. Individually,
each active feature Xk is predictive of the class vari-
able and predicts the class through a categorical dis-
</bodyText>
<footnote confidence="0.97854575">
3The model of Wood et al. (2009) has somewhat higher per-
formance, however, again due to high computational costs the
model has only been trained on training sets of at most 14M
words.
</footnote>
<bodyText confidence="0.992455181818182">
tribution4 distribution, which we denote as p(Y |xk).
Since instances typically have several active features
the question is how to combine the individual pre-
dictions of these features into an overall prediction.
To this end we make the assumption that each fea-
ture Xk has a certain predictive strength θk E R,
where larger values indicate that the feature is more
likely to predict correctly. The individual predic-
tions can then be combined into a mixture model,
which weights individual predictions according to
their predictive strength:
</bodyText>
<equation confidence="0.998019">
p(Y |x, θ) = 1: vk(x)p(Y |xk) (2)
kEA(x)
</equation>
<bodyText confidence="0.706456">
where
</bodyText>
<equation confidence="0.9758715">
eek
vk(x) =
</equation>
<bodyText confidence="0.999781615384615">
Here A(x) denotes the index-set of active features
for instance (y, x). Note that since the set of active
features varies across instances, so do the mixing
proportions vk(x) and thus this is not a conventional
mixture model, but rather a variable one. We will
therefore refer to our model as the variable mixture
model (VMM). In particular, our model differs from
linear or log-linear interpolation models (Klakow,
1998), which combine a typically small number of
components that are common across instances.
In order to compare our model to the maximum-
entropy classifier and other (generalized) linear
models, it is beneficial to rewrite Equation 2 as
</bodyText>
<equation confidence="0.9939795">
= Q(x)β�φ(y, x)
1 (5)
</equation>
<bodyText confidence="0.993346">
where φj,k(y, x) is a sufficient statistics indicating
whether feature Xk is active and class y = yj and
</bodyText>
<equation confidence="0.996718666666667">
βj,k = eek+lo9 p(yj|xk) (6)
Q(x) = 1: eek (7)
kEA(x)
</equation>
<bodyText confidence="0.997125333333333">
Table 1 shows the main differences between the
VMM, the maximum-entropy classifier and the per-
ceptron (Collins, 2002).
</bodyText>
<footnote confidence="0.641279">
4commonly referred to as a multinomial distribution
</footnote>
<equation confidence="0.998945125">
E
kEA(x)
(3)
eek
M |Y|
1
p(Y = y|x, β) =
Q(x) � � φj,k(y, x)βj,k (4)
</equation>
<page confidence="0.913743">
626
</page>
<table confidence="0.7660966">
VMM Maximum Entropy Perceptron
β) = 1 β) = 1 score(y|x, β) = βTφ(y, x)
p(y|x, Q(x)βTφ(y, x) p(y|x, Q(x)eβ�φ(y,x)
Q(x) = �kEA(x) eθk Q(x) = E|Y|
j=1 eβTφ(yj,x)
</table>
<tableCaption confidence="0.995794">
Table 1: A comparison between the VMM, the maximum-entropy classifier and the perceptron. Like the perceptron
</tableCaption>
<bodyText confidence="0.96881775">
and in contrast to the maximum-entropy classifier, the VMM directly uses a predictor βTφ(y, x). For the VMM the
sufficient statistics φ(y, x) correspond to binary indicator variables and the parameters β are constrained according to
Equation 6. This results in a partition function Q(x) which can be efficiently computed, in contrast to the partition
function of the maximum-entropy classifier, which requires a summation over all classes.
</bodyText>
<subsectionHeader confidence="0.980025">
2.1 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.998229">
The VMM has two types of parameters:
</bodyText>
<listItem confidence="0.9665078">
1. the categorical parameters αj,k = p(yj|xk)
which determine the likelihood of class yj in
presence of feature Xk;
2. the parameters θk quantifying the predictive
strength of each feature Xk.
</listItem>
<bodyText confidence="0.999960375">
The two types of parameters are estimated from a
training dataset, consisting of instances (y(h), x(h)).
Parameter estimation proceeds in two separate
stages, resulting in a simple and efficient procedure.
In a first stage, the categorical parameters are com-
puted independently for each feature, as the maxi-
mum likelihood estimates, smoothed using absolute
discounting (Chen and Rosenfeld, 2000):
</bodyText>
<equation confidence="0.898419">
αj,k = p(yj|xk) =
</equation>
<bodyText confidence="0.9978575">
wherec&apos; j,k is the smoothed count of how many times
Y takes value yj when Xk is active, and ck is
the count of how many times Xk is active. The
smoothed count is computed as
</bodyText>
<equation confidence="0.946985">
�
cj,k − D if cj,k &gt; 0
c&apos;j,k = D&apos;NZk if cj,k = 0
Zk
</equation>
<bodyText confidence="0.999952357142857">
where cj,k is the raw count for class yj and fea-
ture Xk, NZk is the number of classes for which
the raw count is non-zero, and Zk is the number of
classes for which the raw count is zero. D is the
discount constant chosen in [0, 1]. The smoothing
thus subtracts D from each non-zero count and re-
distributes the so-obtained mass evenly amongst all
zero counts. If all counts are non-zero no mass is
redistributed.
Once the categorical parameters have been com-
puted, we proceed by estimating the predictive
strengths θ = (θ1, ... , θM). We can do so by con-
ducting a search for the parameter vector θ* which
maximizes the log-likelihood of the training data:
</bodyText>
<equation confidence="0.94903425">
θ* = arg max ll(θ)
θ
�= arg max
θ h
</equation>
<bodyText confidence="0.999804">
While any standard optimization method could
be applied, we use stochastic gradient ascent (SGA,
Bottou (2004)) as this results in a particularly conve-
nient and efficient procedure that requires only one
iteration over the data (see Section 3). SGA is an
online optimization method which iteratively com-
putes the gradient V for each instance and takes a
step of size η in the direction of that gradient:
</bodyText>
<equation confidence="0.988844666666667">
θ(t+1) , θ(t) + ηV (8)
The gradient V = (∂ll(h)
∂θ1 , . . . , ∂ll(h)
</equation>
<bodyText confidence="0.99871425">
∂θM ) computed for
SGA contains the first-order derivatives of the data
log-likelihood of a particular instance with respect
to the θ-parameters which are given by
</bodyText>
<equation confidence="0.98832425">
∂ vk(x)
log p(y|x, θ) = p(y|x, θ) [p(y|xk) − p(y|x, θ)]
∂θk
(9)
</equation>
<bodyText confidence="0.999938">
The resulting parameter-update Equation 8 has
the following intuitive interpretation. If the predic-
tion of a particular active feature Xk is higher than
the current overall prediction, the term in square
brackets in Equation 9 becomes positive and thus
the predictive strength θk for that feature is increased
and conversely for the case where the prediction is
below the overall prediction. The magnitude of the
</bodyText>
<equation confidence="0.716665333333333">
c&apos;j,k
ck
log p(y(h)|x(h), θ)
</equation>
<page confidence="0.915854">
627
</page>
<table confidence="0.999870235294117">
Type Extracted Features
Standard N-Grams (BA,SR,LR) * * * (bias)
Mr Thompson said
* Thompson said
* * said
Skip N-Grams (SR,LR) Mr * said *
Mr Thompson *
Mr * *
* Thompson
Unigram Bag Features (SR,LR) Mr
Thompson
said
Long-Range Unigram Bag Features (LR) Yesterday
at
the
press
conference
</table>
<tableCaption confidence="0.959961666666667">
Table 2: Feature types and examples for a model of order
N=4 and for the context Yesterday at the press
conference Mr Thompson said. For each fea-
</tableCaption>
<bodyText confidence="0.975251">
ture type we write in parentheses the feature sets which
include that type of feature. The wildcard symbol * is
used as a placeholder for arbitrary regular words. The
bias feature, which is active for each instance is written
as * * *. In standard N-Gram models the bias feature
corresponds to the unigram distribution.
update depends on how much overall and feature
prediction differ and on the scaling factor �k��
p������ .
In order to improve generalization, we estimate
the categorical parameters based on the counts from
all instances, except the one whose gradient is being
computed for the online update (leave-one-out). In
other words, we subtract the counts for a particular
instance before computing the update (Equation 8)
and add them back when the update has been ex-
ecuted. In total, training only requires two passes
over the data, as opposed to a single pass (plus
smoothing) required by N-Gram models.
</bodyText>
<sectionHeader confidence="0.999628" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99330425">
All experiments were conducted using the SRI Lan-
guage Modeling Toolkit (SRILM, Stolcke (2002)),
i.e. we implemented5 the VMM within SRILM and
compared to default N-Gram models supplied with
SRILM. The experiments were run on a 64-bit, 2.2
GHz dual-core machine with 8GB RAM.
Data The experiments were carried out on data
from the Reuters Corpus Version 1 (Lewis et al.,
</bodyText>
<footnote confidence="0.737401">
5The code can be downloaded from http://code.
google.com/p/variable-mixture-model.
</footnote>
<bodyText confidence="0.99989232">
2004), which was split into sentences, tokenized and
converted to lower case, not removing punctuation.
All our models were built with the same 30367-
word vocabulary, which includes the sentence-end
symbol and a special symbol for out-of-vocabulary
words (UNK). The vocabulary was compiled by se-
lecting all words which occur more than four times
in the data of week 31, which was not otherwise
used for training or testing. As development set we
used the articles of week 50 (4.1M words) and as
test set the articles of week 51 (3.8M words). For
training we used datasets of four different sizes: D1
(week 1, 3.1M words), D2 (weeks 1-3, 10M words),
D3 (weeks 1-10, 37M words) and D4 (weeks 1-30,
113M words).
Features We use three different feature sets in our
experiments. The first feature set (basic, BA) con-
sists of all features also used in standard N-Gram
models, i.e. all subsequences up to a length N − 1
immediately preceding the target word. The sec-
ond feature set (short-range, SR) consists of all ba-
sic features as well as all skip N-Grams (Ney et al.,
1994) that can be formed with the N −1 length con-
text. Moreover, all words occurring in the context
are included as bag features, i.e. as features which
indicate the occurrence of a word but not the partic-
ular position. The third feature set (long-range, LR)
is an extension of SR which also includes longer-
distance features. Specifically, this feature set ad-
ditionally includes all unigram bag features up to a
distance d = 9. The feature types and examples of
extracted features are given in Table 2.
Model Comparison We compared the VMM to
modified Kneser-Ney (KN, see Section 1). The or-
der of a VMM is defined through the length of the
context from which the basic and short-range fea-
tures are extracted. In particular, VM-BA of a cer-
tain order uses the same features as the N-Gram
models of the same order and VM-SR uses the same
conditioning context as the N-Gram models of the
same order. VM-LR in addition contains longer-
distance features, beyond the order of the corre-
sponding N-Gram models. The order of the models
was varied between N = 2 ... 5, however, for the
larger two datasets D3 and D4 the order 5 models
would not fit into the available RAM which is why
for order 5 we can only report scores for D1 and D2.
We could resort to pruning, but since this would have
an effect on performance it would invalidate a direct
comparison, which we want to avoid.
</bodyText>
<page confidence="0.996203">
628
</page>
<table confidence="0.999938272727273">
Model N D1 D2 D3 D4
3.1M 10M 37M 113M
2 209.2 178.2 155.3 139.3
3 164.9 127.7 98.9 78.1
KN
4 160.9 122.2 91.4 68.4
5 164.5 124.6 – –
2 217.9 209.8 162.8 144.7
3 174.1 159.7 114.3 87.3
VM-BA
4 164.9 147.7 102.7 78.2
5 163.2 144.2 – –
2 215.1 210.1 161.9 144.4
3 180.1 137.3 112.7 84.6
VM-SR
4 157.8 117.7 94.8 68.8
5 147.8 109.7 – –
2 207.5 170.8 147.4 128.2
3 160.6 124.7 103.2 79.3
VM-LR
4 146.7 112.1 89.8 66.0
5 141.4 107.1 – –
</table>
<tableCaption confidence="0.9973715">
Table 3: The test set perplexities of the models for orders
N=2..5 on training datasets D1-D4.
</tableCaption>
<bodyText confidence="0.999641710526316">
Model Parametrization We used the develop-
ment set to determine the values for the absolute dis-
counting parameter D (defined in Section 2.1) and
the number of iterations for stochastic gradient as-
cent. This resulted in a value D = 0.1. Stochas-
tic gradient yields best results with a single pass
through all instances. More iterations result in over-
fitting, i.e. decrease training data log-likelihood but
increase the log-likelihood on the development data.
The step size was kept fixed at q = 1.0.
Results The results of our experiments are given
in Table 3, which shows that for sufficiently high
orders VM-SR matches KN on each dataset. As ex-
pected, the VMM’s strength partly stems from the
fact that compared to KN it makes better use of
the information contained in the conditioning con-
text, as indicated by the fact that VM-SR matches
KN whereas VM-BA doesn’t. At orders 4 and 5,
VM-LR outperforms KN on all datasets, bringing
improvements of around 10% for the two smaller
training datasets D1 and D2. Comparing VM-BA
and VM-SR at order 4 we see that the 7 additional
features used by VM-SR for every instance signifi-
cantly improve performance and the long-range fea-
tures further improve performance. Thus richer fea-
ture sets consistently lead to higher model accuracy.
Similarly, the performance of the VMM improves as
one moves to higher orders, thereby increasing the
amount of contextual information. For orders 2 and
3 VM-SR is inferior to KN, because the SR feature
set at order 2 contains no additional features over
KN and at order 3 it only contains one additional
feature per instance. At order 4 VM-SR matches
KN and, while KN gets worse at order 5, the VMM
improves and outperforms KN by around 14%.
The training time (including disk IO) of the or-
der 4 VM-SR on the largest dataset D4 is about 30
minutes, whereas KN takes about 6 minutes to train.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.985359589743589">
The main contribution of this paper consists of a
novel probabilistic classifier, the VMM, which is
based on the idea of combining predictions made by
individual features into a mixture model whose com-
ponents vary from instance to instance and whose
mixing proportions reflect the predictive strength of
each component. The main advantage of the VMM
is that it is straightforward to incorporate rich fea-
tures without sacrificing scalability or reliability of
parameter estimation. Moreover, the VMM is sim-
ple to implement and works ‘out-of-the-box’ with-
out feature selection, or any special tuning or tweak-
ing.
Applied to language modeling, the VMM re-
sults in a state-of-the-art generative language model
whose relative performance compared to N-Gram
models gets better as one incorporates richer fea-
ture sets. It scales almost as well to large datasets
as standard N-Gram models: training requires only
two passes over the data as opposed to a single pass
required by N-Gram models. Thus, the experiments
provide empirical evidence that the VMM is based
on a reasonable set of modeling assumptions, which
translate into an accurate and scalable model.
Future work includes further evaluation of the
VMM, e.g. as a language model within a speech
recognition or machine translation system. More-
over, optimizing memory usage, for example via
feature pruning or randomized algorithms, would al-
low incorporation of richer feature sets and would
likely lead to further improvements, as indicated by
the experiments in this paper. We also intend to eval-
uate the performance of the VMM on other lexical
prediction tasks and more generally, on other classi-
fication tasks with similar characteristics.
Acknowledgments I would like to thank
Mirella Lapata and Charles Sutton for their
feedback on this work and Abby Levenberg for the
preprocessed datasets.
</bodyText>
<page confidence="0.998779">
629
</page>
<sectionHeader confidence="0.995882" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928714285715">
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of
Machine Learning Research, 3:1137–1155.
A. Berger, V. Della Pietra, and S. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1):39–71.
L. Bottou. 2004. Stochastic Learning. In Advanced
Lectures on Machine Learning, Lecture Notes in Ar-
tificial Intelligence, pages 146–168. Springer Verlag,
Berlin/Heidelberg.
S. Chen and J. Goodman. 1999. An Empirical Study of
Smoothing Techniques for Language Modeling. Com-
puter Speech and Language, 13:359–394.
S. Chen and R. Rosenfeld. 2000. A Survey of Smooth-
ing Techniques for ME Models. IEEE Transactions on
Speech and Audio Processing, 8(1):37–50.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–8, Philadelphia, PA, USA.
J. Goodman. 2001. A Bit of Progress in Language Mod-
eling (Extended Version). Technical report, Microsoft
Research, Redmond, WA, USA.
D. Klakow. 1998. Log-Linear Interpolation of Language
Models. In Proceedings of the 5th International Con-
ference on Spoken Language Processing, pages 1694–
1698, Sydney, Australia.
R. Kneser and H. Ney. 1995. Improved Backing-off
for M-Gram Language Modeling. In Proceedings of
the International Conference on Acoustics, Speech and
Signal Processing, pages 181–184, Detroit, MI, USA.
D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1:
A New Benchmark Collection for Text Categorization
Research. Journal of Machine Learning Research,
5:361–397.
A. Mnih and G. Hinton. 2008. A Scalable Hierarchical
Distributed Language Model. In Advances in Neural
Information Processing Systems 21.
H. Ney, U. Essen, and R. Kneser. 1994. On Structur-
ing Probabilistic Dependences in Stochastic Language
Modeling. Computer, Speech and Language, 8:1–38.
B. Roark, M. Saraclar, and M. Collins. 2007. Discrimi-
native n-gram Language Modeling. Computer, Speech
and Language, 21:373–392.
R. Rosenfeld. 1994. Adaptive Statistical Language Mod-
elling: A Maximum Entropy Approach. Ph.D. thesis,
Carnegie Mellon University.
R. Rosenfeld. 1996. A Maximum Entropy Approach to
Adaptive Statistical Language Modeling. Computer,
Speech and Language, 10:187–228.
A. Stolcke. 2002. SRILM – An Extensible Language
Modeling Toolkit. In Proceedings of the 7th Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, CO, USA.
A. Van den Bosch. 2005. Scalable Classification-based
Word Prediction and Confusible Correction. Traite-
mentAutomatique des Langues, 42(2):39–63.
F. Wood, C. Archambeau, J. Gasthaus, L. James, and
Y. Teh. 2009. A Stochastic Memoizer for Sequence
Data. In Proceedings of the 24th International Con-
ference on Machine learning, pages 1129–1136, Mon-
treal, Quebec, Canada.
</reference>
<page confidence="0.997633">
630
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.849675">
<title confidence="0.999936">A Scalable Probabilistic Classifier for Language Modeling</title>
<author confidence="0.992336">Joel</author>
<affiliation confidence="0.997016">Institute for Language, Cognition and School of Informatics, University of</affiliation>
<address confidence="0.86576">10 Crichton Street, Edinburgh EH8 9AB,</address>
<email confidence="0.990718">J.Lang-3@sms.ed.ac.uk</email>
<abstract confidence="0.999709666666667">We present a novel probabilistic classifier, which scales well to problems that involve a large number of classes and require training on large datasets. A prominent example of such a problem is language modeling. Our classifier is based on the assumption that each feature is associated with a predictive strength, which quantifies how well the feature can predict the class by itself. The predictions of individual features can then be combined according to their predictive strength, resulting in a model, whose parameters can be reliably and efficiently estimated. We show that a generative language model based on our classifier consistently matches modified Kneser-Ney smoothing and can outperform it if sufficiently rich features are incorporated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4028" citStr="Bengio et al., 2003" startWordPosition="627" endWordPosition="630">ive LMs. 2For example, using a vocabulary of 20000 words Rosenfeld (1994) trained his model on up to 40M words, however employing heavy feature pruning and indicating that “the computational load, was quite severe for a system this size”. 625 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 625–630, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ability is however very important, since moving to larger datasets is often the simplest way to obtain a better model. Similarly, neural probabilistic LMs (Bengio et al., 2003) don’t scale very well to large datasets. Even the more scalable variant proposed by Mnih and Hinton (2008) is trained on a dataset consisting of only 14M words, also using a vocabulary of around 20000 words. Van den Bosch (2005) proposes a decision-tree classifier which has been applied to training datasets with more than 100M words. However, his model is non-probabilistic and thus a standard comparison with probabilistic models in terms of perplexity isn’t possible. N-Gram models (Goodman, 2001) obtain estimates for p(wi|wi_N+1 ... wi_1) using counts of N-Grams. Because directly using the ma</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>V Della Pietra</author>
<author>S Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2959" citStr="Berger et al., 1996" startWordPosition="463" endWordPosition="466">e assumed constant. The main advantage of our model is that it is straightforward to incorporate rich features without sacrificing scalability or reliability of parameter estimation. In addition, it is simple to implement and no feature selection is required. Section 3 shows that a generative1 LM built with our classifier is competitive to modified Kneser-Ney smoothing and can outperform it if sufficiently rich features are incorporated. The classification-based approach to language modeling was introduced by Rosenfeld (1996) who proposed an optimized variant of the maximumentropy classifier (Berger et al., 1996) for the task. Unfortunately, data sparsity resulting from the large number of classes makes it difficult to obtain reliable parameter estimates, even on large datasets and the high computational costs make it difficult train models on large datasets in the first place2. Scal1While the classifier itself is discriminative, i.e. conditioning on the contextual features, the resulting LM is generative. See Roark et al. (2007) for work on discriminative LMs. 2For example, using a vocabulary of 20000 words Rosenfeld (1994) trained his model on up to 40M words, however employing heavy feature pruning</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, V. Della Pietra, and S. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
</authors>
<title>Stochastic Learning.</title>
<date>2004</date>
<booktitle>In Advanced Lectures on Machine Learning, Lecture Notes in Artificial Intelligence,</booktitle>
<pages>146--168</pages>
<publisher>Springer Verlag, Berlin/Heidelberg.</publisher>
<contexts>
<context position="10345" citStr="Bottou (2004)" startWordPosition="1703" endWordPosition="1704"> is the discount constant chosen in [0, 1]. The smoothing thus subtracts D from each non-zero count and redistributes the so-obtained mass evenly amongst all zero counts. If all counts are non-zero no mass is redistributed. Once the categorical parameters have been computed, we proceed by estimating the predictive strengths θ = (θ1, ... , θM). We can do so by conducting a search for the parameter vector θ* which maximizes the log-likelihood of the training data: θ* = arg max ll(θ) θ �= arg max θ h While any standard optimization method could be applied, we use stochastic gradient ascent (SGA, Bottou (2004)) as this results in a particularly convenient and efficient procedure that requires only one iteration over the data (see Section 3). SGA is an online optimization method which iteratively computes the gradient V for each instance and takes a step of size η in the direction of that gradient: θ(t+1) , θ(t) + ηV (8) The gradient V = (∂ll(h) ∂θ1 , . . . , ∂ll(h) ∂θM ) computed for SGA contains the first-order derivatives of the data log-likelihood of a particular instance with respect to the θ-parameters which are given by ∂ vk(x) log p(y|x, θ) = p(y|x, θ) [p(y|xk) − p(y|x, θ)] ∂θk (9) The resul</context>
</contexts>
<marker>Bottou, 2004</marker>
<rawString>L. Bottou. 2004. Stochastic Learning. In Advanced Lectures on Machine Learning, Lecture Notes in Artificial Intelligence, pages 146–168. Springer Verlag, Berlin/Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling. Computer Speech and Language,</title>
<date>1999</date>
<pages>13--359</pages>
<contexts>
<context position="4898" citStr="Chen and Goodman, 1999" startWordPosition="762" endWordPosition="765">on-tree classifier which has been applied to training datasets with more than 100M words. However, his model is non-probabilistic and thus a standard comparison with probabilistic models in terms of perplexity isn’t possible. N-Gram models (Goodman, 2001) obtain estimates for p(wi|wi_N+1 ... wi_1) using counts of N-Grams. Because directly using the maximumlikelihood estimate would result in poor predictions, smoothing techniques are applied. A modified interpolated form of Kneser-Ney smoothing (Kneser and Ney, 1995) was shown to consistently outperform a variety of other smoothing techniques (Chen and Goodman, 1999) and currently constitutes a stateof-the-art3 generative LM. 2 Model We are concerned with estimating a probability distribution p(Y |x) over a categorical class variable Y with range Y, conditional on a feature vector x = (x1, ... , xM), containing the feature values xi of M features. While generalizations are conceivable, we will restrict the features Xk to be binary, i.e. xk E 10, 11. For language modeling the class variable Y corresponds to the target word Wi which is to be predicted and thus ranges over all possible words of some vocabulary. The binary input features x are extracted from </context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>S. Chen and J. Goodman. 1999. An Empirical Study of Smoothing Techniques for Language Modeling. Computer Speech and Language, 13:359–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfeld</author>
</authors>
<title>A Survey of Smoothing Techniques for ME Models.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="9297" citStr="Chen and Rosenfeld, 2000" startWordPosition="1497" endWordPosition="1500">The VMM has two types of parameters: 1. the categorical parameters αj,k = p(yj|xk) which determine the likelihood of class yj in presence of feature Xk; 2. the parameters θk quantifying the predictive strength of each feature Xk. The two types of parameters are estimated from a training dataset, consisting of instances (y(h), x(h)). Parameter estimation proceeds in two separate stages, resulting in a simple and efficient procedure. In a first stage, the categorical parameters are computed independently for each feature, as the maximum likelihood estimates, smoothed using absolute discounting (Chen and Rosenfeld, 2000): αj,k = p(yj|xk) = wherec&apos; j,k is the smoothed count of how many times Y takes value yj when Xk is active, and ck is the count of how many times Xk is active. The smoothed count is computed as � cj,k − D if cj,k &gt; 0 c&apos;j,k = D&apos;NZk if cj,k = 0 Zk where cj,k is the raw count for class yj and feature Xk, NZk is the number of classes for which the raw count is non-zero, and Zk is the number of classes for which the raw count is zero. D is the discount constant chosen in [0, 1]. The smoothing thus subtracts D from each non-zero count and redistributes the so-obtained mass evenly amongst all zero co</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>S. Chen and R. Rosenfeld. 2000. A Survey of Smoothing Techniques for ME Models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="7811" citStr="Collins, 2002" startWordPosition="1262" endWordPosition="1263">lar, our model differs from linear or log-linear interpolation models (Klakow, 1998), which combine a typically small number of components that are common across instances. In order to compare our model to the maximumentropy classifier and other (generalized) linear models, it is beneficial to rewrite Equation 2 as = Q(x)β�φ(y, x) 1 (5) where φj,k(y, x) is a sufficient statistics indicating whether feature Xk is active and class y = yj and βj,k = eek+lo9 p(yj|xk) (6) Q(x) = 1: eek (7) kEA(x) Table 1 shows the main differences between the VMM, the maximum-entropy classifier and the perceptron (Collins, 2002). 4commonly referred to as a multinomial distribution E kEA(x) (3) eek M |Y| 1 p(Y = y|x, β) = Q(x) � � φj,k(y, x)βj,k (4) 626 VMM Maximum Entropy Perceptron β) = 1 β) = 1 score(y|x, β) = βTφ(y, x) p(y|x, Q(x)βTφ(y, x) p(y|x, Q(x)eβ�φ(y,x) Q(x) = �kEA(x) eθk Q(x) = E|Y| j=1 eβTφ(yj,x) Table 1: A comparison between the VMM, the maximum-entropy classifier and the perceptron. Like the perceptron and in contrast to the maximum-entropy classifier, the VMM directly uses a predictor βTφ(y, x). For the VMM the sufficient statistics φ(y, x) correspond to binary indicator variables and the parameters β </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1–8, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>A Bit of Progress in Language Modeling (Extended Version).</title>
<date>2001</date>
<tech>Technical report, Microsoft Research,</tech>
<location>Redmond, WA, USA.</location>
<contexts>
<context position="4530" citStr="Goodman, 2001" startWordPosition="710" endWordPosition="711">ts is often the simplest way to obtain a better model. Similarly, neural probabilistic LMs (Bengio et al., 2003) don’t scale very well to large datasets. Even the more scalable variant proposed by Mnih and Hinton (2008) is trained on a dataset consisting of only 14M words, also using a vocabulary of around 20000 words. Van den Bosch (2005) proposes a decision-tree classifier which has been applied to training datasets with more than 100M words. However, his model is non-probabilistic and thus a standard comparison with probabilistic models in terms of perplexity isn’t possible. N-Gram models (Goodman, 2001) obtain estimates for p(wi|wi_N+1 ... wi_1) using counts of N-Grams. Because directly using the maximumlikelihood estimate would result in poor predictions, smoothing techniques are applied. A modified interpolated form of Kneser-Ney smoothing (Kneser and Ney, 1995) was shown to consistently outperform a variety of other smoothing techniques (Chen and Goodman, 1999) and currently constitutes a stateof-the-art3 generative LM. 2 Model We are concerned with estimating a probability distribution p(Y |x) over a categorical class variable Y with range Y, conditional on a feature vector x = (x1, ... </context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>J. Goodman. 2001. A Bit of Progress in Language Modeling (Extended Version). Technical report, Microsoft Research, Redmond, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klakow</author>
</authors>
<title>Log-Linear Interpolation of Language Models.</title>
<date>1998</date>
<booktitle>In Proceedings of the 5th International Conference on Spoken Language Processing,</booktitle>
<pages>1694--1698</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="7281" citStr="Klakow, 1998" startWordPosition="1172" endWordPosition="1173"> predictions can then be combined into a mixture model, which weights individual predictions according to their predictive strength: p(Y |x, θ) = 1: vk(x)p(Y |xk) (2) kEA(x) where eek vk(x) = Here A(x) denotes the index-set of active features for instance (y, x). Note that since the set of active features varies across instances, so do the mixing proportions vk(x) and thus this is not a conventional mixture model, but rather a variable one. We will therefore refer to our model as the variable mixture model (VMM). In particular, our model differs from linear or log-linear interpolation models (Klakow, 1998), which combine a typically small number of components that are common across instances. In order to compare our model to the maximumentropy classifier and other (generalized) linear models, it is beneficial to rewrite Equation 2 as = Q(x)β�φ(y, x) 1 (5) where φj,k(y, x) is a sufficient statistics indicating whether feature Xk is active and class y = yj and βj,k = eek+lo9 p(yj|xk) (6) Q(x) = 1: eek (7) kEA(x) Table 1 shows the main differences between the VMM, the maximum-entropy classifier and the perceptron (Collins, 2002). 4commonly referred to as a multinomial distribution E kEA(x) (3) eek</context>
</contexts>
<marker>Klakow, 1998</marker>
<rawString>D. Klakow. 1998. Log-Linear Interpolation of Language Models. In Proceedings of the 5th International Conference on Spoken Language Processing, pages 1694– 1698, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved Backing-off for M-Gram Language Modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<location>Detroit, MI, USA.</location>
<contexts>
<context position="4796" citStr="Kneser and Ney, 1995" startWordPosition="747" endWordPosition="750">nly 14M words, also using a vocabulary of around 20000 words. Van den Bosch (2005) proposes a decision-tree classifier which has been applied to training datasets with more than 100M words. However, his model is non-probabilistic and thus a standard comparison with probabilistic models in terms of perplexity isn’t possible. N-Gram models (Goodman, 2001) obtain estimates for p(wi|wi_N+1 ... wi_1) using counts of N-Grams. Because directly using the maximumlikelihood estimate would result in poor predictions, smoothing techniques are applied. A modified interpolated form of Kneser-Ney smoothing (Kneser and Ney, 1995) was shown to consistently outperform a variety of other smoothing techniques (Chen and Goodman, 1999) and currently constitutes a stateof-the-art3 generative LM. 2 Model We are concerned with estimating a probability distribution p(Y |x) over a categorical class variable Y with range Y, conditional on a feature vector x = (x1, ... , xM), containing the feature values xi of M features. While generalizations are conceivable, we will restrict the features Xk to be binary, i.e. xk E 10, 11. For language modeling the class variable Y corresponds to the target word Wi which is to be predicted and t</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved Backing-off for M-Gram Language Modeling. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pages 181–184, Detroit, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>Y Yang</author>
<author>T Rose</author>
<author>F Li</author>
</authors>
<title>RCV1: A New Benchmark Collection for Text Categorization Research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1: A New Benchmark Collection for Text Categorization Research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G Hinton</author>
</authors>
<title>A Scalable Hierarchical Distributed Language Model.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 21.</booktitle>
<contexts>
<context position="4135" citStr="Mnih and Hinton (2008)" startWordPosition="645" endWordPosition="648">words, however employing heavy feature pruning and indicating that “the computational load, was quite severe for a system this size”. 625 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 625–630, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ability is however very important, since moving to larger datasets is often the simplest way to obtain a better model. Similarly, neural probabilistic LMs (Bengio et al., 2003) don’t scale very well to large datasets. Even the more scalable variant proposed by Mnih and Hinton (2008) is trained on a dataset consisting of only 14M words, also using a vocabulary of around 20000 words. Van den Bosch (2005) proposes a decision-tree classifier which has been applied to training datasets with more than 100M words. However, his model is non-probabilistic and thus a standard comparison with probabilistic models in terms of perplexity isn’t possible. N-Gram models (Goodman, 2001) obtain estimates for p(wi|wi_N+1 ... wi_1) using counts of N-Grams. Because directly using the maximumlikelihood estimate would result in poor predictions, smoothing techniques are applied. A modified int</context>
</contexts>
<marker>Mnih, Hinton, 2008</marker>
<rawString>A. Mnih and G. Hinton. 2008. A Scalable Hierarchical Distributed Language Model. In Advances in Neural Information Processing Systems 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<date>1994</date>
<booktitle>On Structuring Probabilistic Dependences in Stochastic Language Modeling. Computer, Speech and Language,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="14272" citStr="Ney et al., 1994" startWordPosition="2370" endWordPosition="2373">e articles of week 50 (4.1M words) and as test set the articles of week 51 (3.8M words). For training we used datasets of four different sizes: D1 (week 1, 3.1M words), D2 (weeks 1-3, 10M words), D3 (weeks 1-10, 37M words) and D4 (weeks 1-30, 113M words). Features We use three different feature sets in our experiments. The first feature set (basic, BA) consists of all features also used in standard N-Gram models, i.e. all subsequences up to a length N − 1 immediately preceding the target word. The second feature set (short-range, SR) consists of all basic features as well as all skip N-Grams (Ney et al., 1994) that can be formed with the N −1 length context. Moreover, all words occurring in the context are included as bag features, i.e. as features which indicate the occurrence of a word but not the particular position. The third feature set (long-range, LR) is an extension of SR which also includes longerdistance features. Specifically, this feature set additionally includes all unigram bag features up to a distance d = 9. The feature types and examples of extracted features are given in Table 2. Model Comparison We compared the VMM to modified Kneser-Ney (KN, see Section 1). The order of a VMM is</context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>H. Ney, U. Essen, and R. Kneser. 1994. On Structuring Probabilistic Dependences in Stochastic Language Modeling. Computer, Speech and Language, 8:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Saraclar</author>
<author>M Collins</author>
</authors>
<date>2007</date>
<booktitle>Discriminative n-gram Language Modeling. Computer, Speech and Language,</booktitle>
<pages>21--373</pages>
<contexts>
<context position="3384" citStr="Roark et al. (2007)" startWordPosition="530" endWordPosition="533">e incorporated. The classification-based approach to language modeling was introduced by Rosenfeld (1996) who proposed an optimized variant of the maximumentropy classifier (Berger et al., 1996) for the task. Unfortunately, data sparsity resulting from the large number of classes makes it difficult to obtain reliable parameter estimates, even on large datasets and the high computational costs make it difficult train models on large datasets in the first place2. Scal1While the classifier itself is discriminative, i.e. conditioning on the contextual features, the resulting LM is generative. See Roark et al. (2007) for work on discriminative LMs. 2For example, using a vocabulary of 20000 words Rosenfeld (1994) trained his model on up to 40M words, however employing heavy feature pruning and indicating that “the computational load, was quite severe for a system this size”. 625 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 625–630, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ability is however very important, since moving to larger datasets is often the simplest way to obtain a better model. Similarly, ne</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>B. Roark, M. Saraclar, and M. Collins. 2007. Discriminative n-gram Language Modeling. Computer, Speech and Language, 21:373–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Adaptive Statistical Language Modelling: A Maximum Entropy Approach.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="3481" citStr="Rosenfeld (1994)" startWordPosition="547" endWordPosition="549">1996) who proposed an optimized variant of the maximumentropy classifier (Berger et al., 1996) for the task. Unfortunately, data sparsity resulting from the large number of classes makes it difficult to obtain reliable parameter estimates, even on large datasets and the high computational costs make it difficult train models on large datasets in the first place2. Scal1While the classifier itself is discriminative, i.e. conditioning on the contextual features, the resulting LM is generative. See Roark et al. (2007) for work on discriminative LMs. 2For example, using a vocabulary of 20000 words Rosenfeld (1994) trained his model on up to 40M words, however employing heavy feature pruning and indicating that “the computational load, was quite severe for a system this size”. 625 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 625–630, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics ability is however very important, since moving to larger datasets is often the simplest way to obtain a better model. Similarly, neural probabilistic LMs (Bengio et al., 2003) don’t scale very well to large datasets. Even the mo</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>R. Rosenfeld. 1994. Adaptive Statistical Language Modelling: A Maximum Entropy Approach. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A Maximum Entropy Approach to Adaptive Statistical Language Modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="2870" citStr="Rosenfeld (1996)" startWordPosition="451" endWordPosition="452">rength for a particular feature often doesn’t vary much across classes and can thus be assumed constant. The main advantage of our model is that it is straightforward to incorporate rich features without sacrificing scalability or reliability of parameter estimation. In addition, it is simple to implement and no feature selection is required. Section 3 shows that a generative1 LM built with our classifier is competitive to modified Kneser-Ney smoothing and can outperform it if sufficiently rich features are incorporated. The classification-based approach to language modeling was introduced by Rosenfeld (1996) who proposed an optimized variant of the maximumentropy classifier (Berger et al., 1996) for the task. Unfortunately, data sparsity resulting from the large number of classes makes it difficult to obtain reliable parameter estimates, even on large datasets and the high computational costs make it difficult train models on large datasets in the first place2. Scal1While the classifier itself is discriminative, i.e. conditioning on the contextual features, the resulting LM is generative. See Roark et al. (2007) for work on discriminative LMs. 2For example, using a vocabulary of 20000 words Rosen</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO, USA.</location>
<contexts>
<context position="12841" citStr="Stolcke (2002)" startWordPosition="2130" endWordPosition="2131">�k�� p������ . In order to improve generalization, we estimate the categorical parameters based on the counts from all instances, except the one whose gradient is being computed for the online update (leave-one-out). In other words, we subtract the counts for a particular instance before computing the update (Equation 8) and add them back when the update has been executed. In total, training only requires two passes over the data, as opposed to a single pass (plus smoothing) required by N-Gram models. 3 Experiments All experiments were conducted using the SRI Language Modeling Toolkit (SRILM, Stolcke (2002)), i.e. we implemented5 the VMM within SRILM and compared to default N-Gram models supplied with SRILM. The experiments were run on a 64-bit, 2.2 GHz dual-core machine with 8GB RAM. Data The experiments were carried out on data from the Reuters Corpus Version 1 (Lewis et al., 5The code can be downloaded from http://code. google.com/p/variable-mixture-model. 2004), which was split into sentences, tokenized and converted to lower case, not removing punctuation. All our models were built with the same 30367- word vocabulary, which includes the sentence-end symbol and a special symbol for out-of-v</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing, pages 901–904, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Van den Bosch</author>
</authors>
<date>2005</date>
<booktitle>Scalable Classification-based Word Prediction and Confusible Correction. TraitementAutomatique des Langues,</booktitle>
<volume>42</volume>
<issue>2</issue>
<marker>Van den Bosch, 2005</marker>
<rawString>A. Van den Bosch. 2005. Scalable Classification-based Word Prediction and Confusible Correction. TraitementAutomatique des Langues, 42(2):39–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wood</author>
<author>C Archambeau</author>
<author>J Gasthaus</author>
<author>L James</author>
<author>Y Teh</author>
</authors>
<title>A Stochastic Memoizer for Sequence Data.</title>
<date>2009</date>
<booktitle>In Proceedings of the 24th International Conference on Machine learning,</booktitle>
<pages>1129--1136</pages>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="6108" citStr="Wood et al. (2009)" startWordPosition="976" endWordPosition="979">ed from the conditioning context wi_N+1 ... wi_1. The specific features we use for language modeling are given in Section 3. We assume sparse features, such that typically only a small number of the binary features take value 1. These features are referred to as the active features and predictions are based on them. We introduce a bias feature which is active for every instance, in order to ensure that the set of active features is non-empty for each instance. Individually, each active feature Xk is predictive of the class variable and predicts the class through a categorical dis3The model of Wood et al. (2009) has somewhat higher performance, however, again due to high computational costs the model has only been trained on training sets of at most 14M words. tribution4 distribution, which we denote as p(Y |xk). Since instances typically have several active features the question is how to combine the individual predictions of these features into an overall prediction. To this end we make the assumption that each feature Xk has a certain predictive strength θk E R, where larger values indicate that the feature is more likely to predict correctly. The individual predictions can then be combined into a</context>
</contexts>
<marker>Wood, Archambeau, Gasthaus, James, Teh, 2009</marker>
<rawString>F. Wood, C. Archambeau, J. Gasthaus, L. James, and Y. Teh. 2009. A Stochastic Memoizer for Sequence Data. In Proceedings of the 24th International Conference on Machine learning, pages 1129–1136, Montreal, Quebec, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>