<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99162">
Collective Cross-Document Relation Extraction Without Labelled Data
</title>
<author confidence="0.99507">
Limin Yao Sebastian Riedel Andrew McCallum
</author>
<affiliation confidence="0.999257">
University of Massachusetts, Amherst
</affiliation>
<email confidence="0.993393">
{lmyao,riedel,mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.984608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943333333333">
We present a novel approach to relation ex-
traction that integrates information across doc-
uments, performs global inference and re-
quires no labelled text. In particular, we
tackle relation extraction and entity identifi-
cation jointly. We use distant supervision to
train a factor graph model for relation ex-
traction based on an existing knowledge base
(Freebase, derived in parts from Wikipedia).
For inference we run an efficient Gibbs sam-
pler that leads to linear time joint inference.
We evaluate our approach both for an in-
domain (Wikipedia) and a more realistic out-
of-domain (New York Times Corpus) setting.
For the in-domain setting, our joint model
leads to 4% higher precision than an isolated
local approach, but has no advantage over a
pipeline. For the out-of-domain data, we ben-
efit strongly from joint modelling, and observe
improvements in precision of 13% over the
pipeline, and 15% over the isolated baseline.
</bodyText>
<sectionHeader confidence="0.992457" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731636363636">
Relation Extraction is the task of predicting seman-
tic relations over entities expressed in structured or
semi-structured text. This includes, for example,
the extraction of employer-employee relations men-
tioned in newswire, or protein-protein interactions
expressed in biomedical papers. It also includes the
prediction of entity types such as country, citytown
or person, if we consider entity types as unary rela-
tions.
A particularly attractive approach to relation ex-
traction is based on distant supervision.1 Here in
</bodyText>
<footnote confidence="0.358822">
1Also called self training, or weak supervision.
</footnote>
<bodyText confidence="0.980809594594595">
place of annotated text, only an existing knowl-
edge base (KB) is needed to train a relation extrac-
tor (Mintz et al., 2009; Bunescu and Mooney, 2007;
Riedel et al., 2010). The facts in the KB are heuris-
tically aligned to an unlabelled training corpus, and
the resulting alignment is the basis for learning the
extractor.
Naturally, the predictions of a distantly supervised
relation extractor will be less accurate than those of
a supervised one. While facts of existing knowledge
bases are inexpensive to come by, the heuristic align-
ment to text will often lead to noisy patterns in learn-
ing. When applied to unseen text, these patterns will
produce noisy facts. Indeed, we find that extraction
precision still leaves much room for improvement.
This room is not as large as in previous work (Mintz
et al., 2009) where target text and training KB are
closely related. However, when we use the knowl-
edge base Freebase (Bollacker et al., 2008) and the
New York Times corpus (Sandhaus, 2008), we ob-
serve very low precision. For example, the preci-
sion of the top-ranked 50 nationality relation
instances is only 28%.
On inspection, it turns out that many of the errors
can be easily identified: they amount to violations
of basic compatibility constraints between facts. In
particular, we observe unsatisfied selectional pref-
erences of relations towards particular entity types
as types of their arguments. An example is the fact
that the first argument of nationality is always
a person while the second is a country. A sim-
ple way to address this is a pipeline: first predict
entity types, and then condition on these when pre-
dicting relations. However, this neglects the fact that
relations could as well be used to help entity type
prediction.
1013
</bodyText>
<note confidence="0.9574585">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013–1023,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.983027875">
While there is some existing work on enforcing
such constraints in a joint fashion (Roth and Yih,
2007; Kate and Mooney, 2010; Riedel et al., 2009),
they are not directly applicable here. The difference
is the amount of facts they take into account at the
same time. They focus on single sentence extrac-
tions, and only consider very few interacting facts.
This allows them to work with exact optimization
techniques such as (Integer) Linear Programs and
still remain efficient.2 However, when working on
a sentence level they fail to exploit the redundancy
present in a corpus. Moreover, the fewer facts they
consider at the same time, the lower the chance that
some of these will be incompatible, and that mod-
elling compatibility will make a difference.
In this work we present a novel approach that
performs relation extraction across documents, en-
forces selectional preferences, and needs no labelled
data. It is based on an undirected graphical model
in which variables correspond to facts, and factors
between them measure compatibility. In order to
scale up, we run an efficient Gibbs-Sampler at in-
ference time, and train our model using SampleR-
ank (Wick et al., 2009). In practice this leads to a
runtime behaviour that is linear in the size of the cor-
pus. For example, 200,000 documents take less than
three hours for training and testing.
For evaluation we consider two scenarios. First
we follow Mintz et al. (2009), use Freebase as
source of distant supervision, and employ Wikipedia
as source of unlabelled text—we will call this an
in-domain setting. This scenario is somewhat arti-
ficial in that Freebase itself is partially derived from
Wikipedia, and in practice we cannot expect text and
training knowledge base to be so close. Hence we
also evaluate our approach on the New York Times
corpus (out-of-domain setting).
For in-domain data we make the following find-
ing. When we compare to an isolated baseline that
makes no use of entity types, our joint model im-
proves average precision by 4%. However, it does
not outperform a pipelined system. In the out-of-
domain setting, our collective model substantially
outperforms both other approaches. Compared to
the isolated baseline, we achieve a 15% increase in
2The pyramid algorithm of Kate and Mooney (2010) may
scale well, but it is not clear how to apply their scheme to cross-
document extraction.
precision. With respect to the pipeline approach, the
increase is 13%.
In the following we will first give some back-
ground information on relation extraction with dis-
tant supervision. Then we will present our graphi-
cal model as well as the inference and learning tech-
niques we apply. After discussing related work, we
present our empirical results and conclude.
</bodyText>
<sectionHeader confidence="0.95788" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999966">
In this section we will introduce the terminology and
concepts we use throughout the paper. We will also
give a brief introduction to relation extraction, in
particular in the context of distant supervision.
</bodyText>
<subsectionHeader confidence="0.974502">
2.1 Relations
</subsectionHeader>
<bodyText confidence="0.99997719047619">
We seek to extract facts about entities. Example en-
tities would be the company founder BILL GATES,
the company MICROSOFT, and the country USA.
A relation R is a set of tuples c over entities. We
will follow (Mintz et al., 2009) and call the term
R (c1, ... cn) with c E R a relation instance.3 It
denotes the membership of the tuple c in the re-
lation R. For example, founded (BILL GATES,
MICROSOFT) is a relation instance denoting that
BILL GATES and MICROSOFT are related in the
founded relation.
In the following we will always consider some set
of candidate tuples C that may or may not be re-
lated. We define Cn C C to be set of all n-ary tu-
ples in C. Note that while our definition considers
general n-nary relations, in practice we will restrict
us to unary and binary relations C1 and C2.
Following previous work (Mintz et al., 2009; Ze-
lenko et al., 2003; Culotta and Sorensen, 2004) we
make one more simplifying assumption: every can-
didate tuple can be member of at most one relation.
</bodyText>
<subsectionHeader confidence="0.999125">
2.2 Entity Types
</subsectionHeader>
<bodyText confidence="0.885108285714286">
An entity can be of one or several entity types. For
example, BILL GATES is a person, and a company
founder. Entity types correspond to the special
case of relations with arity one, and will be treated
as such in the following.
3Other commonly used terms are relational facts, ground
facts, ground atoms, and assertions.
</bodyText>
<page confidence="0.769744">
1014
</page>
<bodyText confidence="0.999878444444444">
We care about entity types for two reasons. First,
they can be important for downstream applications:
if consumers of our extracted facts know the type
of entities, they can find them more easily, visu-
alize them more adequately, and perform opera-
tions specific to these types (write emails to persons,
book a hotel in a city, etc.). Second, they are use-
ful for extracting binary relations due to selectional
preferences—see section 2.6.
</bodyText>
<subsectionHeader confidence="0.998173">
2.3 Mentions
</subsectionHeader>
<bodyText confidence="0.99726825">
In natural language text spans of tokens are used to
refer to entities. We call such spans entity mentions.
Consider, for example, the following sentence snip-
pet:
</bodyText>
<listItem confidence="0.8775545">
(1) Political opponents of President Evo Morales
of Bolivia have in recent days stepped up...
</listItem>
<bodyText confidence="0.99737975">
Here “Evo Morales” is an entity mention of pres-
ident EVO MORALES, and “Bolivia” a mention of
the country BOLIVIA he is the president of.
People often express relations between entities in
natural language texts by mentioning the participat-
ing entities in specific syntactic and lexical patterns.
We will refer to any tuple of mentions of entities
(e1,... e,,,) in a sentence as candidate mention tu-
ple. If such a candidate expresses the relation R,
then it is a relation mention of the relation instance
R (el, ... , e,,,).
Consider again example 1. Here the pair of en-
tity mentions (“Evo Morales”, “Bolivia”) is a candi-
date mention tuple. In fact, in this case the candidate
is indeed a relation mention of the relation instance
nationality (EVO MORALES, BOLIVIA).
</bodyText>
<subsectionHeader confidence="0.996934">
2.4 Relation Extraction
</subsectionHeader>
<bodyText confidence="0.999991466666666">
We define the task of relation extraction as follows.
We are given a corpus of documents and a set of
target relations. Then we are asked to predict all re-
lation instances I so that for each R (c) E I there
exists at least one relation mention in the given cor-
pus.
The above definition covers a range of existing
approaches by varying over what we define as tar-
get corpus. On one end, we have extractors that
process text on a per sentence basis (Zelenko et al.,
2003; Culotta and Sorensen, 2004). On the other
end, we have methods that take relation mentions
from several documents and use these as input fea-
tures (Mintz et al., 2009; Bunescu and Mooney,
2007).
There is a compelling reason for performing re-
lation extraction within a larger scope that consid-
ers mentions across documents: redundancy. Often
facts are mentioned in several sentences and doc-
uments. Some of these mentions may be difficult
to parse, or they use unseen patterns. But the more
mentions we consider, the higher the probability that
one does parse, and fits a pattern we have seen in the
training data.
Note that for relation extraction that considers
more than a single mention we have to solve the
coreference problem in order to determine which
mentions refer to the same entity. In the follow-
ing we will assume that coreference clusters are pro-
vided by a preprocessing step.
</bodyText>
<subsectionHeader confidence="0.987914">
2.5 Distant Supervision
</subsectionHeader>
<bodyText confidence="0.9633575">
In relation extraction we often encounter a lack of
explicitly annotated text, but an abundance of struc-
tured data sources such as company databases or col-
laborative knowledge bases like Freebase. In order
to exploit this, many approaches use simple but ef-
fective heuristics to align existing facts with unla-
belled text. This labelled text can then be used as
training material of a supervised learner.
One heuristic is to assume that each candidate
mention tuple of a training fact is indeed expressing
the corresponding relation (Bunescu and Mooney,
2007). Mintz et al. (2009) refer to this as the dis-
tant supervision assumption.
Clearly, this heuristic can fail. Let us again
consider the nationality relation between EVO
MORALES and BOLIVIA. In an 2007 article of the
New York Times we find this relation mention can-
didate:
(2) ...the troubles faced by Evo Morales in
Bolivia...
This sentence does not directly express that EVO
MORALES is a citizen of BOLIVIA, and hence vi-
olates the distant supervision assumption. The prob-
lem with this observation is that at training time
we may learn a relatively large weight for the
feature “&lt;Entity1&gt; in &lt;Entity2&gt;” associated with
1015
.
nationality. When testing our model we then
encounter a sentence such as
</bodyText>
<listItem confidence="0.712287">
(3) Arrest Warrant Issued for Richard Gere in
India.
</listItem>
<bodyText confidence="0.734412">
that leads us to extract that RICHARD GERE is a cit-
izen of INDIA.
</bodyText>
<subsectionHeader confidence="0.998162">
2.6 Global Consistency of Facts
</subsectionHeader>
<bodyText confidence="0.99964675">
As discussed above, distant supervision can lead to
noisy extractions. However, such noise can often be
easily identified by testing how compatible the ex-
tracted facts are to each other. In this work we are
concerned with a particular type of compatibility:
selectional preferences.
Relations require, or prefer, their arguments to be
of certain types. For example, the nationality
relation requires the first argument to be a person,
and the second to be a country. On inspection,
we find that these preferences are often not satis-
fied in a baseline distant supervision system akin to
Mintz et al. (2009). This often results from patterns
such as “&lt;Entity1&gt; in &lt;Entity2&gt;” that fire in many
cases where &lt;Entity2&gt; is a location, but not a
country.
</bodyText>
<sectionHeader confidence="0.985176" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.989998045454545">
Our observations in the previous section suggest
that we should (a) explicitly model compatibil-
ity between extracted facts, and (b) integrate ev-
idence from several documents to exploit redun-
dancy. In this work we choose a Conditional Ran-
dom Field (CRF) to achieve this. CRFs are a natural
fit for this task: They allow us to capture correlations
in an explicit fashion, and to incorporate overlapping
input features from multiple documents.
The hidden output variables of our model are Y =
(Yc)c∈C. That is, we have one variable Yc for each
candidate tuple c E C . This variable can take as
value any relation in C with the same arity as c. See
example relation variables in figure 1.
The observed input variables X consists of a fam-
ily of variables Xc = (X1c,... Xm ) m∈M for each
c
candidate tuple c. Here Xic stores relevant observa-
tions we make for the i-th candidate mention tuple of
c in the corpus. For example, X1BILL GATES,MICROSOFT
in figure 1 would contain, among others, the pattern
“[M2] was founded by [M1]”.
</bodyText>
<subsectionHeader confidence="0.999166">
3.1 Factor Templates
</subsectionHeader>
<bodyText confidence="0.959374">
Our conditional probability distribution over vari-
ables X and Y is defined using using a set T of
factor templates. Each template Tj E T defines
a set of factors {(yi, xi)1, a set Kj of feature in-
dices, parameters {θkj I k∈Kj and feature functions
{ fk I k∈Kj . Together they define the following con-
ditional distribution:
</bodyText>
<equation confidence="0.99056225">
1 T7
p (y|x) = Zx
Tj∈T (yi,xi)∈Tj
(4)
</equation>
<bodyText confidence="0.999873666666667">
In our case the set T consists of four templates
we will describe below. We construct this graphical
model using FACTORIE (McCallum et al., 2009), a
probabilistic programming language that simplifies
the construction process, as well as inference and
learning.
</bodyText>
<subsectionHeader confidence="0.892591">
3.1.1 Bias Template
</subsectionHeader>
<bodyText confidence="0.988568222222222">
We use a bias template TBias that prefers certain
relations a priori over others. When the template
is unrolled, it creates one factor per variable Yc for
candidate tuple c E C. The template also consists of
one weight θBias
r and feature function fBias
r for each
possible relation r. fBias rfires if the relation associ-
ated with tuple c is r.
</bodyText>
<subsectionHeader confidence="0.859247">
3.1.2 Mention Template
</subsectionHeader>
<bodyText confidence="0.974008571428571">
In order to extract relations from text, we need
to model the correlation between relation instances
and their mentions in text. For this purpose we de-
fine the template TMen that connects each relation
instance variable Yc with its observed mention vari-
ables Xc. Crucially, this template gathers mentions
from multiple documents, and enables us to exploit
redundancy.
The feature functions of this template are taken
from Mintz et al. (2009). This includes features that
inspect the lexical content between entity mentions
in the same sentence, and the syntactic path between
them. One example is
{ 1 yc = founded n ]i with
</bodyText>
<figure confidence="0.658077">
&amp;quot;M2 was founded by M1&amp;quot; E xi c
0 otherwise
T7 �k�Kj θj kfj k(yi,xi)
e
def
f101 (yc, xc)
=
1016
</figure>
<figureCaption confidence="0.99006">
Figure 1: FactoraGraph ofrour model that captures selectional preferences and functionality constraints. For of elati and tity type
</figureCaption>
<figure confidence="0.834518361111111">
aken
readability we only label a subsets of equivalent variables and factors. Note that the graph shows an example
orrelation between relation instances and
We also add a templae TPair hat easures he
fica-
ons in text For th
assignment to vriables.
compability h
Yh
e Y
Then
Y h
Y Y
c y a
p
company person
country
Y
TJoint
elatio
TBias
founder
T
nationality
Sele b
es ure
X1
i
p
Microsoft was2With Microsoft chairmancBill Gates was
founded by Bill Gates... Bill Gates soon re
1 linquishing... born in the USA in 1955 lil lt
It tests whether for any mentions of the candidate
vible Y wth ts bd ible
tuple the phrase &amp;quot;founded by&amp;quot; appears between the
riables M�
</figure>
<bodyText confidence="0.730128">
mentions of the argument entities.
</bodyText>
<subsectionHeader confidence="0.680638">
3.1.3 Selectional Preference Templates
</subsectionHeader>
<bodyText confidence="0.973038777777778">
incldes features that inspect the l
To capture the correlations between entity types
and relations the entities participate in, we introduce
the template TJoint. It connects a relation instance
variable Ye1,...,en to the individual entity type vari-
ables Ye1, ... , Yen. To measure the compatibility
between relation and entity variables, we use one
feature fJoint
r,t1...ta (and weight θJoint
r,t1...ta) for each com-
bination of relation and entity types r, tl ... ta.
fJoint fires when the factor variables are in the
r,t1...ta
state r, t1 . . . ta. For example, fJoint founded,person,company
fires if Ye1 is in state person, Ye2 in state company,
and Ye1,e2 in state founded.
We also add a template TPair that measures the
pairwise compatibility between the relation variable
Ye1,...,ea and each entity variable Yei in isolation.
Here we use features fPair
i,r,t that fire if ei is the i-th ar-
gument of c, has the entity type t and the candidate
tuple c is labelled as instance of relation r. For ex-
ample, fPair
1,founded,person fires if Ye1(argument i = 1)
is in state person, and Ye1,e2 instate founded, re-
gardless of the state of Yee.
</bodyText>
<subsectionHeader confidence="0.795911">
3.2 Inference
</subsectionHeader>
<bodyText confidence="0.9679503">
a weight θ
There are two types of inference we have to perform:
sampling from the posterior during training (see sec-
tion 3.3), and finding the most likely configuration
(aka MAP inference). In both settings we employ a eh
Gibbs samplerr(Geman andfGeman,s1990) thattran-
domly picks a variable Yr and samples its relation
value conditioned on its Markov Blanket. At test
time we decrease the temperature of our sampler in
order to find an approximation of the MAP solution.
</bodyText>
<subsectionHeader confidence="0.994297">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.999944941176471">
Most learning methods need to calculate the model
expectations (Lafferty et al., 2001) or the MAP con-
figuration (Collins, 2002) before making an update
to the parameters. This step of inference is usually
the bottleneck for learning, even when performed
approximately.
SampleRank (Wick et al., 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within MCMC infer-
ence. Every pair of consecutive samples in the
MCMC chain is ranked according to the model and
the ground truth, and the parameters are updated
when the rankings disagree. This update can fol-
low different schemes, here we use MIRA (Cram-
mer and Singer, 2003). This allows the learner to
acquire more supervision per instance, and has led
to efficient training for models in which inference
</bodyText>
<page confidence="0.454779">
1017
</page>
<bodyText confidence="0.99192">
is expensive and generally intractable (Singh et al.,
2009).
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999900324324325">
Distant Supervision Learning to extract relations
by using distant supervision has raised much interest
in recent years. Our work is inspired by Mintz et al.
(2009) who also use Freebase as distant supervision
source. We also heuristically align our knowledge
base to text by making the distant supervision as-
sumption (Bunescu and Mooney, 2007; Mintz et al.,
2009). However, in contrast to these previous ap-
proaches, and other related distant supervision meth-
ods (Craven and Kumlien, 1999; Weld et al., 2009;
Hoffmann et al., 2010), we perform relation extrac-
tion collectively with entity type prediction.
Schoenmackers et al. (2008) use entailment rules
on assertion extracted by TextRunner to increase re-
call. They also perform cross-document probabilis-
tic inference based on Markov Networks. However,
they do not infer the types of entities and work in an
open IE setting.
Selectional Preferences In the context of super-
vised relation extraction, selectional preferences
have been applied. For example, Roth and Yih
(2007) have used Linear Programming to enforce
consistency between entity types and extracted re-
lations. Kate and Mooney (2010) use a pyramid
parsing scheme to achieve the same. Riedel et al.
(2009) use Markov Logic to model interactions be-
tween event-argument relations for biomedical event
extraction. However, their work is (a) supervised,
and (b) performs extraction on a per-sentence basis.
Carlson et al. (2010) also use selectional prefer-
ences. However, instead of exploiting them for train-
ing a graphical model using distant supervision, they
use selectional preferences to improve a bootstrap-
ping process. Here in each iteration of bootstrap-
ping, extracted facts that violate compatibility con-
straints will not be used to generate additional pat-
terns in the next iteration.
</bodyText>
<sectionHeader confidence="0.998635" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999822">
We set up experiments to answer the following ques-
tions: (i) Does the explicit modelling of selectional
preferences improve accuracy? (ii) Can we also per-
form joint entity and relation extraction in a pipeline
and achieve similar results? (iii) How does our
cross-document approach scale?
To answer these questions we carry out experi-
ments on two data sets, Wikipedia and New York
Times articles, and use Freebase as distant supervi-
sion source for both.
</bodyText>
<subsectionHeader confidence="0.967963">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99998645">
We follow Mintz et al. (2009) and perform two types
of evaluation: held-out and manual. In both cases
we have a training and a test corpus of documents,
and training and test sets of entities. For held-out
evaluation we split the set of entities in Freebase into
training and test sets. For manual evaluation we use
all Freebase entities during training. For testing we
use all entities that appear in the test document cor-
pus.
For both training and testing we then choose the
candidate tuples C that may or may not be relation
instances. To pick the entities C1 we want to predict
entity types for, we choose all entities that are men-
tioned at least once in the train/test corpus. To pick
the entity pairs C2 that we want to predict the rela-
tions of, we choose those that appear at least once
together in a sentence.
The set of candidates C will contain many tuples
which are not related in any Freebase relations. For
efficiency, we filter out a large fraction of these neg-
ative candidates for training. The number of neg-
ative examples we keep is chosen to be about 10
times the number of positive candidates. This num-
ber stems from trading-off the accuracy it leads to
and the increased training time it requires.
For both manual and held-out evaluation we rank
extracted test relation instances in the MAP state of
the network. This state is found by sampling 20 iter-
ations with a low temperature of 0.00001. The rank-
ing is done according to the log linear score that the
assigned relation for a candidate tuple gets from the
factors in its Markov Blanket. For optimal perfor-
mance, the score is normalized by the number of re-
lation mentions.
For manual evaluation we pick the top ranked 50
relation instances for the most frequent relations.
We ask three annotators to inspect the mentions of
these relation instances to decide whether they are
correct. Upon disagreement, we use majority vote.
To summarize precisions across relations, we take
</bodyText>
<page confidence="0.53892">
1018
</page>
<bodyText confidence="0.994227">
their average, and their average weighted by the pro-
portion of predicted instances for the given relation.
</bodyText>
<subsubsectionHeader confidence="0.914679">
5.1.1 Data preprocessing
</subsubsectionHeader>
<bodyText confidence="0.999987846153846">
We preprocess our textual data as follows:
We first use the Stanford named entity recog-
nizer (Finkel et al., 2005) to find entity mentions in
the corpus. The NER tagger segments each docu-
ment into sentences and classifies each token into
four categories: PERSON, ORGANIZATION, LO-
CATION and NONE. We treat consecutive tokens
which share the same category as single entity men-
tion. Then we associate these mentions with Free-
base entities. This is achieved by performing a
string match between entity mention phrases and the
canonical names of entities as present in Freebase.
For each candidate tuple c with arity 2 and each
of its mention tuples i we extract a set of features X&apos;C
similar to those used in (Mintz et al., 2009): lexical,
Part-Of-Speech (POS), named entity and syntactic
features, i.e. features obtained from the dependency
parsing tree of a sentence. We use the openNLP POS
tagger4 to obtain POS tags and employ the Malt-
Parser (Nivre et al., 2004) for dependency parsing.
For candidate tuples with arity 1 (entity types) we
use the following features: the entity’s word form,
the POS sequence, the head of the entity in the de-
pendency parse tree, the Stanford named entity tag,
and the left and right words to the current entity
mention phrase.
</bodyText>
<subsubsectionHeader confidence="0.601081">
5.1.2 Configurations
</subsubsectionHeader>
<bodyText confidence="0.9999505">
We apply the following configurations of our fac-
tor graphs. As our baseline, and roughly equivalent
to previous work (Mintz et al., 2009), we pick the
templates TBias and TMen. These describe a fully dis-
connected graph, and we will refer to this configu-
ration as isolated. Next, we add the templates TJoint
and TPair to model selectional preferences, and refer
to this setting as joint.
In addition, we evaluate how well selectional pref-
erences can be captured with a simple pipeline. For
this pipeline we first train an isolated system for en-
tity type prediction. Then we use the output of the
entity type prediction system as input for the relation
extraction system.
</bodyText>
<footnote confidence="0.987286">
4available at http://opennlp.sourceforge.net/
</footnote>
<subsubsectionHeader confidence="0.673096">
5.1.3 Entity types and Relation types
</subsubsectionHeader>
<bodyText confidence="0.999921958333334">
Freebase contains many relation types and only
a subset of those relation types occur frequently
in the corpus. Since classes with very few
training instances are generally hard to learn,
we restrict ourselves to the 54 most frequently
mentioned relations. These include, for ex-
ample, nationality, contains, founded
and place_of_birth. Note that we con-
vert two Freebase non-binary temporal relations
to binary relations: employment_tenure and
place_lived. In both cases we simply disregard
the temporal information in the Freebase data.
As our main focus is relation extraction, we re-
strict ourselves to entity types compatible with our
selected relations. To this end we inspect the Free-
base schema information provided for each relation,
and include those entity types that are declared as
arguments of our relations. This leads to 10 entity
types including person, citytown, country,
and company.
Note that a Freebase entity can have several types.
We pick one of these by choosing the most specific
one that is a member of our entity type subset, or
MTSC if no such member exists.
</bodyText>
<subsectionHeader confidence="0.998067">
5.2 Wikipedia
</subsectionHeader>
<bodyText confidence="0.999994166666667">
In our first set of experiments we train and test using
Wikipedia as the text corpus. This is a comparatively
easy scenario because the facts in Freebase are partly
derived from Wikipedia, hence there is an increased
chance of properly aligning training facts and text.
This is similar to the setting of Mintz et al. (2009).
</bodyText>
<subsectionHeader confidence="0.788283">
5.2.1 Held Out Evaluation
</subsectionHeader>
<bodyText confidence="0.999648461538462">
We split 1,300,000 Wikipedia articles into train-
ing and test sets. Table 1 shows the statistics for this
split. The last row provides the number of negative
relation instances (candidates which are not related
according to Freebase) associated with each data set.
Figure 2 shows the precision-recall curves of re-
lation extraction for held-out data of various config-
urations. We notice a slight advantage of the joint
approach in the low recall area. Moreover, the joint
model predicts more relation instances, as can be
seen by its longer line in the graph.
For higher recall, the joint model performs
slightly worse. On closer inspection, we find that
</bodyText>
<table confidence="0.964339285714286">
1019
Wikipedia NYT
Train Test Train Test
#Documents 900K 400K 177K 39K
#Entities 213K 137K 56K 27K
#Positive 36K 24K 5K 2K
#Negative 219K 590K 64K 94K
</table>
<tableCaption confidence="0.9844445">
Table 1: The statistics of held-out evaluation on
Wikipedia and New York Times.
</tableCaption>
<table confidence="0.9998114">
Isolated Pipeline Joint
Wikipedia 0.82 0.87 0.86
Wikipedia (w) 0.95 0.94 0.95
NYT 0.63 0.65 0.78
NYT (w) 0.78 0.82 0.94
</table>
<tableCaption confidence="0.730256333333333">
Table 2: Average and weighted (w) average preci-
sion over frequent relations for New York Times and
Wikipedia data, based on manual evaluation.
</tableCaption>
<figureCaption confidence="0.8508395">
Figure 2: Precision-recall curves for various setups
in Wikipedia held-out setting.
</figureCaption>
<bodyText confidence="0.999698777777778">
this observation is somewhat misleading. Many of
the predictions of the joint model are not in the
held-out test set derived from Freebase, but never-
theless correct. Hence, to understand if one system
really outperforms another, we need to rely on man-
ual evaluation.
Note that the figure only considers binary
relations—for entity types all configurations per-
form similarly.
</bodyText>
<subsectionHeader confidence="0.641776">
5.2.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.999971103448276">
As mentioned above, held-out evaluation in this
context suffers from false negatives in Freebase. Ta-
ble 2 therefore shows the results of our manual eval-
uation. They are based on the average, and weighted
average, of the precisions for the relation instances
of the most frequent relations. We notice that here
all systems perform comparably for weighted aver-
age precision. For average precision we see an ad-
vantage for both the pipeline and the joint model
over the isolated system.
One reason for similar weighted average preci-
sions is the fact that all approaches accurately pre-
dict a large number of contains instances. This is
due to very regular and simple patterns in Wikipedia.
For example, most articles on towns start with “A is
a municipality in the district of B in C, D.” For these
sentences, the relative position of two location men-
tions is a very good predictor of contains. When
used as a feature, it leads to high precision for all
models. And since contains instances are most
frequent, and we take the weighted average, results
are generally close to each other.
To summarize: in this in-domain setting, mod-
elling compatibility between entity types and rela-
tions helps to improve average precision, but not
weighted average precision. This holds for both the
joint and the pipeline model. However, we will see
how this changes substantially when moving to an
out-of-domain scenario.
</bodyText>
<subsectionHeader confidence="0.988675">
5.3 New York Times
</subsectionHeader>
<bodyText confidence="0.99981175">
For our second set of experiments we use New
York Times data as training and test corpora. As
we argued before, this is expected to be the more
difficult—and more realistic—scenario.
</bodyText>
<subsubsectionHeader confidence="0.624688">
5.3.1 Held-out Evaluation
</subsubsectionHeader>
<bodyText confidence="0.99298">
We choose all articles of the New York times dur-
ing 2005 and 2006 as training corpus. As test corpus
we use the first 6 months of 2007.
Figure 3 shows precision-recall curves for our var-
ious setups. We see that jointly modelling entity
</bodyText>
<figure confidence="0.995688888888889">
0.0 0.1 0.2 0.3 0.4
11�ll
Precision
0.4 0.5 0.6 0.7 0.8 0.9 1.0
joint
pipe
isolated
1020
Recall
</figure>
<figureCaption confidence="0.951786">
Figure 3: Precision-recall curves for various setups
in New York Times held-out setting.
</figureCaption>
<bodyText confidence="0.99802668">
types and relations helps to improve precision.
Due to the smaller overlap between Freebase and
NYT data, figure 3 also has to be taken with more
caution. The systems may predict correct relation
instances that just do not appear in Freebase. Hence
manual evaluation is even more important.
When evaluating entity precision we find that for
both models it is about 84%. This raises the ques-
tion why the joint entity type and relation extrac-
tion model outperforms the pipeline on relations.
We take a close look at the entities which partici-
pate in relations and find that joint model performs
better on most entity types, for example, country
and citytown. We also look at the relation in-
stances which are predicted by both systems and find
that the joint model does predict correct entity types
when the pipeline mis-predicts. And exactly these
mis-predictions lead the pipeline astray. Consider-
ing binary relation instances where the pipeline fails
but the joint model does not, we observe an entity
precision of 76% for the pipeline and 86% for our
joint approach. The joint model fails to correctly
predict some entity types that the pipeline gets right,
but these tend to appear in contexts where relation
instances are easy to extract without considering en-
</bodyText>
<table confidence="0.999777111111111">
Relation Type Iso. Pipe Joint
contains 0.92 0.98 0.96
nationality 0.28 0.64 0.82
plc-lived 0.88 0.70 0.96
plc-of-birth 0.32 0.20 0.25
works-for 0.96 0.98 0.98
plc-of-death 0.24 0.40 0.42
children 1.00 0.92 0.98
founded 0.42 0.34 0.71
</table>
<tableCaption confidence="0.9796475">
Table 3: Precision at 50 for the most frequent rela-
tions on New York Times
</tableCaption>
<bodyText confidence="0.666003">
tity types.5
</bodyText>
<subsectionHeader confidence="0.45868">
5.3.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.9999384375">
Manually evaluated precision for New York
Times data can be seen in table 2. In contrast to the
Wiki setting, here modelling entity types and rela-
tions jointly makes a substantial difference. For av-
erage precision, our joint model improves over the
isolated baseline by 15%, and over the pipeline by
13%. Similar improvements can be observed for
weighted average precision.
Let us look at a break-down of precisions with
respect to different relations shown in table 3. We
see dramatic improvements for nationality and
founded when applying the joint model. Note that
the nationality relation takes a larger part in
the predicted relation instances of the joint model
and hence contributes significantly to the weighted
average precision.
</bodyText>
<subsectionHeader confidence="0.976851">
5.4 Scalability
</subsectionHeader>
<bodyText confidence="0.971987833333333">
We propose to perform joint inference for large scale
information extraction. An obvious concern in this
scenario is scalability. In practice we find that infer-
ence (and hence learning) in our model scales lin-
early with the number of candidate tuples. This can
be seen in figure 4a. It is to be expected since the
number of candidates equals the number of variables
the sampler has to process in each iteration.
The above observation also means that our ap-
proach scales linearly with corpus size. To illustrate
5Note that our learned preferences are soft, and hence can
be violated in case of wrong entity type predictions.
</bodyText>
<figure confidence="0.994844">
0.00 0.05 0.10 0.15 0.20
0.2 0.4 0.6 0.8 1.0
joint
PIPs
isolated
Precision
1021
</figure>
<figureCaption confidence="0.8891945">
Figure 4: CPU time for one iteration per candidate
tuple, and candidate tuples per document.
</figureCaption>
<bodyText confidence="0.999719625">
this, figure 4b shows how the number of candidates
scales with the number of documents. Again we ob-
serve a linear behavior. Since both are linear, we can
say that our joint approach is linear in the number of
documents.
Total training and test times are moderate, too.
For example, the held-out experiments with 200,000
NYT documents finish within three hours.
</bodyText>
<sectionHeader confidence="0.998204" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998875">
This paper presents a novel approach to extracting
relational facts from text. Akin to previous work in
relation extraction with distant supervision, we re-
quire no annotated text. However, instead extract-
ing facts in isolation, we model interactions between
facts in order to improve precision. In particular, we
capture selectional preferences of relations. These
preferences are modelled in a cross-document fash-
ion using a large scale factor graph. We show in-
ference and learning can be efficiently performed
in linear time by Gibbs Sampling and SampleRank.
When applied to out-of-domain text, this approach
leads to a 15% increase in precision over an isolated
baseline, and a 13% improvement over a pipelined
system.
A crucial aspect of our approach is its extensibil-
ity. Since it is exclusively framed in terms of an
undirected graphical model, it is conceptually easy
to extend it to other types of compatibilities, such
as functionality constraints. It could also be ex-
tended to tackle coreference resolution. Eventually
we seek to model the complete process of the au-
tomatic construction of KB within this framework,
and capture dependencies between extractions in a
joint and principled fashion. As we have seen here,
in particular when learning is less supervised and
extractions are noisy, capturing such interactions is
paramount.
</bodyText>
<sectionHeader confidence="0.990991" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998442">
This work was supported in part by the Center for
Intelligent Information Retrieval, in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation un-
der NSF grant #IIS-0326249, and in part by UPenn
NSF medium IIS-0803847. The University of Mas-
sachusetts also gratefully acknowledges the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
</bodyText>
<sectionHeader confidence="0.994322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.945044217391304">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a collabo-
ratively created graph database for structuring human
knowledge. In SIGMOD ’08: Proceedings of the 2008
ACM SIGMOD international conference on Manage-
ment of data, pages 1247–1250, New York, NY, USA.
ACM.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using min-
imal supervision. In Proceedings of the 45rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL ’07).
Andrew Carlson, Justin Betteridge, Richard Wang, Es-
tevam Hruschka, and Tom Mitchell. 2010. Cou-
pled semi-supervised learning for information extrac-
tion. In Third ACM International Conference on Web
Search and Data Mining (WSDM ’10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical methods in natural lan-
guage processing (EMNLP ’02), volume 10, pages 1–
8.
</reference>
<figure confidence="0.9754455">
Number of Candidate Tuples 0 Number of Documents
(a) CPU time (b) Candidate tuples
Number of Candidate Tuples
Time per iteration (seconds)
200
1022
</figure>
<reference confidence="0.993059043956044">
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951–991.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge-bases by extracting information from
text sources. In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for Molecular
Biology, pages 77–86, Germany.
Aron Culotta and Jeffery Sorensen. 2004. Dependency
tree kernels for relation extraction. In 42nd Annual
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL
’05), pages 363–370, June.
S. Geman and D. Geman. 1990. Stochastic relaxation,
gibbs distributions, and the bayesian restoration of im-
ages. pages 452–472.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In ACL.
Rohit J. Kate and Raymond J. Mooney. 2010. Joint en-
tity and relation extraction using card-pyramid pars-
ing. In Proceedings of the 12th Conference on Com-
putational Natural Language Learning (CoNLL’ 10).
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In International Conference on Machine Learn-
ing (ICML).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Y. Bengio, D. Schuur-
mans, J. Lafferty, C. K. I. Williams, and A. Culotta, ed-
itors, Advances in Neural Information Processing Sys-
tems 22, pages 1249–1257.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP (ACL ’09),
pages 1003–1011. Association for Computational Lin-
guistics.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of CoNLL, pages
49–56.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun’ichi Tsujii. 2009. A markov logic approach to
bio-molecular event extraction. In Proceedings of the
Natural Language Processing in Biomedicine NAACL
2009 Workshop (BioNLP ’09), pages 41–49.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the European Confer-
ence on Machine Learning and Knowledge Discovery
in Databases (ECML PKDD ’10).
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
Evan Sandhaus, 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In EMNLP ’08: Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 79–88, Morristown, NJ, USA. Association for
Computational Linguistics.
Sameer Singh, Karl Schultz, and Andrew McCallum.
2009. Bi-directional joint inference for entity res-
olution and segmentation using imperatively-defined
factor graphs. In European Conference on Machine
Learning and Principles and Practice of Knowledge
Discovery in Databases (ECML PKDD), pages 414–
429.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009.
Using wikipedia to bootstrap open information extrac-
tion. In ACM SIGMOD Record.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009. Samplerank: Learning
preferences from atomic gradients. In Neural Infor-
mation Processing Systems (NIPS), Workshop on Ad-
vances in Ranking.
Dimitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. JMLR, 3(6):1083 – 1106.
1023
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874663">
<title confidence="0.999572">Collective Cross-Document Relation Extraction Without Labelled Data</title>
<author confidence="0.98944">Limin Yao Sebastian Riedel Andrew McCallum</author>
<affiliation confidence="0.999985">University of Massachusetts,</affiliation>
<email confidence="0.999875">lmyao@cs.umass.edu</email>
<email confidence="0.999875">riedel@cs.umass.edu</email>
<email confidence="0.999875">mccallum@cs.umass.edu</email>
<abstract confidence="0.992329545454545">We present a novel approach to relation extraction that integrates information across documents, performs global inference and requires no labelled text. In particular, we tackle relation extraction and entity identification jointly. We use distant supervision to train a factor graph model for relation extraction based on an existing knowledge base (Freebase, derived in parts from Wikipedia). For inference we run an efficient Gibbs sampler that leads to linear time joint inference. We evaluate our approach both for an indomain (Wikipedia) and a more realistic outof-domain (New York Times Corpus) setting. For the in-domain setting, our joint model leads to 4% higher precision than an isolated local approach, but has no advantage over a pipeline. For the out-of-domain data, we benefit strongly from joint modelling, and observe improvements in precision of 13% over the pipeline, and 15% over the isolated baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In SIGMOD ’08: Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2645" citStr="Bollacker et al., 2008" startWordPosition="409" endWordPosition="412">r. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves much room for improvement. This room is not as large as in previous work (Mintz et al., 2009) where target text and training KB are closely related. However, when we use the knowledge base Freebase (Bollacker et al., 2008) and the New York Times corpus (Sandhaus, 2008), we observe very low precision. For example, the precision of the top-ranked 50 nationality relation instances is only 28%. On inspection, it turns out that many of the errors can be easily identified: they amount to violations of basic compatibility constraints between facts. In particular, we observe unsatisfied selectional preferences of relations towards particular entity types as types of their arguments. An example is the fact that the first argument of nationality is always a person while the second is a country. A simple way to address th</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD ’08: Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45rd Annual Meeting of the Association for Computational Linguistics (ACL ’07).</booktitle>
<contexts>
<context position="1852" citStr="Bunescu and Mooney, 2007" startWordPosition="276" endWordPosition="279">ructured or semi-structured text. This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers. It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in 1Also called self training, or weak supervision. place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves much room for improvement. Th</context>
<context position="10104" citStr="Bunescu and Mooney, 2007" startWordPosition="1672" endWordPosition="1675">ction as follows. We are given a corpus of documents and a set of target relations. Then we are asked to predict all relation instances I so that for each R (c) E I there exists at least one relation mention in the given corpus. The above definition covers a range of existing approaches by varying over what we define as target corpus. On one end, we have extractors that process text on a per sentence basis (Zelenko et al., 2003; Culotta and Sorensen, 2004). On the other end, we have methods that take relation mentions from several documents and use these as input features (Mintz et al., 2009; Bunescu and Mooney, 2007). There is a compelling reason for performing relation extraction within a larger scope that considers mentions across documents: redundancy. Often facts are mentioned in several sentences and documents. Some of these mentions may be difficult to parse, or they use unseen patterns. But the more mentions we consider, the higher the probability that one does parse, and fits a pattern we have seen in the training data. Note that for relation extraction that considers more than a single mention we have to solve the coreference problem in order to determine which mentions refer to the same entity. </context>
<context position="11381" citStr="Bunescu and Mooney, 2007" startWordPosition="1882" endWordPosition="1885">sters are provided by a preprocessing step. 2.5 Distant Supervision In relation extraction we often encounter a lack of explicitly annotated text, but an abundance of structured data sources such as company databases or collaborative knowledge bases like Freebase. In order to exploit this, many approaches use simple but effective heuristics to align existing facts with unlabelled text. This labelled text can then be used as training material of a supervised learner. One heuristic is to assume that each candidate mention tuple of a training fact is indeed expressing the corresponding relation (Bunescu and Mooney, 2007). Mintz et al. (2009) refer to this as the distant supervision assumption. Clearly, this heuristic can fail. Let us again consider the nationality relation between EVO MORALES and BOLIVIA. In an 2007 article of the New York Times we find this relation mention candidate: (2) ...the troubles faced by Evo Morales in Bolivia... This sentence does not directly express that EVO MORALES is a citizen of BOLIVIA, and hence violates the distant supervision assumption. The problem with this observation is that at training time we may learn a relatively large weight for the feature “&lt;Entity1&gt; in &lt;Entity2&gt;</context>
<context position="19483" citStr="Bunescu and Mooney, 2007" startWordPosition="3258" endWordPosition="3261">ollow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per instance, and has led to efficient training for models in which inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extra</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45rd Annual Meeting of the Association for Computational Linguistics (ACL ’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Richard Wang</author>
<author>Estevam Hruschka</author>
<author>Tom Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Third ACM International Conference on Web Search and Data Mining (WSDM ’10).</booktitle>
<contexts>
<context position="20576" citStr="Carlson et al. (2010)" startWordPosition="3426" endWordPosition="3429"> infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate and Mooney (2010) use a pyramid parsing scheme to achieve the same. Riedel et al. (2009) use Markov Logic to model interactions between event-argument relations for biomedical event extraction. However, their work is (a) supervised, and (b) performs extraction on a per-sentence basis. Carlson et al. (2010) also use selectional preferences. However, instead of exploiting them for training a graphical model using distant supervision, they use selectional preferences to improve a bootstrapping process. Here in each iteration of bootstrapping, extracted facts that violate compatibility constraints will not be used to generate additional patterns in the next iteration. 5 Experiments We set up experiments to answer the following questions: (i) Does the explicit modelling of selectional preferences improve accuracy? (ii) Can we also perform joint entity and relation extraction in a pipeline and achiev</context>
</contexts>
<marker>Carlson, Betteridge, Wang, Hruschka, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Richard Wang, Estevam Hruschka, and Tom Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Third ACM International Conference on Web Search and Data Mining (WSDM ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’02),</booktitle>
<volume>10</volume>
<pages>pages</pages>
<contexts>
<context position="18384" citStr="Collins, 2002" startWordPosition="3085" endWordPosition="3086">ce a weight θ There are two types of inference we have to perform: sampling from the posterior during training (see section 3.3), and finding the most likely configuration (aka MAP inference). In both settings we employ a eh Gibbs samplerr(Geman andfGeman,s1990) thattrandomly picks a variable Yr and samples its relation value conditioned on its Markov Blanket. At test time we decrease the temperature of our sampler in order to find an approximation of the MAP solution. 3.3 Training Most learning methods need to calculate the model expectations (Lafferty et al., 2001) or the MAP configuration (Collins, 2002) before making an update to the parameters. This step of inference is usually the bottleneck for learning, even when performed approximately. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical methods in natural language processing (EMNLP ’02), volume 10, pages 1– 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--951</pages>
<contexts>
<context position="18927" citStr="Crammer and Singer, 2003" startWordPosition="3169" endWordPosition="3173">odel expectations (Lafferty et al., 2001) or the MAP configuration (Collins, 2002) before making an update to the parameters. This step of inference is usually the bottleneck for learning, even when performed approximately. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per instance, and has led to efficient training for models in which inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast </context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 3:951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>J Kumlien</author>
</authors>
<title>Constructing biological knowledge-bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,</booktitle>
<pages>77--86</pages>
<contexts>
<context position="19628" citStr="Craven and Kumlien, 1999" startWordPosition="3281" endWordPosition="3284">ed to efficient training for models in which inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between en</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>M. Craven and J. Kumlien. 1999. Constructing biological knowledge-bases by extracting information from text sources. In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology, pages 77–86, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffery Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="7500" citStr="Culotta and Sorensen, 2004" startWordPosition="1223" endWordPosition="1226">ith c E R a relation instance.3 It denotes the membership of the tuple c in the relation R. For example, founded (BILL GATES, MICROSOFT) is a relation instance denoting that BILL GATES and MICROSOFT are related in the founded relation. In the following we will always consider some set of candidate tuples C that may or may not be related. We define Cn C C to be set of all n-ary tuples in C. Note that while our definition considers general n-nary relations, in practice we will restrict us to unary and binary relations C1 and C2. Following previous work (Mintz et al., 2009; Zelenko et al., 2003; Culotta and Sorensen, 2004) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. 2.2 Entity Types An entity can be of one or several entity types. For example, BILL GATES is a person, and a company founder. Entity types correspond to the special case of relations with arity one, and will be treated as such in the following. 3Other commonly used terms are relational facts, ground facts, ground atoms, and assertions. 1014 We care about entity types for two reasons. First, they can be important for downstream applications: if consumers of our extracted facts know the type of</context>
<context position="9939" citStr="Culotta and Sorensen, 2004" startWordPosition="1643" endWordPosition="1646">se the candidate is indeed a relation mention of the relation instance nationality (EVO MORALES, BOLIVIA). 2.4 Relation Extraction We define the task of relation extraction as follows. We are given a corpus of documents and a set of target relations. Then we are asked to predict all relation instances I so that for each R (c) E I there exists at least one relation mention in the given corpus. The above definition covers a range of existing approaches by varying over what we define as target corpus. On one end, we have extractors that process text on a per sentence basis (Zelenko et al., 2003; Culotta and Sorensen, 2004). On the other end, we have methods that take relation mentions from several documents and use these as input features (Mintz et al., 2009; Bunescu and Mooney, 2007). There is a compelling reason for performing relation extraction within a larger scope that considers mentions across documents: redundancy. Often facts are mentioned in several sentences and documents. Some of these mentions may be difficult to parse, or they use unseen patterns. But the more mentions we consider, the higher the probability that one does parse, and fits a pattern we have seen in the training data. Note that for r</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffery Sorensen. 2004. Dependency tree kernels for relation extraction. In 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05),</booktitle>
<pages>363--370</pages>
<contexts>
<context position="23633" citStr="Finkel et al., 2005" startWordPosition="3944" endWordPosition="3947">ance, the score is normalized by the number of relation mentions. For manual evaluation we pick the top ranked 50 relation instances for the most frequent relations. We ask three annotators to inspect the mentions of these relation instances to decide whether they are correct. Upon disagreement, we use majority vote. To summarize precisions across relations, we take 1018 their average, and their average weighted by the proportion of predicted instances for the given relation. 5.1.1 Data preprocessing We preprocess our textual data as follows: We first use the Stanford named entity recognizer (Finkel et al., 2005) to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE. We treat consecutive tokens which share the same category as single entity mention. Then we associate these mentions with Freebase entities. This is achieved by performing a string match between entity mention phrases and the canonical names of entities as present in Freebase. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features X&apos;C similar to those used in (Mintz et</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL ’05), pages 363–370, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.</title>
<date>1990</date>
<pages>452--472</pages>
<marker>Geman, Geman, 1990</marker>
<rawString>S. Geman and D. Geman. 1990. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. pages 452–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19671" citStr="Hoffmann et al., 2010" startWordPosition="3289" endWordPosition="3292">inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate an</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Joint entity and relation extraction using card-pyramid parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL’</booktitle>
<volume>10</volume>
<contexts>
<context position="3775" citStr="Kate and Mooney, 2010" startWordPosition="590" endWordPosition="593">of nationality is always a person while the second is a country. A simple way to address this is a pipeline: first predict entity types, and then condition on these when predicting relations. However, this neglects the fact that relations could as well be used to help entity type prediction. 1013 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013–1023, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics While there is some existing work on enforcing such constraints in a joint fashion (Roth and Yih, 2007; Kate and Mooney, 2010; Riedel et al., 2009), they are not directly applicable here. The difference is the amount of facts they take into account at the same time. They focus on single sentence extractions, and only consider very few interacting facts. This allows them to work with exact optimization techniques such as (Integer) Linear Programs and still remain efficient.2 However, when working on a sentence level they fail to exploit the redundancy present in a corpus. Moreover, the fewer facts they consider at the same time, the lower the chance that some of these will be incompatible, and that modelling compatib</context>
<context position="5922" citStr="Kate and Mooney (2010)" startWordPosition="943" endWordPosition="946"> Wikipedia, and in practice we cannot expect text and training knowledge base to be so close. Hence we also evaluate our approach on the New York Times corpus (out-of-domain setting). For in-domain data we make the following finding. When we compare to an isolated baseline that makes no use of entity types, our joint model improves average precision by 4%. However, it does not outperform a pipelined system. In the out-ofdomain setting, our collective model substantially outperforms both other approaches. Compared to the isolated baseline, we achieve a 15% increase in 2The pyramid algorithm of Kate and Mooney (2010) may scale well, but it is not clear how to apply their scheme to crossdocument extraction. precision. With respect to the pipeline approach, the increase is 13%. In the following we will first give some background information on relation extraction with distant supervision. Then we will present our graphical model as well as the inference and learning techniques we apply. After discussing related work, we present our empirical results and conclude. 2 Background In this section we will introduce the terminology and concepts we use throughout the paper. We will also give a brief introduction to</context>
<context position="20286" citStr="Kate and Mooney (2010)" startWordPosition="3381" endWordPosition="3384">, 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate and Mooney (2010) use a pyramid parsing scheme to achieve the same. Riedel et al. (2009) use Markov Logic to model interactions between event-argument relations for biomedical event extraction. However, their work is (a) supervised, and (b) performs extraction on a per-sentence basis. Carlson et al. (2010) also use selectional preferences. However, instead of exploiting them for training a graphical model using distant supervision, they use selectional preferences to improve a bootstrapping process. Here in each iteration of bootstrapping, extracted facts that violate compatibility constraints will not be used</context>
</contexts>
<marker>Kate, Mooney, 2010</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2010. Joint entity and relation extraction using card-pyramid parsing. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL’ 10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="18343" citStr="Lafferty et al., 2001" startWordPosition="3076" endWordPosition="3079">nded, regardless of the state of Yee. 3.2 Inference a weight θ There are two types of inference we have to perform: sampling from the posterior during training (see section 3.3), and finding the most likely configuration (aka MAP inference). In both settings we employ a eh Gibbs samplerr(Geman andfGeman,s1990) thattrandomly picks a variable Yr and samples its relation value conditioned on its Markov Blanket. At test time we decrease the temperature of our sampler in order to find an approximation of the MAP solution. 3.3 Training Most learning methods need to calculate the model expectations (Lafferty et al., 2001) or the MAP configuration (Collins, 2002) before making an update to the parameters. This step of inference is usually the bottleneck for learning, even when performed approximately. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows th</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Karl Schultz</author>
<author>Sameer Singh</author>
</authors>
<title>Factorie: Probabilistic programming via imperatively defined factor graphs.</title>
<date>2009</date>
<booktitle>Advances in Neural Information Processing Systems 22,</booktitle>
<pages>1249--1257</pages>
<editor>In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors,</editor>
<contexts>
<context position="14535" citStr="McCallum et al., 2009" startWordPosition="2429" endWordPosition="2432">TES,MICROSOFT in figure 1 would contain, among others, the pattern “[M2] was founded by [M1]”. 3.1 Factor Templates Our conditional probability distribution over variables X and Y is defined using using a set T of factor templates. Each template Tj E T defines a set of factors {(yi, xi)1, a set Kj of feature indices, parameters {θkj I k∈Kj and feature functions { fk I k∈Kj . Together they define the following conditional distribution: 1 T7 p (y|x) = Zx Tj∈T (yi,xi)∈Tj (4) In our case the set T consists of four templates we will describe below. We construct this graphical model using FACTORIE (McCallum et al., 2009), a probabilistic programming language that simplifies the construction process, as well as inference and learning. 3.1.1 Bias Template We use a bias template TBias that prefers certain relations a priori over others. When the template is unrolled, it creates one factor per variable Yc for candidate tuple c E C. The template also consists of one weight θBias r and feature function fBias r for each possible relation r. fBias rfires if the relation associated with tuple c is r. 3.1.2 Mention Template In order to extract relations from text, we need to model the correlation between relation insta</context>
</contexts>
<marker>McCallum, Schultz, Singh, 2009</marker>
<rawString>Andrew McCallum, Karl Schultz, and Sameer Singh. 2009. Factorie: Probabilistic programming via imperatively defined factor graphs. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1249–1257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL ’09),</booktitle>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1826" citStr="Mintz et al., 2009" startWordPosition="272" endWordPosition="275">ties expressed in structured or semi-structured text. This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers. It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in 1Also called self training, or weak supervision. place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves muc</context>
<context position="5075" citStr="Mintz et al. (2009)" startWordPosition="806" endWordPosition="809">rms relation extraction across documents, enforces selectional preferences, and needs no labelled data. It is based on an undirected graphical model in which variables correspond to facts, and factors between them measure compatibility. In order to scale up, we run an efficient Gibbs-Sampler at inference time, and train our model using SampleRank (Wick et al., 2009). In practice this leads to a runtime behaviour that is linear in the size of the corpus. For example, 200,000 documents take less than three hours for training and testing. For evaluation we consider two scenarios. First we follow Mintz et al. (2009), use Freebase as source of distant supervision, and employ Wikipedia as source of unlabelled text—we will call this an in-domain setting. This scenario is somewhat artificial in that Freebase itself is partially derived from Wikipedia, and in practice we cannot expect text and training knowledge base to be so close. Hence we also evaluate our approach on the New York Times corpus (out-of-domain setting). For in-domain data we make the following finding. When we compare to an isolated baseline that makes no use of entity types, our joint model improves average precision by 4%. However, it does</context>
<context position="6838" citStr="Mintz et al., 2009" startWordPosition="1098" endWordPosition="1101">our graphical model as well as the inference and learning techniques we apply. After discussing related work, we present our empirical results and conclude. 2 Background In this section we will introduce the terminology and concepts we use throughout the paper. We will also give a brief introduction to relation extraction, in particular in the context of distant supervision. 2.1 Relations We seek to extract facts about entities. Example entities would be the company founder BILL GATES, the company MICROSOFT, and the country USA. A relation R is a set of tuples c over entities. We will follow (Mintz et al., 2009) and call the term R (c1, ... cn) with c E R a relation instance.3 It denotes the membership of the tuple c in the relation R. For example, founded (BILL GATES, MICROSOFT) is a relation instance denoting that BILL GATES and MICROSOFT are related in the founded relation. In the following we will always consider some set of candidate tuples C that may or may not be related. We define Cn C C to be set of all n-ary tuples in C. Note that while our definition considers general n-nary relations, in practice we will restrict us to unary and binary relations C1 and C2. Following previous work (Mintz e</context>
<context position="10077" citStr="Mintz et al., 2009" startWordPosition="1668" endWordPosition="1671">sk of relation extraction as follows. We are given a corpus of documents and a set of target relations. Then we are asked to predict all relation instances I so that for each R (c) E I there exists at least one relation mention in the given corpus. The above definition covers a range of existing approaches by varying over what we define as target corpus. On one end, we have extractors that process text on a per sentence basis (Zelenko et al., 2003; Culotta and Sorensen, 2004). On the other end, we have methods that take relation mentions from several documents and use these as input features (Mintz et al., 2009; Bunescu and Mooney, 2007). There is a compelling reason for performing relation extraction within a larger scope that considers mentions across documents: redundancy. Often facts are mentioned in several sentences and documents. Some of these mentions may be difficult to parse, or they use unseen patterns. But the more mentions we consider, the higher the probability that one does parse, and fits a pattern we have seen in the training data. Note that for relation extraction that considers more than a single mention we have to solve the coreference problem in order to determine which mentions</context>
<context position="11402" citStr="Mintz et al. (2009)" startWordPosition="1886" endWordPosition="1889">processing step. 2.5 Distant Supervision In relation extraction we often encounter a lack of explicitly annotated text, but an abundance of structured data sources such as company databases or collaborative knowledge bases like Freebase. In order to exploit this, many approaches use simple but effective heuristics to align existing facts with unlabelled text. This labelled text can then be used as training material of a supervised learner. One heuristic is to assume that each candidate mention tuple of a training fact is indeed expressing the corresponding relation (Bunescu and Mooney, 2007). Mintz et al. (2009) refer to this as the distant supervision assumption. Clearly, this heuristic can fail. Let us again consider the nationality relation between EVO MORALES and BOLIVIA. In an 2007 article of the New York Times we find this relation mention candidate: (2) ...the troubles faced by Evo Morales in Bolivia... This sentence does not directly express that EVO MORALES is a citizen of BOLIVIA, and hence violates the distant supervision assumption. The problem with this observation is that at training time we may learn a relatively large weight for the feature “&lt;Entity1&gt; in &lt;Entity2&gt;” associated with 101</context>
<context position="12835" citStr="Mintz et al. (2009)" startWordPosition="2126" endWordPosition="2129">ency of Facts As discussed above, distant supervision can lead to noisy extractions. However, such noise can often be easily identified by testing how compatible the extracted facts are to each other. In this work we are concerned with a particular type of compatibility: selectional preferences. Relations require, or prefer, their arguments to be of certain types. For example, the nationality relation requires the first argument to be a person, and the second to be a country. On inspection, we find that these preferences are often not satisfied in a baseline distant supervision system akin to Mintz et al. (2009). This often results from patterns such as “&lt;Entity1&gt; in &lt;Entity2&gt;” that fire in many cases where &lt;Entity2&gt; is a location, but not a country. 3 Model Our observations in the previous section suggest that we should (a) explicitly model compatibility between extracted facts, and (b) integrate evidence from several documents to exploit redundancy. In this work we choose a Conditional Random Field (CRF) to achieve this. CRFs are a natural fit for this task: They allow us to capture correlations in an explicit fashion, and to incorporate overlapping input features from multiple documents. The hidde</context>
<context position="15480" citStr="Mintz et al. (2009)" startWordPosition="2584" endWordPosition="2587"> also consists of one weight θBias r and feature function fBias r for each possible relation r. fBias rfires if the relation associated with tuple c is r. 3.1.2 Mention Template In order to extract relations from text, we need to model the correlation between relation instances and their mentions in text. For this purpose we define the template TMen that connects each relation instance variable Yc with its observed mention variables Xc. Crucially, this template gathers mentions from multiple documents, and enables us to exploit redundancy. The feature functions of this template are taken from Mintz et al. (2009). This includes features that inspect the lexical content between entity mentions in the same sentence, and the syntactic path between them. One example is { 1 yc = founded n ]i with &amp;quot;M2 was founded by M1&amp;quot; E xi c 0 otherwise T7 �k�Kj θj kfj k(yi,xi) e def f101 (yc, xc) = 1016 Figure 1: FactoraGraph ofrour model that captures selectional preferences and functionality constraints. For of elati and tity type aken readability we only label a subsets of equivalent variables and factors. Note that the graph shows an example orrelation between relation instances and We also add a templae TPair hat ea</context>
<context position="19304" citStr="Mintz et al. (2009)" startWordPosition="3230" endWordPosition="3233">r of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per instance, and has led to efficient training for models in which inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic infer</context>
<context position="21462" citStr="Mintz et al. (2009)" startWordPosition="3567" endWordPosition="3570"> compatibility constraints will not be used to generate additional patterns in the next iteration. 5 Experiments We set up experiments to answer the following questions: (i) Does the explicit modelling of selectional preferences improve accuracy? (ii) Can we also perform joint entity and relation extraction in a pipeline and achieve similar results? (iii) How does our cross-document approach scale? To answer these questions we carry out experiments on two data sets, Wikipedia and New York Times articles, and use Freebase as distant supervision source for both. 5.1 Experimental Setup We follow Mintz et al. (2009) and perform two types of evaluation: held-out and manual. In both cases we have a training and a test corpus of documents, and training and test sets of entities. For held-out evaluation we split the set of entities in Freebase into training and test sets. For manual evaluation we use all Freebase entities during training. For testing we use all entities that appear in the test document corpus. For both training and testing we then choose the candidate tuples C that may or may not be relation instances. To pick the entities C1 we want to predict entity types for, we choose all entities that a</context>
<context position="24244" citStr="Mintz et al., 2009" startWordPosition="4049" endWordPosition="4052">., 2005) to find entity mentions in the corpus. The NER tagger segments each document into sentences and classifies each token into four categories: PERSON, ORGANIZATION, LOCATION and NONE. We treat consecutive tokens which share the same category as single entity mention. Then we associate these mentions with Freebase entities. This is achieved by performing a string match between entity mention phrases and the canonical names of entities as present in Freebase. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features X&apos;C similar to those used in (Mintz et al., 2009): lexical, Part-Of-Speech (POS), named entity and syntactic features, i.e. features obtained from the dependency parsing tree of a sentence. We use the openNLP POS tagger4 to obtain POS tags and employ the MaltParser (Nivre et al., 2004) for dependency parsing. For candidate tuples with arity 1 (entity types) we use the following features: the entity’s word form, the POS sequence, the head of the entity in the dependency parse tree, the Stanford named entity tag, and the left and right words to the current entity mention phrase. 5.1.2 Configurations We apply the following configurations of our</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL ’09), pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="24481" citStr="Nivre et al., 2004" startWordPosition="4087" endWordPosition="4090">e category as single entity mention. Then we associate these mentions with Freebase entities. This is achieved by performing a string match between entity mention phrases and the canonical names of entities as present in Freebase. For each candidate tuple c with arity 2 and each of its mention tuples i we extract a set of features X&apos;C similar to those used in (Mintz et al., 2009): lexical, Part-Of-Speech (POS), named entity and syntactic features, i.e. features obtained from the dependency parsing tree of a sentence. We use the openNLP POS tagger4 to obtain POS tags and employ the MaltParser (Nivre et al., 2004) for dependency parsing. For candidate tuples with arity 1 (entity types) we use the following features: the entity’s word form, the POS sequence, the head of the entity in the dependency parse tree, the Stanford named entity tag, and the left and right words to the current entity mention phrase. 5.1.2 Configurations We apply the following configurations of our factor graphs. As our baseline, and roughly equivalent to previous work (Mintz et al., 2009), we pick the templates TBias and TMen. These describe a fully disconnected graph, and we will refer to this configuration as isolated. Next, we</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Hong-Woo Chun</author>
<author>Toshihisa Takagi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A markov logic approach to bio-molecular event extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09),</booktitle>
<pages>41--49</pages>
<contexts>
<context position="3797" citStr="Riedel et al., 2009" startWordPosition="594" endWordPosition="597">s a person while the second is a country. A simple way to address this is a pipeline: first predict entity types, and then condition on these when predicting relations. However, this neglects the fact that relations could as well be used to help entity type prediction. 1013 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013–1023, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics While there is some existing work on enforcing such constraints in a joint fashion (Roth and Yih, 2007; Kate and Mooney, 2010; Riedel et al., 2009), they are not directly applicable here. The difference is the amount of facts they take into account at the same time. They focus on single sentence extractions, and only consider very few interacting facts. This allows them to work with exact optimization techniques such as (Integer) Linear Programs and still remain efficient.2 However, when working on a sentence level they fail to exploit the redundancy present in a corpus. Moreover, the fewer facts they consider at the same time, the lower the chance that some of these will be incompatible, and that modelling compatibility will make a diff</context>
<context position="20357" citStr="Riedel et al. (2009)" startWordPosition="3394" endWordPosition="3397">diction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate and Mooney (2010) use a pyramid parsing scheme to achieve the same. Riedel et al. (2009) use Markov Logic to model interactions between event-argument relations for biomedical event extraction. However, their work is (a) supervised, and (b) performs extraction on a per-sentence basis. Carlson et al. (2010) also use selectional preferences. However, instead of exploiting them for training a graphical model using distant supervision, they use selectional preferences to improve a bootstrapping process. Here in each iteration of bootstrapping, extracted facts that violate compatibility constraints will not be used to generate additional patterns in the next iteration. 5 Experiments W</context>
</contexts>
<marker>Riedel, Chun, Takagi, Tsujii, 2009</marker>
<rawString>Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi, and Jun’ichi Tsujii. 2009. A markov logic approach to bio-molecular event extraction. In Proceedings of the Natural Language Processing in Biomedicine NAACL 2009 Workshop (BioNLP ’09), pages 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</booktitle>
<contexts>
<context position="1874" citStr="Riedel et al., 2010" startWordPosition="280" endWordPosition="283">d text. This includes, for example, the extraction of employer-employee relations mentioned in newswire, or protein-protein interactions expressed in biomedical papers. It also includes the prediction of entity types such as country, citytown or person, if we consider entity types as unary relations. A particularly attractive approach to relation extraction is based on distant supervision.1 Here in 1Also called self training, or weak supervision. place of annotated text, only an existing knowledge base (KB) is needed to train a relation extractor (Mintz et al., 2009; Bunescu and Mooney, 2007; Riedel et al., 2010). The facts in the KB are heuristically aligned to an unlabelled training corpus, and the resulting alignment is the basis for learning the extractor. Naturally, the predictions of a distantly supervised relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves much room for improvement. This room is not as larg</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3752" citStr="Roth and Yih, 2007" startWordPosition="586" endWordPosition="589"> the first argument of nationality is always a person while the second is a country. A simple way to address this is a pipeline: first predict entity types, and then condition on these when predicting relations. However, this neglects the fact that relations could as well be used to help entity type prediction. 1013 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1013–1023, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics While there is some existing work on enforcing such constraints in a joint fashion (Roth and Yih, 2007; Kate and Mooney, 2010; Riedel et al., 2009), they are not directly applicable here. The difference is the amount of facts they take into account at the same time. They focus on single sentence extractions, and only consider very few interacting facts. This allows them to work with exact optimization techniques such as (Integer) Linear Programs and still remain efficient.2 However, when working on a sentence level they fail to exploit the redundancy present in a corpus. Moreover, the fewer facts they consider at the same time, the lower the chance that some of these will be incompatible, and </context>
<context position="20165" citStr="Roth and Yih (2007)" startWordPosition="3363" endWordPosition="3366">pproaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate and Mooney (2010) use a pyramid parsing scheme to achieve the same. Riedel et al. (2009) use Markov Logic to model interactions between event-argument relations for biomedical event extraction. However, their work is (a) supervised, and (b) performs extraction on a per-sentence basis. Carlson et al. (2010) also use selectional preferences. However, instead of exploiting them for training a graphical model using distant supervision, they use selectional preferences to improve a bootstrapping </context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>D. Roth and W. Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia.</location>
<contexts>
<context position="2692" citStr="Sandhaus, 2008" startWordPosition="419" endWordPosition="420"> relation extractor will be less accurate than those of a supervised one. While facts of existing knowledge bases are inexpensive to come by, the heuristic alignment to text will often lead to noisy patterns in learning. When applied to unseen text, these patterns will produce noisy facts. Indeed, we find that extraction precision still leaves much room for improvement. This room is not as large as in previous work (Mintz et al., 2009) where target text and training KB are closely related. However, when we use the knowledge base Freebase (Bollacker et al., 2008) and the New York Times corpus (Sandhaus, 2008), we observe very low precision. For example, the precision of the top-ranked 50 nationality relation instances is only 28%. On inspection, it turns out that many of the errors can be easily identified: they amount to violations of basic compatibility constraints between facts. In particular, we observe unsatisfied selectional preferences of relations towards particular entity types as types of their arguments. An example is the fact that the first argument of nationality is always a person while the second is a country. A simple way to address this is a pipeline: first predict entity types, a</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus, 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling textual inference to the web. In</title>
<date>2008</date>
<booktitle>EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19773" citStr="Schoenmackers et al. (2008)" startWordPosition="3303" endWordPosition="3306">t Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extracted relations. Kate and Mooney (2010) use a pyramid parsing scheme to achieve the same. Riedel et al. (2009) use Markov Logi</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld. 2008. Scaling textual inference to the web. In EMNLP ’08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 79–88, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Karl Schultz</author>
<author>Andrew McCallum</author>
</authors>
<title>Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factor graphs.</title>
<date>2009</date>
<booktitle>In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD),</booktitle>
<pages>414--429</pages>
<contexts>
<context position="19123" citStr="Singh et al., 2009" startWordPosition="3201" endWordPosition="3204">formed approximately. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per instance, and has led to efficient training for models in which inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with e</context>
</contexts>
<marker>Singh, Schultz, McCallum, 2009</marker>
<rawString>Sameer Singh, Karl Schultz, and Andrew McCallum. 2009. Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factor graphs. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), pages 414– 429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel S Weld</author>
<author>Raphael Hoffmann</author>
<author>Fei Wu</author>
</authors>
<title>Using wikipedia to bootstrap open information extraction.</title>
<date>2009</date>
<journal>In ACM SIGMOD Record.</journal>
<contexts>
<context position="19647" citStr="Weld et al., 2009" startWordPosition="3285" endWordPosition="3288">or models in which inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervision Learning to extract relations by using distant supervision has raised much interest in recent years. Our work is inspired by Mintz et al. (2009) who also use Freebase as distant supervision source. We also heuristically align our knowledge base to text by making the distant supervision assumption (Bunescu and Mooney, 2007; Mintz et al., 2009). However, in contrast to these previous approaches, and other related distant supervision methods (Craven and Kumlien, 1999; Weld et al., 2009; Hoffmann et al., 2010), we perform relation extraction collectively with entity type prediction. Schoenmackers et al. (2008) use entailment rules on assertion extracted by TextRunner to increase recall. They also perform cross-document probabilistic inference based on Markov Networks. However, they do not infer the types of entities and work in an open IE setting. Selectional Preferences In the context of supervised relation extraction, selectional preferences have been applied. For example, Roth and Yih (2007) have used Linear Programming to enforce consistency between entity types and extr</context>
</contexts>
<marker>Weld, Hoffmann, Wu, 2009</marker>
<rawString>Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2009. Using wikipedia to bootstrap open information extraction. In ACM SIGMOD Record.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wick</author>
<author>Khashayar Rohanimanesh</author>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>Samplerank: Learning preferences from atomic gradients.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems (NIPS), Workshop on Advances in Ranking.</booktitle>
<contexts>
<context position="4824" citStr="Wick et al., 2009" startWordPosition="762" endWordPosition="765"> present in a corpus. Moreover, the fewer facts they consider at the same time, the lower the chance that some of these will be incompatible, and that modelling compatibility will make a difference. In this work we present a novel approach that performs relation extraction across documents, enforces selectional preferences, and needs no labelled data. It is based on an undirected graphical model in which variables correspond to facts, and factors between them measure compatibility. In order to scale up, we run an efficient Gibbs-Sampler at inference time, and train our model using SampleRank (Wick et al., 2009). In practice this leads to a runtime behaviour that is linear in the size of the corpus. For example, 200,000 documents take less than three hours for training and testing. For evaluation we consider two scenarios. First we follow Mintz et al. (2009), use Freebase as source of distant supervision, and employ Wikipedia as source of unlabelled text—we will call this an in-domain setting. This scenario is somewhat artificial in that Freebase itself is partially derived from Wikipedia, and in practice we cannot expect text and training knowledge base to be so close. Hence we also evaluate our app</context>
<context position="18556" citStr="Wick et al., 2009" startWordPosition="3109" endWordPosition="3112">ion (aka MAP inference). In both settings we employ a eh Gibbs samplerr(Geman andfGeman,s1990) thattrandomly picks a variable Yr and samples its relation value conditioned on its Markov Blanket. At test time we decrease the temperature of our sampler in order to find an approximation of the MAP solution. 3.3 Training Most learning methods need to calculate the model expectations (Lafferty et al., 2001) or the MAP configuration (Collins, 2002) before making an update to the parameters. This step of inference is usually the bottleneck for learning, even when performed approximately. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This update can follow different schemes, here we use MIRA (Crammer and Singer, 2003). This allows the learner to acquire more supervision per instance, and has led to efficient training for models in which inference 1017 is expensive and generally intractable (Singh et al., 2009). 4 Related Work Distant Supervis</context>
</contexts>
<marker>Wick, Rohanimanesh, Culotta, McCallum, 2009</marker>
<rawString>Michael Wick, Khashayar Rohanimanesh, Aron Culotta, and Andrew McCallum. 2009. Samplerank: Learning preferences from atomic gradients. In Neural Information Processing Systems (NIPS), Workshop on Advances in Ranking.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>JMLR,</journal>
<volume>3</volume>
<issue>6</issue>
<pages>1106</pages>
<contexts>
<context position="7471" citStr="Zelenko et al., 2003" startWordPosition="1218" endWordPosition="1222"> term R (c1, ... cn) with c E R a relation instance.3 It denotes the membership of the tuple c in the relation R. For example, founded (BILL GATES, MICROSOFT) is a relation instance denoting that BILL GATES and MICROSOFT are related in the founded relation. In the following we will always consider some set of candidate tuples C that may or may not be related. We define Cn C C to be set of all n-ary tuples in C. Note that while our definition considers general n-nary relations, in practice we will restrict us to unary and binary relations C1 and C2. Following previous work (Mintz et al., 2009; Zelenko et al., 2003; Culotta and Sorensen, 2004) we make one more simplifying assumption: every candidate tuple can be member of at most one relation. 2.2 Entity Types An entity can be of one or several entity types. For example, BILL GATES is a person, and a company founder. Entity types correspond to the special case of relations with arity one, and will be treated as such in the following. 3Other commonly used terms are relational facts, ground facts, ground atoms, and assertions. 1014 We care about entity types for two reasons. First, they can be important for downstream applications: if consumers of our ext</context>
<context position="9910" citStr="Zelenko et al., 2003" startWordPosition="1639" endWordPosition="1642">e. In fact, in this case the candidate is indeed a relation mention of the relation instance nationality (EVO MORALES, BOLIVIA). 2.4 Relation Extraction We define the task of relation extraction as follows. We are given a corpus of documents and a set of target relations. Then we are asked to predict all relation instances I so that for each R (c) E I there exists at least one relation mention in the given corpus. The above definition covers a range of existing approaches by varying over what we define as target corpus. On one end, we have extractors that process text on a per sentence basis (Zelenko et al., 2003; Culotta and Sorensen, 2004). On the other end, we have methods that take relation mentions from several documents and use these as input features (Mintz et al., 2009; Bunescu and Mooney, 2007). There is a compelling reason for performing relation extraction within a larger scope that considers mentions across documents: redundancy. Often facts are mentioned in several sentences and documents. Some of these mentions may be difficult to parse, or they use unseen patterns. But the more mentions we consider, the higher the probability that one does parse, and fits a pattern we have seen in the t</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dimitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. JMLR, 3(6):1083 – 1106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>