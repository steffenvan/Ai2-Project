<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.998769666666667">
Concrete Models and Empirical Evaluations
for the Categorical Compositional
Distributional Model of Meaning
</title>
<author confidence="0.965103">
Edward Grefenstette*†
</author>
<affiliation confidence="0.604563">
Google DeepMind
</affiliation>
<author confidence="0.938988">
Mehrnoosh Sadrzadeh**†
</author>
<affiliation confidence="0.985566">
Queen Mary University of London
</affiliation>
<bodyText confidence="0.978770083333333">
Modeling compositional meaning for sentences using empirical distributional methods has been
a challenge for computational linguists. The categorical model of Clark, Coecke, and Sadrzadeh
(2008) and Coecke, Sadrzadeh, and Clark (2010) provides a solution by unifying a categorial
grammar and a distributional model of meaning. It takes into account syntactic relations
during semantic vector composition operations. But the setting is abstract: It has not been
evaluated on empirical data and applied to any language tasks. We generate concrete models
for this setting by developing algorithms to construct tensors and linear maps and instantiate
the abstract parameters using empirical data. We then evaluate our concrete models against
several experiments, both existing and new, based on measuring how well models align with
human judgments in a paraphrase detection task. Our results show the implementation of this
general abstract framework to perform on par with or outperform other leading models in these
experiments.1
</bodyText>
<sectionHeader confidence="0.997672" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999391125">
The distributional approach to the semantic modeling of natural language, inspired
by the notion—presented by Firth (1957) and Harris (1968)—that the meaning of a
word is tightly related to its context of use, has grown in popularity as a method
of semantic representation. It draws from the frequent use of vector-based document
models in information retrieval, modeling the meaning of words as vectors based on
the distribution of co-occurring terms within the context of a word.
Using various vector similarity metrics as a measure of semantic similarity, these
distributional semantic models (DSMs) are used for a variety of NLP tasks, from
</bodyText>
<note confidence="0.246442">
* DeepMind Technologies Ltd, 5 New Street Square, London EC4A 3 TW.
</note>
<email confidence="0.524158">
E-mail: etg®google.com.
</email>
<note confidence="0.855554222222222">
** School of Electronic Engineering and Computer Science, Queen Mary University of London, Mile End
Road, London E1 4NS, United Kingdom. E-mail: mehrnoosh.sadrzadeh®qmul.ac.uk.
† The work described in this article was performed while the authors were at the University of Oxford.
1 Support from EPSRC grant EP/J002607/1 is acknowledged.
Submission received: 26 September 2012; revised submission received: 31 October 2013;
accepted for publication: 5 April 2014.
doi:10.1162/COLI a 00209
© 2015 Association for Computational Linguistics
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.99989635">
automated thesaurus building (Grefenstette 1994; Curran 2004) to automated essay
marking (Landauer and Dumais 1997). The broader connection to information retrieval
and its applications is also discussed by Manning, Raghavan, and Sch¨utze (2011). The
success of DSMs in essentially word-based tasks such as thesaurus extraction and con-
struction (Grefenstette 1994; Curran 2004) invites an investigation into how DSMs can
be applied to NLP and information retrieval (IR) tasks revolving around larger units of
text, using semantic representations for phrases, sentences, or documents, constructed
from lemma vectors. However, the problem of compositionality in DSMs—of how to go
from word to sentence and beyond—has proved to be non-trivial.
A new framework, which we refer to as DisCoCat, initially presented in Clark,
Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) reconciles
distributional approaches to natural language semantics with the structured, logical
nature of formal semantic models. This framework is abstract; its theoretical predictions
have not been evaluated on real data, and its applications to empirical natural language
processing tasks have not been studied.
This article is the journal version of Grefenstette and Sadrzadeh (2011a, 2011b),
which fill this gap in the DisCoCat literature; in it, we develop a concrete model and
an unsupervised learning algorithm to instantiate the abstract vectors, linear maps, and
vector spaces of the theoretical framework; we develop a series of empirical natural
language processing experiments and data sets and implement our algorithm on large
scale real data; we analyze the outputs of the algorithm in terms of linear algebraic
equations; and we evaluate the model on these experiments and compare the results
with other competing unsupervised models. Furthermore, we provide a linear algebraic
analysis of the algorithm of Grefenstette and Sadrzadeh (2011a) and present an in-depth
study of the better performance of the method of Grefenstette and Sadrzadeh (2011b).
We begin in Section 2 by presenting the background to the task of developing
compositional distributional models. We briefly introduce two approaches to semantic
modeling: formal semantic models and distributional semantic models. We discuss their
differences, and relative advantages and disadvantages. We then present and critique
various approaches to bridging the gap between these models, and their limitations.
In Section 3, we summarize the categorical compositional distributional framework of
Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) and pro-
vide the theoretical background necessary to understand it; we also sketch the road map
of the literature leading to the development of this setting and outline the contributions
of this paper to the field. In Section 4, we present the details of an implementation of this
framework, and introduce learning algorithms used to build semantic representations
in this implementation. In Section 5, we present a series of experiments designed to
evaluate this implementation against other unsupervised distributional compositional
models. Finally, in Section 6 we discuss these results, and posit future directions for this
research area.
</bodyText>
<sectionHeader confidence="0.99605" genericHeader="keywords">
2. Background
</sectionHeader>
<bodyText confidence="0.999193714285714">
Compositional formal semantic models represent words as parts of logical expressions,
and compose them according to grammatical structure. They stem from classical ideas
in logic and philosophy of language, mainly Frege’s principle that the meaning of a
sentence is a function of the meaning of its parts (Frege 1892). These models relate to
well-known and robust logical formalisms, hence offering a scalable theory of meaning
that can be used to reason about language using logical tools of proof and inference.
Distributional models are a more recent approach to semantic modeling, representing
</bodyText>
<page confidence="0.992435">
72
</page>
<note confidence="0.787552">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.998704307692308">
the meaning of words as vectors learned empirically from corpora. They have found
their way into real-world applications such as thesaurus extraction (Grefenstette 1994;
Curran 2004) or automated essay marking (Landauer and Dumais 1997), and have
connections to semantically motivated information retrieval (Manning, Raghavan, and
Sch¨utze 2011). This two-sortedness of defining properties of meaning: “logical form”
versus “contextual use,” has left the quest for “what is the foundational structure of
meaning?”—a question initially the concern of solely linguists and philosophers of
language—even more of a challenge.
In this section, we present a short overview of the background to the work de-
veloped in this article by briefly describing formal and distributional approaches to
natural language semantics, and providing a non-exhaustive list of some approaches
to compositional distributional semantics. For a more complete review of the topic, we
encourage the reader to consult Turney (2012) or Clark (2013).
</bodyText>
<subsectionHeader confidence="0.992741">
2.1 Montague Semantics
</subsectionHeader>
<bodyText confidence="0.977828230769231">
Formal semantic models provide methods for translating sentences of natural language
into logical formulae, which can then be fed to computer-aided automation tools to
reason about them (Alshawi 1992).
To compute the meaning of a sentence consisting of n words, meanings of these
words must interact with one another. In formal semantics, this further interaction is
represented as a function derived from the grammatical structure of the sentence. Such
models consist of a pairing of syntactic analysis rules (in the form of a grammar) with
semantic interpretation rules, as exemplified by the simple model presented on the left
of Figure 1.
The semantic representations of words are lambda expressions over parts of logical
formulae, which can be combined with one another to form well-formed logical expres-
sions. The function  |−  |: G -+ M maps elements of the lexicon G to their interpretation
(e.g., predicates, relations, domain objects) in the logical model M used. Nouns are
typically just logical atoms, whereas adjectives and verbs and other relational words
are interpreted as predicates and relations. The parse of a sentence such as “cats like
milk,” represented here as a binarized parse tree, is used to produce its semantic in-
terpretation by substituting semantic representations for their grammatical constituents
and applying β-reduction where needed. Such a derivation is shown on the right of
Figure 1.
What makes this class of models attractive is that it reduces language meaning
to logical expressions, a subject well studied by philosophers of language, logicians,
and linguists. Its properties are well known, and it becomes simple to evaluate the
meaning of a sentence if given a logical model and domain, as well as verify whether
or not one sentence entails another according to the rules of logical consequence and
deduction.
�
</bodyText>
<subsectionHeader confidence="0.487711">
Syntactic Analysis
</subsectionHeader>
<equation confidence="0.84419425">
S-+NPVP
NP -+ cats, milk, etc.
VP -+ Vt NP
Vt -+ like, hug, etc.
Semantic Interpretation
|VP|(|NP|)
|cats|, |milk|, ...
|Vt|(|NP|)
Tyx.|like|(x,y), ...
|like|(|cats|, |milk|)
|cats |Tx.|like|(x, |milk|)
Tyx.|like|(x,y) |milk|
</equation>
<figureCaption confidence="0.929267">
Figure 1
</figureCaption>
<bodyText confidence="0.797697">
A simple model of formal semantics.
</bodyText>
<page confidence="0.996509">
73
</page>
<note confidence="0.840048">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.9999625">
However, such logical analysis says nothing about the closeness in meaning or
topic of expressions beyond their truth-conditions and which models satisfy these truth
conditions. Hence, formal semantic approaches to modeling language meaning do not
perform well on language tasks where the notion of similarity is not strictly based on
truth conditions, such as document retrieval, topic classification, and so forth. Further-
more, an underlying domain of objects and a valuation function must be provided,
as with any logic, leaving open the question of how we might learn the meaning of
language using such a model, rather than just use it.
</bodyText>
<subsectionHeader confidence="0.997162">
2.2 Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.971925027777778">
A popular way of representing the meaning of words in lexical semantics is as dis-
tributions in a high-dimensional vector space. This approach is based on the distri-
butional hypothesis of Harris (1968), who postulated that the meaning of a word was
dictated by the context of its use. The more famous dictum stating this hypothesis
is the statement of Firth (1957) that “You shall know a word by the company it
keeps.” This view of semantics has furthermore been associated (Grefenstette 2009;
Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein
(presented in Wittgenstein 1953), who stated that language meaning was equivalent to
its real world use.
Practically speaking, the meaning of a word can be learned from a corpus by looking
at what other words occur with it within a certain context, and the resulting distribution
can be represented as a vector in a semantic vector space. This vectorial representation is
convenient because vectors are a familiar structure with a rich set of ways of computing
vector distance, allowing us to experiment with different word similarity metrics. The
geometric nature of this representation entails that we can not only compare individual
words’ meanings with various levels of granularity (e.g., we might, for example, be able
to show that cats are closer to kittens than to dogs, but that all three are mutually closer
than cats and steam engines), but also apply methods frequently called upon in IR tasks,
such as those described by Manning, Raghavan, and Sch¨utze (2011), to group concepts
by topic, sentiment, and so on.
The distribution underlying word meaning is a vector in a vector space, the basis
vectors of which are dictated by the context. In simple models, the basis vectors will
be annotated with words from the lexicon. Traditionally, the vector spaces used in
such models are Hilbert spaces (i.e., vector spaces with orthogonal bases, such that
the inner product of any one basis vector with another [other than itself] is zero). The
semantic vector for any word can be represented as the weighted sum of the basis
vectors:
−−−−−−−→ some word = ci−→ni
i
where {−→ni }i is the basis of the vector space the meaning of the word lives in, and ci E R
is the weight associated with basis vector −→ni .
The construction of the vector for a word is done by counting, for each lexicon
word ni associated with basis vector −→ni , how many times it occurs in the context of
each occurrence of the word for which we are constructing the vector. This count is then
typically adjusted according to a weighting scheme (e.g., TF-IDF). The “context” of a
word can be something as simple as the other words occurring in the same sentence as
</bodyText>
<page confidence="0.994509">
74
</page>
<note confidence="0.786393">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.998201">
the word or within k words of it, or something more complex, such as using dependency
relations (Pad´o and Lapata 2007) or other syntactic features.
Commonly, the similarity of two semantic vectors is computed by taking their
cosine measure, which is the sum of the product of the basis weights of the vectors:
</bodyText>
<equation confidence="0.908585">
cgcb
cosine(−→a , b ) =
�i
</equation>
<bodyText confidence="0.9997335">
where cai and cbi are the basis weights for −→a and −→b , respectively. However, other
options may be a better fit for certain implementations, typically dependent on the
weighting scheme.
Readers interested in learning more about these aspects of distributional lexical
semantics are invited to consult Curran (2004), which contains an extensive overview
of implementation options for distributional models of word meaning.
</bodyText>
<subsectionHeader confidence="0.994697">
2.3 Compositionality and Vector Space Models
</subsectionHeader>
<bodyText confidence="0.9960870625">
In the previous overview of distributional semantic models of lexical semantics, we
have seen that DSMs are a rich and tractable way of learning word meaning from a
corpus, and obtaining a measure of semantic similarity of words or groups of words.
However, it should be fairly obvious that the same method cannot be applied to sen-
tences, whereby the meaning of a sentence would be given by the distribution of other
sentences with which it occurs.
First and foremost, a sentence typically occurs only once in a corpus, and hence
substantial and informative distributions cannot be created in this manner. More im-
portantly, human ability to understand new sentences is a compositional mechanism:
We understand sentences we have never seen before because we can generate sentence
meaning from the words used, and how they are put into relation. To go from word
vectors to sentence vectors, we must provide a composition operation allowing us to con-
struct a sentence vector from a collection of word vectors. In this section, we will discuss
several approaches to solving this problem, their advantages, and their limitations.
2.3.1 Additive Models. The simplest composition operation that comes to mind is straight-
forward vector addition, such that:
</bodyText>
<equation confidence="0.993972">
ab = −→
−→ a + −→b
</equation>
<bodyText confidence="0.972287615384615">
Conceptually speaking, if we view word vectors as semantic information distributed
across a set of properties associated with basis vectors, using vector addition as a
semantic composition operation states that the information of a set of lemmas in a
sentence is simply the sum of the information of the individual lemmas. Although
crude, this approach is computationally cheap, and appears sufficient for certain NLP
tasks: Landauer and Dumais (1997) show it to be sufficient for automated essay marking
tasks, and Grefenstette (2009) shows it to perform better than a collection of other simple
similarity metrics for summarization, sentence paraphrase, and document paraphrase
detection tasks.
However, there are two principal objections to additive models of composition: first,
−−−−−−−−−−−−→ −−→=
vector addition is commutative, therefore, John drank wine = John + drank + wine
✓�i (cai )2 Ei (cbi )2
</bodyText>
<page confidence="0.975463">
75
</page>
<note confidence="0.6450525">
Computational Linguistics Volume 41, Number 1
−−−−−−−−−−−−→
</note>
<bodyText confidence="0.9729035">
Wine drank John, and thus vector addition ignores syntactic structure completely; and
second, vector addition sums the information contained in the vectors, effectively jum-
bling the meaning of words together as sentence length grows.
The first objection is problematic, as the syntactic insensitivity of additive models
leads them to equate the representation of sentences with patently different meanings.
Mitchell and Lapata (2008) propose to add some degree of syntactic sensitivity—namely,
accounting for word order—by weighting word vectors according to their order of
appearance in a sentence as follows:
</bodyText>
<equation confidence="0.993032">
ab = α−→
−→ a + β−→ b
</equation>
<bodyText confidence="0.967222375">
−−−−−−−−−−−−→ not have the same representation as Wine drank John = α · wine + β · drank + γ · −−−→ John.
The question of how to obtain weights and whether they are only used to reflect
word order or can be extended to cover more subtle syntactic information is open,
but it is not immediately clear how such weights may be obtained empirically and
whether this mode of composition scales well with sentence length and increase in
syntactic complexity. Guevara (2010) suggests using machine-learning methods such
as partial least squares regression to determine the weights empirically, but states that
this approach enjoys little success beyond minor composition such as adjective-noun
or noun-verb composition, and that there is a dearth of metrics by which to evaluate
such machine learning–based systems, stunting their growth and development.
The second objection states that vector addition leads to increase in ambiguity as
we construct sentences, rather than decrease in ambiguity as we would expect from
giving words a context; and for this reason Mitchell and Lapata (2008) suggest replacing
additive models with multiplicative models as discussed in Section 2.3.2, or combining
them with multiplicative models to form mixture models as discussed in Section 2.3.3.
2.3.2 Multiplicative Models. The multiplicative model of Mitchell and Lapata (2008) is
an attempt to solve the ambiguity problem discussed in Section 2.3.1 and provide
implicit disambiguation during composition. The composition operation proposed is
the component-wise multiplication (0) of two vectors: Vectors are expressed as the
weighted sum of their basis vectors, and the weight of the basis vectors of the com-
posed vector is the product of the weights of the original vectors; for −→a = �i ci−→ni , and
→− b = �i ci −→ni , we have
−→ ab =−→a O→−b = cici −→ni
i
Such multiplicative models are shown by Mitchell and Lapata (2008) to perform better
at verb disambiguation tasks than additive models for noun-verb composition, against
a baseline set by the original verb vectors. The experiment they use to show this will
also serve to evaluate our own models, and form the basis for further experiments, as
discussed in Section 5.
This approach to compositionality still suffers from two conceptual problems:
First, component-wise multiplication is still commutative and hence word order is not
accounted for; second, rather than “diluting” information during large compositions
</bodyText>
<equation confidence="0.356442">
−−−−−−−−−−−−→ −−→
</equation>
<bodyText confidence="0.675659">
where α, β∈R. Consequently, John
</bodyText>
<figure confidence="0.95124925">
drank wine=α · John
+ β · drank +γ·−−−→
−−−→
wine would
</figure>
<page confidence="0.797089">
76
</page>
<note confidence="0.638635">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.962770787878788">
and creating ambiguity, it may remove too much through the “filtering” effect of
component-wise multiplication.
The first problem is more difficult to deal with for multiplicative models than for
additive models, because both scalar multiplication and component-wise multiplication
are commutative and hence α��a O β�� b = α�� b O βs and thus word order cannot be
taken into account using scalar weights.
To illustrate how the second problem entails that multiplicative models do not scale
well with sentence length, we look at the structure of component-wise multiplication
again: a O b = �i cici n . For any i, if ci = 0 or ci = 0, then cici = 0, and therefore
for any composition, the number of non-zero basis weights of the produced vector is
less than or equal to the number of non-zero basis weights of the original vectors: At
each composition step information is filtered out (or preserved, but never increased).
Hence, as the number of vectors to be composed grows, the number of non-zero
basis weights of the product vector stays the same or—more realistically—decreases.
Therefore, for any composition of the form ���������
a1 . . . ai . . . an = ai O ... O a O ... O n, if
for any i, a is orthogonal to �������
a1 . . . ai−1 then ���������
a1 . . . ai . . . an = 0 . It follows that purely
multiplicative models alone are not apt as a single mode of composition beyond noun-
verb (or adjective-noun) composition operations.
One solution to this second problem not discussed by Mitchell and Lapata (2008)
would be to introduce some smoothing factor s E R+ for point-wise multiplication
such that a O ��b = �i (ci + s)(c�i + s)n , ensuring that information is never completely
filtered out. Seeing how the problem of syntactic insensitivity still stands in the way of
full-blown compositionality for multiplicative models, we leave it to those interested in
salvaging purely multiplicative models to determine whether some suitable value of s
can be determined.
2.3.3 Mixture Models. The problems faced by multiplicative models presented in Sec-
tion 2.3.2 are acknowledged in passing by Mitchell and Lapata (2008), who propose
mixing additive and multiplicative models in the hope of leveraging the advantage of
each while doing away with their pitfalls. This is simply expressed as the weighted sum
of additive and multiplicative models:
</bodyText>
<equation confidence="0.99686">
ab = α��
�� a + β�� b + γ(a O b )
</equation>
<bodyText confidence="0.999620714285714">
where α, β, and γ are predetermined scalar weights.
The problems for these models are threefold. First, the question of how scalar
weights are to be obtained still needs to be determined. Mitchell and Lapata (2008) con-
cede that one advantage of purely multiplicative models over weighted additive or
mixture models is that the lack of scalar weights removes the need to optimize the scalar
weights for particular tasks (at the cost of not accounting for syntactic structure), and
avoids the methodological concerns accompanying this requirement.
Second, the question of how well this process scales from noun-verb composition
to more syntactically rich expressions must be addressed. Using scalar weights to
account for word order seems ad hoc and superficial, as there is more to syntactic
structure than the mere ordering of words. Therefore, an account of how to build
sentence vectors for sentences such as The dog bit the man and The man was bitten by
the dog in order to give both sentences the same (or a similar) representation would
need to give a richer role to scalar weights than mere token order. Perhaps specific
</bodyText>
<page confidence="0.994469">
77
</page>
<note confidence="0.335903">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.942412740740741">
weights could be given to particular syntactic classes (such as nouns) to introduce a
more complex syntactic element into vector composition, but it is clear that this alone
is not a solution, as the weight for nouns dog and man would be the same, allowing
for the same commutative degeneracy observed in non-weighted additive models, in
which −−−−−−−−−−−−−−→
the dog bit the man = −−−−−−−−−−−−−−→
the man bit the dog. Introducing a mixture of weighting
systems accounting for both word order and syntactic roles may be a solution, but it is
not only ad hoc but also arguably only partially reflects the syntactic structure of the
sentence.
The third problem is that Mitchell and Lapata (2008) show that in practice, although
mixture models perform better at verb disambiguation tasks than additive models and
weighted additive models, they perform equivalently to purely multiplicative models
with the added burden of requiring parametric optimization of the scalar weights.
Therefore, whereas mixture models aim to take the best of additive and multiplica-
tive models while avoiding their problems, they are only partly successful in achieving
the latter goal, and demonstrably do little better in achieving the former.
2.3.4 Tensor-Based Models. From Sections 2.3.1–2.3.3 we observe that the need for incor-
porating syntactic information into DSMs to achieve true compositionality is pressing,
if only to develop a non-commutative composition operation that can take into account
word order without the need for adhoc weighting schemes, and hopefully richer syn-
tactic information as well.
An early proposal by Smolensky and colleagues (Smolensky 1990; Smolensky and
Legendre 2006) to use linear algebraic tensors as a composition operation solves the
problem of finding non-commutative vector composition operators. The composition of
two vectors is their tensor product, sometimes called the kronecker product when ap-
plied to vectors rather than vector spaces. For −→a ∈ V = Ei ci−→ni , and →−b ∈ W = Ej cf n ,
</bodyText>
<equation confidence="0.9745605">
j
we have:
→− �ab = −→ a ⊗ −→ b = cicj −→ni ⊗ −→ nj
ij
</equation>
<bodyText confidence="0.999791764705882">
The composition operation takes the original vectors and maps them to a vector in a
larger vector space V ⊗ W, which is the tensor space of the original vectors’ spaces.
Here the second instance of ⊗ is not a recursive application of the kronecker product,
but rather the pairing of basis elements of V and W to form a basis element of V ⊗ W.
The shared notation and occasional conflation of kronecker and tensor products may
seem confusing, but is fairly standard in multilinear algebra.
The advantage of this approach is twofold: First, vectors for different words need
not live in the same spaces but can be composed nonetheless. This allows us to repre-
sent vectors for different word classes (topics, syntactic roles, etc.) in different spaces
with different bases, which was not possible under additive or multiplicative models.
Second, because the product vector lives in a larger space, we obtain the intuitive notion
that the information of the whole is richer and more complex than the information of
the parts.
Dimensionality Problems. However, as observed and commented upon by Smolensky
himself, this increase in dimensionality brings two rather large problems for tensor
based models. The first is computational: The size of the product vector space is the
product of the size of the original vector spaces. If we assume that all words live in the
</bodyText>
<page confidence="0.986682">
78
</page>
<bodyText confidence="0.987325086956522">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
same space N of dimensionality dim(N), then the dimensionality of an n-word sentence
vector is dim(N)n. If we have as many basis vectors for our word semantic space as
there are lexemes in our vocabulary (e.g., approximately 170k in English2), then the size
of our sentence vectors quickly reaches magnitudes for which vector comparison (or
even storage) are computationally intractable.3 Even if, as most DSM implementations
do, we restrict the basis vectors of word semantic spaces to the k (e.g., k = 2,000) most
frequent words in a corpus, the sentence vector size still grows exponentially with
sentence length, and the implementation problems remain.
The second problem is mathematical: Sentences of different length live in different
spaces, and if we assign different vector spaces to different word types (e.g., syntactic
classes), then sentences of different syntactic structure live in different vector spaces,
and hence cannot be compared directly using inner product or cosine measures, leaving
us with no obvious mode of semantic comparison for sentence vectors. If any model
wishes to use tensor products in composition operations, it must find some way of
reducing the dimensionality of product vectors to some common vector space so that
they may be directly compared.
One notable method by which these dimensionality problems can be solved in
general are the holographic reduced representations proposed by Plate (1991). The
product vector of two vectors is projected into a space of smaller dimensionality by
circular convolution to produce a trace vector. The circular correlation of the trace vector
and one of the original vectors produces a noisy version of the other original vector.
The noisy vector can be used to recover the clean original vector by comparing it with a
predefined set of candidates (e.g., the set of word vectors if our original vectors are word
meanings). Traces can be summed to form new traces effectively containing several vec-
tor pairs from which original vectors can be recovered. Using this encoding/decoding
mechanism, the tensor product of sets of vectors can be encoded in a space of smaller
dimensionality, and then recovered for computation without ever having to fully repre-
sent or store the full tensor product, as discussed by Widdows (2008).
There are problems with this approach that make it unsuitable for our purposes,
some of which are discussed in Mitchell and Lapata (2010). First, there is a limit to
the information that can be stored in traces, which is independent of the size of the
vectors stored, but is a logarithmic function of their number. As we wish to be able
to store information for sentences of variable word length without having to directly
represent the tensored sentence vector, setting an upper bound to the number of vectors
that can be composed in this manner limits the length of the sentences we can represent
compositionally using this method.
Second, and perhaps more importantly, there are restrictions on the nature of
the vectors that can be encoded in such a way: The vectors must be independently
distributed such that the mean Euclidean length of each vector is 1. Such conditions
are unlikely to be met in word semantic vectors obtained from a corpus; and as the
failure to do so affects the system’s ability to recover clean vectors, holographic re-
duced representations are not prima facie usable for compositional DSMs, although it
is important to note that Widdows (2008) considers possible application areas where
they may be of use, although once again these mostly involve noun-verb and adjective-
noun compositionality rather than full blown sentence vector construction. We retain
</bodyText>
<footnote confidence="0.9943065">
2 Source:http://www.oxforddictionaries.com/page/howmanywords.
3 At four bytes per integer, and one integer per basis vector weight, the vector for John loves Mary
would require roughly (170, 000 · 4)3 ≈ 280 petabytes of storage, which is over ten times the data Google
processes on a daily basis according to Dean and Ghemawat (2008).
</footnote>
<page confidence="0.996168">
79
</page>
<note confidence="0.35508">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.9824616">
from Plate (1991) the importance of finding methods by which to project the tensored
sentence vectors into a common space for direct comparison, as will be discussed further
in Section 3.
Syntactic Expressivity. An additional problem of a more conceptual nature is that using
the tensor product as a composition operation simply preserves word order. As we
discussed in Section 2.3.3, this is not enough on its own to model sentence meaning. We
need to have some means by which to incorporate syntactic analysis into composition
operations.
Early work on including syntactic sensitivity into DSMs by Grefenstette (1992)
suggests using crude syntactic relations to determine the frame in which the distri-
butions for word vectors are collected from the corpus, thereby embedding syntac-
tic information into the word vectors. This idea was already present in the work of
Smolensky, who used sums of pairs of vector representations and their roles, obtained
by taking their tensor products, to obtain a vector representation for a compound. The
application of these ideas to DSMs was studied by Clark and Pulman (2007), who
suggest instantiating the roles to dependency relations and using the distributional
representations of words as the vectors. For example, in the sentence Simon loves red
wine, Simon is the subject of loves, wine is its object, and red is an adjective describing
wine. Hence, from the dependency tree with loves as root node, its subject and object as
children, and their adjectival descriptors (if any) as their children, we read the following
</bodyText>
<equation confidence="0.5595698">
���� structure: loves ®���
subj®�����
Simon®obj ® wine
inner products of tensor products:
(a ® b  |c ® d ) = (a  |c ) x (b  |d )
</equation>
<bodyText confidence="0.9950685">
We can therefore express inner-products of sentence vectors efficiently without ever
having to actually represent the tensored sentence vector:
</bodyText>
<table confidence="0.821430083333333">
(−−−−−−−−−−−−→
Simon loves red wine  |−−−−−−−−−−−−−−→
Mary likes delicious ros´e)
= �−−→ −→ −→  |−→ o j ⊗ ro- −→ −−−−→
loves ⊗ −→ adj ⊗ red likes ⊗ −→ adj ⊗ delicious�
subj ⊗ −−→ subj ⊗ −−→
Simon ⊗ −→ obj ⊗ −−→ Mary ⊗
wine ⊗
= (−−→ |adj−→� ×(−→red  |−−−−→
loves |likes)× (−→ delicious)
s ubj |−→
s ubj) ounnx(S  |−−→
</table>
<equation confidence="0.747416">
Mary�×�−→ obj |−→obj)×(−−→wine  |−→
ros´e� × �−→ adj
= −−→loves  |−→
likes × −−→Simon  |−−→
Mary~ × −−→wine  |−→
ros´e × −→red  |−−−−→
delicious
</equation>
<bodyText confidence="0.862039285714286">
This example shows that this formalism allows for sentence comparison of sentences
with identical dependency trees to be broken down to term-to-term comparison without
the need for the tensor products to ever be computed or stored, reducing computation
to inner product calculations.
However, although matching terms with identical syntactic roles in the sentence
works well in the given example, this model suffers from the same problems as the
original tensor-based compositionality of Smolensky (1990) in that, by the authors’
own admission, sentences of different syntactic structure live in spaces of different
dimensionality and thus cannot be directly compared. Hence we cannot use this to
measure the similarity between even small variations in sentence structure, such as the
pair “Simon likes red wine” and “Simon likes wine.”
2.3.5 SVS Models. The idea of including syntactic relations to other lemmas in word
representations discussed in Section 2.3.4 is applied differently in the structured vector
a�j&apos; ®re ed. Using the equality relation for
</bodyText>
<page confidence="0.891895">
80
</page>
<bodyText confidence="0.929280142857143">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
space model presented by Erk and Pad´o (2008). They propose to represent word mean-
ings not as simple vectors, but as triplets:
w = (v, R, R−1)
where v is the word vector, constructed as in any other DSM, R and R−1 are selectional
preferences, and take the form of R → D maps where R is the set of dependency
relations, and D is the set of word vectors. Selectional preferences are used to encode
the lemmas that w is typically the parent of in the dependency trees of the corpus in the
case of R, and typically the child of in the case of R−1.
Composition takes the form of vector updates according to the following protocol.
Let a = (va, Ra, R−1
a ) and b = (vb, Rb, R−1
b ) be two words being composed, and let r be the
dependency relation linking a to b. The vector update procedure is as follows:
</bodyText>
<equation confidence="0.7257498">
a&apos; = (va O R−1
b (r), Ra − {r}, R−1
a )
b� = (vb O Ra(r), Rb, R−1
b − {r})
</equation>
<bodyText confidence="0.999926393939394">
where a&apos;, b&apos; are the updated word meanings, and O is whichever vector composition
(addition, component-wise multiplication) we wish to use. The word vectors in the
triplets are effectively filtered by combination with the lemma which the word they
are being composed with expects to bear relation r to, and this relation between the
composed words a and b is considered to be used and hence removed from the domain
of the selectional preference functions used in composition.
This mechanism is therefore a more sophisticated version of the compositional
disambiguation mechanism discussed by Mitchell and Lapata (2008) in that the com-
bination of words filters the meaning of the original vectors that may be ambiguous
(e.g., if we have one vector for bank); but contrary to Mitchell and Lapata (2008), the
information of the original vectors is modified but essentially preserved, allowing for
further combination with other terms, rather than directly producing a joint vector for
the composed words. The added fact that R and R−1 are partial functions associated
with specific lemmas forces grammaticality during composition, since if a holds a
dependency relation r to b which it never expects to hold (for example a verb having
as its subject another verb, rather than the reverse), then Ra and R−1
b are undefined for r
and the update fails. However, there are some problems with this approach if our goal
is true compositionality.
First, this model does not allow some of the “repeated compositionality” we need
because of the update of R and R−1. For example, we expect that an adjective composed
with a noun produces something like a noun in order to be further composed with a verb
or even another adjective. However, here, because the relation adj would be removed
from R−1
b for some noun b composed with an adjective a, this new representation b&apos;
would not have the properties of a noun in that it would no longer expect composition
with an adjective, rendering representations of simple expressions like “the new red
car” impossible. Of course, we could remove the update of the selectional preference
functions from the compositional mechanism, but then we would lose this attractive
feature of grammaticality enforcement through the partial functionality of R and R−1.
Second, this model does little more than represent the implicit disambiguation that
is expected during composition, rather than actually provide a full blown compositional
model. The inability of this system to provide a novel mechanism by which to obtain
</bodyText>
<page confidence="0.994984">
81
</page>
<note confidence="0.354518">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999563538461538">
a joint vector for two composed lemmas—thereby building towards sentence vectors—
entails that this system provides no means by which to obtain semantic representations
of larger syntactic structures that can be compared by inner product or cosine measure
as is done with any other DSM. Of course, this model could be combined with the
compositional models presented in Sections 2.3.1–2.3.3 to produce sentence vectors, but
whereas some syntactic sensitivity would have been obtained, the word ordering and
other problems of the aforementioned models would still remain, and little progress
would have been made towards true compositionality.
We retain from this attempt to introduce compositionality in DSMs that including
information obtained from syntactic dependency relations is important for proper dis-
ambiguation, and that having some mechanism by which the grammaticality of the
expression being composed is a precondition for its composition is a desirable feature
for any compositional mechanism.
</bodyText>
<subsectionHeader confidence="0.988427">
2.4 Matrix-Based Compositionality
</subsectionHeader>
<bodyText confidence="0.999819285714286">
The final class of approaches to vector composition we wish to discuss are three
matrix-based models.
Generic Additive Model. The first is the Generic Additive Model of Zanzotto et al. (2010).
This is a generalization of the weighted additive model presented in Section 2.3.1.
In this model, rather than multiplying lexical vectors by fixed parameters α and β
before adding them to form the representation of their combination, they are instead
the arguments of matrix multiplication by square matrices A and B:
</bodyText>
<equation confidence="0.957598">
ab = A−→
→− a + B−→ b
</equation>
<bodyText confidence="0.999660428571429">
Here, A and B represent the added information provided by putting two words into
relation.
The numerical content of A and B is learned by linear regression over triplets
(−→a , −→b , −→c ) where −→a and −→b are lexical semantic vectors, and −→c is the expected output
of the combination of −→a and −→b . This learning system thereby requires the provision of
labeled data for linear regression to be performed. Zanzotto et al. (2010) suggest several
sources for this labeled data, such as dictionary definitions and word etymologies.
This approach is richer than the weighted additive models because the matrices
act as linear maps on the vectors they take as “arguments,” and thus can encode more
subtle syntactic or semantic relations. However, this model treats all word combinations
as the same operation—for example, treating the combination of an adjective with its
argument and a verb with its subject as the same sort of composition. Because of the
diverse ways there are of training such supervised models, we leave it to those who
wish to further develop this specific line of research to perform such evaluations.
Adjective Matrices. The second approach is the matrix-composition model of Baroni and
Zamparelli (2010), which they develop only for the case of adjective-noun composition,
although their approach can seamlessly be used for any other predicate-argument com-
position. Contrary to most of the earlier approaches proposed, which aim to combine
two lexical vectors to form a lexical vector for their combination, Baroni and Zamparelli
suggest giving different semantic representations to different types, or more specifically
to adjectives and nouns.
</bodyText>
<page confidence="0.990341">
82
</page>
<bodyText confidence="0.9702494">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
In this model, nouns are lexical vectors, as with other models. However, embracing
a view of adjectives that is more in line with formal semantics than with distributional
semantics, they model adjectives as linear maps taking lexical vectors as input and pro-
ducing lexical vectors as output. Such linear maps can be encoded as square matrices,
and applied to their arguments by matrix multiplication. Concretely, let Madjective be the
matrix encoding the adjective’s linear map, and −−→
noun be the lexical semantic vector for
a noun; their combination is simply
adjective noun = Madjective × −−→ −−−−−−−−−→
noun
Similarly to the Generic Additive Model, the matrix for each adjective is learned
by linear regression over a set of pairs (−−→
noun, →−c ) where the vectors −−→
noun are the lexical
semantic vectors for the arguments of the adjective in a corpus, and −→c is the semantic
vector corresponding to the expected output of the composition of the adjective with
that noun.
This may, at first blush, also appear to be a supervised training method for learn-
ing adjective matrices from “labeled data,” seeing how the expected output vectors
are needed. However, Baroni and Zamparelli (2010) work around this constraint by
automatically producing the labeled data from the corpus by treating the adjective-
noun compound as a single token, and learning its vector using the same distributional
learning methods they used to learn the vectors for nouns. This same approach can be
extended to other unary relations without change and, using the general framework
of the current article, an extension of it to binary predicates has been presented in
Grefenstette et al. (2013), using multistep regression. For a direct comparison of the
results of this approach with some of the results of the current article, we refer the
reader to Grefenstette et al. (2013).
Recursive Matrix-Vector Model. The third approach is the recently developed Recursive
Matrix-Vector Model (MV-RNN) of Socher et al. (2012), which claims the two matrix-
based models described here as special cases. In MV-RNN, words are represented as a
pairing of a lexical semantic vector −→a with an operation matrix A. Within this model,
given the parse of a sentence in the form of a binarized tree, the semantic representation
(−→c , C) of each non-terminal node in the tree is produced by performing the following
two operations on its children (−→a , A) and (−→b , B).
First, the vector component −→c is produced by applying the operation matrix of one
child to the vector of the other, and vice versa, and then projecting both of the products
back into the same vector space as the child vectors using a projection matrix W, which
must also be learned:
</bodyText>
<equation confidence="0.932497">
r B × b
c→−= W × L A × −→b 1
</equation>
<bodyText confidence="0.992802">
Second, the matrix C is calculated by projecting the pairing of matrices A and B back
into the same space, using a projection matrix WM, which must also be learned:
</bodyText>
<equation confidence="0.972986666666667">
r A 1
C = WM ×
B
</equation>
<page confidence="0.974434">
83
</page>
<note confidence="0.336348">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999981">
The pairing (−→c , C) obtained through these operations forms the semantic representation
of the phrase falling under the scope of the segment of the parse tree below that node.
This approach to compositionality yields good results in the experiments described
in Socher et al. (2012). It furthermore has appealing characteristics, such as treating
relational words differently through their operation matrices, and allowing for recursive
composition, as the output of each composition operation is of the same type of object
as its inputs. This approach is highly general and has excellent coverage of different
syntactic types, while leaving much room for parametric optimization.
The principal mathematical difference with the compositional framework presented
subsequently is that composition in MV-RNN is always a binary operation; for exam-
ple, to compose a transitive verb with its subject and object one would first need to
compose it with its object, and then compose the output of that operation with the
subject. The framework we discuss in this article allows for the construction of larger
representations for relations of larger arities, permitting the simultaneous composition
of a verb with its subject and object. Whether or not this theoretical difference leads to
significant differences in composition quality requires joint evaluation. Additionally,
the description of MV-RNN models in Socher et al. (2012) specifies the need for a
source of learning error during training, which is easy to measure in the case of label
prediction experiments such as sentiment prediction, but non-trivial in the case of
paraphrase detection where no objective label exists. A direct comparison to MV-RNN
methods within the context of experiments similar to those presented in this article has
been produced by Blacoe and Lapata (2012), showing that simple operations perform
on par with the earlier complex deep learning architectures produced by Socher and
colleagues; we leave direct comparisons to future work. Early work has shown that the
addition of a hidden layer with non-linearities to these simple models will improve the
results.
</bodyText>
<subsectionHeader confidence="0.999612">
2.5 Some Other Approaches to Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.99968552631579">
Domains and Functions. In recent work, Turney (2012) suggests modeling word repre-
sentations not as a single semantic vector, but as a pair of vectors: one containing the
information of the word relative to its domain (the other words that are ontologically
similar), and another containing information relating to its function. The former vector
is learned by looking at what nouns a word co-occurs with, and the latter is learned
by looking at what verb-based patterns the word occurs in. Similarity between sets of
words is not determined by a single similarity function, but rather through a combi-
nation of comparisons of the domain components of words’ representations with the
function components of the words’ representations. Such combinations are designed
on a task-specific basis. Although Turney’s work does not directly deal with vector
composition of the sort we explore in this article, Turney shows that similarity measures
can be designed for tasks similar to those presented here. The particular limitation of
his approach, which Turney discusses, is that similarity measures must be specified for
each task, whereas most of the compositional models described herein produce repre-
sentations that can be compared in a task-independent manner (e.g., through cosine
similarity). Nonetheless, this approach is innovative, and will merit further attention in
future work in this area.
Language as Algebra. A theoretical model of meaning as context has been proposed in
Clarke (2009, 2012). In that model, the meaning of any string of words is a vector built
</bodyText>
<page confidence="0.937803">
84
</page>
<bodyText confidence="0.9705005">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
from the occurrence of the string in a corpus. This is the most natural extension of
distributional models from words to strings of words: in that model, one builds vectors
for strings of words in exactly the same way as one does for words. The main problem,
however, is that of data sparsity for the occurrences of strings of words. Words do
appear repeatedly in a document, but strings of words, especially for longer strings,
rarely do so; for instance, it hardly happens that an exact same sentence appears more
than once in a document. To overcome this problem, the model is based on the hypo-
thetical concept of an infinite corpus, an assumption that prevents it from being applied
to real-world corpora and experimented within natural language processing tasks. On
the positive side, the model provides a theoretical study of the abstract properties of
a general bilinear associative composition operator; in particular, it is shown that this
operator encompasses other composition operators, such as addition, multiplication,
and even tensor product.
</bodyText>
<sectionHeader confidence="0.980098" genericHeader="introduction">
3. DisCoCat
</sectionHeader>
<bodyText confidence="0.9999255">
In Section 2, we discussed lexical DSMs and the problems faced by attempts to provide
a vector composition operation that would allow us to form distributional sentence rep-
resentations as a function of word meaning. In this section, we will present an existing
formalism aimed at solving this compositionality problem, as well as the mathematical
background required to understand it and further extensions, building on the features
and failures of previously discussed attempts at syntactically sensitive compositionality.
Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010)
propose adapting a category theoretic model, inspired by the categorical compositional
vector space model of quantum protocols (Abramsky and Coecke 2004), to the task
of compositionality of semantic vectors. Syntactic analysis in the form of pregroup
grammars—a categorial grammar—is given categorical semantics in order to be repre-
sented as a compact closed category P (a concept explained subsequently), the objects of
which are syntactic types and the morphisms of which are the reductions forming the
basis of syntactic analysis. This syntactic category is then mapped onto the semantic
compact closed category FVect of finite dimensional vector spaces and linear maps.
The mapping is done in the product category FVect × P via the following procedure.
Each syntactic type is interpreted as a vector space in which semantic vectors for words
with that particular syntactic type live; the reductions between the syntactic types are
interpreted as linear maps between the interpreted vector spaces of the syntactic types.
The key feature of category theory exploited here is its ability to relate in a canonical
way different mathematical formalisms that have similar structures, even if the original
formalisms belong in different branches of mathematics. In this context, it has enabled
us to relate syntactic types and reductions to vector spaces and linear maps and obtain
a mechanism by which syntactic analysis guides semantic composition operations.
This pairing of syntactic analysis and semantic composition ensures both that
grammaticality restrictions are in place as in the model of Erk and Pad´o (2008) and
syntactically driven semantic composition in the form of inner-products provide the im-
plicit disambiguation features of the compositional models of Erk and Pad´o (2008) and
Mitchell and Lapata (2008). The composition mechanism also involves the projection of
tensored vectors into a common semantic space without the need for full representation
of the tensored vectors in a manner similar to Plate (1991), without restriction to the
nature of the vector spaces it can be applied to. This avoids the problems faced by other
tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman
(2007).
</bodyText>
<page confidence="0.995585">
85
</page>
<note confidence="0.343009">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999865818181818">
The word vectors can be specified model-theoretically and the sentence space can be
defined over Boolean values to obtain grammatically driven truth-theoretic semantics
in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008).
Some logical operators can be emulated in this setting, such as using swap matrices for
negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based
variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain
a non-truth theoretic semantic model of sentence meaning for which logical operations
have yet to be defined.
Before explaining how this formalism works, in Section 3.3, we will introduce the
notions of pregroup grammars in Section 3.1, and the required basics of category theory
in Section 3.2.
</bodyText>
<subsectionHeader confidence="0.999324">
3.1 Pregroup Grammars
</subsectionHeader>
<bodyText confidence="0.9819565">
Presented by Lambek (1999, 2008) as a successor to his syntactic calculus (Lambek 1958),
pregroup grammars are a class of categorial type grammars with pregroup algebras
as semantics. Pregroups are particularly interesting within the context of this work
because of their well-studied algebraic structure, which can trivially be mapped onto the
structure of the category of vector spaces, as will be discussed subsequently. Logically
speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which
the tensor and its dual par coincide; this logic is sometimes referred to as Bi-Compact
Linear Logic (Lambek 1999). The formalism works alongside the general guidelines of
other categorial grammars, for instance, those of the combinatory categorial grammar
(CCG) designed by Steedman (2001) and Steedman and Baldridge (2011). They consist
of atomic grammatical types that combine to form compound types. A series of CCG-
like application rules allow for type-reductions, forming the basis of syntactic analysis.
As our first step, we show how this syntactic analysis formalism works by presenting
an introduction to pregroup algebras.
Pregroups. A pregroup is an algebraic structure of the form (P, ≤, ·,1, (−)l, (−)r). Its
elements are defined as follows:
</bodyText>
<listItem confidence="0.9972268">
• P is a set {a, b, c,. . .}.
• The relation ≤ is a partial ordering on P.
• The binary operation · is an associative, non-commutative monoid
multiplication with the type − · − : P × P → P, such that if a, b ∈ P then
a · b ∈ P. In other words, P is closed under this operation.
• 1 ∈ P is the unit, satisfying a · 1 = a = 1 · a for all a ∈ P.
• (−)l and (−)r are maps with types (−)l : P → P and (−)r : P → P such that
for any a ∈ P, we have that al, ar ∈ P. The images of these maps are
referred to as the left and the right adjoints. These are unique and satisfy
the following conditions:
– Reversal: if a ≤ b then bl ≤ al (and similarly for ar, br).
– Ordering: a · ar ≤ 1 &lt; ar · a and al · a &lt; 1 ≤ a · al.
– Cancellation: alr- -
– Self-adjointness of identity: 1r = 1 = 1l.
– Self-adjointness of multiplication: (a · b)r = br · ar.
</listItem>
<bodyText confidence="0.856601">
= a = art
86
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
As a notational simplification we write ab for a · b, and if abcd &lt; cd we write abcd -+
cd and call this a reduction, omitting the identity wherever it might appear. Monoid
multiplication is associative, so parentheses may be added or removed for notational
clarity without changing the meaning of the expression as long as they are not directly
under the scope of an adjoint operator.
An example reduction in pregroup might be:
</bodyText>
<equation confidence="0.762361">
aarbclc -+ bclc -+ b
</equation>
<bodyText confidence="0.985360864864865">
We note here that the reduction order is not always unique, as we could have reduced as
follows: aarbclc -+ aarb -+ b. As a further notational simplification, if there exists a chain
of reductions a -+ ... -+ b we may simply write a -+ b (in virtue of the transitivity of
partial ordering relations). Hence in our given example, we can express both reduction
paths as aarbclc -+ b.
Pregroups and Syntactic Analysis. Pregroups are used for grammatical analysis by freely
generating the set P of a pregroup from the basic syntactic types n, s, ..., where here
n stands for the type of both a noun and a noun phrase and s for that of a sentence.
The conflation of nouns and noun phrases suggested here is done to keep the work
discussed in this article as simple as possible, but we could of course model them as
different types in a more sophisticated version of this pregroup grammar. As in any
categorial grammar, words of the lexicon are assigned one or more possible types
(corresponding to different syntactic roles) in a predefined type dictionary, and the
grammaticality of a sentence is verified by demonstrating the existence of a reduction
from the combination of the types of words within the sentence to the sentence type s.
For example, having assigned to noun phrases the type n and to sentences the type s,
the transitive verbs will have the compound type nrsnl. We can read from the type of
a transitive verb that it is the type of a word which “expects” a noun phrase on its left
and a noun phrase on its right, in order to produce a sentence. A sample reduction of
John loves cake with John and cake being noun phrases of type n and loves being a verb of
type nrsnl is as follows:
n(nrsnl)n -+ s
We see that the transitive verb has combined with the subject and object to reduce to a
sentence. Because the combination of the types of the words in the string John loves cake
reduces to s, we say that this string of words is a grammatical sentence. As for more
examples, we recall that intransitive verbs can be given the type nrs such that John sleeps
would be analyzed in terms of the reduction n(nrs) -+ s. Adjectives can be given the
type nnl such that red round rubber ball would be analyzed by (nnl)(nnl)(nnl)n -+ n. And
so on and so forth for other syntactic classes.
Lambek (2008) presents the details of a slightly more complex pregroup grammar
with a richer set of types than presented here. This grammar is hand-constructed and
iteratively extended by expanding the type assignments as more sophisticated gram-
matical constructions are discussed. No general mechanism is proposed to cover all
such types of assignments for larger fragments (e.g., as seen in empirical data). Pregroup
grammars have been proven to be learnable by B´echet, Foret, and Tellier (2007), who
also discuss the difficulty of this task and the nontractability of the procedure. Because
of these constraints and lack of a workable pregroup parser, the pregroup grammars
</bodyText>
<page confidence="0.982995">
87
</page>
<note confidence="0.337793">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999680975">
we will use in our categorical formalism are derived from CCG types, as we explain in
the following.
Pregroup Grammars and Other Categorial Grammars. Pregroup grammars, in contrast with
other categorial grammars such as CCG, do not yet have a large set of tools for parsing
available. If quick implementation of the formalism described later in this paper is
required, it would be useful to be able to leverage the mature state of parsing tools
available for other categorial grammars, such as the Clark and Curran (2007) statistical
CCG parser, as well as Hockenmaier’s CCG lexicon and treebank (Hockenmaier 2003;
Hockenmaier and Steedman 2007). In other words, is there any way we can translate at
least some subset of CCG types into pregroup types?
There are some theoretical obstacles to consider first: Pregroup grammars and CCG
are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to
context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be
weakly equivalent to more expressive mildly context-sensitive grammars. However, if
our goal is to exploit the CCG used in Clark and Curran’s parser, or Hockenmaier’s
lexicon and treebank, we may be in luck: Fowler and Penn (2010) prove that some
CCGs, such as those used in the aforementioned tools, are strongly context-free and thus
expressively equivalent to pregroup grammars. In order to be able to apply the parsing
tools for CCGs to our setting, we use a translation mechanism from CCG types to pre-
group types based on the Lambek-calculus-to-pregroup-grammar translation originally
presented in Lambek (1999). In this mechanism, each atomic CCG type X is assigned a
unique pregroup type x; for any X/Y in CCG we have xyl in the pregroup grammar; and
for any X\Y in CCG we have yrx in pregroup grammar. Therefore, by assigning NP to n
and S to s we could, for example, translate the CCG transitive verb type (S\NP)/NP into
the pregroup type nrsnl, which corresponds to the pregroup type we used for transitive
verbs in Section 3.1. Wherever type replacement (e.g., N → NP) is allowed in CCG we
set an ordering relation in the pregroup grammar (e.g., n¯ ≤ n, where n¯ is the pregroup
type associated with N). Because forward and backward slash “operators” in CCG are
not associative whereas monoid multiplication in pregroups is, it is evident that some
information is lost during the translation process. But because the translation we need
is one-way, we may ignore this problem and use CCG parsing tools to obtain pregroup
parses. Another concern lies with CCG’s crossed composition and substitution rules.
The translations of these rules do not in general hold in a pregroup; this is not a surprise
as pregroups are a simplification of the Lambek Calculus and these rules did not hold
in the Lambek Calculus either, as shown in Moortgat (1997), for example. However, for
the phenomena modeled in this paper, the CCG rules without the backward cross rules
will suffice. In general for the case of English, one can avoid the use of these rules by
overloading the lexicon and using additional categories. To deal with languages that
have cross dependancies, such as Dutch, various solutions have been suggested (e.g.,
see Genkin, Francez, and Kaminski 2010; Preller 2010).
</bodyText>
<subsectionHeader confidence="0.99859">
3.2 Categories
</subsectionHeader>
<bodyText confidence="0.999948833333333">
Category theory is a branch of pure mathematics that allows for a general and uniform
formulation of various different mathematical formalisms in terms of their main struc-
tural properties using a few abstract concepts such as objects, arrows, and combinations
and compositions of these. This uniform conceptual language allows for derivation
of new properties of existing formalisms and for relating these to properties of other
formalisms, if they bear similar categorical representation. In this function, it has been
</bodyText>
<page confidence="0.961924">
88
</page>
<bodyText confidence="0.976633421052632">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
at the center of recent work in unifying two orthogonal models of meaning, a qualitative
categorial grammar model and a quantitative distributional model (Clark, Coecke, and
Sadrzadeh 2008; Coecke, Sadrzadeh, and Clark 2010). Moreover, the unifying categori-
cal structures at work here were inspired by the ones used in the foundations of physics
and the modeling of quantum information flow, as presented in Abramsky and Coecke
(2004), where they relate the logical structure of quantum protocols to their state-based
vector spaces data. The connection between the mathematics used for this branch of
physics and those potentially useful for linguistic modeling has also been noted by
several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004).
In this section, we will briefly examine the basics of category theory, monoidal
categories, and compact closed categories. The focus will be on defining enough ba-
sic concepts to proceed rather than provide a full-blown tutorial on category theory
and the modeling of information flow, as several excellent sources already cover both
aspects (e.g., Mac Lane 1998; Walters 1991; Coecke and Paquette 2011). A categories-
in-a-nutshell crash course is also provided in Clark, Coecke, and Sadrzadeh (2008) and
Coecke, Sadrzadeh, and Clark (2010).
The Basics of Category Theory. A basic category C is defined in terms of the following
elements:
</bodyText>
<listItem confidence="0.999978">
• A collection of objects ob(C).
• A collection of morphisms hom(C).
• A morphism composition operation ◦.
</listItem>
<bodyText confidence="0.999680666666667">
Each morphism f has a domain dom(f) ∈ ob(C) and a codomain codom(f) ∈ ob(C).
For dom(f) = A and codom(f) = B we abbreviate these definitions as f : A → B. Despite
the notational similarity to function definitions (and sets and functions being an ex-
ample of a category), it is important to state that nothing else is presupposed about
morphisms, and we should not treat them a priori as functions.
The following axioms hold in every category C :
</bodyText>
<listItem confidence="0.998803">
• For any f : A → B and g : B → C there exists h : A → C and h = g ◦ f.
• For any f : A → B, g : B → C and h : C → D, ◦ satisfies
(h ◦ g) ◦ f = h ◦ (g ◦ f ).
• For every A ∈ ob(C) there is an identity morphism idA : A → A such
that for any f : A → B, f ◦ idA = f = idB ◦ f.
</listItem>
<bodyText confidence="0.992698272727273">
We can model various mathematical formalisms using these basic concepts, and
verify that these axioms hold for them. For example, category Set with sets as objects and
functions as morphisms, or category Rel with sets as objects and relations as morphisms,
category Pos with posets as objects and order-preserving maps as morphisms, and
category Group with groups as objects and group homomorphisms as morphisms, to
name a few.
The product C × D of two categories C and D is a category with pairs (A, B) as
objects, where A ∈ ob(C) and B ∈ ob(D). There exists a morphism (f, g) : (A, B) → (C, D)
in C × D if and only if there exists f : A → C ∈ hom(C) and g : B → D ∈ hom(D). Product
categories allow us to relate objects and morphisms of one mathematical formalism to
those in another, in this example those of C to D.
</bodyText>
<page confidence="0.995178">
89
</page>
<figure confidence="0.261539666666667">
Computational Linguistics Volume 41, Number 1
Compact Closed Categories. A monoidal category C is a basic category to which we add a
monoidal tensor ® such that:
</figure>
<listItem confidence="0.999179875">
• For all A, B E ob(C) there is an object A ® B E ob(C).
• For all A, B, C E ob(C), we have (A ® B) ® C = A ® (B ® C).
• There exists some I E ob(C) such that for any A E ob(C), we have
I®A=A=A®I.
• For f : A -+ C and g : B -+ D in hom(C) there is f ® g : A ® B -+ C ® D in
hom(C).
• For f1 : A -+ C, f2 : B -+ D, g1 : C -+ E and g2 : D -+ F the following
equality holds:
</listItem>
<equation confidence="0.988564">
(g1 ® g2) ° (f1 ® f2) = (g1 ° f1) ® (g2 ° f2)
</equation>
<bodyText confidence="0.986818">
The product category of two monoidal categories has a monodial tensor, defined point-
wisely by (a, A) ® (b, B) := (a ® b, A ® B).
A compact closed category C is a monoidal category with the following additional
axioms:
</bodyText>
<listItem confidence="0.936915615384615">
• Each object A E ob(C) has left and right “adjoint” objects Al and Ar in
ob(C).
• There exist four structural morphisms for each object A E ob(C):
– ηlA : I -+ A ® Al.
– ηr A : I -+ Ar ® A.
– Cl A : Al ® A -+ I.
– CrA : A ® Ar -+ I.
• The previous structural morphisms satisfy the following equalities:
– (1A ® ClA) ° (ηlA ® 1A) = 1A.
– (ErA ® 1A) ° (1A ® ηrA) = 1A.
– (1Ar ® Cr A) ° (ηr ® 1Ar) = 1Ar.
– (eA® 1Al) ° (1Al ® ηlA = 1Al.
CA
</listItem>
<bodyText confidence="0.999708733333333">
closed categories come equipped with complete graphical calculi, sur-
veyed in Selinger (2010). These calculi visualize and simplify the axiomatic reasoning
within the category to a great extent. In particular, Clark, Coecke, and Sadrzadeh
(2008) and Coecke, Sadrzadeh, and Clark (2010) show how they depict the pregroup
grammatical reductions and visualize the flow of information in composing meanings
of single words and forming meanings for sentences. Although useful at an abstract
level, these calculi do not play the same simplifying role when it comes to the concrete
and empirical computations; therefore we will not discuss them in this article.
A very relevant example of a compact closed category is a pregroup algebra P.
Elements of a pregroup are objects of the category, the partial ordering relation provides
the morphisms, 1 is I, and monoidal multiplication is the monoidal tensor.
Another very relevant example is the category FVect of finite dimensional Hilbert
spaces and linear maps over R—that is, vector spaces over R with orthogonal bases
of finite dimension, and an inner product operation (−  |−) : A x A -+ R for every
vector space A. The objects of FVect are vector spaces, and the morphisms are linear
</bodyText>
<page confidence="0.966705">
90
</page>
<note confidence="0.486053">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.999818">
maps between vector spaces. The unit object is R and the monoidal tensor is the linear
algebraic tensor product. FVect is degenerate in its adjoints, in that for any vector space
A, we have Al = Ar = A*, where A* is the dual space of A. Moreover, by fixing a basis we
obtain that A* ∼= A. As such, we can effectively do away with adjoints in this category,
and “collapse” cl, er, ηl, and ηr maps into “adjoint-free” c and η maps. In this category,
the c maps are inner product operations, CA : A ⊗ A → R, and the η maps η : R → A ⊗ A
generate maximally entangled states, also referred to as Bell-states.
</bodyText>
<subsectionHeader confidence="0.999132">
3.3 A Categorical Passage from Grammar to Semantics
</subsectionHeader>
<bodyText confidence="0.986164423076923">
In Section 3.2 we showed how a pregroup algebra and vector spaces can be modeled
as compact closed categories and how product categories allow us to relate the objects
and morphisms of one category to those of another. In this section, we will present how
Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) suggest
building on this by using categories to relate semantic composition to syntactic analysis
in order to achieve syntax-sensitive composition in DSMs.
3.3.1 Syntax Guides Semantics. The product category FVect × P has as object pairs (A, a),
where A is a vector space and a is a pregroup grammatical type, and as morphism pairs
( f, ≤) where f is a linear map and ≤ a pregroup ordering relation. By the definition of
product categories, for any two vector space-type pairs (A, a) and (B, b), there exists
a morphism (A, a) → (B, b) only if there is both a linear map from A into B and a
partial ordering a → b. If we view these pairings as the association of syntactic types
with vector spaces containing semantic vectors for words of that type, this restriction
effectively states that a linear map from A to B is only “permitted” in the product
category if a reduces to b.
Both P and FVect being compact closed, it is simple to show that FVect × P is as well,
by considering the pairs of unit objects and structural morphisms from the separate
categories: I is now (R, 1), and the structural morphisms are (CA, Cla), (CA, Cra), (ηA,ηla),
and (ηA,ηra). We are particularly interested in the c maps, which are defined as follows
(from the definition of product categories):
(EA, ElA) : (A ⊗ A, ala) → (R, 1) (EA, ErA) : (A ⊗ A, aar) → (R, 1)
This states that whenever there is a reduction step in the grammatical analysis of a
sentence, there is a composition operation in the form of an inner product on the
semantic front. Hence, if nouns of type n live in some noun space N and transitive
verbs of type nlsnr live in some space N ⊗ S ⊗ N, then there must be some structural
morphism of the form:
</bodyText>
<equation confidence="0.638321">
(CN ⊗ 1S ⊗ eN, ern1sCln) : (N ⊗ (N ⊗ S ⊗ N) ⊗ N, n(nrsnl)n) → (S, s)
</equation>
<bodyText confidence="0.999765">
We can read from this morphism the functions required to compose a sentence with a
noun, a transitive verb, and an object to obtain a vector living in some sentence space S,
namely, (EN ⊗ 1S ⊗ EN).
The form of a syntactic type is therefore what dictates the structure of the semantic
space associated with it. The structural morphisms of the product category guarantee
that for every syntactic reduction there is a semantic composition morphism provided
by the product category: syntactic analysis guides semantic composition.
</bodyText>
<page confidence="0.985407">
91
</page>
<figure confidence="0.493641">
Computational Linguistics Volume 41, Number 1
</figure>
<figureCaption confidence="0.3241955">
3.3.2 Example. To give an example, we give syntactic type n to nouns, and nrs to intran-
sitive verbs. The grammatical reduction for kittens sleep, namely, nnrs → s, corresponds
</figureCaption>
<bodyText confidence="0.660009692307692">
−−−−→
to the morphism ern ® 1s in P. The syntactic types dictate that the noun kittens lives
in some vector space N, and the intransitive verb −−−→
sleep in N ® S. The reduction mor-
−−−−→
ph−−−→ism gives us the composition morphism (CN ® 1
S), which we can apply to kittens
® sleep.
Because we can express any vector as the weighted sum of its basis vectors, let
=kittensle
us expand −−−−→
kittens ci −→ni and sleep Eij cij ni ® →−sj ; then we can express the
composition as follows:
</bodyText>
<equation confidence="0.976689692307692">
kittens sleep = (EN ® 1S)(−−−−→
−−−−−−−−→ kittens ® −−−→
sleep)
⎛= (CN ® 1S) Ecl&amp;quot;−→ni ®Ec�k−→nj ® →−sk
i jk
⎛ ⎞
= (EN ® 1S) Eckittens
i ckkn®−→nj ® −→sk ⎠
ijk
�= crtenscjk(−→ni  |−→nj)−→ sk
ijk
�= ckittens
ik i csleep ik −→sk
</equation>
<bodyText confidence="0.999181285714286">
In these equations, we have expressed the vectors in their explicit form, we have
consolidated the sums by virtue of distributivity of linear algebraic tensor product over
addition, we have applied the tensored linear maps to the vector components (as the
weights are scalars), and finally, we have simplified the indices since (−→ni  |−→nj) = 1 if
→−ni = −→nj and 0 otherwise. As a result of these, we have obtained a vector that lives in
sentence space S.
Transitive sentences can be dealt with in a similar fashion:
</bodyText>
<equation confidence="0.981431181818182">
=(CN ® 1S ® CN)(−−−−→
kittens ® −−−→
chase ®−−→
mice)
⎛ ⎛ ⎞
⎝� ⎝� �
= ckien chase —+ Cmcen(EN ® 1S ® EN) tt Cl ri ® s ®
i jkl m
= (EN ® 1S ® EN) ckittenscjklasecmice ni−→ ®nj−→ ®sk−→ ®nl−→ ® nm−→
jklm
(i
</equation>
<table confidence="0.736915666666667">
=E ckittens
ijklm i cchase
jkl cmice m(−→ni |nj−→)−→ sk (−→nl  |−→nm)
kittens chase mice
−−−−−−−−−−−−−→
ens chase ce−→
=E k ,tt
ci- cikm cm sk
ikm
</table>
<page confidence="0.872954">
92
</page>
<note confidence="0.444498">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.998801">
In both cases, it is important to note that the tensor product passed as argument
the composition morphism, namely, kittens ® sleepep in the intransitive case and
in the transitive case, never needs to be computed. We can
treat the tensor products here as commas separating function arguments, thereby
avoiding the dimensionality problems presented by earlier tensor-based approaches to
compositionality.
</bodyText>
<subsectionHeader confidence="0.999388">
3.4 This Article and the DisCoCat Literature
</subsectionHeader>
<bodyText confidence="0.988641">
owing sections.
</bodyText>
<sectionHeader confidence="0.920184" genericHeader="method">
4. Concrete Semantic Spaces
</sectionHeader>
<bodyText confidence="0.999960194444445">
As elaborated on in Section 2.3.4, the first general setting for pairing meaning vec-
tors with syntactic types was proposed in Clark and Pulman (2007). The setting of a
DisCoCat generalized this by making the meaning derivation process rely on a syntactic
type system, hence overcoming its central problem whereby the vector representations
of strings of words with different grammatical structure lived in different spaces. A
preliminary version of a DisCoCat was developed in Clark, Coecke, and Sadrzadeh
(2008), a full version was elaborated on in Coecke, Sadrzadeh, and Clark (2010), where,
based on the developments of Preller and Sadrzadeh (2010), it was also exemplified
how the vector space model may be instantiated in a truth theoretic setting where
meanings of words were sets of their denotations and meanings of sentences were their
truth values. A nontechnical description of this theoretical setting was presented in
Clark (2013), where a plausibility truth-theoretic model for sentence spaces was worked
out and exemplified. The work of Grefenstette et al. (2011) focused on a tangential
branch and developed a toy example where neither words nor sentence spaces were
Boolean. The applicability of the theoretical setting to a real empirical natural language
processing task and data from a large scale corpus was demonstrated in Grefenstette
and Sadrzadeh (2011a, 2011b). There, we presented a general algorithm to build vector
representations for words with simple and complex types and the sentences containing
them; then applied the algorithm to a disambiguation task performed on the British
National Corpus (BNC). We also investigated the vector representation of transitive
verbs and showed how a number of single operations may optimize the performance.
We discuss these developments in detail in the foll
In Section 3.3 we presented a categorical formalism that relates syntactic analysis
steps to semantic composition operations. The structure of our syntactic representation
dictates the structure of our semantic spaces, but in exchange, we are provided with
composition functions by the syntactic analysis, rather than having to stipulate them
ad hoc. Whereas the approaches to compositional DSMs presented in Section 2 either
failed to take syntax into account during composition, or did so at the cost of not being
able to compare sentences of different structure in a common space, this categorical
approach projects all sentences into a common sentence space where they can be directly
compared. However, this alone does not give us a compositional DSM.
As we have seen in the previous examples, the structure of semantic spaces varies
with syntactic types. We therefore cannot construct vectors for different syntactic types
in the same way, as they live in spaces of different structure and dimensionali ty. Further-
more, nothing has yet been said about the structure of the sentence space S into which
expressions reducing to type s are projected. If we wish to have a compositional DSM
</bodyText>
<page confidence="0.969264">
93
</page>
<figure confidence="0.9904636">
to
kittens ®−−−→
−−−−→chase ®−−→
mice
Computational Linguistics Volume 41, Number 1
</figure>
<bodyText confidence="0.997179090909091">
that leverages all the benefits of lexical DSMs and ports them to sentence-level distribu-
tional representations, we must specify a new sort of vector construction procedure.
In the original formulation of this formalism by Clark, Coecke, and Sadrzadeh
(2008) and Coecke, Sadrzadeh, and Clark (2010), examples of how such a compositional
DSM could be used for logical evaluation are presented, where S is defined as a Boolean
space with True and False as basis vectors. However, the word vectors used are hand-
written and specified model-theoretically, as the authors leave it for future research to
determine how such vectors might be obtained from a corpus. In this section, we will
discuss a new way of constructing vectors for compositional DSMs, and of defining
sentence space S, in order to reconcile this powerful categorical formalism with the
applicability and flexibility of standard distributional models.
</bodyText>
<subsectionHeader confidence="0.996908">
4.1 Defining Sentence Space
</subsectionHeader>
<bodyText confidence="0.957829">
Assume the following sentences are all true:
</bodyText>
<listItem confidence="0.9409288">
1. The dogs chased the cats.
2. The dogs annoyed the cats.
3. The puppies followed the kittens.
4. The train left the station.
5. The president followed his agenda.
</listItem>
<bodyText confidence="0.999828">
If asked which sentences have similar meaning, we would most likely point to the
pair (1) and (3), and perhaps to a lesser degree (1) and (2), and (2) and (3). Sentences (4)
and (5) obviously speak of a different state of the world as the other sentences.
If we compare these by truth value, we obviously have no means of making such
distinctions. If we compare these by lexical similarity, (1) and (2) seem to be a closer
match than (1) and (3). If we are classifying these sentences by some higher order
relation such as “topic,” (5) might end up closer to (3) than (1). What, then, might cause
us to pair (1) and (3)?
Intuitively, this similarity seems to be because the subjects and objects brought into
relation by similar verbs are themselves similar. Abstracting away from tokens to some
notion of property, we might say that both (1) and (3) express the fact that something
furry and feline and furtive is being pursued by something aggressive (or playful)
and canine. Playing along with the idea that lexical distributional semantics present
concepts (word meanings) as “messy bundles of properties,” it seems only natural to
have the way these properties are acted upon, qualified, and related as the basis for
sentence-level distributional representations. In this respect, we here suggest that the
sentence space S, instead of qualifying the truth value of a sentence, should express
how the properties of the semantic objects within are qualified or brought into relation
by verbs, adjectives, and other predicates and relations.
More specifically, we examine two suggestions for defining the sentence space,
namely, SI = N for sentences with intransitive verbs and ST = N ® N for sentences
with transitive verbs. These definitions mean that the sentence space’s dimensions are
commensurate with either those of N, or those of N ® N. These are by no means the
only options, but as we will discuss here, they offer practical benefits.
In the case of SI, the basis elements are labeled with unique basis elements of N,
hence, s1�� = n1, s2�� = Z, and so on. In the case of ST, the basis elements are labeled with
</bodyText>
<page confidence="0.989591">
94
</page>
<note confidence="0.492445">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.855147">
unique ordered pairs of elements from N, for example, −→s1 = −−−−→
</bodyText>
<equation confidence="0.969977333333333">
(n1, n1), −→s2 = −−−−→
(n2, n1), −→s3 =
−−−−→
</equation>
<bodyText confidence="0.999455294117647">
(n1, n2), and so on, following the same matrix flattening structure used in the proof of
the equal cardinality of N and Q. Because of the isomorphism between ST and N ® N, we
will use the notations −−−−→
(ni, nj) and −→ni ® −→nj interchangeably, as both constitute appropriate
ways of representing the basis elements of such a space. To propagate this distinction
on the syntactic level, we define types sI and sT for intransitive and transitive sentences,
respectively.
In creating this distinction, we lost one of the most appealing features of the frame-
work of Coecke, Sadrzadeh, and Clark (2010), namely, the result that all sentence vectors
live in the same sentence space. A mathematical solution to this two-space problem was
suggested in Grefenstette et al. (2011), and a variant of the models presented in this
article permitting the non-problematic projection into a single sentence space (S ∼= N)
has been presented by Grefenstette et al. (2013). Keeping this separation allows us to
deal with both the transitive and intransitive cases in a simpler manner, and because
the experiments in this article only compare intransitive sentences with intransitive
sentences, and transitive sentences with transitive sentences, we will not address the
issue of unification here.
</bodyText>
<subsectionHeader confidence="0.87994">
4.2 Noun-Oriented Types
</subsectionHeader>
<bodyText confidence="0.999966571428572">
While Lambek’s pregroup types presented in Lambek (2008) include a rich array of
basic types and hand-designed compound types in order to capture specific grammatic
properties, for the sake of simplicity we will use a simpler set of grammatical types
for experimental purposes, similar to some common types found in the CCG-bank
(Steedman 2001).
We assign a basic pregroup type n for all nouns, with an associated vector space N
for their semantic representations. Furthermore, we will treat noun-phrases as nouns,
assigning to them the same pregroup type and semantic space.
CCG treats intransitive verbs as functions NP\S that consume a noun phrase and
return a sentence, and transitive verbs as functions (NP\S)/NP that consume a noun
phrase and return an intransitive verb function, which in turn consumes a noun phrase
and returns a sentence. Using our distinction between intransitive sentences, we give
intransitive verbs the type nrsI associated with the semantic space N ® SI, and transitive
verbs the type nrsTnl associated with the semantic space N ® ST ® N.
Adjectives, in CCG, are treated as functions NP/NP, consuming a noun phrase and
returning a noun phrase; and hence we give them the type nnl and associated semantic
space N ® N.
With the provision of a learning procedure for vectors in these semantic spaces, we
can use these types to construct sentence vector representations for simple intransitive
verb–based and transitive verb–based sentences, with and without adjectives applied
to subjects and objects.
</bodyText>
<subsectionHeader confidence="0.99896">
4.3 Learning Procedures
</subsectionHeader>
<bodyText confidence="0.9783714">
To begin, we construct the semantic space N for all nouns in our lexicon (typically lim-
ited by the words available in the corpus used). Any distributional semantic model can
be used for this stage, such as those presented in Curran (2004), or the lexical semantic
models used by Mitchell and Lapata (2008). It seems reasonable to assume that higher
quality lexical semantic vectors—as measured by metrics such as the WordSim353 test
</bodyText>
<page confidence="0.988166">
95
</page>
<note confidence="0.560019">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.989345693877551">
of Finkelstein et al. (2001)—will produce better relational vectors from the procedure
designed subsequently. We will not test the hypothesis here, but note that it is an
underlying assumption in most of the current literature on the subject (Erk and Pad´o
2008; Mitchell and Lapata 2008; Baroni and Zamparelli 2010).
Building upon the foundation of the thus constructed noun vectors, we construct
semantic representations for relational words. In pregroup grammars (or other com-
binatorial grammars such as CCG), we can view such words as functions, taking as
arguments those types present as adjoints in the compound pregroup type, and return-
ing a syntactic object whose type is that of the corresponding reduction. For example,
an adjective nnl takes a noun or noun phrase n and returns a noun phrase n from the
reduction (nnl)n → n. It can also compose with another adjective to return an adjective
(nnl)(nnl) → nnl. We wish for our semantic representations to be viewed in the same
way, such that the composition of an adjective with a noun (1N ⊗ CN)((N ⊗ N) ⊗ N) can
be viewed as the application of a function f : N → N to its argument of type N.
To learn the representation of such functions, we assume that their meaning can
be characterized by the properties that their arguments hold in the corpus, rather than
just by their context as is the case in lexical distributional semantic models. To give
an example, rather than learning what the adjective angry means by observing that
it co-occurs with words such as fighting, aggressive, or mean, we learn its meaning by
observing that it typically takes as argument words that have the property of being
(i.e., co-occur with words such as) fighting, aggressive, and mean. Whereas in the lexical
semantic case, such associations might only rarely occur in the corpus, in this indirect
method we learn what properties the adjective relates to even if they do not co-occur
with it directly.
In turn, through composition with its argument, we expect the function for such
an adjective to strengthen the properties that characterize it in the object it takes as
argument. If, indeed, angry is characterized by arguments that have high basis weights
for basis elements corresponding to the concepts (or context words) fighting, aggressive,
and mean, and relatively low counts for semantically different concepts such as passive,
peaceful, and loves, then when we apply angry to dog the vector for the compound
angry dog should contain some of the information found in the vector for dog. But this
vector should also have higher values for the basis weights of fighting, aggressive, and
mean, and correspondingly lower values for the basis weights of passive, peaceful, and
loves.
To turn this idea into a concrete algorithm for constructing the semantic repre-
sentation for relations of any arity, as first presented in Grefenstette et al. (2011), we
examine how we would deal with this for binary relations such as transitive verbs. If
a transitive verb of semantic type N ⊗ ST ⊗ N is viewed as a function f : N × N → ST
that expresses the extent to which the properties of subject and object are brought into
relation by the verb, we learn the meaning of the verb by looking at what properties are
brought into relation by its arguments in a corpus. Recall that the vector for a verb v,
→−v ∈ N ⊗ ST ⊗ N, can be expressed as the weighted sum of its basis elements:
�−→ v = cvijk−→ni ⊗ →−sj ⊗ nk−→
ijk
We take the set of vectors for the subject and object of v in the corpus to be argv =
{(−−−−→
SUBJl, −−−→
OBJl )}l. We wish to calculate the basis weightings {cvijk}ijk for v. Exploiting our
earlier definition of the basis {sj}j of ST, which states that for any value of i and k there
</bodyText>
<page confidence="0.947002">
96
</page>
<note confidence="0.38028">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.593279">
is some value of j such that sj = (ni, nk), we define Δijk = 1 if indeed sj = (ni, nk) and 0
otherwise. Using all of this, we define the calculation of each basis weight cv ijk as:
</bodyText>
<equation confidence="0.6730544">
�cv ijk = ΔijkcSUBJl
l i cOBJl k
This allows for a full formulation of _# as follows:
��� v = E ΔijkcSUBJlickBJlni�� ® sj�� ® nk��
l ijk
</equation>
<bodyText confidence="0.999596833333333">
The nested sums here may seem computationally inefficient, seeing how this
would involve computing size(argv) x dim(N)2 x dim(S) = size(argv) x dim(N)4 prod-
ucts. However, using the decomposition of basis elements of S into pairs of basis
elements of N (effectively basis elements of N ® N), we can remove the Δijk term and
ignore all values of j where sj =� (ni, nk), because the basis weight for this combination
of indices would be 0. Hence we simplify:
</bodyText>
<equation confidence="0.5781055">
��� v = E cSUBJlickBJln ® �����
l ik (ni, nk) ® nk��
</equation>
<bodyText confidence="0.997751666666667">
This representation is still bloated: We perform fewer calculations, but still obtain a
vector in which all the basis weights where sj =� (ni, nk) are 0, hence where only dim(N)2
of the dim(N)4 values are non-zero. In short, the vector weights for _�&apos; are, under this
learning algorithm, entirely characterized by the values of a dim(N) by dim(N) matrix,
the entries of which are products cSUBJl icOBJl kwhere i and k have become row and column
indices.
Using this and our definition of ST as a space isomorphic to N ® N, we can formu-
late a compact expression of _z� as follows. Let the kronecker product of two vectors
�� u, ��w E N, written tV ® ��w E N ® N, be as follows:
</bodyText>
<equation confidence="0.974408142857143">
��� u ® �� w = cui cjw n ® n
ij
Equipped with this definition, we can formulate the compact form of _�&apos; :
��� v = � cSUBJl n ® n
l ik i cOBJl k k
�= S BJ ® oBJI
l
</equation>
<bodyText confidence="0.9997865">
In short, we are only required to iterate through the corpus once, taking for each
instance of a transitive verb v the kronecker product of its subject and object, and
summing these across all instances of v. It is simple to see that no information was
discarded relative to the previous definition of _# : The dimensionality reduction by a
factor of dim(N)2 simply discards all basis elements for which the basis weight was 0 by
default.
</bodyText>
<page confidence="0.997889">
97
</page>
<note confidence="0.560389">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999376833333333">
This raises a small problem though: This compact representation can no longer be
used in the compositional mechanism presented in Section 3.3, as the dimensions of
→−v no longer match those that it is required to have according to its syntactic type.
However, a solution can be devised if we return to the sample calculation shown in
Section 3.3.2 of the composition of a transitive verb with its arguments. The composition
reduces as follows:
</bodyText>
<equation confidence="0.5741755">
(CN ⊗ 1S ⊗ CN)(−−−→ cSUBJ
SUBJ ⊗→−v ⊗−−→ i cvikmcOBJ
OBJ) = E m −→ sk
ikm
</equation>
<bodyText confidence="0.998747">
where the verb v is represented in its non-compact form. If we introduce the compact-
ness given to us by our isomorphism ST ∼= N ⊗ N we can express this as
</bodyText>
<equation confidence="0.93935325">
−−−−−−−−→ E cSUBJ
SUBJ v OBJ = i cv imcOBJ
m ni −→ ⊗ nm −→
im
</equation>
<bodyText confidence="0.990537">
where v is represented in its compact form. Furthermore, by introducing the component
wise multiplication operation O:
</bodyText>
<equation confidence="0.90108125">
−→u O E→− v = cui cvi −→ni
i
we can show the general form of transitive verb composition with the reduced verb
representation to be as follows:
−−−−−−−−→ E cSUBJ
SUBJ v OBJ = i cv imcOBJ
m ni −→ ⊗ nm −→
im
�E�i n®mQcSUBJcOBJ
min ni−→ ⊗ nm−→
im im
= v O (SUBI ⊗ OBI)
</equation>
<bodyText confidence="0.654839">
To summarize what we have done with transitive verbs:
</bodyText>
<listItem confidence="0.990780083333334">
1. We have treated them as functions, taking two nouns and returning a
sentence in a space ST ∼= N ⊗ N.
2. We have built them by counting what properties of subject and object
noun vectors are related by the verb in transitive sentences in a corpus.
3. We have assumed that output properties were a function of input properties
by the use of the Δijk function—for example, the weights associated with
ni from the subject argument and with nk from the object argument only
affect the “output” weight for the sentence basis element sj−→ = ni−→ ⊗ −→nk.
4. We have shown that this leads to a compact representation
of the verb’s semantic form, and an optimized learning procedure.
5. We have shown that the composition operations
of our formalism can be adapted to this compact representation.
</listItem>
<page confidence="0.988882">
98
</page>
<note confidence="0.481204">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.942049">
The compact representation and amended composition operation hinge on the
choice of ST ∼= N ⊗ N as output type for N ⊗ N as an input type (the pair of arguments
the verb takes), justifying our choice of a transitive sentence space. In the intransitive
case, the same phenomenon can be observed, since such a verb takes as argument a
vector in N and produces a vector in SI ∼= N. Furthermore, our choice to make all other
types dependent on one base type—namely, n (with associated semantic space N)—
yields the same property for every relational word we wish to learn: The output type is
the same as the concatenated (on the syntactic level) or tensored (on the semantic level)
input types. It is this symmetry between input and output types that guarantees that
any m-ary relation, expressed in the original formulation as an element of tensor space
N ⊗ . . . ⊗ N , has a compact representation in N ⊗ ... ⊗ N , where the ith basis weight
\ v � ~ Y ol
2m m
of the reduced representation stands for the degree to which the ith element of the input
vector affects the ith element of the output vector.
This allows the specification of the generalized learning algorithm for reduced rep-
resentations, first presented in Grefenstette and Sadrzadeh (2011a), which is as follows.
Each relational word P with grammatical type π and m adjoint types α1, α2, · · ·, αm is en-
coded as an (r × ... × r) multi-dimensional array with m degrees of freedom (i.e., a rank
m tensor). Because our vector space N has a fixed basis, each such array is represented
in vector form as follows:
</bodyText>
<equation confidence="0.988323375">
−→ �P = cij···ζ (−→ n i ⊗ −→ n j ⊗ ··· ⊗ →−n ζ)
ij · · · ζ \ V J
m
\ V J
m
This vector lives in the tensor space N ⊗ N ⊗ · · · ⊗ N. Each cij···ζ is computed according
~ Y �
m
</equation>
<bodyText confidence="0.9334355">
to the procedure described in Figure 2.
Linear algebraically, this procedure corresponds to computing the following
</bodyText>
<equation confidence="0.9248135">
→− �P = �−→ �
k w1 ⊗ −→ w2 ⊗ ··· ⊗ −→ wm k
</equation>
<figureCaption confidence="0.959673">
Figure 2
</figureCaption>
<figure confidence="0.783445357142857">
Procedure for learning weights for matrices of words P with relational types π of m arguments.
1) Consider a sequence of words containing a relational word P and its arguments w1,
w2, · · · , wm, occurring in the same order as described in P’s grammatical type π.
Refer to these sequences as P-relations. Suppose there are k of them.
2) Retrieve the vector −→wl of each argument wl.
3) Suppose w1 has weight c1i on basis vector −→n i, w2 has weight c2j on basis vector −→n j, · · · ,
and wm has weight cmζ on basis vector −→n ζ. Multiply these weights
c1i × c2 j × · · · × cm ζ
4) Repeat the above steps for all the k P-relations, and sum the corresponding weights
�cij···ζ = � �
k c1 i × c2 j × · · · × cm ζ
k
99
Computational Linguistics Volume 41, Number 1
</figure>
<bodyText confidence="0.942145">
The general formulation of composing a relational word P with its arguments
arg1, ... , argm is now expressed as
</bodyText>
<equation confidence="0.981172">
PO (���
arg1 ® ... ® ag
</equation>
<bodyText confidence="0.997741538461538">
For example, the computation of furry cats nag angry dogs would correspond to the
following operations:
furry cats nag angry dogs = naQ O ( (furry O ac t) ® (a-n­gr� O dog) )
This generalized learning algorithm effectively extends the coverage of our ap-
proach to any sentence for which we have a pregroup parse (e.g., as might be obtained
by our translation mechanism from CCG). For example, determiners would have a
type nnl, allowing us to model them as matrices in N ® N; adverbs would be of type
(nrs)r(nrs), and hence be elements of S ® N ® N ® S. We could learn them using the
procedure defined earlier, although for words like determiners and conjunctions, it is
unclear whether we would want to learn such logical words or design them by hand,
as was done in Coecke, Sadrzadeh, and Clark (2010) for negation. Nonetheless, the pro-
cedure given here allows us to generalize the work presented in this article to sentences
of any length or structure based on the pregroup types of the words they contain.
</bodyText>
<subsectionHeader confidence="0.987759">
4.4 Example
</subsectionHeader>
<bodyText confidence="0.987338923076923">
To conclude this section with an example taken from Grefenstette and Sadrzadeh
(2011a), we demonstrate how the meaning of the word show might be learned from a
corpus and then composed.
Suppose there are two instances of the verb show in the corpus:
s1 = table show result
s2 = map show location
The vector of show is ���� ��� ���� + ���
show = table ® result map ® ������
location
Consider a vector space N with four basis vectors far, room, scientific, and elect. The
TF/IDF-weighted values for vectors of selected nouns (built from the BNC) are as
shown in Table 1.
Part of the matrix compact representation of show is presented in Table 2.
</bodyText>
<tableCaption confidence="0.942467">
Table 1
Sample weights for selected noun vectors.
</tableCaption>
<table confidence="0.98539475">
i �� table map result location master dog
ni
1 far 6.6 5.6 7 5.9 4.3 5.5
2 room 27 7.4 1.0 7.3 9.1 7.6
3 scientific 0 5.4 13 6.1 4.1 0
4 elect 0 0 4.2 0 6.2 0
100
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</table>
<tableCaption confidence="0.985567">
Table 2
</tableCaption>
<table confidence="0.977504333333333">
Sample semantic matrix for show.
far room scientific elect
far 79.24 47.41 119.96 27.72
room 232.66 80.75 396.14 113.2
scientific 32.94 31.86 32.94 0
elect 0 0 0 0
</table>
<bodyText confidence="0.838711821428572">
(5.6 x 5.9), and then adding these (46.2 + 33.04) and obtaininn&amp; the
total weight 79.24. Similarly, the weight c21 for vector (��n2, -n+,), that is, (����
room, far), is
computed by multiplying the weight of ����
room for table by the weight of far�� for result
(27 x 7), then multiplying the weight of ����
room for map by the weight of far�� for location
(7.4 x 5.9), and then adding these (189 + 43.66) to obtain the total weight of 232.66.
We now wish to compute the vector for the sentence [the] master shows [his] dog,
omitting the determiner and possessive for simplicity, as we have left open the question
as to whether or not we would want to learn them using the generalized procedure
from Section 4.3 or specify them by design due to their logical structure. The calculation
according to the vectors in Table 1 and the matrix in Table 2 will be:
�������������
master show dog
= show O (=ter ® dog)
= ⎡ 79.24 47.41 119.96 27.72 ⎤ ⎡ 23.65 32.68 0 0
⎢ ⎢ ⎣ 232.66 80.75 396.14 113.2 ⎦⎥⎥ O ⎢ ⎢ ⎣ 50.05 69.16 0 0 ⎤
32.94 31.86 32.94 0 22.55 31.16 0 0 ⎦⎥⎥
0 0 0 0 34.1 47.12 0 0
= ⎡ 1,874.03 1,549.36 0 0
⎢ ⎢ ⎣ 11,644.63 5,584.67 0 0 ⎤
742.80 992.76 0 0 ⎦⎥⎥
0 0 0 0
Row-wise, flattening the final matrix representation gives us the result we seek,
namely, the sentence vector in ST for [the] master showed [his] dog:
�������������
master show dog = [1,874, 1,549, 0, 0,11,645, 5,585, 0, 0, 743, 993, 0, 0, 0, 0, 0, 0]
</bodyText>
<sectionHeader confidence="0.998652" genericHeader="method">
5. Experiments
</sectionHeader>
<bodyText confidence="0.884511888888889">
Evaluating compositional models of semantics is no easy task: First, we are trying
to evaluate how well the compositional process works; second, we also are trying
to determine how useful the final representation—the output of the composition—is,
relative to our needs.
As a sample computation, the weight c11 for vector (ng, n1), that is, (far, far), is computed
��
by multiplying weights of table and result on far (6.6 x 7), multiplying weights of map
��
and location on far
</bodyText>
<page confidence="0.931941">
101
</page>
<note confidence="0.524225">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999980833333333">
The scope of this second problem covers most phrase and sentence-level semantic
models, from bag-of-words approaches in information retrieval to logic-based formal
semantic models, via language models used for machine translation. It is heavily task
dependent, in that a representation that is suitable for machine translation may not be
appropriate for textual inference tasks, and one that is appropriate for IR may not be
ideal for paraphrase detection. Therefore this aspect of semantic model evaluation ide-
ally should take the form of application-oriented testing. For instance, to test semantic
representations designed for machine translation purposes, we should use a machine
translation evaluation task.
The DisCoCat framework (Clark, Coecke, and Sadrzadeh 2008; Coecke, Sadrzadeh,
and Clark 2010), described in Section 3, allows for the composition of any words of
any syntactic type. The general learning algorithm presented in Section 4.3 technically
can be applied to learn and model relations of any semantic type. However, many
open questions remain, such as how to deal with logical words, determiners, and
quantification, and how to reconcile the different semantic types used for sentences
with transitive and intransitive sentences. We will leave these questions for future work,
briefly discussed in Section 6. In the meantime, we concretely are left with a way of
satisfactorily modeling only simple sentences without having to answer these bigger
questions.
With this in mind, in this section we present a series of experiments centered around
evaluating how well various models of semantic vector composition perform (along
with the one described in Section 4) in a phrase similarity comparison task. This task
aims to test the quality of a compositional process by determining how well it forms a
clear joint meaning for potentially ambiguous words. The intuition here is that tokens,
on their own, can have several meanings; and that it is through the compositional
process—through giving them context—that we understand their specific meaning. For
example, bank itself could (among other meanings) mean a river bank or a financial
bank; yet in the context of a sentence such as The bank refunded the deposit, it is likely we
are talking about the financial institution.
In this section, we present three data sets designed to evaluate how well word-sense
disambiguation occurs as a byproduct of composition. We begin by describing the first
data set, based around noun-intransitive verb phrases, in Section 5.1. In Section 5.2,
we present a data set based around short transitive-verb phrases (a transitive verb
with subject and object). In Section 5.3, we discuss a new data set, based around short
transitive-verb phrases where the subject and object are qualified by adjectives. We leave
discussion of these results for Section 6.
</bodyText>
<subsectionHeader confidence="0.9952">
5.1 First Experiment
</subsectionHeader>
<bodyText confidence="0.9926856">
This first experiment, originally presented in Mitchell and Lapata (2008), evaluates
the degree to which an ambiguous intransitive verb (e.g., draws) is disambiguated by
combination with its subject.
Data set Description. The data set4 comprises 120 pairs of intransitive sentences, each of
the form NOUN VERB. These sentence pairs are generated according to the following
</bodyText>
<footnote confidence="0.806065">
4 Available athttp://homepages.inf.ed.ac.uk/s0453356/results.
</footnote>
<page confidence="0.993579">
102
</page>
<note confidence="0.586711">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.991251">
procedure, which will be the basis for the construction of the other data sets discussed
subsequently:
</bodyText>
<listItem confidence="0.9589763125">
1. A number of ambiguous intransitive verbs (15, in this case) are selected
from frequently occurring verbs in the corpus.
2. For each verb V, two mutually exclusive synonyms V1 and V2 of the verb
are produced, and each is paired with the original verb separately (for a
total of 30 verb pairs). These are generated by taking maximally distant
pairs of synonyms of the verb on WordNet, but any method could be used
here.
3. For each pair of verb pairs (V, V1) and (V, V2), two frequently occurring
nouns N1 and N2 are picked from the corpus, one for each synonym of V.
For example, if V is glow and the synonyms V1 and V2 are beam and burn,
we might choose face as N1, because a face glowing and a face beaming
mean roughly the same thing; and fire as N2, because a fire glowing and a
fire burning mean roughly the same thing.
4. By combining the nouns with the verb pairs, we form two high similarity
triplets (V, V1, N1) and (V, V2, N2), and two low similarity triplets
(V, V1, N2) and (V, V2, N1).
</listItem>
<bodyText confidence="0.9999757">
The last two steps can be repeated to form more than four triplets per pair of verb
pairs. In Mitchell and Lapata (2008), eight triplets are generated for each pair of verb
pairs, obtaining a total of 120 triplets from the 15 original verbs. Each triplet, along with
its HIGH or LOW classification (based on the choice of noun for the verb pair) is an
entry in the data set, and can be read as a pair of sentences: (V, Vi, N) translates into the
intransitive sentences N V and N Vi.
Finally, the data set is presented, without the HIGH/LOW ratings, to human an-
notators. These annotators are asked to rate the similarity of meaning of the pairs of
sentences in each entry on a scale of 1 (low similarity) to 7 (high similarity). The final
form of the data set is a set of lines each containing:
</bodyText>
<listItem confidence="0.93470325">
• A (V, Vi, N) triplet.
• A HIGH or LOW label for that triplet.
• An annotator identifier and the annotator’s score for that triplet.
Sample sentences from this data set are shown in Table 3.
</listItem>
<tableCaption confidence="0.990512">
Table 3
</tableCaption>
<footnote confidence="0.633407">
Example entries from the intransitive data set without annotator score, first experiment.
Sentence 1 Sentence 2
butler bow butler submit
head bow head stoop
company bow company submit
government bow government stoop
</footnote>
<page confidence="0.987678">
103
</page>
<note confidence="0.591898">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.8590605">
Evaluation Methodology. This data set is used to compare various compositional distribu-
tional semantic models, according to the following procedure:
</bodyText>
<listItem confidence="0.986828285714286">
1. For each entry in the data set, the representation of the two sentences
N V and N Vi formed from the entry triple (V, Vi, N), which we
will name S1 and S2, is constructed by the model.
2. The similarity of these sentences according to the model’s semantic
distance measure constitutes the model score for the entry.
3. The rank correlation of entry model scores against entry annotator scores
is calculated using Spearman’s rank correlation coefficient p.
</listItem>
<bodyText confidence="0.998727722222222">
The Spearman p scores are values between −1 (perfect negative correlation) and 1
(perfect correlation). The higher the p score, the higher the compositional model can be
said to produce sentence representations that match human understanding of sentence
meaning, when it comes to comparing the meaning of sentences. As such, we will rank
the models evaluated using the task by decreasing order of p score.
One of the principal appealing features of Spearman’s p is that the coefficient is
rank-based: It does not require models’ semantic similarity metrics to be normalized
for a comparison to be made. One consequence is that a model providing excellent
rank correlation with human scores, but producing model scores on a small scale (e.g.,
values between 0.5 and 0.6), will obtain a higher p score than a model producing model
scores on a larger scale (e.g., between 0 and 1) but with less perfect rank correlation.
If we wished to then use the former model in a task requiring some greater degree of
numerical separation (let us say 0 for non-similar sentences and 1 for completely similar
sentences), we could simply renormalize the model scores to fit the scale. By eschewing
score normalization as an evaluation factor, we minimize the risk of erroneously ranking
one model over another.
Finally, in addition to computing the rank alignment coefficient between model
scores and annotator scores, Mitchell and Lapata (2008) calculate the mean model
scores for entries labeled HIGH, and for entries labeled LOW. This information is
reported in their paper as additional means for model comparison. However, for the
same reason we considered Spearman’s p to be a fair means of model comparison—
namely, in that it required no model score normalization procedure and thus was
less likely to introduce error by adding such a degree of freedom—we consider the
HIGH/LOW means to be inadequate grounds for comparison, precisely because it
requires normalized model scores for comparison to be meaningful. As such, we will not
include these mean scores in the presentation of this, or any further experiments in this
article.
Models Compared. In this experiment, we compare the best and worst performing models
of Mitchell and Lapata (2008) to our own. We begin by building a vector space W
for all words in the corpus, using standard distributional semantic model construction
procedures (n-word window) with the parameters of Mitchell and Lapata (2008). These
parameters are as follows: The basis elements of this vector space are the 2,000 most
frequently occurring words in the BNC, excluding common stop words. For our evalu-
ation, the corpus was lemmatized using Carroll’s Morpha (Minnen, Carroll, and Pearce
2001), which was applied as a byproduct of our parsing of the BNC with C&amp;C Tools
(Curran, Clark, and Bos 2007).
</bodyText>
<page confidence="0.993728">
104
</page>
<note confidence="0.585708">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.996051">
The context of each occurrence of a word in the corpus was defined to be five words
on either side. After the vector for each word w was produced, the basis weights cwi
associated with context word bi were normalized by the following ratio of probabilities
weighting scheme:
</bodyText>
<equation confidence="0.990279">
P(bi|w)
cw i = P(w)
</equation>
<bodyText confidence="0.999859333333333">
Let verb and n n be the lexical semantic vectors for the verb and the noun, from W.
It should be evident that there is no significant difference in using W for nouns in lieu
of building a separate vector space N strictly for noun vectors.
As a first baseline, Verb Baseline, we ignored the information provided by the noun
in constructing the sentence representation, effectively comparing the semantic content
of the verbs:
</bodyText>
<figure confidence="0.7962945">
−−−−−−→ Verb Baseline : noun verb =verb
b
</figure>
<bodyText confidence="0.971016">
The models from Mitchell and Lapata (2008) we evaluate here are those which were
strictly unsupervised (i.e., no free parameters for composition). These are the additive
model Add, wherein
</bodyText>
<equation confidence="0.907918">
−−−−−−→ noun + −−→
Add : noun verb = −−→ verb
</equation>
<bodyText confidence="0.9971025">
and the multiplicative model Multiply, wherein
Multiply: noun verb = −−→
−−−−−−→noun O −−→
verb
Other models with parameters that must be optimized against a held-out section of the
data set are presented in Mitchell and Lapata (2008). We omitted them here principally
because they do not perform as well as Multiply, but also because the need to optimize
the free parameters for this and other data sets makes fair comparison with completely
unsupervised models more difficult, and less fair.
We also evaluate the Categorical model from Section 4 here, wherein
</bodyText>
<equation confidence="0.389579333333333">
Categorical: −−−−−−→
noun verb = verbcat O no-u
−−−→
</equation>
<bodyText confidence="0.9806087">
where verbcat is the compact representation of the relation the verb stands for, computed
according to the procedure described in Section 4.3. It should be noted that for the case
of intransitive verbs, this composition operation is mathematically equivalent to that of
the Multiply model (as component-wise multiplication O is a commutative operation),
the difference being the learning procedure for the verb vector.
For all such vector-based models, the similarity of the two sentences s1 and s2 is
taken to be the cosine similarity of their vectors, defined in Section 2.2:
similarity(s1, s2) = cosine(−→s1 , −→s2 )
In addition to these vector-based methods, we define an additional baseline and
an upper-bound. The additional baseline, Bigram Baseline, is a bigram-based language
</bodyText>
<page confidence="0.9968">
105
</page>
<note confidence="0.658161">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.99988275">
model trained on the BNC with SRILM (Stolcke 2002), using the standard language
model settings for computing log-probabilities of bigrams. To determine the semantic
similarity of sentences s1 = noun verb1 and s2 = noun verb2, we assumed sentences have
mutually conditionally independent properties, and computed the joint probability:
</bodyText>
<equation confidence="0.609991">
similarity(s1, s2) = logP(s1 ∧ s2) = log(P(s1)P(s2)) = logP(s1) + logP(s2)
</equation>
<bodyText confidence="0.999923518518519">
The reasoning behind this baseline is as follows. The sentence formed by combining the
first verb with its arguments is, by the design of this data set, a semantically coherent
sentence (e.g., the head bowed and the government bowed both make sense). We therefore
expect language models to treat this sort of bigram as having a higher probability
than bigrams that are not semantically coherent sentences (and therefore unlikely to
be observed in the corpus). A similar (relative) high probability is to be expected when
the sentence formed by taking the second verb in each entry and combining it with
the verb yields a sentence similar to the first in meaning (e.g., as would be the case
with the head stooped), whereas the probability of a semantically incoherent sentence
(e.g., the government stooped) is expected to be low relative to that of the first sentence. By
taking the sum of log probabilities of sentences, we compute the log of the product
of the probabilities, which we expect to be low when the probability of the second
sentence is low, and high when it is high, with the probability of the first sentence
acting as a normalizing factor. To summarize, while this bigram-based measure only
tracks a tangential aspect of semantic similarity, it can be one which plays an artificially
important role in experiments with a predetermined structure such as the one described
in this section. For this reason, we use this bigram baseline for this experiment and all
that follow.
The upper bound of the data set, UpperBound, was taken to be the inter-annotator
agreement: the average of how each annotator’s score aligns with other annotator
scores, using Spearman’s ρ.
Results. The results5 of the first experiment are shown in Table 4. As expected from the
fact that Multiply and Categorical differ only in how the verb vector is learned, the
results of these two models are virtually identical, outperforming both baselines and
the Additive model by a significant margin. However, the distance from these models
to the upper bound is even greater, demonstrating that there is still a lot of progress to
be made.
</bodyText>
<subsectionHeader confidence="0.999748">
5.2 Second Experiment
</subsectionHeader>
<bodyText confidence="0.9231239">
The first experiment was followed by a second similar experiment, in Mitchell
and Lapata (2010), covering different sorts of composition operations for binary
combinations of syntactic types (adjective-noun, noun-noun, verb-object). Such further
experiments are interesting, but rather than continue down this binary road, we now
turn to the development of our second experiment, involving sentences with larger
syntactic structures, to examine how well various compositional models cope with
more complex syntactic and semantic relations.
5 The results were state of the art when the experiments were run in 2011. The binary composition data set
used here are popular; now there are a host of new state-of-the-art systems available in the literature. We
invite the reader to check references to Mitchell and Lapata (2008) to find the current state of the art.
</bodyText>
<page confidence="0.9944">
106
</page>
<note confidence="0.753863">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<tableCaption confidence="0.998838">
Table 4
</tableCaption>
<table confidence="0.951381625">
Model correlation coefficients with human judgments, first experiment. p &lt; 0.05 for each p.
Model p
Verb Baseline 0.08
Bigram Baseline 0.02
Add 0.04
Multiply 0.17
Categorical 0.17
UpperBound 0.40
</table>
<bodyText confidence="0.997319636363636">
This second experiment, which we initially presented in Grefenstette and
Sadrzadeh (2011a), is an extension of the first in the case of sentences centered around
transitive verbs, composed with a subject and an object. The results of the first experi-
ment did not demonstrate any difference between the multiplicative model, which takes
into account no syntactic information or word ordering, and our syntactically motivated
categorical compositional model. By running the same experiment over a new data
set, where the relations expressed by the verb have a higher arity than in the first, we
hope to demonstrate that added structure leads to better results for our syntax-sensitive
model.
Data Set Description. The construction procedure for this data set6 is almost exactly as
for the first data set, with the following differences:
</bodyText>
<listItem confidence="0.990026333333333">
• Verbs are transitive instead of intransitive.
• We arbitrarily took 10 verbs from the most frequent verbs in the BNC,
and for each verb, took two maximally distant synonyms (again, using
WordNet) to obtain 10 pairs of pairs.
• For each pair of verb pairs, we selected a set of subject and object nouns
to use as context, as opposed to just a subject noun.
• Each subject-object pair was manually chosen so that one of the verb
pairs would have high similarity in the context of that subject and
object, and the other would have low similarity. We used these choices
to annotate the entries with HIGH and LOW tags.
• Each combination of a verb pair with a subject-object pair constitutes
an entry of our data set, of which there are 200.
</listItem>
<bodyText confidence="0.9972465">
As a form of quality control, we inserted “gold standard” sentences in the form of
identical sentence pairs and rejected annotators who did not score these gold standard
</bodyText>
<footnote confidence="0.954649">
6 The data set, reannotated by Turkers in 2013, is available at
http://www.cs.ox.ac.uk/activities/compdistmeaning/GS2013data.txt.
</footnote>
<page confidence="0.990959">
107
</page>
<note confidence="0.544329">
Computational Linguistics Volume 41, Number 1
</note>
<tableCaption confidence="0.995664">
Table 5
</tableCaption>
<table confidence="0.706542333333333">
Example entries from the transitive data set without annotator score, second experiment.
Sentence 1 Sentence 2 HIGH-LOW Tag
man draw sword man attract sword LOW
report draw attention report attract attention HIGH
man draw sword man depict sword HIGH
report draw attention report depict attention LOW
</table>
<bodyText confidence="0.999209473684211">
sentences with a high score of 6 or 7.7 Lemmatized sentences from sample entries of this
data set and our HIGH-LOW tags for them are shown in Table 5.
The data set was passed to a group of 50 annotators on Amazon Mechanical Turk, as
for the previous data set. The annotators were shown, for each entry, a pair of sentences
created by adding the in front of the subject and object nouns and putting the verbs in
the past tense,8 and were instructed to score each pair of sentences on the same scale of
1 (not similar in meaning) to 7 (similar in meaning), based on how similar in meaning
they believed the sentence pair was.
Evaluation Methodology. The methodology for this experiment is exactly that of the
previous experiment. Models compositionally construct sentence representations, and
compare them using a distance metric (all vector-based models once again used cosine
similarity). The rank correlation of models scores with annotator scores is calculated
using Spearman’s ρ, which is in turn used to rank models.
Models Compared. The models compared in this experiment are those of the first experi-
ment, with the addition of an extra trigram-based baseline (trained with SRILM, using
the addition of log-probability of the sentence as a similarity metric), and a variation on
our categorical model, presented subsequently. With W as the distributional semantic
space for all words in the corpus, trained using the same parameters as in the first exper-
iment, and
</bodyText>
<equation confidence="0.621631333333333">
object ∈ W as the vectors for subject, verb, and object of a sentence,
−−→
subj, vebr,
</equation>
<bodyText confidence="0.990882666666667">
respectively, and with verbcat as the compact representation of a transitive verb learned
using the algorithm presented in this paper, we have the following compositional
methods:
</bodyText>
<table confidence="0.933185714285714">
−−−−−−−−−−−−→ −−−−→ −−→ −−−→
Add : subject verb object = subject + verb + object
−−−−−−−−−−−−→ −−−−→ −−→ −−−→
Multiply: subject verb object = subject O verb O object
−−−−−−−−−−−−→ −−−→ O (sub ectl ® object)
Categorical: subject verb object = verbcat
Kronecker : subject verb object = (verb ® verb) O (subject) ectl ® ob)
</table>
<footnote confidence="0.9980388">
7 During the 2013 reannotation of this data set, we rejected no Turker contributions, as the answers to
the gold standard sentence pairs were aligned with our expectations. We attribute this to our adding
the requirement that Turkers be based in the US or the UK and have English as the first language.
8 For example, the entry draw table eye depict would yield the sentences The table drew the eye and The table
depicted the eye.
</footnote>
<page confidence="0.993958">
108
</page>
<note confidence="0.587626">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.9972394">
All of these models have been explained earlier, with the exception of Kronecker.
We first presented this new addition in Grefenstette and Sadrzadeh (2011b), where
we observed that the compact representation of a verb in the DisCoCat framework,
under the assumptions presented in Section 4, can be viewed as dim(N) × dim(N)
matrices in N ⊗ N. We considered alternatives to the algorithm presented earlier for
the construction of such matrices, and were surprised by the results of the Kronecker
method, wherein we replaced the matrix learned by our algorithm with the Kronecker
product of the lexical semantic vectors for the verb. Further analysis performed since the
publication of that paper can help to understand why this method might work. Using
the following property, for any vectors −→a , −→b , −→c , −→d in a vector space
</bodyText>
<equation confidence="0.8887345">
(−→a ⊗ −→b ) O (−→c ⊗ −→ d ) = (−→a O −→ c ) ⊗ (−→ b G) −→ d )
we can see that the Kronecker model’s composition operation can be expressed as
Kronecker : subject verb object = (verb O sub ect) ⊗ (−−→
verb O ob)
Bearing in mind that the cosine measure we are using as a similarity metric is equivalent
to the inner product of two vectors normalized by the product of their length
b �
cosine(−→ a , −→ b ) = �−→ a |−→
</equation>
<bodyText confidence="0.899476">
and the following property of the inner product of kronecker products
(−→a ⊗ −→ b |−→ c ⊗ →− d � = �−→ a |−→ c � × �−→ b |−→ d �
we finally observe that comparing two sentences under Kronecker corresponds to the
following computation:
</bodyText>
<equation confidence="0.995772333333333">
cosine(−−−−−−−−−−−−→
subject verb1 object, −−−−−−−−−−−−→
subject verb2 object)
= α C (verb1 ⊗ verb1) O (subject ⊗ object)  |(verb2 ⊗ verb2) O (−−−−→
subject ⊗ object) &gt;
= α C (−−→
verb1 O subject) ⊗ (verb1 O object)  |(−−→
verb2 O subject) ⊗ (−−→
verb2 O object) &gt;
</equation>
<bodyText confidence="0.7936225">
= α C (verb1 O subject)  |(verb2 O subject) &gt; C (verb1 O object)  |(verb2 O object) &gt;
where α is the normalization factor
</bodyText>
<equation confidence="0.84696">
α =−−−−−−−−−−−−→1
IIsubject verb1 objectII × II−−−−−−−−−−−−→
subject verb2 objectII
</equation>
<bodyText confidence="0.9833015">
We note here that the Kronecker is effectively a parallel application of the Multiply
model, combining the subject and verb, and object and verb separately. Within the
context of this task, the comparison of two sentences boils down to how well each verb
combines with the subject multiplied by how well it combines with the object, with the
</bodyText>
<equation confidence="0.771697">
11−→a 11 × 11−→b 11
</equation>
<page confidence="0.979706">
109
</page>
<note confidence="0.647356">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999966428571429">
joint product forming the overall model score. In short, it more or less constitutes the
introduction of some mild syntactic sensitivity into the multiplicative model of Mitchell
and Lapata (2008), although it remains to be seen how well this scales with syntactic
complexity (e.g., extended to ditransitive verbs, or other relations of higher arity).
The UpperBound here is, again, the inter-annotator agreement, calculated by com-
puting the pairwise alignment of each annotator with every other, and averaging the
resulting rank correlation coefficients.
Results. The results of the second experiment are shown in Table 6. The baseline scores
fall in the 0.14–0.16 range, with best results being obtained for the Bigram Baseline and
Trigram Baseline, the difference between both models not being statistically significant.
The additive model Add performs on par with the first experiment. The multiplicative
model Multiply and our Categorical model perform on par with the version used for
the intransitive experiment, but obtain a score comparable to the baselines. The best
performing model here is our newly introduced Kronecker model, which leads the pack
by a steady margin, with a score of 0.26. The inter-annotator agreement UpperBound is
much higher in this experiment than in the previous experiment, indicating even more
room for improvement.
We therefore learn here that the Categorical model continues to operate on par with
the multiplicative model, and that the Kronecker model provides the highest results,
while being as simple in its construction as the multiplicative model, requiring only the
learning of lexical vectors for the verb.
</bodyText>
<subsectionHeader confidence="0.992842">
5.3 Third Experiment
</subsectionHeader>
<bodyText confidence="0.999968111111111">
The third and final experiment we present is a modified version of the second data set
presented earlier, where the nouns in each entry are under the scope of adjectives ap-
plied to them.The intuition behind the data sets presented in Section 5.1 and Section 5.2
was that ambiguous verbs are disambiguated through composition with nouns. These
nouns themselves may also be ambiguous, and a good compositional model will be
capable of separating the noise produced by other meanings through its compositional
mechanism to produce unambiguous phrase representations. The intuition behind this
data set is similar, in that adjectives provide both additional information for disam-
biguation of the nouns they apply to, but also additional semantic noise. Therefore, a
</bodyText>
<tableCaption confidence="0.725547">
Table 6
Model correlation coefficients with human judgments, second experiment. p &lt; 0.05 for each p.
</tableCaption>
<table confidence="0.904581090909091">
Model p
Verb Baseline 0.13
Bigram Baseline 0.16
Trigram Baseline 0.15
Add 0.10
Multiply 0.16
Categorical 0.16
Kronecker 0.26
UpperBound 0.62
110
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</table>
<tableCaption confidence="0.994787">
Table 7
</tableCaption>
<bodyText confidence="0.995844">
Example entries from the adjective-transitive data set without annotator score, third experiment.
Sentence 1 Sentence 2
statistical table show good result statistical table express good result
statistical table show good result statistical table depict good result
good model will also be able to separate the useful information of the adjective from its
semantic noise when composing it with its argument, in addition to doing this when
composing the noun phrases with the verb.
Data Set Description. The construction procedure for this data set was to take the data set
from Section 5.2, and, for each entry, add a pair of adjectives from those most frequently
occurring in the corpus. The first adjective from the pair is applied to the first noun
(subject) of the entry when forming the sentences, and the second adjective is applied
to the second noun (object). For each entry, we chose adjectives which best preserved
the meaning of the phrase constructed by combining the first verb with its subject and
object.
This new data set9 was then annotated again by a group of 50 annotators using
Amazon’s Mechanical Turk service. The annotators were shown, for each entry, a pair
of sentences created by adding the in front of the subject and object noun phrases and
putting the verbs in the past tense,10 and asked to give each sentence pair a meaning
similarity score between 1 and 7, as for the previous data sets. We applied the same
quality control mechanism as in the second experiment. Some 94 users returned anno-
tations, of which we kept 50 according to our gold standard tests. We are unaware of
whether or not it was applied in the production of the first data set, but believe that this
can only lead to the production of higher quality annotations.
Sample sentences from this data set are shown in Table 7.
Evaluation Methodology. The evaluation methodology in this experiment is identical to
that of the previous experiments.
Models Compared. In this experiment, in lieu of simply comparing compositional models
“across the board” (e.g., using the multiplicative model for both adjective-noun compo-
sition and verb-argument composition), we experimented with different combinations of
models. This evaluation procedure was chosen because we believe that adjective-noun
composition need not necessarily be the same kind of compositional process as subject-
verb-object composition, and also because different models may latch onto different
semantic features during the compositional process, and it would be interesting to
see what model mixtures work well together. Naturally, this is not a viable approach
to selecting composition operations in general, as we will not have the luxury of try-
ing every combination of composition operations for every combination of syntactic
types, but it is worthwhile performing these tests to at least verify the hypothesis that
</bodyText>
<footnote confidence="0.896025666666667">
9 Available at http://www.cs.ox.ac.uk/activities/compdistmeaning/GS2012data.txt.
10 For example, the entry statistical table show express good result would yield the sentences The statistical table
showed the good result and The statistical table expressed the good result.
</footnote>
<page confidence="0.991112">
111
</page>
<note confidence="0.629812">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.992267928571429">
operation-specific composition operations (or parameters) is a good thing. Notably, this
idea has been explored very recently, within the context of deep learning networks, by
Chen et al. (2013).
Each mixed model has two components: a verb-argument composition model and
a adjective-noun composition model. For verb-argument composition, we used the
three best models from the previous experiment, namely, Multiply, Categorical, and
Kronecker. For adjective composition we used three different methods of adjective-
noun composition. With −−−−−→
adjective and −−→
noun being the vectors for an adjective and a noun
in our distributional lexical semantic space W (built using the same procedure as the
previous experiments) and −−→
adjcat being the compact representation in the Categorical
model, built according to the algorithm from Section 4.3, we have the following models:
</bodyText>
<figure confidence="0.837861071428571">
−−−−−−−−−→
AdjMult : adjective noun = adjective O none
−−−−−−−−−→
Categorical : adjective noun = adjectivecat O −−→
−−−−−−→
noun
The third model, AdjNoun, is a holistic (non-compositional) model, wherein the
adjective-noun compound was treated as a single token, as its semantic vector
−−−−−−−−−−−−→
(adjective noun)lex E W was learned from the corpus using the same learning procedure
applied to construct other vectors in W. Hence, the model defines adjective-noun
“composition” as:
−−−−−−−−−→ −−−−−−−−−−−−→
AdjNoun : adjective noun = (adjective noun)lex
</figure>
<bodyText confidence="0.999294875">
In addition to these models, we also evaluated three baselines: Verb Baseline,
Bigram Baseline, and Trigram Baseline. As in previous experiments, the verb baseline
uses the verb vector as sentence vector, ignoring the information provided by other
words. The bigram and trigram baselines are calculated from the same language model
as used in the second experiment. In both cases, the log-probability of each sentence is
calculated using SRLIM, and the sum of log-probabilities of two sentences is used as a
similarity measure.
Finally, for comparison, we also considered the full additive model:
</bodyText>
<equation confidence="0.936894">
−−−−−→ −−−−−→Additive : sentence = adjective1 + −−−→
noun1 + verb + −−→ adjective2 + −−−→
noun2
</equation>
<bodyText confidence="0.999942">
Results. The results for the third experiment are shown in Table 8. The best performing
adjective-noun combination operations for each verb-argument combination operation
are shown in bold. Going through the combined models, we notice that in most cases
the results stay the same whether the adjective-noun combination method is AdjMult or
CategoricalAdj. This is because, as was shown in the first experiment, composition of a
unary-relation such as an adjective or intransitive verb with its sole argument, under
the categorical model with reduced representations, is mathematically equivalent to
the multiplicative model. The sole difference is the way the adjective or intransitive
verb vector is constructed. We note, however, that with Categorical as a verb-argument
composition method, the CategoricalAdj outperforms AdjMult by a non-negligible
margin (0.19 vs. 0.14), indicating that the difference in learning procedure can lead to
different results depending on what other models it is combined with.
</bodyText>
<page confidence="0.993504">
112
</page>
<note confidence="0.743915">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<tableCaption confidence="0.996421">
Table 8
</tableCaption>
<table confidence="0.978589105263158">
Model correlation coefficients with human judgments, third experiment. p &lt; 0.05 for each p.
Model p
Verb Baseline 0.20
Bigram Baseline 0.14
Trigram Baseline 0.16
Additive 0.10
Multiplicative
AdjMult 0.20
AdjNoun 0.05
CategoricalAdj 0.20
Categorical
AdjMult 0.14
AdjNoun 0.16
CategoricalAdj 0.19
Kronecker
AdjMult 0.26
AdjNoun 0.17
CategoricalAdj 0.27
Upperbound 0.48
</table>
<bodyText confidence="0.99529">
Overall, the best results are obtained for AdjMult+Kronecker (p = 0.26) and
CategoricalAdj+Kronecker (p = 0.27). Combinations of the adjective composition
methods with other composition methods at best matches the best-performing baseline,
Verb Baseline. In all cases, the holistic model AdjNoun provides the worst results.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="conclusions">
6. Discussion
</sectionHeader>
<bodyText confidence="0.993569125">
In this article, we presented various approaches to compositionality in distributional
semantics. We discussed what mathematical operations could underlie vector combina-
tion, and several different ways of including syntactic information into the combinatory
process. We reviewed an existing compositional framework that leverages the ability to
communicate information across mathematical structures provided by category theory
in order to define a general way of linking syntactic structure to syntax-sensitive com-
position operations. We presented concrete ways to apply this framework to linguistic
tasks and evaluate it, and developed algorithms to construct semantic representations
for words and relations within this framework. In this section, we first briefly comment
upon the combined results of all three experiments, and then conclude by discussing
what aspects of compositionality require further attention, and how experiment design
should adapt towards this goal.
Results Commentary. We evaluated this framework against other unsupervised compo-
sitional distributional models, using non-compositional models and n-gram language
models as baselines, within the context of three experiments. These experiments show
that the concrete categorical model developed here, and the Kronecker-based variant
</bodyText>
<page confidence="0.995627">
113
</page>
<note confidence="0.630339">
Computational Linguistics Volume 41, Number 1
</note>
<bodyText confidence="0.999973244897959">
presented alongside it, outperform all other models in each experiment save the first,
where they perform on par with what was the leading model at the time this experiment
was performed (namely, 2011). As the experiments involved progressively more syntac-
tically complicated sentences, the increased reliability of our categorical approaches rel-
ative to competing models as sentence complexity rises seems to indicate that both the
categorical and Kronecker model successfully leverage the added information provided
by additional terms and syntactic structures.
The third experiment also served to show that using different combinations of
composition operations, depending on the syntactic type of the terms being combined,
can yield better results, and that some models combine better than others. Notably,
the adjective-noun combination models AdjMult and CategoricalAdj, despite their
mathematical similarity, produce noticeably different results when combined with the
categorical verb-argument composition operation, while they perform equally with
most other verb-argument composition operations. We can conclude that different mod-
els combine different semantic aspects more prominently than others, and that through
combination we can find better performance by assuming that different kinds of compo-
sition play on different semantic properties. For example, predicates such as intersective
adjectives add information to their argument (a red ball is a ball that is also red). This
raises the question of how to design models of composition that systematically select
which operations will match the semantic aspects of the words being combined based
on their syntactic type. This is an open question, which we believe warrants further
investigation.
Overall, we observe two advantages of these models over those presented in the
early part of this article, and those evaluated in the later part. First, we have shown
that a linguistically motivated and mathematically sound framework can be imple-
mented and learned. Second, we showed that simple methods such as the Kronecker
model perform well, especially when combined with the multiplicative model (effec-
tively a unary instance of the Kronecker model) for sentences with adjectives. Con-
sidering the “simple is best” position recently argued for experimentally by Blacoe
and Lapata (2012), this is an interesting candidate for dealing with binary relation
composition.
Future Work. These experiments showcased the ability of various compositional models
to produce sentence representations that were less ambiguous than the words that
formed them. They also more generally demonstrated that concrete models could
be built from the general categorical framework and perform adequately in simple
paraphrase detection tasks. However, various aspects of compositionality were not
evaluated here. First, syntax sensitivity is not as important in these experiments as
it might be in “real” language. For instance, whereas models that treat adjectives,
nouns, and verbs differently tended to perform better than those that did not, the
actual capacity of a model to use the syntactic structure was not tested. For instance,
models that ignore word order and syntax altogether were not particularly penalized,
as might be done if some sentences were simply the reverse of another sentence they
are paired with (where syntax insensitive models would erroneously give such sentence
pairs a high similarity score). One way of testing word order while expanding the data
set was suggested by Turney (2012), who for every entry in a phrase similarity test
such as Mitchell and Lapata’s, discussed herein, creates a new entry where one of the
sentences has the order of its words reversed, and is assigned an artificial annotator
score of 1 (the minimum). This is an interesting approach, but we would prefer to
see such reversals designed into the data set and seen by annotators, for instance,
</bodyText>
<page confidence="0.986393">
114
</page>
<note confidence="0.540211">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<bodyText confidence="0.99997848">
in the recent data set presented in Pham et al. (2013). This data set has entries such
as guitar played man and man played guitar, where the subjects and objects of verbs
are indeed reversed but the two sentences do not necessarily express opposite mean-
ings; we will be looking into expanding such data sets to ones where both sentences
make sense and have opposite meanings, such as in the pair man bites dog and dog
bites man.
Furthermore, sentences all have the exact same sentence structure, and therefore
words are aligned. Models that might indirectly benefit from this alignment, such as
Kronecker, may have been given an advantage due to this, as opposed to models such
as Categorical, which are designed to compare sentences of different length or syntactic
structure, when resolving the difference between intransitive and transitive sentence
spaces as has been done in Grefenstette et al. (2011). Future experiments should aim to
do away with this automated alignment, and include sentence comparisons that would
penalize models which do not leverage syntactic information.
Furthermore, each of these experiments dealt with the comparison of a certain
type of sentence (transitive, intransitive). This was convenient for our concrete cate-
gorical model, as we defined the sentence space differently based on the valency of
the head verb. However, sentences with different sorts of verbs should be able to be
directly compared. Not only do several models, both non-syntax sensitive (additive,
multiplicative) and syntax-sensitive (Baroni and Zamparelli 2010; Socher et al. 2012),
not face this problem, as the product of composition is either naturally in the same
space as the vectors being composed or is projected back into it, but the categorical
framework the concrete categorical models were derived from does not commit us to
different sentence spaces either. The topic of how to solve this problem for the concrete
models developed here is beyond the scope of this article, but various options exist,
such as the projection of ST into SI, the embedding of SI into ST, the use of a combined
sentence space S = SI ⊕ ST, and so on. It is clear that future experiments should not
give this degree of convenience to the models being evaluated (and their designers),
by comparing sentences of different types, lengths, and structure so as to more fairly
evaluate the capacity of models to produce meaningful semantic representations in
a single space, which can be directly compared regardless of syntactic structure and
verb-type.
Finally, we mentioned at the beginning of Section 5 that evaluations need to be
application-oriented. The class of experiments presented in this article specifically see
how well disambiguation occurs as a byproduct of composition. We presented concrete
models that would offer ways of combining relations underlying verbs and adjectives,
and tested how well these relations disambiguated their arguments and were in turn
disambiguated by their arguments. We did not address other aspects of language, such
as quantification, logical operations, relative clauses, intensional clauses, embedded
sentences, and many other linguistic aspects of spoken and written language that
have complex syntactic structure and complicated semantic roles. Addressing these
sorts of problems requires determining how negation, conjunction, and other logical
relations should be modeled within compositional distributional formalisms, a problem
we share with other similar approaches. The development of newer models within the
categorical framework we based our work on, and the further development of other
approaches mentioned here, must be driven by new experimental goals different from
those offered by the task and experiments discussed in this article. Thinking not only
about how to address these aspects of language within compositional models, but how
to evaluate them, should be a priority for all those interested in further developing
this field.
</bodyText>
<page confidence="0.996279">
115
</page>
<note confidence="0.7179">
Computational Linguistics Volume 41, Number 1
</note>
<sectionHeader confidence="0.994679" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995015416666667">
We thank the EPSRC for funding the research
leading to this article via EPSRC grant
EP/J002607/1. Furthermore, we would like
to thank John Harding and the equational
theorem prover “Prover 9”11 for checking the
identities occurred as a result of translating
CCG’s rules to the language of a pregroup
grammar. We would also like to thank
Stephen Clark, Stephen Pulman, Bob Coecke,
Karl Moritz Hermann, Richard Socher, and
Dimitri Kartsaklis for valuable discussions
and comments.
</bodyText>
<sectionHeader confidence="0.997932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999400265306122">
Abramsky, S. and B. Coecke. 2004.
A categorical semantics of quantum
protocols. In Proceedings of the 19th Annual
IEEE Symposium on Logic in Computer
Science, pages 415–425, Turku, Finland.
Alshawi, H., editor. 1992. The Core Language
Engine. MIT Press.
Baroni, M. and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices:
Representing adjective-noun constructions
in semantic space. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,183–1,193,
Boston, MA.
B´echet, D., A. Foret, and I. Tellier. 2007.
Learnability of Pregroup Grammars.
Studia Logica, 87(2–3), pages 225–252.
Blacoe, W. and M. Lapata. 2012.
A comparison of vector-based
representations for semantic composition.
Proceedings of the 2012 Conference on
Empirical Methods in Natural Language
Processing, Jeju Island.
Buszkowski, W. 2001. Lambek grammars
based on pregroups. In P. de Groote,
G. Morrill, and C. Retor´e, editors, Logical
Aspects of Computational Linguistics.
Lecture Notes in Computer Science,
volume 2099. Springer, Berlin Heidelberg,
pages 95–109.
Chen, Danqi, Richard Socher, Christopher D.
Manning, and Andrew Y. Ng. 2013.
Learning new facts from knowledge
bases with neural tensor networks and
semantic word vectors. arXiv preprint
arXiv:1301.3618.
Clark, S. 2013. Type-driven syntax and
semantics for composing meaning vectors.
In Quantum Physics and Linguistics:
A Compositional, Diagrammatic Discourse.
Oxford University Press.
11 http://www.cs.unm.edu/∼mccune/mace4/.
Clark, S., B. Coecke, and M. Sadrzadeh.
2008. A compositional distributional
model of meaning. In Proceedings of the
Second Quantum Interaction Symposium
(QI-2008), Oxford.
Clark, S. and J. R. Curran. 2007.
Wide-coverage efficient statistical
parsing with CCG and log-linear models.
Computational Linguistics, 33:493–552.
Clark, S. and S. Pulman. 2007. Combining
symbolic and distributional models of
meaning. In AAAI Spring Symposium on
Quantum Interaction, Stanford, USA.
Clarke, Daoud. 2009. Context-theoretic
semantics for natural language: An
overview. In Proceedings of the Workshop
on Geometrical Models of Natural Language
Semantics, pages 112–119, Edinburgh.
Clarke, Daoud. 2012. A context-theoretic
framework for compositionality in
distributional semantics. Computational
Linguistics, 38(1):41–71.
Coecke, B. and ´E. O. Paquette. 2011.
Categories for the practising physicist.
In B. Coecke, editor, New Structures
for Physics. Lecture Notes in Physics,
volume 813, pages 173–286, Springer,
Berlin Heidelberg.
Coecke, B., M. Sadrzadeh, and S. Clark.
2010. Mathematical Foundations for a
Compositional Distributional Model of
Meaning. Linguistic Analysis, 36:345–384.
Curran, James, Stephen Clark, and
Johan Bos. 2007. Linguistically motivated
large-scale NLP with C&amp;C and boxer.
In Proceedings of the 45th Annual Meeting
of the Association for Computational
Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions,
pages 33–36, Prague.
Curran, J. R. 2004. From distributional to
semantic similarity. Ph.D. thesis, School of
Informatics, University of Edinburgh.
Dean, Jeffrey and Sanjay Ghemawat. 2008.
MapReduce: Simplified data processing
on large clusters. Communications of the
ACM, 51(1):107–113.
Erk, Katrin and Sebastian Pad´o. 2008.
A structured vector space model for
word meaning in context. Proceedings
of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ’08,
pages 897–906, Edinburgh.
Finkelstein, L., E. Gabrilovich, Y. Matias,
E. Rivlin, Z. Solan, G. Wolfman, and
E. Ruppin. 2001. Placing search in
</reference>
<page confidence="0.986735">
116
</page>
<note confidence="0.428402">
Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning
</note>
<reference confidence="0.998826779661016">
context: The concept revisited.
In Proceedings of the 10th International
Conference on World Wide Web,
pages 406–414, Hong Kong.
Firth, J. R. 1957. A synopsis of linguistic
theory 1930-1955. Studies in linguistic
analysis.
Fowler, T. A. D. and G. Penn. 2010. Accurate
context-free parsing with combinatory
categorial grammar. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics, pages 335–344,
Uppsala.
Frege, G. 1892. Uber sinn und bedeutung.
Zeitschrift f¨ur Philosophie und philosophische
Kritik, 100(1):25–50.
Genkin, Daniel, Nissim Francez, and Michael
Kaminski. 2010. Mildly context-sensitive
languages via buffer augmented pregroup
grammars. In Z. Manna and D. A. Peled,
editors, Time for Verification, pages 144–166,
Springer-Verlag, Berlin, Heidelberg.
Girard, Jean-Yves. 1987. Linear logic.
Theoretical Computer Science, 50:1–102.
Grefenstette, E. 2009. Analysing document
similarity measures. Master’s thesis,
University of Oxford.
Grefenstette, E., G. Dinu, Y. Zhang,
M. Sadrzadeh, and M. Baroni. 2013. Multi-
step regression learning for compositional
distributional semantics. In Proceedings
of the Tenth International Conference on
Computational Semantics, Potsdam.
Grefenstette, E. and M. Sadrzadeh. 2011a.
Experimental support for a categorical
compositional distributional model
of meaning. In Proceedings of the
2011 Conference on Empirical Methods
in Natural Language Processing,
pages 1,394–1,404, Edinburgh.
Grefenstette, E. and M. Sadrzadeh. 2011b.
Experimenting with transitive verbs in
a DisCoCat. In Proceedings of the 2011
EMNLP Workshop on Geometric Models of
Natural Language Semantics, pages 62–66,
Edinburgh.
Grefenstette, E., M. Sadrzadeh, S. Clark,
B. Coecke, and S. Pulman. 2011. Concrete
sentence spaces for compositional
distributional models of meaning.
In Proceedings of the Ninth International
Conference on Computational Semantics,
pages 125–134, Oxford.
Grefenstette, G. 1992. Use of syntactic context
to produce term association lists for text
retrieval. In Proceedings of the 15th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 89–97, Copenhagen.
Grefenstette, G. 1994. Explorations in
automatic thesaurus discovery.
Guevara, E. 2010. A regression model of
adjective-noun compositionality in
distributional semantics. In Proceedings
of the 2010 Workshop on Geometrical
Models of Natural Language Semantics,
pages 33–37, Uppsala.
Harris, Z. S. 1968. Mathematical structures of
language. Wiley.
Hockenmaier, Julia. 2003. Data and Models
for Statistical Parsing with Combinatory
Categorial Grammar. Ph.D. thesis,
School of Informatics, University
of Edinburgh.
Hockenmaier, Julia and Mark Steedman.
2007. CCGBank: A corpus of CCG
derivations and dependency structures
extracted from the Penn treebank.
Computational Linguistics, 33(3):355–396.
Joshi, A. K., K. Vijay-Shanker, and
D. J. Weir. 1989. The convergence of
mildly context-sensitive grammar
formalisms. Working paper, University
of Pennsylvania, School of Engineering
and Applied Science, Dept. of Computer
and Information Science.
Lambek, J. 1958. The mathematics of
sentence structure. The American
Mathematical Monthly, 65(3):154–170.
Lambek, J. 1999. Type grammar revisited.
Logical Aspects of Computational Linguistics,
pages 1–27.
Lambek, J. 2008. From word to sentence.
A computational algebraic approach to
grammar. Milan, Polimetrica.
Lambek, J., 2010. Compact Monoidal
Categories from Linguistics to Physics,
pages 451–469.
Landauer, T. K. and S. T. Dumais. 1997.
A solution to Plato’s problem: The
latent semantic analysis theory
of acquisition, induction, and
representation of knowledge.
Psychological review.
Mac Lane, S. 1998. Categories for the
Working Mathematician. Springer Verlag.
Manning, C. D., P. Raghavan, and H.
Sch¨utze. 2011. Introduction to information
retrieval. Cambridge University Press,
New York, NY.
Minnen, G., J. Carroll, and D. Pearce. 2001.
Applied morphological processing of
English. Natural Language Engineering,
7(03):207–223.
Mitchell, J. and M. Lapata. 2008. Vector-
based models of semantic composition.
In Proceedings of ACL, volume 8,
pages 236–244, Columbus, OH.
</reference>
<page confidence="0.955131">
117
</page>
<reference confidence="0.993608257731959">
Computational Linguistics Volume 41, Number 1
Mitchell, J. and M. Lapata. 2010.
Composition in Distributional Models
of Semantics. Cognitive Science
34(8):1388–1429.
Montague, R. 1974. English as a Formal
Language. In R. H. Thomason, editor,
Formal Semantics: The Essential Readings.
Moortgat, M. 1997. Categorial type logics.
In H. van Ditmarsch and L. S. Moss,
editors, Handbook of Logic and Language.
Elsevier, pages 93–177.
Pad´o, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161–199.
Pham, N., R. Bernardi, Y.-Z. Zhang, and
M. Baroni. 2013. Sentence paraphrase
detection: When determiners and word
order make the difference. In Proceedings
of the Towards a Formal Distributional
Semantics Workshop at IWCS 2013,
pages 21–29, Potsdam.
Plate, T. A. 1991. Holographic reduced
representations: Convolution algebra for
compositional distributed representations.
In Proceedings of the 12th International
Joint Conference on Artificial Intelligence,
pages 30–35, Hyderabad.
Preller, A. 2010. Polynomial pregroup
grammars parse context sensitive
languages. Linguistic Analysis, 36:483–516.
Preller, A. and M. Sadrzadeh. 2010. Bell
states and negative sentences in the
distributed model of meaning. In
P. Selinger, B. Coecke, P. Panangaden,
editors, Electronic Notes in Theoretical
Computer Science, Proceedings of the
6th QPL Workshop on Quantum Physics
and Logic. University of Oxford.
Selinger, P. 2010. A survey of graphical
languages for monoidal categories. New
Structures for Physics, pages 275–337.
Smolensky, P. 1990. Tensor product variable
binding and the representation of symbolic
structures in connectionist systems.
Artificial Intelligence, 46(1-2):159–216.
Smolensky, P. and G. Legendre. 2006. The
Harmonic Mind: From Neural Computation
to Optimality-Theoretic Grammar Volume I:
Cognitive Architecture. MIT Press.
Socher, R., B. Huval, C. D. Manning, and
A. Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces.
In Proceedings of the 2012 Conference
on Empirical Methods in Natural Language
Processing, pages 1,201–1,211, Jeju
Island.
Steedman, M. 2001. The Syntactic Process.
The MIT Press.
Steedman, M. and J. Baldridge. 2011.
Combinatory Categorial Grammar.
Wiley-Blackwell.
Stolcke, A. 2002. SRILM—An extensible
language modeling toolkit. In Seventh
International Conference on Spoken
Language Processing.
Turney, P. D. and P. Pantel. 2010. From
frequency to meaning: Vector space
models of semantics. Journal of Artificial
Intelligence Research, 37(1):141–188.
Turney, Peter D. 2012. Domain and
function: A dual-space model of
semantic relations and compositions.
Journal of Artificial Intelligence Research,
44:533–585.
Van Rijsbergen, C. J. 2004. The Geometry
of Information Retrieval. Cambridge
University Press.
Walters, R. F. 1991. Categories and Computer
Science. Cambridge University Press.
Widdows, D. 2005. Geometry and Meaning.
University of Chicago Press.
Widdows, D. 2008. Semantic vector
products: Some initial investigations.
In Proceedings of the Second Quantum
Interaction Symposium (QI-2008). College
Publications. CITESEER, Oxford.
Wittgenstein, L. 1953. Philosophical
Investigations. Blackwell.
Zanzotto, F. M., I. Korkontzelos, F. Fallucchi,
and S. Manandhar. 2010. Estimating
linear models for compositional
distributional semantics. In Proceedings
of the 23rd International Conference
on Computational Linguistics,
pages 1,263–1,271, Beijing.
</reference>
<page confidence="0.996088">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.277089">
<title confidence="0.820629">Concrete Models and Empirical Evaluations for the Categorical Compositional Distributional Model of Meaning Google DeepMind Queen Mary University of London Modeling compositional meaning for sentences using empirical distributional methods has been</title>
<abstract confidence="0.9776683">a challenge for computational linguists. The categorical model of Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) provides a solution by unifying a categorial grammar and a distributional model of meaning. It takes into account syntactic relations during semantic vector composition operations. But the setting is abstract: It has not been evaluated on empirical data and applied to any language tasks. We generate concrete models for this setting by developing algorithms to construct tensors and linear maps and instantiate the abstract parameters using empirical data. We then evaluate our concrete models against several experiments, both existing and new, based on measuring how well models align with human judgments in a paraphrase detection task. Our results show the implementation of this general abstract framework to perform on par with or outperform other leading models in these</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abramsky</author>
<author>B Coecke</author>
</authors>
<title>A categorical semantics of quantum protocols.</title>
<date>2004</date>
<booktitle>In Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science,</booktitle>
<pages>415--425</pages>
<location>Turku, Finland.</location>
<contexts>
<context position="49617" citStr="Abramsky and Coecke 2004" startWordPosition="7911" endWordPosition="7914">allow us to form distributional sentence representations as a function of word meaning. In this section, we will present an existing formalism aimed at solving this compositionality problem, as well as the mathematical background required to understand it and further extensions, building on the features and failures of previously discussed attempts at syntactically sensitive compositionality. Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) propose adapting a category theoretic model, inspired by the categorical compositional vector space model of quantum protocols (Abramsky and Coecke 2004), to the task of compositionality of semantic vectors. Syntactic analysis in the form of pregroup grammars—a categorial grammar—is given categorical semantics in order to be represented as a compact closed category P (a concept explained subsequently), the objects of which are syntactic types and the morphisms of which are the reductions forming the basis of syntactic analysis. This syntactic category is then mapped onto the semantic compact closed category FVect of finite dimensional vector spaces and linear maps. The mapping is done in the product category FVect × P via the following procedu</context>
<context position="62596" citStr="Abramsky and Coecke (2004)" startWordPosition="10086" endWordPosition="10089">ies of other formalisms, if they bear similar categorical representation. In this function, it has been 88 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning at the center of recent work in unifying two orthogonal models of meaning, a qualitative categorial grammar model and a quantitative distributional model (Clark, Coecke, and Sadrzadeh 2008; Coecke, Sadrzadeh, and Clark 2010). Moreover, the unifying categorical structures at work here were inspired by the ones used in the foundations of physics and the modeling of quantum information flow, as presented in Abramsky and Coecke (2004), where they relate the logical structure of quantum protocols to their state-based vector spaces data. The connection between the mathematics used for this branch of physics and those potentially useful for linguistic modeling has also been noted by several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004). In this section, we will briefly examine the basics of category theory, monoidal categories, and compact closed categories. The focus will be on defining enough basic concepts to proceed rather than provide a full-blown tutorial on category theory and the modeling o</context>
</contexts>
<marker>Abramsky, Coecke, 2004</marker>
<rawString>Abramsky, S. and B. Coecke. 2004. A categorical semantics of quantum protocols. In Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science, pages 415–425, Turku, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>editor</author>
</authors>
<title>The Core Language Engine.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<marker>Alshawi, editor, 1992</marker>
<rawString>Alshawi, H., editor. 1992. The Core Language Engine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--183</pages>
<location>Boston, MA.</location>
<contexts>
<context position="40426" citStr="Baroni and Zamparelli (2010)" startWordPosition="6455" endWordPosition="6458">the matrices act as linear maps on the vectors they take as “arguments,” and thus can encode more subtle syntactic or semantic relations. However, this model treats all word combinations as the same operation—for example, treating the combination of an adjective with its argument and a verb with its subject as the same sort of composition. Because of the diverse ways there are of training such supervised models, we leave it to those who wish to further develop this specific line of research to perform such evaluations. Adjective Matrices. The second approach is the matrix-composition model of Baroni and Zamparelli (2010), which they develop only for the case of adjective-noun composition, although their approach can seamlessly be used for any other predicate-argument composition. Contrary to most of the earlier approaches proposed, which aim to combine two lexical vectors to form a lexical vector for their combination, Baroni and Zamparelli suggest giving different semantic representations to different types, or more specifically to adjectives and nouns. 82 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning In this model, nouns are lexical vectors, as with other models. Howev</context>
<context position="42153" citStr="Baroni and Zamparelli (2010)" startWordPosition="6725" endWordPosition="6728">ctive noun = Madjective × −−→ −−−−−−−−−→ noun Similarly to the Generic Additive Model, the matrix for each adjective is learned by linear regression over a set of pairs (−−→ noun, →−c ) where the vectors −−→ noun are the lexical semantic vectors for the arguments of the adjective in a corpus, and −→c is the semantic vector corresponding to the expected output of the composition of the adjective with that noun. This may, at first blush, also appear to be a supervised training method for learning adjective matrices from “labeled data,” seeing how the expected output vectors are needed. However, Baroni and Zamparelli (2010) work around this constraint by automatically producing the labeled data from the corpus by treating the adjectivenoun compound as a single token, and learning its vector using the same distributional learning methods they used to learn the vectors for nouns. This same approach can be extended to other unary relations without change and, using the general framework of the current article, an extension of it to binary predicates has been presented in Grefenstette et al. (2013), using multistep regression. For a direct comparison of the results of this approach with some of the results of the cu</context>
<context position="83683" citStr="Baroni and Zamparelli 2010" startWordPosition="13790" endWordPosition="13793">an be used for this stage, such as those presented in Curran (2004), or the lexical semantic models used by Mitchell and Lapata (2008). It seems reasonable to assume that higher quality lexical semantic vectors—as measured by metrics such as the WordSim353 test 95 Computational Linguistics Volume 41, Number 1 of Finkelstein et al. (2001)—will produce better relational vectors from the procedure designed subsequently. We will not test the hypothesis here, but note that it is an underlying assumption in most of the current literature on the subject (Erk and Pad´o 2008; Mitchell and Lapata 2008; Baroni and Zamparelli 2010). Building upon the foundation of the thus constructed noun vectors, we construct semantic representations for relational words. In pregroup grammars (or other combinatorial grammars such as CCG), we can view such words as functions, taking as arguments those types present as adjoints in the compound pregroup type, and returning a syntactic object whose type is that of the corresponding reduction. For example, an adjective nnl takes a noun or noun phrase n and returns a noun phrase n from the reduction (nnl)n → n. It can also compose with another adjective to return an adjective (nnl)(nnl) → n</context>
<context position="137806" citStr="Baroni and Zamparelli 2010" startWordPosition="22782" endWordPosition="22785"> do away with this automated alignment, and include sentence comparisons that would penalize models which do not leverage syntactic information. Furthermore, each of these experiments dealt with the comparison of a certain type of sentence (transitive, intransitive). This was convenient for our concrete categorical model, as we defined the sentence space differently based on the valency of the head verb. However, sentences with different sorts of verbs should be able to be directly compared. Not only do several models, both non-syntax sensitive (additive, multiplicative) and syntax-sensitive (Baroni and Zamparelli 2010; Socher et al. 2012), not face this problem, as the product of composition is either naturally in the same space as the vectors being composed or is projected back into it, but the categorical framework the concrete categorical models were derived from does not commit us to different sentence spaces either. The topic of how to solve this problem for the concrete models developed here is beyond the scope of this article, but various options exist, such as the projection of ST into SI, the embedding of SI into ST, the use of a combined sentence space S = SI ⊕ ST, and so on. It is clear that fut</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Baroni, M. and R. Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1,183–1,193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B´echet</author>
<author>A Foret</author>
<author>I Tellier</author>
</authors>
<date>2007</date>
<booktitle>Learnability of Pregroup Grammars. Studia Logica,</booktitle>
<volume>87</volume>
<issue>2</issue>
<pages>225--252</pages>
<marker>B´echet, Foret, Tellier, 2007</marker>
<rawString>B´echet, D., A. Foret, and I. Tellier. 2007. Learnability of Pregroup Grammars. Studia Logica, 87(2–3), pages 225–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Blacoe</author>
<author>M Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing, Jeju Island.</booktitle>
<contexts>
<context position="45776" citStr="Blacoe and Lapata (2012)" startWordPosition="7319" endWordPosition="7322">ect and object. Whether or not this theoretical difference leads to significant differences in composition quality requires joint evaluation. Additionally, the description of MV-RNN models in Socher et al. (2012) specifies the need for a source of learning error during training, which is easy to measure in the case of label prediction experiments such as sentiment prediction, but non-trivial in the case of paraphrase detection where no objective label exists. A direct comparison to MV-RNN methods within the context of experiments similar to those presented in this article has been produced by Blacoe and Lapata (2012), showing that simple operations perform on par with the earlier complex deep learning architectures produced by Socher and colleagues; we leave direct comparisons to future work. Early work has shown that the addition of a hidden layer with non-linearities to these simple models will improve the results. 2.5 Some Other Approaches to Distributional Semantics Domains and Functions. In recent work, Turney (2012) suggests modeling word representations not as a single semantic vector, but as a pair of vectors: one containing the information of the word relative to its domain (the other words that </context>
<context position="134557" citStr="Blacoe and Lapata (2012)" startWordPosition="22277" endWordPosition="22280">e warrants further investigation. Overall, we observe two advantages of these models over those presented in the early part of this article, and those evaluated in the later part. First, we have shown that a linguistically motivated and mathematically sound framework can be implemented and learned. Second, we showed that simple methods such as the Kronecker model perform well, especially when combined with the multiplicative model (effectively a unary instance of the Kronecker model) for sentences with adjectives. Considering the “simple is best” position recently argued for experimentally by Blacoe and Lapata (2012), this is an interesting candidate for dealing with binary relation composition. Future Work. These experiments showcased the ability of various compositional models to produce sentence representations that were less ambiguous than the words that formed them. They also more generally demonstrated that concrete models could be built from the general categorical framework and perform adequately in simple paraphrase detection tasks. However, various aspects of compositionality were not evaluated here. First, syntax sensitivity is not as important in these experiments as it might be in “real” lang</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>Blacoe, W. and M. Lapata. 2012. A comparison of vector-based representations for semantic composition. Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Buszkowski</author>
</authors>
<title>Lambek grammars based on pregroups. In</title>
<date>2001</date>
<booktitle>Logical Aspects of Computational Linguistics. Lecture Notes in Computer Science,</booktitle>
<volume>volume</volume>
<pages>95--109</pages>
<editor>P. de Groote, G. Morrill, and C. Retor´e, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin Heidelberg,</location>
<contexts>
<context position="59130" citStr="Buszkowski (2001)" startWordPosition="9532" endWordPosition="9533">f tools for parsing available. If quick implementation of the formalism described later in this paper is required, it would be useful to be able to leverage the mature state of parsing tools available for other categorial grammars, such as the Clark and Curran (2007) statistical CCG parser, as well as Hockenmaier’s CCG lexicon and treebank (Hockenmaier 2003; Hockenmaier and Steedman 2007). In other words, is there any way we can translate at least some subset of CCG types into pregroup types? There are some theoretical obstacles to consider first: Pregroup grammars and CCG are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be weakly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used in Clark and Curran’s parser, or Hockenmaier’s lexicon and treebank, we may be in luck: Fowler and Penn (2010) prove that some CCGs, such as those used in the aforementioned tools, are strongly context-free and thus expressively equivalent to pregroup grammars. In order to be able to apply the parsing tools for CCGs to our setting, we use a translat</context>
</contexts>
<marker>Buszkowski, 2001</marker>
<rawString>Buszkowski, W. 2001. Lambek grammars based on pregroups. In P. de Groote, G. Morrill, and C. Retor´e, editors, Logical Aspects of Computational Linguistics. Lecture Notes in Computer Science, volume 2099. Springer, Berlin Heidelberg, pages 95–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. arXiv preprint arXiv:1301.3618.</title>
<date>2013</date>
<contexts>
<context position="127101" citStr="Chen et al. (2013)" startWordPosition="21203" endWordPosition="21206">syntactic types, but it is worthwhile performing these tests to at least verify the hypothesis that 9 Available at http://www.cs.ox.ac.uk/activities/compdistmeaning/GS2012data.txt. 10 For example, the entry statistical table show express good result would yield the sentences The statistical table showed the good result and The statistical table expressed the good result. 111 Computational Linguistics Volume 41, Number 1 operation-specific composition operations (or parameters) is a good thing. Notably, this idea has been explored very recently, within the context of deep learning networks, by Chen et al. (2013). Each mixed model has two components: a verb-argument composition model and a adjective-noun composition model. For verb-argument composition, we used the three best models from the previous experiment, namely, Multiply, Categorical, and Kronecker. For adjective composition we used three different methods of adjectivenoun composition. With −−−−−→ adjective and −−→ noun being the vectors for an adjective and a noun in our distributional lexical semantic space W (built using the same procedure as the previous experiments) and −−→ adjcat being the compact representation in the Categorical model,</context>
</contexts>
<marker>Chen, Socher, Manning, Ng, 2013</marker>
<rawString>Chen, Danqi, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2013. Learning new facts from knowledge bases with neural tensor networks and semantic word vectors. arXiv preprint arXiv:1301.3618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
</authors>
<title>Type-driven syntax and semantics for composing meaning vectors.</title>
<date>2013</date>
<booktitle>In Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse.</booktitle>
<volume>11</volume>
<pages>4</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7556" citStr="Clark (2013)" startWordPosition="1099" endWordPosition="1100">ogical form” versus “contextual use,” has left the quest for “what is the foundational structure of meaning?”—a question initially the concern of solely linguists and philosophers of language—even more of a challenge. In this section, we present a short overview of the background to the work developed in this article by briefly describing formal and distributional approaches to natural language semantics, and providing a non-exhaustive list of some approaches to compositional distributional semantics. For a more complete review of the topic, we encourage the reader to consult Turney (2012) or Clark (2013). 2.1 Montague Semantics Formal semantic models provide methods for translating sentences of natural language into logical formulae, which can then be fed to computer-aided automation tools to reason about them (Alshawi 1992). To compute the meaning of a sentence consisting of n words, meanings of these words must interact with one another. In formal semantics, this further interaction is represented as a function derived from the grammatical structure of the sentence. Such models consist of a pairing of syntactic analysis rules (in the form of a grammar) with semantic interpretation rules, as</context>
<context position="74383" citStr="Clark (2013)" startWordPosition="12288" endWordPosition="12289">ntations of strings of words with different grammatical structure lived in different spaces. A preliminary version of a DisCoCat was developed in Clark, Coecke, and Sadrzadeh (2008), a full version was elaborated on in Coecke, Sadrzadeh, and Clark (2010), where, based on the developments of Preller and Sadrzadeh (2010), it was also exemplified how the vector space model may be instantiated in a truth theoretic setting where meanings of words were sets of their denotations and meanings of sentences were their truth values. A nontechnical description of this theoretical setting was presented in Clark (2013), where a plausibility truth-theoretic model for sentence spaces was worked out and exemplified. The work of Grefenstette et al. (2011) focused on a tangential branch and developed a toy example where neither words nor sentence spaces were Boolean. The applicability of the theoretical setting to a real empirical natural language processing task and data from a large scale corpus was demonstrated in Grefenstette and Sadrzadeh (2011a, 2011b). There, we presented a general algorithm to build vector representations for words with simple and complex types and the sentences containing them; then app</context>
</contexts>
<marker>Clark, 2013</marker>
<rawString>Clark, S. 2013. Type-driven syntax and semantics for composing meaning vectors. In Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse. Oxford University Press. 11 http://www.cs.unm.edu/∼mccune/mace4/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
</authors>
<title>A compositional distributional model of meaning.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Quantum Interaction Symposium (QI-2008),</booktitle>
<location>Oxford.</location>
<marker>Clark, Coecke, Sadrzadeh, 2008</marker>
<rawString>Clark, S., B. Coecke, and M. Sadrzadeh. 2008. A compositional distributional model of meaning. In Proceedings of the Second Quantum Interaction Symposium (QI-2008), Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--493</pages>
<contexts>
<context position="58780" citStr="Clark and Curran (2007)" startWordPosition="9476" endWordPosition="9479">of a workable pregroup parser, the pregroup grammars 87 Computational Linguistics Volume 41, Number 1 we will use in our categorical formalism are derived from CCG types, as we explain in the following. Pregroup Grammars and Other Categorial Grammars. Pregroup grammars, in contrast with other categorial grammars such as CCG, do not yet have a large set of tools for parsing available. If quick implementation of the formalism described later in this paper is required, it would be useful to be able to leverage the mature state of parsing tools available for other categorial grammars, such as the Clark and Curran (2007) statistical CCG parser, as well as Hockenmaier’s CCG lexicon and treebank (Hockenmaier 2003; Hockenmaier and Steedman 2007). In other words, is there any way we can translate at least some subset of CCG types into pregroup types? There are some theoretical obstacles to consider first: Pregroup grammars and CCG are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be weakly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used i</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Clark, S. and J. R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33:493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>S Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In AAAI Spring Symposium on Quantum Interaction,</booktitle>
<location>Stanford, USA.</location>
<contexts>
<context position="31839" citStr="Clark and Pulman (2007)" startWordPosition="5032" endWordPosition="5035">te syntactic analysis into composition operations. Early work on including syntactic sensitivity into DSMs by Grefenstette (1992) suggests using crude syntactic relations to determine the frame in which the distributions for word vectors are collected from the corpus, thereby embedding syntactic information into the word vectors. This idea was already present in the work of Smolensky, who used sums of pairs of vector representations and their roles, obtained by taking their tensor products, to obtain a vector representation for a compound. The application of these ideas to DSMs was studied by Clark and Pulman (2007), who suggest instantiating the roles to dependency relations and using the distributional representations of words as the vectors. For example, in the sentence Simon loves red wine, Simon is the subject of loves, wine is its object, and red is an adjective describing wine. Hence, from the dependency tree with loves as root node, its subject and object as children, and their adjectival descriptors (if any) as their children, we read the following ���� structure: loves ®��� subj®����� Simon®obj ® wine inner products of tensor products: (a ® b |c ® d ) = (a |c ) x (b |d ) We can therefore expres</context>
<context position="51698" citStr="Clark and Pulman (2007)" startWordPosition="8232" endWordPosition="8235">(2008) and syntactically driven semantic composition in the form of inner-products provide the implicit disambiguation features of the compositional models of Erk and Pad´o (2008) and Mitchell and Lapata (2008). The composition mechanism also involves the projection of tensored vectors into a common semantic space without the need for full representation of the tensored vectors in a manner similar to Plate (1991), without restriction to the nature of the vector spaces it can be applied to. This avoids the problems faced by other tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). 85 Computational Linguistics Volume 41, Number 1 The word vectors can be specified model-theoretically and the sentence space can be defined over Boolean values to obtain grammatically driven truth-theoretic semantics in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008). Some logical operators can be emulated in this setting, such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non-truth theoretic semantic mo</context>
<context position="73589" citStr="Clark and Pulman (2007)" startWordPosition="12164" endWordPosition="12167"> important to note that the tensor product passed as argument the composition morphism, namely, kittens ® sleepep in the intransitive case and in the transitive case, never needs to be computed. We can treat the tensor products here as commas separating function arguments, thereby avoiding the dimensionality problems presented by earlier tensor-based approaches to compositionality. 3.4 This Article and the DisCoCat Literature owing sections. 4. Concrete Semantic Spaces As elaborated on in Section 2.3.4, the first general setting for pairing meaning vectors with syntactic types was proposed in Clark and Pulman (2007). The setting of a DisCoCat generalized this by making the meaning derivation process rely on a syntactic type system, hence overcoming its central problem whereby the vector representations of strings of words with different grammatical structure lived in different spaces. A preliminary version of a DisCoCat was developed in Clark, Coecke, and Sadrzadeh (2008), a full version was elaborated on in Coecke, Sadrzadeh, and Clark (2010), where, based on the developments of Preller and Sadrzadeh (2010), it was also exemplified how the vector space model may be instantiated in a truth theoretic sett</context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>Clark, S. and S. Pulman. 2007. Combining symbolic and distributional models of meaning. In AAAI Spring Symposium on Quantum Interaction, Stanford, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: An overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>112--119</pages>
<location>Edinburgh.</location>
<contexts>
<context position="47629" citStr="Clarke (2009" startWordPosition="7606" endWordPosition="7607">re in this article, Turney shows that similarity measures can be designed for tasks similar to those presented here. The particular limitation of his approach, which Turney discusses, is that similarity measures must be specified for each task, whereas most of the compositional models described herein produce representations that can be compared in a task-independent manner (e.g., through cosine similarity). Nonetheless, this approach is innovative, and will merit further attention in future work in this area. Language as Algebra. A theoretical model of meaning as context has been proposed in Clarke (2009, 2012). In that model, the meaning of any string of words is a vector built 84 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning from the occurrence of the string in a corpus. This is the most natural extension of distributional models from words to strings of words: in that model, one builds vectors for strings of words in exactly the same way as one does for words. The main problem, however, is that of data sparsity for the occurrences of strings of words. Words do appear repeatedly in a document, but strings of words, especially for longer strings, rarely</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Clarke, Daoud. 2009. Context-theoretic semantics for natural language: An overview. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 112–119, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Clarke, 2012</marker>
<rawString>Clarke, Daoud. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>´E O Paquette</author>
</authors>
<title>Categories for the practising physicist.</title>
<date>2011</date>
<booktitle>New Structures for Physics. Lecture Notes in Physics,</booktitle>
<volume>813</volume>
<pages>173--286</pages>
<editor>In B. Coecke, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="63333" citStr="Coecke and Paquette 2011" startWordPosition="10199" endWordPosition="10202">ection between the mathematics used for this branch of physics and those potentially useful for linguistic modeling has also been noted by several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004). In this section, we will briefly examine the basics of category theory, monoidal categories, and compact closed categories. The focus will be on defining enough basic concepts to proceed rather than provide a full-blown tutorial on category theory and the modeling of information flow, as several excellent sources already cover both aspects (e.g., Mac Lane 1998; Walters 1991; Coecke and Paquette 2011). A categoriesin-a-nutshell crash course is also provided in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010). The Basics of Category Theory. A basic category C is defined in terms of the following elements: • A collection of objects ob(C). • A collection of morphisms hom(C). • A morphism composition operation ◦. Each morphism f has a domain dom(f) ∈ ob(C) and a codomain codom(f) ∈ ob(C). For dom(f) = A and codom(f) = B we abbreviate these definitions as f : A → B. Despite the notational similarity to function definitions (and sets and functions being an example of a</context>
</contexts>
<marker>Coecke, Paquette, 2011</marker>
<rawString>Coecke, B. and ´E. O. Paquette. 2011. Categories for the practising physicist. In B. Coecke, editor, New Structures for Physics. Lecture Notes in Physics, volume 813, pages 173–286, Springer, Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<title>Mathematical Foundations for a Compositional Distributional Model of Meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Coecke, B., M. Sadrzadeh, and S. Clark. 2010. Mathematical Foundations for a Compositional Distributional Model of Meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically motivated large-scale NLP with C&amp;C and boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>33--36</pages>
<location>Prague.</location>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>Curran, James, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&amp;C and boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33–36, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
</authors>
<title>From distributional to semantic similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="2626" citStr="Curran 2004" startWordPosition="377" endWordPosition="378">ing and Computer Science, Queen Mary University of London, Mile End Road, London E1 4NS, United Kingdom. E-mail: mehrnoosh.sadrzadeh®qmul.ac.uk. † The work described in this article was performed while the authors were at the University of Oxford. 1 Support from EPSRC grant EP/J002607/1 is acknowledged. Submission received: 26 September 2012; revised submission received: 31 October 2013; accepted for publication: 5 April 2014. doi:10.1162/COLI a 00209 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 automated thesaurus building (Grefenstette 1994; Curran 2004) to automated essay marking (Landauer and Dumais 1997). The broader connection to information retrieval and its applications is also discussed by Manning, Raghavan, and Sch¨utze (2011). The success of DSMs in essentially word-based tasks such as thesaurus extraction and construction (Grefenstette 1994; Curran 2004) invites an investigation into how DSMs can be applied to NLP and information retrieval (IR) tasks revolving around larger units of text, using semantic representations for phrases, sentences, or documents, constructed from lemma vectors. However, the problem of compositionality in D</context>
<context position="6722" citStr="Curran 2004" startWordPosition="978" endWordPosition="979">ntence is a function of the meaning of its parts (Frege 1892). These models relate to well-known and robust logical formalisms, hence offering a scalable theory of meaning that can be used to reason about language using logical tools of proof and inference. Distributional models are a more recent approach to semantic modeling, representing 72 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning the meaning of words as vectors learned empirically from corpora. They have found their way into real-world applications such as thesaurus extraction (Grefenstette 1994; Curran 2004) or automated essay marking (Landauer and Dumais 1997), and have connections to semantically motivated information retrieval (Manning, Raghavan, and Sch¨utze 2011). This two-sortedness of defining properties of meaning: “logical form” versus “contextual use,” has left the quest for “what is the foundational structure of meaning?”—a question initially the concern of solely linguists and philosophers of language—even more of a challenge. In this section, we present a short overview of the background to the work developed in this article by briefly describing formal and distributional approaches </context>
<context position="13906" citStr="Curran (2004)" startWordPosition="2130" endWordPosition="2131">hing more complex, such as using dependency relations (Pad´o and Lapata 2007) or other syntactic features. Commonly, the similarity of two semantic vectors is computed by taking their cosine measure, which is the sum of the product of the basis weights of the vectors: cgcb cosine(−→a , b ) = �i where cai and cbi are the basis weights for −→a and −→b , respectively. However, other options may be a better fit for certain implementations, typically dependent on the weighting scheme. Readers interested in learning more about these aspects of distributional lexical semantics are invited to consult Curran (2004), which contains an extensive overview of implementation options for distributional models of word meaning. 2.3 Compositionality and Vector Space Models In the previous overview of distributional semantic models of lexical semantics, we have seen that DSMs are a rich and tractable way of learning word meaning from a corpus, and obtaining a measure of semantic similarity of words or groups of words. However, it should be fairly obvious that the same method cannot be applied to sentences, whereby the meaning of a sentence would be given by the distribution of other sentences with which it occurs</context>
<context position="83123" citStr="Curran (2004)" startWordPosition="13705" endWordPosition="13706">hence we give them the type nnl and associated semantic space N ® N. With the provision of a learning procedure for vectors in these semantic spaces, we can use these types to construct sentence vector representations for simple intransitive verb–based and transitive verb–based sentences, with and without adjectives applied to subjects and objects. 4.3 Learning Procedures To begin, we construct the semantic space N for all nouns in our lexicon (typically limited by the words available in the corpus used). Any distributional semantic model can be used for this stage, such as those presented in Curran (2004), or the lexical semantic models used by Mitchell and Lapata (2008). It seems reasonable to assume that higher quality lexical semantic vectors—as measured by metrics such as the WordSim353 test 95 Computational Linguistics Volume 41, Number 1 of Finkelstein et al. (2001)—will produce better relational vectors from the procedure designed subsequently. We will not test the hypothesis here, but note that it is an underlying assumption in most of the current literature on the subject (Erk and Pad´o 2008; Mitchell and Lapata 2008; Baroni and Zamparelli 2010). Building upon the foundation of the th</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>Curran, J. R. 2004. From distributional to semantic similarity. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>MapReduce: Simplified data processing on large clusters.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>1</issue>
<contexts>
<context position="30675" citStr="Dean and Ghemawat (2008)" startWordPosition="4847" endWordPosition="4850">e for compositional DSMs, although it is important to note that Widdows (2008) considers possible application areas where they may be of use, although once again these mostly involve noun-verb and adjectivenoun compositionality rather than full blown sentence vector construction. We retain 2 Source:http://www.oxforddictionaries.com/page/howmanywords. 3 At four bytes per integer, and one integer per basis vector weight, the vector for John loves Mary would require roughly (170, 000 · 4)3 ≈ 280 petabytes of storage, which is over ten times the data Google processes on a daily basis according to Dean and Ghemawat (2008). 79 Computational Linguistics Volume 41, Number 1 from Plate (1991) the importance of finding methods by which to project the tensored sentence vectors into a common space for direct comparison, as will be discussed further in Section 3. Syntactic Expressivity. An additional problem of a more conceptual nature is that using the tensor product as a composition operation simply preserves word order. As we discussed in Section 2.3.3, this is not enough on its own to model sentence meaning. We need to have some means by which to incorporate syntactic analysis into composition operations. Early wo</context>
</contexts>
<marker>Dean, Ghemawat, 2008</marker>
<rawString>Dean, Jeffrey and Sanjay Ghemawat. 2008. MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1):107–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08,</booktitle>
<pages>897--906</pages>
<location>Edinburgh.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Erk, Katrin and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08, pages 897–906, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<contexts>
<context position="83395" citStr="Finkelstein et al. (2001)" startWordPosition="13745" endWordPosition="13748">nsitive verb–based sentences, with and without adjectives applied to subjects and objects. 4.3 Learning Procedures To begin, we construct the semantic space N for all nouns in our lexicon (typically limited by the words available in the corpus used). Any distributional semantic model can be used for this stage, such as those presented in Curran (2004), or the lexical semantic models used by Mitchell and Lapata (2008). It seems reasonable to assume that higher quality lexical semantic vectors—as measured by metrics such as the WordSim353 test 95 Computational Linguistics Volume 41, Number 1 of Finkelstein et al. (2001)—will produce better relational vectors from the procedure designed subsequently. We will not test the hypothesis here, but note that it is an underlying assumption in most of the current literature on the subject (Erk and Pad´o 2008; Mitchell and Lapata 2008; Baroni and Zamparelli 2010). Building upon the foundation of the thus constructed noun vectors, we construct semantic representations for relational words. In pregroup grammars (or other combinatorial grammars such as CCG), we can view such words as functions, taking as arguments those types present as adjoints in the compound pregroup t</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2001. Placing search in context: The concept revisited.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 10th International Conference on World Wide Web,</booktitle>
<pages>406--414</pages>
<location>Hong Kong.</location>
<marker></marker>
<rawString>In Proceedings of the 10th International Conference on World Wide Web, pages 406–414, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis.</title>
<date>1957</date>
<contexts>
<context position="1367" citStr="Firth (1957)" startWordPosition="191" endWordPosition="192">r this setting by developing algorithms to construct tensors and linear maps and instantiate the abstract parameters using empirical data. We then evaluate our concrete models against several experiments, both existing and new, based on measuring how well models align with human judgments in a paraphrase detection task. Our results show the implementation of this general abstract framework to perform on par with or outperform other leading models in these experiments.1 1. Introduction The distributional approach to the semantic modeling of natural language, inspired by the notion—presented by Firth (1957) and Harris (1968)—that the meaning of a word is tightly related to its context of use, has grown in popularity as a method of semantic representation. It draws from the frequent use of vector-based document models in information retrieval, modeling the meaning of words as vectors based on the distribution of co-occurring terms within the context of a word. Using various vector similarity metrics as a measure of semantic similarity, these distributional semantic models (DSMs) are used for a variety of NLP tasks, from * DeepMind Technologies Ltd, 5 New Street Square, London EC4A 3 TW. E-mail: e</context>
<context position="10809" citStr="Firth (1957)" startWordPosition="1610" endWordPosition="1611">hermore, an underlying domain of objects and a valuation function must be provided, as with any logic, leaving open the question of how we might learn the meaning of language using such a model, rather than just use it. 2.2 Distributional Semantics A popular way of representing the meaning of words in lexical semantics is as distributions in a high-dimensional vector space. This approach is based on the distributional hypothesis of Harris (1968), who postulated that the meaning of a word was dictated by the context of its use. The more famous dictum stating this hypothesis is the statement of Firth (1957) that “You shall know a word by the company it keeps.” This view of semantics has furthermore been associated (Grefenstette 2009; Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein (presented in Wittgenstein 1953), who stated that language meaning was equivalent to its real world use. Practically speaking, the meaning of a word can be learned from a corpus by looking at what other words occur with it within a certain context, and the resulting distribution can be represented as a vector in a semantic vector space. This vectorial representation is convenient bec</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J. R. 1957. A synopsis of linguistic theory 1930-1955. Studies in linguistic analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A D Fowler</author>
<author>G Penn</author>
</authors>
<title>Accurate context-free parsing with combinatory categorial grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>335--344</pages>
<location>Uppsala.</location>
<contexts>
<context position="59489" citStr="Fowler and Penn (2010)" startWordPosition="9586" endWordPosition="9589"> 2003; Hockenmaier and Steedman 2007). In other words, is there any way we can translate at least some subset of CCG types into pregroup types? There are some theoretical obstacles to consider first: Pregroup grammars and CCG are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be weakly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used in Clark and Curran’s parser, or Hockenmaier’s lexicon and treebank, we may be in luck: Fowler and Penn (2010) prove that some CCGs, such as those used in the aforementioned tools, are strongly context-free and thus expressively equivalent to pregroup grammars. In order to be able to apply the parsing tools for CCGs to our setting, we use a translation mechanism from CCG types to pregroup types based on the Lambek-calculus-to-pregroup-grammar translation originally presented in Lambek (1999). In this mechanism, each atomic CCG type X is assigned a unique pregroup type x; for any X/Y in CCG we have xyl in the pregroup grammar; and for any X\Y in CCG we have yrx in pregroup grammar. Therefore, by assign</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Fowler, T. A. D. and G. Penn. 2010. Accurate context-free parsing with combinatory categorial grammar. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335–344, Uppsala.</rawString>
</citation>
<citation valid="false">
<authors>
<author>G Frege</author>
</authors>
<title>Uber sinn und bedeutung.</title>
<booktitle>Zeitschrift f¨ur Philosophie und philosophische Kritik,</booktitle>
<volume>100</volume>
<issue>1</issue>
<marker>Frege, </marker>
<rawString>Frege, G. 1892. Uber sinn und bedeutung. Zeitschrift f¨ur Philosophie und philosophische Kritik, 100(1):25–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Genkin</author>
<author>Nissim Francez</author>
<author>Michael Kaminski</author>
</authors>
<title>Mildly context-sensitive languages via buffer augmented pregroup grammars.</title>
<date>2010</date>
<booktitle>Time for Verification,</booktitle>
<pages>144--166</pages>
<editor>In Z. Manna and D. A. Peled, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Genkin, Francez, Kaminski, 2010</marker>
<rawString>Genkin, Daniel, Nissim Francez, and Michael Kaminski. 2010. Mildly context-sensitive languages via buffer augmented pregroup grammars. In Z. Manna and D. A. Peled, editors, Time for Verification, pages 144–166, Springer-Verlag, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Yves Girard</author>
</authors>
<title>Linear logic.</title>
<date>1987</date>
<journal>Theoretical Computer Science,</journal>
<pages>50--1</pages>
<contexts>
<context position="53093" citStr="Girard 1987" startWordPosition="8444" endWordPosition="8445">mars in Section 3.1, and the required basics of category theory in Section 3.2. 3.1 Pregroup Grammars Presented by Lambek (1999, 2008) as a successor to his syntactic calculus (Lambek 1958), pregroup grammars are a class of categorial type grammars with pregroup algebras as semantics. Pregroups are particularly interesting within the context of this work because of their well-studied algebraic structure, which can trivially be mapped onto the structure of the category of vector spaces, as will be discussed subsequently. Logically speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which the tensor and its dual par coincide; this logic is sometimes referred to as Bi-Compact Linear Logic (Lambek 1999). The formalism works alongside the general guidelines of other categorial grammars, for instance, those of the combinatory categorial grammar (CCG) designed by Steedman (2001) and Steedman and Baldridge (2011). They consist of atomic grammatical types that combine to form compound types. A series of CCGlike application rules allow for type-reductions, forming the basis of syntactic analysis. As our first step, we show how this syntactic analysis formalism works by presen</context>
</contexts>
<marker>Girard, 1987</marker>
<rawString>Girard, Jean-Yves. 1987. Linear logic. Theoretical Computer Science, 50:1–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
</authors>
<title>Analysing document similarity measures. Master’s thesis,</title>
<date>2009</date>
<institution>University of Oxford.</institution>
<contexts>
<context position="10937" citStr="Grefenstette 2009" startWordPosition="1631" endWordPosition="1632">stion of how we might learn the meaning of language using such a model, rather than just use it. 2.2 Distributional Semantics A popular way of representing the meaning of words in lexical semantics is as distributions in a high-dimensional vector space. This approach is based on the distributional hypothesis of Harris (1968), who postulated that the meaning of a word was dictated by the context of its use. The more famous dictum stating this hypothesis is the statement of Firth (1957) that “You shall know a word by the company it keeps.” This view of semantics has furthermore been associated (Grefenstette 2009; Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein (presented in Wittgenstein 1953), who stated that language meaning was equivalent to its real world use. Practically speaking, the meaning of a word can be learned from a corpus by looking at what other words occur with it within a certain context, and the resulting distribution can be represented as a vector in a semantic vector space. This vectorial representation is convenient because vectors are a familiar structure with a rich set of ways of computing vector distance, allowing us to experiment with diffe</context>
<context position="15862" citStr="Grefenstette (2009)" startWordPosition="2441" endWordPosition="2442">hat comes to mind is straightforward vector addition, such that: ab = −→ −→ a + −→b Conceptually speaking, if we view word vectors as semantic information distributed across a set of properties associated with basis vectors, using vector addition as a semantic composition operation states that the information of a set of lemmas in a sentence is simply the sum of the information of the individual lemmas. Although crude, this approach is computationally cheap, and appears sufficient for certain NLP tasks: Landauer and Dumais (1997) show it to be sufficient for automated essay marking tasks, and Grefenstette (2009) shows it to perform better than a collection of other simple similarity metrics for summarization, sentence paraphrase, and document paraphrase detection tasks. However, there are two principal objections to additive models of composition: first, −−−−−−−−−−−−→ −−→= vector addition is commutative, therefore, John drank wine = John + drank + wine ✓�i (cai )2 Ei (cbi )2 75 Computational Linguistics Volume 41, Number 1 −−−−−−−−−−−−→ Wine drank John, and thus vector addition ignores syntactic structure completely; and second, vector addition sums the information contained in the vectors, effective</context>
</contexts>
<marker>Grefenstette, 2009</marker>
<rawString>Grefenstette, E. 2009. Analysing document similarity measures. Master’s thesis, University of Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>G Dinu</author>
<author>Y Zhang</author>
<author>M Sadrzadeh</author>
<author>M Baroni</author>
</authors>
<title>Multistep regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Tenth International Conference on Computational Semantics,</booktitle>
<location>Potsdam.</location>
<contexts>
<context position="42633" citStr="Grefenstette et al. (2013)" startWordPosition="6802" endWordPosition="6805">thod for learning adjective matrices from “labeled data,” seeing how the expected output vectors are needed. However, Baroni and Zamparelli (2010) work around this constraint by automatically producing the labeled data from the corpus by treating the adjectivenoun compound as a single token, and learning its vector using the same distributional learning methods they used to learn the vectors for nouns. This same approach can be extended to other unary relations without change and, using the general framework of the current article, an extension of it to binary predicates has been presented in Grefenstette et al. (2013), using multistep regression. For a direct comparison of the results of this approach with some of the results of the current article, we refer the reader to Grefenstette et al. (2013). Recursive Matrix-Vector Model. The third approach is the recently developed Recursive Matrix-Vector Model (MV-RNN) of Socher et al. (2012), which claims the two matrixbased models described here as special cases. In MV-RNN, words are represented as a pairing of a lexical semantic vector −→a with an operation matrix A. Within this model, given the parse of a sentence in the form of a binarized tree, the semantic</context>
<context position="80982" citStr="Grefenstette et al. (2013)" startWordPosition="13364" endWordPosition="13367">ce. To propagate this distinction on the syntactic level, we define types sI and sT for intransitive and transitive sentences, respectively. In creating this distinction, we lost one of the most appealing features of the framework of Coecke, Sadrzadeh, and Clark (2010), namely, the result that all sentence vectors live in the same sentence space. A mathematical solution to this two-space problem was suggested in Grefenstette et al. (2011), and a variant of the models presented in this article permitting the non-problematic projection into a single sentence space (S ∼= N) has been presented by Grefenstette et al. (2013). Keeping this separation allows us to deal with both the transitive and intransitive cases in a simpler manner, and because the experiments in this article only compare intransitive sentences with intransitive sentences, and transitive sentences with transitive sentences, we will not address the issue of unification here. 4.2 Noun-Oriented Types While Lambek’s pregroup types presented in Lambek (2008) include a rich array of basic types and hand-designed compound types in order to capture specific grammatic properties, for the sake of simplicity we will use a simpler set of grammatical types </context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Grefenstette, E., G. Dinu, Y. Zhang, M. Sadrzadeh, and M. Baroni. 2013. Multistep regression learning for compositional distributional semantics. In Proceedings of the Tenth International Conference on Computational Semantics, Potsdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--394</pages>
<location>Edinburgh.</location>
<contexts>
<context position="3839" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="551" endWordPosition="554">mpositionality in DSMs—of how to go from word to sentence and beyond—has proved to be non-trivial. A new framework, which we refer to as DisCoCat, initially presented in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) reconciles distributional approaches to natural language semantics with the structured, logical nature of formal semantic models. This framework is abstract; its theoretical predictions have not been evaluated on real data, and its applications to empirical natural language processing tasks have not been studied. This article is the journal version of Grefenstette and Sadrzadeh (2011a, 2011b), which fill this gap in the DisCoCat literature; in it, we develop a concrete model and an unsupervised learning algorithm to instantiate the abstract vectors, linear maps, and vector spaces of the theoretical framework; we develop a series of empirical natural language processing experiments and data sets and implement our algorithm on large scale real data; we analyze the outputs of the algorithm in terms of linear algebraic equations; and we evaluate the model on these experiments and compare the results with other competing unsupervised models. Furthermore, we provide a linear al</context>
<context position="74817" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="12352" endWordPosition="12355">setting where meanings of words were sets of their denotations and meanings of sentences were their truth values. A nontechnical description of this theoretical setting was presented in Clark (2013), where a plausibility truth-theoretic model for sentence spaces was worked out and exemplified. The work of Grefenstette et al. (2011) focused on a tangential branch and developed a toy example where neither words nor sentence spaces were Boolean. The applicability of the theoretical setting to a real empirical natural language processing task and data from a large scale corpus was demonstrated in Grefenstette and Sadrzadeh (2011a, 2011b). There, we presented a general algorithm to build vector representations for words with simple and complex types and the sentences containing them; then applied the algorithm to a disambiguation task performed on the British National Corpus (BNC). We also investigated the vector representation of transitive verbs and showed how a number of single operations may optimize the performance. We discuss these developments in detail in the foll In Section 3.3 we presented a categorical formalism that relates syntactic analysis steps to semantic composition operations. The structure of our s</context>
<context position="92648" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="15423" endWordPosition="15426">(on the syntactic level) or tensored (on the semantic level) input types. It is this symmetry between input and output types that guarantees that any m-ary relation, expressed in the original formulation as an element of tensor space N ⊗ . . . ⊗ N , has a compact representation in N ⊗ ... ⊗ N , where the ith basis weight \ v � ~ Y ol 2m m of the reduced representation stands for the degree to which the ith element of the input vector affects the ith element of the output vector. This allows the specification of the generalized learning algorithm for reduced representations, first presented in Grefenstette and Sadrzadeh (2011a), which is as follows. Each relational word P with grammatical type π and m adjoint types α1, α2, · · ·, αm is encoded as an (r × ... × r) multi-dimensional array with m degrees of freedom (i.e., a rank m tensor). Because our vector space N has a fixed basis, each such array is represented in vector form as follows: −→ �P = cij···ζ (−→ n i ⊗ −→ n j ⊗ ··· ⊗ →−n ζ) ij · · · ζ \ V J m \ V J m This vector lives in the tensor space N ⊗ N ⊗ · · · ⊗ N. Each cij···ζ is computed according ~ Y � m to the procedure described in Figure 2. Linear algebraically, this procedure corresponds to computing the</context>
<context position="95313" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="15955" endWordPosition="15958">; adverbs would be of type (nrs)r(nrs), and hence be elements of S ® N ® N ® S. We could learn them using the procedure defined earlier, although for words like determiners and conjunctions, it is unclear whether we would want to learn such logical words or design them by hand, as was done in Coecke, Sadrzadeh, and Clark (2010) for negation. Nonetheless, the procedure given here allows us to generalize the work presented in this article to sentences of any length or structure based on the pregroup types of the words they contain. 4.4 Example To conclude this section with an example taken from Grefenstette and Sadrzadeh (2011a), we demonstrate how the meaning of the word show might be learned from a corpus and then composed. Suppose there are two instances of the verb show in the corpus: s1 = table show result s2 = map show location The vector of show is ���� ��� ���� + ��� show = table ® result map ® ������ location Consider a vector space N with four basis vectors far, room, scientific, and elect. The TF/IDF-weighted values for vectors of selected nouns (built from the BNC) are as shown in Table 1. Part of the matrix compact representation of show is presented in Table 2. Table 1 Sample weights for selected noun</context>
<context position="113844" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="19038" endWordPosition="19041">011. The binary composition data set used here are popular; now there are a host of new state-of-the-art systems available in the literature. We invite the reader to check references to Mitchell and Lapata (2008) to find the current state of the art. 106 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning Table 4 Model correlation coefficients with human judgments, first experiment. p &lt; 0.05 for each p. Model p Verb Baseline 0.08 Bigram Baseline 0.02 Add 0.04 Multiply 0.17 Categorical 0.17 UpperBound 0.40 This second experiment, which we initially presented in Grefenstette and Sadrzadeh (2011a), is an extension of the first in the case of sentences centered around transitive verbs, composed with a subject and an object. The results of the first experiment did not demonstrate any difference between the multiplicative model, which takes into account no syntactic information or word ordering, and our syntactically motivated categorical compositional model. By running the same experiment over a new data set, where the relations expressed by the verb have a higher arity than in the first, we hope to demonstrate that added structure leads to better results for our syntax-sensitive model</context>
<context position="118720" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="19842" endWordPosition="19845">a set, we rejected no Turker contributions, as the answers to the gold standard sentence pairs were aligned with our expectations. We attribute this to our adding the requirement that Turkers be based in the US or the UK and have English as the first language. 8 For example, the entry draw table eye depict would yield the sentences The table drew the eye and The table depicted the eye. 108 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning All of these models have been explained earlier, with the exception of Kronecker. We first presented this new addition in Grefenstette and Sadrzadeh (2011b), where we observed that the compact representation of a verb in the DisCoCat framework, under the assumptions presented in Section 4, can be viewed as dim(N) × dim(N) matrices in N ⊗ N. We considered alternatives to the algorithm presented earlier for the construction of such matrices, and were surprised by the results of the Kronecker method, wherein we replaced the matrix learned by our algorithm with the Kronecker product of the lexical semantic vectors for the verb. Further analysis performed since the publication of that paper can help to understand why this method might work. Using th</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Grefenstette, E. and M. Sadrzadeh. 2011a. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1,394–1,404, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
</authors>
<title>Experimenting with transitive verbs in a DisCoCat.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 EMNLP Workshop on Geometric Models of Natural Language Semantics,</booktitle>
<pages>62--66</pages>
<location>Edinburgh.</location>
<contexts>
<context position="3839" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="551" endWordPosition="554">mpositionality in DSMs—of how to go from word to sentence and beyond—has proved to be non-trivial. A new framework, which we refer to as DisCoCat, initially presented in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) reconciles distributional approaches to natural language semantics with the structured, logical nature of formal semantic models. This framework is abstract; its theoretical predictions have not been evaluated on real data, and its applications to empirical natural language processing tasks have not been studied. This article is the journal version of Grefenstette and Sadrzadeh (2011a, 2011b), which fill this gap in the DisCoCat literature; in it, we develop a concrete model and an unsupervised learning algorithm to instantiate the abstract vectors, linear maps, and vector spaces of the theoretical framework; we develop a series of empirical natural language processing experiments and data sets and implement our algorithm on large scale real data; we analyze the outputs of the algorithm in terms of linear algebraic equations; and we evaluate the model on these experiments and compare the results with other competing unsupervised models. Furthermore, we provide a linear al</context>
<context position="74817" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="12352" endWordPosition="12355">setting where meanings of words were sets of their denotations and meanings of sentences were their truth values. A nontechnical description of this theoretical setting was presented in Clark (2013), where a plausibility truth-theoretic model for sentence spaces was worked out and exemplified. The work of Grefenstette et al. (2011) focused on a tangential branch and developed a toy example where neither words nor sentence spaces were Boolean. The applicability of the theoretical setting to a real empirical natural language processing task and data from a large scale corpus was demonstrated in Grefenstette and Sadrzadeh (2011a, 2011b). There, we presented a general algorithm to build vector representations for words with simple and complex types and the sentences containing them; then applied the algorithm to a disambiguation task performed on the British National Corpus (BNC). We also investigated the vector representation of transitive verbs and showed how a number of single operations may optimize the performance. We discuss these developments in detail in the foll In Section 3.3 we presented a categorical formalism that relates syntactic analysis steps to semantic composition operations. The structure of our s</context>
<context position="92648" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="15423" endWordPosition="15426">(on the syntactic level) or tensored (on the semantic level) input types. It is this symmetry between input and output types that guarantees that any m-ary relation, expressed in the original formulation as an element of tensor space N ⊗ . . . ⊗ N , has a compact representation in N ⊗ ... ⊗ N , where the ith basis weight \ v � ~ Y ol 2m m of the reduced representation stands for the degree to which the ith element of the input vector affects the ith element of the output vector. This allows the specification of the generalized learning algorithm for reduced representations, first presented in Grefenstette and Sadrzadeh (2011a), which is as follows. Each relational word P with grammatical type π and m adjoint types α1, α2, · · ·, αm is encoded as an (r × ... × r) multi-dimensional array with m degrees of freedom (i.e., a rank m tensor). Because our vector space N has a fixed basis, each such array is represented in vector form as follows: −→ �P = cij···ζ (−→ n i ⊗ −→ n j ⊗ ··· ⊗ →−n ζ) ij · · · ζ \ V J m \ V J m This vector lives in the tensor space N ⊗ N ⊗ · · · ⊗ N. Each cij···ζ is computed according ~ Y � m to the procedure described in Figure 2. Linear algebraically, this procedure corresponds to computing the</context>
<context position="95313" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="15955" endWordPosition="15958">; adverbs would be of type (nrs)r(nrs), and hence be elements of S ® N ® N ® S. We could learn them using the procedure defined earlier, although for words like determiners and conjunctions, it is unclear whether we would want to learn such logical words or design them by hand, as was done in Coecke, Sadrzadeh, and Clark (2010) for negation. Nonetheless, the procedure given here allows us to generalize the work presented in this article to sentences of any length or structure based on the pregroup types of the words they contain. 4.4 Example To conclude this section with an example taken from Grefenstette and Sadrzadeh (2011a), we demonstrate how the meaning of the word show might be learned from a corpus and then composed. Suppose there are two instances of the verb show in the corpus: s1 = table show result s2 = map show location The vector of show is ���� ��� ���� + ��� show = table ® result map ® ������ location Consider a vector space N with four basis vectors far, room, scientific, and elect. The TF/IDF-weighted values for vectors of selected nouns (built from the BNC) are as shown in Table 1. Part of the matrix compact representation of show is presented in Table 2. Table 1 Sample weights for selected noun</context>
<context position="113844" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="19038" endWordPosition="19041">011. The binary composition data set used here are popular; now there are a host of new state-of-the-art systems available in the literature. We invite the reader to check references to Mitchell and Lapata (2008) to find the current state of the art. 106 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning Table 4 Model correlation coefficients with human judgments, first experiment. p &lt; 0.05 for each p. Model p Verb Baseline 0.08 Bigram Baseline 0.02 Add 0.04 Multiply 0.17 Categorical 0.17 UpperBound 0.40 This second experiment, which we initially presented in Grefenstette and Sadrzadeh (2011a), is an extension of the first in the case of sentences centered around transitive verbs, composed with a subject and an object. The results of the first experiment did not demonstrate any difference between the multiplicative model, which takes into account no syntactic information or word ordering, and our syntactically motivated categorical compositional model. By running the same experiment over a new data set, where the relations expressed by the verb have a higher arity than in the first, we hope to demonstrate that added structure leads to better results for our syntax-sensitive model</context>
<context position="118720" citStr="Grefenstette and Sadrzadeh (2011" startWordPosition="19842" endWordPosition="19845">a set, we rejected no Turker contributions, as the answers to the gold standard sentence pairs were aligned with our expectations. We attribute this to our adding the requirement that Turkers be based in the US or the UK and have English as the first language. 8 For example, the entry draw table eye depict would yield the sentences The table drew the eye and The table depicted the eye. 108 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning All of these models have been explained earlier, with the exception of Kronecker. We first presented this new addition in Grefenstette and Sadrzadeh (2011b), where we observed that the compact representation of a verb in the DisCoCat framework, under the assumptions presented in Section 4, can be viewed as dim(N) × dim(N) matrices in N ⊗ N. We considered alternatives to the algorithm presented earlier for the construction of such matrices, and were surprised by the results of the Kronecker method, wherein we replaced the matrix learned by our algorithm with the Kronecker product of the lexical semantic vectors for the verb. Further analysis performed since the publication of that paper can help to understand why this method might work. Using th</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Grefenstette, E. and M. Sadrzadeh. 2011b. Experimenting with transitive verbs in a DisCoCat. In Proceedings of the 2011 EMNLP Workshop on Geometric Models of Natural Language Semantics, pages 62–66, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Grefenstette</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
<author>B Coecke</author>
<author>S Pulman</author>
</authors>
<title>Concrete sentence spaces for compositional distributional models of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Ninth International Conference on Computational Semantics,</booktitle>
<pages>125--134</pages>
<location>Oxford.</location>
<contexts>
<context position="52254" citStr="Grefenstette et al. (2011)" startWordPosition="8314" endWordPosition="8317">sition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). 85 Computational Linguistics Volume 41, Number 1 The word vectors can be specified model-theoretically and the sentence space can be defined over Boolean values to obtain grammatically driven truth-theoretic semantics in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008). Some logical operators can be emulated in this setting, such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non-truth theoretic semantic model of sentence meaning for which logical operations have yet to be defined. Before explaining how this formalism works, in Section 3.3, we will introduce the notions of pregroup grammars in Section 3.1, and the required basics of category theory in Section 3.2. 3.1 Pregroup Grammars Presented by Lambek (1999, 2008) as a successor to his syntactic calculus (Lambek 1958), pregroup grammars are a class of categorial type grammars with pregroup algebras as semantics. Pregroups are particularly interesting within the context of this work because of their</context>
<context position="74518" citStr="Grefenstette et al. (2011)" startWordPosition="12306" endWordPosition="12309">isCoCat was developed in Clark, Coecke, and Sadrzadeh (2008), a full version was elaborated on in Coecke, Sadrzadeh, and Clark (2010), where, based on the developments of Preller and Sadrzadeh (2010), it was also exemplified how the vector space model may be instantiated in a truth theoretic setting where meanings of words were sets of their denotations and meanings of sentences were their truth values. A nontechnical description of this theoretical setting was presented in Clark (2013), where a plausibility truth-theoretic model for sentence spaces was worked out and exemplified. The work of Grefenstette et al. (2011) focused on a tangential branch and developed a toy example where neither words nor sentence spaces were Boolean. The applicability of the theoretical setting to a real empirical natural language processing task and data from a large scale corpus was demonstrated in Grefenstette and Sadrzadeh (2011a, 2011b). There, we presented a general algorithm to build vector representations for words with simple and complex types and the sentences containing them; then applied the algorithm to a disambiguation task performed on the British National Corpus (BNC). We also investigated the vector representat</context>
<context position="80798" citStr="Grefenstette et al. (2011)" startWordPosition="13334" endWordPosition="13337">ism between ST and N ® N, we will use the notations −−−−→ (ni, nj) and −→ni ® −→nj interchangeably, as both constitute appropriate ways of representing the basis elements of such a space. To propagate this distinction on the syntactic level, we define types sI and sT for intransitive and transitive sentences, respectively. In creating this distinction, we lost one of the most appealing features of the framework of Coecke, Sadrzadeh, and Clark (2010), namely, the result that all sentence vectors live in the same sentence space. A mathematical solution to this two-space problem was suggested in Grefenstette et al. (2011), and a variant of the models presented in this article permitting the non-problematic projection into a single sentence space (S ∼= N) has been presented by Grefenstette et al. (2013). Keeping this separation allows us to deal with both the transitive and intransitive cases in a simpler manner, and because the experiments in this article only compare intransitive sentences with intransitive sentences, and transitive sentences with transitive sentences, we will not address the issue of unification here. 4.2 Noun-Oriented Types While Lambek’s pregroup types presented in Lambek (2008) include a </context>
<context position="86263" citStr="Grefenstette et al. (2011)" startWordPosition="14222" endWordPosition="14225">ighting, aggressive, and mean, and relatively low counts for semantically different concepts such as passive, peaceful, and loves, then when we apply angry to dog the vector for the compound angry dog should contain some of the information found in the vector for dog. But this vector should also have higher values for the basis weights of fighting, aggressive, and mean, and correspondingly lower values for the basis weights of passive, peaceful, and loves. To turn this idea into a concrete algorithm for constructing the semantic representation for relations of any arity, as first presented in Grefenstette et al. (2011), we examine how we would deal with this for binary relations such as transitive verbs. If a transitive verb of semantic type N ⊗ ST ⊗ N is viewed as a function f : N × N → ST that expresses the extent to which the properties of subject and object are brought into relation by the verb, we learn the meaning of the verb by looking at what properties are brought into relation by its arguments in a corpus. Recall that the vector for a verb v, →−v ∈ N ⊗ ST ⊗ N, can be expressed as the weighted sum of its basis elements: �−→ v = cvijk−→ni ⊗ →−sj ⊗ nk−→ ijk We take the set of vectors for the subject </context>
<context position="137146" citStr="Grefenstette et al. (2011)" startWordPosition="22686" endWordPosition="22689">to expanding such data sets to ones where both sentences make sense and have opposite meanings, such as in the pair man bites dog and dog bites man. Furthermore, sentences all have the exact same sentence structure, and therefore words are aligned. Models that might indirectly benefit from this alignment, such as Kronecker, may have been given an advantage due to this, as opposed to models such as Categorical, which are designed to compare sentences of different length or syntactic structure, when resolving the difference between intransitive and transitive sentence spaces as has been done in Grefenstette et al. (2011). Future experiments should aim to do away with this automated alignment, and include sentence comparisons that would penalize models which do not leverage syntactic information. Furthermore, each of these experiments dealt with the comparison of a certain type of sentence (transitive, intransitive). This was convenient for our concrete categorical model, as we defined the sentence space differently based on the valency of the head verb. However, sentences with different sorts of verbs should be able to be directly compared. Not only do several models, both non-syntax sensitive (additive, mult</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, Clark, Coecke, Pulman, 2011</marker>
<rawString>Grefenstette, E., M. Sadrzadeh, S. Clark, B. Coecke, and S. Pulman. 2011. Concrete sentence spaces for compositional distributional models of meaning. In Proceedings of the Ninth International Conference on Computational Semantics, pages 125–134, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Use of syntactic context to produce term association lists for text retrieval.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>89--97</pages>
<location>Copenhagen.</location>
<contexts>
<context position="31345" citStr="Grefenstette (1992)" startWordPosition="4954" endWordPosition="4955">from Plate (1991) the importance of finding methods by which to project the tensored sentence vectors into a common space for direct comparison, as will be discussed further in Section 3. Syntactic Expressivity. An additional problem of a more conceptual nature is that using the tensor product as a composition operation simply preserves word order. As we discussed in Section 2.3.3, this is not enough on its own to model sentence meaning. We need to have some means by which to incorporate syntactic analysis into composition operations. Early work on including syntactic sensitivity into DSMs by Grefenstette (1992) suggests using crude syntactic relations to determine the frame in which the distributions for word vectors are collected from the corpus, thereby embedding syntactic information into the word vectors. This idea was already present in the work of Smolensky, who used sums of pairs of vector representations and their roles, obtained by taking their tensor products, to obtain a vector representation for a compound. The application of these ideas to DSMs was studied by Clark and Pulman (2007), who suggest instantiating the roles to dependency relations and using the distributional representations</context>
</contexts>
<marker>Grefenstette, 1992</marker>
<rawString>Grefenstette, G. 1992. Use of syntactic context to produce term association lists for text retrieval. In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 89–97, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery.</title>
<date>1994</date>
<contexts>
<context position="2612" citStr="Grefenstette 1994" startWordPosition="375" endWordPosition="376">Electronic Engineering and Computer Science, Queen Mary University of London, Mile End Road, London E1 4NS, United Kingdom. E-mail: mehrnoosh.sadrzadeh®qmul.ac.uk. † The work described in this article was performed while the authors were at the University of Oxford. 1 Support from EPSRC grant EP/J002607/1 is acknowledged. Submission received: 26 September 2012; revised submission received: 31 October 2013; accepted for publication: 5 April 2014. doi:10.1162/COLI a 00209 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 automated thesaurus building (Grefenstette 1994; Curran 2004) to automated essay marking (Landauer and Dumais 1997). The broader connection to information retrieval and its applications is also discussed by Manning, Raghavan, and Sch¨utze (2011). The success of DSMs in essentially word-based tasks such as thesaurus extraction and construction (Grefenstette 1994; Curran 2004) invites an investigation into how DSMs can be applied to NLP and information retrieval (IR) tasks revolving around larger units of text, using semantic representations for phrases, sentences, or documents, constructed from lemma vectors. However, the problem of composi</context>
<context position="6708" citStr="Grefenstette 1994" startWordPosition="976" endWordPosition="977">the meaning of a sentence is a function of the meaning of its parts (Frege 1892). These models relate to well-known and robust logical formalisms, hence offering a scalable theory of meaning that can be used to reason about language using logical tools of proof and inference. Distributional models are a more recent approach to semantic modeling, representing 72 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning the meaning of words as vectors learned empirically from corpora. They have found their way into real-world applications such as thesaurus extraction (Grefenstette 1994; Curran 2004) or automated essay marking (Landauer and Dumais 1997), and have connections to semantically motivated information retrieval (Manning, Raghavan, and Sch¨utze 2011). This two-sortedness of defining properties of meaning: “logical form” versus “contextual use,” has left the quest for “what is the foundational structure of meaning?”—a question initially the concern of solely linguists and philosophers of language—even more of a challenge. In this section, we present a short overview of the background to the work developed in this article by briefly describing formal and distribution</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, G. 1994. Explorations in automatic thesaurus discovery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>33--37</pages>
<location>Uppsala.</location>
<contexts>
<context position="17396" citStr="Guevara (2010)" startWordPosition="2684" endWordPosition="2685">namely, accounting for word order—by weighting word vectors according to their order of appearance in a sentence as follows: ab = α−→ −→ a + β−→ b −−−−−−−−−−−−→ not have the same representation as Wine drank John = α · wine + β · drank + γ · −−−→ John. The question of how to obtain weights and whether they are only used to reflect word order or can be extended to cover more subtle syntactic information is open, but it is not immediately clear how such weights may be obtained empirically and whether this mode of composition scales well with sentence length and increase in syntactic complexity. Guevara (2010) suggests using machine-learning methods such as partial least squares regression to determine the weights empirically, but states that this approach enjoys little success beyond minor composition such as adjective-noun or noun-verb composition, and that there is a dearth of metrics by which to evaluate such machine learning–based systems, stunting their growth and development. The second objection states that vector addition leads to increase in ambiguity as we construct sentences, rather than decrease in ambiguity as we would expect from giving words a context; and for this reason Mitchell a</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Guevara, E. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of the 2010 Workshop on Geometrical Models of Natural Language Semantics, pages 33–37, Uppsala.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>Mathematical structures of language.</title>
<date>1968</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="1385" citStr="Harris (1968)" startWordPosition="194" endWordPosition="195"> developing algorithms to construct tensors and linear maps and instantiate the abstract parameters using empirical data. We then evaluate our concrete models against several experiments, both existing and new, based on measuring how well models align with human judgments in a paraphrase detection task. Our results show the implementation of this general abstract framework to perform on par with or outperform other leading models in these experiments.1 1. Introduction The distributional approach to the semantic modeling of natural language, inspired by the notion—presented by Firth (1957) and Harris (1968)—that the meaning of a word is tightly related to its context of use, has grown in popularity as a method of semantic representation. It draws from the frequent use of vector-based document models in information retrieval, modeling the meaning of words as vectors based on the distribution of co-occurring terms within the context of a word. Using various vector similarity metrics as a measure of semantic similarity, these distributional semantic models (DSMs) are used for a variety of NLP tasks, from * DeepMind Technologies Ltd, 5 New Street Square, London EC4A 3 TW. E-mail: etg®google.com. ** </context>
<context position="10646" citStr="Harris (1968)" startWordPosition="1581" endWordPosition="1582">ell on language tasks where the notion of similarity is not strictly based on truth conditions, such as document retrieval, topic classification, and so forth. Furthermore, an underlying domain of objects and a valuation function must be provided, as with any logic, leaving open the question of how we might learn the meaning of language using such a model, rather than just use it. 2.2 Distributional Semantics A popular way of representing the meaning of words in lexical semantics is as distributions in a high-dimensional vector space. This approach is based on the distributional hypothesis of Harris (1968), who postulated that the meaning of a word was dictated by the context of its use. The more famous dictum stating this hypothesis is the statement of Firth (1957) that “You shall know a word by the company it keeps.” This view of semantics has furthermore been associated (Grefenstette 2009; Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein (presented in Wittgenstein 1953), who stated that language meaning was equivalent to its real world use. Practically speaking, the meaning of a word can be learned from a corpus by looking at what other words occur with it </context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Harris, Z. S. 1968. Mathematical structures of language. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="58872" citStr="Hockenmaier 2003" startWordPosition="9491" endWordPosition="9492"> 1 we will use in our categorical formalism are derived from CCG types, as we explain in the following. Pregroup Grammars and Other Categorial Grammars. Pregroup grammars, in contrast with other categorial grammars such as CCG, do not yet have a large set of tools for parsing available. If quick implementation of the formalism described later in this paper is required, it would be useful to be able to leverage the mature state of parsing tools available for other categorial grammars, such as the Clark and Curran (2007) statistical CCG parser, as well as Hockenmaier’s CCG lexicon and treebank (Hockenmaier 2003; Hockenmaier and Steedman 2007). In other words, is there any way we can translate at least some subset of CCG types into pregroup types? There are some theoretical obstacles to consider first: Pregroup grammars and CCG are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be weakly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used in Clark and Curran’s parser, or Hockenmaier’s lexicon and treebank, we may be in luck: Fowle</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Hockenmaier, Julia. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGBank: A corpus of CCG derivations and dependency structures extracted from the Penn treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="58904" citStr="Hockenmaier and Steedman 2007" startWordPosition="9493" endWordPosition="9496">our categorical formalism are derived from CCG types, as we explain in the following. Pregroup Grammars and Other Categorial Grammars. Pregroup grammars, in contrast with other categorial grammars such as CCG, do not yet have a large set of tools for parsing available. If quick implementation of the formalism described later in this paper is required, it would be useful to be able to leverage the mature state of parsing tools available for other categorial grammars, such as the Clark and Curran (2007) statistical CCG parser, as well as Hockenmaier’s CCG lexicon and treebank (Hockenmaier 2003; Hockenmaier and Steedman 2007). In other words, is there any way we can translate at least some subset of CCG types into pregroup types? There are some theoretical obstacles to consider first: Pregroup grammars and CCG are not equivalent. Buszkowski (2001) shows pregroup grammars to be equivalent to context-free grammars, whereas Joshi, Vijay-Shanker, and Weir (1989) show CCG to be weakly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used in Clark and Curran’s parser, or Hockenmaier’s lexicon and treebank, we may be in luck: Fowler and Penn (2010) prove that som</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Hockenmaier, Julia and Mark Steedman. 2007. CCGBank: A corpus of CCG derivations and dependency structures extracted from the Penn treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The convergence of mildly context-sensitive grammar formalisms. Working paper,</title>
<date>1989</date>
<institution>University of Pennsylvania, School of Engineering and Applied Science, Dept. of Computer and Information Science.</institution>
<marker>Joshi, Vijay-Shanker, Weir, 1989</marker>
<rawString>Joshi, A. K., K. Vijay-Shanker, and D. J. Weir. 1989. The convergence of mildly context-sensitive grammar formalisms. Working paper, University of Pennsylvania, School of Engineering and Applied Science, Dept. of Computer and Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>The mathematics of sentence structure.</title>
<date>1958</date>
<journal>The American Mathematical Monthly,</journal>
<volume>65</volume>
<issue>3</issue>
<contexts>
<context position="52670" citStr="Lambek 1958" startWordPosition="8382" endWordPosition="8383"> such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non-truth theoretic semantic model of sentence meaning for which logical operations have yet to be defined. Before explaining how this formalism works, in Section 3.3, we will introduce the notions of pregroup grammars in Section 3.1, and the required basics of category theory in Section 3.2. 3.1 Pregroup Grammars Presented by Lambek (1999, 2008) as a successor to his syntactic calculus (Lambek 1958), pregroup grammars are a class of categorial type grammars with pregroup algebras as semantics. Pregroups are particularly interesting within the context of this work because of their well-studied algebraic structure, which can trivially be mapped onto the structure of the category of vector spaces, as will be discussed subsequently. Logically speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which the tensor and its dual par coincide; this logic is sometimes referred to as Bi-Compact Linear Logic (Lambek 1999). The formalism works alongside the general guideline</context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>Lambek, J. 1958. The mathematics of sentence structure. The American Mathematical Monthly, 65(3):154–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>Type grammar revisited. Logical Aspects of Computational Linguistics,</title>
<date>1999</date>
<pages>1--27</pages>
<contexts>
<context position="52608" citStr="Lambek (1999" startWordPosition="8372" endWordPosition="8373">2008). Some logical operators can be emulated in this setting, such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non-truth theoretic semantic model of sentence meaning for which logical operations have yet to be defined. Before explaining how this formalism works, in Section 3.3, we will introduce the notions of pregroup grammars in Section 3.1, and the required basics of category theory in Section 3.2. 3.1 Pregroup Grammars Presented by Lambek (1999, 2008) as a successor to his syntactic calculus (Lambek 1958), pregroup grammars are a class of categorial type grammars with pregroup algebras as semantics. Pregroups are particularly interesting within the context of this work because of their well-studied algebraic structure, which can trivially be mapped onto the structure of the category of vector spaces, as will be discussed subsequently. Logically speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which the tensor and its dual par coincide; this logic is sometimes referred to as Bi-Compact Linear Logic (Lam</context>
<context position="59875" citStr="Lambek (1999)" startWordPosition="9647" endWordPosition="9648">kly equivalent to more expressive mildly context-sensitive grammars. However, if our goal is to exploit the CCG used in Clark and Curran’s parser, or Hockenmaier’s lexicon and treebank, we may be in luck: Fowler and Penn (2010) prove that some CCGs, such as those used in the aforementioned tools, are strongly context-free and thus expressively equivalent to pregroup grammars. In order to be able to apply the parsing tools for CCGs to our setting, we use a translation mechanism from CCG types to pregroup types based on the Lambek-calculus-to-pregroup-grammar translation originally presented in Lambek (1999). In this mechanism, each atomic CCG type X is assigned a unique pregroup type x; for any X/Y in CCG we have xyl in the pregroup grammar; and for any X\Y in CCG we have yrx in pregroup grammar. Therefore, by assigning NP to n and S to s we could, for example, translate the CCG transitive verb type (S\NP)/NP into the pregroup type nrsnl, which corresponds to the pregroup type we used for transitive verbs in Section 3.1. Wherever type replacement (e.g., N → NP) is allowed in CCG we set an ordering relation in the pregroup grammar (e.g., n¯ ≤ n, where n¯ is the pregroup type associated with N). B</context>
</contexts>
<marker>Lambek, 1999</marker>
<rawString>Lambek, J. 1999. Type grammar revisited. Logical Aspects of Computational Linguistics, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>From word to sentence. A computational algebraic approach to grammar.</title>
<date>2008</date>
<location>Milan, Polimetrica.</location>
<contexts>
<context position="57545" citStr="Lambek (2008)" startWordPosition="9282" endWordPosition="9283">llows: n(nrsnl)n -+ s We see that the transitive verb has combined with the subject and object to reduce to a sentence. Because the combination of the types of the words in the string John loves cake reduces to s, we say that this string of words is a grammatical sentence. As for more examples, we recall that intransitive verbs can be given the type nrs such that John sleeps would be analyzed in terms of the reduction n(nrs) -+ s. Adjectives can be given the type nnl such that red round rubber ball would be analyzed by (nnl)(nnl)(nnl)n -+ n. And so on and so forth for other syntactic classes. Lambek (2008) presents the details of a slightly more complex pregroup grammar with a richer set of types than presented here. This grammar is hand-constructed and iteratively extended by expanding the type assignments as more sophisticated grammatical constructions are discussed. No general mechanism is proposed to cover all such types of assignments for larger fragments (e.g., as seen in empirical data). Pregroup grammars have been proven to be learnable by B´echet, Foret, and Tellier (2007), who also discuss the difficulty of this task and the nontractability of the procedure. Because of these constrain</context>
<context position="81387" citStr="Lambek (2008)" startWordPosition="13424" endWordPosition="13425">efenstette et al. (2011), and a variant of the models presented in this article permitting the non-problematic projection into a single sentence space (S ∼= N) has been presented by Grefenstette et al. (2013). Keeping this separation allows us to deal with both the transitive and intransitive cases in a simpler manner, and because the experiments in this article only compare intransitive sentences with intransitive sentences, and transitive sentences with transitive sentences, we will not address the issue of unification here. 4.2 Noun-Oriented Types While Lambek’s pregroup types presented in Lambek (2008) include a rich array of basic types and hand-designed compound types in order to capture specific grammatic properties, for the sake of simplicity we will use a simpler set of grammatical types for experimental purposes, similar to some common types found in the CCG-bank (Steedman 2001). We assign a basic pregroup type n for all nouns, with an associated vector space N for their semantic representations. Furthermore, we will treat noun-phrases as nouns, assigning to them the same pregroup type and semantic space. CCG treats intransitive verbs as functions NP\S that consume a noun phrase and r</context>
</contexts>
<marker>Lambek, 2008</marker>
<rawString>Lambek, J. 2008. From word to sentence. A computational algebraic approach to grammar. Milan, Polimetrica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>Compact Monoidal Categories from Linguistics to Physics,</title>
<date>2010</date>
<pages>451--469</pages>
<contexts>
<context position="62901" citStr="Lambek (2010)" startWordPosition="10134" endWordPosition="10135">tative distributional model (Clark, Coecke, and Sadrzadeh 2008; Coecke, Sadrzadeh, and Clark 2010). Moreover, the unifying categorical structures at work here were inspired by the ones used in the foundations of physics and the modeling of quantum information flow, as presented in Abramsky and Coecke (2004), where they relate the logical structure of quantum protocols to their state-based vector spaces data. The connection between the mathematics used for this branch of physics and those potentially useful for linguistic modeling has also been noted by several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004). In this section, we will briefly examine the basics of category theory, monoidal categories, and compact closed categories. The focus will be on defining enough basic concepts to proceed rather than provide a full-blown tutorial on category theory and the modeling of information flow, as several excellent sources already cover both aspects (e.g., Mac Lane 1998; Walters 1991; Coecke and Paquette 2011). A categoriesin-a-nutshell crash course is also provided in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010). The Basics of Category Theory.</context>
</contexts>
<marker>Lambek, 2010</marker>
<rawString>Lambek, J., 2010. Compact Monoidal Categories from Linguistics to Physics, pages 451–469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<note>Psychological review.</note>
<contexts>
<context position="2680" citStr="Landauer and Dumais 1997" startWordPosition="383" endWordPosition="386">ersity of London, Mile End Road, London E1 4NS, United Kingdom. E-mail: mehrnoosh.sadrzadeh®qmul.ac.uk. † The work described in this article was performed while the authors were at the University of Oxford. 1 Support from EPSRC grant EP/J002607/1 is acknowledged. Submission received: 26 September 2012; revised submission received: 31 October 2013; accepted for publication: 5 April 2014. doi:10.1162/COLI a 00209 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 automated thesaurus building (Grefenstette 1994; Curran 2004) to automated essay marking (Landauer and Dumais 1997). The broader connection to information retrieval and its applications is also discussed by Manning, Raghavan, and Sch¨utze (2011). The success of DSMs in essentially word-based tasks such as thesaurus extraction and construction (Grefenstette 1994; Curran 2004) invites an investigation into how DSMs can be applied to NLP and information retrieval (IR) tasks revolving around larger units of text, using semantic representations for phrases, sentences, or documents, constructed from lemma vectors. However, the problem of compositionality in DSMs—of how to go from word to sentence and beyond—has </context>
<context position="6776" citStr="Landauer and Dumais 1997" startWordPosition="984" endWordPosition="987">s parts (Frege 1892). These models relate to well-known and robust logical formalisms, hence offering a scalable theory of meaning that can be used to reason about language using logical tools of proof and inference. Distributional models are a more recent approach to semantic modeling, representing 72 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning the meaning of words as vectors learned empirically from corpora. They have found their way into real-world applications such as thesaurus extraction (Grefenstette 1994; Curran 2004) or automated essay marking (Landauer and Dumais 1997), and have connections to semantically motivated information retrieval (Manning, Raghavan, and Sch¨utze 2011). This two-sortedness of defining properties of meaning: “logical form” versus “contextual use,” has left the quest for “what is the foundational structure of meaning?”—a question initially the concern of solely linguists and philosophers of language—even more of a challenge. In this section, we present a short overview of the background to the work developed in this article by briefly describing formal and distributional approaches to natural language semantics, and providing a non-exh</context>
<context position="15778" citStr="Landauer and Dumais (1997)" startWordPosition="2426" endWordPosition="2429">antages, and their limitations. 2.3.1 Additive Models. The simplest composition operation that comes to mind is straightforward vector addition, such that: ab = −→ −→ a + −→b Conceptually speaking, if we view word vectors as semantic information distributed across a set of properties associated with basis vectors, using vector addition as a semantic composition operation states that the information of a set of lemmas in a sentence is simply the sum of the information of the individual lemmas. Although crude, this approach is computationally cheap, and appears sufficient for certain NLP tasks: Landauer and Dumais (1997) show it to be sufficient for automated essay marking tasks, and Grefenstette (2009) shows it to perform better than a collection of other simple similarity metrics for summarization, sentence paraphrase, and document paraphrase detection tasks. However, there are two principal objections to additive models of composition: first, −−−−−−−−−−−−→ −−→= vector addition is commutative, therefore, John drank wine = John + drank + wine ✓�i (cai )2 Ei (cbi )2 75 Computational Linguistics Volume 41, Number 1 −−−−−−−−−−−−→ Wine drank John, and thus vector addition ignores syntactic structure completely; </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. K. and S. T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mac Lane</author>
<author>S</author>
</authors>
<title>Categories for the Working Mathematician.</title>
<date>1998</date>
<publisher>Springer Verlag.</publisher>
<marker>Lane, S, 1998</marker>
<rawString>Mac Lane, S. 1998. Categories for the Working Mathematician. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to information retrieval.</title>
<date>2011</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY.</location>
<marker>Manning, Raghavan, Sch¨utze, 2011</marker>
<rawString>Manning, C. D., P. Raghavan, and H. Sch¨utze. 2011. Introduction to information retrieval. Cambridge University Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>03</issue>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Minnen, G., J. Carroll, and D. Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(03):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Vectorbased models of semantic composition.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>8</volume>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="16729" citStr="Mitchell and Lapata (2008)" startWordPosition="2565" endWordPosition="2568">first, −−−−−−−−−−−−→ −−→= vector addition is commutative, therefore, John drank wine = John + drank + wine ✓�i (cai )2 Ei (cbi )2 75 Computational Linguistics Volume 41, Number 1 −−−−−−−−−−−−→ Wine drank John, and thus vector addition ignores syntactic structure completely; and second, vector addition sums the information contained in the vectors, effectively jumbling the meaning of words together as sentence length grows. The first objection is problematic, as the syntactic insensitivity of additive models leads them to equate the representation of sentences with patently different meanings. Mitchell and Lapata (2008) propose to add some degree of syntactic sensitivity—namely, accounting for word order—by weighting word vectors according to their order of appearance in a sentence as follows: ab = α−→ −→ a + β−→ b −−−−−−−−−−−−→ not have the same representation as Wine drank John = α · wine + β · drank + γ · −−−→ John. The question of how to obtain weights and whether they are only used to reflect word order or can be extended to cover more subtle syntactic information is open, but it is not immediately clear how such weights may be obtained empirically and whether this mode of composition scales well with s</context>
<context position="18012" citStr="Mitchell and Lapata (2008)" startWordPosition="2773" endWordPosition="2776">ara (2010) suggests using machine-learning methods such as partial least squares regression to determine the weights empirically, but states that this approach enjoys little success beyond minor composition such as adjective-noun or noun-verb composition, and that there is a dearth of metrics by which to evaluate such machine learning–based systems, stunting their growth and development. The second objection states that vector addition leads to increase in ambiguity as we construct sentences, rather than decrease in ambiguity as we would expect from giving words a context; and for this reason Mitchell and Lapata (2008) suggest replacing additive models with multiplicative models as discussed in Section 2.3.2, or combining them with multiplicative models to form mixture models as discussed in Section 2.3.3. 2.3.2 Multiplicative Models. The multiplicative model of Mitchell and Lapata (2008) is an attempt to solve the ambiguity problem discussed in Section 2.3.1 and provide implicit disambiguation during composition. The composition operation proposed is the component-wise multiplication (0) of two vectors: Vectors are expressed as the weighted sum of their basis vectors, and the weight of the basis vectors of</context>
<context position="21095" citStr="Mitchell and Lapata (2008)" startWordPosition="3290" endWordPosition="3293"> preserved, but never increased). Hence, as the number of vectors to be composed grows, the number of non-zero basis weights of the product vector stays the same or—more realistically—decreases. Therefore, for any composition of the form ��������� a1 . . . ai . . . an = ai O ... O a O ... O n, if for any i, a is orthogonal to ������� a1 . . . ai−1 then ��������� a1 . . . ai . . . an = 0 . It follows that purely multiplicative models alone are not apt as a single mode of composition beyond nounverb (or adjective-noun) composition operations. One solution to this second problem not discussed by Mitchell and Lapata (2008) would be to introduce some smoothing factor s E R+ for point-wise multiplication such that a O ��b = �i (ci + s)(c�i + s)n , ensuring that information is never completely filtered out. Seeing how the problem of syntactic insensitivity still stands in the way of full-blown compositionality for multiplicative models, we leave it to those interested in salvaging purely multiplicative models to determine whether some suitable value of s can be determined. 2.3.3 Mixture Models. The problems faced by multiplicative models presented in Section 2.3.2 are acknowledged in passing by Mitchell and Lapata</context>
<context position="23831" citStr="Mitchell and Lapata (2008)" startWordPosition="3744" endWordPosition="3747">ns) to introduce a more complex syntactic element into vector composition, but it is clear that this alone is not a solution, as the weight for nouns dog and man would be the same, allowing for the same commutative degeneracy observed in non-weighted additive models, in which −−−−−−−−−−−−−−→ the dog bit the man = −−−−−−−−−−−−−−→ the man bit the dog. Introducing a mixture of weighting systems accounting for both word order and syntactic roles may be a solution, but it is not only ad hoc but also arguably only partially reflects the syntactic structure of the sentence. The third problem is that Mitchell and Lapata (2008) show that in practice, although mixture models perform better at verb disambiguation tasks than additive models and weighted additive models, they perform equivalently to purely multiplicative models with the added burden of requiring parametric optimization of the scalar weights. Therefore, whereas mixture models aim to take the best of additive and multiplicative models while avoiding their problems, they are only partly successful in achieving the latter goal, and demonstrably do little better in achieving the former. 2.3.4 Tensor-Based Models. From Sections 2.3.1–2.3.3 we observe that the</context>
<context position="35652" citStr="Mitchell and Lapata (2008)" startWordPosition="5689" endWordPosition="5692"> − {r}) where a&apos;, b&apos; are the updated word meanings, and O is whichever vector composition (addition, component-wise multiplication) we wish to use. The word vectors in the triplets are effectively filtered by combination with the lemma which the word they are being composed with expects to bear relation r to, and this relation between the composed words a and b is considered to be used and hence removed from the domain of the selectional preference functions used in composition. This mechanism is therefore a more sophisticated version of the compositional disambiguation mechanism discussed by Mitchell and Lapata (2008) in that the combination of words filters the meaning of the original vectors that may be ambiguous (e.g., if we have one vector for bank); but contrary to Mitchell and Lapata (2008), the information of the original vectors is modified but essentially preserved, allowing for further combination with other terms, rather than directly producing a joint vector for the composed words. The added fact that R and R−1 are partial functions associated with specific lemmas forces grammaticality during composition, since if a holds a dependency relation r to b which it never expects to hold (for example </context>
<context position="51285" citStr="Mitchell and Lapata (2008)" startWordPosition="8167" endWordPosition="8170">riginal formalisms belong in different branches of mathematics. In this context, it has enabled us to relate syntactic types and reductions to vector spaces and linear maps and obtain a mechanism by which syntactic analysis guides semantic composition operations. This pairing of syntactic analysis and semantic composition ensures both that grammaticality restrictions are in place as in the model of Erk and Pad´o (2008) and syntactically driven semantic composition in the form of inner-products provide the implicit disambiguation features of the compositional models of Erk and Pad´o (2008) and Mitchell and Lapata (2008). The composition mechanism also involves the projection of tensored vectors into a common semantic space without the need for full representation of the tensored vectors in a manner similar to Plate (1991), without restriction to the nature of the vector spaces it can be applied to. This avoids the problems faced by other tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). 85 Computational Linguistics Volume 41, Number 1 The word vectors can be specified model-theoretically and the sentence space can be defined over Boolean values to obtain grammatically </context>
<context position="83190" citStr="Mitchell and Lapata (2008)" startWordPosition="13714" endWordPosition="13717">c space N ® N. With the provision of a learning procedure for vectors in these semantic spaces, we can use these types to construct sentence vector representations for simple intransitive verb–based and transitive verb–based sentences, with and without adjectives applied to subjects and objects. 4.3 Learning Procedures To begin, we construct the semantic space N for all nouns in our lexicon (typically limited by the words available in the corpus used). Any distributional semantic model can be used for this stage, such as those presented in Curran (2004), or the lexical semantic models used by Mitchell and Lapata (2008). It seems reasonable to assume that higher quality lexical semantic vectors—as measured by metrics such as the WordSim353 test 95 Computational Linguistics Volume 41, Number 1 of Finkelstein et al. (2001)—will produce better relational vectors from the procedure designed subsequently. We will not test the hypothesis here, but note that it is an underlying assumption in most of the current literature on the subject (Erk and Pad´o 2008; Mitchell and Lapata 2008; Baroni and Zamparelli 2010). Building upon the foundation of the thus constructed noun vectors, we construct semantic representations </context>
<context position="101262" citStr="Mitchell and Lapata (2008)" startWordPosition="16973" endWordPosition="16976">e data sets designed to evaluate how well word-sense disambiguation occurs as a byproduct of composition. We begin by describing the first data set, based around noun-intransitive verb phrases, in Section 5.1. In Section 5.2, we present a data set based around short transitive-verb phrases (a transitive verb with subject and object). In Section 5.3, we discuss a new data set, based around short transitive-verb phrases where the subject and object are qualified by adjectives. We leave discussion of these results for Section 6. 5.1 First Experiment This first experiment, originally presented in Mitchell and Lapata (2008), evaluates the degree to which an ambiguous intransitive verb (e.g., draws) is disambiguated by combination with its subject. Data set Description. The data set4 comprises 120 pairs of intransitive sentences, each of the form NOUN VERB. These sentence pairs are generated according to the following 4 Available athttp://homepages.inf.ed.ac.uk/s0453356/results. 102 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning procedure, which will be the basis for the construction of the other data sets discussed subsequently: 1. A number of ambiguous intransitive verbs (1</context>
<context position="102937" citStr="Mitchell and Lapata (2008)" startWordPosition="17261" endWordPosition="17264">ng nouns N1 and N2 are picked from the corpus, one for each synonym of V. For example, if V is glow and the synonyms V1 and V2 are beam and burn, we might choose face as N1, because a face glowing and a face beaming mean roughly the same thing; and fire as N2, because a fire glowing and a fire burning mean roughly the same thing. 4. By combining the nouns with the verb pairs, we form two high similarity triplets (V, V1, N1) and (V, V2, N2), and two low similarity triplets (V, V1, N2) and (V, V2, N1). The last two steps can be repeated to form more than four triplets per pair of verb pairs. In Mitchell and Lapata (2008), eight triplets are generated for each pair of verb pairs, obtaining a total of 120 triplets from the 15 original verbs. Each triplet, along with its HIGH or LOW classification (based on the choice of noun for the verb pair) is an entry in the data set, and can be read as a pair of sentences: (V, Vi, N) translates into the intransitive sentences N V and N Vi. Finally, the data set is presented, without the HIGH/LOW ratings, to human annotators. These annotators are asked to rate the similarity of meaning of the pairs of sentences in each entry on a scale of 1 (low similarity) to 7 (high simil</context>
<context position="106113" citStr="Mitchell and Lapata (2008)" startWordPosition="17796" endWordPosition="17799">an a model producing model scores on a larger scale (e.g., between 0 and 1) but with less perfect rank correlation. If we wished to then use the former model in a task requiring some greater degree of numerical separation (let us say 0 for non-similar sentences and 1 for completely similar sentences), we could simply renormalize the model scores to fit the scale. By eschewing score normalization as an evaluation factor, we minimize the risk of erroneously ranking one model over another. Finally, in addition to computing the rank alignment coefficient between model scores and annotator scores, Mitchell and Lapata (2008) calculate the mean model scores for entries labeled HIGH, and for entries labeled LOW. This information is reported in their paper as additional means for model comparison. However, for the same reason we considered Spearman’s p to be a fair means of model comparison— namely, in that it required no model score normalization procedure and thus was less likely to introduce error by adding such a degree of freedom—we consider the HIGH/LOW means to be inadequate grounds for comparison, precisely because it requires normalized model scores for comparison to be meaningful. As such, we will not incl</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Mitchell, J. and M. Lapata. 2008. Vectorbased models of semantic composition. In Proceedings of ACL, volume 8, pages 236–244, Columbus, OH. Computational Linguistics Volume 41, Number 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mitchell</author>
<author>M Lapata</author>
</authors>
<title>Composition in Distributional Models of Semantics.</title>
<date>2010</date>
<journal>Cognitive Science</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="112697" citStr="Mitchell and Lapata (2010)" startWordPosition="18868" endWordPosition="18871">s score aligns with other annotator scores, using Spearman’s ρ. Results. The results5 of the first experiment are shown in Table 4. As expected from the fact that Multiply and Categorical differ only in how the verb vector is learned, the results of these two models are virtually identical, outperforming both baselines and the Additive model by a significant margin. However, the distance from these models to the upper bound is even greater, demonstrating that there is still a lot of progress to be made. 5.2 Second Experiment The first experiment was followed by a second similar experiment, in Mitchell and Lapata (2010), covering different sorts of composition operations for binary combinations of syntactic types (adjective-noun, noun-noun, verb-object). Such further experiments are interesting, but rather than continue down this binary road, we now turn to the development of our second experiment, involving sentences with larger syntactic structures, to examine how well various compositional models cope with more complex syntactic and semantic relations. 5 The results were state of the art when the experiments were run in 2011. The binary composition data set used here are popular; now there are a host of n</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Mitchell, J. and M. Lapata. 2010. Composition in Distributional Models of Semantics. Cognitive Science 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>English as a Formal Language. In</title>
<date>1974</date>
<editor>R. H. Thomason, editor,</editor>
<contexts>
<context position="51949" citStr="Montague (1974)" startWordPosition="8270" endWordPosition="8271">jection of tensored vectors into a common semantic space without the need for full representation of the tensored vectors in a manner similar to Plate (1991), without restriction to the nature of the vector spaces it can be applied to. This avoids the problems faced by other tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). 85 Computational Linguistics Volume 41, Number 1 The word vectors can be specified model-theoretically and the sentence space can be defined over Boolean values to obtain grammatically driven truth-theoretic semantics in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008). Some logical operators can be emulated in this setting, such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non-truth theoretic semantic model of sentence meaning for which logical operations have yet to be defined. Before explaining how this formalism works, in Section 3.3, we will introduce the notions of pregroup grammars in Section 3.1, and the required basics of category theory in S</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Montague, R. 1974. English as a Formal Language. In R. H. Thomason, editor, Formal Semantics: The Essential Readings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<title>Categorial type logics.</title>
<date>1997</date>
<booktitle>Handbook of Logic and Language.</booktitle>
<pages>93--177</pages>
<editor>In H. van Ditmarsch and L. S. Moss, editors,</editor>
<publisher>Elsevier,</publisher>
<contexts>
<context position="61112" citStr="Moortgat (1997)" startWordPosition="9863" endWordPosition="9864">kward slash “operators” in CCG are not associative whereas monoid multiplication in pregroups is, it is evident that some information is lost during the translation process. But because the translation we need is one-way, we may ignore this problem and use CCG parsing tools to obtain pregroup parses. Another concern lies with CCG’s crossed composition and substitution rules. The translations of these rules do not in general hold in a pregroup; this is not a surprise as pregroups are a simplification of the Lambek Calculus and these rules did not hold in the Lambek Calculus either, as shown in Moortgat (1997), for example. However, for the phenomena modeled in this paper, the CCG rules without the backward cross rules will suffice. In general for the case of English, one can avoid the use of these rules by overloading the lexicon and using additional categories. To deal with languages that have cross dependancies, such as Dutch, various solutions have been suggested (e.g., see Genkin, Francez, and Kaminski 2010; Preller 2010). 3.2 Categories Category theory is a branch of pure mathematics that allows for a general and uniform formulation of various different mathematical formalisms in terms of the</context>
</contexts>
<marker>Moortgat, 1997</marker>
<rawString>Moortgat, M. 1997. Categorial type logics. In H. van Ditmarsch and L. S. Moss, editors, Handbook of Logic and Language. Elsevier, pages 93–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Pad´o, Sebastian and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pham</author>
<author>R Bernardi</author>
<author>Y-Z Zhang</author>
<author>M Baroni</author>
</authors>
<title>Sentence paraphrase detection: When determiners and word order make the difference.</title>
<date>2013</date>
<booktitle>In Proceedings of the Towards a Formal Distributional Semantics Workshop at IWCS 2013,</booktitle>
<pages>21--29</pages>
<location>Potsdam.</location>
<contexts>
<context position="136294" citStr="Pham et al. (2013)" startWordPosition="22547" endWordPosition="22550"> way of testing word order while expanding the data set was suggested by Turney (2012), who for every entry in a phrase similarity test such as Mitchell and Lapata’s, discussed herein, creates a new entry where one of the sentences has the order of its words reversed, and is assigned an artificial annotator score of 1 (the minimum). This is an interesting approach, but we would prefer to see such reversals designed into the data set and seen by annotators, for instance, 114 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning in the recent data set presented in Pham et al. (2013). This data set has entries such as guitar played man and man played guitar, where the subjects and objects of verbs are indeed reversed but the two sentences do not necessarily express opposite meanings; we will be looking into expanding such data sets to ones where both sentences make sense and have opposite meanings, such as in the pair man bites dog and dog bites man. Furthermore, sentences all have the exact same sentence structure, and therefore words are aligned. Models that might indirectly benefit from this alignment, such as Kronecker, may have been given an advantage due to this, as</context>
</contexts>
<marker>Pham, Bernardi, Zhang, Baroni, 2013</marker>
<rawString>Pham, N., R. Bernardi, Y.-Z. Zhang, and M. Baroni. 2013. Sentence paraphrase detection: When determiners and word order make the difference. In Proceedings of the Towards a Formal Distributional Semantics Workshop at IWCS 2013, pages 21–29, Potsdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A Plate</author>
</authors>
<title>Holographic reduced representations: Convolution algebra for compositional distributed representations.</title>
<date>1991</date>
<booktitle>In Proceedings of the 12th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>30--35</pages>
<location>Hyderabad.</location>
<contexts>
<context position="28092" citStr="Plate (1991)" startWordPosition="4427" endWordPosition="4428">actic classes), then sentences of different syntactic structure live in different vector spaces, and hence cannot be compared directly using inner product or cosine measures, leaving us with no obvious mode of semantic comparison for sentence vectors. If any model wishes to use tensor products in composition operations, it must find some way of reducing the dimensionality of product vectors to some common vector space so that they may be directly compared. One notable method by which these dimensionality problems can be solved in general are the holographic reduced representations proposed by Plate (1991). The product vector of two vectors is projected into a space of smaller dimensionality by circular convolution to produce a trace vector. The circular correlation of the trace vector and one of the original vectors produces a noisy version of the other original vector. The noisy vector can be used to recover the clean original vector by comparing it with a predefined set of candidates (e.g., the set of word vectors if our original vectors are word meanings). Traces can be summed to form new traces effectively containing several vector pairs from which original vectors can be recovered. Using </context>
<context position="30743" citStr="Plate (1991)" startWordPosition="4859" endWordPosition="4860">onsiders possible application areas where they may be of use, although once again these mostly involve noun-verb and adjectivenoun compositionality rather than full blown sentence vector construction. We retain 2 Source:http://www.oxforddictionaries.com/page/howmanywords. 3 At four bytes per integer, and one integer per basis vector weight, the vector for John loves Mary would require roughly (170, 000 · 4)3 ≈ 280 petabytes of storage, which is over ten times the data Google processes on a daily basis according to Dean and Ghemawat (2008). 79 Computational Linguistics Volume 41, Number 1 from Plate (1991) the importance of finding methods by which to project the tensored sentence vectors into a common space for direct comparison, as will be discussed further in Section 3. Syntactic Expressivity. An additional problem of a more conceptual nature is that using the tensor product as a composition operation simply preserves word order. As we discussed in Section 2.3.3, this is not enough on its own to model sentence meaning. We need to have some means by which to incorporate syntactic analysis into composition operations. Early work on including syntactic sensitivity into DSMs by Grefenstette (199</context>
<context position="51491" citStr="Plate (1991)" startWordPosition="8201" endWordPosition="8202">sis guides semantic composition operations. This pairing of syntactic analysis and semantic composition ensures both that grammaticality restrictions are in place as in the model of Erk and Pad´o (2008) and syntactically driven semantic composition in the form of inner-products provide the implicit disambiguation features of the compositional models of Erk and Pad´o (2008) and Mitchell and Lapata (2008). The composition mechanism also involves the projection of tensored vectors into a common semantic space without the need for full representation of the tensored vectors in a manner similar to Plate (1991), without restriction to the nature of the vector spaces it can be applied to. This avoids the problems faced by other tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). 85 Computational Linguistics Volume 41, Number 1 The word vectors can be specified model-theoretically and the sentence space can be defined over Boolean values to obtain grammatically driven truth-theoretic semantics in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008). Some logical operators can be emulated in this setting, such as using swap matrices for </context>
</contexts>
<marker>Plate, 1991</marker>
<rawString>Plate, T. A. 1991. Holographic reduced representations: Convolution algebra for compositional distributed representations. In Proceedings of the 12th International Joint Conference on Artificial Intelligence, pages 30–35, Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Preller</author>
</authors>
<title>Polynomial pregroup grammars parse context sensitive languages. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--483</pages>
<contexts>
<context position="61537" citStr="Preller 2010" startWordPosition="9931" endWordPosition="9932"> hold in a pregroup; this is not a surprise as pregroups are a simplification of the Lambek Calculus and these rules did not hold in the Lambek Calculus either, as shown in Moortgat (1997), for example. However, for the phenomena modeled in this paper, the CCG rules without the backward cross rules will suffice. In general for the case of English, one can avoid the use of these rules by overloading the lexicon and using additional categories. To deal with languages that have cross dependancies, such as Dutch, various solutions have been suggested (e.g., see Genkin, Francez, and Kaminski 2010; Preller 2010). 3.2 Categories Category theory is a branch of pure mathematics that allows for a general and uniform formulation of various different mathematical formalisms in terms of their main structural properties using a few abstract concepts such as objects, arrows, and combinations and compositions of these. This uniform conceptual language allows for derivation of new properties of existing formalisms and for relating these to properties of other formalisms, if they bear similar categorical representation. In this function, it has been 88 Grefenstette and Sadrzadeh Categorical Compositional Distrib</context>
</contexts>
<marker>Preller, 2010</marker>
<rawString>Preller, A. 2010. Polynomial pregroup grammars parse context sensitive languages. Linguistic Analysis, 36:483–516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Preller</author>
<author>M Sadrzadeh</author>
</authors>
<title>Bell states and negative sentences in the distributed model of meaning.</title>
<date>2010</date>
<booktitle>Electronic Notes in Theoretical Computer Science, Proceedings of the 6th QPL Workshop on Quantum Physics and Logic. University of Oxford.</booktitle>
<editor>In P. Selinger, B. Coecke, P. Panangaden, editors,</editor>
<contexts>
<context position="74091" citStr="Preller and Sadrzadeh (2010)" startWordPosition="12240" endWordPosition="12243">tion 2.3.4, the first general setting for pairing meaning vectors with syntactic types was proposed in Clark and Pulman (2007). The setting of a DisCoCat generalized this by making the meaning derivation process rely on a syntactic type system, hence overcoming its central problem whereby the vector representations of strings of words with different grammatical structure lived in different spaces. A preliminary version of a DisCoCat was developed in Clark, Coecke, and Sadrzadeh (2008), a full version was elaborated on in Coecke, Sadrzadeh, and Clark (2010), where, based on the developments of Preller and Sadrzadeh (2010), it was also exemplified how the vector space model may be instantiated in a truth theoretic setting where meanings of words were sets of their denotations and meanings of sentences were their truth values. A nontechnical description of this theoretical setting was presented in Clark (2013), where a plausibility truth-theoretic model for sentence spaces was worked out and exemplified. The work of Grefenstette et al. (2011) focused on a tangential branch and developed a toy example where neither words nor sentence spaces were Boolean. The applicability of the theoretical setting to a real empi</context>
</contexts>
<marker>Preller, Sadrzadeh, 2010</marker>
<rawString>Preller, A. and M. Sadrzadeh. 2010. Bell states and negative sentences in the distributed model of meaning. In P. Selinger, B. Coecke, P. Panangaden, editors, Electronic Notes in Theoretical Computer Science, Proceedings of the 6th QPL Workshop on Quantum Physics and Logic. University of Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Selinger</author>
</authors>
<title>A survey of graphical languages for monoidal categories. New Structures for Physics,</title>
<date>2010</date>
<pages>275--337</pages>
<contexts>
<context position="66545" citStr="Selinger (2010)" startWordPosition="10911" endWordPosition="10912">mpact closed category C is a monoidal category with the following additional axioms: • Each object A E ob(C) has left and right “adjoint” objects Al and Ar in ob(C). • There exist four structural morphisms for each object A E ob(C): – ηlA : I -+ A ® Al. – ηr A : I -+ Ar ® A. – Cl A : Al ® A -+ I. – CrA : A ® Ar -+ I. • The previous structural morphisms satisfy the following equalities: – (1A ® ClA) ° (ηlA ® 1A) = 1A. – (ErA ® 1A) ° (1A ® ηrA) = 1A. – (1Ar ® Cr A) ° (ηr ® 1Ar) = 1Ar. – (eA® 1Al) ° (1Al ® ηlA = 1Al. CA closed categories come equipped with complete graphical calculi, surveyed in Selinger (2010). These calculi visualize and simplify the axiomatic reasoning within the category to a great extent. In particular, Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010) show how they depict the pregroup grammatical reductions and visualize the flow of information in composing meanings of single words and forming meanings for sentences. Although useful at an abstract level, these calculi do not play the same simplifying role when it comes to the concrete and empirical computations; therefore we will not discuss them in this article. A very relevant example of a compact c</context>
</contexts>
<marker>Selinger, 2010</marker>
<rawString>Selinger, P. 2010. A survey of graphical languages for monoidal categories. New Structures for Physics, pages 275–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="24789" citStr="Smolensky 1990" startWordPosition="3886" endWordPosition="3887">itive and multiplicative models while avoiding their problems, they are only partly successful in achieving the latter goal, and demonstrably do little better in achieving the former. 2.3.4 Tensor-Based Models. From Sections 2.3.1–2.3.3 we observe that the need for incorporating syntactic information into DSMs to achieve true compositionality is pressing, if only to develop a non-commutative composition operation that can take into account word order without the need for adhoc weighting schemes, and hopefully richer syntactic information as well. An early proposal by Smolensky and colleagues (Smolensky 1990; Smolensky and Legendre 2006) to use linear algebraic tensors as a composition operation solves the problem of finding non-commutative vector composition operators. The composition of two vectors is their tensor product, sometimes called the kronecker product when applied to vectors rather than vector spaces. For −→a ∈ V = Ei ci−→ni , and →−b ∈ W = Ej cf n , j we have: →− �ab = −→ a ⊗ −→ b = cicj −→ni ⊗ −→ nj ij The composition operation takes the original vectors and maps them to a vector in a larger vector space V ⊗ W, which is the tensor space of the original vectors’ spaces. Here the seco</context>
<context position="33517" citStr="Smolensky (1990)" startWordPosition="5317" endWordPosition="5318">ne |−→ ros´e� × �−→ adj = −−→loves |−→ likes × −−→Simon |−−→ Mary~ × −−→wine |−→ ros´e × −→red |−−−−→ delicious This example shows that this formalism allows for sentence comparison of sentences with identical dependency trees to be broken down to term-to-term comparison without the need for the tensor products to ever be computed or stored, reducing computation to inner product calculations. However, although matching terms with identical syntactic roles in the sentence works well in the given example, this model suffers from the same problems as the original tensor-based compositionality of Smolensky (1990) in that, by the authors’ own admission, sentences of different syntactic structure live in spaces of different dimensionality and thus cannot be directly compared. Hence we cannot use this to measure the similarity between even small variations in sentence structure, such as the pair “Simon likes red wine” and “Simon likes wine.” 2.3.5 SVS Models. The idea of including syntactic relations to other lemmas in word representations discussed in Section 2.3.4 is applied differently in the structured vector a�j&apos; ®re ed. Using the equality relation for 80 Grefenstette and Sadrzadeh Categorical Compo</context>
<context position="51670" citStr="Smolensky (1990)" startWordPosition="8229" endWordPosition="8230">del of Erk and Pad´o (2008) and syntactically driven semantic composition in the form of inner-products provide the implicit disambiguation features of the compositional models of Erk and Pad´o (2008) and Mitchell and Lapata (2008). The composition mechanism also involves the projection of tensored vectors into a common semantic space without the need for full representation of the tensored vectors in a manner similar to Plate (1991), without restriction to the nature of the vector spaces it can be applied to. This avoids the problems faced by other tensor-based composition mechanisms such as Smolensky (1990) and Clark and Pulman (2007). 85 Computational Linguistics Volume 41, Number 1 The word vectors can be specified model-theoretically and the sentence space can be defined over Boolean values to obtain grammatically driven truth-theoretic semantics in the style of Montague (1974), as proposed by Clark, Coecke, and Sadrzadeh (2008). Some logical operators can be emulated in this setting, such as using swap matrices for negation as shown by Coecke, Sadrzadeh, and Clark (2010). Alternatively, corpus-based variations on this formalism have been proposed by Grefenstette et al. (2011) to obtain a non</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Smolensky, P. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46(1-2):159–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
<author>G Legendre</author>
</authors>
<title>The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar Volume I: Cognitive Architecture.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="24819" citStr="Smolensky and Legendre 2006" startWordPosition="3888" endWordPosition="3891">licative models while avoiding their problems, they are only partly successful in achieving the latter goal, and demonstrably do little better in achieving the former. 2.3.4 Tensor-Based Models. From Sections 2.3.1–2.3.3 we observe that the need for incorporating syntactic information into DSMs to achieve true compositionality is pressing, if only to develop a non-commutative composition operation that can take into account word order without the need for adhoc weighting schemes, and hopefully richer syntactic information as well. An early proposal by Smolensky and colleagues (Smolensky 1990; Smolensky and Legendre 2006) to use linear algebraic tensors as a composition operation solves the problem of finding non-commutative vector composition operators. The composition of two vectors is their tensor product, sometimes called the kronecker product when applied to vectors rather than vector spaces. For −→a ∈ V = Ei ci−→ni , and →−b ∈ W = Ej cf n , j we have: →− �ab = −→ a ⊗ −→ b = cicj −→ni ⊗ −→ nj ij The composition operation takes the original vectors and maps them to a vector in a larger vector space V ⊗ W, which is the tensor space of the original vectors’ spaces. Here the second instance of ⊗ is not a recu</context>
</contexts>
<marker>Smolensky, Legendre, 2006</marker>
<rawString>Smolensky, P. and G. Legendre. 2006. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar Volume I: Cognitive Architecture. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>B Huval</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--201</pages>
<location>Jeju Island.</location>
<contexts>
<context position="42957" citStr="Socher et al. (2012)" startWordPosition="6852" endWordPosition="6855">e distributional learning methods they used to learn the vectors for nouns. This same approach can be extended to other unary relations without change and, using the general framework of the current article, an extension of it to binary predicates has been presented in Grefenstette et al. (2013), using multistep regression. For a direct comparison of the results of this approach with some of the results of the current article, we refer the reader to Grefenstette et al. (2013). Recursive Matrix-Vector Model. The third approach is the recently developed Recursive Matrix-Vector Model (MV-RNN) of Socher et al. (2012), which claims the two matrixbased models described here as special cases. In MV-RNN, words are represented as a pairing of a lexical semantic vector −→a with an operation matrix A. Within this model, given the parse of a sentence in the form of a binarized tree, the semantic representation (−→c , C) of each non-terminal node in the tree is produced by performing the following two operations on its children (−→a , A) and (−→b , B). First, the vector component −→c is produced by applying the operation matrix of one child to the vector of the other, and vice versa, and then projecting both of th</context>
<context position="44222" citStr="Socher et al. (2012)" startWordPosition="7084" endWordPosition="7087"> the child vectors using a projection matrix W, which must also be learned: r B × b c→−= W × L A × −→b 1 Second, the matrix C is calculated by projecting the pairing of matrices A and B back into the same space, using a projection matrix WM, which must also be learned: r A 1 C = WM × B 83 Computational Linguistics Volume 41, Number 1 The pairing (−→c , C) obtained through these operations forms the semantic representation of the phrase falling under the scope of the segment of the parse tree below that node. This approach to compositionality yields good results in the experiments described in Socher et al. (2012). It furthermore has appealing characteristics, such as treating relational words differently through their operation matrices, and allowing for recursive composition, as the output of each composition operation is of the same type of object as its inputs. This approach is highly general and has excellent coverage of different syntactic types, while leaving much room for parametric optimization. The principal mathematical difference with the compositional framework presented subsequently is that composition in MV-RNN is always a binary operation; for example, to compose a transitive verb with </context>
<context position="137827" citStr="Socher et al. 2012" startWordPosition="22786" endWordPosition="22789"> alignment, and include sentence comparisons that would penalize models which do not leverage syntactic information. Furthermore, each of these experiments dealt with the comparison of a certain type of sentence (transitive, intransitive). This was convenient for our concrete categorical model, as we defined the sentence space differently based on the valency of the head verb. However, sentences with different sorts of verbs should be able to be directly compared. Not only do several models, both non-syntax sensitive (additive, multiplicative) and syntax-sensitive (Baroni and Zamparelli 2010; Socher et al. 2012), not face this problem, as the product of composition is either naturally in the same space as the vectors being composed or is projected back into it, but the categorical framework the concrete categorical models were derived from does not commit us to different sentence spaces either. The topic of how to solve this problem for the concrete models developed here is beyond the scope of this article, but various options exist, such as the projection of ST into SI, the embedding of SI into ST, the use of a combined sentence space S = SI ⊕ ST, and so on. It is clear that future experiments shoul</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Socher, R., B. Huval, C. D. Manning, and A. Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing, pages 1,201–1,211, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="53393" citStr="Steedman (2001)" startWordPosition="8489" endWordPosition="8490">re particularly interesting within the context of this work because of their well-studied algebraic structure, which can trivially be mapped onto the structure of the category of vector spaces, as will be discussed subsequently. Logically speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which the tensor and its dual par coincide; this logic is sometimes referred to as Bi-Compact Linear Logic (Lambek 1999). The formalism works alongside the general guidelines of other categorial grammars, for instance, those of the combinatory categorial grammar (CCG) designed by Steedman (2001) and Steedman and Baldridge (2011). They consist of atomic grammatical types that combine to form compound types. A series of CCGlike application rules allow for type-reductions, forming the basis of syntactic analysis. As our first step, we show how this syntactic analysis formalism works by presenting an introduction to pregroup algebras. Pregroups. A pregroup is an algebraic structure of the form (P, ≤, ·,1, (−)l, (−)r). Its elements are defined as follows: • P is a set {a, b, c,. . .}. • The relation ≤ is a partial ordering on P. • The binary operation · is an associative, non-commutative </context>
<context position="81675" citStr="Steedman 2001" startWordPosition="13470" endWordPosition="13471">sitive cases in a simpler manner, and because the experiments in this article only compare intransitive sentences with intransitive sentences, and transitive sentences with transitive sentences, we will not address the issue of unification here. 4.2 Noun-Oriented Types While Lambek’s pregroup types presented in Lambek (2008) include a rich array of basic types and hand-designed compound types in order to capture specific grammatic properties, for the sake of simplicity we will use a simpler set of grammatical types for experimental purposes, similar to some common types found in the CCG-bank (Steedman 2001). We assign a basic pregroup type n for all nouns, with an associated vector space N for their semantic representations. Furthermore, we will treat noun-phrases as nouns, assigning to them the same pregroup type and semantic space. CCG treats intransitive verbs as functions NP\S that consume a noun phrase and return a sentence, and transitive verbs as functions (NP\S)/NP that consume a noun phrase and return an intransitive verb function, which in turn consumes a noun phrase and returns a sentence. Using our distinction between intransitive sentences, we give intransitive verbs the type nrsI a</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Steedman, M. 2001. The Syntactic Process. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
<author>J Baldridge</author>
</authors>
<date>2011</date>
<journal>Combinatory Categorial Grammar. Wiley-Blackwell.</journal>
<contexts>
<context position="53427" citStr="Steedman and Baldridge (2011)" startWordPosition="8492" endWordPosition="8495">resting within the context of this work because of their well-studied algebraic structure, which can trivially be mapped onto the structure of the category of vector spaces, as will be discussed subsequently. Logically speaking, a pregroup is a non-commutative form of Linear Logic (Girard 1987) in which the tensor and its dual par coincide; this logic is sometimes referred to as Bi-Compact Linear Logic (Lambek 1999). The formalism works alongside the general guidelines of other categorial grammars, for instance, those of the combinatory categorial grammar (CCG) designed by Steedman (2001) and Steedman and Baldridge (2011). They consist of atomic grammatical types that combine to form compound types. A series of CCGlike application rules allow for type-reductions, forming the basis of syntactic analysis. As our first step, we show how this syntactic analysis formalism works by presenting an introduction to pregroup algebras. Pregroups. A pregroup is an algebraic structure of the form (P, ≤, ·,1, (−)l, (−)r). Its elements are defined as follows: • P is a set {a, b, c,. . .}. • The relation ≤ is a partial ordering on P. • The binary operation · is an associative, non-commutative monoid multiplication with the typ</context>
</contexts>
<marker>Steedman, Baldridge, 2011</marker>
<rawString>Steedman, M. and J. Baldridge. 2011. Combinatory Categorial Grammar. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM—An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Seventh International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="110094" citStr="Stolcke 2002" startWordPosition="18445" endWordPosition="18446">e Multiply model (as component-wise multiplication O is a commutative operation), the difference being the learning procedure for the verb vector. For all such vector-based models, the similarity of the two sentences s1 and s2 is taken to be the cosine similarity of their vectors, defined in Section 2.2: similarity(s1, s2) = cosine(−→s1 , −→s2 ) In addition to these vector-based methods, we define an additional baseline and an upper-bound. The additional baseline, Bigram Baseline, is a bigram-based language 105 Computational Linguistics Volume 41, Number 1 model trained on the BNC with SRILM (Stolcke 2002), using the standard language model settings for computing log-probabilities of bigrams. To determine the semantic similarity of sentences s1 = noun verb1 and s2 = noun verb2, we assumed sentences have mutually conditionally independent properties, and computed the joint probability: similarity(s1, s2) = logP(s1 ∧ s2) = log(P(s1)P(s2)) = logP(s1) + logP(s2) The reasoning behind this baseline is as follows. The sentence formed by combining the first verb with its arguments is, by the design of this data set, a semantically coherent sentence (e.g., the head bowed and the government bowed both ma</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A. 2002. SRILM—An extensible language modeling toolkit. In Seventh International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="10962" citStr="Turney and Pantel 2010" startWordPosition="1633" endWordPosition="1636">ht learn the meaning of language using such a model, rather than just use it. 2.2 Distributional Semantics A popular way of representing the meaning of words in lexical semantics is as distributions in a high-dimensional vector space. This approach is based on the distributional hypothesis of Harris (1968), who postulated that the meaning of a word was dictated by the context of its use. The more famous dictum stating this hypothesis is the statement of Firth (1957) that “You shall know a word by the company it keeps.” This view of semantics has furthermore been associated (Grefenstette 2009; Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein (presented in Wittgenstein 1953), who stated that language meaning was equivalent to its real world use. Practically speaking, the meaning of a word can be learned from a corpus by looking at what other words occur with it within a certain context, and the resulting distribution can be represented as a vector in a semantic vector space. This vectorial representation is convenient because vectors are a familiar structure with a rich set of ways of computing vector distance, allowing us to experiment with different word similarity metr</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, P. D. and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>44--533</pages>
<contexts>
<context position="7540" citStr="Turney (2012)" startWordPosition="1096" endWordPosition="1097">es of meaning: “logical form” versus “contextual use,” has left the quest for “what is the foundational structure of meaning?”—a question initially the concern of solely linguists and philosophers of language—even more of a challenge. In this section, we present a short overview of the background to the work developed in this article by briefly describing formal and distributional approaches to natural language semantics, and providing a non-exhaustive list of some approaches to compositional distributional semantics. For a more complete review of the topic, we encourage the reader to consult Turney (2012) or Clark (2013). 2.1 Montague Semantics Formal semantic models provide methods for translating sentences of natural language into logical formulae, which can then be fed to computer-aided automation tools to reason about them (Alshawi 1992). To compute the meaning of a sentence consisting of n words, meanings of these words must interact with one another. In formal semantics, this further interaction is represented as a function derived from the grammatical structure of the sentence. Such models consist of a pairing of syntactic analysis rules (in the form of a grammar) with semantic interpre</context>
<context position="46189" citStr="Turney (2012)" startWordPosition="7383" endWordPosition="7384">ection where no objective label exists. A direct comparison to MV-RNN methods within the context of experiments similar to those presented in this article has been produced by Blacoe and Lapata (2012), showing that simple operations perform on par with the earlier complex deep learning architectures produced by Socher and colleagues; we leave direct comparisons to future work. Early work has shown that the addition of a hidden layer with non-linearities to these simple models will improve the results. 2.5 Some Other Approaches to Distributional Semantics Domains and Functions. In recent work, Turney (2012) suggests modeling word representations not as a single semantic vector, but as a pair of vectors: one containing the information of the word relative to its domain (the other words that are ontologically similar), and another containing information relating to its function. The former vector is learned by looking at what nouns a word co-occurs with, and the latter is learned by looking at what verb-based patterns the word occurs in. Similarity between sets of words is not determined by a single similarity function, but rather through a combination of comparisons of the domain components of wo</context>
<context position="135762" citStr="Turney (2012)" startWordPosition="22461" endWordPosition="22462">al” language. For instance, whereas models that treat adjectives, nouns, and verbs differently tended to perform better than those that did not, the actual capacity of a model to use the syntactic structure was not tested. For instance, models that ignore word order and syntax altogether were not particularly penalized, as might be done if some sentences were simply the reverse of another sentence they are paired with (where syntax insensitive models would erroneously give such sentence pairs a high similarity score). One way of testing word order while expanding the data set was suggested by Turney (2012), who for every entry in a phrase similarity test such as Mitchell and Lapata’s, discussed herein, creates a new entry where one of the sentences has the order of its words reversed, and is assigned an artificial annotator score of 1 (the minimum). This is an interesting approach, but we would prefer to see such reversals designed into the data set and seen by annotators, for instance, 114 Grefenstette and Sadrzadeh Categorical Compositional Distributional Model of Meaning in the recent data set presented in Pham et al. (2013). This data set has entries such as guitar played man and man played</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Turney, Peter D. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>The Geometry of Information Retrieval.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<marker>Van Rijsbergen, 2004</marker>
<rawString>Van Rijsbergen, C. J. 2004. The Geometry of Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Walters</author>
</authors>
<title>Categories and Computer Science.</title>
<date>1991</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="63306" citStr="Walters 1991" startWordPosition="10197" endWordPosition="10198">data. The connection between the mathematics used for this branch of physics and those potentially useful for linguistic modeling has also been noted by several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004). In this section, we will briefly examine the basics of category theory, monoidal categories, and compact closed categories. The focus will be on defining enough basic concepts to proceed rather than provide a full-blown tutorial on category theory and the modeling of information flow, as several excellent sources already cover both aspects (e.g., Mac Lane 1998; Walters 1991; Coecke and Paquette 2011). A categoriesin-a-nutshell crash course is also provided in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010). The Basics of Category Theory. A basic category C is defined in terms of the following elements: • A collection of objects ob(C). • A collection of morphisms hom(C). • A morphism composition operation ◦. Each morphism f has a domain dom(f) ∈ ob(C) and a codomain codom(f) ∈ ob(C). For dom(f) = A and codom(f) = B we abbreviate these definitions as f : A → B. Despite the notational similarity to function definitions (and sets and func</context>
</contexts>
<marker>Walters, 1991</marker>
<rawString>Walters, R. F. 1991. Categories and Computer Science. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Geometry and Meaning.</title>
<date>2005</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="62886" citStr="Widdows (2005)" startWordPosition="10132" endWordPosition="10133">del and a quantitative distributional model (Clark, Coecke, and Sadrzadeh 2008; Coecke, Sadrzadeh, and Clark 2010). Moreover, the unifying categorical structures at work here were inspired by the ones used in the foundations of physics and the modeling of quantum information flow, as presented in Abramsky and Coecke (2004), where they relate the logical structure of quantum protocols to their state-based vector spaces data. The connection between the mathematics used for this branch of physics and those potentially useful for linguistic modeling has also been noted by several sources, such as Widdows (2005), Lambek (2010), and Van Rijsbergen (2004). In this section, we will briefly examine the basics of category theory, monoidal categories, and compact closed categories. The focus will be on defining enough basic concepts to proceed rather than provide a full-blown tutorial on category theory and the modeling of information flow, as several excellent sources already cover both aspects (e.g., Mac Lane 1998; Walters 1991; Coecke and Paquette 2011). A categoriesin-a-nutshell crash course is also provided in Clark, Coecke, and Sadrzadeh (2008) and Coecke, Sadrzadeh, and Clark (2010). The Basics of C</context>
</contexts>
<marker>Widdows, 2005</marker>
<rawString>Widdows, D. 2005. Geometry and Meaning. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Quantum Interaction Symposium (QI-2008). College Publications. CITESEER,</booktitle>
<location>Oxford.</location>
<contexts>
<context position="28955" citStr="Widdows (2008)" startWordPosition="4569" endWordPosition="4570">ther original vector. The noisy vector can be used to recover the clean original vector by comparing it with a predefined set of candidates (e.g., the set of word vectors if our original vectors are word meanings). Traces can be summed to form new traces effectively containing several vector pairs from which original vectors can be recovered. Using this encoding/decoding mechanism, the tensor product of sets of vectors can be encoded in a space of smaller dimensionality, and then recovered for computation without ever having to fully represent or store the full tensor product, as discussed by Widdows (2008). There are problems with this approach that make it unsuitable for our purposes, some of which are discussed in Mitchell and Lapata (2010). First, there is a limit to the information that can be stored in traces, which is independent of the size of the vectors stored, but is a logarithmic function of their number. As we wish to be able to store information for sentences of variable word length without having to directly represent the tensored sentence vector, setting an upper bound to the number of vectors that can be composed in this manner limits the length of the sentences we can represent</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Widdows, D. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second Quantum Interaction Symposium (QI-2008). College Publications. CITESEER, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wittgenstein</author>
</authors>
<title>Philosophical Investigations.</title>
<date>1953</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="11055" citStr="Wittgenstein 1953" startWordPosition="1648" endWordPosition="1649">emantics A popular way of representing the meaning of words in lexical semantics is as distributions in a high-dimensional vector space. This approach is based on the distributional hypothesis of Harris (1968), who postulated that the meaning of a word was dictated by the context of its use. The more famous dictum stating this hypothesis is the statement of Firth (1957) that “You shall know a word by the company it keeps.” This view of semantics has furthermore been associated (Grefenstette 2009; Turney and Pantel 2010) with earlier work in philosophy of language by Wittgenstein (presented in Wittgenstein 1953), who stated that language meaning was equivalent to its real world use. Practically speaking, the meaning of a word can be learned from a corpus by looking at what other words occur with it within a certain context, and the resulting distribution can be represented as a vector in a semantic vector space. This vectorial representation is convenient because vectors are a familiar structure with a rich set of ways of computing vector distance, allowing us to experiment with different word similarity metrics. The geometric nature of this representation entails that we can not only compare individ</context>
</contexts>
<marker>Wittgenstein, 1953</marker>
<rawString>Wittgenstein, L. 1953. Philosophical Investigations. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>I Korkontzelos</author>
<author>F Fallucchi</author>
<author>S Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1--263</pages>
<location>Beijing.</location>
<contexts>
<context position="38855" citStr="Zanzotto et al. (2010)" startWordPosition="6194" endWordPosition="6197">e towards true compositionality. We retain from this attempt to introduce compositionality in DSMs that including information obtained from syntactic dependency relations is important for proper disambiguation, and that having some mechanism by which the grammaticality of the expression being composed is a precondition for its composition is a desirable feature for any compositional mechanism. 2.4 Matrix-Based Compositionality The final class of approaches to vector composition we wish to discuss are three matrix-based models. Generic Additive Model. The first is the Generic Additive Model of Zanzotto et al. (2010). This is a generalization of the weighted additive model presented in Section 2.3.1. In this model, rather than multiplying lexical vectors by fixed parameters α and β before adding them to form the representation of their combination, they are instead the arguments of matrix multiplication by square matrices A and B: ab = A−→ →− a + B−→ b Here, A and B represent the added information provided by putting two words into relation. The numerical content of A and B is learned by linear regression over triplets (−→a , −→b , −→c ) where −→a and −→b are lexical semantic vectors, and −→c is the expec</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Fallucchi, Manandhar, 2010</marker>
<rawString>Zanzotto, F. M., I. Korkontzelos, F. Fallucchi, and S. Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1,263–1,271, Beijing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>