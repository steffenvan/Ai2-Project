<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.96503" genericHeader="abstract">
CONSTRAINT PROJECTION: AN EFFICIENT TREATMENT OF
DISJUNCTIVE FEATURE DESCRIPTIONS
</sectionHeader>
<address confidence="0.680742">
Mikio Nakano
NTT Basic Research Laboratories
3-9-11 Midori-cho, Musashino-shi, Tokyo 180 JAPAN
</address>
<email confidence="0.999928">
e-mail: nakano@atom.ntt.jp
</email>
<sectionHeader confidence="0.997404" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.9999546">
Unification of disjunctive feature descriptions
is important for efficient unification-based pars-
ing. This paper presents constraint projection,
a new method for unification of disjunctive fea-
ture structures represented by logical constraints.
Constraint projection is a generalization of con-
straint unification, and is more efficient because
constraint projection has a mechanism for aban-
doning information irrelevant to a goal specified
by a list of variables.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996879770270271">
Unification is a central operation in recent com-
putational linguistic research. Much work on
syntactic theory and natural language parsing
is based on unification because unification-based
approaches have many advantages over other syn-
tactic and computational theories. Unification-
based formalisms make it easy to write a gram-
mar. In particular, they allow rules and lexicon
to be written declaratively and do not need trans-
formations.
Some problems remain, however. One of the
main problems is the computational inefficiency
of the unification of disjunctive feature struc-
tures. Functional unification grammar (FUG)
(Kay 1985) uses disjunctive feature structures for
economical representation of lexical items. Using
disjunctive feature structures reduces the num-
ber of lexical items. However, if disjunctive fea-
ture structures were expanded to disjunctive nor-
mal form (DNF) as in definite clause grammar
(Pereira and Warren 1980) and Kay&apos;s parser (Kay
1985), unification would take exponential time in
the number of disjuncts. Avoiding unnecessary
expansion of disjunction is important for efficient
disjunctive unification. Kasper (1987) and Eisele
and Done (1988) have tackled this problem and
proposed unification methods for disjunctive fea-
ture descriptions.
&apos;DNF has a form 01 v 02 V Os V...v On, where 4,,
includes no disjunctions.
These works are based on graph unification
rather than on term unification. Graph unifica-
tion has the advantage that the number of argu-
ments is free and arguments are selected by la-
bels so that it is easy to write a grammar and
lexicon. Graph unification, however, has two dis-
advantages: it takes excessive time to search for
a specified feature and it requires much copying.
We adopt term unification for these reasons.
Although Eisele and Diirre (1988) have men-
tioned that their algorithm is applicable to term
unification as well as graph unification, this
method would lose term unification&apos;s advantage
of not requiring so much copying. On the con-
trary, constraint unification (CU) (Hasida 1986,
Tuda et al. 1989), a disjunctive unification
method, makes full use of term unification ad-
vantages. In CU, disjunctive feature structures
are represented by logical constraints, particu-
larly by Horn clauses, and unification is regarded
as a constraint satisfaction problem. Further-
more, solving a constraint satisfaction problem
is identical to transforming a constraint into an
equivalent and satisfiable constraint. CU unifies
feature structures by transforming the constraints
on them. The basic idea of CU is to transform
constraints in a demand-driven way; that is, to
transform only those constraints which may not
be satisfiable. This is why CU is efficient and does
not require excessive copying.
However, CU has a serious disadvantage. It
does not have a mechanism for abandoning irrel-
evant information, so the number of arguments
in constraint-terms (atomic formulas) becomes
so large that transformation takes much time.
Therefore, from the viewpoint of general natu-
ral language processing, although CU is suitable
for processing logical constraints with small struc-
tures, it is not suitable for constraints with large
structures.
This paper presents constraint projection
(CP), another method for disjunctive unifica-
tion.• The basic idea of CP is to abandon in-
formation irrelevant to goals. For example, in
</bodyText>
<page confidence="0.996416">
307
</page>
<bodyText confidence="0.9998709">
bottom-up parsing, if grammar consists of local
constraints as in contemporary unification-based
formalisms, it is possible to abandon informa-
tion about daughter nodes after the application
of rules, because the feature structure of a mother
node is determined only by the feature structures
of its daughter nodes and phrase structure rules.
Since abandoning irrelevant information makes
the resulting structure tighter, another applica-
tion of phrase structure rules to it will be efficient.
We use the term projection in the sense that CP
returns a projection of the input constraint on the
specified variables.
We explain how to express disjunctive feature
structures by logical constraints in Section 2. Sec-
tion 3 introduces CU and indicates its disadvan-
tages. Section 4 explains the basic ideas and the
algorithm of CP. Section 5 presents some results
of implementation and shows that adopting CP
makes parsing efficient.
</bodyText>
<sectionHeader confidence="0.692707333333333" genericHeader="method">
2 Expressing Disjunctive Feature
Structures by Logical
Constraints
</sectionHeader>
<bodyText confidence="0.847360153846154">
This section explains the representation of dis-
junctive feature structures by Horn clauses. We
use the DEC-10 Prolog notation for writing Horn
clauses.
First, we can express a feature structure with-
out disjunctions by a logical term. For example,
(1) is translated into (2).
( 1) posy num sing&apos; [ rerm tncf
agr per 3rd
subj agr
(2) cat(v,
agr(sing,3rd),
cat(_,agr(sing,3rd),_))
The arguments of the functor cat correspond to
the pos (part of speech), agr (agreement), and
subj (subject) features.
Disjunction and sharing are represented by
the bodies of Horn clauses. An atomic formula
in the body whose predicate has multiple defini-
tion clauses represents a disjunction. For exam-
ple, a disjunctive feature structure (3) in FUG
(Kay 1985) notation, is translated into (4).
-pos n
agr 0
agr [num singll
pos v
</bodyText>
<equation confidence="0.69359625">
subj (agr
per 3rcl-
1 [num plural] f
[ npuerm strsntd 1
</equation>
<page confidence="0.892252">
(4) p(cat(v,Agr,cat(_,Agr,_)))
not_3s(Agr).
p(cat(n,agr(sing,3rd),_)).
</page>
<figure confidence="0.5497978">
not_3s(agr(sing,Per))
:-lst_or_2nd(Per).
not_3s(agr(plural, _)).
1st_or_2nd(1st).
1st_or_2nd(2nd).
</figure>
<bodyText confidence="0.994198153846154">
Here, the predicate p corresponds to the specifica-
tion of the feature structure. A term p(X) means
that the variable X is a candidate of the disjunc-
tive feature structure specified by the predicate
p. The ANY value used in FUG or the value of
an unspecified feature can be represented by an
anonymous variable &apos;J.
We consider atomic formulas to be constraints
on the variables they include. The atomic formula
lst_or_2nd(Per) in (4) constrains the variable
Per to be either 1st or 2nd. In a similar way,
not_3s(Agr) means that Agr is a term which has
the form agr (Num , Per) , and that Num is sing and
Per is subject to the constraint lst_or_2nd(Per)
or that Num is plural.
We do not use or consider predicates with-
out their definition clauses because they make
no sense as constraints. We call an atomic
formula whose predicate has definition clauses
a constraint-term, and we call a sequence of
constraint-terms a constraint. A set of definition
clauses like (4) is called a structure of a constraint.
Phrase structure rules are also represented by
logical constraints. For example, If rules are bi-
nary and if L, R, and 14 stand for the left daughter,
the right daughter, and the mother, respectively,
they stand in a ternary relation, which we repre-
sent as psr(L,R,M). Each definition clause of psr
corresponds to a phrase structure rule. Clause (5)
is an example.
(5) psr(Subj,
cat(v,Agr,Subj),
cat(s,Agr,_)).
Definition clauses of psr may have their own bod-
ies.
If a disjunctive feature structure is specified
by a constraint-term p(X) and another is specified
by q(Y), the unification of X and Y is equivalent
to the problem of finding X which satisfies (6).
</bodyText>
<listItem confidence="0.529939">
(6) Cp(X),q(X)]
</listItem>
<bodyText confidence="0.998925444444444">
Thus a unification of disjunctive feature struc-
tures is equivalent to a constraint satisfaction
problem. An application of a phrase structure
rule also can be considered to be a constraint sat-
isfaction problem. For instance, if categories of
left daughter and right daughter are stipulated
by ci (L) and c2(R), computing a mother cate-
gory is equivalent to finding M which satisfies con-
straint (7).
</bodyText>
<listItem confidence="0.460657">
(7) Cci(L),c2(R),psr(L,R,M)]
</listItem>
<bodyText confidence="0.945417">
A Prolog call like (8) realizes this constraint
</bodyText>
<page confidence="0.986653">
308
</page>
<bodyText confidence="0.936035666666667">
satisfaction.
(8) :-cl(L),c2(R),psr(L,R,M),
assert(c3(M)),fail.
This method, however, is inefficient. Since Pro-
log chooses one definition clause when multiple
definition clauses are available, it must repeat a
procedure many times. This method is equivalent
to expanding disjunctions to DNF before unifica-
tion.
</bodyText>
<sectionHeader confidence="0.8164065" genericHeader="method">
3 Constraint Unification and Its
Problem
</sectionHeader>
<bodyText confidence="0.99924525">
This section explains constraint unification2
(Hasida 1986, Tuda et al. 1989), a method of dis-
junctive unification, and indicates its disadvan-
tage.
</bodyText>
<subsectionHeader confidence="0.998306">
3.1 Basic Ideas of Constraint Unification
</subsectionHeader>
<bodyText confidence="0.999947727272727">
As mentioned in Section 1, we can solve a con-
straint satisfaction problem by constraint trans-
formation. What we seek is an efficient algo-
rithm of transformation whose resulting structure
is guaranteed satisfiability and includes a small
number of disjuncts.
CU is a constraint transformation system
which avoids excessive expansion of disjunctions.
The goal of CU is to transform an input con-
straint to a modular constraint. Modular con-
straints are defined as follows.
</bodyText>
<listItem confidence="0.998408857142857">
(9) (Definition: modular) A constraint is mod-
ular, if
1. every argument of every atomic formula
is a variable,
2. no variable occurs in two distinct places,
and
3. every predicate is modularly defined.
</listItem>
<bodyText confidence="0.9995694">
A predicate is modularly defined if the bodies of
its definition clauses are either modular or NIL.
For example, (10) is a modular constraint,
while (11), (12), and (13) are not modular, when
all the predicates are modularly defined.
</bodyText>
<equation confidence="0.558077">
(10) Ep(X,Y),q(2,107
(11) tp(X,X))
(12) Cp(X,Y),q(Y.2)]
(13) Cp(f(a),g(Z))]
</equation>
<bodyText confidence="0.994720285714286">
Constraint (10) is satisfiable because the predi-
cates have definition clauses. Omitting the proof,
a modular constraint is necessarily satisfiable.
Transforming a constraint into a modular one is
equivalent to finding the set of instances which
satisfy the constraint. On the contrary, non-
modular constraint may not be satisfiable. When
</bodyText>
<footnote confidence="0.5235215">
2Constraint unification is called conditioned unifi-
cation in earlier papers.
</footnote>
<bodyText confidence="0.998445142857143">
a constraint is not modular, it is said to have de-
pendencies. For example, (12) has a dependency
concerning Y.
The main ideas of CU are (a) it classi-
fies constraint-terms in the input constraint into
groups so that they do not share a variable and
it transforms them into modular constraints sepa-
rately, and (b) it does not transform modular con-
straints. Briefly, CU processes only constraints
which have dependencies. This corresponds to
avoiding unnecessary expansion of disjunctions.
In CU, the order of processes is decided accord-
ing to dependencies. This flexibility enables CU
to reduce the amount of processing.
We explain these ideas and the algorithm of
CU briefly through an example. CU consists of
two functions, namely, modularize(constraint)
and integrate(constraint). We can execute CU
by calling modularize. Function modularize di-
vides the input constraint into several constraints,
and returns a list of their integrations. If one of
the integrations fails, modularization also fails.
The function integrate creates a new constraint-
term equivalent to the input constraint, finds
its modular definition clauses, and returns the
new constraint-term. Functions modularize and
integrate call each other.
Let us consider the execution of (14).
</bodyText>
<equation confidence="0.501723">
(14) modularize(
[p(X,Y),q(Y,Z),p(A,B),r(A),r(C)))
</equation>
<bodyText confidence="0.96275">
The predicates are defined as follows.
</bodyText>
<listItem confidence="0.994487833333333">
(15) p(f (A) ,C):-r(A),r(C).
(16) p(a,b).
(17) q(a,b).
(18) q(b,a).
(19) r(a).
(20) r(b).
</listItem>
<bodyText confidence="0.95693">
The input constraint is divided into (21), (22),
and (23), which are processed independently
(idea (a)).
</bodyText>
<listItem confidence="0.967512">
(21) [p(X,Y),q(Y,Z))
(22) [p(A,B),r(A)]
(23) [r(C)]
</listItem>
<bodyText confidence="0.9990649">
If the input constraint were not divided and (21)
had multiple solutions, the processing of (22)
would be repeated many times. This is one rea-
son for the efficiency of CU. Constraint (23) is not
transformed because it is already modular (idea
(b)). Prolog would exploit the definition clauses
of r and expend unnecessary computation time.
This is another reason for CU&apos;s efficiency.
To transform (21) and (22) into modular
constraint-terms, (24) and (25) are called.
</bodyText>
<listItem confidence="0.6343335">
(24) integrate(Cp(X,Y) ,q(Y ,Z)])
(25) integrate(Ep(A,B),r(A)))
</listItem>
<page confidence="0.99234">
309
</page>
<bodyText confidence="0.949515291666667">
Since (24) and (25) succeed and return
cO(X,Y,Z) 4 and ci(A,B), respectively, (14) re-
turns (26).
(26) [cO(X,Y,Z),c1(A,B),r(C)]
This modularization would fail if either (24) or
(25) failed.
Next, we explain integrate through the exe-
cution of (24). First, a new predicate c0 is made
so that we can suppose (27).
(27) cO(X,Y,Z)&lt;=&gt;p(X,Y),q(Y,Z)
Formula (27) means that (24) returns cO(X,Y,Z)
if the constraint [p(X,Y),q(Y,Z)] is satisfiable;
that is, cO(X,Y,Z) can be modularly defined so
that cO(X,Y,Z) and p(X,Y),q(Y,Z) constrain
X, Y, and Z in the same way. Next, a target
constraint-term is chosen. Although some heuris-
tics may be applicable to this choice, we simply
choose the first element p(X,Y) here. Then, the
definition clauses of p are consulted. Note that
this corresponds to the expansion of a disjunc-
tion.
First, (15) is exploited. The head of (15)
is unified with p(X,Y) in (27) so that (27) be-
comes (28).
</bodyText>
<listItem confidence="0.546568">
(28) cO(f (A) ,C,Z)&lt;=&gt;,r(A) ,r(C) ,q(C,Z)
</listItem>
<bodyText confidence="0.998404">
The term p(f(A),C) has been replaced by its
body r(A),r(C) in the right-hand side of (28).
Formula (28) means that cO(f (A) ,C,Z) is true if
the variables satisfy the right-hand side of (28).
Since the right-hand side of (28) is not modu-
lar, (29) is called and it must return a constraint
like (30).
</bodyText>
<equation confidence="0.841796166666667">
(29) modu/arize(Er(A) sr(C),q(C,Z)))
(30) [r(A),c2(C,Z))
Then, (31) is created as a definition clause of cO.
(31) cO(f (A) ,C,Z):-r(A) ,c2(C,Z) .
Second, (16) is exploited. Then, (28) be-
comes (32), (33) is called and returns (34), and
</equation>
<listItem confidence="0.7063574">
(35) is created.
(32) c0(a,b,Z)t=4.q(b,Z)
(33) modularize( [q(b,Z)))
(34) [c3(Z)]
(35) c0(a,b,Z):-c3(Z).
</listItem>
<bodyText confidence="0.895234230769231">
As a result, (24) returns cO(X,Y,Z) because its
definition clauses are made.
All the Horn clauses made in this CU invoked
by (14) are shown in (36).
(36) c0(f(A),C,Z):-r(A),c2(C,Z).
c0(a,b,Z):-c3(Z).
c2(a,b).
3 We use cn (n = 0,1,2, • -) for the names of newly-
made predicates.
c2(b,a).
c3(a).
cl(a,b).
When a new clause is created, if the predicate of
a term in its body has only one definition clause,
the term is unified with the head of the definition
clause and is replaced by the body. This opera-
tion is called reduction. For example, the second
clause of (36) is reduced to (37) because c3 has
only one definition clause.
(37) c0(a,b,a).
CU has another operation called folding. It
avoids repeating the same type of integrations
so that it makes the transformation efficient.
Folding also enables CU to handle some of the
recursively-defined predicates such as member and
append.
</bodyText>
<subsectionHeader confidence="0.999867">
3.2 Parsing with Constraint Unification
</subsectionHeader>
<bodyText confidence="0.998001125">
We adopt the CYK algorithm (Aho and Ull-
man 1972) for simplicity, although any algorithms
may be adopted. Suppose the constraint-term
cat_n_m(X) means X is the category of a phrase
from the (n 1)th word to the mth word in an
input sentence. Then, application of a phrase
structure rule is reduced to creating Horn clauses
like (38).
</bodyText>
<equation confidence="0.9298455">
(38) cat_n_m(M) :-
modularize( [cat_n_k(L) ,
</equation>
<bodyText confidence="0.9903885">
cat_k_m(R),
psr(L,R,M)]).
(2&lt;m&lt;1, 0&lt;n&lt;m -2, n 1&lt;k&lt;m - 1,
where I is the sentence length.)
The body of the created clause is the constraint
returned by the modularization in the right-hand
side. If the modularization fails, the clause is not
created.
</bodyText>
<subsectionHeader confidence="0.99986">
3.3 Problem of Constraint Unification
</subsectionHeader>
<bodyText confidence="0.846943125">
The main problem of a CU-based parser is
that the number of constraint-term arguments
increases as parsing proceeds. For example,
cat_0_2(M) is computed by (39).
(39) modularize(Ccat_0_1(L),
cat_1_2(R),
psr(L,R,M)))
This returns a constraint like [cO(L,R,M)]. Then
</bodyText>
<listItem confidence="0.701684">
(40) is created.
</listItem>
<equation confidence="0.530173">
(40) cat_0_2(M) :-c0(L,R,M) .
</equation>
<bodyText confidence="0.9842655">
Next, suppose that (40) is exploited in the follow-
ing application of rules.
</bodyText>
<listItem confidence="0.337234">
(41) rnodu/arize( [cat_0_2(M) ,
cat_2_3 (R1) ,
psr(M,R1,M1)])
</listItem>
<page confidence="0.898253">
310
</page>
<bodyText confidence="0.872593066666667">
Then (42) will be called.
(42) rnodu/arize(CcO(L,R,M),
cat _2_3 (R1) ,
psr(M,R1,M1)])
It returns a constraint like cl(L,R,M,R1,M1).
Thus the number of the constraint-term argu-
ments increases.
This causes computation time explosion for
two reasons: (a) the augmentation of arguments
increases the computation time for making new
terms and environments, dividing into groups,
unification, and so on, and (b) resulting struc-
tures may include excessive disjunctions because
of the ambiguity of features irrelevant to the
mother categories.
</bodyText>
<sectionHeader confidence="0.993324" genericHeader="method">
4 Constraint Projection
</sectionHeader>
<bodyText confidence="0.999843666666667">
This section describes constraint projection (CP),
which is a generalization of CU and overcomes the
disadvantage explained in the previous section.
</bodyText>
<subsectionHeader confidence="0.998882">
4.1 Basic Ideas of Constraint Projection
</subsectionHeader>
<bodyText confidence="0.999971333333333">
Inefficiency of parsing based on CU is caused by
keeping information about daughter nodes. Such
information can be abandoned if it is assumed
that we want only information about mother
nodes. That is, transformation (43) is more useful
in parsing than (44).
</bodyText>
<equation confidence="0.592921666666667">
(43) Cc i (L) , c2 (R) ,psr(I„R,M)] [c3(M)J
(44) Ecl(L),c2(R) ,psr(L,R,M)]
[c3(L,11,M)]
</equation>
<bodyText confidence="0.9970332">
Constraint Ec3(M)] in (43) must be satisfiable
and equivalent to the left-hand side concerning M.
Since [c3(M)) includes only information about H,
it must be a normal constraint, which is defined
in (45).
</bodyText>
<listItem confidence="0.921468333333333">
(45) (Definition: Normal) A constraint is normal
if
(a) it is modular, and
(b) each definition clause is a normal defini-
tion clause; that is, its body does not include
variables which do not appear in the head.
</listItem>
<bodyText confidence="0.664146">
For example, (46) is a normal definition clause
while (47) is not.
</bodyText>
<listItem confidence="0.382805">
(46) p(a,X):—r(X).
(47) q(X):—s(X,Y).
</listItem>
<bodyText confidence="0.998699375">
The operation (43) is generalized into a new
operation constraint projection which is defined
in (48).
(48) Given a constraint C and a list of variables
which we call goal, CP returns a normal con-
straint which is equivalent to C concerning
the variables in the goal, and includes only
variables in the goal.
</bodyText>
<listItem confidence="0.9929136">
• Symbols used:
— X, Y, ...; lists of variables.
— P, Q, ...; constraint-terms or sometimes
&amp;quot;fail&amp;quot;.
— P, Q, ...; constraints or sometimes &amp;quot;fail&amp;quot;.
— H, E, ...; lists of constraints.
• project(P, X) returns a normal constraint (list of
atomic formulas) on X.
1. If P = NIL then return NIL.
2. If X = NIL,
</listItem>
<bodyText confidence="0.5475545">
If not(satisfiable(P)), then return &amp;quot;fail&amp;quot;,
Else return NIL.
</bodyText>
<listItem confidence="0.960085551724138">
3. H := divide(P).
4. H„., := the list of the members of H which
include variables in X.
5. II, := the list of the members of H other
than the members of IL,
6. For each member R. of Hes.
If not(satisfiable(R)) then return &amp;quot;fail&amp;quot;
7. S := NIL.
8. For each member T of
— V := intersection(X, variables ap-
pearing in T).
— R := normalize(T, V).
— If R = &amp;quot;fail&amp;quot;, then return &amp;quot;fail&amp;quot;,
Else add R to S.
9. Return S.
• normalize(S, V) returns a normal constraint-
term (atomic formula) on V.
1. If S does not include variables appearing in
V, and S consists of a modular term, then
Return S.
2. S := a member of S that includes a variable
in V.
3. S&apos; := the rest of S.
4. C := a term cn(v1, v2, ..., vn). where vl,
vn are all the members of V and c,„ is a
new functor.
5. success-flag := NIL.
6. For each definition clause H :- B. of the
predicate of S:
</listItem>
<bodyText confidence="0.653848454545455">
— 0 := mgu(S, H).
— If 0 = fail, go to the next definition
clause.
— X := a list of variables in CO.
— Q := project(append(B0, S&apos;0), X).
— If Q = fail, then go to the next defini-
tion clause
Else add CO:-Q. to the database with
reduction.
7. If success-flag = NIL, then return &amp;quot;fail&amp;quot;,
else return C.
</bodyText>
<listItem confidence="0.984914625">
• mgu returns the most general unifier (Lloyd
1984).
• divide(P) divides P into a number of constraints
which share no variables and returns the list of
the constraints.
• satisfiable(P) returns T if P is satisfiable, and
NIL otherwise. (satisfiable is a slight modifica-
tion of modularize of Cu.)
</listItem>
<figureCaption confidence="0.997847">
Figure 1: Algorithm of Constraint Projection
</figureCaption>
<page confidence="0.964179">
311
</page>
<figure confidence="0.794514111111111">
project(Cp(X,Y),q(Y,Z),p(A B),r(A),r(C)],[X,C]) normahze((p(X,Y),q(Y,Z)],[X])
c0(I)4*p(X,Y),q(Y,Z)
(p(I,T) ,q(T„Z)3 (p(A.11) MA)] (r(C)]
s check exploit exploit
normalize( Ep(I, ),q(Y,Z)),(1)) otitis liability p(1(1) C):-r(A),r(C) . p(a,b).
i unify Ian*
c0(i(A))4* r(A),r(C),q(C,Z) cO(a)gq(ba)
r(C)
(c0(1),r(C))
</figure>
<figureCaption confidence="0.961583">
Figure 2: A Sample Execution of project
</figureCaption>
<figure confidence="0.8590536">
projectar(A)V,r(C),q(C,Z)7,40) t
V projeclaq(b,2)].0)
Es(.03 e
El
assert v assert i
</figure>
<figureCaption confidence="0.389717">
c0(f(A)):-r(1). c0(a).
</figureCaption>
<bodyText confidence="0.999163692307693">
CP also divides input constraint C into several
constraints according to dependencies, and trans-
forms them separately. The divided constraints
are classified into two groups: constraints which
include variables in the goal, and the others.
We call the former goal-relevant constraints and
the latter goal-irrelevant constraints. Only goal-
relevant constraints are transformed into normal
constraints. As for goal-irrelevant constraints,
only their satisfiability is examined, because they
are no longer used and examining satisfiability is
easier than transforming. This is a reason for the
efficiency of CP.
</bodyText>
<subsectionHeader confidence="0.999529">
4.2 Algorithm of Constraint Projection
</subsectionHeader>
<bodyText confidence="0.999678875">
CP consists of two functions, projeckconstraint,
goal variable list)) and normalize (constraint
goal variable list)), which respectively correspond
to modularize and integrate in CU. We can ex-
ecute CP by calling project. The algorithm of
constraint projection is shown in Figure 14.
We explain the algorithm of CP through the
execution of (49).
</bodyText>
<equation confidence="0.480366">
(49) project(
[p(X,Y),q(Y,Z),p(A,B),r(A),r(C)],
[X, C])
</equation>
<bodyText confidence="0.957853">
The predicates are defined in the same way as (15)
to (20). This execution is illustrated in Figure 2.
First, the input constraint is divided into (50),
(51) and (52) according to dependency.
</bodyText>
<listItem confidence="0.990485333333333">
(50) (p(X,Y),q(Y,Z)]
(51) Ep(A,B),r(A)]
(52) [“C)]
</listItem>
<bodyText confidence="0.953915">
Constraints (50) and (52) are goal-relevant be-
cause they include X and C, respectively. Since
</bodyText>
<footnote confidence="0.994162">
4Since the current version of CP does not have an
operation corresponding to folding, it cannot handle
recursively-defined predicates.
</footnote>
<figure confidence="0.467409">
cO(I)
</figure>
<figureCaption confidence="0.99566">
Figure 3: A Sample Execution of normalize
</figureCaption>
<listItem confidence="0.858548285714286">
(51) is goal-irrelevant, only its satisfiability is ex-
amined and confirmed. If some goal-irrelevant
constraints were proved not satisfiable, the pro-
jection would fail. Constraint (52) is already nor-
mal, so it is not processed. Then (53) is called to
transform (50).
(53) normalize([p(X,Y),q(Y,Z)], [X])
</listItem>
<bodyText confidence="0.940944294117647">
The second argument (goal) is the list of variables
that appear in both (50) and the goal of (49).
Since this normalization must return a constraint
like [cO(X)) , (49) returns (54).
(54) EcO(X),r(C)]
This includes only variables in the goal. This con-
straint has a tighter structure than (26).
Next, we explain the function normalize
through the execution of (53). This execution is
illustrated in Figure 3. First, a new term c0 (X) is
made so that we can suppose (55). Its arguments
are all the variables in the goal.
(55) cO(X)..p(X,Y),q(Y,Z)
The normal definition of c0 should be found.
Since a target constraint must include a variable
in the goal, p(X,Y) is chosen. The definition
clauses of p are (15) and (16).
</bodyText>
<listItem confidence="0.899195">
(15) p(t(A),C):-r(A),r(C).
(16) p(a,b).
</listItem>
<bodyText confidence="0.989729333333333">
The clause (15) is exploited at first. Its head is
unified with p(X,Y) in (55) so that (55) becomes
(56). (If this unification failed, the next definition
clause would be exploited.)
(56) c0(f(A))4=&gt;r(A),r(C),q(C,Z)
The right-hand side includes some variables which
</bodyText>
<page confidence="0.995406">
312
</page>
<bodyText confidence="0.922236529411764">
do not appear in the left-hand side. Therefore,
(57) is called.
(57) project( Cr ( A) , r (C) , q(C , Z)] , [A])
This returns r(A), and (58) is created.
(58) c0(f(A)):-r(A).
Second, (16) is exploited and (59) is created
in the same way.
(59) c0(a).
Consequently, (53) returns cO(X) because
some definition clauses of CO have been created.
All the Horn clauses created in this CP are
shown in (60).
(60) c0(f (A)) :-r(A).
c0(a).
Comparing (60) with (36), we see that CP not
only is efficient but also needs less memory space
than CU.
</bodyText>
<subsectionHeader confidence="0.999683">
4.3 Parsing with Constraint Projection
</subsectionHeader>
<bodyText confidence="0.997049">
We can construct a CYK parser by using CP as
in (61).
</bodyText>
<equation confidence="0.7306055">
(61) cat_n_m(M) :-
project(Ccat_n_k(L),
</equation>
<bodyText confidence="0.898697461538461">
cat_k_m(R),
psr(L,R,10],
NJ).
(2&lt;m&lt;/, 0&lt;n&lt;m —2, n-1-1&lt;k&lt;m — 1,
where / is the sentence length.)
For a simple example, let us consider parsing
the sentence &amp;quot;Japanese work.&amp;quot; by the following
projection.
(62) project([cat_of_japanese(L),
cat_of_work(R),
psr(L,R,14)1,
CM])
The rules and lexicon are defined as follows:
</bodyText>
<reference confidence="0.736210970588235">
(63) psr(n(Num,Per),
v(Num,Per,Tense),
s(Tense)).
(64) cat_of_japanese(n(Num,third)).
(65) cat_of_work(v(Num,Per,present))
:-not_3s(Num,Per).
(66) not_3s(plural,_).
(67) not_3s (s jugular, Per)
:-first_or_second(Per).
(68) first_or_second(first).
(69) first_or_second(second).
Since the constraint cannot be divided, (70) is
called.
(70) normalize(Ccat_of_japanese(L),
cat_of_work(R),
psr(L,R,M)],
[14])
The new term cO(M) is made, and (63) is ex-
ploited. Then (71) is to be created if its right-
hand side succeeds.
(71) c0(s(Tense)):-
project(Ccat_of_japanese(n(Num,Per)),
cat_of_work(v(Num,Per,Tense))),
[Tense]).
This projection calls (72).
(72) normalize((cat_of_japanese(n(Mum,Per)).
cat_of_work(v(Num,Per,Tense))],
[Tens El] ).
New term cl(Tense) is made and (65) is ex-
ploited. Then (73) is to be created if the right-
hand side succeeds.
(73) cl(present):-
project([cat_of_japanese(n(Num,Per)),
not_3s (Num , Per)] ,
</reference>
<bodyText confidence="0.786085571428571">
).
Since the first of argument of the projection is
satisfiable, it returns NIL. Therefore, (74) is cre-
ated, and (75) is created since the right-hand side
of (71) returns cl (Tense).
(74) cl(present).
(75) c0(s(Tense)):-cl(Tense).
When asserted, (75) is reduced to (76).
(76) c0(s(present)).
Consequently, [c0(4)] is returned.
Thus CP can be applied to CYK parsing, but
needless to say, CP can be applied to parsing al-
gorithms other than CYK, such as active chart
parsing.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="method">
5 Implementation
</sectionHeader>
<bodyText confidence="0.999517928571429">
Both CU and CP have been implemented in Sun
Common Lisp 3.0 on a Sun 4 sparc station 1.
They are based on a small Prolog interpreter
written in Lisp so that they use the same non-
disjunctive unification mechanism. We also im-
plemented three CYK parsers that adopt Prolog,
CU, and CP as the disjunctive unification mecha-
nism. Grammar and lexicon are based on HPSG
(Pollard and Sag 1987). Each lexical item has
about three disjuncts on average.
Table 1 shows comparison of the computation
time of the three parsers. It indicates CU is not
as efficient as CP when the input sentences are
long.
</bodyText>
<page confidence="0.997786">
313
</page>
<bodyText confidence="0.994684166666667">
Input sentence CPU time (sec.)
Prolog CU CP
He wanted to be a doctor. 3.88 6.88 5.64
You were a doctor when you were young. 29.84 19.54 12.49
I saw a man with a telescope on the hill. (out of memory) 245.34 17.32
He wanted to be a doctor when he was a student. 65.27 19.34 14.66
</bodyText>
<tableCaption confidence="0.997113">
Table 1: Computation Time
</tableCaption>
<sectionHeader confidence="0.999953" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999986958333334">
In the context of graph unification, Carter (1990)
proposed a bottom-up parsing method which
abandons information irrelevant to the mother
structures. His method, however, fails to check
the inconsistency of the abandoned information.
Furthermore, it abandons irrelevant information
after the application of the rule is completed,
while CP abandons goal-irrelevant constraints dy-
namically in its processes. This is another reason
why our method is better.
Another advantage of CP is that it does not
need much copying. CP copies only the Horn
clauses which are to be exploited. This is why
CP is expected to be more efficient and need less
memory space than other disjunctive unification
methods.
Hasida (1990) proposed another method
called dependency propagation for overcoming the
problem explained in Section 3.3. It uses Iran-
sclausal variables for efficient detection of depen-
dencies. Under the assumption that informa-
tion about daughter categories can be abandoned,
however, CP should be more efficient because of
its simplicity.
</bodyText>
<sectionHeader confidence="0.987879" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9999823">
We have presented constraint projection, a new
operation for efficient disjunctive unification. The
important feature of CP is that it returns con-
straints only on the specified variables. CP can
be considered not only as a disjunctive unifica-
tion method but also as a logical inference sys-
tem. Therefore, it is expected to play an impor-
tant role in synthesizing linguistic analyses such
as parsing and semantic analysis, and linguistic
and non-linguistic inferences.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998625">
I would like to thank Kiyoshi Kogure and Akira
Shimazu for their helpful comments. I had pre-
cious discussions with Koichi Hasida and Hiroshi
Tuda concerning constraint unification.
</bodyText>
<sectionHeader confidence="0.999401" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844363636364">
Aho, A. V. and Ullman, J. D. (1972) The Theory
of Parsing, Translation, and Compiling, Vol-
ume 1: Parsing. Prentice-Hall.
Carter, D. (1990) Efficient Disjunctive Unifica-
tion for Bottom-Up Parsing. In Proceedings of
the 13th International Conference on Computa-
tional Linguistics, Volume 3. pages 70-75.
Eisele, A. and DOrre, J. (1988) Unification of
Disjunctive Feature Descriptions. In Proceedings
of the 26th Annual Meeting of the Association
for Computational Linguistics.
Hasida, K. (1986) Conditioned Unification for
Natural Language Processing. In Proceedings of
the 11th International Conference on Computa-
tional Linguistics, pages 85-87.
Hasida, K. (1990) Sentence Processing as Con-
straint Transformation. In Proceedings of the 9th
European Conference on Artificial Intelligence,
pages 339-344.
Kasper, R. T. (1987) A Unification Method for
Disjunctive Feature Descriptions. In Proceedings
of the 25th Annual Meeting of the Association
for Computational Linguistics, pages 235-242.
Kay, M. (1985) Parsing in Functional Unifi-
cation Grammar. In Natural Language Pars-
ing: Psychological, Computational and Theoreti-
cal Perspectives, pages 251-278. Cambridge Uni-
versity Press.
Lloyd, J. W. (1984) Foundations of Logic Pro-
gramming. Springer-Verlag.
Pereira, F. C. N. and Warren, D. H. D.
(1980) Definite Clause Grammar for Language
Analysis—A Survay of the Formalism and a
Comparison with Augmented Transition Net-
works. Artificial Intelligence, 13:231-278.
Pollard, C. J. and Sag, I. A. (1987) Information-
Based Syntax and Semantics, Volume 1 Funda-
mentals. CSLI Lecture Notes Series No.13. Stan-
ford :CSLI
Tuda, H., Hasida, K., and Sirai, H. (1989) JPSG
Parser on Constraint Logic Programming. In
Proceedings of 4th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 95-102.
</reference>
<page confidence="0.999133">
314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897824">
<title confidence="0.9974975">CONSTRAINT PROJECTION: AN EFFICIENT TREATMENT OF DISJUNCTIVE FEATURE DESCRIPTIONS</title>
<author confidence="0.997096">Mikio Nakano</author>
<affiliation confidence="0.999372">NTT Basic Research Laboratories</affiliation>
<address confidence="0.935384">3-9-11 Midori-cho, Musashino-shi, Tokyo 180 JAPAN</address>
<email confidence="0.968414">nakano@atom.ntt.jp</email>
<abstract confidence="0.999524545454545">Unification of disjunctive feature descriptions is important for efficient unification-based parsing. This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints. Constraint projection is a generalization of constraint unification, and is more efficient because constraint projection has a mechanism for abandoning information irrelevant to a goal specified by a list of variables.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>(63) psr(n(Num,Per), v(Num,Per,Tense), s(Tense)).</note>
<marker></marker>
<rawString>(63) psr(n(Num,Per), v(Num,Per,Tense), s(Tense)).</rawString>
</citation>
<citation valid="false">
<marker></marker>
<rawString>(64) cat_of_japanese(n(Num,third)). (65) cat_of_work(v(Num,Per,present)) :-not_3s(Num,Per). (66) not_3s(plural,_).</rawString>
</citation>
<citation valid="false">
<title>(67) not_3s (s jugular, Per) :-first_or_second(Per). (68) first_or_second(first). (69) first_or_second(second). Since the constraint cannot be divided, (70) is called.</title>
<marker></marker>
<rawString>(67) not_3s (s jugular, Per) :-first_or_second(Per). (68) first_or_second(first). (69) first_or_second(second). Since the constraint cannot be divided, (70) is called.</rawString>
</citation>
<citation valid="false">
<note>(70) normalize(Ccat_of_japanese(L), cat_of_work(R),</note>
<marker></marker>
<rawString>(70) normalize(Ccat_of_japanese(L), cat_of_work(R),</rawString>
</citation>
<citation valid="false">
<note>psr(L,R,M)], [14</note>
<marker></marker>
<rawString>psr(L,R,M)], [14])</rawString>
</citation>
<citation valid="false">
<title>The new term cO(M) is made, and (63) is exploited. Then (71) is to be created if its righthand side succeeds.</title>
<marker></marker>
<rawString>The new term cO(M) is made, and (63) is exploited. Then (71) is to be created if its righthand side succeeds.</rawString>
</citation>
<citation valid="false">
<title>(71) c0(s(Tense)):-project(Ccat_of_japanese(n(Num,Per)), cat_of_work(v(Num,Per,Tense))), [Tense]). This projection calls (72).</title>
<marker></marker>
<rawString>(71) c0(s(Tense)):-project(Ccat_of_japanese(n(Num,Per)), cat_of_work(v(Num,Per,Tense))), [Tense]). This projection calls (72).</rawString>
</citation>
<citation valid="false">
<location>[Tens El] ).</location>
<marker></marker>
<rawString>(72) normalize((cat_of_japanese(n(Mum,Per)). cat_of_work(v(Num,Per,Tense))], [Tens El] ).</rawString>
</citation>
<citation valid="false">
<title>New term cl(Tense) is made and (65) is exploited. Then (73) is to be created if the righthand side succeeds. (73) cl(present):-</title>
<marker></marker>
<rawString>New term cl(Tense) is made and (65) is exploited. Then (73) is to be created if the righthand side succeeds. (73) cl(present):-</rawString>
</citation>
<citation valid="false">
<booktitle>not_3s (Num , Per)] ,</booktitle>
<marker></marker>
<rawString>project([cat_of_japanese(n(Num,Per)), not_3s (Num , Per)] ,</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing.</booktitle>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="14878" citStr="Aho and Ullman 1972" startWordPosition="2305" endWordPosition="2309">term in its body has only one definition clause, the term is unified with the head of the definition clause and is replaced by the body. This operation is called reduction. For example, the second clause of (36) is reduced to (37) because c3 has only one definition clause. (37) c0(a,b,a). CU has another operation called folding. It avoids repeating the same type of integrations so that it makes the transformation efficient. Folding also enables CU to handle some of the recursively-defined predicates such as member and append. 3.2 Parsing with Constraint Unification We adopt the CYK algorithm (Aho and Ullman 1972) for simplicity, although any algorithms may be adopted. Suppose the constraint-term cat_n_m(X) means X is the category of a phrase from the (n 1)th word to the mth word in an input sentence. Then, application of a phrase structure rule is reduced to creating Horn clauses like (38). (38) cat_n_m(M) :- modularize( [cat_n_k(L) , cat_k_m(R), psr(L,R,M)]). (2&lt;m&lt;1, 0&lt;n&lt;m -2, n 1&lt;k&lt;m - 1, where I is the sentence length.) The body of the created clause is the constraint returned by the modularization in the right-hand side. If the modularization fails, the clause is not created. 3.3 Problem of Constr</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, A. V. and Ullman, J. D. (1972) The Theory of Parsing, Translation, and Compiling, Volume 1: Parsing. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Carter</author>
</authors>
<title>Efficient Disjunctive Unification for Bottom-Up Parsing.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics, Volume</booktitle>
<volume>3</volume>
<pages>70--75</pages>
<marker>Carter, 1990</marker>
<rawString>Carter, D. (1990) Efficient Disjunctive Unification for Bottom-Up Parsing. In Proceedings of the 13th International Conference on Computational Linguistics, Volume 3. pages 70-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Eisele</author>
<author>J DOrre</author>
</authors>
<title>Unification of Disjunctive Feature Descriptions.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Eisele, DOrre, 1988</marker>
<rawString>Eisele, A. and DOrre, J. (1988) Unification of Disjunctive Feature Descriptions. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hasida</author>
</authors>
<title>Conditioned Unification for Natural Language Processing.</title>
<date>1986</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Linguistics,</booktitle>
<pages>85--87</pages>
<contexts>
<context position="2729" citStr="Hasida 1986" startWordPosition="402" endWordPosition="403">cation has the advantage that the number of arguments is free and arguments are selected by labels so that it is easy to write a grammar and lexicon. Graph unification, however, has two disadvantages: it takes excessive time to search for a specified feature and it requires much copying. We adopt term unification for these reasons. Although Eisele and Diirre (1988) have mentioned that their algorithm is applicable to term unification as well as graph unification, this method would lose term unification&apos;s advantage of not requiring so much copying. On the contrary, constraint unification (CU) (Hasida 1986, Tuda et al. 1989), a disjunctive unification method, makes full use of term unification advantages. In CU, disjunctive feature structures are represented by logical constraints, particularly by Horn clauses, and unification is regarded as a constraint satisfaction problem. Furthermore, solving a constraint satisfaction problem is identical to transforming a constraint into an equivalent and satisfiable constraint. CU unifies feature structures by transforming the constraints on them. The basic idea of CU is to transform constraints in a demand-driven way; that is, to transform only those con</context>
<context position="8650" citStr="Hasida 1986" startWordPosition="1326" endWordPosition="1327"> stipulated by ci (L) and c2(R), computing a mother category is equivalent to finding M which satisfies constraint (7). (7) Cci(L),c2(R),psr(L,R,M)] A Prolog call like (8) realizes this constraint 308 satisfaction. (8) :-cl(L),c2(R),psr(L,R,M), assert(c3(M)),fail. This method, however, is inefficient. Since Prolog chooses one definition clause when multiple definition clauses are available, it must repeat a procedure many times. This method is equivalent to expanding disjunctions to DNF before unification. 3 Constraint Unification and Its Problem This section explains constraint unification2 (Hasida 1986, Tuda et al. 1989), a method of disjunctive unification, and indicates its disadvantage. 3.1 Basic Ideas of Constraint Unification As mentioned in Section 1, we can solve a constraint satisfaction problem by constraint transformation. What we seek is an efficient algorithm of transformation whose resulting structure is guaranteed satisfiability and includes a small number of disjuncts. CU is a constraint transformation system which avoids excessive expansion of disjunctions. The goal of CU is to transform an input constraint to a modular constraint. Modular constraints are defined as follows.</context>
</contexts>
<marker>Hasida, 1986</marker>
<rawString>Hasida, K. (1986) Conditioned Unification for Natural Language Processing. In Proceedings of the 11th International Conference on Computational Linguistics, pages 85-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hasida</author>
</authors>
<title>Sentence Processing as Constraint Transformation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 9th European Conference on Artificial Intelligence,</booktitle>
<pages>339--344</pages>
<marker>Hasida, 1990</marker>
<rawString>Hasida, K. (1990) Sentence Processing as Constraint Transformation. In Proceedings of the 9th European Conference on Artificial Intelligence, pages 339-344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Kasper</author>
</authors>
<title>A Unification Method for Disjunctive Feature Descriptions.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<contexts>
<context position="1831" citStr="Kasper (1987)" startWordPosition="252" endWordPosition="253">f the unification of disjunctive feature structures. Functional unification grammar (FUG) (Kay 1985) uses disjunctive feature structures for economical representation of lexical items. Using disjunctive feature structures reduces the number of lexical items. However, if disjunctive feature structures were expanded to disjunctive normal form (DNF) as in definite clause grammar (Pereira and Warren 1980) and Kay&apos;s parser (Kay 1985), unification would take exponential time in the number of disjuncts. Avoiding unnecessary expansion of disjunction is important for efficient disjunctive unification. Kasper (1987) and Eisele and Done (1988) have tackled this problem and proposed unification methods for disjunctive feature descriptions. &apos;DNF has a form 01 v 02 V Os V...v On, where 4,, includes no disjunctions. These works are based on graph unification rather than on term unification. Graph unification has the advantage that the number of arguments is free and arguments are selected by labels so that it is easy to write a grammar and lexicon. Graph unification, however, has two disadvantages: it takes excessive time to search for a specified feature and it requires much copying. We adopt term unificatio</context>
</contexts>
<marker>Kasper, 1987</marker>
<rawString>Kasper, R. T. (1987) A Unification Method for Disjunctive Feature Descriptions. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 235-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Parsing in Functional Unification Grammar.</title>
<date>1985</date>
<booktitle>In Natural Language Parsing: Psychological, Computational and Theoretical Perspectives,</booktitle>
<pages>251--278</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1318" citStr="Kay 1985" startWordPosition="180" endWordPosition="181">entral operation in recent computational linguistic research. Much work on syntactic theory and natural language parsing is based on unification because unification-based approaches have many advantages over other syntactic and computational theories. Unificationbased formalisms make it easy to write a grammar. In particular, they allow rules and lexicon to be written declaratively and do not need transformations. Some problems remain, however. One of the main problems is the computational inefficiency of the unification of disjunctive feature structures. Functional unification grammar (FUG) (Kay 1985) uses disjunctive feature structures for economical representation of lexical items. Using disjunctive feature structures reduces the number of lexical items. However, if disjunctive feature structures were expanded to disjunctive normal form (DNF) as in definite clause grammar (Pereira and Warren 1980) and Kay&apos;s parser (Kay 1985), unification would take exponential time in the number of disjuncts. Avoiding unnecessary expansion of disjunction is important for efficient disjunctive unification. Kasper (1987) and Eisele and Done (1988) have tackled this problem and proposed unification methods </context>
<context position="5785" citStr="Kay 1985" startWordPosition="872" endWordPosition="873"> writing Horn clauses. First, we can express a feature structure without disjunctions by a logical term. For example, (1) is translated into (2). ( 1) posy num sing&apos; [ rerm tncf agr per 3rd subj agr (2) cat(v, agr(sing,3rd), cat(_,agr(sing,3rd),_)) The arguments of the functor cat correspond to the pos (part of speech), agr (agreement), and subj (subject) features. Disjunction and sharing are represented by the bodies of Horn clauses. An atomic formula in the body whose predicate has multiple definition clauses represents a disjunction. For example, a disjunctive feature structure (3) in FUG (Kay 1985) notation, is translated into (4). -pos n agr 0 agr [num singll pos v subj (agr per 3rcl1 [num plural] f [ npuerm strsntd 1 (4) p(cat(v,Agr,cat(_,Agr,_))) not_3s(Agr). p(cat(n,agr(sing,3rd),_)). not_3s(agr(sing,Per)) :-lst_or_2nd(Per). not_3s(agr(plural, _)). 1st_or_2nd(1st). 1st_or_2nd(2nd). Here, the predicate p corresponds to the specification of the feature structure. A term p(X) means that the variable X is a candidate of the disjunctive feature structure specified by the predicate p. The ANY value used in FUG or the value of an unspecified feature can be represented by an anonymous varia</context>
</contexts>
<marker>Kay, 1985</marker>
<rawString>Kay, M. (1985) Parsing in Functional Unification Grammar. In Natural Language Parsing: Psychological, Computational and Theoretical Perspectives, pages 251-278. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Lloyd</author>
</authors>
<title>Foundations of Logic Programming.</title>
<date>1984</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="19431" citStr="Lloyd 1984" startWordPosition="3095" endWordPosition="3096">. S := a member of S that includes a variable in V. 3. S&apos; := the rest of S. 4. C := a term cn(v1, v2, ..., vn). where vl, vn are all the members of V and c,„ is a new functor. 5. success-flag := NIL. 6. For each definition clause H :- B. of the predicate of S: — 0 := mgu(S, H). — If 0 = fail, go to the next definition clause. — X := a list of variables in CO. — Q := project(append(B0, S&apos;0), X). — If Q = fail, then go to the next definition clause Else add CO:-Q. to the database with reduction. 7. If success-flag = NIL, then return &amp;quot;fail&amp;quot;, else return C. • mgu returns the most general unifier (Lloyd 1984). • divide(P) divides P into a number of constraints which share no variables and returns the list of the constraints. • satisfiable(P) returns T if P is satisfiable, and NIL otherwise. (satisfiable is a slight modification of modularize of Cu.) Figure 1: Algorithm of Constraint Projection 311 project(Cp(X,Y),q(Y,Z),p(A B),r(A),r(C)],[X,C]) normahze((p(X,Y),q(Y,Z)],[X]) c0(I)4*p(X,Y),q(Y,Z) (p(I,T) ,q(T„Z)3 (p(A.11) MA)] (r(C)] s check exploit exploit normalize( Ep(I, ),q(Y,Z)),(1)) otitis liability p(1(1) C):-r(A),r(C) . p(a,b). i unify Ian* c0(i(A))4* r(A),r(C),q(C,Z) cO(a)gq(ba) r(C) (c0(1)</context>
</contexts>
<marker>Lloyd, 1984</marker>
<rawString>Lloyd, J. W. (1984) Foundations of Logic Programming. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Definite Clause Grammar for Language Analysis—A Survay of the Formalism and a Comparison with Augmented Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<pages>13--231</pages>
<contexts>
<context position="1622" citStr="Pereira and Warren 1980" startWordPosition="222" endWordPosition="225">asy to write a grammar. In particular, they allow rules and lexicon to be written declaratively and do not need transformations. Some problems remain, however. One of the main problems is the computational inefficiency of the unification of disjunctive feature structures. Functional unification grammar (FUG) (Kay 1985) uses disjunctive feature structures for economical representation of lexical items. Using disjunctive feature structures reduces the number of lexical items. However, if disjunctive feature structures were expanded to disjunctive normal form (DNF) as in definite clause grammar (Pereira and Warren 1980) and Kay&apos;s parser (Kay 1985), unification would take exponential time in the number of disjuncts. Avoiding unnecessary expansion of disjunction is important for efficient disjunctive unification. Kasper (1987) and Eisele and Done (1988) have tackled this problem and proposed unification methods for disjunctive feature descriptions. &apos;DNF has a form 01 v 02 V Os V...v On, where 4,, includes no disjunctions. These works are based on graph unification rather than on term unification. Graph unification has the advantage that the number of arguments is free and arguments are selected by labels so th</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F. C. N. and Warren, D. H. D. (1980) Definite Clause Grammar for Language Analysis—A Survay of the Formalism and a Comparison with Augmented Transition Networks. Artificial Intelligence, 13:231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Pollard</author>
<author>I A Sag</author>
</authors>
<title>InformationBased Syntax and Semantics,</title>
<date>1987</date>
<booktitle>Fundamentals. CSLI Lecture Notes Series No.13. Stanford :CSLI</booktitle>
<volume>1</volume>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, C. J. and Sag, I. A. (1987) InformationBased Syntax and Semantics, Volume 1 Fundamentals. CSLI Lecture Notes Series No.13. Stanford :CSLI</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tuda</author>
<author>K Hasida</author>
<author>H Sirai</author>
</authors>
<title>JPSG Parser on Constraint Logic Programming.</title>
<date>1989</date>
<booktitle>In Proceedings of 4th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>95--102</pages>
<contexts>
<context position="2748" citStr="Tuda et al. 1989" startWordPosition="404" endWordPosition="407">e advantage that the number of arguments is free and arguments are selected by labels so that it is easy to write a grammar and lexicon. Graph unification, however, has two disadvantages: it takes excessive time to search for a specified feature and it requires much copying. We adopt term unification for these reasons. Although Eisele and Diirre (1988) have mentioned that their algorithm is applicable to term unification as well as graph unification, this method would lose term unification&apos;s advantage of not requiring so much copying. On the contrary, constraint unification (CU) (Hasida 1986, Tuda et al. 1989), a disjunctive unification method, makes full use of term unification advantages. In CU, disjunctive feature structures are represented by logical constraints, particularly by Horn clauses, and unification is regarded as a constraint satisfaction problem. Furthermore, solving a constraint satisfaction problem is identical to transforming a constraint into an equivalent and satisfiable constraint. CU unifies feature structures by transforming the constraints on them. The basic idea of CU is to transform constraints in a demand-driven way; that is, to transform only those constraints which may </context>
<context position="8669" citStr="Tuda et al. 1989" startWordPosition="1328" endWordPosition="1331">y ci (L) and c2(R), computing a mother category is equivalent to finding M which satisfies constraint (7). (7) Cci(L),c2(R),psr(L,R,M)] A Prolog call like (8) realizes this constraint 308 satisfaction. (8) :-cl(L),c2(R),psr(L,R,M), assert(c3(M)),fail. This method, however, is inefficient. Since Prolog chooses one definition clause when multiple definition clauses are available, it must repeat a procedure many times. This method is equivalent to expanding disjunctions to DNF before unification. 3 Constraint Unification and Its Problem This section explains constraint unification2 (Hasida 1986, Tuda et al. 1989), a method of disjunctive unification, and indicates its disadvantage. 3.1 Basic Ideas of Constraint Unification As mentioned in Section 1, we can solve a constraint satisfaction problem by constraint transformation. What we seek is an efficient algorithm of transformation whose resulting structure is guaranteed satisfiability and includes a small number of disjuncts. CU is a constraint transformation system which avoids excessive expansion of disjunctions. The goal of CU is to transform an input constraint to a modular constraint. Modular constraints are defined as follows. (9) (Definition: m</context>
</contexts>
<marker>Tuda, Hasida, Sirai, 1989</marker>
<rawString>Tuda, H., Hasida, K., and Sirai, H. (1989) JPSG Parser on Constraint Logic Programming. In Proceedings of 4th Conference of the European Chapter of the Association for Computational Linguistics, pages 95-102.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>