<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019873">
<title confidence="0.962466">
NUS-ML: Improving Word Sense Disambiguation Using Topic Features
</title>
<author confidence="0.997556">
Jun Fu Cai, Wee Sun Lee
</author>
<affiliation confidence="0.924690666666667">
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
</affiliation>
<email confidence="0.995877">
{caijunfu, leews}@comp.nus.edu.sg
</email>
<author confidence="0.929871">
Yee Whye Teh
</author>
<affiliation confidence="0.9360235">
Gatsby Computational Neuroscience Unit
University College London
</affiliation>
<address confidence="0.883977">
17 Queen Square, London WC1N 3AR, UK
</address>
<email confidence="0.999036">
ywteh@gatsby.ucl.ac.uk
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937916666667">
We participated in SemEval-1 English
coarse-grained all-words task (task 7), En-
glish fine-grained all-words task (task 17,
subtask 3) and English coarse-grained lex-
ical sample task (task 17, subtask 1). The
same method with different labeled data is
used for the tasks; SemCor is the labeled
corpus used to train our system for the all-
words tasks while the labeled corpus that
is provided is used for the lexical sam-
ple task. The knowledge sources include
part-of-speech of neighboring words, single
words in the surrounding context, local col-
locations, and syntactic patterns. In addi-
tion, we constructed a topic feature, targeted
to capture the global context information,
using the latent dirichlet allocation (LDA)
algorithm with unlabeled corpus. A modi-
fied naive Bayes classifier is constructed to
incorporate all the features. We achieved
81.6%, 57.6%, 88.7% for coarse-grained all-
words task, fine-grained all-words task and
coarse-grained lexical sample task respec-
tively.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991">
Supervised corpus-based approach has been the
most successful in WSD to date. However, this ap-
proach faces severe data scarcity problem, resulting
features being sparsely represented in the training
data. This problem is especially prominent for the
bag-of-words feature. A direct consequence is that
the global context information, which the bag-of-
words feature is supposed to capture, may be poorly
represented.
Our system tries to address this problem by
clustering features to relieve the scarcity problem,
specifically on the bag-of-words feature. In the pro-
cess, we construct topic features, trained using the
latent dirichlet allocation (LDA) algorithm. We train
the topic model (Blei et al., 2003) on unlabeled data,
clustering the words occurring in the corpus to a pre-
defined number of topics. We then use the resulting
topic model to tag the bag-of-words in the labeled
corpus with topic distributions.
We incorporate the distributions, called the topic
features, using a simple Bayesian network, modified
from naive Bayes model, alongside other features
and train the model on the labeled corpus.
</bodyText>
<sectionHeader confidence="0.984438" genericHeader="method">
2 Feature Construction
</sectionHeader>
<subsectionHeader confidence="0.975742">
2.1 Baseline Features
</subsectionHeader>
<bodyText confidence="0.876762083333333">
For both the lexical sample and all-words tasks, we
use the following standard baseline features.
POS Tags For each word instance w, we include
POS tags for P words prior to as well as after w
within the same sentence boundary. We also include
the POS tag of w. If there are fewer than P words
prior or after w in the same sentence, we denote the
corresponding feature as NIL.
Local Collocations We adopt the same 11 col-
location features as (Lee and Ng, 2002), namely
C−1,−1, C1,1, C−2,−2, C2,2, C−2,−1, C−1,1, C1,2,
C−3,−1, C−2,1, C−1,2, and C1,3.
</bodyText>
<page confidence="0.981777">
249
</page>
<bodyText confidence="0.9691405">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 249–252,
Prague, June 2007. c�2007 Association for Computational Linguistics
Bag-of-Words For each training or testing word,
w, we get G words prior to as well as after w, within
the same document. These features are position in-
sensitive. The words we extract are converted back
to their morphological root forms.
Syntactic Relations We adopt the same syntactic
relations as (Lee and Ng, 2002). For easy reference,
we summarize the features into Table 1.
</bodyText>
<table confidence="0.999701166666667">
POS of w Features
Noun Parent headword h
POS of h
Relative position of h to w
Verb Left nearest child word of w, l
Right nearest child word of w, r
POS of l
POS of r
POS of w
Voice of w
Adjective Parent headword h
POS of h
</table>
<tableCaption confidence="0.999694">
Table 1: Syntactic Relations Features
</tableCaption>
<bodyText confidence="0.999167">
The exact values of P and G for each task are set
according to validation result.
</bodyText>
<subsectionHeader confidence="0.999249">
2.2 Latent Dirichlet Allocation
</subsectionHeader>
<bodyText confidence="0.99960795">
We present here the latent dirichlet allocation algo-
rithm and its inference procedures, adapted from the
original paper (Blei et al., 2003).
LDA is a probabilistic model for collections of
discrete data and has been used in document mod-
eling and text classification. It can be represented
as a three level hierarchical Bayesian model, shown
graphically in Figure 1. Given a corpus consisting of
M documents, LDA models each document using a
mixture over K topics, which are in turn character-
ized as distributions over words.
In the generative process of LDA, for each doc-
ument d we first draw the mixing proportion over
topics Bd from a Dirichlet prior with parameters α.
Next, for each of the Nd words wdn in document d, a
topic zdn is first drawn from a multinomial distribu-
tion with parameters Bd. Finally wdn is drawn from
the topic specific distribution over words. The prob-
ability of a word token w taking on value i given
that topic z = j was chosen is parameterized using
</bodyText>
<figureCaption confidence="0.986669">
Figure 1: Graphical Model for LDA
</figureCaption>
<bodyText confidence="0.853614333333333">
a matrix Q with Qij = p(w = i|z = j). Integrating
out Bd’s and zdn’s, the probability p(D|α, Q) of the
corpus is thus:
</bodyText>
<table confidence="0.614948">
M Nd �p(zdn|Bd)p(wdn|zdn, Q) dBd
H Jp(Bd|α) H 1:
d=1 (n=1 zdn
</table>
<bodyText confidence="0.999727571428571">
In variational inference, the latent variables Bd
and zdn are assumed independent and updates to
the variational posteriors for Bd and zdn are derived
(Blei et al., 2003). It can be shown that the varia-
tional posterior for Bd is a Dirichlet distribution, say
with variational parameters &apos;yd, which we shall use
in the following to construct topic features.
</bodyText>
<subsectionHeader confidence="0.993096">
2.3 Topic Features
</subsectionHeader>
<bodyText confidence="0.999581">
We first select an unlabeled corpus, such as 20
Newsgroups, and extract individual words from it
(excluding stopwords). We choose the number of
topics, K, for the unlabeled corpus and we apply the
LDA algorithm to obtain the Q parameters, where Q
represents the probability of a word w = i given a
topic z = j, p(w = i|z = j) = Qij.
The model essentially clusters words that oc-
curred in the unlabeled corpus according to K top-
ics. The conditional probability p(w = i|z = j) =
Qij is later used to tag the words in the unseen test
example with the probability of each topic.
We also use the document-specific &apos;yd parameters.
Specifically, we need to run the inference algorithm
on the labeled corpus to get &apos;yd for each document d
in the corpus. The &apos;yd parameter provides an approx-
imation to the probability of selecting topic i in the
document:
</bodyText>
<equation confidence="0.8791848">
α g
z
β
w
N
M
p(zi |d) = &apos;Ydz
&apos;y
(1)
EK &apos;ydk
</equation>
<page confidence="0.989295">
250
</page>
<sectionHeader confidence="0.99533" genericHeader="method">
3 Classifier Construction
</sectionHeader>
<bodyText confidence="0.9999255">
We construct a variant of the naive Bayes network
as shown in Figure 2. Here, w refers to the word.
s refers to the sense of the word. In training, s is
observed while in testing, it is not. The features f1
to fn are baseline features mentioned in Section 2.1
(including bag-of-words) while z refers to the la-
tent topic that we set for clustering unlabeled corpus.
The bag-of-words b are extracted from the neigh-
bours of w and there are L of them. Note that L can
be different from G, which is the number of bag-of-
words in baseline features. Both will be determined
by the validation result.
</bodyText>
<figureCaption confidence="0.949306">
Figure 2: Graphical Model with LDA feature
</figureCaption>
<bodyText confidence="0.999753">
The log-likelihood of an instance, `(w, s, F, b)
where F denotes the set of baseline features, can be
written as
</bodyText>
<equation confidence="0.997141">
= logp(w) + logp(s|w) + 1: log(p(f|s))
F
)p(zk|s)p(bl|zk) .
</equation>
<bodyText confidence="0.998838923076923">
The log p(w) term is constant and thus can be
ignored. The first portion is normal naive Bayes.
And second portion represents the additional LDA
plate. We decouple the training process into separate
stages. We first extract baseline features from the
task training data, and estimate, using normal naive
Bayes, p(s|w) and p(f|s) for all w, s and f.
Next, the parameters associated with p(b|z) are
estimated using LDA from unlabeled data, which is
β. To estimate p(z|s), we perform LDA inference
on the training corpus in order to obtain γd for each
document d. We then use the γd and β to obtain
p(z|b) for each word using
</bodyText>
<equation confidence="0.9854835">
p(zi|bl,γd) = p(bl|zi)p(zi|γd)
EK p(bl|zk)p(zk|γd),
</equation>
<bodyText confidence="0.99939475">
where equation (1) is used for estimation of p(zi|γd).
This effectively transforms b to a topical distri-
bution which we call a soft tag where each soft
tag is probability distribution t1, ... , tK on topics.
We then use this topical distribution for estimating
p(z|s). Let si be the observed sense of instance i
and tij 1 ,.. . , tijK be the soft tag of the j-th bag-of-
word feature of instance i. We estimate p(z|s) as
</bodyText>
<equation confidence="0.826664">
E Ek&apos; tij
s&apos;&apos;=s k&apos;
</equation>
<bodyText confidence="0.999846833333333">
This approach requires us to do LDA inference on
the corpus formed by the labeled training data, but
not the testing data. This is because we need γ to
get transformed topical distribution in order to learn
p(z|s) in the training. In the testing, we only apply
the learnt parameters to the model.
</bodyText>
<sectionHeader confidence="0.99684" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999428222222222">
We describe here the experimental setup on the En-
glish lexical sample task and all-words task. Note
that we do not distinguish the two all-words tasks as
the same parameters will be applied.
For lexical sample task, we use 5-fold cross val-
idation on the training data provided to determine
our parameters. For all-words task, we use SemCor
as our training data and validate on Senseval-2 and
Senseval-3 all-words test data.
We use MXPOST tagger (Adwait, 1996) for POS
tagging, Charniak parser (Charniak, 2000) for ex-
tracting syntactic relations, and David Blei’s version
of LDA1 for LDA training and inference. All default
parameters are used unless mentioned otherwise.
For the all-word tasks, we use sense 1 as back-off
for words that have not appeared in SemCor. We use
the same fine-grained system for both the coarse and
fine-grained all-words tasks. We make predictions
</bodyText>
<footnote confidence="0.807835">
1http://www.cs.princeton.edu/˜blei/lda-c/
</footnote>
<figure confidence="0.996763583333333">
f, ··· fn
� bmselsnefemtures
w
s
z
b
L
+1: log (1:
L K
Es&apos;&apos;=s tij k
p(zjk|s) =
(2)
</figure>
<page confidence="0.995752">
251
</page>
<bodyText confidence="0.999089321428571">
for all words for all the systems - precision, recall
and accuracy scores are all the same.
Baseline features For lexical sample task, we
choose P = 3 and G = 3. For all-words task, we
choose P = 3 and G = 1. (G = 1 means only the
nearest word prior and after the test word.)
Smoothing For all standard baseline features, we
use Laplace smoothing but for the soft tag (equation
(2)), we use a smoothing parameter value of 2 for
all-words task and 0.1 for lexical sample task.
Unlabeled Corpus Selection The unlabeled cor-
pus we select from for LDA training include 20
Newsgroups, Reuters, SemCor, Senseval-2 lexical
sample data, Senseval-3 lexical sample data and
SemEval-1 lexical sample data. Although the last
four are labeled corpora, we only need the words
from these corpora and thus they can be regarded as
unlabeled too. For lexical sample data, we define the
whole passage for each training and testing instance
as one document.
For lexical sample task, we use all the unlabeled
corpus mentioned with K = 60 and L = 18. For
all-words task, we use a corpora consisting only 20
Newsgroups and SemCor with K = 40 and L = 14.
Validation Result Table 2 shows the results we
get on the validation sets. We give both the system
accuracy (named as Soft Tag) and the naive Bayes
result with only standard features as baseline.
</bodyText>
<table confidence="0.999703">
Validation Set Soft Tag NB baseline
SE-2 All-words 66.3 63.7
SE-3 All-words 66.1 64.6
Lexical Sample 89.3 87.9
</table>
<tableCaption confidence="0.999493">
Table 2: Validation set results (best configuration).
</tableCaption>
<sectionHeader confidence="0.998443" genericHeader="conclusions">
5 Official Results
</sectionHeader>
<bodyText confidence="0.9974705">
We now present the official results on all three tasks
we participated in, summarized in Table 3.
The system ranked first, fourth and second in
the lexical sample task, fine-grained all-words task
and coarse-grained all-words task respectively. For
coarse-grained all-words task, we obtained 86.1,
88.3, 81.4, 76.7 and 79.1 for each document, from
d001 to d005.
</bodyText>
<table confidence="0.96679925">
Task Precision/Recall
Lexical sample(Task 17) 88.7
Fine-grained all-words(Task 17) 57.6
Course-grained all-words(Task 7) 81.6
</table>
<tableCaption confidence="0.996711">
Table 3: Official Results
</tableCaption>
<subsectionHeader confidence="0.998444">
5.1 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999295">
For the lexical sample task, we compare the re-
sults to that of our naive Bayes baseline and Sup-
port Vector Machine (SVM) (Vapnik, 1995) base-
line. Our SVM classifier (using SVMlight) follows
that of (Lee and Ng, 2002), which ranked the third
in Senseval-3 English lexical sample task. We also
analyse the result according to the test instance’s
part-of-speech and find that the improvements are
consistent for both noun and verb.
</bodyText>
<table confidence="0.9949395">
System Noun Verb Total
Soft Tag 92.7 84.2 88.7
NB baseline 91.7 83.5 87.8
SVM baseline 91.6 83.1 87.6
</table>
<tableCaption confidence="0.7462925">
Table 4: Analysis on different POS on English lexi-
cal sample task
</tableCaption>
<bodyText confidence="0.982361333333333">
Our coarse-grained all-words task result outper-
formed the first sense baseline score of 0.7889 by
about 2.7%.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999723071428572">
Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation
of Knowledge Sources and Learning Algorithms for
Word Sense Disambiguation. In Proc. of EMNLP.
D. M. Blei and A. Y. Ng and M. I. Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research.
A. Ratnaparkhi 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proc. of EMNLP.
E. Charniak 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of the 1st Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
V. N. Vapnik 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
</reference>
<page confidence="0.997436">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.686146">
<title confidence="0.999789">NUS-ML: Improving Word Sense Disambiguation Using Topic Features</title>
<author confidence="0.999872">Jun Fu Cai</author>
<author confidence="0.999872">Wee Sun Lee</author>
<affiliation confidence="0.952760666666667">Department of Computer Science National University of Singapore 3 Science Drive 2, Singapore 117543 Yee Whye Teh Gatsby Computational Neuroscience Unit University College London</affiliation>
<address confidence="0.998702">17 Queen Square, London WC1N 3AR, UK</address>
<email confidence="0.997947">ywteh@gatsby.ucl.ac.uk</email>
<abstract confidence="0.99379092">We participated in SemEval-1 English coarse-grained all-words task (task 7), English fine-grained all-words task (task 17, subtask 3) and English coarse-grained lexical sample task (task 17, subtask 1). The same method with different labeled data is used for the tasks; SemCor is the labeled corpus used to train our system for the allwords tasks while the labeled corpus that is provided is used for the lexical sample task. The knowledge sources include part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic patterns. In addition, we constructed a topic feature, targeted to capture the global context information, using the latent dirichlet allocation (LDA) algorithm with unlabeled corpus. A modified naive Bayes classifier is constructed to incorporate all the features. We achieved 81.6%, 57.6%, 88.7% for coarse-grained allwords task, fine-grained all-words task and coarse-grained lexical sample task respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y K Lee</author>
<author>H T Ng</author>
</authors>
<title>An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2979" citStr="Lee and Ng, 2002" startWordPosition="459" endWordPosition="462">an network, modified from naive Bayes model, alongside other features and train the model on the labeled corpus. 2 Feature Construction 2.1 Baseline Features For both the lexical sample and all-words tasks, we use the following standard baseline features. POS Tags For each word instance w, we include POS tags for P words prior to as well as after w within the same sentence boundary. We also include the POS tag of w. If there are fewer than P words prior or after w in the same sentence, we denote the corresponding feature as NIL. Local Collocations We adopt the same 11 collocation features as (Lee and Ng, 2002), namely C−1,−1, C1,1, C−2,−2, C2,2, C−2,−1, C−1,1, C1,2, C−3,−1, C−2,1, C−1,2, and C1,3. 249 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 249–252, Prague, June 2007. c�2007 Association for Computational Linguistics Bag-of-Words For each training or testing word, w, we get G words prior to as well as after w, within the same document. These features are position insensitive. The words we extract are converted back to their morphological root forms. Syntactic Relations We adopt the same syntactic relations as (Lee and Ng, 2002). For easy reference,</context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Y. K. Lee and H. T. Ng. 2002. An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="2071" citStr="Blei et al., 2003" startWordPosition="303" endWordPosition="306">this approach faces severe data scarcity problem, resulting features being sparsely represented in the training data. This problem is especially prominent for the bag-of-words feature. A direct consequence is that the global context information, which the bag-ofwords feature is supposed to capture, may be poorly represented. Our system tries to address this problem by clustering features to relieve the scarcity problem, specifically on the bag-of-words feature. In the process, we construct topic features, trained using the latent dirichlet allocation (LDA) algorithm. We train the topic model (Blei et al., 2003) on unlabeled data, clustering the words occurring in the corpus to a predefined number of topics. We then use the resulting topic model to tag the bag-of-words in the labeled corpus with topic distributions. We incorporate the distributions, called the topic features, using a simple Bayesian network, modified from naive Bayes model, alongside other features and train the model on the labeled corpus. 2 Feature Construction 2.1 Baseline Features For both the lexical sample and all-words tasks, we use the following standard baseline features. POS Tags For each word instance w, we include POS tag</context>
<context position="4134" citStr="Blei et al., 2003" startWordPosition="655" endWordPosition="658"> syntactic relations as (Lee and Ng, 2002). For easy reference, we summarize the features into Table 1. POS of w Features Noun Parent headword h POS of h Relative position of h to w Verb Left nearest child word of w, l Right nearest child word of w, r POS of l POS of r POS of w Voice of w Adjective Parent headword h POS of h Table 1: Syntactic Relations Features The exact values of P and G for each task are set according to validation result. 2.2 Latent Dirichlet Allocation We present here the latent dirichlet allocation algorithm and its inference procedures, adapted from the original paper (Blei et al., 2003). LDA is a probabilistic model for collections of discrete data and has been used in document modeling and text classification. It can be represented as a three level hierarchical Bayesian model, shown graphically in Figure 1. Given a corpus consisting of M documents, LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words. In the generative process of LDA, for each document d we first draw the mixing proportion over topics Bd from a Dirichlet prior with parameters α. Next, for each of the Nd words wdn in document d, a topic zdn is fi</context>
<context position="5360" citStr="Blei et al., 2003" startWordPosition="876" endWordPosition="879">n from a multinomial distribution with parameters Bd. Finally wdn is drawn from the topic specific distribution over words. The probability of a word token w taking on value i given that topic z = j was chosen is parameterized using Figure 1: Graphical Model for LDA a matrix Q with Qij = p(w = i|z = j). Integrating out Bd’s and zdn’s, the probability p(D|α, Q) of the corpus is thus: M Nd �p(zdn|Bd)p(wdn|zdn, Q) dBd H Jp(Bd|α) H 1: d=1 (n=1 zdn In variational inference, the latent variables Bd and zdn are assumed independent and updates to the variational posteriors for Bd and zdn are derived (Blei et al., 2003). It can be shown that the variational posterior for Bd is a Dirichlet distribution, say with variational parameters &apos;yd, which we shall use in the following to construct topic features. 2.3 Topic Features We first select an unlabeled corpus, such as 20 Newsgroups, and extract individual words from it (excluding stopwords). We choose the number of topics, K, for the unlabeled corpus and we apply the LDA algorithm to obtain the Q parameters, where Q represents the probability of a word w = i given a topic z = j, p(w = i|z = j) = Qij. The model essentially clusters words that occurred in the unl</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei and A. Y. Ng and M. I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A Maximum-Entropy-Inspired Parser.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9235" citStr="Charniak, 2000" startWordPosition="1573" endWordPosition="1574">) in the training. In the testing, we only apply the learnt parameters to the model. 4 Experimental Setup We describe here the experimental setup on the English lexical sample task and all-words task. Note that we do not distinguish the two all-words tasks as the same parameters will be applied. For lexical sample task, we use 5-fold cross validation on the training data provided to determine our parameters. For all-words task, we use SemCor as our training data and validate on Senseval-2 and Senseval-3 all-words test data. We use MXPOST tagger (Adwait, 1996) for POS tagging, Charniak parser (Charniak, 2000) for extracting syntactic relations, and David Blei’s version of LDA1 for LDA training and inference. All default parameters are used unless mentioned otherwise. For the all-word tasks, we use sense 1 as back-off for words that have not appeared in SemCor. We use the same fine-grained system for both the coarse and fine-grained all-words tasks. We make predictions 1http://www.cs.princeton.edu/˜blei/lda-c/ f, ··· fn � bmselsnefemtures w s z b L +1: log (1: L K Es&apos;&apos;=s tij k p(zjk|s) = (2) 251 for all words for all the systems - precision, recall and accuracy scores are all the same. Baseline fea</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak 2000. A Maximum-Entropy-Inspired Parser. In Proc. of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N</author>
</authors>
<title>Vapnik</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<marker>N, 1995</marker>
<rawString>V. N. Vapnik 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>