<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025793">
<title confidence="0.9992035">
UoW: Multi-task Learning Gaussian Process
for Semantic Textual Similarity
</title>
<author confidence="0.991287">
Miguel Rios Lucia Specia
</author>
<affiliation confidence="0.990965">
Research Group in Computational Linguistics Department of Computer Science
University of Wolverhampton University of Sheffield
</affiliation>
<address confidence="0.976189">
Stafford Street, Wolverhampton, Regent Court, 211 Portobello,
WV1 1SB, UK Sheffield, S1 4DP, UK
</address>
<email confidence="0.998816">
M.Rios@wlv.ac.uk L.Specia@sheffield.ac.uk
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999687705882353">
We report results obtained by the UoW
method in SemEval-2014’s Task 10 – Mul-
tilingual Semantic Textual Similarity. We
propose to model Semantic Textual Simi-
larity in the context of Multi-task Learning
in order to deal with inherent challenges of
the task such as unbalanced performance
across domains and the lack of training
data for some domains (i.e. unknown
domains). We show that the Multi-task
Learning approach outperforms previous
work on the 2012 dataset, achieves a ro-
bust performance on the 2013 dataset and
competitive results on the 2014 dataset.
We highlight the importance of the chal-
lenge of unknown domains, as it affects
overall performance substantially.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.959695901639345">
The task of Semantic Textual Similarity (STS)
(Agirre et al., 2012) is aimed at measuring the
degree of semantic equivalence between a pair of
texts. Natural Language Processing (NLP) ap-
plications such as Question Answering (Lin and
Pantel, 2001), Text Summarisation (Lin and Hovy,
2003) and Information Retrieval (Park et al., 2005)
rely heavily on the ability to measure semantic
similarity between pairs of texts. The STS eval-
uation campaign provides datasets that consist of
pairs of sentences from different NLP domains
such as paraphrasing, video paraphrasing, and ma-
chine translation (MT) evaluation. The participat-
ing systems are required to predict a graded simi-
larity score from 0 to 5, where a score of 0 means
that the two sentences are on different topics and
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
a score of 5 means that the two sentences have ex-
actly the same meaning.
Methods for STS are commonly based on com-
puting various types of similarity metrics between
the pair of sentences, where the similarity scores
are used as features to train regression algorithms.
B¨ar et al. (2012) use similarity metrics of vary-
ing complexity. The range of features goes from
simple string similarity metrics to complex vector
space models. The method yielded the best av-
erage results based on the official evaluation met-
rics, despite not having achieved the best results
in all individual domains. ˇSari´c et al. (2012) use a
similar set up, extracting features from similarity
metrics, where these features are based on word-
overlap and syntax similarity. The method was
among the best for domains related to paraphras-
ing. It also achieved a high correlation between
the training and test data. In contrast, for the ma-
chine translation data the performance in the test
set was lower than the one over the training data.
A possible reason for the poor results on this do-
main is the difference in length between the train-
ing and test sentences, as in the test data the pairs
tend to be short and share similar words. ˇSari´c et
al. (2012) claim that these differences show that
the MT training data is not representative of the
test set given their choice of features.
Most of the participating systems in the STS
challenges achieve good results on certain do-
mains (i.e. STS datasets), but poor results on oth-
ers. Even the most robust methods still show a big
gap in performances for different datasets. In the
second evaluation campaign of STS a new chal-
lenge was proposed: domains for which no train-
ing sets are provided, but only test sets. Heilman
and Madnani (2013) propose to incorporate do-
main adaptation techniques (Daum´e et al., 2010)
for STS to generalise models to new domains.
They add new features into the model, where the
feature set contains domain specific features plus
</bodyText>
<page confidence="0.980773">
779
</page>
<note confidence="0.729727">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 779–784,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.99978709375">
general task features. The machine learning al-
gorithm infers the extra weights of each specific
domain and of the general domain. When an in-
stance of a specific domain is to be predicted, only
the copy of the features of that domain will be ac-
tive; if the domain is unknown, the general fea-
tures will be active. Severyn et al. (2013) pro-
pose to use meta-classification to cope with do-
main adaptation. They merge each pair into a sin-
gle text and extract meta-features such as bag-of-
words and syntactic similarity scores. The meta-
classification model predicts, for each instance, its
most likely domain based on these features.
A possible solution to alleviate unbalanced per-
formances on different domains is to model STS
in the context of Multi-task Learning (MTL). The
motivation behind MTL is that by learning multi-
ple related tasks simultaneously the model perfor-
mance may improve compared to the case where
the tasks are learnt separately. MTL is based on
the assumption that related tasks can be clustered
and inter-task correlations between tasks within
the same cluster can be transferred.
We propose to model STS using MTL based
on a state-of-the-art STS feature set (ˇSari´c et al.,
2012). As algorithm we use a non-parametric
Bayesian approach, namely Gaussian Processes
(GP) (Rasmussen, 2006). We show that the MTL
model outperforms previous work on the 2012
datasets and leads to robust performance on the
2013 datasets. On the STS 2014 challenge, our
method shows competitive results.
</bodyText>
<sectionHeader confidence="0.988835" genericHeader="method">
2 Experimental Setting
</sectionHeader>
<bodyText confidence="0.999940333333333">
We apply MTL to cope with the challenge of un-
balanced performances across domains and un-
known domains present in the STS datasets.
</bodyText>
<subsectionHeader confidence="0.99354">
2.1 TakeLab Features
</subsectionHeader>
<bodyText confidence="0.9995485">
We use the features from one the top perform-
ing system in STS 2012: the TakeLab1 system,
which is publicly available. It extracts the follow-
ing types of features:
N-gram overlap is the harmonic mean of the de-
gree of matching between the first and second
texts, and vice-versa. The overlap is com-
puted for unigrams, bigrams, and trigrams.
WordNet-augmented word overlap is the par-
tial WordNet path length similarity score as-
</bodyText>
<footnote confidence="0.905003">
1http://takelab.fer.hr/sts/
</footnote>
<bodyText confidence="0.999643071428571">
signed to words that are not common to both
texts.
Vector space sentence similarity is the repre-
sentation of each text as a distributional vec-
tor by summing the distributional (i.e., LSA)
vectors of each word in the text and taking the
cosine distance between these texts vectors.
Shallow NE similarity is the matching between
Named Entities (NE) that indicates whether
they were found in both texts.
Numbers overlap is an heuristic that penalises
differences between numbers in texts.
Altogether, these features make up a vector of 21
similarity scores.
</bodyText>
<subsectionHeader confidence="0.999762">
2.2 Multi-task Gaussian Processes
</subsectionHeader>
<bodyText confidence="0.998835833333333">
Gaussian Processes (Rasmussen, 2006) is a
Bayesian non-parametric machine learning frame-
work based on kernels for regression and classifi-
cation. In GP regression, for the inputs x we want
to learn a function f that is inferred from a GP
prior:
</bodyText>
<equation confidence="0.975847">
f(x) ∼ GP(m(x), k(x, x1)), (1)
</equation>
<bodyText confidence="0.9998582">
where m(x) defines a 0 mean and k(x, x1) defines
the covariance or kernel functions. In the single
output case, the random variables are associated
to a process f evaluated at different values of the
input x. In the multiple output case, the random
variables are associated to different processes and
evaluated at different values of x.
We are interested in the intrinsic coregionaliza-
tion model for GP. A coregionalization model is
a heterotopic MTL model in which each output is
associated with a different set of inputs. In our
case the different set of inputs are the STS do-
mains (i.e. datasets). The intrinsic coregionaliza-
tion model (i.e. MTL-GP) is based on a separable
multi-task kernel ( ´Alvarez et al., 2012) of the form
</bodyText>
<equation confidence="0.838023">
K(X, X) = B ⊗ k(X, X), (2)
</equation>
<bodyText confidence="0.999620857142857">
where k(X, X) is a standard kernel over the in-
put points and B is a positive semi-definite ma-
trix encoding task covariances, called coregion-
alization matrix. B is built from other matrices
B = WWT + diag(k), where W is a matrix that
determines the correlations between the different
outputs and k is a matrix which allows the outputs
</bodyText>
<page confidence="0.969457">
780
</page>
<bodyText confidence="0.999979632653061">
(i.e. tasks) to behave independently. The repre-
sentation of data points is augmented with task ids
and given the id of a pair of data points the co-
variance from the standard kernel between them
is multiplied by a corresponding covariance from
B, which modifies the data points’ covariance de-
pending on whether they belong to the same task
or different tasks.
The coregionalization matrix B allows us to
control the amount of inter and intra task transfer
of learning among tasks. Cohn and Specia (2013)
propose different types of B matrices to model
the problem of predicting the quality of machine
translations. They developed B matrices that rep-
resent an explicit intra-task transfer to be a part of
the parameterised kernel function. We use a de-
fault B where the weights of the matrix are learnt
along with the hyper-parameters by the GP tool.
For training our method we use the GPy toolkit2
with a combination of RBF and coregionalization
kernels. The parameters used to build the core-
gionalization matrix are the number of outputs to
coregionalize and the rank of W. For example,
in the 2012 training set, the number of outputs
to coregionalize is 3, given that we have three
tasks/domains. The B matrix and the RBF kernel
hyper-parameters are jointly optimised. Each in-
stance of the training data is then augmented with
the id of their corresponding task. During test-
ing a new instance has to be matched to a specific
task/domain id from the training data. In the case
of an unknown test domain, we match it to a train-
ing domain which is similar, given the description
of the test dataset.
For the STS 2014 dataset, given the large num-
ber of training instances, we train a sparse GP
model within GPy. The main limitation of the GP
model is the that memory demands grow O(n2),
and the computational demands grow O(n3), with
n equals the number of training instances. Sparse
methods (e.g. (Titsias, 2009)) try to overcome this
limitation by constructing an approximation of the
full model on a smaller set of m support or induc-
ing instances that allow the reduction of compu-
tational demands to O(nm2). For the sparse GP
we use the same combination of kernels as the full
model, where we chose empirically the number of
inducing instances m and the GP tool randomly
selects the instances from the training data.
</bodyText>
<footnote confidence="0.829742">
2https://github.com/SheffieldML/GPy
</footnote>
<sectionHeader confidence="0.995101" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999386">
In what follows we show a comparison with previ-
ous work on the STS 2012 and 2013 datasets, and
the official results for English and Spanish STS
2014 datasets.
</bodyText>
<subsectionHeader confidence="0.998358">
3.1 STS 2012 and STS 2013
</subsectionHeader>
<bodyText confidence="0.999979863636364">
For training we use the STS 2012 training datasets
and we compare the results on the STS 2012 with
publicly available systems and with the official
Baseline, which is based on the cosine metric com-
puted over word overlaps. The official evaluation
metric is Pearson’s correlation. We match the un-
known domain OnWN to MSRpar given that the
domain of paraphrasing is that of news from the
web, which potentially contains a broad enough
vocabulary to cover OnWN.
Table 3.1 shows a comparison of the MTL-GP
with previous work on the STS 2012 data, where
our method outperforms them for most of the do-
mains. Our method improves the results of Take-
Lab with the same feature set. In other words,
the transfer learning improves over (ˇSari´c et al.,
2012), which is trained with a separate Support
Vector Regression model for each domain. We
note that we can only compare our method against
the simpler version of TakeLab that is available.
A different version using syntactic features was
also proposed, where most results do not show a
significant variation, except for an improvement
of r=0.4683 in the SMTnews dataset. For the
complete alternative results we refer the reader to
(ˇSari´c et al., 2012).
On the STS 2013 dataset, we compare our
method with work based on domain adaptation
and the official baseline. We use the 2012 data for
training as no additional training data is provided
in 2013. Table 3.1 shows all the possible match-
ing combinations between the STS 2013 test sets
and STS 2012 training sets. The best results are
given by matching the STS 2013 test sets with the
MSRvid domain, where all 2013 sets achieve their
best results.
In Table 3.1, we show the comparison with
previous work on the 2013 datasets, where we
use the best matching result from Table 3.1
(MSRvid). Our method shows very competitive
results but only with the correct matching of do-
mains, whereas the worst performed matching
(SMTeuroparl, Table 3.1) shows results that are
closer to the official Baseline. In previous work
</bodyText>
<page confidence="0.994951">
781
</page>
<table confidence="0.9986502">
Method MSRpar MSRvid SMTeuroparl SMTnews OnWN
ˇSari´c et al. (2012) 0.7343 0.8803 0.4771 0.3989 0.6797
B¨ar et al. (2012) 0.68 0.8739 0.5280 0.4937 0.6641
MTL-GP 0.7324 0.8877 0.5615 0.6053 0.7256
Baseline 0.4334 0.2996 0.4542 0.3908 0.5864
</table>
<tableCaption confidence="0.999994">
Table 1: Comparison with previous work on the STS 2012 test datasets.
</tableCaption>
<bodyText confidence="0.999634">
(Heilman and Madnani, 2013), domain adaptation
is performed with the addition of extra features
and the subsequent extra parameters to the model,
where in the MTL-GP the transfer learning is done
with the coregionalization matrix and does not de-
pend on large amounts of data.
</bodyText>
<subsectionHeader confidence="0.996163">
3.2 English STS 2014
</subsectionHeader>
<bodyText confidence="0.999978111111111">
The training dataset consists of the combination of
each English training and test STS datasets from
2012 and 2013, which results in 7 domains. For
testing, in our first run we matched similar do-
mains with each other and the unknown domain
with MSRpar. For our second run, we matched
the unknown domains with a similar one. The
domain matching (test/training) was done as fol-
lows: deft-forum/MSRpar, deft-news/SMTnews,
tweet-news/SMTnews and images/MSRvid. For
our third run, the difference in matching is for deft-
news/headlines and tweet-news/headlines, where
the other domains remain with the same match-
ing. Table 3.2 shows the official STS 2014 results
where our best method (i.e. run3) achieves rank
10.
In Table 3.2, we show the comparison of the
MTL-GP and the sparse MTL-GP with the best
2014 system (DLSCU run2). For both MTL meth-
ods we match the 2014 domains with the train-
ing domain headlines. For the sparse MTL-GP,
we chose empirically a number m of 500 ran-
domly induced points. For reference, the corre-
lation of sparse MTL-GP with 50 points on deft-
forum is r=0.4691 obtained in 0.23 hours, with
100 points, r=0.4895, with 500 points, r=0.4912,
and with 1000 points, r=0.4911. The sparse MTL-
</bodyText>
<table confidence="0.9986806">
Train MSRvid MSRpar SMTeuroparl
Test
Headlines 0.6666 0.6595 0.5693
OnWN 0.6516 0.4635 0.4113
FNWN 0.4062 0.3217 0.2344
</table>
<tableCaption confidence="0.9466185">
Table 2: Matching of new 2013 domains with
2012 training data.
</tableCaption>
<bodyText confidence="0.99933">
GP with 500 points runs in 1.38 hours, compared
to 2.39 hours for the full MTL-GP3. Addition-
ally, the sparse version achieves similar results to
the full model and very competitive performance
compared to the best STS 2014 system. However,
the result for OnWN is substantially lower than the
best system. This result can be highly improved
(r=0.7990) if the test set is matched with the cor-
respondent training domain.
</bodyText>
<subsectionHeader confidence="0.999244">
3.3 Spanish STS 2014
</subsectionHeader>
<bodyText confidence="0.999895935483871">
For the Spanish STS subtask we use both sim-
ple and state-of-the-art (SoA) features to train the
MTL-GP. The simple features are similarity scores
from string metrics such as Levenshtein, Gotoh,
Jaro, etc.4 The SoA similarity features come again
from TakeLab. The training dataset consists of the
combination of each English STS domains from
2012 and 2013 and the Spanish trial dataset with
task-id matching each instance to a given domain.
We represent the feature vectors with sparse fea-
tures for the English and Spanish training datasets,
where in English the pairs have simple and SoA
features, and for Spanish, only the simple features.
In other words, the feature vectors have the same
number of features (34): 13 simple features and 21
SoA features. However, for Spanish the SoA fea-
tures are set to 0 in training and testing. The moti-
vation to use SoA and simple features in English is
that the extra information will improve the transfer
learning on the English domains and discriminate
between the English domains and the Spanish do-
main, which only contains simple features. For
testing we only extracted the simple features; the
SoA features were set to 0. For the coregionaliza-
tion matrix we set the number of domains to be the
English STS domains from 2012 and 2013, plus
the Spanish trial, where the Spanish is treated as an
additional domain, which results in 8 domains. In
the first run of testing, we matched the test datasets
to the Spanish domain, and in the second run we
matched the datasets to the English MSRpar do-
</bodyText>
<footnote confidence="0.9997755">
3Intel Xeon(R) at 2.67GHz with 24 cores
4https://github.com/Simmetrics/simmetrics
</footnote>
<page confidence="0.981436">
782
</page>
<table confidence="0.9973002">
Method Headlines OnWN FNWN
Heilman and Madnani (2013) 0.7601 0.4631 0.3516
Severyn et al. (2013) 0.7465 0.5572 0.3875
MTL-GP 0.6666 0.6516 0.4062
Baseline 0.5399 0.2828 0.2146
</table>
<tableCaption confidence="0.9514155">
Table 3: Comparison between best matching MTL-GP (MSRvid) and previous work on the STS 2013
test datasets.
</tableCaption>
<table confidence="0.9999235">
Run deft-forum deft-news headlines images OnWN tweet-news Weighted mean rank
UoW run1 0.3419 0.7512 0.7535 0.7763 0.7990 0.7368 0.7143 11
UoW run2 0.3419 0.5875 0.7535 0.7877 0.7990 0.6281 0.6817 17
UoW run3 0.3419 0.7634 0.7535 0.7877 0.7990 0.7529 0.7207 10
</table>
<tableCaption confidence="0.999706">
Table 4: Official English STS 2014 results.
</tableCaption>
<bodyText confidence="0.9997616">
main. Table 3.3 shows the official results for the
Spanish subtask, where our method achieves com-
petitive performance, placed 7 in the systems rank-
ing. We only show the results for the first run as
both runs achieved the same performance.
</bodyText>
<tableCaption confidence="0.772957">
Table 6: Official Spanish STS 2014 results.
</tableCaption>
<bodyText confidence="0.9982804">
Table 3.3 shows the comparison of the best
Spanish STS 2014 system (UMCC DLSI run2)
against two different sparse MTL-GP matched
with the Spanish trial with 500 inducing points.
Sparse MTL-GP run1 uses the sparse features de-
scribed above, while run2 uses a modification of
the feature set consisting in specific features for
each type of domain. For the English domains
the simple features are set to 0, and for Spanish
the SoA are still set to 0. The difference between
sparse MTL-GP models is very small, where the
use of all the features on the English domains im-
proves the results. However, the performance of
both models is still substantially lower than that of
the best system.
</bodyText>
<table confidence="0.97789675">
Run Wikipedia News
UMCC DLSI run2 0.7802 0.8254
Sparse MTL-GP run1 0.7468 0.7959
Sparse MTL-GP run2 0.7380 0.7878
</table>
<tableCaption confidence="0.948745">
Table 7: Comparison of best system against sparse
MTL-GP STS 2014 results.
</tableCaption>
<sectionHeader confidence="0.999274" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999959611111111">
We propose the use of MTL for STS. We show
that MTL improves the results of one of the best
STS systems, TakeLab. However, the match-
ing of an unknown domain during testing proved
a key challenge that affects performance signifi-
cantly. Given the results of STS 2013 and 2014,
our method tends to achieve best results when
known/unknown domains are matched to the same
training domains (i.e. MSRpar for 2013 and head-
lines for 2014). The sparse MTL-GP shows sim-
ilar performance to the full GP model, but takes
half the time to be trained. In the Spanish subtask,
we train our method with English datasets and the
Spanish trial data as an additional domain. For
this subtask our method also shows competitive re-
sults. Future work involves the automatic match-
ing of unknown domains at test time via meta-
classification (Severyn et al., 2013).
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998428">
This work was supported by the Mexican National
Council for Science and Technology (CONA-
CYT), scholarship reference 309261, and by the
QTLaunchPad (EU FP7 CSA No. 296347)
project.
</bodyText>
<sectionHeader confidence="0.998529" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998227857142857">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics, SemEval ’12, pages 385–393,
Stroudsburg, PA, USA.
Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for vector-valued func-
tions: A review. Found. Trends Mach. Learn.,
4(3):195–266, March.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of the First
</reference>
<figure confidence="0.818144">
Run Wikipedia News Weighted rank
mean
UoW 0.7483 0.8001 0.7792 7
</figure>
<page confidence="0.913395">
783
</page>
<table confidence="0.9995895">
Run deft-forum deft-news headlines images OnWN tweet-news
DLSCU run2 0.4828 0.7657 0.7646 0.8214 0.8589 0.7639
Best matching MTL-GP 0.4903 0.7633 0.7535 0.8063 0.7222 0.7528
Sparse MTL-GP 0.4910 0.7642 0.7540 0.8057 0.7276 0.7539
</table>
<tableCaption confidence="0.99315">
Table 5: Comparison between best matching MTL-GP (headlines), Sparse MTL-GP and best STS 2014
</tableCaption>
<reference confidence="0.98982131372549">
system.
Joint Conference on Lexical and Computational Se-
mantics, SemEval ’12, pages 435–440, Stroudsburg,
PA, USA.
Trevor Cohn and Lucia Specia. 2013. Modelling an-
notator bias with multi-task gaussian processes: An
application to machine translation quality estima-
tion. In 51st Annual Meeting of the Association for
Computational Linguistics, ACL-2013, pages 32–
42, Sofia, Bulgaria.
Hal Daum´e, III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Pro-
cessing, DANLP 2010, pages 53–59, Stroudsburg,
PA, USA.
Michael Heilman and Nitin Madnani. 2013. Henry-
core: Domain adaptation and stacking for text simi-
larity. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), pages 96–102,
Atlanta, Georgia, USA, June.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, NAACL ’03,
pages 71–78, Stroudsburg, PA, USA.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules for question-answering. Nat. Lang.
Eng., 7(4):343–360.
Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang.
2005. Techniques for improving web retrieval effec-
tiveness. Inf. Process. Manage., 41(5):1207–1223.
Carl Edward Rasmussen. 2006. Gaussian processes
for machine learning. MIT Press.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. ikernels-core: Tree kernel learning
for textual similarity. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 53–58, Atlanta, Georgia, USA, June.
Michalis Titsias. 2009. Variational Learning of Induc-
ing Variables in Sparse Gaussian Processes. In the
12th International Conference on Artificial Intelli-
gence and Statistics (AISTATS).
Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder,
and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, SemEval ’12, pages 441–
448, Stroudsburg, PA, USA.
</reference>
<page confidence="0.998329">
784
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.842782">
<title confidence="0.9983355">UoW: Multi-task Learning Gaussian for Semantic Textual Similarity</title>
<author confidence="0.999953">Miguel Rios Lucia Specia</author>
<affiliation confidence="0.962499333333333">Research Group in Computational Linguistics Department of Computer Science University of Wolverhampton University of Sheffield Stafford Street, Wolverhampton, Regent Court, 211 Portobello,</affiliation>
<address confidence="0.999511">WV1 1SB, UK Sheffield, S1 4DP, UK</address>
<email confidence="0.964197">M.Rios@wlv.ac.ukL.Specia@sheffield.ac.uk</email>
<abstract confidence="0.999253166666667">We report results obtained by the UoW method in SemEval-2014’s Task 10 – Multilingual Semantic Textual Similarity. We propose to model Semantic Textual Similarity in the context of Multi-task Learning in order to deal with inherent challenges of the task such as unbalanced performance across domains and the lack of training data for some domains (i.e. unknown domains). We show that the Multi-task Learning approach outperforms previous work on the 2012 dataset, achieves a robust performance on the 2013 dataset and competitive results on the 2014 dataset. We highlight the importance of the challenge of unknown domains, as it affects overall performance substantially.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics, SemEval ’12,</booktitle>
<pages>385--393</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1129" citStr="Agirre et al., 2012" startWordPosition="161" endWordPosition="164">emantic Textual Similarity in the context of Multi-task Learning in order to deal with inherent challenges of the task such as unbalanced performance across domains and the lack of training data for some domains (i.e. unknown domains). We show that the Multi-task Learning approach outperforms previous work on the 2012 dataset, achieves a robust performance on the 2013 dataset and competitive results on the 2014 dataset. We highlight the importance of the challenge of unknown domains, as it affects overall performance substantially. 1 Introduction The task of Semantic Textual Similarity (STS) (Agirre et al., 2012) is aimed at measuring the degree of semantic equivalence between a pair of texts. Natural Language Processing (NLP) applications such as Question Answering (Lin and Pantel, 2001), Text Summarisation (Lin and Hovy, 2003) and Information Retrieval (Park et al., 2005) rely heavily on the ability to measure semantic similarity between pairs of texts. The STS evaluation campaign provides datasets that consist of pairs of sentences from different NLP domains such as paraphrasing, video paraphrasing, and machine translation (MT) evaluation. The participating systems are required to predict a graded </context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, SemEval ’12, pages 385–393, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauricio A ´Alvarez</author>
<author>Lorenzo Rosasco</author>
<author>Neil D Lawrence</author>
</authors>
<title>Kernels for vector-valued functions: A review.</title>
<date>2012</date>
<journal>Found. Trends Mach. Learn.,</journal>
<volume>4</volume>
<issue>3</issue>
<marker>´Alvarez, Rosasco, Lawrence, 2012</marker>
<rawString>Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. 2012. Kernels for vector-valued functions: A review. Found. Trends Mach. Learn., 4(3):195–266, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First system.</booktitle>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the First system.</rawString>
</citation>
<citation valid="false">
<booktitle>Joint Conference on Lexical and Computational Semantics, SemEval ’12,</booktitle>
<pages>435--440</pages>
<location>Stroudsburg, PA, USA.</location>
<marker></marker>
<rawString>Joint Conference on Lexical and Computational Semantics, SemEval ’12, pages 435–440, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics, ACL-2013,</booktitle>
<pages>32--42</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8752" citStr="Cohn and Specia (2013)" startWordPosition="1431" endWordPosition="1434"> a matrix that determines the correlations between the different outputs and k is a matrix which allows the outputs 780 (i.e. tasks) to behave independently. The representation of data points is augmented with task ids and given the id of a pair of data points the covariance from the standard kernel between them is multiplied by a corresponding covariance from B, which modifies the data points’ covariance depending on whether they belong to the same task or different tasks. The coregionalization matrix B allows us to control the amount of inter and intra task transfer of learning among tasks. Cohn and Specia (2013) propose different types of B matrices to model the problem of predicting the quality of machine translations. They developed B matrices that represent an explicit intra-task transfer to be a part of the parameterised kernel function. We use a default B where the weights of the matrix are learnt along with the hyper-parameters by the GP tool. For training our method we use the GPy toolkit2 with a combination of RBF and coregionalization kernels. The parameters used to build the coregionalization matrix are the number of outputs to coregionalize and the rank of W. For example, in the 2012 train</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation. In 51st Annual Meeting of the Association for Computational Linguistics, ACL-2013, pages 32– 42, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Frustratingly easy semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, DANLP 2010,</booktitle>
<pages>53--59</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>Daum´e, Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e, III, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation. In Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, DANLP 2010, pages 53–59, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Nitin Madnani</author>
</authors>
<title>Henrycore: Domain adaptation and stacking for text similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>96--102</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="3806" citStr="Heilman and Madnani (2013)" startWordPosition="606" endWordPosition="609"> as in the test data the pairs tend to be short and share similar words. ˇSari´c et al. (2012) claim that these differences show that the MT training data is not representative of the test set given their choice of features. Most of the participating systems in the STS challenges achieve good results on certain domains (i.e. STS datasets), but poor results on others. Even the most robust methods still show a big gap in performances for different datasets. In the second evaluation campaign of STS a new challenge was proposed: domains for which no training sets are provided, but only test sets. Heilman and Madnani (2013) propose to incorporate domain adaptation techniques (Daum´e et al., 2010) for STS to generalise models to new domains. They add new features into the model, where the feature set contains domain specific features plus 779 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 779–784, Dublin, Ireland, August 23-24, 2014. general task features. The machine learning algorithm infers the extra weights of each specific domain and of the general domain. When an instance of a specific domain is to be predicted, only the copy of the features of that domain will be</context>
<context position="13161" citStr="Heilman and Madnani, 2013" startWordPosition="2182" endWordPosition="2185">re we use the best matching result from Table 3.1 (MSRvid). Our method shows very competitive results but only with the correct matching of domains, whereas the worst performed matching (SMTeuroparl, Table 3.1) shows results that are closer to the official Baseline. In previous work 781 Method MSRpar MSRvid SMTeuroparl SMTnews OnWN ˇSari´c et al. (2012) 0.7343 0.8803 0.4771 0.3989 0.6797 B¨ar et al. (2012) 0.68 0.8739 0.5280 0.4937 0.6641 MTL-GP 0.7324 0.8877 0.5615 0.6053 0.7256 Baseline 0.4334 0.2996 0.4542 0.3908 0.5864 Table 1: Comparison with previous work on the STS 2012 test datasets. (Heilman and Madnani, 2013), domain adaptation is performed with the addition of extra features and the subsequent extra parameters to the model, where in the MTL-GP the transfer learning is done with the coregionalization matrix and does not depend on large amounts of data. 3.2 English STS 2014 The training dataset consists of the combination of each English training and test STS datasets from 2012 and 2013, which results in 7 domains. For testing, in our first run we matched similar domains with each other and the unknown domain with MSRpar. For our second run, we matched the unknown domains with a similar one. The do</context>
<context position="16930" citStr="Heilman and Madnani (2013)" startWordPosition="2810" endWordPosition="2813">ich only contains simple features. For testing we only extracted the simple features; the SoA features were set to 0. For the coregionalization matrix we set the number of domains to be the English STS domains from 2012 and 2013, plus the Spanish trial, where the Spanish is treated as an additional domain, which results in 8 domains. In the first run of testing, we matched the test datasets to the Spanish domain, and in the second run we matched the datasets to the English MSRpar do3Intel Xeon(R) at 2.67GHz with 24 cores 4https://github.com/Simmetrics/simmetrics 782 Method Headlines OnWN FNWN Heilman and Madnani (2013) 0.7601 0.4631 0.3516 Severyn et al. (2013) 0.7465 0.5572 0.3875 MTL-GP 0.6666 0.6516 0.4062 Baseline 0.5399 0.2828 0.2146 Table 3: Comparison between best matching MTL-GP (MSRvid) and previous work on the STS 2013 test datasets. Run deft-forum deft-news headlines images OnWN tweet-news Weighted mean rank UoW run1 0.3419 0.7512 0.7535 0.7763 0.7990 0.7368 0.7143 11 UoW run2 0.3419 0.5875 0.7535 0.7877 0.7990 0.6281 0.6817 17 UoW run3 0.3419 0.7634 0.7535 0.7877 0.7990 0.7529 0.7207 10 Table 4: Official English STS 2014 results. main. Table 3.3 shows the official results for the Spanish subtask</context>
</contexts>
<marker>Heilman, Madnani, 2013</marker>
<rawString>Michael Heilman and Nitin Madnani. 2013. Henrycore: Domain adaptation and stacking for text similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), pages 96–102, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram cooccurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>71--78</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1349" citStr="Lin and Hovy, 2003" startWordPosition="195" endWordPosition="198">nown domains). We show that the Multi-task Learning approach outperforms previous work on the 2012 dataset, achieves a robust performance on the 2013 dataset and competitive results on the 2014 dataset. We highlight the importance of the challenge of unknown domains, as it affects overall performance substantially. 1 Introduction The task of Semantic Textual Similarity (STS) (Agirre et al., 2012) is aimed at measuring the degree of semantic equivalence between a pair of texts. Natural Language Processing (NLP) applications such as Question Answering (Lin and Pantel, 2001), Text Summarisation (Lin and Hovy, 2003) and Information Retrieval (Park et al., 2005) rely heavily on the ability to measure semantic similarity between pairs of texts. The STS evaluation campaign provides datasets that consist of pairs of sentences from different NLP domains such as paraphrasing, video paraphrasing, and machine translation (MT) evaluation. The participating systems are required to predict a graded similarity score from 0 to 5, where a score of 0 means that the two sentences are on different topics and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 71–78, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question-answering.</title>
<date>2001</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1308" citStr="Lin and Pantel, 2001" startWordPosition="189" endWordPosition="192">of training data for some domains (i.e. unknown domains). We show that the Multi-task Learning approach outperforms previous work on the 2012 dataset, achieves a robust performance on the 2013 dataset and competitive results on the 2014 dataset. We highlight the importance of the challenge of unknown domains, as it affects overall performance substantially. 1 Introduction The task of Semantic Textual Similarity (STS) (Agirre et al., 2012) is aimed at measuring the degree of semantic equivalence between a pair of texts. Natural Language Processing (NLP) applications such as Question Answering (Lin and Pantel, 2001), Text Summarisation (Lin and Hovy, 2003) and Information Retrieval (Park et al., 2005) rely heavily on the ability to measure semantic similarity between pairs of texts. The STS evaluation campaign provides datasets that consist of pairs of sentences from different NLP domains such as paraphrasing, video paraphrasing, and machine translation (MT) evaluation. The participating systems are required to predict a graded similarity score from 0 to 5, where a score of 0 means that the two sentences are on different topics and This work is licensed under a Creative Commons Attribution 4.0 Internatio</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question-answering. Nat. Lang. Eng., 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eui-Kyu Park</author>
<author>Dong-Yul Ra</author>
<author>Myung-Gil Jang</author>
</authors>
<title>Techniques for improving web retrieval effectiveness.</title>
<date>2005</date>
<journal>Inf. Process. Manage.,</journal>
<volume>41</volume>
<issue>5</issue>
<contexts>
<context position="1395" citStr="Park et al., 2005" startWordPosition="202" endWordPosition="205">ning approach outperforms previous work on the 2012 dataset, achieves a robust performance on the 2013 dataset and competitive results on the 2014 dataset. We highlight the importance of the challenge of unknown domains, as it affects overall performance substantially. 1 Introduction The task of Semantic Textual Similarity (STS) (Agirre et al., 2012) is aimed at measuring the degree of semantic equivalence between a pair of texts. Natural Language Processing (NLP) applications such as Question Answering (Lin and Pantel, 2001), Text Summarisation (Lin and Hovy, 2003) and Information Retrieval (Park et al., 2005) rely heavily on the ability to measure semantic similarity between pairs of texts. The STS evaluation campaign provides datasets that consist of pairs of sentences from different NLP domains such as paraphrasing, video paraphrasing, and machine translation (MT) evaluation. The participating systems are required to predict a graded similarity score from 0 to 5, where a score of 0 means that the two sentences are on different topics and This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence d</context>
</contexts>
<marker>Park, Ra, Jang, 2005</marker>
<rawString>Eui-Kyu Park, Dong-Yul Ra, and Myung-Gil Jang. 2005. Techniques for improving web retrieval effectiveness. Inf. Process. Manage., 41(5):1207–1223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
</authors>
<title>Gaussian processes for machine learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5468" citStr="Rasmussen, 2006" startWordPosition="878" endWordPosition="879">different domains is to model STS in the context of Multi-task Learning (MTL). The motivation behind MTL is that by learning multiple related tasks simultaneously the model performance may improve compared to the case where the tasks are learnt separately. MTL is based on the assumption that related tasks can be clustered and inter-task correlations between tasks within the same cluster can be transferred. We propose to model STS using MTL based on a state-of-the-art STS feature set (ˇSari´c et al., 2012). As algorithm we use a non-parametric Bayesian approach, namely Gaussian Processes (GP) (Rasmussen, 2006). We show that the MTL model outperforms previous work on the 2012 datasets and leads to robust performance on the 2013 datasets. On the STS 2014 challenge, our method shows competitive results. 2 Experimental Setting We apply MTL to cope with the challenge of unbalanced performances across domains and unknown domains present in the STS datasets. 2.1 TakeLab Features We use the features from one the top performing system in STS 2012: the TakeLab1 system, which is publicly available. It extracts the following types of features: N-gram overlap is the harmonic mean of the degree of matching betwe</context>
<context position="6914" citStr="Rasmussen, 2006" startWordPosition="1110" endWordPosition="1111"> words that are not common to both texts. Vector space sentence similarity is the representation of each text as a distributional vector by summing the distributional (i.e., LSA) vectors of each word in the text and taking the cosine distance between these texts vectors. Shallow NE similarity is the matching between Named Entities (NE) that indicates whether they were found in both texts. Numbers overlap is an heuristic that penalises differences between numbers in texts. Altogether, these features make up a vector of 21 similarity scores. 2.2 Multi-task Gaussian Processes Gaussian Processes (Rasmussen, 2006) is a Bayesian non-parametric machine learning framework based on kernels for regression and classification. In GP regression, for the inputs x we want to learn a function f that is inferred from a GP prior: f(x) ∼ GP(m(x), k(x, x1)), (1) where m(x) defines a 0 mean and k(x, x1) defines the covariance or kernel functions. In the single output case, the random variables are associated to a process f evaluated at different values of the input x. In the multiple output case, the random variables are associated to different processes and evaluated at different values of x. We are interested in the</context>
</contexts>
<marker>Rasmussen, 2006</marker>
<rawString>Carl Edward Rasmussen. 2006. Gaussian processes for machine learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>ikernels-core: Tree kernel learning for textual similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>53--58</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="4499" citStr="Severyn et al. (2013)" startWordPosition="722" endWordPosition="725">0) for STS to generalise models to new domains. They add new features into the model, where the feature set contains domain specific features plus 779 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 779–784, Dublin, Ireland, August 23-24, 2014. general task features. The machine learning algorithm infers the extra weights of each specific domain and of the general domain. When an instance of a specific domain is to be predicted, only the copy of the features of that domain will be active; if the domain is unknown, the general features will be active. Severyn et al. (2013) propose to use meta-classification to cope with domain adaptation. They merge each pair into a single text and extract meta-features such as bag-ofwords and syntactic similarity scores. The metaclassification model predicts, for each instance, its most likely domain based on these features. A possible solution to alleviate unbalanced performances on different domains is to model STS in the context of Multi-task Learning (MTL). The motivation behind MTL is that by learning multiple related tasks simultaneously the model performance may improve compared to the case where the tasks are learnt se</context>
<context position="16973" citStr="Severyn et al. (2013)" startWordPosition="2817" endWordPosition="2820">e only extracted the simple features; the SoA features were set to 0. For the coregionalization matrix we set the number of domains to be the English STS domains from 2012 and 2013, plus the Spanish trial, where the Spanish is treated as an additional domain, which results in 8 domains. In the first run of testing, we matched the test datasets to the Spanish domain, and in the second run we matched the datasets to the English MSRpar do3Intel Xeon(R) at 2.67GHz with 24 cores 4https://github.com/Simmetrics/simmetrics 782 Method Headlines OnWN FNWN Heilman and Madnani (2013) 0.7601 0.4631 0.3516 Severyn et al. (2013) 0.7465 0.5572 0.3875 MTL-GP 0.6666 0.6516 0.4062 Baseline 0.5399 0.2828 0.2146 Table 3: Comparison between best matching MTL-GP (MSRvid) and previous work on the STS 2013 test datasets. Run deft-forum deft-news headlines images OnWN tweet-news Weighted mean rank UoW run1 0.3419 0.7512 0.7535 0.7763 0.7990 0.7368 0.7143 11 UoW run2 0.3419 0.5875 0.7535 0.7877 0.7990 0.6281 0.6817 17 UoW run3 0.3419 0.7634 0.7535 0.7877 0.7990 0.7529 0.7207 10 Table 4: Official English STS 2014 results. main. Table 3.3 shows the official results for the Spanish subtask, where our method achieves competitive per</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013. ikernels-core: Tree kernel learning for textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), pages 53–58, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michalis Titsias</author>
</authors>
<title>Variational Learning of Inducing Variables in Sparse Gaussian Processes.</title>
<date>2009</date>
<booktitle>In the 12th International Conference on Artificial Intelligence and Statistics (AISTATS).</booktitle>
<contexts>
<context position="10158" citStr="Titsias, 2009" startWordPosition="1678" endWordPosition="1679">ing data is then augmented with the id of their corresponding task. During testing a new instance has to be matched to a specific task/domain id from the training data. In the case of an unknown test domain, we match it to a training domain which is similar, given the description of the test dataset. For the STS 2014 dataset, given the large number of training instances, we train a sparse GP model within GPy. The main limitation of the GP model is the that memory demands grow O(n2), and the computational demands grow O(n3), with n equals the number of training instances. Sparse methods (e.g. (Titsias, 2009)) try to overcome this limitation by constructing an approximation of the full model on a smaller set of m support or inducing instances that allow the reduction of computational demands to O(nm2). For the sparse GP we use the same combination of kernels as the full model, where we chose empirically the number of inducing instances m and the GP tool randomly selects the instances from the training data. 2https://github.com/SheffieldML/GPy 3 Results and Discussion In what follows we show a comparison with previous work on the STS 2012 and 2013 datasets, and the official results for English and </context>
</contexts>
<marker>Titsias, 2009</marker>
<rawString>Michalis Titsias. 2009. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In the 12th International Conference on Artificial Intelligence and Statistics (AISTATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane Sari´c</author>
<author>Goran Glavas</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Ba&amp;quot;si´c.</title>
<date></date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics, SemEval ’12,</booktitle>
<pages>441--448</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>Sari´c, Glavas, Karan, </marker>
<rawString>Frane &amp;quot;Sari´c, Goran Glava&amp;quot;s, Mladen Karan, Jan &amp;quot;Snajder, and Bojana Dalbelo Ba&amp;quot;si´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, SemEval ’12, pages 441– 448, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>