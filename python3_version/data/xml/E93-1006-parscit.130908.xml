<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.454459">
Using an Annotated Corpus as a Stochastic Grammar
</title>
<author confidence="0.868895">
Rens Bod
</author>
<affiliation confidence="0.894842">
Department of Computational Linguistics
University of Amsterdam
</affiliation>
<address confidence="0.504041666666667">
Spuistraat 134
NL-1012 VB Amsterdam
rens @ alf.let.uva.n1
</address>
<sectionHeader confidence="0.979493" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99721168">
In Data Oriented Parsing (DOP), an annotated
corpus is used as a stochastic grammar. An
input string is parsed by combining subtrees
from the corpus. As a consequence, one parse
tree can usually be generated by several
derivations that involve different subtrees. This
leads to a statistics where the probability of a
parse is equal to the sum of the probabilities of
all its derivations. In (Scha, 1990) an informal
introduction to DOP is given, while (Bod,
1992a) provides a formalization of the theory.
In this paper we compare DOP with other
stochastic grammars in the context of Formal
Language Theory. It it proved that it is not
possible to create for every DOP-model a
strongly equivalent stochastic CFG which also
assigns the same probabilities to the parses.
We show that the maximum probability parse
can be estimated in polynomial time by
applying Monte Carlo techniques. The model
was tested on a set of hand-parsed strings from
the Air Travel Information System (ATIS)
spoken language corpus. Preliminary
experiments yield 96% test set parsing
accuracy.
</bodyText>
<sectionHeader confidence="0.989566" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.998885547619048">
As soon as a formal grammar characterizes a non-
trivial part of a natural language, almost every input
string of reasonable length gets an unmanageably large
number of different analyses. Since most of these
analyses are not perceived as plausible by a human
language user, there is a need for distinguishing the
plausible parse(s) of an input string from the
implausible ones. In stochastic language processing, it
is assumed that the most plausible parse of an input
string is its most probable parse. Most instantiations
of this idea estimate the probability of a parse by
assigning application probabilities to context free
rewrite rules (Jelinek, 1990), or by assigning
combination probabilities to elementary structures
(Resnik, 1992; Schabes, 1992).
There is some agreement now that context free rewrite
rules are not adequate for estimating the probability of
a parse, since they cannot capture syntactic/lexical
context, and hence cannot describe how the probability
of syntactic structures or lexical items depends on that
context. In stochastic tree-adjoining grammar
(Schabes, 1992), this lack of context-sensitivity is
overcome by assigning probabilities to larger
structural units. However, it is not always evident
which structures should be considered as elementary
structures. In (Schabes, 1992) it is proposed to infer a
stochastic TAG from a large training corpus using an
inside-outside-like iterative algorithm.
Data Oriented Parsing (DOP) (Scha, 1990; Bod,
1992a), distinguishes itself from other statistical
approaches in that it omits the step of inferring a
grammar from a corpus. Instead, an annotated corpus
is directly used as a stochastic grammar. An input
string is parsed by combining subtrees from the
corpus. In this view, every subtree can be considered
as an elementary structure. As a consequence, one
parse tree can usually be generated by several
derivations that involve different subtrees. This leads
to a statistics where the probability of a parse is equal
to the sum of the probabilities of all its derivations. It
is hoped that this approach can accommodate all
statistical properties of a language corpus.
</bodyText>
<page confidence="0.998874">
37
</page>
<bodyText confidence="0.980792">
Let us illustrate DOP with an extremely simple
example. Suppose that a corpus consists of only two
trees:
</bodyText>
<figure confidence="0.5649795">
NP VP NP VP
I I
John V NP Peter V NP
I I
</figure>
<subsectionHeader confidence="0.938596">
likes 1Viary IJan
</subsectionHeader>
<bodyText confidence="0.998770166666667">
Suppose that our combination operation (indicated
with 0) consists of substituting a subtree on the
leftmost identically labeled leaf node of another
subtree. Then the sentence Mary likes Susan can be
parsed as an S by combining the following subtrees
from the corpus.
</bodyText>
<subsectionHeader confidence="0.7438715">
NP
Susan
</subsectionHeader>
<bodyText confidence="0.9998244">
Thus, a parse can have several derivations involving
different subtrees. These derivations have different
probabilities. Using the corpus as our stochastic
grammar, we estimate the probability of substituting a
certain subtree on a specific node as the probability of
selecting this subtree among all subtrees in the corpus
that could be substituted on that node. The probability
of a derivation can be computed as the product of the
probabilities of the subtrees that are combined. For the
example derivations above, this yields:
</bodyText>
<figure confidence="0.982501263157895">
Or
NP
Past example) = 1/20 * 1/4 * 1/4 = 1/320
P(2nd example) = 1120 * 1/4 * 1/2 = 1/160
P(3rd example) = 2/20 * 1/4 * 1/8 * 1/4 = 1/1280
NP NP
NP VP Mary Susan
V NP
NP VP
I
Mary V NP
I I
likes Susan
But the same parse tree can also be derived by
combining other subtrees, for instance:
NP 0 V
Mazy Woes
NP VP
V NP
</figure>
<page confidence="0.475337">
Susan
</page>
<bodyText confidence="0.999910090909091">
This example illustrates that a statistical language
model which defines probabilities over parses by
taking into account only one &apos;derivation, does not
accommodate all statistical properties of a language
corpus. Instead, we will define the probability of a
parse as the sum of the probabilities of all its
derivations. Finally, the probability of a string is
equal to the sum of the probabilities of all its parses.
We will show that conventional parsing techniques
can be applied to DOP, but that this becomes very
inefficient, since the number of derivations of a parse
grows exponentially with the length of the input
string. However, we will show that DOP can be
parsed in polynomial time by using Monte Carlo
techniques.
An important advantage of using a corpus for
probability calculation, is that no training of
parameters is needed, as is the case for other stochastic
grammars (Jelinek et al., 1990; Pereira and Schabes,
1992; Schabes, 1992). Secondly, since we take into
account all derivations of a parse, no relationship that
might possibly be of statistical interest is ignored.
</bodyText>
<page confidence="0.999462">
38
</page>
<sectionHeader confidence="0.995461" genericHeader="introduction">
2 The Model
</sectionHeader>
<bodyText confidence="0.999801666666667">
As might be clear by now, a DOP-model is
characterized by a corpus of tree structures, together
with a set of operations that combine subtrees from
the corpus into new trees. In this section we explain
more precisely what we mean by subtree, operations
etc., in order to arrive at definitions of a parse and the
probability of a parse with respect to a corpus. For a
treatment of DOP in more formal terms we refer to
(Bod, 1992a).
</bodyText>
<subsectionHeader confidence="0.9913">
2.1 Subtree
</subsectionHeader>
<bodyText confidence="0.999024285714286">
A subtree of a tree T is a connected subgraph S of T
such that for every node in S holds that if it has
daughter nodes, then these are equal to the daughter
nodes of the corresponding node in T. It is trivial to
see that a subtree is also a tree. In the following
example Ti and T2 are subtrees of T, whereas T3
isn&apos;t.
</bodyText>
<figure confidence="0.661578666666667">
I
John V NP
I I
</figure>
<subsectionHeader confidence="0.958465">
likes Mary
</subsectionHeader>
<bodyText confidence="0.999734444444444">
are left to future research. If (and u are trees, such that
the leftmost non-terminal leaf of (is equal to the root
of u, then NJ is the tree that results from substituting
this non-terminal leaf in t by tree u. The partial
function * is called substitution. We will write
((0u)0v as t0u0v, and in general (..fftiot2)0t3)°..Mn as
t1ot2°t30...°tn. The restriction leftmost in the defin-
ition is motivated by the fact that it eliminates
different derivations consisting of the same subtrees.
</bodyText>
<subsectionHeader confidence="0.998578">
2.3 Parse
</subsectionHeader>
<bodyText confidence="0.9966375">
Tree T is a parse of input string s with respect to a
corpus C, iff the yield of Tis equal to s and there are
subtrees C such that T = tp...otn. The set
of parses of s with respect to C, is thus given by:
</bodyText>
<equation confidence="0.999692">
Parses(s,C) =
(TI yield(T) = $ A 3 e C: T = tio...01n)
</equation>
<bodyText confidence="0.999884333333333">
The definition correctly includes the trivial case of a
subtree from the corpus whose yield is equal to the
complete input string.
</bodyText>
<subsectionHeader confidence="0.99387">
2.4 Derivation
</subsectionHeader>
<bodyText confidence="0.9998735">
A derivation of a parse T with respect to a corpus C,
is a tuple of subtrees (4,...,tn) such that 4,...,47 e C
and 4 0... cfn = 7&apos;. The set of derivations of T with
respect to C, is thus given by:
</bodyText>
<equation confidence="0.79747775">
T2 vp Derivations(T,C) =
NPVP V NP ((t1,—,tn) e C A 1110...ein = T)
Al 2.5 Probability
V NP likes
</equation>
<bodyText confidence="0.997766">
The general definition above also includes subtrees
consisting of one node. Since such subtrees do not
contribute to the parsing process, we exclude these
pathological cases and consider as the set of subtrees
the non-trivial ones consisting of more than one node.
We shall use the following notation to indicate that a
tree t is a non-trivial subtree of a tree in a corpus C:
</bodyText>
<equation confidence="0.557101">
t e C def= 3 T C: t is a non-trivial subtree of T
</equation>
<subsectionHeader confidence="0.996724">
2.2 Operations
</subsectionHeader>
<bodyText confidence="0.998948">
In this article we will limit ourselves to the basic
operation of substitution. Other possible operations
</bodyText>
<subsectionHeader confidence="0.911346">
2.5.1 Subtree
</subsectionHeader>
<bodyText confidence="0.9938693">
Given a subtree tj e C, a function root that yields the
root of a tree, and a node labeled X, the conditional
probability P(t=ti I toot(t)=X) denotes the probability
that ti is substituted on X. If root(ti) X, this
probability is 0. If root(ti) = X, this probability can
be estimated as the ratio between the number of
occurrences of ti in C and the total number of
occurrences of subtrees t&apos; in C for which holds that
root(t) = X. Evidently, L P(t=ti I root(t)=X) = 1
holds.
</bodyText>
<subsectionHeader confidence="0.824304">
2.5.2 Derivation
</subsectionHeader>
<bodyText confidence="0.999765333333333">
The probability of a derivation (4,...,tn) is equal to
the probability that the subtrees ti,...,tn are combined.
This probability can be computed as the product of the
</bodyText>
<page confidence="0.996877">
39
</page>
<bodyText confidence="0.873716">
conditional probabilities of the subtrees tn. Let
1n1(x) be the leftmost non-terminal leaf of tree x, then:
</bodyText>
<equation confidence="0.7900055">
h)) =
P(t=titroot(t)=S) * /7m2 to . P(t=t; I mot(t) = In1(4_1))
</equation>
<subsectionHeader confidence="0.571778">
2.5.3 Parse
</subsectionHeader>
<bodyText confidence="0.995203111111111">
The probability of a parse is equal to the probability
that any of its derivations occurs. Since the
derivations are mutually exclusive, the probability of a
parse T is the sum of the probabilities of all its
derivations. Let Derivations(T,C) = then:
P() = A P(c1;). The conditional probability of a
parse T given input string s, can be computed as the
ratio between the probability of T and the sum of the
probabilities of all parses of S.
</bodyText>
<subsectionHeader confidence="0.775595">
2.5.4 String
</subsectionHeader>
<bodyText confidence="0.9959145">
The probability of a string is equal to the probability
that any of its parses occurs. Since the parses are
mutually exclusive, the probability of a string s can be
computed as the sum of the probabilities of all its
parses. Let Parses(s,C) = (11,...,;), then: P(s) =
Ei PaP. It can be shown that A P(s;) =1 holds.
</bodyText>
<sectionHeader confidence="0.976302" genericHeader="method">
3 Superstrong Equivalence
</sectionHeader>
<bodyText confidence="0.999700333333333">
There is an important question as to whether it is
possible to create for every DOP-model a strongly
equivalent stochastic CFG which also assigns the
same probabilities to the parses. In order to discuss
this question, we introduce the notion of superstrong
equivalence. Two stochastic grammars are called
superstrongly equivalent, if they are strongly
equivalent (i.e. they generate the same strings with the
same trees) and they generate the same probability
distribution over the trees.
The question as to whether for every DOP-model there
exists a strongly equivalent stochastic CFG, is rather
trivial, since every subtree can be decomposed into
rewrite rules describing exactly every level of
constituent structure of that subtree. The question as
to whether for every DOP-model there exists a
supeistrongly equivalent stochastic CFG, can also be
answered without too much difficulty. We shall give a
counter-example, showing that there exists a DOP-
model for which there is no superstrongly equivalent
stochastic CFG.
</bodyText>
<construct confidence="0.74685">
Proposition It is not the case that for every DOP-
model there exists a superstrongly equivalent
stochastic CFG.
</construct>
<bodyText confidence="0.707169">
Proof
Consider the following DOP-model, consisting of a
corpus with just one tree.
This corpus contains three subtrees, namely
</bodyText>
<figure confidence="0.68838675">
A I
a
ti t2 t3
The conditional probabilities of the subtrees are:
</figure>
<equation confidence="0.965813">
P(t=ti / mot(t)=S) = 1/3, P(t=t2 I root(t)=S) = 113,
P(&amp;t3 I root(t)=S) = 1/3. Thus, I P(t=t; troot(t)=S) =
</equation>
<bodyText confidence="0.9694635">
1 holds. The language generated by this model is
{abl . Let us consider the probabilities of the parses
of the strings a and ab. The parse of string a can be
generated by exactly one derivation: by applying
subtree t3. The probability of this parse is hence equal
to 1/3. The parse of ab can be generated by two
derivations: by applying subtree t1, or by combining
subtrees t2 and t3. The probability of this parse is
equal to the sum of the probabilities of its two
derivations, which is equal to P(t=tilroot(tS)
</bodyText>
<equation confidence="0.615094">
P(&amp;t2Zroot(tS) * P(t=t3lroot(t)=S) = 1/3 + 1/3*1/3
= 4/9.
</equation>
<bodyText confidence="0.999710166666667">
If we now want to construct a superstrongly equivalent
stochastic CFG, it should assign the same
probabilities to these parses. We will show that this is
impossible. A CFG which is strongly equivalent with
the DOP-model above, should contain the following
rewrite rules.
</bodyText>
<subsectionHeader confidence="0.5744055">
S Sb (1)
S a (2)
</subsectionHeader>
<bodyText confidence="0.988391333333333">
There may be other rules as well, but they should not
modify the language or structures generated by the
CFG above. Thus, the rewrite rule S —* A may be
</bodyText>
<page confidence="0.996363">
40
</page>
<bodyText confidence="0.994209888888889">
added to the rules, as well as A --&gt; B, whereas the
rewrite rule S ab may not be added.
Our problem is now whether we can assign
probabilities to these rules such that the probability of
the parse of a equals 1/3, and the probability of the
parse of ab equals 4/9. The parse of a can exhaustively
be generated by applying rule (2), while the parse of
ab can exhaustively be generated by applying rules (1)
and (2). Thus the following should hold:
</bodyText>
<equation confidence="0.9960575">
P(2) = 113
P(1)*P(2) = 4/9
</equation>
<tableCaption confidence="0.571079571428571">
This implies that P(1)*1/3 = 4/9, thus P(1) = 4/9 * 3
= 4/3. This means that the probability of rule (1)
should be larger than 1, which is not allowed. Thus,
we have proved that not for every DOP-model there
exists a superstrongly equivalent stochastic CFG. In
(Bod, 1992b) superstrong equivalence relations
between other stochastic grammars are studied.
</tableCaption>
<sectionHeader confidence="0.973944" genericHeader="method">
4 Monte Carlo Parsing
</sectionHeader>
<bodyText confidence="0.998587472222222">
It is easy to show that an input string can be parsed
with conventional parsing techniques, by applying
subtrees instead of rules to the input string (Bed,
1992a). Every subtree t can be seen as a production
rule root(t) 4 where the non-terminals of the yield
of the right hand side constitute the symbols to which
new rules/subtrees are applied. Given a polynomial
time parsing algorithm, a derivation of the input
string, and hence a parse, can be calculated in
polynomial time. But if we calculate the probability
of a parse by exhaustively calculating all its
derivations, the time complexity becomes exponential,
since the number of derivations of a parse of an input
string grows exponentially with the length of the
input string.
Nevertheless, by applying Monte Carlo techniques
(Hammersley and Handscomb, 1964), we can estimate
the probability of a parse and make its error arbitrarily
small in polynomial time. The essence of Monte
Carlo is very simple: it estimates a probability
distribution of events by taking random samples. The
larger the samples we take, the higher the reliability.
For DOP this means that, instead of exhaustively
calculating all parses with all their derivations, we
randomly calculate N parses of an input string (by
taking random samples from the subtrees that can be
substituted on a specific node in the parsing process).
The estimated probability of a certain parse given the
input string, is then equal to the number of times that
parse occurred normalized with respect to N We can
estimate a probability as accurately as we want by
choosing Nas large as we want, since according to the
Strong Law of Large Numbers the estimated
probability converges to the actual probability. From a
classical result of probability theory (Chebyshev&apos;s
inequality) it follows that the time complexity of
achieving a maximum error e is given by 0(e4). Thus
the error of probability estimation can be made
arbitrarily small in polynomial time - provided that
the parsing algorithm is not worse than polynomial.
Obviously, probable parses of an input string are more
likely to be generated than improbable ones. Thus, in
order to estimate the maximum probability parse, it
suffices to sample until stability in the top of the
parse distribution occurs. The parse which is generated
most often is then the maximum probability parse.
We now show that the probability that a certain parse
is generated by Monte Carlo, is exactly the probability
of that parse according to the DOP-model. First, the
probability that a subtree t e C is sampled at a certain
point in the parsing process (where a non-terminal X
is to be substituted) is equal to P( t I root(t) = X).
Secondly, the probability that a certain sequence
of subtrees that constitutes a derivation of a
parse T, is sampled, is equal to the product of the
conditional probabilities of these subtrees. Finally, the
probability that any sequence of subtrees that
constitutes a derivation of a certain parse T, is
sampled, is equal to the sum of the probabilities that
these derivations are sampled. This is the probability
that a certain parse T is sampled, which is equivalent
to the probability of T according to the DOP-model.
We shall call a parser which applies this Monte Carlo
technique, a Monte Carlo parser. With respect to the
theory of computation, a Monte Carlo parser is a
probabilistic algorithm which belongs to the class of
Bounded error Probabilistic Polynomial time (BPI&apos;)
algorithms. BPP-problems are characterized by the
following: it may take exponential time to solve them
exactly, but there exists an estimation algorithm with
a probability of error that becomes arbitrarily small in
polynomial time.
</bodyText>
<sectionHeader confidence="0.942408" genericHeader="evaluation">
5 Experiments on the ATIS corpus
</sectionHeader>
<subsectionHeader confidence="0.550542333333333">
For our experiments we used part-of-speech sequences
of spoken-language transcriptions from the Air Travel
Information System (ATIS) corpus (Hemphill et al.,
</subsectionHeader>
<footnote confidence="0.5663335">
1990), with the labeled-bracketings of those sequences
in the Penn Treebank (Marcus, 1991). The 750
</footnote>
<page confidence="0.999056">
41
</page>
<bodyText confidence="0.9957432">
labeled-bracketings were divided at random into a
DOP-corpus of 675 trees and a test set of 75 part-of-
speech sequences. The following tree is an example
from the DOP-corpus, where for reasons of readability
the lexical items are added to the part-of-speech tags.
</bodyText>
<table confidence="0.926163461538462">
( (S (NP *)
(VP (VB Show)
(NP (PP me))
(NP (NP (PDT all))
(DT the) (JJ nonstop) (NNS flights)
(PP (PP (IN from)
(NP (NP Dallas)))
(PP (TO to)
(NP (NP Denver))))
(ADJP (JJ early)
(PP (IN in)
(NP (DT the)
(NN morning))))))
</table>
<bodyText confidence="0.998828615384615">
The table shows that there is a relatively rapid increase
in parsing accuracy when enlarging the maximum
depth of the subtrees to 3. The accuracy keeps
increasing, at a slower rate, when the depth is enlarged
further. The highest accuracy is obtained by using all
subtrees from the corpus: 72 out of the 75 sentences
from the test set are parsed correctly.
In the following figure, parsing accuracy is plotted
against the sample size N for three of our experiments:
the experiments where the depth of the subtrees is
constrained to 2 and 3, and the experiment where the
depth is unconstrained. (The maximum depth in the
ATIS corpus is 13.)
</bodyText>
<page confidence="0.3107885">
100
75
</page>
<bodyText confidence="0.998936466666667">
As a measure for parsing accuracy we took the
percentage of the test sentences for which the
maximum probability parse derived by the Monte
Carlo parser (for a sample size N) is identical to the
Treebankparse.
It is one of the most essential features of the DOP
approach, that arbitrarily large subtrees are taken into
consideration. In order to test the usefulness of this
feature, we performed different experiments
constraining the depth of the subtrees. The depth of a
tree is defined as the length of its longest path. The
following table shows the results of seven
experiments. The accuracy refers to the parsing
accuracy at sample size N= 100, and is rounded off to
the nearest integer.
</bodyText>
<figure confidence="0.519144">
depth accumcy
52 87%
53 92%
54 93%
55 93%
56 95%
57 95%
unbounded 96%
Parsing accuracy for the ATIS corpus, sample size N= 100.
25 50 715 100
sample size N
</figure>
<bodyText confidence="0.996757421052632">
Parsing accuracy for the ATIS corpus, with depth 5 2, with
depth 5 3 and with unbounded depth.
In (Pereira and Schabes, 1992), 90.36% bracketing
accuracy was reported using a stochastic CFG trained
on bracketings from the ATIS corpus. Though we
cannot make a direct comparison, our pilot experiment
suggests that our model may have better performance
than a stochastic CFG. However, there is still an error
rate of 4%. Although there is no reason to expect
100% accuracy in the absence of any semantic or
pragmatic analysis, it seems that the accuracy might
be further improved. Three limitations of the current
experiments are worth mentioning,
First, the Treebank annotations are not rich enough.
Although the Treebank uses a relatively rich part-of-
speech system (48 terminal symbols), there are only
15 non-terminal symbols. Especially the internal
structure of noun phrases is very poor. Semantic
annotations are completely absent.
</bodyText>
<page confidence="0.996995">
42
</page>
<bodyText confidence="0.999524611111111">
Secondly, it could be that subtrees which occur only
once in the corpus, give bad estimations of their actual
probabilities. The question as to whether reestimation
techniques would further improve the accuracy, must
be considered in future research.
Thirdly, it could be that our corpus is not large
enough. This brings us to the question as to how
much parsing accuracy depends on the size of the
corpus. For studying this question, we performed
additional experiments with different corpus sizes.
Starting with a corpus of only 50 parse trees
(randomly chosen from the initial DOP-corpus of 675
trees), we increased its size with intervals of 50. As
our test set, we took the same 75 p-o-s sequences as
used in the previous experiments. In the next figure
the parsing accuracy, for sample size N = 100, is
plotted against the corpus size, using all corpus
subtrees.
</bodyText>
<figure confidence="0.997125857142857">
0 0
0 0
0 0
0 0
0
25.
0
</figure>
<bodyText confidence="0.908900875">
Parsing accuracy for the MIS corpus, with unbounded
depth.
The figure shows the increase in parsing accuracy. For
a corpus size of 450 trees, the accuracy reaches already
88%. After this, the growth decreases, but the accuracy
is still growing at corpus size 675. Thus, we would
expect a higher accuracy if the corpus is further
enlarged.
</bodyText>
<sectionHeader confidence="0.999152" genericHeader="conclusions">
6 Conclusions and Future Research
</sectionHeader>
<bodyText confidence="0.999982708333333">
We have presented a language model that uses an
annotated corpus as a stochastic grammar. We
restricted ourselves to substitution as the only
combination operation between corpus subtrees. A
statistical parsing theory was developed, where one
parse can be generated by different derivations, and
where the probability of a parse is computed as the
sum of the probabilities of all its derivations. It was
shown that our model cannot always be described by a
stochastic CFG. It turned out that the maximum
probability parse can be estimated as accurately as
desired in polynomial time by using Monte Carlo
techniques. The method has been succesfully tested on
a set of part-of-speech sequences derived from the
ATIS corpus. It turned out that parsing accuracy
improved if larger subtrees were used.
We would like to extend our experiments to larger
corpora, like the Wall Street Journal corpus. This
might raise computational problems, since the number
of subtrees becomes extremely large. Furthermore, in
order to tackle the problem of data sparseness, the
possibility of abstracting from corpus data should be
included, but statistical models of abstractions of
features and categories are not yet available.
</bodyText>
<sectionHeader confidence="0.996536" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99994025">
The author is very much indebted to Remko Scha for
many valuable comments on earlier versions of this
paper. The author is also grateful to Mitch Marcus for
supplying the ATIS corpus.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998419235294118">
R. Bod, 1992a. &amp;quot;A Computational Model of
Language Performance: Data Oriented Parsing&amp;quot;,
Proceedings COLING92, Nantes.
it Bod, 1992b. &amp;quot;Mathematical Properties of the Data
Oriented Parsing Model&amp;quot;, paper presented at the 77rird
Meeting on Mathematics of Language (MOL3),
Austin, Texas.
J.M. Hammersley and D.C. Handscomb, 1964. Monte
Carlo Methods, Chapman and Hall, London.
C.T. Hemphill, J.J. Godfrey and G.R. Doddington,
1990. &amp;quot;The ATIS spoken language systems pilot
corpus&amp;quot;. DARPA Speech and Natural Language
Workshop, Hidden Valley, Morgan Kaufmann.
F. Jelinek, J.D. Lafferty and R.L. Mercer, 1990. Basic
Methods of Probabilistic Context Free Grammars,
Technical Report IBM RC 16374 (#72684), Yorktown
Heights.
</reference>
<figure confidence="0.949059333333333">
100
leo 210 300 400 ?co 600 675
corpus size
</figure>
<page confidence="0.999282">
43
</page>
<reference confidence="0.99960675">
M. Marcus, 1991. &amp;quot;Very Large Annotated Database of
American English&amp;quot;. DARPA Speech and Natural
Language Wodcshop, Pacific Grove, Morgan
1Caufmann.
F. Pereira and Y. Schabes, 1992. &amp;quot;Inside-Outside
Reestimation from Partially Bracketed Corpora&amp;quot;,
Proceedings AC/. 92, Newark.
P. Resnik, 1992. &amp;quot;Probabilistic Tree-Adjoining
Grammar as a Framework for Statistical Natural
Language Processing&amp;quot;, Proceedings COLING&apos;92
Nantes.
R. Scha, 1990. &amp;quot;Language Theory and Language
Technology; Competence and Performance&amp;quot; (in
Dutch), in Q.A.M. de Kort &amp; G.L.J. Leeniam (eds.),
Computertoepassingen in de Neerlandistiek, Almere:
Landelijke Vereniging van Neerlandici (LVVN-
jaarboek).
Y. Schabes, 1992. &amp;quot;Stochastic Lexicalized Tree-
Adjoining Grammars&amp;quot;, Proceedings COLING&apos;92
Nantes.
</reference>
<page confidence="0.999291">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.428773">
<title confidence="0.999899">Using an Annotated Corpus as a Stochastic Grammar</title>
<author confidence="0.976881">Rens Bod</author>
<affiliation confidence="0.9995885">Department of Computational Linguistics University of Amsterdam</affiliation>
<address confidence="0.801806666666667">Spuistraat 134 NL-1012 VB Amsterdam rens @ alf.let.uva.n1</address>
<abstract confidence="0.9924916">In Data Oriented Parsing (DOP), an annotated corpus is used as a stochastic grammar. An input string is parsed by combining subtrees from the corpus. As a consequence, one parse tree can usually be generated by several derivations that involve different subtrees. This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations. In (Scha, 1990) an informal</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>A Computational Model of Language Performance: Data Oriented Parsing&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings COLING92,</booktitle>
<location>Nantes.</location>
<contexts>
<context position="653" citStr="Bod, 1992" startWordPosition="102" endWordPosition="103">ammar Rens Bod Department of Computational Linguistics University of Amsterdam Spuistraat 134 NL-1012 VB Amsterdam rens @ alf.let.uva.n1 Abstract In Data Oriented Parsing (DOP), an annotated corpus is used as a stochastic grammar. An input string is parsed by combining subtrees from the corpus. As a consequence, one parse tree can usually be generated by several derivations that involve different subtrees. This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations. In (Scha, 1990) an informal introduction to DOP is given, while (Bod, 1992a) provides a formalization of the theory. In this paper we compare DOP with other stochastic grammars in the context of Formal Language Theory. It it proved that it is not possible to create for every DOP-model a strongly equivalent stochastic CFG which also assigns the same probabilities to the parses. We show that the maximum probability parse can be estimated in polynomial time by applying Monte Carlo techniques. The model was tested on a set of hand-parsed strings from the Air Travel Information System (ATIS) spoken language corpus. Preliminary experiments yield 96% test set parsing accur</context>
<context position="2753" citStr="Bod, 1992" startWordPosition="422" endWordPosition="423">e, since they cannot capture syntactic/lexical context, and hence cannot describe how the probability of syntactic structures or lexical items depends on that context. In stochastic tree-adjoining grammar (Schabes, 1992), this lack of context-sensitivity is overcome by assigning probabilities to larger structural units. However, it is not always evident which structures should be considered as elementary structures. In (Schabes, 1992) it is proposed to infer a stochastic TAG from a large training corpus using an inside-outside-like iterative algorithm. Data Oriented Parsing (DOP) (Scha, 1990; Bod, 1992a), distinguishes itself from other statistical approaches in that it omits the step of inferring a grammar from a corpus. Instead, an annotated corpus is directly used as a stochastic grammar. An input string is parsed by combining subtrees from the corpus. In this view, every subtree can be considered as an elementary structure. As a consequence, one parse tree can usually be generated by several derivations that involve different subtrees. This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations. It is hoped that this approa</context>
<context position="6255" citStr="Bod, 1992" startWordPosition="1030" endWordPosition="1031">bes, 1992; Schabes, 1992). Secondly, since we take into account all derivations of a parse, no relationship that might possibly be of statistical interest is ignored. 38 2 The Model As might be clear by now, a DOP-model is characterized by a corpus of tree structures, together with a set of operations that combine subtrees from the corpus into new trees. In this section we explain more precisely what we mean by subtree, operations etc., in order to arrive at definitions of a parse and the probability of a parse with respect to a corpus. For a treatment of DOP in more formal terms we refer to (Bod, 1992a). 2.1 Subtree A subtree of a tree T is a connected subgraph S of T such that for every node in S holds that if it has daughter nodes, then these are equal to the daughter nodes of the corresponding node in T. It is trivial to see that a subtree is also a tree. In the following example Ti and T2 are subtrees of T, whereas T3 isn&apos;t. I John V NP I I likes Mary are left to future research. If (and u are trees, such that the leftmost non-terminal leaf of (is equal to the root of u, then NJ is the tree that results from substituting this non-terminal leaf in t by tree u. The partial function * is </context>
<context position="13218" citStr="Bod, 1992" startWordPosition="2281" endWordPosition="2282">lities to these rules such that the probability of the parse of a equals 1/3, and the probability of the parse of ab equals 4/9. The parse of a can exhaustively be generated by applying rule (2), while the parse of ab can exhaustively be generated by applying rules (1) and (2). Thus the following should hold: P(2) = 113 P(1)*P(2) = 4/9 This implies that P(1)*1/3 = 4/9, thus P(1) = 4/9 * 3 = 4/3. This means that the probability of rule (1) should be larger than 1, which is not allowed. Thus, we have proved that not for every DOP-model there exists a superstrongly equivalent stochastic CFG. In (Bod, 1992b) superstrong equivalence relations between other stochastic grammars are studied. 4 Monte Carlo Parsing It is easy to show that an input string can be parsed with conventional parsing techniques, by applying subtrees instead of rules to the input string (Bed, 1992a). Every subtree t can be seen as a production rule root(t) 4 where the non-terminals of the yield of the right hand side constitute the symbols to which new rules/subtrees are applied. Given a polynomial time parsing algorithm, a derivation of the input string, and hence a parse, can be calculated in polynomial time. But if we cal</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>R. Bod, 1992a. &amp;quot;A Computational Model of Language Performance: Data Oriented Parsing&amp;quot;, Proceedings COLING92, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>it Bod</author>
</authors>
<title>Mathematical Properties of the Data Oriented Parsing Model&amp;quot;, paper presented at the 77rird Meeting on Mathematics of Language (MOL3),</title>
<date>1992</date>
<location>Austin, Texas.</location>
<contexts>
<context position="653" citStr="Bod, 1992" startWordPosition="102" endWordPosition="103">ammar Rens Bod Department of Computational Linguistics University of Amsterdam Spuistraat 134 NL-1012 VB Amsterdam rens @ alf.let.uva.n1 Abstract In Data Oriented Parsing (DOP), an annotated corpus is used as a stochastic grammar. An input string is parsed by combining subtrees from the corpus. As a consequence, one parse tree can usually be generated by several derivations that involve different subtrees. This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations. In (Scha, 1990) an informal introduction to DOP is given, while (Bod, 1992a) provides a formalization of the theory. In this paper we compare DOP with other stochastic grammars in the context of Formal Language Theory. It it proved that it is not possible to create for every DOP-model a strongly equivalent stochastic CFG which also assigns the same probabilities to the parses. We show that the maximum probability parse can be estimated in polynomial time by applying Monte Carlo techniques. The model was tested on a set of hand-parsed strings from the Air Travel Information System (ATIS) spoken language corpus. Preliminary experiments yield 96% test set parsing accur</context>
<context position="2753" citStr="Bod, 1992" startWordPosition="422" endWordPosition="423">e, since they cannot capture syntactic/lexical context, and hence cannot describe how the probability of syntactic structures or lexical items depends on that context. In stochastic tree-adjoining grammar (Schabes, 1992), this lack of context-sensitivity is overcome by assigning probabilities to larger structural units. However, it is not always evident which structures should be considered as elementary structures. In (Schabes, 1992) it is proposed to infer a stochastic TAG from a large training corpus using an inside-outside-like iterative algorithm. Data Oriented Parsing (DOP) (Scha, 1990; Bod, 1992a), distinguishes itself from other statistical approaches in that it omits the step of inferring a grammar from a corpus. Instead, an annotated corpus is directly used as a stochastic grammar. An input string is parsed by combining subtrees from the corpus. In this view, every subtree can be considered as an elementary structure. As a consequence, one parse tree can usually be generated by several derivations that involve different subtrees. This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations. It is hoped that this approa</context>
<context position="6255" citStr="Bod, 1992" startWordPosition="1030" endWordPosition="1031">bes, 1992; Schabes, 1992). Secondly, since we take into account all derivations of a parse, no relationship that might possibly be of statistical interest is ignored. 38 2 The Model As might be clear by now, a DOP-model is characterized by a corpus of tree structures, together with a set of operations that combine subtrees from the corpus into new trees. In this section we explain more precisely what we mean by subtree, operations etc., in order to arrive at definitions of a parse and the probability of a parse with respect to a corpus. For a treatment of DOP in more formal terms we refer to (Bod, 1992a). 2.1 Subtree A subtree of a tree T is a connected subgraph S of T such that for every node in S holds that if it has daughter nodes, then these are equal to the daughter nodes of the corresponding node in T. It is trivial to see that a subtree is also a tree. In the following example Ti and T2 are subtrees of T, whereas T3 isn&apos;t. I John V NP I I likes Mary are left to future research. If (and u are trees, such that the leftmost non-terminal leaf of (is equal to the root of u, then NJ is the tree that results from substituting this non-terminal leaf in t by tree u. The partial function * is </context>
<context position="13218" citStr="Bod, 1992" startWordPosition="2281" endWordPosition="2282">lities to these rules such that the probability of the parse of a equals 1/3, and the probability of the parse of ab equals 4/9. The parse of a can exhaustively be generated by applying rule (2), while the parse of ab can exhaustively be generated by applying rules (1) and (2). Thus the following should hold: P(2) = 113 P(1)*P(2) = 4/9 This implies that P(1)*1/3 = 4/9, thus P(1) = 4/9 * 3 = 4/3. This means that the probability of rule (1) should be larger than 1, which is not allowed. Thus, we have proved that not for every DOP-model there exists a superstrongly equivalent stochastic CFG. In (Bod, 1992b) superstrong equivalence relations between other stochastic grammars are studied. 4 Monte Carlo Parsing It is easy to show that an input string can be parsed with conventional parsing techniques, by applying subtrees instead of rules to the input string (Bed, 1992a). Every subtree t can be seen as a production rule root(t) 4 where the non-terminals of the yield of the right hand side constitute the symbols to which new rules/subtrees are applied. Given a polynomial time parsing algorithm, a derivation of the input string, and hence a parse, can be calculated in polynomial time. But if we cal</context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>it Bod, 1992b. &amp;quot;Mathematical Properties of the Data Oriented Parsing Model&amp;quot;, paper presented at the 77rird Meeting on Mathematics of Language (MOL3), Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Hammersley</author>
<author>D C Handscomb</author>
</authors>
<title>Monte Carlo Methods,</title>
<date>1964</date>
<publisher>Chapman and Hall,</publisher>
<location>London.</location>
<contexts>
<context position="14142" citStr="Hammersley and Handscomb, 1964" startWordPosition="2425" endWordPosition="2428">seen as a production rule root(t) 4 where the non-terminals of the yield of the right hand side constitute the symbols to which new rules/subtrees are applied. Given a polynomial time parsing algorithm, a derivation of the input string, and hence a parse, can be calculated in polynomial time. But if we calculate the probability of a parse by exhaustively calculating all its derivations, the time complexity becomes exponential, since the number of derivations of a parse of an input string grows exponentially with the length of the input string. Nevertheless, by applying Monte Carlo techniques (Hammersley and Handscomb, 1964), we can estimate the probability of a parse and make its error arbitrarily small in polynomial time. The essence of Monte Carlo is very simple: it estimates a probability distribution of events by taking random samples. The larger the samples we take, the higher the reliability. For DOP this means that, instead of exhaustively calculating all parses with all their derivations, we randomly calculate N parses of an input string (by taking random samples from the subtrees that can be substituted on a specific node in the parsing process). The estimated probability of a certain parse given the in</context>
</contexts>
<marker>Hammersley, Handscomb, 1964</marker>
<rawString>J.M. Hammersley and D.C. Handscomb, 1964. Monte Carlo Methods, Chapman and Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Hemphill</author>
<author>J J Godfrey</author>
<author>G R Doddington</author>
</authors>
<title>The ATIS spoken language systems pilot corpus&amp;quot;. DARPA Speech and Natural Language Workshop,</title>
<date>1990</date>
<publisher>Morgan Kaufmann.</publisher>
<location>Hidden Valley,</location>
<contexts>
<context position="17217" citStr="Hemphill et al., 1990" startWordPosition="2929" endWordPosition="2932">onte Carlo parser. With respect to the theory of computation, a Monte Carlo parser is a probabilistic algorithm which belongs to the class of Bounded error Probabilistic Polynomial time (BPI&apos;) algorithms. BPP-problems are characterized by the following: it may take exponential time to solve them exactly, but there exists an estimation algorithm with a probability of error that becomes arbitrarily small in polynomial time. 5 Experiments on the ATIS corpus For our experiments we used part-of-speech sequences of spoken-language transcriptions from the Air Travel Information System (ATIS) corpus (Hemphill et al., 1990), with the labeled-bracketings of those sequences in the Penn Treebank (Marcus, 1991). The 750 41 labeled-bracketings were divided at random into a DOP-corpus of 675 trees and a test set of 75 part-ofspeech sequences. The following tree is an example from the DOP-corpus, where for reasons of readability the lexical items are added to the part-of-speech tags. ( (S (NP *) (VP (VB Show) (NP (PP me)) (NP (NP (PDT all)) (DT the) (JJ nonstop) (NNS flights) (PP (PP (IN from) (NP (NP Dallas))) (PP (TO to) (NP (NP Denver)))) (ADJP (JJ early) (PP (IN in) (NP (DT the) (NN morning)))))) The table shows th</context>
</contexts>
<marker>Hemphill, Godfrey, Doddington, 1990</marker>
<rawString>C.T. Hemphill, J.J. Godfrey and G.R. Doddington, 1990. &amp;quot;The ATIS spoken language systems pilot corpus&amp;quot;. DARPA Speech and Natural Language Workshop, Hidden Valley, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Basic Methods of Probabilistic Context Free Grammars,</title>
<date>1990</date>
<tech>Technical Report IBM RC 16374 (#72684),</tech>
<location>Yorktown Heights.</location>
<contexts>
<context position="5628" citStr="Jelinek et al., 1990" startWordPosition="917" endWordPosition="920">ilities of all its derivations. Finally, the probability of a string is equal to the sum of the probabilities of all its parses. We will show that conventional parsing techniques can be applied to DOP, but that this becomes very inefficient, since the number of derivations of a parse grows exponentially with the length of the input string. However, we will show that DOP can be parsed in polynomial time by using Monte Carlo techniques. An important advantage of using a corpus for probability calculation, is that no training of parameters is needed, as is the case for other stochastic grammars (Jelinek et al., 1990; Pereira and Schabes, 1992; Schabes, 1992). Secondly, since we take into account all derivations of a parse, no relationship that might possibly be of statistical interest is ignored. 38 2 The Model As might be clear by now, a DOP-model is characterized by a corpus of tree structures, together with a set of operations that combine subtrees from the corpus into new trees. In this section we explain more precisely what we mean by subtree, operations etc., in order to arrive at definitions of a parse and the probability of a parse with respect to a corpus. For a treatment of DOP in more formal t</context>
</contexts>
<marker>Jelinek, Lafferty, Mercer, 1990</marker>
<rawString>F. Jelinek, J.D. Lafferty and R.L. Mercer, 1990. Basic Methods of Probabilistic Context Free Grammars, Technical Report IBM RC 16374 (#72684), Yorktown Heights.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>Very Large Annotated Database of</title>
<date>1991</date>
<contexts>
<context position="17302" citStr="Marcus, 1991" startWordPosition="2943" endWordPosition="2944">ilistic algorithm which belongs to the class of Bounded error Probabilistic Polynomial time (BPI&apos;) algorithms. BPP-problems are characterized by the following: it may take exponential time to solve them exactly, but there exists an estimation algorithm with a probability of error that becomes arbitrarily small in polynomial time. 5 Experiments on the ATIS corpus For our experiments we used part-of-speech sequences of spoken-language transcriptions from the Air Travel Information System (ATIS) corpus (Hemphill et al., 1990), with the labeled-bracketings of those sequences in the Penn Treebank (Marcus, 1991). The 750 41 labeled-bracketings were divided at random into a DOP-corpus of 675 trees and a test set of 75 part-ofspeech sequences. The following tree is an example from the DOP-corpus, where for reasons of readability the lexical items are added to the part-of-speech tags. ( (S (NP *) (VP (VB Show) (NP (PP me)) (NP (NP (PDT all)) (DT the) (JJ nonstop) (NNS flights) (PP (PP (IN from) (NP (NP Dallas))) (PP (TO to) (NP (NP Denver)))) (ADJP (JJ early) (PP (IN in) (NP (DT the) (NN morning)))))) The table shows that there is a relatively rapid increase in parsing accuracy when enlarging the maximu</context>
</contexts>
<marker>Marcus, 1991</marker>
<rawString>M. Marcus, 1991. &amp;quot;Very Large Annotated Database of</rawString>
</citation>
<citation valid="true">
<authors>
<author>American English</author>
</authors>
<title>DARPA Speech and Natural Language Wodcshop,</title>
<date></date>
<location>Pacific Grove, Morgan</location>
<marker>English, </marker>
<rawString>American English&amp;quot;. DARPA Speech and Natural Language Wodcshop, Pacific Grove, Morgan 1Caufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Schabes</author>
</authors>
<title>Inside-Outside Reestimation from Partially Bracketed Corpora&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings AC/. 92,</booktitle>
<location>Newark.</location>
<contexts>
<context position="5655" citStr="Pereira and Schabes, 1992" startWordPosition="921" endWordPosition="924">ivations. Finally, the probability of a string is equal to the sum of the probabilities of all its parses. We will show that conventional parsing techniques can be applied to DOP, but that this becomes very inefficient, since the number of derivations of a parse grows exponentially with the length of the input string. However, we will show that DOP can be parsed in polynomial time by using Monte Carlo techniques. An important advantage of using a corpus for probability calculation, is that no training of parameters is needed, as is the case for other stochastic grammars (Jelinek et al., 1990; Pereira and Schabes, 1992; Schabes, 1992). Secondly, since we take into account all derivations of a parse, no relationship that might possibly be of statistical interest is ignored. 38 2 The Model As might be clear by now, a DOP-model is characterized by a corpus of tree structures, together with a set of operations that combine subtrees from the corpus into new trees. In this section we explain more precisely what we mean by subtree, operations etc., in order to arrive at definitions of a parse and the probability of a parse with respect to a corpus. For a treatment of DOP in more formal terms we refer to (Bod, 1992</context>
<context position="19421" citStr="Pereira and Schabes, 1992" startWordPosition="3318" endWordPosition="3321">t the usefulness of this feature, we performed different experiments constraining the depth of the subtrees. The depth of a tree is defined as the length of its longest path. The following table shows the results of seven experiments. The accuracy refers to the parsing accuracy at sample size N= 100, and is rounded off to the nearest integer. depth accumcy 52 87% 53 92% 54 93% 55 93% 56 95% 57 95% unbounded 96% Parsing accuracy for the ATIS corpus, sample size N= 100. 25 50 715 100 sample size N Parsing accuracy for the ATIS corpus, with depth 5 2, with depth 5 3 and with unbounded depth. In (Pereira and Schabes, 1992), 90.36% bracketing accuracy was reported using a stochastic CFG trained on bracketings from the ATIS corpus. Though we cannot make a direct comparison, our pilot experiment suggests that our model may have better performance than a stochastic CFG. However, there is still an error rate of 4%. Although there is no reason to expect 100% accuracy in the absence of any semantic or pragmatic analysis, it seems that the accuracy might be further improved. Three limitations of the current experiments are worth mentioning, First, the Treebank annotations are not rich enough. Although the Treebank uses</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>F. Pereira and Y. Schabes, 1992. &amp;quot;Inside-Outside Reestimation from Partially Bracketed Corpora&amp;quot;, Proceedings AC/. 92, Newark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings COLING&apos;92</booktitle>
<location>Nantes.</location>
<contexts>
<context position="2009" citStr="Resnik, 1992" startWordPosition="315" endWordPosition="316">gth gets an unmanageably large number of different analyses. Since most of these analyses are not perceived as plausible by a human language user, there is a need for distinguishing the plausible parse(s) of an input string from the implausible ones. In stochastic language processing, it is assumed that the most plausible parse of an input string is its most probable parse. Most instantiations of this idea estimate the probability of a parse by assigning application probabilities to context free rewrite rules (Jelinek, 1990), or by assigning combination probabilities to elementary structures (Resnik, 1992; Schabes, 1992). There is some agreement now that context free rewrite rules are not adequate for estimating the probability of a parse, since they cannot capture syntactic/lexical context, and hence cannot describe how the probability of syntactic structures or lexical items depends on that context. In stochastic tree-adjoining grammar (Schabes, 1992), this lack of context-sensitivity is overcome by assigning probabilities to larger structural units. However, it is not always evident which structures should be considered as elementary structures. In (Schabes, 1992) it is proposed to infer a </context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>P. Resnik, 1992. &amp;quot;Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing&amp;quot;, Proceedings COLING&apos;92 Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Scha</author>
</authors>
<title>Language Theory and Language Technology; Competence and Performance&amp;quot;</title>
<date>1990</date>
<booktitle>Computertoepassingen in de Neerlandistiek, Almere: Landelijke Vereniging</booktitle>
<editor>(in Dutch), in Q.A.M. de Kort &amp; G.L.J. Leeniam (eds.),</editor>
<contexts>
<context position="2742" citStr="Scha, 1990" startWordPosition="420" endWordPosition="421">ty of a parse, since they cannot capture syntactic/lexical context, and hence cannot describe how the probability of syntactic structures or lexical items depends on that context. In stochastic tree-adjoining grammar (Schabes, 1992), this lack of context-sensitivity is overcome by assigning probabilities to larger structural units. However, it is not always evident which structures should be considered as elementary structures. In (Schabes, 1992) it is proposed to infer a stochastic TAG from a large training corpus using an inside-outside-like iterative algorithm. Data Oriented Parsing (DOP) (Scha, 1990; Bod, 1992a), distinguishes itself from other statistical approaches in that it omits the step of inferring a grammar from a corpus. Instead, an annotated corpus is directly used as a stochastic grammar. An input string is parsed by combining subtrees from the corpus. In this view, every subtree can be considered as an elementary structure. As a consequence, one parse tree can usually be generated by several derivations that involve different subtrees. This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations. It is hoped that </context>
</contexts>
<marker>Scha, 1990</marker>
<rawString>R. Scha, 1990. &amp;quot;Language Theory and Language Technology; Competence and Performance&amp;quot; (in Dutch), in Q.A.M. de Kort &amp; G.L.J. Leeniam (eds.), Computertoepassingen in de Neerlandistiek, Almere: Landelijke Vereniging van Neerlandici (LVVNjaarboek).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Stochastic Lexicalized TreeAdjoining Grammars&amp;quot;,</title>
<date>1992</date>
<booktitle>Proceedings COLING&apos;92</booktitle>
<location>Nantes.</location>
<contexts>
<context position="2025" citStr="Schabes, 1992" startWordPosition="317" endWordPosition="318">manageably large number of different analyses. Since most of these analyses are not perceived as plausible by a human language user, there is a need for distinguishing the plausible parse(s) of an input string from the implausible ones. In stochastic language processing, it is assumed that the most plausible parse of an input string is its most probable parse. Most instantiations of this idea estimate the probability of a parse by assigning application probabilities to context free rewrite rules (Jelinek, 1990), or by assigning combination probabilities to elementary structures (Resnik, 1992; Schabes, 1992). There is some agreement now that context free rewrite rules are not adequate for estimating the probability of a parse, since they cannot capture syntactic/lexical context, and hence cannot describe how the probability of syntactic structures or lexical items depends on that context. In stochastic tree-adjoining grammar (Schabes, 1992), this lack of context-sensitivity is overcome by assigning probabilities to larger structural units. However, it is not always evident which structures should be considered as elementary structures. In (Schabes, 1992) it is proposed to infer a stochastic TAG f</context>
<context position="5655" citStr="Schabes, 1992" startWordPosition="923" endWordPosition="924">nally, the probability of a string is equal to the sum of the probabilities of all its parses. We will show that conventional parsing techniques can be applied to DOP, but that this becomes very inefficient, since the number of derivations of a parse grows exponentially with the length of the input string. However, we will show that DOP can be parsed in polynomial time by using Monte Carlo techniques. An important advantage of using a corpus for probability calculation, is that no training of parameters is needed, as is the case for other stochastic grammars (Jelinek et al., 1990; Pereira and Schabes, 1992; Schabes, 1992). Secondly, since we take into account all derivations of a parse, no relationship that might possibly be of statistical interest is ignored. 38 2 The Model As might be clear by now, a DOP-model is characterized by a corpus of tree structures, together with a set of operations that combine subtrees from the corpus into new trees. In this section we explain more precisely what we mean by subtree, operations etc., in order to arrive at definitions of a parse and the probability of a parse with respect to a corpus. For a treatment of DOP in more formal terms we refer to (Bod, 1992</context>
<context position="19421" citStr="Schabes, 1992" startWordPosition="3320" endWordPosition="3321">ness of this feature, we performed different experiments constraining the depth of the subtrees. The depth of a tree is defined as the length of its longest path. The following table shows the results of seven experiments. The accuracy refers to the parsing accuracy at sample size N= 100, and is rounded off to the nearest integer. depth accumcy 52 87% 53 92% 54 93% 55 93% 56 95% 57 95% unbounded 96% Parsing accuracy for the ATIS corpus, sample size N= 100. 25 50 715 100 sample size N Parsing accuracy for the ATIS corpus, with depth 5 2, with depth 5 3 and with unbounded depth. In (Pereira and Schabes, 1992), 90.36% bracketing accuracy was reported using a stochastic CFG trained on bracketings from the ATIS corpus. Though we cannot make a direct comparison, our pilot experiment suggests that our model may have better performance than a stochastic CFG. However, there is still an error rate of 4%. Although there is no reason to expect 100% accuracy in the absence of any semantic or pragmatic analysis, it seems that the accuracy might be further improved. Three limitations of the current experiments are worth mentioning, First, the Treebank annotations are not rich enough. Although the Treebank uses</context>
</contexts>
<marker>Schabes, 1992</marker>
<rawString>Y. Schabes, 1992. &amp;quot;Stochastic Lexicalized TreeAdjoining Grammars&amp;quot;, Proceedings COLING&apos;92 Nantes.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>