<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9734925">
Dependency Tree-based Sentiment Classification using CRFs with Hidden
Variables
</title>
<author confidence="0.956987">
Tetsuji Nakagawa∗, Kentaro Inui∗† and Sadao Kurohashi∗‡
</author>
<affiliation confidence="0.997288333333333">
∗National Institute of Information and Communications Technology
†Tohoku University
‡Kyoto University
</affiliation>
<email confidence="0.994589">
tnaka@nict.go.jp, inui@ecei.tohoku.ac.jp, kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999736125">
In this paper, we present a dependency tree-
based method for sentiment classification of
Japanese and English subjective sentences us-
ing conditional random fields with hidden
variables. Subjective sentences often con-
tain words which reverse the sentiment po-
larities of other words. Therefore, interac-
tions between words need to be considered
in sentiment classification, which is difficult
to be handled with simple bag-of-words ap-
proaches, and the syntactic dependency struc-
tures of subjective sentences are exploited in
our method. In the method, the sentiment po-
larity of each dependency subtree in a sen-
tence, which is not observable in training data,
is represented by a hidden variable. The po-
larity of the whole sentence is calculated in
consideration of interactions between the hid-
den variables. Sum-product belief propaga-
tion is used for inference. Experimental re-
sults of sentiment classification for Japanese
and English subjective sentences showed that
the method performs better than other meth-
ods based on bag-of-features.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934603773585">
Sentiment classification is a useful technique for an-
alyzing subjective information in a large number of
texts, and many studies have been conducted (Pang
and Lee, 2008). A typical approach for sentiment
classification is to use supervised machine learning
algorithms with bag-of-words as features (Pang et
al., 2002), which is widely used in topic-based text
classification. In the approach, a subjective sen-
tence is represented as a set of words in the sen-
tence, ignoring word order and head-modifier rela-
tion between words. However, sentiment classifi-
cation is different from traditional topic-based text
classification. Topic-based text classification is gen-
erally a linearly separable problem ((Chakrabarti,
2002), p.168). For example, when a document con-
tains some domain-specific words, the document
will probably belong to the domain. However, in
sentiment classification, sentiment polarities can be
reversed. For example, let us consider the sentence
“The medicine kills cancer cells.” While the phrase
cancer cells has negative polarity, the word kills re-
verses the polarity, and the whole sentence has pos-
itive polarity. Thus, in sentiment classification, a
sentence which contains positive (or negative) polar-
ity words does not necessarily have the same polar-
ity as a whole, and we need to consider interactions
between words instead of handling words indepen-
dently.
Recently, several methods have been proposed to
cope with the problem (Zaenen, 2004; Ikeda et al.,
2008). However, these methods are based on flat
bag-of-features representation, and do not consider
syntactic structures which seem essential to infer
the polarity of a whole sentence. Other methods
have been proposed which utilize composition of
sentences (Moilanen and Pulman, 2007; Choi and
Cardie, 2008; Jia et al., 2009), but these methods
use rules to handle polarity reversal, and whether po-
larity reversal occurs or not cannot be learned from
labeled data. Statistical machine learning can learn
useful information from training data and generally
robust for noisy data, and using it instead of rigid
rules seems useful. Wilson et al. (2005) proposed
a method for sentiment classification which utilizes
head-modifier relation and machine learning. How-
ever, the method is based on bag-of-features and po-
larity reversal occurred by content words is not han-
dled. One issue of the approach to use sentence
composition and machine learning is that only the
whole sentence is labeled with its polarity in gen-
eral corpora for sentiment classification, and each
component of the sentence is not labeled, though
such information is necessary for supervised ma-
</bodyText>
<page confidence="0.970699">
786
</page>
<note confidence="0.767111">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 786–794,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.979270212121212">
Whole Dependency Tree
It
prevents
cancer
and heart disease.
Polarities of Dependency Subtrees
cancer
− −
and heart disease.
cancer
prevents
+
and heart disease.
S1
0
It prevents cancer and heart disease.
S0
+
&lt;root&gt;
S2
+
S3 S4
− −
Figure 2: Probabilistic Model based on Dependency Tree
s0 s1 s2 s3 s4
g6
g1
gs
g2
g7
g3
g8
g4
</figure>
<figureCaption confidence="0.999981">
Figure 1: Polarities of Dependency Subtrees
</figureCaption>
<bodyText confidence="0.999879055555556">
chine learning to infer the sentence polarity from its
components.
In this paper, we propose a dependency tree-based
method for Japanese and English sentiment classifi-
cation using conditional random fields (CRFs) with
hidden variables. In the method, the sentiment po-
larity of each dependency subtree, which is not ob-
servable in training data, is represented by a hidden
variable. The polarity of the whole sentence is cal-
culated in consideration of interactions between the
hidden variables.
The rest of this paper is organized as follows: Sec-
tion 2 describes a dependency tree-based method
for sentiment classification using CRFs with hid-
den variables, and Section 3 shows experimental re-
sults on Japanese and English corpora. Section 4
discusses related work, and Section 5 gives conclu-
sions.
</bodyText>
<sectionHeader confidence="0.947214" genericHeader="method">
2 Dependency Tree-based Sentiment
</sectionHeader>
<subsectionHeader confidence="0.571215">
Classification using CRFs with Hidden
Variables
</subsectionHeader>
<bodyText confidence="0.999917166666667">
In this study, we handle a task to classify the polar-
ities (positive or negative) of given subjective sen-
tences. In the rest of this section, we describe a prob-
abilistic model for sentiment classification based on
dependency trees, methods for inference and param-
eter estimation, and features we use.
</bodyText>
<subsectionHeader confidence="0.8990015">
2.1 A Probabilistic Model based on
Dependency Trees
</subsectionHeader>
<bodyText confidence="0.998896333333333">
Let us consider the subjective sentence “It prevents
cancer and heart disease.” In the sentence, cancer
and heart disease have themselves negative polari-
</bodyText>
<figureCaption confidence="0.995792">
Figure 3: Factor Graph
</figureCaption>
<bodyText confidence="0.942432235294117">
ties. However, the polarities are reversed by modi-
fying the word prevents, and the dependency subtree
“prevents cancer and heart disease” has positive po-
larity. As a result, the whole dependency tree “It
prevents cancer and heart disease.” has positive po-
larity (Figure 1). In such a way, we can consider
the sentiment polarity for each dependency subtree
of a subjective sentence. Note that we use phrases as
a basic unit instead of words in this study, because
phrases are useful as a meaningful unit for sentiment
classification&apos;. In this paper, a dependency subtree
means the subtree of a dependency tree whose root
node is one of the phrases in the sentence.
We use a probabilistic model as shown in Fig-
ure 2. We consider that each phrase in the subjective
sentence has a random variable (indicated by a cir-
cle in Figure 2). The random variable represents the
polarity of the dependency subtree whose root node
is the corresponding phrase. Two random variables
are dependent (indicated by an edge in Figure 2) if
their corresponding phrases have head-modifier re-
lation in the dependency tree. The node denoted as
&lt;root&gt; in Figure 2 indicates a virtual phrase which
represents the root node of the sentence, and we re-
gard that the random variable of the root node is the
polarity of the whole sentence. In usual annotated
corpora for sentiment classification, only each sen-
tence is labeled with its polarity, and each phrase
(dependency subtree) is not labeled, so all the ran-
dom variables except the one for the root node are
&apos;From an empirical view, in our preliminary experiments
with the proposed method, phrase-based processing performed
better than word-based processing in accuracy and in computa-
tional efficiency.
</bodyText>
<page confidence="0.995686">
787
</page>
<bodyText confidence="0.999958947368421">
hidden variables that cannot be observed in labeled
data (indicated by gray circles in Figure 2). With
such a probabilistic model, it is possible to utilize
properties such that phrases which contain positive
(or negative) words tend to have positive (negative)
polarities, and two phrases with head-modifier rela-
tion tend to have opposite polarities if the head con-
tains a word which reverses sentiment polarity.
Next, we define the probabilistic model as shown
in Figure 2 in detail. Let n denote the number of
phrases in a subjective sentence, wi the i-th phrase,
and hi the head index of the i-th phrase. Let si de-
note the random variable which represents the po-
larity of the dependency subtree whose root is the
i-th phrase (si ∈ {+1, −1}), and let p denote the
polarity of the whole sentence (p ∈ {+1, −1}). We
regard the 0-th phrase as a virtual phrase which rep-
resents the root of the sentence. w, h, s respectively
denote the sequence of wi, hi, si.
</bodyText>
<equation confidence="0.9966555">
w = w1 ··· wn, h = h1 ··· hn, s = s0 ··· sn,
p = s0.
</equation>
<bodyText confidence="0.994336571428572">
For the example sentence in Figure 1, w1 =It,
w2 =prevents, w3 =cancer, w4 =and heart dis-
ease., h1 = 2, h2 = 0, h3 = 2, h4 = 2. We define
the joint probability distribution of the sentiment po-
larities of dependency subtrees s, given a subjective
sentence w and its dependency tree h, using log-
linear models:
</bodyText>
<equation confidence="0.996676">
{ ∑K }
1
PΛ(s|w, h)= ZΛ(w, h) exp λkFk(w, h, s) ,
k=1
(1)
{ ∑K }
exp λkFk(w,h,s) , (2)
k=1
fk(i, w, h, s), (3)
</equation>
<bodyText confidence="0.882338363636364">
where Λ = {λ1, · · · , λK} is the set of parameters
of the model. fk(i, w, h, s) is the feature function
of the i-th phrase, and is classified to node feature
which considers only the corresponding node, or
edge feature which considers both the correspond-
ing node and its head, as follows:
{ fn k (wi, si) (k ∈ Kn),
fk(i, w, h, s)= fek(wi, si, whi, shi) (k ∈ Ke),
(4)
where Kn and Ke respectively represent the sets of
indices of node features and edge features.
</bodyText>
<subsectionHeader confidence="0.999604">
2.2 Classification of Sentiment Polarity
</subsectionHeader>
<bodyText confidence="0.9998748">
Let us consider how to infer the sentiment polarity
p ∈ {+1, −1}, given a subjective sentence w and
its dependency tree h. The polarity of the root node
(s0) is regarded as the polarity of the whole sentence,
and p can be calculated as follows:
</bodyText>
<equation confidence="0.9516795">
p=argmax PΛ(p&apos;|w, h), (5)
pI
PΛ(p|w, h)= ∑ PΛ(s|w, h). (6)
S:so=p
</equation>
<bodyText confidence="0.999968192307693">
That is, the polarity of the subjective sentence is ob-
tained as the marginal probability of the root node
polarity, by summing the probabilities for all the
possible configurations of hidden variables. How-
ever, enumerating all the possible configurations of
hidden variables is computationally hard, and we use
sum-product belief propagation (MacKay, 2003) for
the calculation.
Belief propagation enables us to efficiently calcu-
late marginal probabilities. In this study, the graph-
ical model to be solved has a tree structure (identi-
cal to the syntactic dependency tree) which has no
loops, and an exact solution can be obtained us-
ing belief propagation. Dependencies among ran-
dom variables in Figure 2 are represented by a factor
graph in Figure 3. The factor graph consists of vari-
able nodes si indicated by circles, and factor (fea-
ture) nodes gi indicated by squares. In the exam-
ple in Figure 3, gi(1 ≤ i ≤ 4) correspond to the
node features in Equation (4), and gi(5 ≤ i ≤ 8)
correspond to the edge features. In belief propa-
gation, marginal distribution is calculated by pass-
ing messages (beliefs) among the variables and fac-
tors connected by edges in the factor graph (Refer
to (MacKay, 2003) for detailed description of belief
propagation).
</bodyText>
<subsectionHeader confidence="0.996883">
2.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.9999906">
Let us consider how to estimate model parameters Λ,
given L training examples D = {hwl, hl, pli}Ll=1.
In this study, we use the maximum a posteriori es-
timation with Gaussian priors for parameter estima-
tion. We define the following objective function LΛ,
</bodyText>
<equation confidence="0.9954855">
ZΛ(w, h)= ∑
S
Fk(w, h, s)= ∑n
i=1
</equation>
<page confidence="0.949752">
788
</page>
<bodyText confidence="0.982059">
and calculate the parameters Λˆ which maximize the
value:
</bodyText>
<equation confidence="0.878668636363636">
ˆΛ=argmax LΛ, (8)
Λ
where σ is a parameter of Gaussian priors and is set
to 1.0 in later experiments. The partial derivatives of
LΛ are as follows:
∑[∑
L
PΛ(s|wl, hl, pl)Fk(wl, hl, s)
l=1 $
∑− ]PΛ(s|wl, hl)Fk(wl, hl, s) − σ2 1 λk.
(9)
</equation>
<bodyText confidence="0.999869333333333">
The model parameters can be calculated with the
L-BFGS quasi-Newton method (Liu and Nocedal,
1989) using the objective function and its partial
derivatives. While the partial derivatives contain
summation over all the possible configurations of
hidden variables, it can be calculated efficiently us-
ing belief propagation as explained in Section 2.2.
This parameter estimation method is same to one
used for Latent-Dynamic Conditional Random Field
(Morency et al., 2007). Note that the objective func-
tion LΛ is not convex, and there is no guarantee for
global optimality. The estimated model parameters
depend on the initial values of the parameters, and
the setting of the initial values of model parameters
will be explained in Section 2.4.
</bodyText>
<subsectionHeader confidence="0.890969">
2.4 Features
</subsectionHeader>
<bodyText confidence="0.999677307692308">
Table 1 shows the features used in this study. Fea-
tures (a)–(h) in Table 1 are used as the node fea-
tures (Equation (4)) for the i-th phrase, and fea-
tures (A)–(E) are used as the edge features for the
i-th and j-th phrases (j=hi). In Table 1, si denotes
the hidden variable which represents the polarity of
the dependency subtree whose root node is the i-
th phrase, qi denotes the prior polarity of the i-th
phrase (explained later), ri denotes the polarity re-
versal of the i-th phrase (explained later), mi de-
notes the number of words in the i-th phrase, ui,k,
bi,k, ci,k, fi,k respectively denote the surface form,
base form, coarse-grained part-of-speech (POS) tag,
</bodyText>
<table confidence="0.9105386">
Node Features
a si
b si&amp;qi
c si&amp;qi&amp;ri
d si&amp;ui,1, ··· ,si&amp;ui,mi
e si&amp;ci,1, ··· ,si&amp;ci,mi
f si&amp;fi,1, ··· , si&amp;fi,mi
g si&amp;ui,1&amp;ui,2, ··· , si&amp;ui,mi−1&amp;ui,mi
h si&amp;bi,1&amp;bi,2, · · · , si&amp;bi,mi−1&amp;bi,mi
Edge Features
A si&amp;sj
B si&amp;sj&amp;rj
C si&amp;sj&amp;rj&amp;qj
D si&amp;sj&amp;bi,1, · · · , si&amp;sj&amp;bi,mi
E si&amp;sj&amp;bj,1, · · · , si&amp;sj&amp;bj,mj
</table>
<tableCaption confidence="0.999358">
Table 1: Features Used in This Study
</tableCaption>
<bodyText confidence="0.999681111111111">
fine-grained POS tag of the k-th word in the i-th
phrase.
We used the morphological analysis system JU-
MAN and the dependency parser KNP2 for pro-
cessing Japanese data, and the POS tagger MX-
POST (Ratnaparkhi, 1996) and the dependency
parser MaltParser3 for English data. KNP outputs
phrase-based dependency trees, but MaltParser out-
puts word-based dependency trees, and we con-
verted the word-based ones to phrase-based ones us-
ing simple heuristic rules explained in Appendix A.
The prior polarity of a phrase qi ∈ {+1, 0, −1} is
the innate sentiment polarity of a word contained in
the phrase, which can be obtained from sentiment
polarity dictionaries. We used sentiment polarity
dictionaries made by Kobayashi et al. (2007) and Hi-
gashiyama et al. (2008)4 for Japanese experiments
(The resulting dictionary contains 6,974 positive ex-
pressions and 8,428 negative expressions), and a dic-
tionary made by Wilson et al. (2005)5 for English
experiments (The dictionary contains 2,289 positive
expressions and 4,143 negative expressions). When
a phrase contains the words registered in the dictio-
naries, its prior polarity is set to the registered po-
larity, otherwise the prior polarity is set to 0. When
a phrase contains multiple words in the dictionaries,
the registered polarity of the last (nearest to the end
</bodyText>
<footnote confidence="0.9998455">
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
3http://maltparser.org/
4http://cl.naist.jp/˜inui/research/EM/sentiment-lexicon.html
5http://www.cs.pitt.edu/mpqa/
</footnote>
<equation confidence="0.965909333333333">
log PΛ(pl|wl,hl) −
∑2σ2
k=1
λ2k, (7)
K
1
∑L
l=1
LΛ=
∂LΛ
=
∂λk
</equation>
<page confidence="0.983409">
789
</page>
<bodyText confidence="0.999573617647059">
of the sentence) word is used.
The polarity reversal of a phrase ri E 10, 11 rep-
resents whether it reverses the polarities of other
phrases (1) of not (0). We prepared polarity revers-
ing word dictionaries, and the polarity reversal of
a phrase is set to 1 if the phrase contains a word
in the dictionaries, otherwise set to 0. We con-
structed polarity reversing word dictionaries which
contain such words as decrease and vanish that re-
verse sentiment polarity. A Japanese polarity revers-
ing word dictionary was constructed from an auto-
matically constructed corpus, and the construction
procedure is described in Appendix B (The dictio-
nary contains 219 polarity reversing words). An
English polarity reversing word dictionary was con-
structed from the General Inquirer dictionary6 in the
same way as Choi and Cardie (2008), by collecting
words which belong to either NOTLW or DECREAS
categories (The dictionary contains 121 polarity re-
versing words).
Choi and Cardie (2008) categorized polarity re-
versing words into two categories: function-word
negators such as not and content-word negators such
as eliminate. The polarity reversal of a phrase ri ex-
plained above handles only the content-word nega-
tors, and function-word negators are handled in an-
other way, since the scope of a function-word nega-
tor is generally limited to the phrase containing it in
Japanese, and the number of function-word negators
is small. The prior polarity qi and the polarity rever-
sal ri of a phrase are changed to the following q&apos;i and
r&apos;i, if the phrase contains a function-word negator (in
Japanese) or if the phrase is modified by a function-
word negator (in English):
</bodyText>
<equation confidence="0.9941115">
q&apos;i=−qi, (10)
r&apos;i=1 − ri. (11)
</equation>
<bodyText confidence="0.9999585">
In this paper, unless otherwise noted, the word po-
larity reversal is used to indicate polarity reversing
caused by content-word negators, and function-word
negators are assumed to be applied to qi and ri in the
above way beforehand.
As described in Section 2.3, there is no guaran-
tee of global optimality for estimated parameters,
since the objective function is not convex. In our
</bodyText>
<footnote confidence="0.967716">
6http://www.wjh.harvard.edu/ inquirer/
</footnote>
<bodyText confidence="0.999458083333333">
preliminary experiments, L-BFGS often did not con-
verge and classification accuracy was unstable when
the initial values of parameters were randomly set.
Therefore, in later experiments, we set the initial
values in the following way. For the feature (A) in
Table 1 in which si and sj are equal, we set the ini-
tial parameter Ai of the feature to a random number
in [0.9,1.1], otherwise we set to a random number in
[−0.1, 0.1]7. By setting such initial values, the initial
model parameters have a property that two phrases
with head-modifier relation tend to have the same
polarity, which is intuitively reasonable.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999781333333333">
We conducted experiments of sentiment classifica-
tion on four Japanese corpora and four English cor-
pora.
</bodyText>
<subsectionHeader confidence="0.992575">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999578590909091">
We used four corpora for experiments of Japanese
sentiment classification: the Automatically Con-
structed Polarity-tagged corpus (ACP) (Kaji and
Kitsuregawa, 2006), the Kyoto University and NTT
Blog corpus (KNB) 8, the NTCIR Japanese opinion
corpus (NTC-J) (Seki et al., 2007; Seki et al., 2008),
the 50 Topics Evaluative Information corpus (50
Topics) (Nakagawa et al., 2008). The ACP corpus
is an automatically constructed corpus from HTML
documents on the Web using lexico-syntactic pat-
terns and layout structures. The size of the corpus
is large (it consists of 650,951 instances), and we
used 1/100 of the whole corpus. The KNB corpus
consists of Japanese blogs, and is manually anno-
tated. The NTC-J corpus consists of Japanese news-
paper articles. There are two NTCIR Japanese opin-
ion corpora available, the NTCIR-6 corpus and the
NTCIR-7 corpus; and we combined the two cor-
pora. The 50 Topics corpus is collected from various
pages on the Web, and is manually annotated.
We used four corpora for experiments of English
sentiment classification: the Customer Review data
</bodyText>
<footnote confidence="0.984529166666667">
7The values of most learned parameters distributed between
-1.0 and 1.0 in our preliminary experiments. Therefore, we de-
cided to give values around the upper bound (1.0) and the mean
(0.0) to the features in order to incorporate minimal prior knowl-
edge into the model.
8http://nlp.kuee.kyoto-u.ac.jp/kuntt/
</footnote>
<page confidence="0.996755">
790
</page>
<bodyText confidence="0.99972435">
(CR)9, the MPQA Opinion corpus (MPQA)10, the
Movie Review Data (MR) 11, and the NTCIR En-
glish opinion corpus (NTC-E) (Seki et al., 2007;
Seki et al., 2008). The CR corpus consists of re-
view articles about products such as digital cameras
and cellular phones. There are two customer review
datasets, the 5 products dataset and the 9 products
dataset, and we combined the two datasets. In the
MPQA corpus, sentiment polarities are attached not
to sentences but expressions (sub-sentences), and we
regarded the expressions as sentences and classified
the polarities. There are two NTCIR English cor-
pora available, the NTCIR-6 corpus and the NTCIR-
7 corpus, and we combined the two corpora.
The statistical information of the corpora we used
is shown in Table 2. We randomly split each corpus
into 10 portions, and conducted 10-fold cross valida-
tion. Accuracy of sentiment classification was cal-
culated as the number of correctly predicted labels
(polarities) divided by the number of test examples.
</bodyText>
<subsectionHeader confidence="0.92303">
3.2 Compared Methods
</subsectionHeader>
<bodyText confidence="0.999974333333333">
We compared our method to 6 baseline methods,
and this section describes them. In the following,
p0 E 1+1, −11 denotes the major polarity in train-
ing data, Hi denotes the set consisting of all the an-
cestor nodes of the i-th phrase in the dependency
tree, and sgn(x) is defined as below:
</bodyText>
<equation confidence="0.994632">
+1 (x &gt; 0),
0 (x = 0),
−1 (x &lt; 0).
</equation>
<bodyText confidence="0.9948442">
Voting without Polarity Reversal The polarity of
a subjective sentence is decided by voting of
each phrase’s prior polarity. In the case of a
tie, the major polarity in the training data is
adopted.
</bodyText>
<equation confidence="0.9967843">
( ∑n )
p=sgn qi + 0.5p0 . (12)
i=1
10http://www.cs.pitt.edu/mpqa/11http://www.cs.cornell.edu/People/pabo/movie-review-
data/
cestors are reversed
before voting.
n
p=sgn�� qi ∏ (−1)rj + 0.5p0 I . (13)
\z=1 jEHi /
</equation>
<bodyText confidence="0.8631854">
pendency tree, an
d the polarity of the root node
is decided at the last.
( )
∑
</bodyText>
<equation confidence="0.884162">
si=sgn qi + sj(−1)ri , (14)
j:hj=i
p=sgn(s0 + 0.5p0). (15)
reversal phrases in their an
</equation>
<bodyText confidence="0.976932944444445">
Bag-of-Features with No Dictionaries The polar-
ity of a subjective sentence is classified us-
ing Support Vector Machines. Surface forms,
base forms, coarse-grained POS tags and fine-
grained POS tags of word unigrams and bi-
grams in the subjective sentence are used as
features12. The second order polynomial ker-
nel is used and the cost parameter C is set to
1.0. No prior polarity information (dictionary)
is used.
Bag-of-Features without Polarity Reversal Same
to Bag-of-Features with No Dictionaries, ex-
cept that the voting result of prior polarities
(one of positive, negative or tie) is also used
as a feature.
Bag-of-Features with Polarity Reversal Same to
Bag-of-Features without Polarity Reversal, ex-
cept that the polarities of phrases which have
</bodyText>
<footnote confidence="0.69612025">
Voting with Polarity Reversal Same to Voting
without Polarity Reversal, except that the po-
larities of phrases which have odd numbers of
9http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
</footnote>
<bodyText confidence="0.999173272727273">
Rule The polarity of a subjective sentence is deter-
ministically decided basing on rules, by con-
sidering the sentiment polarities of dependency
subtrees. The polarity of the dependency sub-
tree whose root is the i-th phrase is decided by
voting the prior polarity of the i-th phrase and
the polarities of the dependency subtrees whose
root nodes are the modifiers of the i-th phrase.
The polarities of the modifiers are reversed if
their head phrase has a reversal word. The de-
cision rule is applied from leaf nodes in the de-
</bodyText>
<footnote confidence="0.8310694">
12In experiments on English corpora, only the features of un-
igrams are used and those of bigrams are not used, since the
bigram features decreased accuracies in our preliminary experi-
ments as reported in previous work (Andreevskaia and Bergler,
2008).
</footnote>
<figure confidence="0.71409025">



sgn(x)=
</figure>
<page confidence="0.918559">
791
</page>
<table confidence="0.998355666666667">
Language Corpus Number of Instances (Positive / Negative)
ACP 6,510 (2,738 / 3,772)
Japanese KNB 2,288 (1,423 / 865)
NTC-J 3,485 (1,083 / 2,402)
50 Topics 5,366 (3,175 / 2,191)
CR 3,772 (2,406 / 1,366)
English MPQA 10,624 (3,316 / 7,308)
MR 10,662 (5,331 / 5,331)
NTC-E 3,812 (1,226 / 2,586)
</table>
<tableCaption confidence="0.895987">
Table 2: Statistical Information of Corpora
</tableCaption>
<table confidence="0.999910444444444">
Method Japanese English
ACP KNB NTC-J 50 Topics CR MPQA MR NTC-E
Voting-w/o Rev. 0.686 0.764 0.665 0.727 0.714 0.804 0.629 0.730
Voting-w/ Rev. 0.732 0.792 0.714 0.765 0.742 0.817 0.631 0.740
Rule 0.734 0.792 0.742 0.764 0.743 0.818 0.629 0.750
BoF-no Dic. 0.798 0.758 0.754 0.761 0.793 0.818 0.757 0.768
BoF-w/o Rev. 0.812 0.823 0.794 0.805 0.802 0.840 0.761 0.793
BoF-w/ Rev. 0.822 0.830 0.804 0.819 0.814 0.841 0.764 0.797
Tree-CRF 0.846* 0.847* 0.826* 0.841* 0.814 0.861* 0.773* 0.804
</table>
<tableCaption confidence="0.7812645">
(* indicates statistical significance at p &lt; 0.05)
Table 3: Accuracy of Sentiment Classification
</tableCaption>
<bodyText confidence="0.9552795">
odd numbers of reversal phrases in their ances-
tors are reversed before voting.
Tree-CRF The proposed method based on depen-
dency trees using CRFs, described in Section 2.
</bodyText>
<subsectionHeader confidence="0.999901">
3.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999266923076923">
The experimental results are shown in Table 3. The
proposed method Tree-CRF obtained the best ac-
curacies for all the four Japanese corpora and the
four English corpora, and the differences against
the second best methods were statistically signifi-
cant (p &lt; 0.05) with the paired t-test for the six
of the eight corpora. Tree-CRF performed better
for the Japanese corpora than for the English cor-
pora. For both the Voting methods and the Bag-of-
Features methods, the methods with polarity rever-
sal performed better than those without it13.
Both BoF-w/ Rev. and Tree-CRF use supervised
machine learning and the same dictionaries (the
</bodyText>
<footnote confidence="0.9553264">
13The Japanese polarity reversing word dictionary was con-
structed from the ACP corpus as described in Appendix B, and
it is not reasonable to compare the methods with and without
polarity reversal on the ACP corpus. However, the tendency
can be seen on the other 7 corpora.
</footnote>
<bodyText confidence="0.999909473684211">
prior polarity dictionaries and the polarity revers-
ing word dictionaries), but the latter performed bet-
ter than the former. Our error analysis showed that
BoF-w/ Rev. was not robust for erroneous words in
the prior polarity dictionaries. BoF-w/ Rev. uses the
voting result of the prior polarities as a feature, and
the feature is sensitive to the errors in the dictionary,
while Tree-CRF uses several information as well as
the prior polarities to decide the polarities of depen-
dency subtrees, and was robust to the dictionary er-
rors. We investigated the trained model parameters
of Tree-CRF, and found that the features (E) in Ta-
ble 1, in which the head and the modifier have op-
posite polarities and the head word is such as pro-
tect and withdraw, have large positive weights. Al-
though these words were not included in the polar-
ity reversing word dictionary, the property that these
words reverse polarities of other words seems to be
learned with the model.
</bodyText>
<sectionHeader confidence="0.999991" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.980253">
Various studies on sentiment classification have
been conducted, and there are several methods pro-
</bodyText>
<page confidence="0.989929">
792
</page>
<bodyText confidence="0.999963951219513">
posed for handling reversal of polarities. In this pa-
per, our method was not directly compared with the
other methods, since it is difficult to completely im-
plement them or conduct experiments with exactly
the same settings.
Choi and Cardie (2008) proposed a method to
classify the sentiment polarity of a sentence bas-
ing on compositional semantics. In their method,
the polarity of the whole sentence is determined
from the prior polarities of the composing words by
pre-defined rules, and the method differs from ours
which uses the probabilistic model to handle interac-
tions between hidden variables. Syntactic structures
were used in the studies of Moilanen and Pulman
(2007) and, Jia et al. (2009), but their methods are
based on rules and supervised learning was not used
to handle polarity reversal. As discussed in Sec-
tion 1, Wilson et al. (2005) studied a bag-of-features
based statistical sentiment classification method in-
corporating head-modifier relation.
Ikeda et al. (2008) proposed a machine learning
approach to handle sentiment polarity reversal. For
each word with prior polarity, whether the polarity is
reversed or not is learned with a statistical learning
algorithm using its surrounding words as features.
The method can handle only words with prior polar-
ities, and does not use syntactic dependency struc-
tures.
Conditional random fields with hidden variables
have been studied so far for other tasks. Latent-
Dynamic Conditional Random Fields (LDCRF)
(Morency et al., 2007; Sun et al., 2008) are prob-
abilistic models with hidden variables for sequen-
tial labeling, and belief propagation is used for in-
ference. Out method is similar to the models, but
there are several differences. In our method, only
one variable which represents the polarity of the
whole sentence is observable, and dependency re-
lation among random variables is not a linear chain
but a tree structure which is identical to the syntactic
dependency.
</bodyText>
<sectionHeader confidence="0.999542" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999922466666667">
In this paper, we presented a dependency tree-based
method for sentiment classification using condi-
tional random fields with hidden variables. In this
method, the polarity of each dependency subtree
of a subjective sentence is represented by a hid-
den variable. The values of the hidden variables
are calculated in consideration of interactions be-
tween variables whose nodes have head-modifier re-
lation in the dependency tree. The value of the
hidden variable of the root node is identified with
the polarity of the whole sentence. Experimental
results showed that the proposed method performs
better for Japanese and English data than the base-
line methods which represents subjective sentences
as bag-of-features.
</bodyText>
<sectionHeader confidence="0.863782666666667" genericHeader="method">
Appendix
A Rules for Converting Word Sequence to
Phrase Sequence
</sectionHeader>
<bodyText confidence="0.999715">
Let vi, · · · , vN denote an English word sequence, yi
the part-of-speech of the i-th word, and zi the head
index of the i-th word. The word sequence was con-
verted to a phrase sequence as follows, by applying
rules which combine two adjacent words:
</bodyText>
<equation confidence="0.975739833333333">
LT ≡ {“,(,-LRB-,-LSB-,-LCB-,CC}
RT ≡ {”,),,,--,.,:,POS,-RRB-,-RSB-,-RCB-}
PP ≡ {IN,RP,TO,DT,PDT,PRP,WDT,WP,WP$,WRB}
NN ≡ {CD,FW,NN,NNP,NNPS,NNS,SYM,JJ}
do
for i := 1 to N − 1
</equation>
<bodyText confidence="0.677047">
if xi and xi+1 are not yet combined ∧
</bodyText>
<equation confidence="0.9969642">
(xi ∈ LT ∨
xi+1 ∈ RT ∨
((yi = yi+1 ∨ yi = i + 1 ∨ yi+1 = i) ∧
(xi ∈ PP ∨
(xi ∈ NN ∧ xi+1 ∈ NN)))) then
</equation>
<bodyText confidence="0.823849">
Combine the words vi and vi+1
until No rules are applied
</bodyText>
<subsectionHeader confidence="0.971148">
B Construction of Japanese Polarity
Reversing Word Dictionary
</subsectionHeader>
<bodyText confidence="0.9999865">
We constructed a Japanese polarity reversing word
dictionary from the Automatically Constructed
Polarity-tagged corpus (Kaji and Kitsuregawa,
2006). First, we collected sentences, each of which
contains just one phrase having prior polarity, and
the phrase modifies a phrase which modifies the root
node. Among them, we selected sentences in which
the prior polarity is not equal to the polarity of the
whole sentence. We extracted all the words in the
head phrase, and manually checked them whether
they should be put into the dictionary or not. The ra-
tionale behind the procedure is that the prior polarity
can be considered to be reversed by a certain word
in the head phrase.
</bodyText>
<page confidence="0.998053">
793
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894999999999">
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Overcom-
ing Domain Dependence in Sentiment Tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 290–298.
Soumen Chakrabarti. 2002. Mining the Web: Dis-
covering Knowledge from Hypertext Data. Morgan-
Kauffman.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 793–801.
Masahiko Higashiyama, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring Noun Polarity Knowledge
Using Selectional Preferences. In Proceedings of the
14th Annual Meeting of the Association for Natural
Language Processing, pages 584–587. (in Japanese).
Daisuke Ikeda, Hiroya Takamura, Lev-Arie Ratinov, and
Manabu Okumura. 2008. Learning to Shift the Po-
larity of Words for Sentiment Classification. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Language Processing, pages 296–303.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The Ef-
fect of Negation on Sentiment Analysis and Retrieval
Effectiveness. In Proceeding of the 18th ACM Con-
ference on Information and Knowledge Management,
pages 1827–1830.
Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Auto-
matic Construction of Polarity-Tagged Corpus from
HTML Documents. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 452–459.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Opinion Mining from Web Documents: Extrac-
tion and Structurization. Journal of the Japanese So-
ciety for Artificial Intelligence, 22(2):227–238.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503–528.
David J. C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of the Recent Advances
in Natural Language Processing International Confer-
ence, pages 378–382.
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-Dynamic Discriminative Mod-
els for Continuous Gesture Recognition. In Proceed-
ings of the 2007 IEEE Conference on Computer Vision
and Pattern Recognition, pages 1–8.
Tetsuji Nakagawa, Takuya Kawada, Kentaro Inui, and
Sadao Kurohashi. 2008. Extracting Subjective and
Objective Evaluative Expressions from the Web. In
Proceedings of the 2nd International Symposium on
Universal Communication.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing, pages 79–86.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proceedings of the
1996 Conference on Empirical Methods in Natural
Language Processing Conference, pages 133–142.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-His
Chen, Noriko Kando, and Chin-Yew Lin. 2007.
Overview of Opinion Analysis Pilot Task at NTCIR-
6. In Proceedings of the 6th NTCIR Workshop, pages
265–278.
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun,
Hsin-Hsi Chen, and Noriko Kando. 2008. Overview
of Multilingual Opinion Analysis Task at NTCIR-7. In
Proceedings of the 7th NTCIR Workshop.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun’ichi Tsujii. 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 841–848.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of the 2005
Joint Conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 347–354.
Livia Polanyi Annie Zaenen. 2004. Contextual Lexical
Valence Shifters. In Proceedings of the AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
</reference>
<page confidence="0.998366">
794
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.475737">
<title confidence="0.740334333333333">Dependency Tree-based Sentiment Classification using CRFs with Hidden Variables Institute of Information and Communications</title>
<email confidence="0.80612">tnaka@nict.go.jp,inui@ecei.tohoku.ac.jp,kuro@i.kyoto-u.ac.jp</email>
<abstract confidence="0.99870388">In this paper, we present a dependency treebased method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>290--298</pages>
<contexts>
<context position="22970" citStr="Andreevskaia and Bergler, 2008" startWordPosition="3759" endWordPosition="3762"> subtrees. The polarity of the dependency subtree whose root is the i-th phrase is decided by voting the prior polarity of the i-th phrase and the polarities of the dependency subtrees whose root nodes are the modifiers of the i-th phrase. The polarities of the modifiers are reversed if their head phrase has a reversal word. The decision rule is applied from leaf nodes in the de12In experiments on English corpora, only the features of unigrams are used and those of bigrams are not used, since the bigram features decreased accuracies in our preliminary experiments as reported in previous work (Andreevskaia and Bergler, 2008).    sgn(x)= 791 Language Corpus Number of Instances (Positive / Negative) ACP 6,510 (2,738 / 3,772) Japanese KNB 2,288 (1,423 / 865) NTC-J 3,485 (1,083 / 2,402) 50 Topics 5,366 (3,175 / 2,191) CR 3,772 (2,406 / 1,366) English MPQA 10,624 (3,316 / 7,308) MR 10,662 (5,331 / 5,331) NTC-E 3,812 (1,226 / 2,586) Table 2: Statistical Information of Corpora Method Japanese English ACP KNB NTC-J 50 Topics CR MPQA MR NTC-E Voting-w/o Rev. 0.686 0.764 0.665 0.727 0.714 0.804 0.629 0.730 Voting-w/ Rev. 0.732 0.792 0.714 0.765 0.742 0.817 0.631 0.740 Rule 0.734 0.792 0.742 0.764 0.743 0.818 0.629 0.7</context>
</contexts>
<marker>Andreevskaia, Bergler, 2008</marker>
<rawString>Alina Andreevskaia and Sabine Bergler. 2008. When Specialists and Generalists Work Together: Overcoming Domain Dependence in Sentiment Tagging. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 290–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
</authors>
<title>Mining the Web: Discovering Knowledge from Hypertext Data.</title>
<date>2002</date>
<publisher>MorganKauffman.</publisher>
<contexts>
<context position="2080" citStr="Chakrabarti, 2002" startWordPosition="295" endWordPosition="296">r of texts, and many studies have been conducted (Pang and Lee, 2008). A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002), which is widely used in topic-based text classification. In the approach, a subjective sentence is represented as a set of words in the sentence, ignoring word order and head-modifier relation between words. However, sentiment classification is different from traditional topic-based text classification. Topic-based text classification is generally a linearly separable problem ((Chakrabarti, 2002), p.168). For example, when a document contains some domain-specific words, the document will probably belong to the domain. However, in sentiment classification, sentiment polarities can be reversed. For example, let us consider the sentence “The medicine kills cancer cells.” While the phrase cancer cells has negative polarity, the word kills reverses the polarity, and the whole sentence has positive polarity. Thus, in sentiment classification, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider intera</context>
</contexts>
<marker>Chakrabarti, 2002</marker>
<rawString>Soumen Chakrabarti. 2002. Mining the Web: Discovering Knowledge from Hypertext Data. MorganKauffman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>793--801</pages>
<contexts>
<context position="3147" citStr="Choi and Cardie, 2008" startWordPosition="456" endWordPosition="459">on, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on flat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classification which utilizes head-modifier relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of the approach to use sent</context>
<context position="15930" citStr="Choi and Cardie (2008)" startWordPosition="2610" endWordPosition="2613">onaries, and the polarity reversal of a phrase is set to 1 if the phrase contains a word in the dictionaries, otherwise set to 0. We constructed polarity reversing word dictionaries which contain such words as decrease and vanish that reverse sentiment polarity. A Japanese polarity reversing word dictionary was constructed from an automatically constructed corpus, and the construction procedure is described in Appendix B (The dictionary contains 219 polarity reversing words). An English polarity reversing word dictionary was constructed from the General Inquirer dictionary6 in the same way as Choi and Cardie (2008), by collecting words which belong to either NOTLW or DECREAS categories (The dictionary contains 121 polarity reversing words). Choi and Cardie (2008) categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate. The polarity reversal of a phrase ri explained above handles only the content-word negators, and function-word negators are handled in another way, since the scope of a function-word negator is generally limited to the phrase containing it in Japanese, and the number of function-word negators is small. The pr</context>
<context position="26337" citStr="Choi and Cardie (2008)" startWordPosition="4318" endWordPosition="4321">the head word is such as protect and withdraw, have large positive weights. Although these words were not included in the polarity reversing word dictionary, the property that these words reverse polarities of other words seems to be learned with the model. 4 Related Work Various studies on sentiment classification have been conducted, and there are several methods pro792 posed for handling reversal of polarities. In this paper, our method was not directly compared with the other methods, since it is difficult to completely implement them or conduct experiments with exactly the same settings. Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics. In their method, the polarity of the whole sentence is determined from the prior polarities of the composing words by pre-defined rules, and the method differs from ours which uses the probabilistic model to handle interactions between hidden variables. Syntactic structures were used in the studies of Moilanen and Pulman (2007) and, Jia et al. (2009), but their methods are based on rules and supervised learning was not used to handle polarity reversal. As discussed in Section 1, Wilson et al.</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 793–801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiko Higashiyama</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Acquiring Noun Polarity Knowledge Using Selectional Preferences.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<pages>584--587</pages>
<note>(in Japanese).</note>
<contexts>
<context position="14332" citStr="Higashiyama et al. (2008)" startWordPosition="2367" endWordPosition="2371">ser KNP2 for processing Japanese data, and the POS tagger MXPOST (Ratnaparkhi, 1996) and the dependency parser MaltParser3 for English data. KNP outputs phrase-based dependency trees, but MaltParser outputs word-based dependency trees, and we converted the word-based ones to phrase-based ones using simple heuristic rules explained in Appendix A. The prior polarity of a phrase qi ∈ {+1, 0, −1} is the innate sentiment polarity of a word contained in the phrase, which can be obtained from sentiment polarity dictionaries. We used sentiment polarity dictionaries made by Kobayashi et al. (2007) and Higashiyama et al. (2008)4 for Japanese experiments (The resulting dictionary contains 6,974 positive expressions and 8,428 negative expressions), and a dictionary made by Wilson et al. (2005)5 for English experiments (The dictionary contains 2,289 positive expressions and 4,143 negative expressions). When a phrase contains the words registered in the dictionaries, its prior polarity is set to the registered polarity, otherwise the prior polarity is set to 0. When a phrase contains multiple words in the dictionaries, the registered polarity of the last (nearest to the end 2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/ 3h</context>
</contexts>
<marker>Higashiyama, Inui, Matsumoto, 2008</marker>
<rawString>Masahiko Higashiyama, Kentaro Inui, and Yuji Matsumoto. 2008. Acquiring Noun Polarity Knowledge Using Selectional Preferences. In Proceedings of the 14th Annual Meeting of the Association for Natural Language Processing, pages 584–587. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Ikeda</author>
<author>Hiroya Takamura</author>
<author>Lev-Arie Ratinov</author>
<author>Manabu Okumura</author>
</authors>
<title>Learning to Shift the Polarity of Words for Sentiment Classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>296--303</pages>
<contexts>
<context position="2846" citStr="Ikeda et al., 2008" startWordPosition="412" endWordPosition="415">classification, sentiment polarities can be reversed. For example, let us consider the sentence “The medicine kills cancer cells.” While the phrase cancer cells has negative polarity, the word kills reverses the polarity, and the whole sentence has positive polarity. Thus, in sentiment classification, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on flat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of ri</context>
<context position="27078" citStr="Ikeda et al. (2008)" startWordPosition="4434" endWordPosition="4437">e polarity of the whole sentence is determined from the prior polarities of the composing words by pre-defined rules, and the method differs from ours which uses the probabilistic model to handle interactions between hidden variables. Syntactic structures were used in the studies of Moilanen and Pulman (2007) and, Jia et al. (2009), but their methods are based on rules and supervised learning was not used to handle polarity reversal. As discussed in Section 1, Wilson et al. (2005) studied a bag-of-features based statistical sentiment classification method incorporating head-modifier relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use syntactic dependency structures. Conditional random fields with hidden variables have been studied so far for other tasks. LatentDynamic Conditional Random Fields (LDCRF) (Morency et al., 2007; Sun et al., 2008) are probabilistic models with hidden variables for sequential labeling, a</context>
</contexts>
<marker>Ikeda, Takamura, Ratinov, Okumura, 2008</marker>
<rawString>Daisuke Ikeda, Hiroya Takamura, Lev-Arie Ratinov, and Manabu Okumura. 2008. Learning to Shift the Polarity of Words for Sentiment Classification. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 296–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lifeng Jia</author>
<author>Clement Yu</author>
<author>Weiyi Meng</author>
</authors>
<title>The Effect of Negation on Sentiment Analysis and Retrieval Effectiveness.</title>
<date>2009</date>
<booktitle>In Proceeding of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>1827--1830</pages>
<contexts>
<context position="3166" citStr="Jia et al., 2009" startWordPosition="460" endWordPosition="463">ntains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on flat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classification which utilizes head-modifier relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of the approach to use sentence composition an</context>
<context position="26792" citStr="Jia et al. (2009)" startWordPosition="4391" endWordPosition="4394"> compared with the other methods, since it is difficult to completely implement them or conduct experiments with exactly the same settings. Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics. In their method, the polarity of the whole sentence is determined from the prior polarities of the composing words by pre-defined rules, and the method differs from ours which uses the probabilistic model to handle interactions between hidden variables. Syntactic structures were used in the studies of Moilanen and Pulman (2007) and, Jia et al. (2009), but their methods are based on rules and supervised learning was not used to handle polarity reversal. As discussed in Section 1, Wilson et al. (2005) studied a bag-of-features based statistical sentiment classification method incorporating head-modifier relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use</context>
</contexts>
<marker>Jia, Yu, Meng, 2009</marker>
<rawString>Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The Effect of Negation on Sentiment Analysis and Retrieval Effectiveness. In Proceeding of the 18th ACM Conference on Information and Knowledge Management, pages 1827–1830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Automatic Construction of Polarity-Tagged Corpus from HTML Documents.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>452--459</pages>
<contexts>
<context position="18115" citStr="Kaji and Kitsuregawa, 2006" startWordPosition="2961" endWordPosition="2964">and sj are equal, we set the initial parameter Ai of the feature to a random number in [0.9,1.1], otherwise we set to a random number in [−0.1, 0.1]7. By setting such initial values, the initial model parameters have a property that two phrases with head-modifier relation tend to have the same polarity, which is intuitively reasonable. 3 Experiments We conducted experiments of sentiment classification on four Japanese corpora and four English corpora. 3.1 Data We used four corpora for experiments of Japanese sentiment classification: the Automatically Constructed Polarity-tagged corpus (ACP) (Kaji and Kitsuregawa, 2006), the Kyoto University and NTT Blog corpus (KNB) 8, the NTCIR Japanese opinion corpus (NTC-J) (Seki et al., 2007; Seki et al., 2008), the 50 Topics Evaluative Information corpus (50 Topics) (Nakagawa et al., 2008). The ACP corpus is an automatically constructed corpus from HTML documents on the Web using lexico-syntactic patterns and layout structures. The size of the corpus is large (it consists of 650,951 instances), and we used 1/100 of the whole corpus. The KNB corpus consists of Japanese blogs, and is manually annotated. The NTC-J corpus consists of Japanese newspaper articles. There are </context>
</contexts>
<marker>Kaji, Kitsuregawa, 2006</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Automatic Construction of Polarity-Tagged Corpus from HTML Documents. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 452–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Opinion Mining from Web Documents: Extraction and Structurization.</title>
<date>2007</date>
<journal>Journal of the Japanese Society for Artificial Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="14302" citStr="Kobayashi et al. (2007)" startWordPosition="2362" endWordPosition="2365">JUMAN and the dependency parser KNP2 for processing Japanese data, and the POS tagger MXPOST (Ratnaparkhi, 1996) and the dependency parser MaltParser3 for English data. KNP outputs phrase-based dependency trees, but MaltParser outputs word-based dependency trees, and we converted the word-based ones to phrase-based ones using simple heuristic rules explained in Appendix A. The prior polarity of a phrase qi ∈ {+1, 0, −1} is the innate sentiment polarity of a word contained in the phrase, which can be obtained from sentiment polarity dictionaries. We used sentiment polarity dictionaries made by Kobayashi et al. (2007) and Higashiyama et al. (2008)4 for Japanese experiments (The resulting dictionary contains 6,974 positive expressions and 8,428 negative expressions), and a dictionary made by Wilson et al. (2005)5 for English experiments (The dictionary contains 2,289 positive expressions and 4,143 negative expressions). When a phrase contains the words registered in the dictionaries, its prior polarity is set to the registered polarity, otherwise the prior polarity is set to 0. When a phrase contains multiple words in the dictionaries, the registered polarity of the last (nearest to the end 2http://nlp.kuee</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Opinion Mining from Web Documents: Extraction and Structurization. Journal of the Japanese Society for Artificial Intelligence, 22(2):227–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="11909" citStr="Liu and Nocedal, 1989" startWordPosition="1962" endWordPosition="1965">raining examples D = {hwl, hl, pli}Ll=1. In this study, we use the maximum a posteriori estimation with Gaussian priors for parameter estimation. We define the following objective function LΛ, ZΛ(w, h)= ∑ S Fk(w, h, s)= ∑n i=1 788 and calculate the parameters Λˆ which maximize the value: ˆΛ=argmax LΛ, (8) Λ where σ is a parameter of Gaussian priors and is set to 1.0 in later experiments. The partial derivatives of LΛ are as follows: ∑[∑ L PΛ(s|wl, hl, pl)Fk(wl, hl, s) l=1 $ ∑− ]PΛ(s|wl, hl)Fk(wl, hl, s) − σ2 1 λk. (9) The model parameters can be calculated with the L-BFGS quasi-Newton method (Liu and Nocedal, 1989) using the objective function and its partial derivatives. While the partial derivatives contain summation over all the possible configurations of hidden variables, it can be calculated efficiently using belief propagation as explained in Section 2.2. This parameter estimation method is same to one used for Latent-Dynamic Conditional Random Field (Morency et al., 2007). Note that the objective function LΛ is not convex, and there is no guarantee for global optimality. The estimated model parameters depend on the initial values of the parameters, and the setting of the initial values of model p</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
</authors>
<title>Information Theory, Inference, and Learning Algorithms.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10310" citStr="MacKay, 2003" startWordPosition="1683" endWordPosition="1684">∈ {+1, −1}, given a subjective sentence w and its dependency tree h. The polarity of the root node (s0) is regarded as the polarity of the whole sentence, and p can be calculated as follows: p=argmax PΛ(p&apos;|w, h), (5) pI PΛ(p|w, h)= ∑ PΛ(s|w, h). (6) S:so=p That is, the polarity of the subjective sentence is obtained as the marginal probability of the root node polarity, by summing the probabilities for all the possible configurations of hidden variables. However, enumerating all the possible configurations of hidden variables is computationally hard, and we use sum-product belief propagation (MacKay, 2003) for the calculation. Belief propagation enables us to efficiently calculate marginal probabilities. In this study, the graphical model to be solved has a tree structure (identical to the syntactic dependency tree) which has no loops, and an exact solution can be obtained using belief propagation. Dependencies among random variables in Figure 2 are represented by a factor graph in Figure 3. The factor graph consists of variable nodes si indicated by circles, and factor (feature) nodes gi indicated by squares. In the example in Figure 3, gi(1 ≤ i ≤ 4) correspond to the node features in Equation</context>
</contexts>
<marker>MacKay, 2003</marker>
<rawString>David J. C. MacKay. 2003. Information Theory, Inference, and Learning Algorithms. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karo Moilanen</author>
<author>Stephen Pulman</author>
</authors>
<title>Sentiment Composition.</title>
<date>2007</date>
<booktitle>In Proceedings of the Recent Advances in Natural Language Processing International Conference,</booktitle>
<pages>378--382</pages>
<contexts>
<context position="3124" citStr="Moilanen and Pulman, 2007" startWordPosition="452" endWordPosition="455">, in sentiment classification, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on flat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classification which utilizes head-modifier relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of t</context>
<context position="26769" citStr="Moilanen and Pulman (2007)" startWordPosition="4386" endWordPosition="4389">per, our method was not directly compared with the other methods, since it is difficult to completely implement them or conduct experiments with exactly the same settings. Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics. In their method, the polarity of the whole sentence is determined from the prior polarities of the composing words by pre-defined rules, and the method differs from ours which uses the probabilistic model to handle interactions between hidden variables. Syntactic structures were used in the studies of Moilanen and Pulman (2007) and, Jia et al. (2009), but their methods are based on rules and supervised learning was not used to handle polarity reversal. As discussed in Section 1, Wilson et al. (2005) studied a bag-of-features based statistical sentiment classification method incorporating head-modifier relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polar</context>
</contexts>
<marker>Moilanen, Pulman, 2007</marker>
<rawString>Karo Moilanen and Stephen Pulman. 2007. Sentiment Composition. In Proceedings of the Recent Advances in Natural Language Processing International Conference, pages 378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louis-Philippe Morency</author>
<author>Ariadna Quattoni</author>
<author>Trevor Darrell</author>
</authors>
<title>Latent-Dynamic Discriminative Models for Continuous Gesture Recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="12280" citStr="Morency et al., 2007" startWordPosition="2016" endWordPosition="2019">later experiments. The partial derivatives of LΛ are as follows: ∑[∑ L PΛ(s|wl, hl, pl)Fk(wl, hl, s) l=1 $ ∑− ]PΛ(s|wl, hl)Fk(wl, hl, s) − σ2 1 λk. (9) The model parameters can be calculated with the L-BFGS quasi-Newton method (Liu and Nocedal, 1989) using the objective function and its partial derivatives. While the partial derivatives contain summation over all the possible configurations of hidden variables, it can be calculated efficiently using belief propagation as explained in Section 2.2. This parameter estimation method is same to one used for Latent-Dynamic Conditional Random Field (Morency et al., 2007). Note that the objective function LΛ is not convex, and there is no guarantee for global optimality. The estimated model parameters depend on the initial values of the parameters, and the setting of the initial values of model parameters will be explained in Section 2.4. 2.4 Features Table 1 shows the features used in this study. Features (a)–(h) in Table 1 are used as the node features (Equation (4)) for the i-th phrase, and features (A)–(E) are used as the edge features for the i-th and j-th phrases (j=hi). In Table 1, si denotes the hidden variable which represents the polarity of the depe</context>
<context position="27585" citStr="Morency et al., 2007" startWordPosition="4512" endWordPosition="4515">tures based statistical sentiment classification method incorporating head-modifier relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use syntactic dependency structures. Conditional random fields with hidden variables have been studied so far for other tasks. LatentDynamic Conditional Random Fields (LDCRF) (Morency et al., 2007; Sun et al., 2008) are probabilistic models with hidden variables for sequential labeling, and belief propagation is used for inference. Out method is similar to the models, but there are several differences. In our method, only one variable which represents the polarity of the whole sentence is observable, and dependency relation among random variables is not a linear chain but a tree structure which is identical to the syntactic dependency. 5 Conclusion In this paper, we presented a dependency tree-based method for sentiment classification using conditional random fields with hidden variabl</context>
</contexts>
<marker>Morency, Quattoni, Darrell, 2007</marker>
<rawString>Louis-Philippe Morency, Ariadna Quattoni, and Trevor Darrell. 2007. Latent-Dynamic Discriminative Models for Continuous Gesture Recognition. In Proceedings of the 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Takuya Kawada</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Extracting Subjective and Objective Evaluative Expressions from the Web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd International Symposium on Universal Communication.</booktitle>
<contexts>
<context position="18328" citStr="Nakagawa et al., 2008" startWordPosition="2996" endWordPosition="2999"> property that two phrases with head-modifier relation tend to have the same polarity, which is intuitively reasonable. 3 Experiments We conducted experiments of sentiment classification on four Japanese corpora and four English corpora. 3.1 Data We used four corpora for experiments of Japanese sentiment classification: the Automatically Constructed Polarity-tagged corpus (ACP) (Kaji and Kitsuregawa, 2006), the Kyoto University and NTT Blog corpus (KNB) 8, the NTCIR Japanese opinion corpus (NTC-J) (Seki et al., 2007; Seki et al., 2008), the 50 Topics Evaluative Information corpus (50 Topics) (Nakagawa et al., 2008). The ACP corpus is an automatically constructed corpus from HTML documents on the Web using lexico-syntactic patterns and layout structures. The size of the corpus is large (it consists of 650,951 instances), and we used 1/100 of the whole corpus. The KNB corpus consists of Japanese blogs, and is manually annotated. The NTC-J corpus consists of Japanese newspaper articles. There are two NTCIR Japanese opinion corpora available, the NTCIR-6 corpus and the NTCIR-7 corpus; and we combined the two corpora. The 50 Topics corpus is collected from various pages on the Web, and is manually annotated.</context>
</contexts>
<marker>Nakagawa, Kawada, Inui, Kurohashi, 2008</marker>
<rawString>Tetsuji Nakagawa, Takuya Kawada, Kentaro Inui, and Sadao Kurohashi. 2008. Extracting Subjective and Objective Evaluative Expressions from the Web. In Proceedings of the 2nd International Symposium on Universal Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1531" citStr="Pang and Lee, 2008" startWordPosition="214" endWordPosition="217"> sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features. 1 Introduction Sentiment classification is a useful technique for analyzing subjective information in a large number of texts, and many studies have been conducted (Pang and Lee, 2008). A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002), which is widely used in topic-based text classification. In the approach, a subjective sentence is represented as a set of words in the sentence, ignoring word order and head-modifier relation between words. However, sentiment classification is different from traditional topic-based text classification. Topic-based text classification is generally a linearly separable problem ((Chakrabarti, 2002), p.168). For example, when a document contains som</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1679" citStr="Pang et al., 2002" startWordPosition="235" endWordPosition="238">ration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features. 1 Introduction Sentiment classification is a useful technique for analyzing subjective information in a large number of texts, and many studies have been conducted (Pang and Lee, 2008). A typical approach for sentiment classification is to use supervised machine learning algorithms with bag-of-words as features (Pang et al., 2002), which is widely used in topic-based text classification. In the approach, a subjective sentence is represented as a set of words in the sentence, ignoring word order and head-modifier relation between words. However, sentiment classification is different from traditional topic-based text classification. Topic-based text classification is generally a linearly separable problem ((Chakrabarti, 2002), p.168). For example, when a document contains some domain-specific words, the document will probably belong to the domain. However, in sentiment classification, sentiment polarities can be reversed</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the 1996 Conference on Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="13791" citStr="Ratnaparkhi, 1996" startWordPosition="2283" endWordPosition="2284">e form, base form, coarse-grained part-of-speech (POS) tag, Node Features a si b si&amp;qi c si&amp;qi&amp;ri d si&amp;ui,1, ··· ,si&amp;ui,mi e si&amp;ci,1, ··· ,si&amp;ci,mi f si&amp;fi,1, ··· , si&amp;fi,mi g si&amp;ui,1&amp;ui,2, ··· , si&amp;ui,mi−1&amp;ui,mi h si&amp;bi,1&amp;bi,2, · · · , si&amp;bi,mi−1&amp;bi,mi Edge Features A si&amp;sj B si&amp;sj&amp;rj C si&amp;sj&amp;rj&amp;qj D si&amp;sj&amp;bi,1, · · · , si&amp;sj&amp;bi,mi E si&amp;sj&amp;bj,1, · · · , si&amp;sj&amp;bj,mj Table 1: Features Used in This Study fine-grained POS tag of the k-th word in the i-th phrase. We used the morphological analysis system JUMAN and the dependency parser KNP2 for processing Japanese data, and the POS tagger MXPOST (Ratnaparkhi, 1996) and the dependency parser MaltParser3 for English data. KNP outputs phrase-based dependency trees, but MaltParser outputs word-based dependency trees, and we converted the word-based ones to phrase-based ones using simple heuristic rules explained in Appendix A. The prior polarity of a phrase qi ∈ {+1, 0, −1} is the innate sentiment polarity of a word contained in the phrase, which can be obtained from sentiment polarity dictionaries. We used sentiment polarity dictionaries made by Kobayashi et al. (2007) and Higashiyama et al. (2008)4 for Japanese experiments (The resulting dictionary contai</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Proceedings of the 1996 Conference on Empirical Methods in Natural Language Processing Conference, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-His Chen</author>
<author>Noriko Kando</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Overview of Opinion Analysis Pilot Task at NTCIR6.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th NTCIR Workshop,</booktitle>
<pages>265--278</pages>
<contexts>
<context position="18227" citStr="Seki et al., 2007" startWordPosition="2980" endWordPosition="2983">ndom number in [−0.1, 0.1]7. By setting such initial values, the initial model parameters have a property that two phrases with head-modifier relation tend to have the same polarity, which is intuitively reasonable. 3 Experiments We conducted experiments of sentiment classification on four Japanese corpora and four English corpora. 3.1 Data We used four corpora for experiments of Japanese sentiment classification: the Automatically Constructed Polarity-tagged corpus (ACP) (Kaji and Kitsuregawa, 2006), the Kyoto University and NTT Blog corpus (KNB) 8, the NTCIR Japanese opinion corpus (NTC-J) (Seki et al., 2007; Seki et al., 2008), the 50 Topics Evaluative Information corpus (50 Topics) (Nakagawa et al., 2008). The ACP corpus is an automatically constructed corpus from HTML documents on the Web using lexico-syntactic patterns and layout structures. The size of the corpus is large (it consists of 650,951 instances), and we used 1/100 of the whole corpus. The KNB corpus consists of Japanese blogs, and is manually annotated. The NTC-J corpus consists of Japanese newspaper articles. There are two NTCIR Japanese opinion corpora available, the NTCIR-6 corpus and the NTCIR-7 corpus; and we combined the two</context>
<context position="19474" citStr="Seki et al., 2007" startWordPosition="3182" endWordPosition="3185"> is collected from various pages on the Web, and is manually annotated. We used four corpora for experiments of English sentiment classification: the Customer Review data 7The values of most learned parameters distributed between -1.0 and 1.0 in our preliminary experiments. Therefore, we decided to give values around the upper bound (1.0) and the mean (0.0) to the features in order to incorporate minimal prior knowledge into the model. 8http://nlp.kuee.kyoto-u.ac.jp/kuntt/ 790 (CR)9, the MPQA Opinion corpus (MPQA)10, the Movie Review Data (MR) 11, and the NTCIR English opinion corpus (NTC-E) (Seki et al., 2007; Seki et al., 2008). The CR corpus consists of review articles about products such as digital cameras and cellular phones. There are two customer review datasets, the 5 products dataset and the 9 products dataset, and we combined the two datasets. In the MPQA corpus, sentiment polarities are attached not to sentences but expressions (sub-sentences), and we regarded the expressions as sentences and classified the polarities. There are two NTCIR English corpora available, the NTCIR-6 corpus and the NTCIR7 corpus, and we combined the two corpora. The statistical information of the corpora we use</context>
</contexts>
<marker>Seki, Evans, Ku, Chen, Kando, Lin, 2007</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-His Chen, Noriko Kando, and Chin-Yew Lin. 2007. Overview of Opinion Analysis Pilot Task at NTCIR6. In Proceedings of the 6th NTCIR Workshop, pages 265–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>David Kirk Evans</author>
<author>Lun-Wei Ku</author>
<author>Hsin-Hsi Chen Le Sun</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of Multilingual Opinion Analysis Task at NTCIR-7.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th NTCIR Workshop.</booktitle>
<marker>Seki, Evans, Ku, Le Sun, Kando, 2008</marker>
<rawString>Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and Noriko Kando. 2008. Overview of Multilingual Opinion Analysis Task at NTCIR-7. In Proceedings of the 7th NTCIR Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Louis-Philippe Morency</author>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Modeling Latent-Dynamic in Shallow Parsing: A Latent Conditional Model with Improved Inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>841--848</pages>
<contexts>
<context position="27604" citStr="Sun et al., 2008" startWordPosition="4516" endWordPosition="4519">l sentiment classification method incorporating head-modifier relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use syntactic dependency structures. Conditional random fields with hidden variables have been studied so far for other tasks. LatentDynamic Conditional Random Fields (LDCRF) (Morency et al., 2007; Sun et al., 2008) are probabilistic models with hidden variables for sequential labeling, and belief propagation is used for inference. Out method is similar to the models, but there are several differences. In our method, only one variable which represents the polarity of the whole sentence is observable, and dependency relation among random variables is not a linear chain but a tree structure which is identical to the syntactic dependency. 5 Conclusion In this paper, we presented a dependency tree-based method for sentiment classification using conditional random fields with hidden variables. In this method,</context>
</contexts>
<marker>Sun, Morency, Okanohara, Tsujii, 2008</marker>
<rawString>Xu Sun, Louis-Philippe Morency, Daisuke Okanohara, and Jun’ichi Tsujii. 2008. Modeling Latent-Dynamic in Shallow Parsing: A Latent Conditional Model with Improved Inference. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 841–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="3490" citStr="Wilson et al. (2005)" startWordPosition="512" endWordPosition="515"> are based on flat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and using it instead of rigid rules seems useful. Wilson et al. (2005) proposed a method for sentiment classification which utilizes head-modifier relation and machine learning. However, the method is based on bag-of-features and polarity reversal occurred by content words is not handled. One issue of the approach to use sentence composition and machine learning is that only the whole sentence is labeled with its polarity in general corpora for sentiment classification, and each component of the sentence is not labeled, though such information is necessary for supervised ma786 Human Language Technologies: The 2010 Annual Conference of the North American Chapter </context>
<context position="14499" citStr="Wilson et al. (2005)" startWordPosition="2393" endWordPosition="2396">ency trees, but MaltParser outputs word-based dependency trees, and we converted the word-based ones to phrase-based ones using simple heuristic rules explained in Appendix A. The prior polarity of a phrase qi ∈ {+1, 0, −1} is the innate sentiment polarity of a word contained in the phrase, which can be obtained from sentiment polarity dictionaries. We used sentiment polarity dictionaries made by Kobayashi et al. (2007) and Higashiyama et al. (2008)4 for Japanese experiments (The resulting dictionary contains 6,974 positive expressions and 8,428 negative expressions), and a dictionary made by Wilson et al. (2005)5 for English experiments (The dictionary contains 2,289 positive expressions and 4,143 negative expressions). When a phrase contains the words registered in the dictionaries, its prior polarity is set to the registered polarity, otherwise the prior polarity is set to 0. When a phrase contains multiple words in the dictionaries, the registered polarity of the last (nearest to the end 2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/ 3http://maltparser.org/ 4http://cl.naist.jp/˜inui/research/EM/sentiment-lexicon.html 5http://www.cs.pitt.edu/mpqa/ log PΛ(pl|wl,hl) − ∑2σ2 k=1 λ2k, (7) K 1 ∑L l=1 LΛ= ∂L</context>
<context position="26944" citStr="Wilson et al. (2005)" startWordPosition="4418" endWordPosition="4421">Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics. In their method, the polarity of the whole sentence is determined from the prior polarities of the composing words by pre-defined rules, and the method differs from ours which uses the probabilistic model to handle interactions between hidden variables. Syntactic structures were used in the studies of Moilanen and Pulman (2007) and, Jia et al. (2009), but their methods are based on rules and supervised learning was not used to handle polarity reversal. As discussed in Section 1, Wilson et al. (2005) studied a bag-of-features based statistical sentiment classification method incorporating head-modifier relation. Ikeda et al. (2008) proposed a machine learning approach to handle sentiment polarity reversal. For each word with prior polarity, whether the polarity is reversed or not is learned with a statistical learning algorithm using its surrounding words as features. The method can handle only words with prior polarities, and does not use syntactic dependency structures. Conditional random fields with hidden variables have been studied so far for other tasks. LatentDynamic Conditional Ra</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis. In Proceedings of the 2005 Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi Annie Zaenen</author>
</authors>
<title>Contextual Lexical Valence Shifters.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text.</booktitle>
<contexts>
<context position="2825" citStr="Zaenen, 2004" startWordPosition="410" endWordPosition="411"> in sentiment classification, sentiment polarities can be reversed. For example, let us consider the sentence “The medicine kills cancer cells.” While the phrase cancer cells has negative polarity, the word kills reverses the polarity, and the whole sentence has positive polarity. Thus, in sentiment classification, a sentence which contains positive (or negative) polarity words does not necessarily have the same polarity as a whole, and we need to consider interactions between words instead of handling words independently. Recently, several methods have been proposed to cope with the problem (Zaenen, 2004; Ikeda et al., 2008). However, these methods are based on flat bag-of-features representation, and do not consider syntactic structures which seem essential to infer the polarity of a whole sentence. Other methods have been proposed which utilize composition of sentences (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Jia et al., 2009), but these methods use rules to handle polarity reversal, and whether polarity reversal occurs or not cannot be learned from labeled data. Statistical machine learning can learn useful information from training data and generally robust for noisy data, and u</context>
</contexts>
<marker>Zaenen, 2004</marker>
<rawString>Livia Polanyi Annie Zaenen. 2004. Contextual Lexical Valence Shifters. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>