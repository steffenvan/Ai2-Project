<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<title confidence="0.834678">
QALL-ME needs AIR: a portability study
</title>
<author confidence="0.895624">
Constantin Or˘asan, Iustin Dornescu and Natalia Ponomareva
</author>
<affiliation confidence="0.8921965">
Research Group in Computational Linguistics
University of Wolverhampton, UK
</affiliation>
<email confidence="0.992666">
C.Orasan, I.Dornescu2, Nata.Ponomareva@wlv.ac.uk
</email>
<sectionHeader confidence="0.987396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9984341875">
Currently access to institutional repositories is
gained using dedicated web interfaces where
users can enter keywords in an attempt to
express their needs. In many cases this approach
is rather cumbersome for users who are required
to learn a syntax specific to that particular
interface. To address this problem, we propose
to adapt the QALL-ME framework, a reusable
framework for fast development of question
answering systems, in order to allow users
to access information using natural language
questions. This paper describes how the web
services part of the QALL-ME framework had to
be adapted in order to give access to information
gathered from unstructured web pages by the
AIR project.
</bodyText>
<sectionHeader confidence="0.977211" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.8100545">
QALL-ME framework, web services, question answering, textual
entailment
</keyword>
<sectionHeader confidence="0.997574" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892625">
Currently access to institutional repositories is gained
using dedicated web interfaces where users can enter
keywords. In many cases this approach is rather
cumbersome for users who are required to learn a
syntax specific to that particular interface. A solution
to this problem is offered by question answering
(QA), a field in computational linguistics, which
develops systems that take a natural language question
and provide the exact answer to it. This paper
presents how the QALL-ME framework1, a reusable
framework for fast development of question answering
systems, was adapted in order to allow users to access
information stored in institutional repositories using
natural language questions.
The AIR project [10] developed a system that
extracts information about scientific publications from
unstructured documents and stores this information
in a database. The QALL-ME project [12] has
developed a framework for implementing question
answering systems for restricted domains. The first
implementation of this framework was for the domain
of tourism, but it is not in any particular way bound
to this domain. For this reason, the QALL-ME
framework can offer the ideal and most natural way
</bodyText>
<footnote confidence="0.999152">
1 The QALL-ME framework is available as an open source
project at http://qallme.sourceforge.net/
</footnote>
<page confidence="0.981321">
50
</page>
<bodyText confidence="0.996305333333333">
of accessing the information extracted in the AIR
project.
The remainder of the paper is structured as follows:
Section 2 presents some background information about
the QALL-ME and AIR projects. The domain in
which the system is expected to run is presented in
Section 3, followed by a description of the framework
and how it was adapted to the new domain in Section
4. The paper finishes with conclusions.
</bodyText>
<sectionHeader confidence="0.897316" genericHeader="method">
2 Background information
</sectionHeader>
<subsectionHeader confidence="0.850712">
2.1 The QALL-ME project
</subsectionHeader>
<bodyText confidence="0.99925624">
QALL-ME (Question Answering Learning
technologies in a multiLingual and Multimodal
Environment) is an EU-funded project with the
objective of developing a shared infrastructure
for multilingual and multimodal open domain
Question Answering.2 It allows users to express their
information needs in the form of multilingual natural
language questions using mobile phones and returns a
list of ranked specific answers rather than the whole
web pages. In the first phase, the tourism domain is
highlighted as the domain in which the operates.
Language variability, one of the main difficulties
of dealing with natural language, is addressed in
QALL-ME by reformulating it as a textual entailment
recognition problem. In textual entailment a text (T)
is said to entail a hypothesis (H), if the meaning of
H can be derived from the meaning of T. To this end,
each question is treated as the text and the hypothesis
is a procedure to answer the question [6]. This concept
is embedded in the QALL-ME Framework [12], one of
the main outputs of the project. The purpose of this
framework is to provide an architecture skeleton for
QA systems that extract answers from structured data
sources. This framework is exploited in this paper to
provide an access to data collected by the AIR project.
</bodyText>
<subsectionHeader confidence="0.978603">
2.2 The AIR project
</subsectionHeader>
<bodyText confidence="0.998911714285714">
Manual population of institutional repositories with
citation data is an extremely time- and resource-
consuming process, and usually acts as a bottleneck
on the fast growth and update of large repositories.
The aim of the AIR project [10] was to develop a
semi-automatic approach for archiving institutional
repositories. To achieve this, it automatically
</bodyText>
<footnote confidence="0.986086">
2 More information about the QALL-ME project can be found
at http://qallme.fbk.eu
</footnote>
<note confidence="0.696881">
Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 50–57
</note>
<figureCaption confidence="0.997841">
Fig. 1: Overview of the AIR architecture
</figureCaption>
<bodyText confidence="0.999763">
discovers and extracts bibliographical data from web
sites, and then interacts with users, authors or
librarians, who verify and correct extracted data. The
components of the AIR system are:
</bodyText>
<listItem confidence="0.999525">
1. A web crawler which populates the database
with all web pages belonging to the domain under
consideration.
2. the AIR core module which processes
unstructured data (web pages) in order to extract
bibliographical references and automatically
annotate them with Dublin Core Metadata tags.
3. Web interfaces developed for user interaction
and data verification. This step was introduced
to ensure the reliability of information transferred
to the digital repository.
4. Data deposit is accomplished automatically using
SWORD3 protocol.
</listItem>
<bodyText confidence="0.98878">
An overview of the system is presented in Figure 1.
In this section only the information extraction
component is briefly presented. The other components
are not relevant for the current research; more details
about them can be found in [10].
Automatic extraction of bibliographical metadata
from unstructured web pages is achieved in three
consecutive steps using a machine learning approach:
</bodyText>
<listItem confidence="0.981323">
1. A page classifier extracts web pages containing
bibliographies from the whole amount of data
provided by the crawler. The classifier exploits
the structure of HTML format such as metadata
(&lt;keywords&gt;, &lt;title&gt;, &lt;description&gt;) and
headers (&lt;h1&gt;, &lt;h2&gt;, etc.) in order to give
different weights to distinct data sources. As a
machine learning method, we used and compared
several methods contained in WEKA [17].
2. A record classifier selects bibliographical entries
from all document records using Conditional
Random Fields (CRF) [4]. As HTML-format
</listItem>
<footnote confidence="0.8630135">
3 http://www.ukoln.ac.uk/repositories/digirep/index/SWORD
guide
</footnote>
<bodyText confidence="0.648722333333333">
provides some structural elements like tags, we
incorporated them into the classifier for revealing
enumerations or lists of equivalent records.
</bodyText>
<listItem confidence="0.990001142857143">
3. An information extraction (IE) module identifies
5 types of bibliographical metadata from Dublin
Core Metadata Element Set: author, title,
date, publisher and citation. As bibliographical
reference represents a logical consequence of
metadata tags, the use of CRF is the most
appropriate.
</listItem>
<bodyText confidence="0.9339675">
Given that all three modules focus on similar
problems we used very similar features while
constructing corresponding classifiers. These features
are:
</bodyText>
<listItem confidence="0.965334037037037">
• Named Entities, such as PERSONS,
LOCATIONS, ORGANIZATIONS and DATES
identified using ANNIE4, a named entity
recogniser distributed together with GATE5
framework [2].
• Staff names: A list of all university members was
collected and used to annotate the input text.
• Sherpa/Romeo publishers and journals the
list of publishers and journals stored in the
Sherpa/Romeo database was retrieved and used
to annotate the input text.
• Presence of year: This feature indicates
whether an element contains a year.
• Publication triggers: We built different lists
of triggers that can indicate different types of
information about a publication. Examples of
such triggers are header indicators (the most
frequent words that occur in &lt;h&gt; tags of
files containing publications), publication triggers
(words appearing in a bibliographical entry itself),
citation triggers (words appeared in citation of an
entry).
• Orthographic features, which capture
capitalisation, digits, punctuation marks, etc.
This feature was only used for implementation of
the IE module.
• Parts of speech (POS). Preliminary
</listItem>
<bodyText confidence="0.8912918">
experiments revealed many cases when
automatically annotated bibliographical fields
ended with articles, conjunctions or prepositions.
In order to correct this situation, we incorporated
parts of speech information into the IE module.
All three modules of the IE component were
evaluated on manually annotated data using cross-
validation [10]. The page classifier obtained an f-
score of 0.882 when using JRIP [1]. The record
classifier obtains the best results (f-score of 0.919) with
a Markov order 1. The information extraction module
can recognise the author, date, title and citation
with high accuracy, but perform rather poorly on
identification of publisher. Detailed evaluation results
can be found in [10].
</bodyText>
<footnote confidence="0.9991545">
4 http://gate.ac.uk/ie/annie.html
5 http://gate.ac.uk/
</footnote>
<page confidence="0.999156">
51
</page>
<bodyText confidence="0.921520375">
3 Domain description and
modelling
An investigation of the domain of bibliographical
references and the data extracted by the AIR project
was carried out in order to define which questions can
be answered by the system and to model the domain
using an ontology. This section describes the domain
and ontology used to represent this domain.
</bodyText>
<subsectionHeader confidence="0.999052">
3.1 The domain
</subsectionHeader>
<bodyText confidence="0.999985076923077">
The data stored by the AIR information extraction
module in the database contains information about
the author(s) of a publication, its title, year of
publication, publisher and the rest of the information
lumped together as “citation”. Using the annotation
modules presented in Section 4.2 it is possible
to extract from the citation the name of the
journal or conference proceeding that published the
article. Using automatic term extraction techniques
or external databases it is possible to automatically
assign keywords to each article. On the basis of
this information, the following types of questions are
foreseen to be answered by the system:
</bodyText>
<listItem confidence="0.998525818181818">
1. Questions about the name of an author who
published in a year, in journal/conference and/or
on a topic (e.g. Which authors had a publication
in &lt;JOURNAL&gt; in &lt;YEAR&gt;?)
2. Questions about the year when an author
published in a journal/conference, on a topic
or with other authors (e.g. What year did
&lt;AUTHOR&gt; have a paper in &lt;JOURNAL&gt;?)
3. Questions about the title of a publication by an
author, in a journal/conference, on a topic, in a
certain year and/or with other authors (e.g. What
papers about &lt;KEYWORDS&gt; did &lt;AUTHOR&gt;
publish in &lt;YEAR&gt;?)
4. Questions about the title of a conference/journal
where an author published on a topic, in a
certain year and/or with other authors (e.g. Who
published &lt;AUTHOR&gt; in &lt;YEAR&gt;?)
5. Questions about the topic of an article published
by an author in a journal/conference, in a certain
year and/or with other authors (e.g. What are
the topics of the papers published in &lt;YEAR&gt; by
&lt;AUTHOR&gt; and &lt;AUTHOR&gt;?)
</listItem>
<bodyText confidence="0.999469272727273">
Each question can have one or several constraints,
but none expects the user to specify the title of the
publication. This is due to the fact that scientific
articles tend to have long titles, which are unlikely
to be remembered correctly and completely by a
user. For this reason, even with the fuzzy matching
implemented in our annotators, it is unlikely that
the system can correctly guess what title a user is
referring to. On the basis of the five types of questions
identified above, 36 types of questions were proposed
to be answered by the system.
</bodyText>
<subsectionHeader confidence="0.999231">
3.2 The ontology
</subsectionHeader>
<bodyText confidence="0.998873444444444">
The purpose of the ontology is to provide a
conceptualised description of the selected domain and
to act as a link between different components of
the system and different languages. The ontology
developed in QALL-ME for the domain of tourism
is described in [9]. Given that the scope of the AIR
project consists of academic citations, we could not use
this ontology and instead we had to find an ontology
which:
</bodyText>
<listItem confidence="0.994446555555556">
• uses standard metadata terminologies such as
Dublin Core (dc and dcterms);
• supports the entry types used by the open BIETEX
reference management software or other similar
schemes;
• allows arbitrary keyword indexing schemes; and
• uses dereferenceable URIs for interoperability
with other systems (faceted browsing, semantic
web mash-ups)
</listItem>
<bodyText confidence="0.9999895">
We decided to use BIETEX as it is very popular in
the academic community and it is supported by many
citation management systems. Moreover, the format
in which AIR stores data can be easily mapped into
the BIETEX format. Fields which are not explicitly
identified by AIR, such as the name of the proceedings,
can be easily extracted using the annotators presented
in Section 4.2. In addition, by using this approach, we
are not limited to using only the output of the AIR
project and we can apply our QA interface to a large
number of sources.
The advantage of using BIETEX as the format of
the input data is that there are several ontologies
that can be used (e.g. the MIT bibtex ontology6,
bibo7, SWRCB. The differences between them are
the vocabularies used and details such as author list
representation and event representation. We chose to
use a subset of the SWRC ontology [14], an ontology
for modelling entities of research communities such
as persons, organisations, publications (bibliographic
metadata) and relationships amongst them. The
main entities involved are: persons (authors and
editors), organisations (publishers, research institutes,
universities), publications (articles, conference papers,
theses, book chapters) and collections (proceedings,
journals, books, series). A relevant subset of the
Dublin Core metadata terminology is used to describe
the properties of the bibliographic entries. An example
of a conference paper in the TURTLE syntax defined
by our ontology can be seen in Listing 1.
</bodyText>
<footnote confidence="0.9717884">
6 http://zeitkunst.org/bibtex/0.1/
7 Bibliographic Ontology Specification
http://bibliontology.com/
8 Semantic Web for Research Communities
http://ontoware.org/projects/swrc/
</footnote>
<page confidence="0.996212">
52
</page>
<tableCaption confidence="0.5486275">
Listing 1: Example of conference paper presented in
TURTLE syntax
</tableCaption>
<table confidence="0.7301288">
qa2 : Mitkov1998
rdf : type swrc : InProceedings ;
dc : title ”Robust pronoun resolution with
limited knowledge” ;
terms: issued ”1998” ;
swrc: pages ”869−875” ;
terms: isPartOf qa2 : conf/acl /2008;
dc:creator qa2 : Mitkov R .
qa2 : conf / acl /2008
rdf:type swrc: Proceedings;
dc:title ”Proceedings of the 18 th
International Conference on
Computational Linguistics (COLING
’98) /ACL’98 Conference” ;
swrc : address ”Montreal , Canada” .
</table>
<bodyText confidence="0.9996615">
The SWRC terminology is also used by the DBLP9
(Digital Bibliography &amp; Library Project) computer
science bibliography website. This makes it easy
to augment the data collected by the AIR project
with bibliographic information from other sources.
This can be further extended by employing protocols
such as Open Archives Initiative (OAI) Metadata
Protocol Handler which is an interchange format
that facilitates metadata harvesting from electronic
repositories or the PRISM10 protocol. Whilst AIR
does not provide us with details such as affiliation
relations, this information could be added from such
sources, enabling the system to answer questions such
as ”Scientists working in which German universities
have published papers about Question Answering in
2008?”.
Using existing software, the data collected by the
AIR project in BIdTEX format was converted to the
RDF format defined by our ontology. For data access
the SPARQL query language was used.
</bodyText>
<subsectionHeader confidence="0.999196">
3.3 Representation of terms
</subsectionHeader>
<bodyText confidence="0.999542">
As seen in Section 3.1, a large number of questions that
can be asked are restricted by topics. These topics
are normally represented using terms and therefore it
was necessary to find a convenient way to represent
terminologies and relationships between them.
Investigation of existing resources revealed that
the skos (Simple Knowledge Organisation System)
ontology 11 is appropriate as it provides a model
for expressing the basic structure and content of
concept schemes. A concept scheme is defined
in the skos ontology as “a set of concepts,
optionally including statements about semantic
relationships between those concepts.” (e.g. thesauri,
classification schemes, terminologies, glossaries,
etc.) The skos ontology is useful for our purposes
as it encodes relations such as skos:broader,
skos:narrower, skos:broaderTransitive and
skos:narrowerTransitive and allows the asking
of questions such as: ”What did Constantin
</bodyText>
<footnote confidence="0.79353075">
9 http://dblp.uni-trier.de/
10 http://www.prismstandard.org/about/
11 (http://www.w3.org/TR/2005/WD-swbp-skos-core-guide-
20050510/)
</footnote>
<figureCaption confidence="0.994191333333333">
Fig. 2: Test indexing terminology (full lines represent
the skos:broader relation while the dotted lines
represent the skos:broaderTransitive relation)
</figureCaption>
<bodyText confidence="0.8993228">
Orasan publish about summarization”? and the
retrieval of papers which were tagged with multi-
document summarization. A part of the terminology
corresponding to computational linguistics can be
found in Figure 2.
</bodyText>
<sectionHeader confidence="0.611414" genericHeader="method">
4 The QALL-ME framework
</sectionHeader>
<bodyText confidence="0.811841">
and its adaptation to a new
domain
The QALL-ME framework is based on a Service
Oriented Architecture (SOA) which, for the domain of
tourism, is realised using the following web services:
</bodyText>
<listItem confidence="0.997997666666667">
1. Context providers: are used to anchor questions
in space and time in this way enabling answers to
temporally and spatially restricted questions
2. Annotators: Currently three types of annotators
are available:
• named entity annotators which identify
names of cinemas, movies, persons, etc.
• term annotators which identify hotel
facilities, movie genres and other domain-
specific terminology
• temporal annotators that are used
to recognise and normalise temporal
expressions in user questions
3. Entailment engine: is used to overcome
the problem of user question variability and
determine whether a user question entails a
retrieval procedure associated with predefined
question patterns.
4. Query generator: which relies on an entailment
engine to generate a query that can be used to
extract the answer to a question from a database.
</listItem>
<page confidence="0.995779">
53
</page>
<bodyText confidence="0.998486857142857">
For the tourism demonstrator the output of this
web service is a SPARQL query.
5. Answer pool: retrieves the answers from a
database. In the case of the tourism demonstrator
the answers are extracted from RDF encoded data
using SPARQL queries.
Given that the QALL-ME framework was
implemented using a modular approach, in order
to adapt the system to a new domain all that was
required was to re-implement or adapt some of the
existing web services. This section describes each
of the web services in more detail emphasising the
changes that had to be made in order to adapt the
system to the new domain.
</bodyText>
<subsectionHeader confidence="0.991901">
4.1 Context provider web service
</subsectionHeader>
<bodyText confidence="0.999756272727273">
Investigation of the bibliographical domain revealed
that the role of this web service is rather limited as
the spatial information is not used in questions. For
this reason, this web service was reduced to returning
the current date and time in order to be able to answer
questions with temporal restrictions such as:
What papers were published in 2009?.
What papers were published this year?.
The context information is used by the temporal
annotator (described in Section 4.2.3) to normalise the
this year expression to the TIMEX2 standard.
</bodyText>
<subsectionHeader confidence="0.979693">
4.2 Annotators
</subsectionHeader>
<bodyText confidence="0.9999656">
The QALL-ME framework provides the possibility
of annotating the input sentences with named
entities, terms specific to the domain and temporal
expressions. This section presents the annotator
services implemented for the bibliographical domain.
</bodyText>
<subsubsectionHeader confidence="0.920329">
4.2.1 Named entity and term annotators
</subsubsectionHeader>
<bodyText confidence="0.998730166666667">
Unlike standard Named Entity Recognition that are
able to determine unknown and new entities, in
restricted domains the entities are known and the
annotation task is reduced to a database look-up that
needs to deal with spelling mistakes, inaccurate entity
references and partial matches. The ability to deal
with noisy input becomes even more important for the
domain of tourism where users have the possibility
of asking questions using speech which means that
the system needs to deal with automatic speech
recognition errors.
For the domain of tourism we had to recognise
named entities such as names of hotels, movies
and persons, as well as terms which are multi-word
expressions related to the domain such as genre
of a movie (e.g. action movie) and site facilities
(e.g. disabled access). Both types of expressions
are identified using the same greedy algorithm that
annotates expressions from a gazetteer, based on a
character-based similarity distance between the tokens
and an adapted TFIDF score.
The main difference between our method and other
approaches, such as [5] and TagLink [13], is that it
distinguishes between high probability similar tokens
and tokens that are most probably distinct. Using
a character-based similarity threshold, we compute a
partial one to one matching and evaluate the negative
impact of the unmatched tokens. The more words
that are matched, the greater the confidence that the
two strings represent the same entity, while a great
number of mismatched words means that the two
strings represent different entities. While matching
tokens a certain amount of spelling variations and
mistakes are allowed by using the character-based
similarity measure proposed by Jaro-Winkler [16].
For the bibliographical domain the same approach
was employed, but different types of entities had to
be annotated. This was achieved by training the
algorithm with different gazetteers. Given the nature
of the domain, a large number of ambiguities were
noticed. These ambiguities are discussed in the next
section.
4.2.2 Ambiguities at the level of entity
annotation
One of the main challenges we had to address when we
ported the annotators to the new domain was that the
names of authors and conferences can be expressed in
several ways and that some entities can have several
types. The former problem is referred to as instance
ambiguity (IA) – entities having the same type and
the same name, and the latter type ambiguity (TA)
– entities of different types which have the same
name (common for acronyms). For example in What
papers has Ruslan Mitkov published in computational
linguistics? the expression computational linguistics
can be both a named entity and refer to the Journal
of Computational Linguistics, or can be a term and
refer to the field of computational linguistics. This
represents a type ambiguity. In the same question, it is
possible to refer to Ruslan Mitkov using R. Mitkov or
just Mitkov which exemplifies an instance ambiguity.
The Entity Annotator marks spans of text from
the question with the type and ID of the entities
they refer to. It retrieves a list of all the entities
of a given type (e.g. [swrc:Person]) from the RDF
data along with their known aliases and creates
an index which will be used to identify and rank
candidate matches in questions. The Entity Annotator
aggregates several distinct annotators, one for each
type (e.g. persons, publishers, conferences) and
applies specialised semantic rules when indexing the
lists extracted from the RDF data (e.g. derive the alias
”Dornescu I.” from ”Dornescu Iustin”, or ”ACL08”
from ”ACL 2008”). Employing such semantic rules
off-line, during data pre-processing is computationally
motivated. At query time, the annotator ranks all
the known alternative names (synonymy) and is also
able to select an ambiguous list of entities which are
equally ranked (polysemy). The latter is an extension
of the initial QALL-ME web services, which allowed
any span of the question to refer to at most one entity.
Since the QALL-ME web services definition does
not allow annotated spans to overlap, when ranking
the candidates, the annotator will prefer longer spans
</bodyText>
<page confidence="0.995995">
54
</page>
<bodyText confidence="0.851509333333333">
and fewer distinct IDs. This constrains both types
of ambiguity. For example when the user inputs the
following question:
What did I. Dornescu publish in CLEF
2008?
the following candidates are considered:
</bodyText>
<equation confidence="0.719573181818182">
1 qa2 : dornescu/iustin ”Dornescu Iustin ”
2 qa2 : dornescu/ iustin * ”Dornescu I . ”
3 qa2 : dornescu/iulian ”Dornescu Iulian ”
4 qa2 : dornescu/iulian * ”Dornescu I . ”
5 qa2 : i /dornescu * ”I . Dornescu”
6 qa2 : dornescu ”Dornescu”
7 qa2 : domnescu/I . ”Domnescu I . ”
8 qa2 : conf / c l e f /2008 * ”CLEF 2008”
9 qa2 : conf / c l e f * ”CLEF”
10 qa2 : lcns / clef2008 * ”CLEF 2008”
11 qa2 : ev/wk/ c l e f /08 * ”CLEF 2008”
</equation>
<bodyText confidence="0.9998608">
has three ambiguous author matches (two of
which could be duplicates), and three ambiguous
acronym matches. Candidate 6 is discarded because
its span overlaps a larger one which has priority
(more matching words means a greater confidence);
candidate 7 has a lower confidence score due to the
edit distance; while candidates 1 and 3 are aliases of
the highest ranked candidates. The advantage when
using this scheme is that the system can inform the
user that ”I. Dornescu” is ambiguous and can suggest
which are the alternatives. ”CLEF 2008” can refer
to: an event (an workshop), a series of events, or two
published proceedings (the electronic Working Notes
and the LCNS volume).
In contrast to the annotators used for the domain of
tourism, due to the large number of ambiguities that
can be present in the questions, the annotators used
here return all the possible annotations of a question,
leaving the rest of the components to select the correct
interpretations.
</bodyText>
<subsectionHeader confidence="0.551473">
4.2.3 Temporal annotator
</subsectionHeader>
<bodyText confidence="0.999967357142857">
Investigation of the domain of tourism, in which
the QALL-ME framework was initially developed,
revealed that a large number of questions contain
temporal constraints. For this reason, the framework
provides the possibility of using a temporal annotator
that identifies temporal expressions and normalises
them using the TIMEX2 standard [3].
The temporal tagger used for the domain of
tourism follows the design and methodology of
the temporal tagger described in [11] that is
capable of identifying both self-contained temporal
expressions(TEs) and indexical/under-specified TEs.
The annotator described in [11] is rule-based and
tackles more cases than necessary, making it too
slow for our purposes. For this reason, a simplified
temporal annotator was implemented [15]. Evaluation
of this simplified temporal annotator revealed that
it performs with high accuracy, most of the errors
being due to the reduced number of rules implemented
to increase its speed and ambiguities specific to the
domain of tourism.
In contrast, questions about publications features
very few temporal expressions, most of them being
references to years (e.g. Which journals/conferences
published [AUTHOR] in [YEAR]?). Theoretically it
is possible to use more precise temporal expressions
which specify both the month and the year, or even the
day, but there are very few bibliographical databases
which contain enough information to allow retrieval
of articles based on the precise date when they were
published. In light of this, we decided to use the
temporal annotator implemented for the domain of
tourism without any change, knowing that it can
handle without a problem both references to years and
specific dates. In addition, it can deal with indexical
references such as this year without a problem.
The output of the temporal annotator is passed
to the TIMEX2SPARQL web service which converts
the TIMEX2 expressions to SPARQL snippets that
are used to extract the answer to questions. This
web service was used directly from the QALL-ME
framework.
</bodyText>
<subsectionHeader confidence="0.991717">
4.3 Entailment engine
</subsectionHeader>
<bodyText confidence="0.999857948717949">
The most common way to answer questions in
restricted domains is to take a natural language
question and transform it to a standard query
language such as SQL. Often this requires performing
deep linguistic analysis and reconstructing the logical
form of the question. Despite the extensive manual
work that goes into such a method, this approach
fails quite often due to the language variability which
allows the same question to be expressed in numerous
ways. This problem is addressed in the QALL-ME
project by using an entailment module that determines
whether different expressions entail the same meaning
and thus can share the same retrieval procedure [6].
The retrieval procedure is a SPARQL12 template that
is instantiated by the query generator (Section 4.4).
The English prototype that works in the domain
of tourism uses an entailment engine which relies
on domain ontology and its alignment to WordNet
[9] to calculate the similarity between two questions
and determine whether there is an entailment relation
between them. Before this similarity score is
calculated, as a pre-processing step, the expected
answer types of the two questions and the types of
entities appearing in the questions are compared and
if they are not compatible the entailment relation is
rejected [7].
For the entailment engine used in the bibliographical
domain, we could not use the existing entailment
engine as we could not easily incorporate the new
domain ontology and we do not have its alignment
to WordNet. Instead, we adapted the expected
answer type module to recognise the five types
of questions presented in Section 3.1 and use
the similarity metric distributed together with the
framework which is language independent. This
similarity metric combines Levenshtein distance,
cosine similarity, Euclidean distance and Monge Elkan
distance. Despite this rather simple approach, the
results are good.
</bodyText>
<footnote confidence="0.834502">
12 http://www.w3.org/2001/sw/DataAccess/
</footnote>
<page confidence="0.996646">
55
</page>
<subsectionHeader confidence="0.973774">
4.4 Query generator
</subsectionHeader>
<bodyText confidence="0.999167351351351">
The role of the Query generator web service is to
produce a valid SPARQL query which can be used to
answer a given question. To achieve this, it has a pre-
prepared set of question patterns together with their
retrieval procedure. The query generator relies on the
entailment engine to determine which question pattern
is entailed by a user question and in this way determine
the retrieval procedure. The retrieval procedure is a
SPARQL template which contains slots that are filled
in using entities from the question.
The query generator used for the domain of tourism
had to be changed in order to be used for the new
domain. The first change was to produce question
patterns and their SPARQL templates for the new
domain. Ou et. al. [8] show how these can be
automatically produced starting from the ontology,
but the results they report are lower than for the
manually produced set. In light of this, the question
patterns and their SPARQL templates were manually
produced on the basis of the types presented in Section
3.1.
The second change introduced was to allow the
query generator to deal with ambiguous entities. The
original query generator expects that each entity has
exactly one interpretation. This is not the case in
the bibliographical domain and the query generator
was modified to gather all the possible interpretations
when calling the entailment engine. For example, in
What did J. Dornescu publish in CLEF
2008?
CLEF 2008 can be interpreted both as name of
proceedings and as an event. Given that there is no
question pattern referring to an event, the entailment
engine is used to rule out the interpretation where
CLEF 2008 is an event.
The SPARQL query generated in this web service is
used in the next step to retrieve the actual answer.
</bodyText>
<subsectionHeader confidence="0.978523">
4.5 Answer pool
</subsectionHeader>
<bodyText confidence="0.999962272727273">
The role of the Answer Pool web service is to take
the SPARQL query generated by the query generator
and retrieve the results from an RDF database. The
answer pool service is domain independent and was
used with almost no changes. For cases where entities
are ambiguous (both type and instance ambiguities)
and the question can be interpreted in several
ways, several SPARQLs are generated and therefore
several answer sets are retrieved. This represents an
improvement over the service used in the domain of
tourism.
The answer pool also plays a role in dealing with
ambiguities. Some SPARQL queries retrieve no results
due to the interpretations of some entities (e.g. in the
case of What did J. Dornescu publish in CLEF 2008?,
J. Dornescu can be interpreted as Julian Dornescu
who is not an author in our database). In this case
the ambiguity is not shown to the user.
For cases where several interpretations of the
question yield answers, a presentation module is used
to show the results in a user-friendly manner. This is
presented in the next section.
</bodyText>
<subsectionHeader confidence="0.965637">
4.6 Presentation module
</subsectionHeader>
<bodyText confidence="0.999949411764706">
The presentation module is domain dependent and
for this reason is not part of the framework. For
the domain of tourism, the output of the system is
textual answers, speech, maps, images and videos and
different presentation modules were used depending
on where the results were displayed (i.e. computer
screen or mobile phone). This are not appropriate for
the bibliographical domain, where for presenting the
results to the user, we currently use the Citeline, a
tool which turns a publication list in BIETEX format
into a visual exhibit.13 To be able to use Citeline the
RDF data retrieved by the Answer Pool web service
is converted back to BIETEX format. A screenshot
of the presentation module is presented in Figure 3.
No language processing is performed at this stage and
ambiguities present in the input question are dealt
with using faceted browsing.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999994428571429">
This paper presented the adaptation of the QALL-
ME Framework to the bibliographical domain. This
process required to create a domain ontology and
adapt the web services that constitute the framework.
To model the bibliographical domain, a subset of
the SWRC ontology [14], an ontology for modelling
entities of research communities such as persons,
organisations, publications (bibliographic metadata)
and relationships amongst them was used. The
terminology specific to the domain was encoded using
the skos ontology.
The QALL-ME Framework is based on a Service
Oriented Architecture (SOA) and realised using web
services. Each of the web services were described
with emphasise on the changes necessary due to the
new domain. One of the main problems that had to
be faced was the large number of ambiguities (both
instance and type ambiguities) that can be present in a
user question. As a result, the annotation web services
had to be changed to allow multiple annotations for
a text span. This in turn, determined changes in the
query generator and answer pool web services, in order
to allow them to deal with multiple interpretations for
a question.
Currently no formal evaluation of the system was
carried out. In the future, we plan to collect questions
from users and perform an on-field evaluation in order
to be able to assess the performance of the system.
</bodyText>
<sectionHeader confidence="0.993113" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9724375">
This work is supported by the EU-funded project
QALL-ME (FP6 IST-033860).
</bodyText>
<sectionHeader confidence="0.995545" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996036333333333">
[1] W. W. Cohen. Fast effective rule induction. In Proceedings of
the Twelfth International Conference on Machine Learning,
pages 115–123, 1995.
</reference>
<footnote confidence="0.501318">
13 http://citeline.mit.edu/
</footnote>
<page confidence="0.994576">
56
</page>
<figureCaption confidence="0.833033">
Fig. 3: Faceted browsing of results using powered by MIT Citeline, Babel and Exhibit
</figureCaption>
<reference confidence="0.999863347826087">
[2] H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. GATE: A Framework and Graphical Development
Environment for Robust NLP Tools and Applications. In
Proceedings of ACL02, 2002.
[3] L. Ferro, L. Gerber, I. Mani, B. Sundheim, and W. G. TIDES
2005 Standard for the Annotation of Temporal Expressions.
Technical report, MITRE, April 2005.
[4] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML ’01: Proceedings of the
Eighteenth International Conference on Machine Learning,
pages 282–289, San Francisco, CA, USA, 2001. Morgan
Kaufmann Publishers Inc.
[5] A. E. Monge and C. Elkan. The field matching problem:
Algorithms and applications. In KDD, pages 267–270, 1996.
[6] M. Negri, B. Magnini, and M. O. Kouylekov. Detecting
expected answer relations through textual entailment. In
9th International Conference on Intelligent Text Processing
and Computational Linguistics, pages 532–543, Heidelberg,
Germany, 2008. Springer.
[7] S. Ou, D. Mekhaldi, and C. Or˘asan. An ontology-based
question answering method with the use of textual entailment.
In Proceedings of NLPKE09, 2009.
[8] S. Ou, C. Or˘asan, D. Mekhaldi, and L. Hasler. Automatic
question pattern generation for ontology-based question
answering. In Proceedings of the 21st International
Florida Artificial Intelligence Research Society Conference
(FLAIRS2008), pages 183 – 188. Menlo Park, CA: AAAI
Press, 2008.
[9] S. Ou, V. Pekar, C. Or˘asan, C. Spurk, and M. Negri.
Development and alignment of a domain-specific ontology
for question answering. In European Language Resources
Association (ELRA), editor, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC’08), Marrakech, Morocco, May 28 – 30 2008.
[10] N. Ponomareva, J. M. Gomez, and V. Pekar. Air: a
semi-automatic system for archiving institutional repositories.
In Proceedings of the 14th International Conference on
Applications of Natural Language to Information Systems
(NLDB 2009), Saarbrucken, Germany, June 24-26 2009.
[11] G. Pu¸sca¸su. A framework for temporal resolution. In
Proceedings of the 4th Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal, May, 26-28
2004.
[12] B. Sacaleanu, C. Orasan, C. Spurk, S. Ou, O. Ferrandez,
M. Kouylekov, and M. Negri. Entailment-based question
answering for structured data. In Coling 2008: Companion
volume: Posters and Demonstrations, pages 29 – 32,
Manchester, UK, August 2008.
[13] A. Salhi and H. Camacho. A string metric based on a one-
to-one greedy matching algorithm. Research in Computer
Science, number 19:171–182, 2006.
[14] Y. Sure, S. Bloehdorn, P. Haase, J. Hartmann, and D. Oberle.
The swrc ontology - semantic web for research communities.
In C. Bento, A. Cardoso, and G. Dias, editors, Proceedings
of the 12th Portuguese Conference on Artificial Intelligence
- Progress in Artificial Intelligence (EPIA 2005), volume
3803 of LNCS, pages 218 – 231, Covilha, Portugal, DEC 2005.
Springer.
[15] A. Varga, G. Pu¸sca¸su, and C. Or˘asan. Identification of
temporal expressions in the domain of tourism. In Knowledge
Engineering: Principles and Techniques, volume 1, pages 29
– 32, Cluj-Napoca, Romania, July 2 – 4 2009.
[16] W. E. Winkler. The state of record linkage and current research
problems. Technical report, Statistical Research Division, U.S.
Bureau of the Census, 1999.
[17] I. H. Witten and E. Frank. Data Mining: Practical
Machine Learning Tools and Techniques. Morgan Kaufmann
Publishers, 2005.
</reference>
<page confidence="0.999142">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.545214">
<title confidence="0.997156">QALL-ME needs AIR: a portability study</title>
<author confidence="0.99283">Constantin Or˘asan</author>
<author confidence="0.99283">Iustin Dornescu</author>
<author confidence="0.99283">Natalia</author>
<affiliation confidence="0.995377">Research Group in Computational University of Wolverhampton,</affiliation>
<email confidence="0.711575">I.Dornescu2,</email>
<abstract confidence="0.9988">Currently access to institutional repositories is gained using dedicated web interfaces where users can enter keywords in an attempt to express their needs. In many cases this approach is rather cumbersome for users who are required to learn a syntax specific to that particular interface. To address this problem, we propose to adapt the QALL-ME framework, a reusable framework for fast development of question answering systems, in order to allow users to access information using natural language questions. This paper describes how the web services part of the QALL-ME framework had to be adapted in order to give access to information gathered from unstructured web pages by the AIR project.</abstract>
<keyword confidence="0.9561725">Keywords QALL-ME framework, web services, question answering, textual</keyword>
<intro confidence="0.850447">entailment</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Twelfth International Conference on Machine Learning,</booktitle>
<pages>115--123</pages>
<contexts>
<context position="8499" citStr="[1]" startWordPosition="1277" endWordPosition="1277">ry). • Orthographic features, which capture capitalisation, digits, punctuation marks, etc. This feature was only used for implementation of the IE module. • Parts of speech (POS). Preliminary experiments revealed many cases when automatically annotated bibliographical fields ended with articles, conjunctions or prepositions. In order to correct this situation, we incorporated parts of speech information into the IE module. All three modules of the IE component were evaluated on manually annotated data using crossvalidation [10]. The page classifier obtained an fscore of 0.882 when using JRIP [1]. The record classifier obtains the best results (f-score of 0.919) with a Markov order 1. The information extraction module can recognise the author, date, title and citation with high accuracy, but perform rather poorly on identification of publisher. Detailed evaluation results can be found in [10]. 4 http://gate.ac.uk/ie/annie.html 5 http://gate.ac.uk/ 51 3 Domain description and modelling An investigation of the domain of bibliographical references and the data extracted by the AIR project was carried out in order to define which questions can be answered by the system and to model the do</context>
</contexts>
<marker>[1]</marker>
<rawString>W. W. Cohen. Fast effective rule induction. In Proceedings of the Twelfth International Conference on Machine Learning, pages 115–123, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GATE</author>
</authors>
<title>A Framework and Graphical Development Environment for Robust NLP Tools and Applications.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02,</booktitle>
<contexts>
<context position="7163" citStr="[2]" startWordPosition="1076" endWordPosition="1076">information extraction (IE) module identifies 5 types of bibliographical metadata from Dublin Core Metadata Element Set: author, title, date, publisher and citation. As bibliographical reference represents a logical consequence of metadata tags, the use of CRF is the most appropriate. Given that all three modules focus on similar problems we used very similar features while constructing corresponding classifiers. These features are: • Named Entities, such as PERSONS, LOCATIONS, ORGANIZATIONS and DATES identified using ANNIE4, a named entity recogniser distributed together with GATE5 framework [2]. • Staff names: A list of all university members was collected and used to annotate the input text. • Sherpa/Romeo publishers and journals the list of publishers and journals stored in the Sherpa/Romeo database was retrieved and used to annotate the input text. • Presence of year: This feature indicates whether an element contains a year. • Publication triggers: We built different lists of triggers that can indicate different types of information about a publication. Examples of such triggers are header indicators (the most frequent words that occur in &lt;h&gt; tags of files containing publication</context>
</contexts>
<marker>[2]</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. In Proceedings of ACL02, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferro</author>
<author>L Gerber</author>
<author>I Mani</author>
<author>B Sundheim</author>
<author>W G</author>
</authors>
<title>Standard for the Annotation of Temporal Expressions.</title>
<date>2005</date>
<tech>TIDES</tech>
<contexts>
<context position="25437" citStr="[3]" startWordPosition="3960" endWordPosition="3960">f tourism, due to the large number of ambiguities that can be present in the questions, the annotators used here return all the possible annotations of a question, leaving the rest of the components to select the correct interpretations. 4.2.3 Temporal annotator Investigation of the domain of tourism, in which the QALL-ME framework was initially developed, revealed that a large number of questions contain temporal constraints. For this reason, the framework provides the possibility of using a temporal annotator that identifies temporal expressions and normalises them using the TIMEX2 standard [3]. The temporal tagger used for the domain of tourism follows the design and methodology of the temporal tagger described in [11] that is capable of identifying both self-contained temporal expressions(TEs) and indexical/under-specified TEs. The annotator described in [11] is rule-based and tackles more cases than necessary, making it too slow for our purposes. For this reason, a simplified temporal annotator was implemented [15]. Evaluation of this simplified temporal annotator revealed that it performs with high accuracy, most of the errors being due to the reduced number of rules implemented</context>
</contexts>
<marker>[3]</marker>
<rawString>L. Ferro, L. Gerber, I. Mani, B. Sundheim, and W. G. TIDES 2005 Standard for the Annotation of Temporal Expressions. Technical report, MITRE, April 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Lafferty</author>
<author>A McCallum</author>
<author>F C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="6328" citStr="[4]" startWordPosition="965" endWordPosition="965">chieved in three consecutive steps using a machine learning approach: 1. A page classifier extracts web pages containing bibliographies from the whole amount of data provided by the crawler. The classifier exploits the structure of HTML format such as metadata (&lt;keywords&gt;, &lt;title&gt;, &lt;description&gt;) and headers (&lt;h1&gt;, &lt;h2&gt;, etc.) in order to give different weights to distinct data sources. As a machine learning method, we used and compared several methods contained in WEKA [17]. 2. A record classifier selects bibliographical entries from all document records using Conditional Random Fields (CRF) [4]. As HTML-format 3 http://www.ukoln.ac.uk/repositories/digirep/index/SWORD guide provides some structural elements like tags, we incorporated them into the classifier for revealing enumerations or lists of equivalent records. 3. An information extraction (IE) module identifies 5 types of bibliographical metadata from Dublin Core Metadata Element Set: author, title, date, publisher and citation. As bibliographical reference represents a logical consequence of metadata tags, the use of CRF is the most appropriate. Given that all three modules focus on similar problems we used very similar featur</context>
</contexts>
<marker>[4]</marker>
<rawString>J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A E Monge</author>
<author>C Elkan</author>
</authors>
<title>The field matching problem: Algorithms and applications. In</title>
<date>1996</date>
<booktitle>KDD,</booktitle>
<pages>267--270</pages>
<contexts>
<context position="20372" citStr="[5]" startWordPosition="3120" endWordPosition="3120">s to deal with automatic speech recognition errors. For the domain of tourism we had to recognise named entities such as names of hotels, movies and persons, as well as terms which are multi-word expressions related to the domain such as genre of a movie (e.g. action movie) and site facilities (e.g. disabled access). Both types of expressions are identified using the same greedy algorithm that annotates expressions from a gazetteer, based on a character-based similarity distance between the tokens and an adapted TFIDF score. The main difference between our method and other approaches, such as [5] and TagLink [13], is that it distinguishes between high probability similar tokens and tokens that are most probably distinct. Using a character-based similarity threshold, we compute a partial one to one matching and evaluate the negative impact of the unmatched tokens. The more words that are matched, the greater the confidence that the two strings represent the same entity, while a great number of mismatched words means that the two strings represent different entities. While matching tokens a certain amount of spelling variations and mistakes are allowed by using the character-based simil</context>
</contexts>
<marker>[5]</marker>
<rawString>A. E. Monge and C. Elkan. The field matching problem: Algorithms and applications. In KDD, pages 267–270, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Negri</author>
<author>B Magnini</author>
<author>M O Kouylekov</author>
</authors>
<title>Detecting expected answer relations through textual entailment.</title>
<date>2008</date>
<booktitle>In 9th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>532--543</pages>
<publisher>Springer.</publisher>
<location>Heidelberg, Germany,</location>
<contexts>
<context position="3732" citStr="[6]" startWordPosition="570" endWordPosition="570">s using mobile phones and returns a list of ranked specific answers rather than the whole web pages. In the first phase, the tourism domain is highlighted as the domain in which the operates. Language variability, one of the main difficulties of dealing with natural language, is addressed in QALL-ME by reformulating it as a textual entailment recognition problem. In textual entailment a text (T) is said to entail a hypothesis (H), if the meaning of H can be derived from the meaning of T. To this end, each question is treated as the text and the hypothesis is a procedure to answer the question [6]. This concept is embedded in the QALL-ME Framework [12], one of the main outputs of the project. The purpose of this framework is to provide an architecture skeleton for QA systems that extract answers from structured data sources. This framework is exploited in this paper to provide an access to data collected by the AIR project. 2.2 The AIR project Manual population of institutional repositories with citation data is an extremely time- and resourceconsuming process, and usually acts as a bottleneck on the fast growth and update of large repositories. The aim of the AIR project [10] was to d</context>
<context position="27820" citStr="[6]" startWordPosition="4331" endWordPosition="4331">ns is to take a natural language question and transform it to a standard query language such as SQL. Often this requires performing deep linguistic analysis and reconstructing the logical form of the question. Despite the extensive manual work that goes into such a method, this approach fails quite often due to the language variability which allows the same question to be expressed in numerous ways. This problem is addressed in the QALL-ME project by using an entailment module that determines whether different expressions entail the same meaning and thus can share the same retrieval procedure [6]. The retrieval procedure is a SPARQL12 template that is instantiated by the query generator (Section 4.4). The English prototype that works in the domain of tourism uses an entailment engine which relies on domain ontology and its alignment to WordNet [9] to calculate the similarity between two questions and determine whether there is an entailment relation between them. Before this similarity score is calculated, as a pre-processing step, the expected answer types of the two questions and the types of entities appearing in the questions are compared and if they are not compatible the entailm</context>
</contexts>
<marker>[6]</marker>
<rawString>M. Negri, B. Magnini, and M. O. Kouylekov. Detecting expected answer relations through textual entailment. In 9th International Conference on Intelligent Text Processing and Computational Linguistics, pages 532–543, Heidelberg, Germany, 2008. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ou</author>
<author>D Mekhaldi</author>
<author>C Or˘asan</author>
</authors>
<title>An ontology-based question answering method with the use of textual entailment.</title>
<date>2009</date>
<booktitle>In Proceedings of NLPKE09,</booktitle>
<contexts>
<context position="28448" citStr="[7]" startWordPosition="4430" endWordPosition="4430"> is a SPARQL12 template that is instantiated by the query generator (Section 4.4). The English prototype that works in the domain of tourism uses an entailment engine which relies on domain ontology and its alignment to WordNet [9] to calculate the similarity between two questions and determine whether there is an entailment relation between them. Before this similarity score is calculated, as a pre-processing step, the expected answer types of the two questions and the types of entities appearing in the questions are compared and if they are not compatible the entailment relation is rejected [7]. For the entailment engine used in the bibliographical domain, we could not use the existing entailment engine as we could not easily incorporate the new domain ontology and we do not have its alignment to WordNet. Instead, we adapted the expected answer type module to recognise the five types of questions presented in Section 3.1 and use the similarity metric distributed together with the framework which is language independent. This similarity metric combines Levenshtein distance, cosine similarity, Euclidean distance and Monge Elkan distance. Despite this rather simple approach, the result</context>
</contexts>
<marker>[7]</marker>
<rawString>S. Ou, D. Mekhaldi, and C. Or˘asan. An ontology-based question answering method with the use of textual entailment. In Proceedings of NLPKE09, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ou</author>
<author>C Or˘asan</author>
<author>D Mekhaldi</author>
<author>L Hasler</author>
</authors>
<title>Automatic question pattern generation for ontology-based question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of the 21st International Florida Artificial Intelligence Research Society Conference (FLAIRS2008), pages 183 – 188. Menlo</booktitle>
<publisher>AAAI Press,</publisher>
<location>Park, CA:</location>
<contexts>
<context position="29862" citStr="[8]" startWordPosition="4657" endWordPosition="4657"> this, it has a preprepared set of question patterns together with their retrieval procedure. The query generator relies on the entailment engine to determine which question pattern is entailed by a user question and in this way determine the retrieval procedure. The retrieval procedure is a SPARQL template which contains slots that are filled in using entities from the question. The query generator used for the domain of tourism had to be changed in order to be used for the new domain. The first change was to produce question patterns and their SPARQL templates for the new domain. Ou et. al. [8] show how these can be automatically produced starting from the ontology, but the results they report are lower than for the manually produced set. In light of this, the question patterns and their SPARQL templates were manually produced on the basis of the types presented in Section 3.1. The second change introduced was to allow the query generator to deal with ambiguous entities. The original query generator expects that each entity has exactly one interpretation. This is not the case in the bibliographical domain and the query generator was modified to gather all the possible interpretation</context>
</contexts>
<marker>[8]</marker>
<rawString>S. Ou, C. Or˘asan, D. Mekhaldi, and L. Hasler. Automatic question pattern generation for ontology-based question answering. In Proceedings of the 21st International Florida Artificial Intelligence Research Society Conference (FLAIRS2008), pages 183 – 188. Menlo Park, CA: AAAI Press, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ou</author>
<author>V Pekar</author>
<author>C Or˘asan</author>
<author>C Spurk</author>
<author>M Negri</author>
</authors>
<title>Development and alignment of a domain-specific ontology for question answering.</title>
<date></date>
<booktitle>In European Language Resources Association (ELRA), editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08),</booktitle>
<volume>28</volume>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="11647" citStr="[9]" startWordPosition="1792" endWordPosition="1792">rectly and completely by a user. For this reason, even with the fuzzy matching implemented in our annotators, it is unlikely that the system can correctly guess what title a user is referring to. On the basis of the five types of questions identified above, 36 types of questions were proposed to be answered by the system. 3.2 The ontology The purpose of the ontology is to provide a conceptualised description of the selected domain and to act as a link between different components of the system and different languages. The ontology developed in QALL-ME for the domain of tourism is described in [9]. Given that the scope of the AIR project consists of academic citations, we could not use this ontology and instead we had to find an ontology which: • uses standard metadata terminologies such as Dublin Core (dc and dcterms); • supports the entry types used by the open BIETEX reference management software or other similar schemes; • allows arbitrary keyword indexing schemes; and • uses dereferenceable URIs for interoperability with other systems (faceted browsing, semantic web mash-ups) We decided to use BIETEX as it is very popular in the academic community and it is supported by many citat</context>
<context position="28076" citStr="[9]" startWordPosition="4372" endWordPosition="4372">to such a method, this approach fails quite often due to the language variability which allows the same question to be expressed in numerous ways. This problem is addressed in the QALL-ME project by using an entailment module that determines whether different expressions entail the same meaning and thus can share the same retrieval procedure [6]. The retrieval procedure is a SPARQL12 template that is instantiated by the query generator (Section 4.4). The English prototype that works in the domain of tourism uses an entailment engine which relies on domain ontology and its alignment to WordNet [9] to calculate the similarity between two questions and determine whether there is an entailment relation between them. Before this similarity score is calculated, as a pre-processing step, the expected answer types of the two questions and the types of entities appearing in the questions are compared and if they are not compatible the entailment relation is rejected [7]. For the entailment engine used in the bibliographical domain, we could not use the existing entailment engine as we could not easily incorporate the new domain ontology and we do not have its alignment to WordNet. Instead, we </context>
</contexts>
<marker>[9]</marker>
<rawString>S. Ou, V. Pekar, C. Or˘asan, C. Spurk, and M. Negri. Development and alignment of a domain-specific ontology for question answering. In European Language Resources Association (ELRA), editor, Proceedings of the Sixth International Language Resources and Evaluation (LREC’08), Marrakech, Morocco, May 28 – 30 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ponomareva</author>
<author>J M Gomez</author>
<author>V Pekar</author>
</authors>
<title>Air: a semi-automatic system for archiving institutional repositories.</title>
<date>2009</date>
<booktitle>In Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems (NLDB</booktitle>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="1749" citStr="[10]" startWordPosition="255" endWordPosition="255">In many cases this approach is rather cumbersome for users who are required to learn a syntax specific to that particular interface. A solution to this problem is offered by question answering (QA), a field in computational linguistics, which develops systems that take a natural language question and provide the exact answer to it. This paper presents how the QALL-ME framework1, a reusable framework for fast development of question answering systems, was adapted in order to allow users to access information stored in institutional repositories using natural language questions. The AIR project [10] developed a system that extracts information about scientific publications from unstructured documents and stores this information in a database. The QALL-ME project [12] has developed a framework for implementing question answering systems for restricted domains. The first implementation of this framework was for the domain of tourism, but it is not in any particular way bound to this domain. For this reason, the QALL-ME framework can offer the ideal and most natural way 1 The QALL-ME framework is available as an open source project at http://qallme.sourceforge.net/ 50 of accessing the infor</context>
<context position="4323" citStr="[10]" startWordPosition="668" endWordPosition="668"> question [6]. This concept is embedded in the QALL-ME Framework [12], one of the main outputs of the project. The purpose of this framework is to provide an architecture skeleton for QA systems that extract answers from structured data sources. This framework is exploited in this paper to provide an access to data collected by the AIR project. 2.2 The AIR project Manual population of institutional repositories with citation data is an extremely time- and resourceconsuming process, and usually acts as a bottleneck on the fast growth and update of large repositories. The aim of the AIR project [10] was to develop a semi-automatic approach for archiving institutional repositories. To achieve this, it automatically 2 More information about the QALL-ME project can be found at http://qallme.fbk.eu Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 50–57 Fig. 1: Overview of the AIR architecture discovers and extracts bibliographical data from web sites, and then interacts with users, authors or librarians, who verify and correct extracted data. The components of the AIR system are: 1. A web crawler which populates the database with all we</context>
<context position="5642" citStr="[10]" startWordPosition="865" endWordPosition="865">pages) in order to extract bibliographical references and automatically annotate them with Dublin Core Metadata tags. 3. Web interfaces developed for user interaction and data verification. This step was introduced to ensure the reliability of information transferred to the digital repository. 4. Data deposit is accomplished automatically using SWORD3 protocol. An overview of the system is presented in Figure 1. In this section only the information extraction component is briefly presented. The other components are not relevant for the current research; more details about them can be found in [10]. Automatic extraction of bibliographical metadata from unstructured web pages is achieved in three consecutive steps using a machine learning approach: 1. A page classifier extracts web pages containing bibliographies from the whole amount of data provided by the crawler. The classifier exploits the structure of HTML format such as metadata (&lt;keywords&gt;, &lt;title&gt;, &lt;description&gt;) and headers (&lt;h1&gt;, &lt;h2&gt;, etc.) in order to give different weights to distinct data sources. As a machine learning method, we used and compared several methods contained in WEKA [17]. 2. A record classifier selects bibli</context>
<context position="8430" citStr="[10]" startWordPosition="1264" endWordPosition="1264">entry itself), citation triggers (words appeared in citation of an entry). • Orthographic features, which capture capitalisation, digits, punctuation marks, etc. This feature was only used for implementation of the IE module. • Parts of speech (POS). Preliminary experiments revealed many cases when automatically annotated bibliographical fields ended with articles, conjunctions or prepositions. In order to correct this situation, we incorporated parts of speech information into the IE module. All three modules of the IE component were evaluated on manually annotated data using crossvalidation [10]. The page classifier obtained an fscore of 0.882 when using JRIP [1]. The record classifier obtains the best results (f-score of 0.919) with a Markov order 1. The information extraction module can recognise the author, date, title and citation with high accuracy, but perform rather poorly on identification of publisher. Detailed evaluation results can be found in [10]. 4 http://gate.ac.uk/ie/annie.html 5 http://gate.ac.uk/ 51 3 Domain description and modelling An investigation of the domain of bibliographical references and the data extracted by the AIR project was carried out in order to def</context>
</contexts>
<marker>[10]</marker>
<rawString>N. Ponomareva, J. M. Gomez, and V. Pekar. Air: a semi-automatic system for archiving institutional repositories. In Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems (NLDB 2009), Saarbrucken, Germany, June 24-26 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Pu¸sca¸su</author>
</authors>
<title>A framework for temporal resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="25565" citStr="[11]" startWordPosition="3981" endWordPosition="3981"> possible annotations of a question, leaving the rest of the components to select the correct interpretations. 4.2.3 Temporal annotator Investigation of the domain of tourism, in which the QALL-ME framework was initially developed, revealed that a large number of questions contain temporal constraints. For this reason, the framework provides the possibility of using a temporal annotator that identifies temporal expressions and normalises them using the TIMEX2 standard [3]. The temporal tagger used for the domain of tourism follows the design and methodology of the temporal tagger described in [11] that is capable of identifying both self-contained temporal expressions(TEs) and indexical/under-specified TEs. The annotator described in [11] is rule-based and tackles more cases than necessary, making it too slow for our purposes. For this reason, a simplified temporal annotator was implemented [15]. Evaluation of this simplified temporal annotator revealed that it performs with high accuracy, most of the errors being due to the reduced number of rules implemented to increase its speed and ambiguities specific to the domain of tourism. In contrast, questions about publications features ver</context>
</contexts>
<marker>[11]</marker>
<rawString>G. Pu¸sca¸su. A framework for temporal resolution. In Proceedings of the 4th Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal, May, 26-28 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sacaleanu</author>
<author>C Orasan</author>
<author>C Spurk</author>
<author>S Ou</author>
<author>O Ferrandez</author>
<author>M Kouylekov</author>
<author>M Negri</author>
</authors>
<title>Entailment-based question answering for structured data.</title>
<date>2008</date>
<booktitle>In Coling 2008: Companion volume: Posters and Demonstrations, pages 29 – 32,</booktitle>
<location>Manchester, UK,</location>
<contexts>
<context position="1920" citStr="[12]" startWordPosition="278" endWordPosition="278">by question answering (QA), a field in computational linguistics, which develops systems that take a natural language question and provide the exact answer to it. This paper presents how the QALL-ME framework1, a reusable framework for fast development of question answering systems, was adapted in order to allow users to access information stored in institutional repositories using natural language questions. The AIR project [10] developed a system that extracts information about scientific publications from unstructured documents and stores this information in a database. The QALL-ME project [12] has developed a framework for implementing question answering systems for restricted domains. The first implementation of this framework was for the domain of tourism, but it is not in any particular way bound to this domain. For this reason, the QALL-ME framework can offer the ideal and most natural way 1 The QALL-ME framework is available as an open source project at http://qallme.sourceforge.net/ 50 of accessing the information extracted in the AIR project. The remainder of the paper is structured as follows: Section 2 presents some background information about the QALL-ME and AIR projects</context>
<context position="3788" citStr="[12]" startWordPosition="579" endWordPosition="579">ific answers rather than the whole web pages. In the first phase, the tourism domain is highlighted as the domain in which the operates. Language variability, one of the main difficulties of dealing with natural language, is addressed in QALL-ME by reformulating it as a textual entailment recognition problem. In textual entailment a text (T) is said to entail a hypothesis (H), if the meaning of H can be derived from the meaning of T. To this end, each question is treated as the text and the hypothesis is a procedure to answer the question [6]. This concept is embedded in the QALL-ME Framework [12], one of the main outputs of the project. The purpose of this framework is to provide an architecture skeleton for QA systems that extract answers from structured data sources. This framework is exploited in this paper to provide an access to data collected by the AIR project. 2.2 The AIR project Manual population of institutional repositories with citation data is an extremely time- and resourceconsuming process, and usually acts as a bottleneck on the fast growth and update of large repositories. The aim of the AIR project [10] was to develop a semi-automatic approach for archiving instituti</context>
</contexts>
<marker>[12]</marker>
<rawString>B. Sacaleanu, C. Orasan, C. Spurk, S. Ou, O. Ferrandez, M. Kouylekov, and M. Negri. Entailment-based question answering for structured data. In Coling 2008: Companion volume: Posters and Demonstrations, pages 29 – 32, Manchester, UK, August 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Salhi</author>
<author>H Camacho</author>
</authors>
<title>A string metric based on a oneto-one greedy matching algorithm.</title>
<date>2006</date>
<journal>Research in Computer Science,</journal>
<contexts>
<context position="20389" citStr="[13]" startWordPosition="3123" endWordPosition="3123">utomatic speech recognition errors. For the domain of tourism we had to recognise named entities such as names of hotels, movies and persons, as well as terms which are multi-word expressions related to the domain such as genre of a movie (e.g. action movie) and site facilities (e.g. disabled access). Both types of expressions are identified using the same greedy algorithm that annotates expressions from a gazetteer, based on a character-based similarity distance between the tokens and an adapted TFIDF score. The main difference between our method and other approaches, such as [5] and TagLink [13], is that it distinguishes between high probability similar tokens and tokens that are most probably distinct. Using a character-based similarity threshold, we compute a partial one to one matching and evaluate the negative impact of the unmatched tokens. The more words that are matched, the greater the confidence that the two strings represent the same entity, while a great number of mismatched words means that the two strings represent different entities. While matching tokens a certain amount of spelling variations and mistakes are allowed by using the character-based similarity measure pro</context>
</contexts>
<marker>[13]</marker>
<rawString>A. Salhi and H. Camacho. A string metric based on a oneto-one greedy matching algorithm. Research in Computer Science, number 19:171–182, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Sure</author>
<author>S Bloehdorn</author>
<author>P Haase</author>
<author>J Hartmann</author>
<author>D Oberle</author>
</authors>
<title>The swrc ontology - semantic web for research communities.</title>
<date>2005</date>
<booktitle>Proceedings of the 12th Portuguese Conference on Artificial Intelligence - Progress in Artificial Intelligence (EPIA 2005),</booktitle>
<volume>3803</volume>
<pages>218--231</pages>
<editor>In C. Bento, A. Cardoso, and G. Dias, editors,</editor>
<publisher>Springer.</publisher>
<location>Covilha, Portugal,</location>
<contexts>
<context position="13025" citStr="[14]" startWordPosition="2024" endWordPosition="2024">the name of the proceedings, can be easily extracted using the annotators presented in Section 4.2. In addition, by using this approach, we are not limited to using only the output of the AIR project and we can apply our QA interface to a large number of sources. The advantage of using BIETEX as the format of the input data is that there are several ontologies that can be used (e.g. the MIT bibtex ontology6, bibo7, SWRCB. The differences between them are the vocabularies used and details such as author list representation and event representation. We chose to use a subset of the SWRC ontology [14], an ontology for modelling entities of research communities such as persons, organisations, publications (bibliographic metadata) and relationships amongst them. The main entities involved are: persons (authors and editors), organisations (publishers, research institutes, universities), publications (articles, conference papers, theses, book chapters) and collections (proceedings, journals, books, series). A relevant subset of the Dublin Core metadata terminology is used to describe the properties of the bibliographic entries. An example of a conference paper in the TURTLE syntax defined by o</context>
<context position="33133" citStr="[14]" startWordPosition="5201" endWordPosition="5201">3 To be able to use Citeline the RDF data retrieved by the Answer Pool web service is converted back to BIETEX format. A screenshot of the presentation module is presented in Figure 3. No language processing is performed at this stage and ambiguities present in the input question are dealt with using faceted browsing. 5 Conclusions This paper presented the adaptation of the QALLME Framework to the bibliographical domain. This process required to create a domain ontology and adapt the web services that constitute the framework. To model the bibliographical domain, a subset of the SWRC ontology [14], an ontology for modelling entities of research communities such as persons, organisations, publications (bibliographic metadata) and relationships amongst them was used. The terminology specific to the domain was encoded using the skos ontology. The QALL-ME Framework is based on a Service Oriented Architecture (SOA) and realised using web services. Each of the web services were described with emphasise on the changes necessary due to the new domain. One of the main problems that had to be faced was the large number of ambiguities (both instance and type ambiguities) that can be present in a </context>
</contexts>
<marker>[14]</marker>
<rawString>Y. Sure, S. Bloehdorn, P. Haase, J. Hartmann, and D. Oberle. The swrc ontology - semantic web for research communities. In C. Bento, A. Cardoso, and G. Dias, editors, Proceedings of the 12th Portuguese Conference on Artificial Intelligence - Progress in Artificial Intelligence (EPIA 2005), volume 3803 of LNCS, pages 218 – 231, Covilha, Portugal, DEC 2005. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Varga</author>
<author>G Pu¸sca¸su</author>
<author>C Or˘asan</author>
</authors>
<title>Identification of temporal expressions in the domain of tourism.</title>
<date></date>
<booktitle>In Knowledge Engineering: Principles and Techniques,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<location>Cluj-Napoca, Romania,</location>
<contexts>
<context position="25869" citStr="[15]" startWordPosition="4023" endWordPosition="4023">For this reason, the framework provides the possibility of using a temporal annotator that identifies temporal expressions and normalises them using the TIMEX2 standard [3]. The temporal tagger used for the domain of tourism follows the design and methodology of the temporal tagger described in [11] that is capable of identifying both self-contained temporal expressions(TEs) and indexical/under-specified TEs. The annotator described in [11] is rule-based and tackles more cases than necessary, making it too slow for our purposes. For this reason, a simplified temporal annotator was implemented [15]. Evaluation of this simplified temporal annotator revealed that it performs with high accuracy, most of the errors being due to the reduced number of rules implemented to increase its speed and ambiguities specific to the domain of tourism. In contrast, questions about publications features very few temporal expressions, most of them being references to years (e.g. Which journals/conferences published [AUTHOR] in [YEAR]?). Theoretically it is possible to use more precise temporal expressions which specify both the month and the year, or even the day, but there are very few bibliographical dat</context>
</contexts>
<marker>[15]</marker>
<rawString>A. Varga, G. Pu¸sca¸su, and C. Or˘asan. Identification of temporal expressions in the domain of tourism. In Knowledge Engineering: Principles and Techniques, volume 1, pages 29 – 32, Cluj-Napoca, Romania, July 2 – 4 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Winkler</author>
</authors>
<title>The state of record linkage and current research problems.</title>
<date>1999</date>
<booktitle>Technical report, Statistical Research Division, U.S. Bureau of the Census,</booktitle>
<contexts>
<context position="21015" citStr="[16]" startWordPosition="3217" endWordPosition="3217">shes between high probability similar tokens and tokens that are most probably distinct. Using a character-based similarity threshold, we compute a partial one to one matching and evaluate the negative impact of the unmatched tokens. The more words that are matched, the greater the confidence that the two strings represent the same entity, while a great number of mismatched words means that the two strings represent different entities. While matching tokens a certain amount of spelling variations and mistakes are allowed by using the character-based similarity measure proposed by Jaro-Winkler [16]. For the bibliographical domain the same approach was employed, but different types of entities had to be annotated. This was achieved by training the algorithm with different gazetteers. Given the nature of the domain, a large number of ambiguities were noticed. These ambiguities are discussed in the next section. 4.2.2 Ambiguities at the level of entity annotation One of the main challenges we had to address when we ported the annotators to the new domain was that the names of authors and conferences can be expressed in several ways and that some entities can have several types. The former </context>
</contexts>
<marker>[16]</marker>
<rawString>W. E. Winkler. The state of record linkage and current research problems. Technical report, Statistical Research Division, U.S. Bureau of the Census, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<contexts>
<context position="6204" citStr="[17]" startWordPosition="948" endWordPosition="948">re details about them can be found in [10]. Automatic extraction of bibliographical metadata from unstructured web pages is achieved in three consecutive steps using a machine learning approach: 1. A page classifier extracts web pages containing bibliographies from the whole amount of data provided by the crawler. The classifier exploits the structure of HTML format such as metadata (&lt;keywords&gt;, &lt;title&gt;, &lt;description&gt;) and headers (&lt;h1&gt;, &lt;h2&gt;, etc.) in order to give different weights to distinct data sources. As a machine learning method, we used and compared several methods contained in WEKA [17]. 2. A record classifier selects bibliographical entries from all document records using Conditional Random Fields (CRF) [4]. As HTML-format 3 http://www.ukoln.ac.uk/repositories/digirep/index/SWORD guide provides some structural elements like tags, we incorporated them into the classifier for revealing enumerations or lists of equivalent records. 3. An information extraction (IE) module identifies 5 types of bibliographical metadata from Dublin Core Metadata Element Set: author, title, date, publisher and citation. As bibliographical reference represents a logical consequence of metadata tags</context>
</contexts>
<marker>[17]</marker>
<rawString>I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann Publishers, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>