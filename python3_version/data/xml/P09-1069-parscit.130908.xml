<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029103">
<title confidence="0.9973695">
Learning a Compositional Semantic Parser
using an Existing Syntactic Parser
</title>
<author confidence="0.999792">
Ruifang Ge Raymond J. Mooney
</author>
<affiliation confidence="0.998718">
Department of Computer Sciences
University of Texas at Austin
</affiliation>
<address confidence="0.908703">
Austin, TX 78712
</address>
<email confidence="0.999632">
{grf,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999827909090909">
We present a new approach to learning a
semantic parser (a system that maps natu-
ral language sentences into logical form).
Unlike previous methods, it exploits an ex-
isting syntactic parser to produce disam-
biguated parse trees that drive the compo-
sitional semantic interpretation. The re-
sulting system produces improved results
on standard corpora on natural language
interfaces for database querying and sim-
ulated robot control.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.955847074074074">
Semantic parsing is the task of mapping a natu-
ral language (NL) sentence into a completely for-
mal meaning representation (MR) or logical form.
A meaning representation language (MRL) is a
formal unambiguous language that supports au-
tomated inference, such as first-order predicate
logic. This distinguishes it from related tasks
such as semantic role labeling (SRL) (Carreras
and Marquez, 2004) and other forms of “shallow”
semantic analysis that do not produce completely
formal representations. A number of systems for
automatically learning semantic parsers have been
proposed (Ge and Mooney, 2005; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2007; Lu et al.,
2008). Given a training corpus of NL sentences
annotated with their correct MRs, these systems
induce an interpreter for mapping novel sentences
into the given MRL.
Previous methods for learning semantic parsers
do not utilize an existing syntactic parser that pro-
vides disambiguated parse trees.1 However, ac-
curate syntactic parsers are available for many
1Ge and Mooney (2005) use training examples with
semantically annotated parse trees, and Zettlemoyer and
Collins (2005) learn a probabilistic semantic parsing model
which initially requires a hand-built, ambiguous CCG gram-
mar template.
</bodyText>
<figure confidence="0.995444166666667">
(a) If our player 2 has the ball,
then position our player 5 in the midfield.
((bowner (player our {2}))
(do (player our {5}) (pos (midfield))))
(b) Which river is the longest?
answer(xl,longest(xl,river(xl)))
</figure>
<figureCaption confidence="0.962345">
Figure 1: Sample NLs and their MRs in the
ROBOCUP and GEOQUERY domains respectively.
</figureCaption>
<bodyText confidence="0.999600095238095">
languages and could potentially be used to learn
more effective semantic analyzers. This paper
presents an approach to learning semantic parsers
that uses parse trees from an existing syntactic
analyzer to drive the interpretation process. The
learned parser uses standard compositional seman-
tics to construct alternative MRs for a sentence
based on its syntax tree, and then chooses the best
MR based on a trained statistical disambiguation
model. The learning system first employs a word
alignment method from statistical machine trans-
lation (GIZA++ (Och and Ney, 2003)) to acquire
a semantic lexicon that maps words to logical
predicates. Then it induces rules for composing
MRs and estimates the parameters of a maximum-
entropy model for disambiguating semantic inter-
pretations. After describing the details of our ap-
proach, we present experimental results on stan-
dard corpora demonstrating improved results on
learning NL interfaces for database querying and
simulated robot control.
</bodyText>
<sectionHeader confidence="0.995924" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9974415">
In this paper, we consider two domains. The
first is ROBOCUP (www.robocup.org). In the
ROBOCUP Coach Competition, soccer agents
compete on a simulated soccer field and receive
coaching instructions in a formal language called
CLANG (Chen et al., 2003). Figure 1(a) shows a
sample instruction. The second domain is GEO-
QUERY, where a logical query language based on
Prolog is used to query a database on U.S. geog-
raphy (Zelle and Mooney, 1996). The logical lan-
</bodyText>
<page confidence="0.974037">
611
</page>
<note confidence="0.9996385">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 611–619,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.993459517241379">
CONDITION
(bowner PLAYER)
(player TEAM
{UNUM})
2
P BOWNER
NP
VP
S
P PLAYER
P OUR P UNUM
(b)
our
DET
has
NN
player
2
the
ball
CD
NN
(c)
VB
NP
NP
PRP$
our
(a)
</figure>
<figureCaption confidence="0.997093">
Figure 2: Parses for the condition part of the CLANG in Figure 1(a): (a) The parse of the MR. (b) The
predicate argument structure of (a). (c) The parse of the NL.
</figureCaption>
<table confidence="0.998026777777778">
PRODUCTION PREDICATE
RULES(CONDITION DIRECTIVE) P RULE
CONDITIONS(bowner PLAYER) P BOWNER
PLAYERS(player TEAM {UNUM}) P PLAYER
TEAMSour P OUR
UNUMS2 P UNUM
DIRECTIVES(do PLAYER ACTION) P DO
ACTIONS(pos REGION) P POS
REGIONS(midfield) P MIDFIELD
</table>
<tableCaption confidence="0.819973333333333">
Table 1: Sample production rules for parsing the
CLANG example in Figure 1(a) and their corre-
sponding predicates.
</tableCaption>
<bodyText confidence="0.999021714285714">
guage consists of both first-order and higher-order
predicates. Figure 1(b) shows a sample query in
this domain.
We assume that an MRL is defined by an un-
ambiguous context-free grammar (MRLG), so that
MRs can be uniquely parsed, a standard require-
ment for computer languages. In an MRLG, each
production rule introduces a single predicate in the
MRL, where the type of the predicate is given in
the left hand side (LHS), and the number and types
of its arguments are defined by the nonterminals in
the right hand side (RHS). Therefore, the parse of
an MR also gives its predicate-argument structure.
Figure 2(a) shows the parse of the condition
part of the MR in Figure 1(a) using the MRLG
described in (Wong, 2007), and its predicate-
argument structure is in Figure 2(b). Sample
MRLG productions and their predicates for pars-
ing this example are shown in Table 1, where the
predicate P PLAYER takes two arguments (a1 and
a2) of type TEAM and UNUM (uniform number).
</bodyText>
<sectionHeader confidence="0.989092" genericHeader="method">
3 Semantic Parsing Framework
</sectionHeader>
<bodyText confidence="0.999971477272727">
This section describes our basic framework, which
is based on a fairly standard approach to computa-
tional semantics (Blackburn and Bos, 2005). The
framework is composed of three components: 1)
an existing syntactic parser to produce parse trees
for NL sentences; 2) learned semantic knowledge
(cf. Sec. 5), including a semantic lexicon to assign
possible predicates (meanings) to words, and a set
of semantic composition rules to construct possi-
ble MRs for each internal node in a syntactic parse
given its children’s MRs; and 3) a statistical dis-
ambiguation model (cf. Sec. 6) to choose among
multiple possible semantic constructs as defined
by the semantic knowledge.
The process of generating the semantic parse
for an NL sentence is as follows. First, the syn-
tactic parser produces a parse tree for the NL
sentence. Second, the semantic lexicon assigns
possible predicates to each word in the sentence.
Third, all possible MRs for the sentence are con-
structed compositionally in a recursive, bottom-up
fashion following its syntactic parse using com-
position rules. Lastly, the statistical disambigua-
tion model scores each possible MR and returns
the one with the highest score. Fig. 3(a) shows
one possible semantically-augmented parse tree
(SAPT) (Ge and Mooney, 2005) for the condition
part of the example in Fig. 1(a) given its syntac-
tic parse in Fig. 2(c). A SAPT adds a semantic
label to each non-leaf node in the syntactic parse
tree. The label specifies the MRL predicate for
the node and its remaining (unfilled) arguments.
The compositional process assumes a binary parse
tree suitable for predicate-argument composition;
parses in Penn-treebank style are binarized using
Collins’ (1999) method.
Consider the construction of the SAPT in
Fig. 3(a). First, each word is assigned a semantic
label. Most words are assigned an MRL predicate.
For example, the word player is assigned the pred-
icate P PLAYER with its two unbound arguments,
a1 and a2, indicated using A. Words that do not
introduce a predicate are given the label NULL,
like the and ball.2 Next, a semantic label is as-
</bodyText>
<footnote confidence="0.9924545">
2The words the and ball are not truly “meaningless” since
the predicate P BOWNER (ball owner) is conveyed by the
</footnote>
<page confidence="0.993858">
612
</page>
<figure confidence="0.998449264705882">
P BOWNER
P PLAYER Aa1P BOWNER
P OUR
Aa1P PLAYER
the
ball
NULL
NULL
NULL
Aa1P BOWNER
has
P UNUM
2
our
(Aa1Aa2)P PLAYER
player
(a) SAPT
(bowner (player our {2}))
(player our {2}) Aa1(bowner a1)
our
our
(Aa1Aa2)(player a1 {a2} )
player
Aa1 (player a1 {2})
2
2
Aa1(bowner a1)
has
NULL
the
NULL
NULL
ball
(b) Semantic Derivation
</figure>
<figureCaption confidence="0.9999585">
Figure 3: Semantic parse for the condition part of the example in Fig. 1(a) using the syntactic parse in
Fig. 2(c): (a) A SAPT with syntactic labels omitted for brevity. (b) The semantic derivation of the MR.
</figureCaption>
<bodyText confidence="0.997964576923077">
signed to each internal node using learned compo-
sition rules that specify how arguments are filled
when composing two MRs (cf. Sec. 5). The label
Aa1P PLAYER indicates that the remaining argu-
ment a2 of the P PLAYER child is filled by the MR
of the other child (labeled P UNUM).
Finally, the SAPT is used to guide the composi-
tion of the sentence’s MR. At each internal node,
an MR for the node is built from the MRs of its
children by filling an argument of a predicate, as
illustrated in the semantic derivation shown in Fig.
3(b). Semantic composition rules (cf. Sec. 5) are
used to specify the argument to be filled. For the
node spanning player 2, the predicate P PLAYER
and its second argument P UNUM are composed to
formtheMR: Aa1 (player a1 121). Composing
an MR with NULL leaves the MR unchanged. An
MR is said to be complete when it contains no re-
maining A variables. This process continues up the
phrase has the ball. For simplicity, predicates are intro-
duced by a single word, but statistical disambiguation (cf.
Sec. 6) uses surrounding words to choose a meaning for a
word whose lexicon entry contains multiple possible predi-
cates.
tree until a complete MR for the entire sentence is
constructed at the root.
</bodyText>
<sectionHeader confidence="0.992356" genericHeader="method">
4 Ensuring Meaning Composition
</sectionHeader>
<bodyText confidence="0.999989473684211">
The basic compositional method in Sec. 3 only
works if the syntactic parse tree strictly follows
the predicate-argument structure of the MR, since
meaning composition at each node is assumed to
combine a predicate with one of its arguments.
However, this assumption is not always satisfied,
for example, in the case of verb gapping and flex-
ible word order. We use constructing the MR for
the directive part of the example in Fig. 1(a) ac-
cording to the syntactic parse in Fig. 4(b) as an
example. Given the appropriate possible predicate
attached to each word in Fig. 5(a), the node span-
ning position our player 5 has children, P POS and
P PLAYER, that are not in a predicate-argument re-
lation in the MR (see Fig. 4(a)).
To ensure meaning composition in this case,
we automatically create macro-predicates that
combine multiple predicates into one, so that
the children’s MRs can be composed as argu-
</bodyText>
<page confidence="0.993296">
613
</page>
<figure confidence="0.99943388">
VP
P PLAYER
P POS
P OUR P UNUM
(a)
P MIDFIELD
ADVP
RB
then
VP
PP
VB
NP
IN
NP
the
midfield
VP
in
DT
NN
position
our player 5
P DO
(b)
</figure>
<figureCaption confidence="0.965653">
Figure 4: Parses for the directive part of the CLANG in Fig. 1(a): (a) The predicate-argument structure
of the MR. (b) The parse of the NL (the parse of the phrase our player 5 is omitted for brevity).
</figureCaption>
<bodyText confidence="0.999843241379311">
ments to a macro-predicate. Fig. 5(b) shows
the macro-predicate P DO POS (DIRECTIVE→(do
PLAYER (pos REGION))) formed by merging the
P DO and P POS in Fig. 4(a). The macro-predicate
has two arguments, one of type PLAYER (a1)
and one of type REGION (a2). Now, P POS and
P PLAYER can be composed as arguments to this
macro-predicate as shown in Fig. 5(c). However,
it requires assuming a P DO predicate that has
not been formally introduced. To indicate this, a
lambda variable, p1, is introduced that ranges over
predicates and is provisionally bound to P DO, as
indicated in Fig. 5(c) using the notation p1:do.
Eventually, this predicate variable must be bound
to a matching predicate introduced from the lexi-
con. In the example, p1:do is eventually bound to
the P DO predicate introduced by the word then to
form a complete MR.
Macro-predicates are introduced as needed dur-
ing training in order to ensure that each MR in
the training set can be composed using the syn-
tactic parse of its corresponding NL given reason-
able assignments of predicates to words. For each
SAPT node that does not combine a predicate with
a legal argument, a macro-predicate is formed by
merging all predicates on the paths from the child
predicates to their lowest common ancestor (LCA)
in the MR parse. Specifically, a child MR be-
comes an argument of the macro-predicate if it
is complete (i.e. contains no A variables); other-
wise, it also becomes part of the macro-predicate
and its A variables become additional arguments
of the macro-predicate. For the node spanning po-
sition our player 5 in the example, the LCA of the
children P PLAYER and P POS is their immedi-
ate parent P DO, therefore P DO is included in the
macro-predicate. The complete child P PLAYER
becomes the first argument of the macro-predicate.
The incomplete child P POS is added to the macro-
predicate P DO POS and its A variable becomes
another argument.
For improved generalization, once a predicate
in a macro-predicate becomes complete, it is re-
moved from the corresponding macro-predicate
label in the SAPT. For the node spanning position
our player 5 in the midfield in Fig. 5(a), P DO POS
becomes P DO once the arguments of pos are
filled.
In the following two sections, we describe the
two subtasks of inducing semantic knowledge and
a disambiguation model for this enhanced compo-
sitional framework. Both subtasks require a train-
ing set of NLs paired with their MRs. Each NL
sentence also requires a syntactic parse generated
using Bikel’s (2004) implementation of Collins
parsing model 2. Note that unlike SCISSOR (Ge
and Mooney, 2005), training our method does not
require gold-standard SAPTs.
</bodyText>
<sectionHeader confidence="0.961965" genericHeader="method">
5 Learning Semantic Knowledge
</sectionHeader>
<bodyText confidence="0.999824285714286">
Learning semantic knowledge starts from learning
the mapping from words to predicates. We use
an approach based on Wong and Mooney (2006),
which constructs word alignments between NL
sentences and their MRs. Normally, word align-
ment is used in statistical machine translation to
match words in one NL to words in another; here
it is used to align words with predicates based on
a ”parallel corpus” of NL sentences and MRs. We
assume that each word alignment defines a possi-
ble mapping from words to predicates for building
a SAPT and semantic derivation which compose
the correct MR. A semantic lexicon and compo-
sition rules are then extracted directly from the
</bodyText>
<page confidence="0.97864">
614
</page>
<figure confidence="0.998500567567568">
(Aa1Aa2)P DO
then
Ap1P DO POS = Ap1P DO
P DO
a1:PLAYER P POS
P MIDFIELD
(Ap1Aa2)P DO POS
P DO
a2:REGION
(b) Macro-Predicate P DO POS
Aa1P POS
position
our player 5
P PLAYER
NULL
in
NULL
P MIDFIELD
P MIDFIELD
the midfield
(a) SAPT
(do (player our {5}) (pos (midfield)))
(Aa1Aa2)(do a1a2)
then
Ap1(p1:do (player our {5}) (pos (midfield)))
(Ap1Aa2)(p1:do (player our {5}) (pos a2))
(midfield)
Aa1(pos a1)
position
(player our {5})
our player 5
(midfield)
NULL (midfield)
the midfield
NULL
in
(c) Semantic Derivation
</figure>
<figureCaption confidence="0.997560333333333">
Figure 5: Semantic parse for the directive part of the example in Fig. 1(a) using the syntactic parse in
Fig. 4(b): (a) A SAPT with syntactic labels omitted for brevity. (b) The predicate-argument structure of
macro-predicate P DO POS (c) The semantic derivation of the MR.
</figureCaption>
<bodyText confidence="0.998216">
nodes of the resulting semantic derivations.
Generation of word alignments for each train-
ing example proceeds as follows. First, each MR
in the training corpus is parsed using the MRLG.
Next, each resulting parse tree is linearized to pro-
duce a sequence of predicates by using a top-
down, left-to-right traversal of the parse tree. Then
the GIZA++ implementation (Och and Ney, 2003)
of IBM Model 5 is used to generate the five best
word/predicate alignments from the corpus of NL
sentences each paired with the predicate sequence
for its MR.
After predicates are assigned to words using
word alignment, for each alignment of a training
example and its syntactic parse, a SAPT is gener-
ated for composing the correct MR using the pro-
cesses discussed in Sections 3 and 4. Specifically,
a semantic label is assigned to each internal node
of each SAPT, so that the MRs of its children are
composed correctly according to the MR for this
example.
There are two cases that require special han-
dling. First, when a predicate is not aligned to any
word, the predicate must be inferred from context.
For example, in CLANG, our player is frequently
just referred to as player and the our must be in-
ferred. When building a SAPT for such an align-
ment, the assumed predicates and arguments are
simply bound to their values in the MR. Second,
when a predicate is aligned to several words, i.e. it
is represented by a phrase, the alignment is trans-
formed into several alignments where each predi-
cate is aligned to each single word in order to fit
the assumptions of compositional semantics.
Given the SAPTs constructed from the results
of word-alignment, a semantic derivation for each
training sentence is constructed using the methods
described in Sections 3 and 4. Composition rules
</bodyText>
<page confidence="0.998099">
615
</page>
<bodyText confidence="0.976823714285714">
where P1, P2 and Pp are predicates for the left
child, right child, and parent node, respectively.
Each predicate includes a lambda term A of
the form (Api1, ... , Api., Aaj1, ... , Aajj, an un-
ordered set of all unbound predicate and argument
variables for the predicate. The component R
specifies how some arguments of the parent predi-
cate are filled when composing the MR for the par-
ent node. It is of the form: {ak1=R1, ... , ak,=Rl},
where Ri can be either a child (ci), or a child’s
complete argument (ci, aj) if the child itself is not
complete.
For instance, the rule extracted for the node for
player 2 in Fig. 3(b) is:
</bodyText>
<figure confidence="0.826617">
hλa1λa2i.P PLAYER + P UNUM ⇒ {λa1.P PLAYER, a2=c2},
and for position our player 5 in Fig. 5(c):
λa1.P POS + P PLAYER ⇒ {hλp1λa2i.P DO POS, a1=c2},
and for position our player 5 in the midfield:
hλp1λa2i.P DO POS + P MIDFIELD
⇒ {λp1.P DO POS, {a1=(c1,a1), a2=c2}}.
</figure>
<bodyText confidence="0.958494833333333">
The learned semantic knowledge is necessary
for handling ambiguity, such as that involving
word senses and semantic roles. It is also used to
ensure that each MR is a legal string in the MRL.
are then extracted from these derivations.
Formally, composition rules are of the form:
</bodyText>
<equation confidence="0.753247">
A1.P1 + A2.P2 ⇒ {Ap.Pp, R} (1)
</equation>
<sectionHeader confidence="0.939521" genericHeader="method">
6 Learning a Disambiguation Model
</sectionHeader>
<bodyText confidence="0.999950625">
Usually, multiple possible semantic derivations for
an NL sentence are warranted by the acquired se-
mantic knowledge, thus disambiguation is needed.
To learn a disambiguation model, the learned se-
mantic knowledge (see Section 5) is applied to
each training example to generate all possible se-
mantic derivations for an NL sentence given its
syntactic parse. Here, unique word alignments are
not required, and alternative interpretations com-
pete for the best semantic parse.
We use a maximum-entropy model similar
to that of Zettlemoyer and Collins (2005) and
Wong and Mooney (2006). The model defines a
conditional probability distribution over semantic
derivations (D) given an NL sentence S and its
syntactic parse T:
</bodyText>
<equation confidence="0.981393">
Pr(D|S, T; �0) = exp Ei 0ifi(D) (2)
ZB(S,T)
</equation>
<bodyText confidence="0.999965727272727">
where f (f1, ... , fn) is a feature vector parame-
terized by 0, and Z6(S, T) is a normalizing fac-
tor. Three simple types of features are used in
the model. First, are lexical features which count
the number of times a word is assigned a particu-
lar predicate. Second, are bilexical features which
count the number of times a word is assigned a
particular predicate and a particular word precedes
or follows it. Last, are rule features which count
the number of times a particular composition rule
is applied in the derivation.
The training process finds a parameter 0∗ that
(approximately) maximizes the sum of the condi-
tional log-likelihood of the MRs in the training set.
Since no specific semantic derivation for an MR is
provided in the training data, the conditional log-
likelihood of an MR is calculated as the sum of the
conditional probability of all semantic derivations
that lead to the MR. Formally, given a set of NL-
MR pairs {(S1, M1), (S2, M2), ..., (Sn, Mn)} and
the syntactic parses of the NLs {T1, T2,..., Tn},
the parameter �0∗ is calculated as:
</bodyText>
<equation confidence="0.98482">
log Pr(Mi|Si, Ti; 0) (3)
log
� Pr(D∗i |Si, Ti; 0)
D*
z
</equation>
<bodyText confidence="0.9999199375">
where D∗i is a semantic derivation that produces
the correct MR Mi.
L-BFGS (Nocedal, 1980) is used to estimate the
parameters �0∗. The estimation requires statistics
that depend on all possible semantic derivations
and all correct semantic derivations of an exam-
ple, which are not feasibly enumerated. A vari-
ant of the Inside-Outside algorithm (Miyao and
Tsujii, 2002) is used to efficiently collect the nec-
essary statistics. Following Wong and Mooney
(2006), only candidate predicates and composi-
tion rules that are used in the best semantic deriva-
tions for the training set are retained for testing.
No smoothing is used to regularize the model; We
tried using a Gaussian prior (Chen and Rosenfeld,
1999), but it did not improve the results.
</bodyText>
<sectionHeader confidence="0.992696" genericHeader="evaluation">
7 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9998075">
We evaluated our approach on two standard cor-
pora in CLANG and GEOQUERY. For CLANG,
300 instructions were randomly selected from
the log files of the 2003 ROBOCUP Coach
</bodyText>
<equation confidence="0.998931">
0∗ = arg max
θ�
= arg max
θ�
n
i=1
n
i=1
</equation>
<page confidence="0.993746">
616
</page>
<bodyText confidence="0.999855196078432">
Competition and manually translated into En-
glish (Kuhlmann et al., 2004). For GEOQUERY,
880 English questions were gathered from vari-
ous sources and manually translated into Prolog
queries (Tang and Mooney, 2001). The average
sentence lengths for the CLANG and GEOQUERY
corpora are 22.52 and 7.48, respectively.
Our experiments used 10-fold cross validation
and proceeded as follows. First Bikel’s imple-
mentation of Collins parsing model 2 was trained
to generate syntactic parses. Second, a seman-
tic parser was learned from the training set aug-
mented with their syntactic parses. Finally, the
learned semantic parser was used to generate the
MRs for the test sentences using their syntactic
parses. If a test example contains constructs that
did not occur in training, the parser may fail to re-
turn an MR.
We measured the performance of semantic pars-
ing using precision (percentage of returned MRs
that were correct), recall (percentage of test exam-
ples with correct MRs returned), and F-measure
(harmonic mean of precision and recall). For
CLANG, an MR was correct if it exactly matched
the correct MR, up to reordering of arguments
of commutative predicates like and. For GEO-
QUERY, an MR was correct if it retrieved the same
answer as the gold-standard query, thereby reflect-
ing the quality of the final result returned to the
user.
The performance of a syntactic parser trained
only on the Wall Street Journal (WSJ) can de-
grade dramatically in new domains due to cor-
pus variation (Gildea, 2001). Experiments on
CLANG and GEOQUERY showed that the perfor-
mance can be greatly improved by adding a small
number of treebanked examples from the corre-
sponding training set together with the WSJ cor-
pus. Our semantic parser was evaluated using
three kinds of syntactic parses. Listed together
with their PARSEVAL F-measures these are:
gold-standard parses from the treebank (GoldSyn,
100%), a parser trained on WSJ plus a small
number of in-domain training sentences required
to achieve good performance, 20 for CLANG
(Syn20, 88.21%) and 40 for GEOQUERY (Syn40,
91.46%), and a parser trained on no in-domain
data (Syn0, 82.15% for CLANG and 76.44% for
GEOQUERY).
We compared our approach to the following al-
ternatives (where results for the given corpus were
</bodyText>
<table confidence="0.999883">
Precision Recall F-measure
GOLDSYN 84.73 74.00 79.00
SYN20 85.37 70.00 76.92
SYN0 87.01 67.00 75.71
WASP 88.85 61.93 72.99
KRISP 85.20 61.85 71.67
SCISSOR 89.50 73.70 80.80
LU 82.50 67.70 74.40
</table>
<tableCaption confidence="0.938114">
Table 2: Performance on CLANG.
</tableCaption>
<table confidence="0.999981888888889">
Precision Recall F-measure
GOLDSYN 91.94 88.18 90.02
SYN40 90.21 86.93 88.54
SYN0 81.76 78.98 80.35
WASP 91.95 86.59 89.19
Z&amp;C 91.63 86.07 88.76
SCISSOR 95.50 77.20 85.38
KRISP 93.34 71.70 81.10
LU 89.30 81.50 85.20
</table>
<tableCaption confidence="0.999879">
Table 3: Performance on GEOQUERY.
</tableCaption>
<bodyText confidence="0.999493575757576">
available): SCISSOR (Ge and Mooney, 2005), an
integrated syntactic-semantic parser; KRISP (Kate
and Mooney, 2006), an SVM-based parser using
string kernels; WASP (Wong and Mooney, 2006;
Wong and Mooney, 2007), a system based on
synchronous grammars; Z&amp;C (Zettlemoyer and
Collins, 2007)3, a probabilistic parser based on re-
laxed CCG grammars; and LU (Lu et al., 2008),
a generative model with discriminative reranking.
Note that some of these approaches require ad-
ditional human supervision, knowledge, or engi-
neered features that are unavailable to the other
systems; namely, SCISSOR requires gold-standard
SAPTs, Z&amp;C requires hand-built template gram-
mar rules, LU requires a reranking model using
specially designed global features, and our ap-
proach requires an existing syntactic parser. The
F-measures for syntactic parses that generate cor-
rect MRs in CLANG are 85.50% for syn0 and
91.16% for syn20, showing that our method can
produce correct MRs even when given imperfect
syntactic parses. The results of semantic parsers
are shown in Tables 2 and 3.
First, not surprisingly, more accurate syntac-
tic parsers (i.e. ones trained on more in-domain
data) improved our approach. Second, in CLANG,
all of our methods outperform WASP and KRISP,
which also require no additional information dur-
ing training. In GEOQUERY, Syn0 has signifi-
cantly worse results than WASP and our other sys-
tems using better syntactic parses. This is not sur-
prising since Syn0’s F-measure for syntactic pars-
ing is only 76.44% in GEOQUERY due to a lack
</bodyText>
<footnote confidence="0.7158435">
3These results used a different experimental setup, train-
ing on 600 examples, and testing on 280 examples.
</footnote>
<page confidence="0.984697">
617
</page>
<table confidence="0.99985">
Precision Recall F-measure
GOLDSYN 61.14 35.67 45.05
SYN20 57.76 31.00 40.35
SYN0 53.54 22.67 31.85
WASP 88.00 14.37 24.71
KRISP 68.35 20.00 30.95
SCISSOR 85.00 23.00 36.20
</table>
<tableCaption confidence="0.986474">
Table 4: Performance on CLANG40.
</tableCaption>
<table confidence="0.99997975">
Precision Recall F-measure
GOLDSYN 95.73 89.60 92.56
SYN20 93.19 87.60 90.31
SYN0 91.81 85.20 88.38
WASP 91.76 75.60 82.90
SCISSOR 98.50 74.40 84.77
KRISP 84.43 71.60 77.49
LU 91.46 72.80 81.07
</table>
<tableCaption confidence="0.834359333333333">
Table 5: Performance on GEO250 (20 in-domain
sentences are used in SYN20 to train the syntactic
parser).
</tableCaption>
<bodyText confidence="0.99997825">
of interrogative sentences (questions) in the WSJ
corpus. Note the results for SCISSOR, KRISP and
LU on GEOQUERY are based on a different mean-
ing representation language, FUNQL, which has
been shown to produce lower results (Wong and
Mooney, 2007). Third, SCISSOR performs better
than our methods on CLANG, but it requires extra
human supervision that is not available to the other
systems. Lastly, a detailed analysis showed that
our improved performance on CLANG compared
to WASP and KRISP is mainly for long sentences
(&gt; 20 words), while performance on shorter sen-
tences is similar. This is consistent with their
relative performance on GEOQUERY, where sen-
tences are normally short. Longer sentences typ-
ically have more complex syntax, and the tradi-
tional syntactic analysis used by our approach re-
sults in better compositional semantic analysis in
this situation.
We also ran experiments with less training data.
For CLANG, 40 random examples from the train-
ing sets (CLANG40) were used. For GEOQUERY,
an existing 250-example subset (GEO250) (Zelle
and Mooney, 1996) was used. The results are
shown in Tables 4 and 5. Note the performance
of our systems on GEO250 is higher than that
on GEOQUERY since GEOQUERY includes more
complex queries (Tang and Mooney, 2001). First,
all of our systems gave the best F-measures (ex-
cept SYN0 compared to SCISSOR in CLANG40),
and the differences are generally quite substantial.
This shows that our approach significantly im-
proves results when limited training data is avail-
able. Second, in CLANG, reducing the training
data increased the difference between SYN20 and
SYN0. This suggests that the quality of syntactic
parsing becomes more important when less train-
ing data is available. This demonstrates the advan-
tage of utilizing existing syntactic parsers that are
learned from large open domain treebanks instead
of relying just on the training data.
We also evaluated the impact of the word align-
ment component by replacing Giza++ by gold-
standard word alignments manually annotated for
the CLANG corpus. The results consistently
showed that compared to using gold-standard
word alignment, Giza++ produced lower seman-
tic parsing accuracy when given very little training
data, but similar or better results when given suf-
ficient training data (&gt; 160 examples). This sug-
gests that, given sufficient data, Giza++ can pro-
duce effective word alignments, and that imper-
fect word alignments do not seriously impair our
semantic parsers since the disambiguation model
evaluates multiple possible interpretations of am-
biguous words. Using multiple potential align-
ments from Giza++ sometimes performs even bet-
ter than using a single gold-standard word align-
ment because it allows multiple interpretations to
be evaluated by the global disambiguation model.
</bodyText>
<sectionHeader confidence="0.99381" genericHeader="conclusions">
8 Conclusion and Future work
</sectionHeader>
<bodyText confidence="0.99938295">
We have presented a new approach to learning a
semantic parser that utilizes an existing syntactic
parser to drive compositional semantic interpre-
tation. By exploiting an existing syntactic parser
trained on a large treebank, our approach produces
improved results on standard corpora, particularly
when training data is limited or sentences are long.
The approach also exploits methods from statisti-
cal MT (word alignment) and therefore integrates
techniques from statistical syntactic parsing, MT,
and compositional semantics to produce an effec-
tive semantic parser.
Currently, our results comparing performance
on long versus short sentences indicates that our
approach is particularly beneficial for syntactically
complex sentences. Follow up experiments us-
ing a more refined measure of syntactic complex-
ity could help confirm this hypothesis. Reranking
could also potentially improve the results (Ge and
Mooney, 2006; Lu et al., 2008).
</bodyText>
<sectionHeader confidence="0.998833" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.963572">
This research was partially supported by NSF
grant IIS–0712097.
</bodyText>
<page confidence="0.997247">
618
</page>
<sectionHeader confidence="0.998331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929452830189">
Daniel M. Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics, 30(4):479–511.
Patrick Blackburn and Johan Bos. 2005. Represen-
tation and Inference for Natural Language: A First
Course in Computational Semantics. CSLI Publica-
tions, Stanford, CA.
Xavier Carreras and Luis Marquez. 2004. Introduc-
tion to the CoNLL-2004 shared task: Semantic role
labeling. In Proc. of 8th Conf. on Computational
Natural Language Learning (CoNLL-2004), Boston,
MA.
Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaus-
sian prior for smoothing maximum entropy model.
Technical Report CMU-CS-99-108, School of Com-
puter Science, Carnegie Mellon University.
Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros
Kapetanakis, Kostas Kostiadis, Johan Kummeneje,
Itsuki Noda, Oliver Obst, Patrick Riley, Timo Stef-
fens, Yi Wang, and Xiang Yin. 2003. Users
manual: RoboCup soccer server manual for soccer
server version 7.07 and later. Available at http://
sourceforge.net/projects/sserver/.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Ruifang Ge and Raymond J. Mooney. 2005. A statisti-
cal semantic parser that integrates syntax and seman-
tics. In Proc. of 9th Conf. on Computational Natural
Language Learning (CoNLL-2005), pages 9–16.
Ruifang Ge and Raymond J. Mooney. 2006. Dis-
criminative reranking for semantic parsing. In Proc.
of the 21st Intl. Conf. on Computational Linguis-
tics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL-06),
Sydney, Australia, July.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc. of the 2001 Conf. on Empirical
Methods in Natural Language Processing (EMNLP-
01), Pittsburgh, PA, June.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proc. of the 21st Intl. Conf. on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL-06),
pages 913–920, Sydney, Australia, July.
Greg Kuhlmann, Peter Stone, Raymond J. Mooney, and
Jude W. Shavlik. 2004. Guiding a reinforcement
learner with natural language advice: Initial results
in RoboCup soccer. In Proc. of the AAAI-04 Work-
shop on Supervisory Control of Learning and Adap-
tive Systems, San Jose, CA, July.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.
Zettlemoyer. 2008. A generative model for pars-
ing natural language to meaning representations. In
Proc. of the Conf. on Empirical Methods in Natu-
ral Language Processing (EMNLP-08), Honolulu,
Hawaii, October.
Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc.
of Human Language Technology Conf.(HLT-2002),
San Diego, CA, March.
Jorge Nocedal. 1980. Updating quasi-Newton matri-
ces with limited storage. Mathematics of Computa-
tion, 35(151):773–782, July.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Lappoon R. Tang and Raymond J. Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Proc. of the
12th European Conf. on Machine Learning, pages
466–477, Freiburg, Germany.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In Proc. of Human Language
Technology Conf. / N. American Chapter of the
Association for Computational Linguistics Annual
Meeting (HLT-NAACL-2006), pages 439–446.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proc. of the 45th An-
nual Meeting of the Association for Computational
Linguistics (ACL-07), pages 960–967.
Yuk Wah Wong. 2007. Learning for Semantic Pars-
ing and Natural Language Generation Using Statis-
tical Machine Translation Techniques. Ph.D. the-
sis, Department of Computer Sciences, University of
Texas, Austin, TX, August. Also appears as Artifi-
cial Intelligence Laboratory Technical Report AI07-
343.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of 13th Natl. Conf. on Artifi-
cial Intelligence (AAAI-96), pages 1050–1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of the 21th Annual Conf. on Un-
certainty in Artificial Intelligence (UAI-05).
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proc. of the 2007 Joint Conf. on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learn-
ing (EMNLP-CoNLL-07), pages 678–687, Prague,
Czech Republic, June.
</reference>
<page confidence="0.998661">
619
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.862491">
<title confidence="0.997278">Learning a Compositional Semantic Parser using an Existing Syntactic Parser</title>
<author confidence="0.999957">Ruifang Ge Raymond J Mooney</author>
<affiliation confidence="0.999945">Department of Computer Sciences University of Texas at Austin</affiliation>
<address confidence="0.995178">Austin, TX 78712</address>
<abstract confidence="0.989131916666667">We present a new approach to learning a semantic parser (a system that maps natural language sentences into logical form). Unlike previous methods, it exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation. The resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Blackburn</author>
<author>Johan Bos</author>
</authors>
<title>Representation and Inference for Natural Language: A First Course in Computational Semantics.</title>
<date>2005</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="5652" citStr="Blackburn and Bos, 2005" startWordPosition="895" endWordPosition="898">d side (RHS). Therefore, the parse of an MR also gives its predicate-argument structure. Figure 2(a) shows the parse of the condition part of the MR in Figure 1(a) using the MRLG described in (Wong, 2007), and its predicateargument structure is in Figure 2(b). Sample MRLG productions and their predicates for parsing this example are shown in Table 1, where the predicate P PLAYER takes two arguments (a1 and a2) of type TEAM and UNUM (uniform number). 3 Semantic Parsing Framework This section describes our basic framework, which is based on a fairly standard approach to computational semantics (Blackburn and Bos, 2005). The framework is composed of three components: 1) an existing syntactic parser to produce parse trees for NL sentences; 2) learned semantic knowledge (cf. Sec. 5), including a semantic lexicon to assign possible predicates (meanings) to words, and a set of semantic composition rules to construct possible MRs for each internal node in a syntactic parse given its children’s MRs; and 3) a statistical disambiguation model (cf. Sec. 6) to choose among multiple possible semantic constructs as defined by the semantic knowledge. The process of generating the semantic parse for an NL sentence is as f</context>
</contexts>
<marker>Blackburn, Bos, 2005</marker>
<rawString>Patrick Blackburn and Johan Bos. 2005. Representation and Inference for Natural Language: A First Course in Computational Semantics. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Luis Marquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proc. of 8th Conf. on Computational Natural Language Learning (CoNLL-2004),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1058" citStr="Carreras and Marquez, 2004" startWordPosition="153" endWordPosition="156">ed parse trees that drive the compositional semantic interpretation. The resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control. 1 Introduction Semantic parsing is the task of mapping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic p</context>
</contexts>
<marker>Carreras, Marquez, 2004</marker>
<rawString>Xavier Carreras and Luis Marquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proc. of 8th Conf. on Computational Natural Language Learning (CoNLL-2004), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy model.</title>
<date>1999</date>
<tech>Technical Report CMU-CS-99-108,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="20466" citStr="Chen and Rosenfeld, 1999" startWordPosition="3445" endWordPosition="3448">al, 1980) is used to estimate the parameters �0∗. The estimation requires statistics that depend on all possible semantic derivations and all correct semantic derivations of an example, which are not feasibly enumerated. A variant of the Inside-Outside algorithm (Miyao and Tsujii, 2002) is used to efficiently collect the necessary statistics. Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. No smoothing is used to regularize the model; We tried using a Gaussian prior (Chen and Rosenfeld, 1999), but it did not improve the results. 7 Experimental Evaluation We evaluated our approach on two standard corpora in CLANG and GEOQUERY. For CLANG, 300 instructions were randomly selected from the log files of the 2003 ROBOCUP Coach 0∗ = arg max θ� = arg max θ� n i=1 n i=1 616 Competition and manually translated into English (Kuhlmann et al., 2004). For GEOQUERY, 880 English questions were gathered from various sources and manually translated into Prolog queries (Tang and Mooney, 2001). The average sentence lengths for the CLANG and GEOQUERY corpora are 22.52 and 7.48, respectively. Our experi</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 1999. A Gaussian prior for smoothing maximum entropy model. Technical Report CMU-CS-99-108, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mao Chen</author>
</authors>
<title>Ehsan Foroughi, Fredrik Heintz, Spiros Kapetanakis, Kostas Kostiadis, Johan Kummeneje,</title>
<date>2003</date>
<location>Itsuki Noda, Oliver Obst, Patrick</location>
<marker>Chen, 2003</marker>
<rawString>Mao Chen, Ehsan Foroughi, Fredrik Heintz, Spiros Kapetanakis, Kostas Kostiadis, Johan Kummeneje, Itsuki Noda, Oliver Obst, Patrick Riley, Timo Steffens, Yi Wang, and Xiang Yin. 2003. Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http:// sourceforge.net/projects/sserver/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1264" citStr="Ge and Mooney, 2005" startWordPosition="183" endWordPosition="186">ol. 1 Introduction Semantic parsing is the task of mapping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic parsers are available for many 1Ge and Mooney (2005) use training examples with semantically annotated parse trees, and Zettlemoyer and Collins (2005) learn a probabilistic semantic parsing model which initi</context>
<context position="6784" citStr="Ge and Mooney, 2005" startWordPosition="1074" endWordPosition="1077">antic knowledge. The process of generating the semantic parse for an NL sentence is as follows. First, the syntactic parser produces a parse tree for the NL sentence. Second, the semantic lexicon assigns possible predicates to each word in the sentence. Third, all possible MRs for the sentence are constructed compositionally in a recursive, bottom-up fashion following its syntactic parse using composition rules. Lastly, the statistical disambiguation model scores each possible MR and returns the one with the highest score. Fig. 3(a) shows one possible semantically-augmented parse tree (SAPT) (Ge and Mooney, 2005) for the condition part of the example in Fig. 1(a) given its syntactic parse in Fig. 2(c). A SAPT adds a semantic label to each non-leaf node in the syntactic parse tree. The label specifies the MRL predicate for the node and its remaining (unfilled) arguments. The compositional process assumes a binary parse tree suitable for predicate-argument composition; parses in Penn-treebank style are binarized using Collins’ (1999) method. Consider the construction of the SAPT in Fig. 3(a). First, each word is assigned a semantic label. Most words are assigned an MRL predicate. For example, the word p</context>
<context position="13331" citStr="Ge and Mooney, 2005" startWordPosition="2224" endWordPosition="2227">ecomes complete, it is removed from the corresponding macro-predicate label in the SAPT. For the node spanning position our player 5 in the midfield in Fig. 5(a), P DO POS becomes P DO once the arguments of pos are filled. In the following two sections, we describe the two subtasks of inducing semantic knowledge and a disambiguation model for this enhanced compositional framework. Both subtasks require a training set of NLs paired with their MRs. Each NL sentence also requires a syntactic parse generated using Bikel’s (2004) implementation of Collins parsing model 2. Note that unlike SCISSOR (Ge and Mooney, 2005), training our method does not require gold-standard SAPTs. 5 Learning Semantic Knowledge Learning semantic knowledge starts from learning the mapping from words to predicates. We use an approach based on Wong and Mooney (2006), which constructs word alignments between NL sentences and their MRs. Normally, word alignment is used in statistical machine translation to match words in one NL to words in another; here it is used to align words with predicates based on a ”parallel corpus” of NL sentences and MRs. We assume that each word alignment defines a possible mapping from words to predicates </context>
<context position="23515" citStr="Ge and Mooney, 2005" startWordPosition="3946" endWordPosition="3949">QUERY). We compared our approach to the following alternatives (where results for the given corpus were Precision Recall F-measure GOLDSYN 84.73 74.00 79.00 SYN20 85.37 70.00 76.92 SYN0 87.01 67.00 75.71 WASP 88.85 61.93 72.99 KRISP 85.20 61.85 71.67 SCISSOR 89.50 73.70 80.80 LU 82.50 67.70 74.40 Table 2: Performance on CLANG. Precision Recall F-measure GOLDSYN 91.94 88.18 90.02 SYN40 90.21 86.93 88.54 SYN0 81.76 78.98 80.35 WASP 91.95 86.59 89.19 Z&amp;C 91.63 86.07 88.76 SCISSOR 95.50 77.20 85.38 KRISP 93.34 71.70 81.10 LU 89.30 81.50 85.20 Table 3: Performance on GEOQUERY. available): SCISSOR (Ge and Mooney, 2005), an integrated syntactic-semantic parser; KRISP (Kate and Mooney, 2006), an SVM-based parser using string kernels; WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), a system based on synchronous grammars; Z&amp;C (Zettlemoyer and Collins, 2007)3, a probabilistic parser based on relaxed CCG grammars; and LU (Lu et al., 2008), a generative model with discriminative reranking. Note that some of these approaches require additional human supervision, knowledge, or engineered features that are unavailable to the other systems; namely, SCISSOR requires gold-standard SAPTs, Z&amp;C requires hand-built tem</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proc. of 9th Conf. on Computational Natural Language Learning (CoNLL-2005), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifang Ge</author>
<author>Raymond J Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<location>Sydney, Australia,</location>
<marker>Ge, Mooney, 2006</marker>
<rawString>Ruifang Ge and Raymond J. Mooney. 2006. Discriminative reranking for semantic parsing. In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06), Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proc. of the 2001 Conf. on Empirical Methods in Natural Language Processing (EMNLP01),</booktitle>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="22242" citStr="Gildea, 2001" startWordPosition="3745" endWordPosition="3746"> returned MRs that were correct), recall (percentage of test examples with correct MRs returned), and F-measure (harmonic mean of precision and recall). For CLANG, an MR was correct if it exactly matched the correct MR, up to reordering of arguments of commutative predicates like and. For GEOQUERY, an MR was correct if it retrieved the same answer as the gold-standard query, thereby reflecting the quality of the final result returned to the user. The performance of a syntactic parser trained only on the Wall Street Journal (WSJ) can degrade dramatically in new domains due to corpus variation (Gildea, 2001). Experiments on CLANG and GEOQUERY showed that the performance can be greatly improved by adding a small number of treebanked examples from the corresponding training set together with the WSJ corpus. Our semantic parser was evaluated using three kinds of syntactic parses. Listed together with their PARSEVAL F-measures these are: gold-standard parses from the treebank (GoldSyn, 100%), a parser trained on WSJ plus a small number of in-domain training sentences required to achieve good performance, 20 for CLANG (Syn20, 88.21%) and 40 for GEOQUERY (Syn40, 91.46%), and a parser trained on no in-d</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proc. of the 2001 Conf. on Empirical Methods in Natural Language Processing (EMNLP01), Pittsburgh, PA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06),</booktitle>
<pages>913--920</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="23587" citStr="Kate and Mooney, 2006" startWordPosition="3955" endWordPosition="3958">results for the given corpus were Precision Recall F-measure GOLDSYN 84.73 74.00 79.00 SYN20 85.37 70.00 76.92 SYN0 87.01 67.00 75.71 WASP 88.85 61.93 72.99 KRISP 85.20 61.85 71.67 SCISSOR 89.50 73.70 80.80 LU 82.50 67.70 74.40 Table 2: Performance on CLANG. Precision Recall F-measure GOLDSYN 91.94 88.18 90.02 SYN40 90.21 86.93 88.54 SYN0 81.76 78.98 80.35 WASP 91.95 86.59 89.19 Z&amp;C 91.63 86.07 88.76 SCISSOR 95.50 77.20 85.38 KRISP 93.34 71.70 81.10 LU 89.30 81.50 85.20 Table 3: Performance on GEOQUERY. available): SCISSOR (Ge and Mooney, 2005), an integrated syntactic-semantic parser; KRISP (Kate and Mooney, 2006), an SVM-based parser using string kernels; WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), a system based on synchronous grammars; Z&amp;C (Zettlemoyer and Collins, 2007)3, a probabilistic parser based on relaxed CCG grammars; and LU (Lu et al., 2008), a generative model with discriminative reranking. Note that some of these approaches require additional human supervision, knowledge, or engineered features that are unavailable to the other systems; namely, SCISSOR requires gold-standard SAPTs, Z&amp;C requires hand-built template grammar rules, LU requires a reranking model using specially desig</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proc. of the 21st Intl. Conf. on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL-06), pages 913–920, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Kuhlmann</author>
<author>Peter Stone</author>
<author>Raymond J Mooney</author>
<author>Jude W Shavlik</author>
</authors>
<title>Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer.</title>
<date>2004</date>
<booktitle>In Proc. of the AAAI-04 Workshop on Supervisory Control of Learning and Adaptive Systems,</booktitle>
<location>San Jose, CA,</location>
<contexts>
<context position="20816" citStr="Kuhlmann et al., 2004" startWordPosition="3509" endWordPosition="3512">g Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. No smoothing is used to regularize the model; We tried using a Gaussian prior (Chen and Rosenfeld, 1999), but it did not improve the results. 7 Experimental Evaluation We evaluated our approach on two standard corpora in CLANG and GEOQUERY. For CLANG, 300 instructions were randomly selected from the log files of the 2003 ROBOCUP Coach 0∗ = arg max θ� = arg max θ� n i=1 n i=1 616 Competition and manually translated into English (Kuhlmann et al., 2004). For GEOQUERY, 880 English questions were gathered from various sources and manually translated into Prolog queries (Tang and Mooney, 2001). The average sentence lengths for the CLANG and GEOQUERY corpora are 22.52 and 7.48, respectively. Our experiments used 10-fold cross validation and proceeded as follows. First Bikel’s implementation of Collins parsing model 2 was trained to generate syntactic parses. Second, a semantic parser was learned from the training set augmented with their syntactic parses. Finally, the learned semantic parser was used to generate the MRs for the test sentences us</context>
</contexts>
<marker>Kuhlmann, Stone, Mooney, Shavlik, 2004</marker>
<rawString>Greg Kuhlmann, Peter Stone, Raymond J. Mooney, and Jude W. Shavlik. 2004. Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer. In Proc. of the AAAI-04 Workshop on Supervisory Control of Learning and Adaptive Systems, San Jose, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP-08),</booktitle>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="1336" citStr="Lu et al., 2008" startWordPosition="195" endWordPosition="198">ge (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic parsers are available for many 1Ge and Mooney (2005) use training examples with semantically annotated parse trees, and Zettlemoyer and Collins (2005) learn a probabilistic semantic parsing model which initially requires a hand-built, ambiguous CCG grammar template. (a) If our p</context>
<context position="23840" citStr="Lu et al., 2008" startWordPosition="3996" endWordPosition="3999">Precision Recall F-measure GOLDSYN 91.94 88.18 90.02 SYN40 90.21 86.93 88.54 SYN0 81.76 78.98 80.35 WASP 91.95 86.59 89.19 Z&amp;C 91.63 86.07 88.76 SCISSOR 95.50 77.20 85.38 KRISP 93.34 71.70 81.10 LU 89.30 81.50 85.20 Table 3: Performance on GEOQUERY. available): SCISSOR (Ge and Mooney, 2005), an integrated syntactic-semantic parser; KRISP (Kate and Mooney, 2006), an SVM-based parser using string kernels; WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), a system based on synchronous grammars; Z&amp;C (Zettlemoyer and Collins, 2007)3, a probabilistic parser based on relaxed CCG grammars; and LU (Lu et al., 2008), a generative model with discriminative reranking. Note that some of these approaches require additional human supervision, knowledge, or engineered features that are unavailable to the other systems; namely, SCISSOR requires gold-standard SAPTs, Z&amp;C requires hand-built template grammar rules, LU requires a reranking model using specially designed global features, and our approach requires an existing syntactic parser. The F-measures for syntactic parses that generate correct MRs in CLANG are 85.50% for syn0 and 91.16% for syn20, showing that our method can produce correct MRs even when given</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP-08), Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proc. of Human Language Technology Conf.(HLT-2002),</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="20128" citStr="Miyao and Tsujii, 2002" startWordPosition="3389" endWordPosition="3392">ntic derivations that lead to the MR. Formally, given a set of NLMR pairs {(S1, M1), (S2, M2), ..., (Sn, Mn)} and the syntactic parses of the NLs {T1, T2,..., Tn}, the parameter �0∗ is calculated as: log Pr(Mi|Si, Ti; 0) (3) log � Pr(D∗i |Si, Ti; 0) D* z where D∗i is a semantic derivation that produces the correct MR Mi. L-BFGS (Nocedal, 1980) is used to estimate the parameters �0∗. The estimation requires statistics that depend on all possible semantic derivations and all correct semantic derivations of an example, which are not feasibly enumerated. A variant of the Inside-Outside algorithm (Miyao and Tsujii, 2002) is used to efficiently collect the necessary statistics. Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. No smoothing is used to regularize the model; We tried using a Gaussian prior (Chen and Rosenfeld, 1999), but it did not improve the results. 7 Experimental Evaluation We evaluated our approach on two standard corpora in CLANG and GEOQUERY. For CLANG, 300 instructions were randomly selected from the log files of the 2003 ROBOCUP Coach 0∗ = arg max θ� = arg max θ� </context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proc. of Human Language Technology Conf.(HLT-2002), San Diego, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating quasi-Newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<volume>35</volume>
<issue>151</issue>
<contexts>
<context position="19850" citStr="Nocedal, 1980" startWordPosition="3348" endWordPosition="3349">imizes the sum of the conditional log-likelihood of the MRs in the training set. Since no specific semantic derivation for an MR is provided in the training data, the conditional loglikelihood of an MR is calculated as the sum of the conditional probability of all semantic derivations that lead to the MR. Formally, given a set of NLMR pairs {(S1, M1), (S2, M2), ..., (Sn, Mn)} and the syntactic parses of the NLs {T1, T2,..., Tn}, the parameter �0∗ is calculated as: log Pr(Mi|Si, Ti; 0) (3) log � Pr(D∗i |Si, Ti; 0) D* z where D∗i is a semantic derivation that produces the correct MR Mi. L-BFGS (Nocedal, 1980) is used to estimate the parameters �0∗. The estimation requires statistics that depend on all possible semantic derivations and all correct semantic derivations of an example, which are not feasibly enumerated. A variant of the Inside-Outside algorithm (Miyao and Tsujii, 2002) is used to efficiently collect the necessary statistics. Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. No smoothing is used to regularize the model; We tried using a Gaussian prior (Chen and </context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Jorge Nocedal. 1980. Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35(151):773–782, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2790" citStr="Och and Ney, 2003" startWordPosition="415" endWordPosition="418">the ROBOCUP and GEOQUERY domains respectively. languages and could potentially be used to learn more effective semantic analyzers. This paper presents an approach to learning semantic parsers that uses parse trees from an existing syntactic analyzer to drive the interpretation process. The learned parser uses standard compositional semantics to construct alternative MRs for a sentence based on its syntax tree, and then chooses the best MR based on a trained statistical disambiguation model. The learning system first employs a word alignment method from statistical machine translation (GIZA++ (Och and Ney, 2003)) to acquire a semantic lexicon that maps words to logical predicates. Then it induces rules for composing MRs and estimates the parameters of a maximumentropy model for disambiguating semantic interpretations. After describing the details of our approach, we present experimental results on standard corpora demonstrating improved results on learning NL interfaces for database querying and simulated robot control. 2 Background In this paper, we consider two domains. The first is ROBOCUP (www.robocup.org). In the ROBOCUP Coach Competition, soccer agents compete on a simulated soccer field and re</context>
<context position="15255" citStr="Och and Ney, 2003" startWordPosition="2544" endWordPosition="2547">ctive part of the example in Fig. 1(a) using the syntactic parse in Fig. 4(b): (a) A SAPT with syntactic labels omitted for brevity. (b) The predicate-argument structure of macro-predicate P DO POS (c) The semantic derivation of the MR. nodes of the resulting semantic derivations. Generation of word alignments for each training example proceeds as follows. First, each MR in the training corpus is parsed using the MRLG. Next, each resulting parse tree is linearized to produce a sequence of predicates by using a topdown, left-to-right traversal of the parse tree. Then the GIZA++ implementation (Och and Ney, 2003) of IBM Model 5 is used to generate the five best word/predicate alignments from the corpus of NL sentences each paired with the predicate sequence for its MR. After predicates are assigned to words using word alignment, for each alignment of a training example and its syntactic parse, a SAPT is generated for composing the correct MR using the processes discussed in Sections 3 and 4. Specifically, a semantic label is assigned to each internal node of each SAPT, so that the MRs of its children are composed correctly according to the MR for this example. There are two cases that require special </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proc. of the 12th European Conf. on Machine Learning,</booktitle>
<pages>466--477</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="20956" citStr="Tang and Mooney, 2001" startWordPosition="3530" endWordPosition="3533">t are retained for testing. No smoothing is used to regularize the model; We tried using a Gaussian prior (Chen and Rosenfeld, 1999), but it did not improve the results. 7 Experimental Evaluation We evaluated our approach on two standard corpora in CLANG and GEOQUERY. For CLANG, 300 instructions were randomly selected from the log files of the 2003 ROBOCUP Coach 0∗ = arg max θ� = arg max θ� n i=1 n i=1 616 Competition and manually translated into English (Kuhlmann et al., 2004). For GEOQUERY, 880 English questions were gathered from various sources and manually translated into Prolog queries (Tang and Mooney, 2001). The average sentence lengths for the CLANG and GEOQUERY corpora are 22.52 and 7.48, respectively. Our experiments used 10-fold cross validation and proceeded as follows. First Bikel’s implementation of Collins parsing model 2 was trained to generate syntactic parses. Second, a semantic parser was learned from the training set augmented with their syntactic parses. Finally, the learned semantic parser was used to generate the MRs for the test sentences using their syntactic parses. If a test example contains constructs that did not occur in training, the parser may fail to return an MR. We me</context>
<context position="26884" citStr="Tang and Mooney, 2001" startWordPosition="4483" endWordPosition="4486">ERY, where sentences are normally short. Longer sentences typically have more complex syntax, and the traditional syntactic analysis used by our approach results in better compositional semantic analysis in this situation. We also ran experiments with less training data. For CLANG, 40 random examples from the training sets (CLANG40) were used. For GEOQUERY, an existing 250-example subset (GEO250) (Zelle and Mooney, 1996) was used. The results are shown in Tables 4 and 5. Note the performance of our systems on GEO250 is higher than that on GEOQUERY since GEOQUERY includes more complex queries (Tang and Mooney, 2001). First, all of our systems gave the best F-measures (except SYN0 compared to SCISSOR in CLANG40), and the differences are generally quite substantial. This shows that our approach significantly improves results when limited training data is available. Second, in CLANG, reducing the training data increased the difference between SYN20 and SYN0. This suggests that the quality of syntactic parsing becomes more important when less training data is available. This demonstrates the advantage of utilizing existing syntactic parsers that are learned from large open domain treebanks instead of relying</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon R. Tang and Raymond J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proc. of the 12th European Conf. on Machine Learning, pages 466–477, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of Human Language Technology Conf. / N. American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL-2006),</booktitle>
<pages>439--446</pages>
<contexts>
<context position="13558" citStr="Wong and Mooney (2006)" startWordPosition="2258" endWordPosition="2261"> the following two sections, we describe the two subtasks of inducing semantic knowledge and a disambiguation model for this enhanced compositional framework. Both subtasks require a training set of NLs paired with their MRs. Each NL sentence also requires a syntactic parse generated using Bikel’s (2004) implementation of Collins parsing model 2. Note that unlike SCISSOR (Ge and Mooney, 2005), training our method does not require gold-standard SAPTs. 5 Learning Semantic Knowledge Learning semantic knowledge starts from learning the mapping from words to predicates. We use an approach based on Wong and Mooney (2006), which constructs word alignments between NL sentences and their MRs. Normally, word alignment is used in statistical machine translation to match words in one NL to words in another; here it is used to align words with predicates based on a ”parallel corpus” of NL sentences and MRs. We assume that each word alignment defines a possible mapping from words to predicates for building a SAPT and semantic derivation which compose the correct MR. A semantic lexicon and composition rules are then extracted directly from the 614 (Aa1Aa2)P DO then Ap1P DO POS = Ap1P DO P DO a1:PLAYER P POS P MIDFIELD</context>
<context position="18462" citStr="Wong and Mooney (2006)" startWordPosition="3100" endWordPosition="3103">, R} (1) 6 Learning a Disambiguation Model Usually, multiple possible semantic derivations for an NL sentence are warranted by the acquired semantic knowledge, thus disambiguation is needed. To learn a disambiguation model, the learned semantic knowledge (see Section 5) is applied to each training example to generate all possible semantic derivations for an NL sentence given its syntactic parse. Here, unique word alignments are not required, and alternative interpretations compete for the best semantic parse. We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006). The model defines a conditional probability distribution over semantic derivations (D) given an NL sentence S and its syntactic parse T: Pr(D|S, T; �0) = exp Ei 0ifi(D) (2) ZB(S,T) where f (f1, ... , fn) is a feature vector parameterized by 0, and Z6(S, T) is a normalizing factor. Three simple types of features are used in the model. First, are lexical features which count the number of times a word is assigned a particular predicate. Second, are bilexical features which count the number of times a word is assigned a particular predicate and a particular word precedes or follows it. Last, ar</context>
<context position="20218" citStr="Wong and Mooney (2006)" startWordPosition="3403" endWordPosition="3406">2), ..., (Sn, Mn)} and the syntactic parses of the NLs {T1, T2,..., Tn}, the parameter �0∗ is calculated as: log Pr(Mi|Si, Ti; 0) (3) log � Pr(D∗i |Si, Ti; 0) D* z where D∗i is a semantic derivation that produces the correct MR Mi. L-BFGS (Nocedal, 1980) is used to estimate the parameters �0∗. The estimation requires statistics that depend on all possible semantic derivations and all correct semantic derivations of an example, which are not feasibly enumerated. A variant of the Inside-Outside algorithm (Miyao and Tsujii, 2002) is used to efficiently collect the necessary statistics. Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. No smoothing is used to regularize the model; We tried using a Gaussian prior (Chen and Rosenfeld, 1999), but it did not improve the results. 7 Experimental Evaluation We evaluated our approach on two standard corpora in CLANG and GEOQUERY. For CLANG, 300 instructions were randomly selected from the log files of the 2003 ROBOCUP Coach 0∗ = arg max θ� = arg max θ� n i=1 n i=1 616 Competition and manually translated into English (Kuhlmann et al., 2004). </context>
<context position="23658" citStr="Wong and Mooney, 2006" startWordPosition="3966" endWordPosition="3969">73 74.00 79.00 SYN20 85.37 70.00 76.92 SYN0 87.01 67.00 75.71 WASP 88.85 61.93 72.99 KRISP 85.20 61.85 71.67 SCISSOR 89.50 73.70 80.80 LU 82.50 67.70 74.40 Table 2: Performance on CLANG. Precision Recall F-measure GOLDSYN 91.94 88.18 90.02 SYN40 90.21 86.93 88.54 SYN0 81.76 78.98 80.35 WASP 91.95 86.59 89.19 Z&amp;C 91.63 86.07 88.76 SCISSOR 95.50 77.20 85.38 KRISP 93.34 71.70 81.10 LU 89.30 81.50 85.20 Table 3: Performance on GEOQUERY. available): SCISSOR (Ge and Mooney, 2005), an integrated syntactic-semantic parser; KRISP (Kate and Mooney, 2006), an SVM-based parser using string kernels; WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), a system based on synchronous grammars; Z&amp;C (Zettlemoyer and Collins, 2007)3, a probabilistic parser based on relaxed CCG grammars; and LU (Lu et al., 2008), a generative model with discriminative reranking. Note that some of these approaches require additional human supervision, knowledge, or engineered features that are unavailable to the other systems; namely, SCISSOR requires gold-standard SAPTs, Z&amp;C requires hand-built template grammar rules, LU requires a reranking model using specially designed global features, and our approach requires an existing syntactic pa</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proc. of Human Language Technology Conf. / N. American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL-2006), pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07),</booktitle>
<pages>960--967</pages>
<contexts>
<context position="1318" citStr="Wong and Mooney, 2007" startWordPosition="191" endWordPosition="194">apping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic parsers are available for many 1Ge and Mooney (2005) use training examples with semantically annotated parse trees, and Zettlemoyer and Collins (2005) learn a probabilistic semantic parsing model which initially requires a hand-built, ambiguous CCG grammar temp</context>
<context position="23682" citStr="Wong and Mooney, 2007" startWordPosition="3970" endWordPosition="3973">.37 70.00 76.92 SYN0 87.01 67.00 75.71 WASP 88.85 61.93 72.99 KRISP 85.20 61.85 71.67 SCISSOR 89.50 73.70 80.80 LU 82.50 67.70 74.40 Table 2: Performance on CLANG. Precision Recall F-measure GOLDSYN 91.94 88.18 90.02 SYN40 90.21 86.93 88.54 SYN0 81.76 78.98 80.35 WASP 91.95 86.59 89.19 Z&amp;C 91.63 86.07 88.76 SCISSOR 95.50 77.20 85.38 KRISP 93.34 71.70 81.10 LU 89.30 81.50 85.20 Table 3: Performance on GEOQUERY. available): SCISSOR (Ge and Mooney, 2005), an integrated syntactic-semantic parser; KRISP (Kate and Mooney, 2006), an SVM-based parser using string kernels; WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), a system based on synchronous grammars; Z&amp;C (Zettlemoyer and Collins, 2007)3, a probabilistic parser based on relaxed CCG grammars; and LU (Lu et al., 2008), a generative model with discriminative reranking. Note that some of these approaches require additional human supervision, knowledge, or engineered features that are unavailable to the other systems; namely, SCISSOR requires gold-standard SAPTs, Z&amp;C requires hand-built template grammar rules, LU requires a reranking model using specially designed global features, and our approach requires an existing syntactic parser. The F-measures for</context>
<context position="25864" citStr="Wong and Mooney, 2007" startWordPosition="4318" endWordPosition="4321">8.35 20.00 30.95 SCISSOR 85.00 23.00 36.20 Table 4: Performance on CLANG40. Precision Recall F-measure GOLDSYN 95.73 89.60 92.56 SYN20 93.19 87.60 90.31 SYN0 91.81 85.20 88.38 WASP 91.76 75.60 82.90 SCISSOR 98.50 74.40 84.77 KRISP 84.43 71.60 77.49 LU 91.46 72.80 81.07 Table 5: Performance on GEO250 (20 in-domain sentences are used in SYN20 to train the syntactic parser). of interrogative sentences (questions) in the WSJ corpus. Note the results for SCISSOR, KRISP and LU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). Third, SCISSOR performs better than our methods on CLANG, but it requires extra human supervision that is not available to the other systems. Lastly, a detailed analysis showed that our improved performance on CLANG compared to WASP and KRISP is mainly for long sentences (&gt; 20 words), while performance on shorter sentences is similar. This is consistent with their relative performance on GEOQUERY, where sentences are normally short. Longer sentences typically have more complex syntax, and the traditional syntactic analysis used by our approach results in better compositional semantic analysi</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics (ACL-07), pages 960–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
</authors>
<title>Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Sciences, University of Texas,</institution>
<location>Austin, TX,</location>
<contexts>
<context position="5232" citStr="Wong, 2007" startWordPosition="828" endWordPosition="829"> domain. We assume that an MRL is defined by an unambiguous context-free grammar (MRLG), so that MRs can be uniquely parsed, a standard requirement for computer languages. In an MRLG, each production rule introduces a single predicate in the MRL, where the type of the predicate is given in the left hand side (LHS), and the number and types of its arguments are defined by the nonterminals in the right hand side (RHS). Therefore, the parse of an MR also gives its predicate-argument structure. Figure 2(a) shows the parse of the condition part of the MR in Figure 1(a) using the MRLG described in (Wong, 2007), and its predicateargument structure is in Figure 2(b). Sample MRLG productions and their predicates for parsing this example are shown in Table 1, where the predicate P PLAYER takes two arguments (a1 and a2) of type TEAM and UNUM (uniform number). 3 Semantic Parsing Framework This section describes our basic framework, which is based on a fairly standard approach to computational semantics (Blackburn and Bos, 2005). The framework is composed of three components: 1) an existing syntactic parser to produce parse trees for NL sentences; 2) learned semantic knowledge (cf. Sec. 5), including a se</context>
</contexts>
<marker>Wong, 2007</marker>
<rawString>Yuk Wah Wong. 2007. Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques. Ph.D. thesis, Department of Computer Sciences, University of Texas, Austin, TX, August. Also appears as Artificial Intelligence Laboratory Technical Report AI07-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proc. of 13th Natl. Conf. on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="3661" citStr="Zelle and Mooney, 1996" startWordPosition="552" endWordPosition="555">pproach, we present experimental results on standard corpora demonstrating improved results on learning NL interfaces for database querying and simulated robot control. 2 Background In this paper, we consider two domains. The first is ROBOCUP (www.robocup.org). In the ROBOCUP Coach Competition, soccer agents compete on a simulated soccer field and receive coaching instructions in a formal language called CLANG (Chen et al., 2003). Figure 1(a) shows a sample instruction. The second domain is GEOQUERY, where a logical query language based on Prolog is used to query a database on U.S. geography (Zelle and Mooney, 1996). The logical lan611 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 611–619, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP CONDITION (bowner PLAYER) (player TEAM {UNUM}) 2 P BOWNER NP VP S P PLAYER P OUR P UNUM (b) our DET has NN player 2 the ball CD NN (c) VB NP NP PRP$ our (a) Figure 2: Parses for the condition part of the CLANG in Figure 1(a): (a) The parse of the MR. (b) The predicate argument structure of (a). (c) The parse of the NL. PRODUCTION PREDICATE RULES(CONDITION DIRECTIVE) P RULE CONDITIONS(bowner PLAYER) P BOWNER PLAYERS(playe</context>
<context position="26686" citStr="Zelle and Mooney, 1996" startWordPosition="4448" endWordPosition="4451">rformance on CLANG compared to WASP and KRISP is mainly for long sentences (&gt; 20 words), while performance on shorter sentences is similar. This is consistent with their relative performance on GEOQUERY, where sentences are normally short. Longer sentences typically have more complex syntax, and the traditional syntactic analysis used by our approach results in better compositional semantic analysis in this situation. We also ran experiments with less training data. For CLANG, 40 random examples from the training sets (CLANG40) were used. For GEOQUERY, an existing 250-example subset (GEO250) (Zelle and Mooney, 1996) was used. The results are shown in Tables 4 and 5. Note the performance of our systems on GEO250 is higher than that on GEOQUERY since GEOQUERY includes more complex queries (Tang and Mooney, 2001). First, all of our systems gave the best F-measures (except SYN0 compared to SCISSOR in CLANG40), and the differences are generally quite substantial. This shows that our approach significantly improves results when limited training data is available. Second, in CLANG, reducing the training data increased the difference between SYN20 and SYN0. This suggests that the quality of syntactic parsing bec</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proc. of 13th Natl. Conf. on Artificial Intelligence (AAAI-96), pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proc. of the 21th Annual Conf. on Uncertainty in Artificial Intelligence (UAI-05).</booktitle>
<contexts>
<context position="1295" citStr="Zettlemoyer and Collins, 2005" startWordPosition="187" endWordPosition="190">mantic parsing is the task of mapping a natural language (NL) sentence into a completely formal meaning representation (MR) or logical form. A meaning representation language (MRL) is a formal unambiguous language that supports automated inference, such as first-order predicate logic. This distinguishes it from related tasks such as semantic role labeling (SRL) (Carreras and Marquez, 2004) and other forms of “shallow” semantic analysis that do not produce completely formal representations. A number of systems for automatically learning semantic parsers have been proposed (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008). Given a training corpus of NL sentences annotated with their correct MRs, these systems induce an interpreter for mapping novel sentences into the given MRL. Previous methods for learning semantic parsers do not utilize an existing syntactic parser that provides disambiguated parse trees.1 However, accurate syntactic parsers are available for many 1Ge and Mooney (2005) use training examples with semantically annotated parse trees, and Zettlemoyer and Collins (2005) learn a probabilistic semantic parsing model which initially requires a hand-built, amb</context>
<context position="18435" citStr="Zettlemoyer and Collins (2005)" startWordPosition="3095" endWordPosition="3098">of the form: A1.P1 + A2.P2 ⇒ {Ap.Pp, R} (1) 6 Learning a Disambiguation Model Usually, multiple possible semantic derivations for an NL sentence are warranted by the acquired semantic knowledge, thus disambiguation is needed. To learn a disambiguation model, the learned semantic knowledge (see Section 5) is applied to each training example to generate all possible semantic derivations for an NL sentence given its syntactic parse. Here, unique word alignments are not required, and alternative interpretations compete for the best semantic parse. We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006). The model defines a conditional probability distribution over semantic derivations (D) given an NL sentence S and its syntactic parse T: Pr(D|S, T; �0) = exp Ei 0ifi(D) (2) ZB(S,T) where f (f1, ... , fn) is a feature vector parameterized by 0, and Z6(S, T) is a normalizing factor. Three simple types of features are used in the model. First, are lexical features which count the number of times a word is assigned a particular predicate. Second, are bilexical features which count the number of times a word is assigned a particular predicate and a particular word prece</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proc. of the 21th Annual Conf. on Uncertainty in Artificial Intelligence (UAI-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-07),</booktitle>
<pages>678--687</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="23759" citStr="Zettlemoyer and Collins, 2007" startWordPosition="3981" endWordPosition="3984">5.20 61.85 71.67 SCISSOR 89.50 73.70 80.80 LU 82.50 67.70 74.40 Table 2: Performance on CLANG. Precision Recall F-measure GOLDSYN 91.94 88.18 90.02 SYN40 90.21 86.93 88.54 SYN0 81.76 78.98 80.35 WASP 91.95 86.59 89.19 Z&amp;C 91.63 86.07 88.76 SCISSOR 95.50 77.20 85.38 KRISP 93.34 71.70 81.10 LU 89.30 81.50 85.20 Table 3: Performance on GEOQUERY. available): SCISSOR (Ge and Mooney, 2005), an integrated syntactic-semantic parser; KRISP (Kate and Mooney, 2006), an SVM-based parser using string kernels; WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), a system based on synchronous grammars; Z&amp;C (Zettlemoyer and Collins, 2007)3, a probabilistic parser based on relaxed CCG grammars; and LU (Lu et al., 2008), a generative model with discriminative reranking. Note that some of these approaches require additional human supervision, knowledge, or engineered features that are unavailable to the other systems; namely, SCISSOR requires gold-standard SAPTs, Z&amp;C requires hand-built template grammar rules, LU requires a reranking model using specially designed global features, and our approach requires an existing syntactic parser. The F-measures for syntactic parses that generate correct MRs in CLANG are 85.50% for syn0 and </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proc. of the 2007 Joint Conf. on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-07), pages 678–687, Prague, Czech Republic, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>