<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001596">
<title confidence="0.960562">
Language Dynamics and Capitalization using Maximum Entropy
</title>
<author confidence="0.852197">
Fernando Batistaa,b, Nuno Mamedea,c and Isabel Trancosoa,c
</author>
<note confidence="0.794252333333333">
a
L2F – Spoken Language Systems Laboratory - INESC ID Lisboa
R. Alves Redol, 9, 1000-029 Lisboa, Portugal
</note>
<email confidence="0.324183">
http://www.l2f.inesc-id.pt/
</email>
<author confidence="0.353806">
b ISCTE – Instituto de Ciências do Trabalho e da Empresa, Portugal
</author>
<affiliation confidence="0.227025">
c IST – Instituto Superior Técnico, Portugal.
</affiliation>
<email confidence="0.985528">
{fmmb,njm,imt}@l2f.inesc-id.pt
</email>
<sectionHeader confidence="0.995159" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884846153846">
This paper studies the impact of written lan-
guage variations and the way it affects the cap-
italization task over time. A discriminative
approach, based on maximum entropy mod-
els, is proposed to perform capitalization, tak-
ing the language changes into consideration.
The proposed method makes it possible to use
large corpora for training. The evaluation is
performed over newspaper corpora using dif-
ferent testing periods. The achieved results
reveal a strong relation between the capital-
ization performance and the elapsed time be-
tween the training and testing data periods.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999879">
The capitalization task, also known as truecasing
(Lita et al., 2003), consists of rewriting each word
of an input text with its proper case information.
The capitalization of a word sometimes depends on
its current context, and the intelligibility of texts is
strongly influenced by this information. Different
practical applications benefit from automatic capi-
talization as a preprocessing step: when applied to
speech recognition output, which usually consists
of raw text, automatic capitalization provides rele-
vant information for automatic content extraction,
named entity recognition, and machine translation;
many computer applications, such as word process-
ing and e-mail clients, perform automatic capital-
ization along with spell corrections and grammar
check.
The capitalization problem can be seen as a se-
quence tagging problem (Chelba and Acero, 2004;
</bodyText>
<page confidence="0.845102">
1
</page>
<bodyText confidence="0.999445941176471">
Lita et al., 2003; Kim and Woodland, 2004), where
each lower-case word is associated to a tag that de-
scribes its capitalization form. (Chelba and Acero,
2004) study the impact of using increasing amounts
of training data as well as a small amount of adap-
tation. This work uses a Maximum Entropy Markov
Model (MEMM) based approach, which allows to
combine different features. A large written news-
paper corpora is used for training and the test data
consists of Broadcast News (BN) data. (Lita et al.,
2003) builds a trigram language model (LM) with
pairs (word, tag), estimated from a corpus with case
information, and then uses dynamic programming to
disambiguate over all possible tag assignments on a
sentence. Other related work includes a bilingual
capitalization model for capitalizing machine trans-
lation (MT) outputs, using conditional random fields
(CRFs) reported by (Wang et al., 2006). This work
exploits case information both from source and tar-
get sentences of the MT system, producing better
performance than a baseline capitalizer using a tri-
gram language model. A preparatory study on the
capitalization of Portuguese BN has been performed
by (Batista et al., 2007).
One important aspect related with capitalization
concerns the language dynamics: new words are in-
troduced everyday in our vocabularies and the usage
of some other words decays with time. Concerning
this subject, (Mota, 2008) shows that, as the time
gap between training and test data increases, the per-
formance of a named tagger based on co-training
(Collins and Singer, 1999) decreases.
This paper studies and evaluates the effects of lan-
guage dynamics in the capitalization of newspaper
</bodyText>
<subsubsectionHeader confidence="0.352898">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1–4,
</subsubsectionHeader>
<page confidence="0.482075">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.996005666666667">
corpora. Section 2 describes the corpus and presents
a short analysis on the lexicon variation. Section 3
presents experiments concerning the capitalization
task, either using isolated training sets or by retrain-
ing with different training sets. Section 4 concludes
and presents future plans.
</bodyText>
<sectionHeader confidence="0.992368" genericHeader="method">
2 Newspaper Corpus
</sectionHeader>
<bodyText confidence="0.999971181818182">
Experiments here described use the RecPub news-
paper corpus, which consists of collected editions
of the Portuguese “Público” newspaper. The corpus
was collected from 1999 to 2004 and contains about
148 Million words. The corpus was split into 59 sub-
sets of about 2.5 Million words each (between 9 to
11 per year). The last subset is only used for testing,
nevertheless, most of the experiments here described
use different training and test subsets for better un-
derstanding the time effects on capitalization. Each
subset corresponds to about five weeks of data.
</bodyText>
<subsectionHeader confidence="0.996936">
2.1 Data Analysis
</subsectionHeader>
<bodyText confidence="0.999784833333333">
The number of unique words in each subset is
around 86K but only about 50K occur more than
once. In order to assess the relation between the
word usage and the time gap, we created a number
of vocabularies with the 30K more frequent words
appearing in each training set (roughly corresponds
to a freq &gt; 3). Then, the first and last corpora subsets
were checked against each one of the vocabularies.
Figure 1 shows the correspondent results, revealing
that the number of OOVs (Out of Vocabulary Words)
decreases as the time gap between the train and test
periods gets smaller.
</bodyText>
<figure confidence="0.651211">
1999-01 2004-12
Vocabulary period
</figure>
<figureCaption confidence="0.999256">
Figure 1: Number of OOVs using a 30K vocabulary.
</figureCaption>
<sectionHeader confidence="0.978759" genericHeader="method">
3 Capitalization
</sectionHeader>
<bodyText confidence="0.999988243243243">
The present study explores only three ways of
writing a word: lower-case, all-upper, and first-
capitalized, not covering mixed-case words such as
“McLaren” and “SuSE”. In fact, mixed-case words
are also being treated by means of a small lexicon,
but they are not evaluated in the scope of this paper.
The following experiments assume that the capi-
talization of the first word of each sentence is per-
formed in a separated processing stage (after punc-
tuation for instance), since its correct graphical form
depends on its position in the sentence. Evaluation
results may be influenced when taking such words
into account (Kim and Woodland, 2004).
The evaluation is performed using the met-
rics: Precision, Recall and SER (Slot Error Rate)
(Makhoul et al., 1999). Only capitalized words (not
lowercase) are considered as slots and used by these
metrics. For example: Precision is calculated by di-
viding the number of correct capitalized words by
the number of capitalized words in the testing data.
The modeling approach here described is discrim-
inative, and is based on maximum entropy (ME)
models, firstly applied to natural language problems
in (Berger et al., 1996). An ME model estimates
the conditional probability of the events given the
corresponding features. Therefore, all the infor-
mation must be expressed in terms of features in
a pre-processing step. Experiments here described
only use features comprising word unigrams and bi-
grams: wz (current word), (wz_1, wz) and (wz, wz+1)
(bigrams). Only words occurring more than once
were included for training, thus reducing the number
of misspelled words. All the experiments used the
MegaM tool (Daumé III, 2004), which uses conju-
gate gradient and a limited memory optimization of
logistic regression. The following subsections de-
scribe the achieved results.
</bodyText>
<subsectionHeader confidence="0.988311">
3.1 Isolated Training
</subsectionHeader>
<bodyText confidence="0.999917428571429">
In order to assess how time affects the capitalization
performance, the first experiments consist of pro-
ducing six isolated language models, one for each
year of training data. For each year, the first 8 sub-
sets were used for training and the last one was used
for evaluation. Table 1 shows the corresponding
capitalization results for the first and last testing sub-
</bodyText>
<figure confidence="0.997804035714286">
OOV
140k
130k
120k
110k
100k
90k
80k
1999-01
1999-05
1999-10
2000-02
2000-05
2000-09
2000-12
2001-03
2001-07
2001-10
2002-01
2002-04
2002-08
2002-12
2003-04
2003-08
2003-12
2004-03
2004-06
2004-10
</figure>
<page confidence="0.988581">
2
</page>
<table confidence="0.999775125">
Train 1999-12 test set 2004-12 test set
Prec Rec SER Prec Rec SER
1999 94% 81% 0.240 92% 76% 0.296
2000 94% 81% 0.242 92% 77% 0.291
2001 94% 79% 0.262 93% 76% 0.291
2002 93% 79% 0.265 93% 78% 0.277
2003 94% 77% 0.276 93% 78% 0.273
2004 93% 77% 0.285 93% 80% 0.264
</table>
<tableCaption confidence="0.999976">
Table 1: Using 8 subsets of each year for training.
</tableCaption>
<figure confidence="0.998324857142857">
0.31
0.29
0.27
0.25
0.23
1999 2000 2001 2002 2003 2004
Training period
</figure>
<figureCaption confidence="0.999774">
Figure 2: Performance for different training periods.
</figureCaption>
<bodyText confidence="0.9998587">
sets, revealing that performance is affected by the
time lapse between the training and testing periods.
The best results were always produced with nearby
the testing data. A similar behavior was observed on
the other four testing subsets, corresponding to the
last subset of each year. Results also reveal a degra-
dation of performance when the training data is from
a time period after the evaluation data.
Results from previous experiment are still worse
than results achieved by other work on the area
(Batista et al., 2007) (about 94% precision and 88%
recall), specially in terms of recall. This is caused
by a low coverage of the training data, thus reveal-
ing that each training set (20 Million words) does not
provide sufficient data for the capitalization task.
One important problem related with this discrim-
inative approach concerns memory limitations. The
memory required increases with the size of the cor-
pus (number of observations), preventing the use
of large corpora, such as RecPub for training, with
</bodyText>
<table confidence="0.842228">
Evaluation Set Prec Rec SER
2004-12 test set 93% 82% 0.233
</table>
<tableCaption confidence="0.99901">
Table 2: Training with all RecPub training data.
</tableCaption>
<table confidence="0.999327857142857">
Checkpoint LM #lines Prec Rec SER
1999-12 1.27 Million 92% 77% 0.290
2000-12 1.86 Million 93% 79% 0.266
2001-12 2.36 Million 93% 80% 0.257
2002-12 2.78 Million 93% 81% 0.247
2003-12 3.10 Million 93% 82% 0.236
2004-08 3.36 Million 93% 83% 0.225
</table>
<tableCaption confidence="0.999943">
Table 3: Retraining from Jan. 1999 to Sep. 2004.
</tableCaption>
<bodyText confidence="0.999649">
available computers. For example, four million
events require about 8GB of RAM to process. This
problem can be minimized using a modified train-
ing strategy, based on the fact that scaling the event
by the number of occurrences is equivalent to multi-
ple occurrences of that event. Accordingly to this,
our strategy to use large training corpora consists
of counting all n-gram occurrences in the training
data and then use such counts to produce the cor-
responding input features. This strategy allows us
to use much larger corpora and also to remove less
frequent n-grams if desired. Table 2 shows the per-
formance achieved by following this strategy with
all the RecPub training data. Only word frequen-
cies greater than 4 were considered, minimizing the
effects of misspelled words and reducing memory
limitations. Results reveal the expected increase of
performance, specially in terms of recall. However,
these results can not be directly compared with pre-
vious work on this subject, because of the different
corpora used.
</bodyText>
<subsectionHeader confidence="0.999251">
3.2 Retraining
</subsectionHeader>
<bodyText confidence="0.999876866666667">
Results presented so far use isolated training. A new
approach is now proposed, which consists of train-
ing with new data, but starting with previously cal-
culated models. In other words, previously trained
models provide initialized models for the new train.
As the training is still performed with the new data,
the old models are iteratively adjusted to the new
data. This approach is a very clean framework for
language dynamics adaptation, offering a number of
advantages: (1) new events are automatically con-
sidered in the new models; (2) with time, unused
events slowly decrease in weight; (3) by sorting the
trained models by their relevance, the amount of data
used in next training stage can be limited without
much impact in the results. Table 3 shows the re-
</bodyText>
<figure confidence="0.850426">
1999-12 test set 2004-12 test set
SER
3
Checkpoint data
</figure>
<figureCaption confidence="0.997907">
Figure 3: Training forward and backwards
</figureCaption>
<bodyText confidence="0.9999801">
sults achieved with this approach, revealing higher
performance as more training data is available.
The next experiment shows that the training or-
der is important. In fact, from previous results, the
increase of performance may be related only with
the number of events seen so far. For this reason,
another experiment have been performed, using the
same training data, but retraining backwards. Corre-
sponding results are illustrated in Figure 3, revealing
that: the backwards training results are worse than
forward training results, and that backward training
results do not allways increase, rather stabilize af-
ter a certain amount of data. Despite the fact that
both training use all training data, in the case of for-
ward training the time gap between the training and
testing data gets smaller for each iteration, while in
the backwards training is grows. From these results
we can conclude that a strategy based on retraining
is suitable for using large amounts of data and for
language adaptation.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99992505">
This paper shows that maximum entropy models
can be used to perform the capitalization task, spe-
cially when dealing with language dynamics. This
approach provides a clean framework for learning
with new data, while slowly discarding unused data.
The performance achieved is almost as good as us-
ing generative approaches, found in related work.
This approach also allows to combine different data
sources and to explore different features. In terms
of language changes, our proposal states that differ-
ent capitalization models should be used for differ-
ent time periods.
Future plans include the application of this work
to BN data, automatically produced by our speech
recognition system. In fact, subtitling of BN has led
us into using a baseline vocabulary of 100K words
combined with a daily modification of the vocabu-
lary (Martins et al., 2007) and a re-estimation of the
language model. This dynamic vocabulary provides
an interesting scenario for our experiments.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999653333333333">
This work was funded by PRIME National Project
TECNOVOZ number 03/165, and FCT project
CMU-PT/0005/2007.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822685714286">
F. Batista, N. J. Mamede, D. Caseiro, and I. Trancoso.
2007. A lightweight on-the-fly capitalization system
for automatic speech recognition. In Proc. of the
RANLP 2007, Borovets, Bulgaria, September.
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–71.
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot.
EMNLP04.
M. Collins and Y. Singer. 1999. Unsupervised models
for named entity classification. In Proc. of the Joint
SIGDAT Conference on EMNLP.
H. Daumé III. 2004. Notes on CG and LM-BFGS opti-
mization of logistic regression.
J. Kim and P. C. Woodland. 2004. Automatic capitalisa-
tion generation for speech input. Computer Speech &amp;
Language, 18(1):67–90.
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla.
2003. tRuEcasIng. In Proc. of the 41&amp;quot; annual meet-
ing on ACL, pages 152–159, Morristown, NJ, USA.
J. Makhoul, F. Kubala, R. Schwartz, and R. Weischedel.
1999. Performance measures for information extrac-
tion. In Proceedings of the DARPA Broadcast News
Workshop, Herndon, VA, Feb.
C. Martins, A. Teixeira, and J. P. Neto. 2007. Dynamic
language modeling for a daily broadcast news tran-
scription system. In ASRU 2007, December.
Cristina Mota. 2008. How to keep up with language
dynamics? A case study on Named Entity Recognition.
Ph.D. thesis, IST / UTL.
Wei Wang, Kevin Knight, and Daniel Marcu. 2006. Cap-
italizing machine translation. In HLT-NAACL, pages
1–8, Morristown, NJ, USA. ACL.
</reference>
<figure confidence="0.997932481481482">
Forward Training Backward Training
SER
0.45
0.40
0.35
0.30
0.25
0.20
1999-01
1999-05
1999-10
2000-02
2000-05
2000-09
2000-12
2001-03
2001-07
2001-10
2002-01
2002-04
2002-08
2002-12
2003-04
2003-08
2003-12
2004-03
2004-06
</figure>
<page confidence="0.871624">
4
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.135561">
<title confidence="0.5968015">Language Dynamics and Capitalization using Maximum Entropy Nuno and Isabel a Spoken Language Systems Laboratory - INESC ID Lisboa</title>
<author confidence="0.283371">R Alves Redol</author>
<affiliation confidence="0.4384795">Instituto de Ciências do Trabalho e da Empresa, Portugal – Instituto Superior Técnico, Portugal.</affiliation>
<email confidence="0.812086">fmmb@l2f.inesc-id.pt</email>
<email confidence="0.812086">njm@l2f.inesc-id.pt</email>
<email confidence="0.812086">imt@l2f.inesc-id.pt</email>
<abstract confidence="0.999284785714286">This paper studies the impact of written language variations and the way it affects the capitalization task over time. A discriminative approach, based on maximum entropy models, is proposed to perform capitalization, taking the language changes into consideration. The proposed method makes it possible to use large corpora for training. The evaluation is performed over newspaper corpora using different testing periods. The achieved results reveal a strong relation between the capitalization performance and the elapsed time between the training and testing data periods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Batista</author>
<author>N J Mamede</author>
<author>D Caseiro</author>
<author>I Trancoso</author>
</authors>
<title>A lightweight on-the-fly capitalization system for automatic speech recognition.</title>
<date>2007</date>
<booktitle>In Proc. of the RANLP</booktitle>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="3043" citStr="Batista et al., 2007" startWordPosition="459" endWordPosition="462"> tag), estimated from a corpus with case information, and then uses dynamic programming to disambiguate over all possible tag assignments on a sentence. Other related work includes a bilingual capitalization model for capitalizing machine translation (MT) outputs, using conditional random fields (CRFs) reported by (Wang et al., 2006). This work exploits case information both from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model. A preparatory study on the capitalization of Portuguese BN has been performed by (Batista et al., 2007). One important aspect related with capitalization concerns the language dynamics: new words are introduced everyday in our vocabularies and the usage of some other words decays with time. Concerning this subject, (Mota, 2008) shows that, as the time gap between training and test data increases, the performance of a named tagger based on co-training (Collins and Singer, 1999) decreases. This paper studies and evaluates the effects of language dynamics in the capitalization of newspaper Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1–4, Columbus, Ohio, USA, June 2008. c�200</context>
<context position="8624" citStr="Batista et al., 2007" startWordPosition="1367" endWordPosition="1370">000 2001 2002 2003 2004 Training period Figure 2: Performance for different training periods. sets, revealing that performance is affected by the time lapse between the training and testing periods. The best results were always produced with nearby the testing data. A similar behavior was observed on the other four testing subsets, corresponding to the last subset of each year. Results also reveal a degradation of performance when the training data is from a time period after the evaluation data. Results from previous experiment are still worse than results achieved by other work on the area (Batista et al., 2007) (about 94% precision and 88% recall), specially in terms of recall. This is caused by a low coverage of the training data, thus revealing that each training set (20 Million words) does not provide sufficient data for the capitalization task. One important problem related with this discriminative approach concerns memory limitations. The memory required increases with the size of the corpus (number of observations), preventing the use of large corpora, such as RecPub for training, with Evaluation Set Prec Rec SER 2004-12 test set 93% 82% 0.233 Table 2: Training with all RecPub training data. C</context>
</contexts>
<marker>Batista, Mamede, Caseiro, Trancoso, 2007</marker>
<rawString>F. Batista, N. J. Mamede, D. Caseiro, and I. Trancoso. 2007. A lightweight on-the-fly capitalization system for automatic speech recognition. In Proc. of the RANLP 2007, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="6420" citStr="Berger et al., 1996" startWordPosition="1002" endWordPosition="1005"> Evaluation results may be influenced when taking such words into account (Kim and Woodland, 2004). The evaluation is performed using the metrics: Precision, Recall and SER (Slot Error Rate) (Makhoul et al., 1999). Only capitalized words (not lowercase) are considered as slots and used by these metrics. For example: Precision is calculated by dividing the number of correct capitalized words by the number of capitalized words in the testing data. The modeling approach here described is discriminative, and is based on maximum entropy (ME) models, firstly applied to natural language problems in (Berger et al., 1996). An ME model estimates the conditional probability of the events given the corresponding features. Therefore, all the information must be expressed in terms of features in a pre-processing step. Experiments here described only use features comprising word unigrams and bigrams: wz (current word), (wz_1, wz) and (wz, wz+1) (bigrams). Only words occurring more than once were included for training, thus reducing the number of misspelled words. All the experiments used the MegaM tool (Daumé III, 2004), which uses conjugate gradient and a limited memory optimization of logistic regression. The foll</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>A Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<pages>04</pages>
<contexts>
<context position="1858" citStr="Chelba and Acero, 2004" startWordPosition="267" endWordPosition="270">gibility of texts is strongly influenced by this information. Different practical applications benefit from automatic capitalization as a preprocessing step: when applied to speech recognition output, which usually consists of raw text, automatic capitalization provides relevant information for automatic content extraction, named entity recognition, and machine translation; many computer applications, such as word processing and e-mail clients, perform automatic capitalization along with spell corrections and grammar check. The capitalization problem can be seen as a sequence tagging problem (Chelba and Acero, 2004; 1 Lita et al., 2003; Kim and Woodland, 2004), where each lower-case word is associated to a tag that describes its capitalization form. (Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. This work uses a Maximum Entropy Markov Model (MEMM) based approach, which allows to combine different features. A large written newspaper corpora is used for training and the test data consists of Broadcast News (BN) data. (Lita et al., 2003) builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with </context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>C. Chelba and A. Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. EMNLP04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conference on EMNLP.</booktitle>
<contexts>
<context position="3421" citStr="Collins and Singer, 1999" startWordPosition="519" endWordPosition="522">oth from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model. A preparatory study on the capitalization of Portuguese BN has been performed by (Batista et al., 2007). One important aspect related with capitalization concerns the language dynamics: new words are introduced everyday in our vocabularies and the usage of some other words decays with time. Concerning this subject, (Mota, 2008) shows that, as the time gap between training and test data increases, the performance of a named tagger based on co-training (Collins and Singer, 1999) decreases. This paper studies and evaluates the effects of language dynamics in the capitalization of newspaper Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1–4, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics corpora. Section 2 describes the corpus and presents a short analysis on the lexicon variation. Section 3 presents experiments concerning the capitalization task, either using isolated training sets or by retraining with different training sets. Section 4 concludes and presents future plans. 2 Newspaper Corpus Experiments here descr</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proc. of the Joint SIGDAT Conference on EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daumé</author>
</authors>
<date>2004</date>
<booktitle>Notes on CG and LM-BFGS optimization of logistic regression.</booktitle>
<marker>Daumé, 2004</marker>
<rawString>H. Daumé III. 2004. Notes on CG and LM-BFGS optimization of logistic regression.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>P C Woodland</author>
</authors>
<title>Automatic capitalisation generation for speech input.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="1904" citStr="Kim and Woodland, 2004" startWordPosition="276" endWordPosition="279">his information. Different practical applications benefit from automatic capitalization as a preprocessing step: when applied to speech recognition output, which usually consists of raw text, automatic capitalization provides relevant information for automatic content extraction, named entity recognition, and machine translation; many computer applications, such as word processing and e-mail clients, perform automatic capitalization along with spell corrections and grammar check. The capitalization problem can be seen as a sequence tagging problem (Chelba and Acero, 2004; 1 Lita et al., 2003; Kim and Woodland, 2004), where each lower-case word is associated to a tag that describes its capitalization form. (Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. This work uses a Maximum Entropy Markov Model (MEMM) based approach, which allows to combine different features. A large written newspaper corpora is used for training and the test data consists of Broadcast News (BN) data. (Lita et al., 2003) builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with case information, and then uses dynamic progra</context>
<context position="5898" citStr="Kim and Woodland, 2004" startWordPosition="918" endWordPosition="921">xplores only three ways of writing a word: lower-case, all-upper, and firstcapitalized, not covering mixed-case words such as “McLaren” and “SuSE”. In fact, mixed-case words are also being treated by means of a small lexicon, but they are not evaluated in the scope of this paper. The following experiments assume that the capitalization of the first word of each sentence is performed in a separated processing stage (after punctuation for instance), since its correct graphical form depends on its position in the sentence. Evaluation results may be influenced when taking such words into account (Kim and Woodland, 2004). The evaluation is performed using the metrics: Precision, Recall and SER (Slot Error Rate) (Makhoul et al., 1999). Only capitalized words (not lowercase) are considered as slots and used by these metrics. For example: Precision is calculated by dividing the number of correct capitalized words by the number of capitalized words in the testing data. The modeling approach here described is discriminative, and is based on maximum entropy (ME) models, firstly applied to natural language problems in (Berger et al., 1996). An ME model estimates the conditional probability of the events given the co</context>
</contexts>
<marker>Kim, Woodland, 2004</marker>
<rawString>J. Kim and P. C. Woodland. 2004. Automatic capitalisation generation for speech input. Computer Speech &amp; Language, 18(1):67–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L V Lita</author>
<author>A Ittycheriah</author>
<author>S Roukos</author>
<author>N Kambhatla</author>
</authors>
<title>tRuEcasIng.</title>
<date>2003</date>
<booktitle>In Proc. of the 41&amp;quot; annual meeting on ACL,</booktitle>
<pages>152--159</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1065" citStr="Lita et al., 2003" startWordPosition="153" endWordPosition="156">anguage variations and the way it affects the capitalization task over time. A discriminative approach, based on maximum entropy models, is proposed to perform capitalization, taking the language changes into consideration. The proposed method makes it possible to use large corpora for training. The evaluation is performed over newspaper corpora using different testing periods. The achieved results reveal a strong relation between the capitalization performance and the elapsed time between the training and testing data periods. 1 Introduction The capitalization task, also known as truecasing (Lita et al., 2003), consists of rewriting each word of an input text with its proper case information. The capitalization of a word sometimes depends on its current context, and the intelligibility of texts is strongly influenced by this information. Different practical applications benefit from automatic capitalization as a preprocessing step: when applied to speech recognition output, which usually consists of raw text, automatic capitalization provides relevant information for automatic content extraction, named entity recognition, and machine translation; many computer applications, such as word processing </context>
<context position="2367" citStr="Lita et al., 2003" startWordPosition="355" endWordPosition="358">d grammar check. The capitalization problem can be seen as a sequence tagging problem (Chelba and Acero, 2004; 1 Lita et al., 2003; Kim and Woodland, 2004), where each lower-case word is associated to a tag that describes its capitalization form. (Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. This work uses a Maximum Entropy Markov Model (MEMM) based approach, which allows to combine different features. A large written newspaper corpora is used for training and the test data consists of Broadcast News (BN) data. (Lita et al., 2003) builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with case information, and then uses dynamic programming to disambiguate over all possible tag assignments on a sentence. Other related work includes a bilingual capitalization model for capitalizing machine translation (MT) outputs, using conditional random fields (CRFs) reported by (Wang et al., 2006). This work exploits case information both from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model. A preparatory study on the </context>
</contexts>
<marker>Lita, Ittycheriah, Roukos, Kambhatla, 2003</marker>
<rawString>L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 2003. tRuEcasIng. In Proc. of the 41&amp;quot; annual meeting on ACL, pages 152–159, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Makhoul</author>
<author>F Kubala</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Performance measures for information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop,</booktitle>
<location>Herndon, VA,</location>
<contexts>
<context position="6013" citStr="Makhoul et al., 1999" startWordPosition="937" endWordPosition="940"> such as “McLaren” and “SuSE”. In fact, mixed-case words are also being treated by means of a small lexicon, but they are not evaluated in the scope of this paper. The following experiments assume that the capitalization of the first word of each sentence is performed in a separated processing stage (after punctuation for instance), since its correct graphical form depends on its position in the sentence. Evaluation results may be influenced when taking such words into account (Kim and Woodland, 2004). The evaluation is performed using the metrics: Precision, Recall and SER (Slot Error Rate) (Makhoul et al., 1999). Only capitalized words (not lowercase) are considered as slots and used by these metrics. For example: Precision is calculated by dividing the number of correct capitalized words by the number of capitalized words in the testing data. The modeling approach here described is discriminative, and is based on maximum entropy (ME) models, firstly applied to natural language problems in (Berger et al., 1996). An ME model estimates the conditional probability of the events given the corresponding features. Therefore, all the information must be expressed in terms of features in a pre-processing ste</context>
</contexts>
<marker>Makhoul, Kubala, Schwartz, Weischedel, 1999</marker>
<rawString>J. Makhoul, F. Kubala, R. Schwartz, and R. Weischedel. 1999. Performance measures for information extraction. In Proceedings of the DARPA Broadcast News Workshop, Herndon, VA, Feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Martins</author>
<author>A Teixeira</author>
<author>J P Neto</author>
</authors>
<title>Dynamic language modeling for a daily broadcast news transcription system.</title>
<date>2007</date>
<booktitle>In ASRU</booktitle>
<marker>Martins, Teixeira, Neto, 2007</marker>
<rawString>C. Martins, A. Teixeira, and J. P. Neto. 2007. Dynamic language modeling for a daily broadcast news transcription system. In ASRU 2007, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Mota</author>
</authors>
<title>How to keep up with language dynamics? A case study on Named Entity Recognition.</title>
<date>2008</date>
<tech>Ph.D. thesis, IST / UTL.</tech>
<contexts>
<context position="3269" citStr="Mota, 2008" startWordPosition="495" endWordPosition="496">ne translation (MT) outputs, using conditional random fields (CRFs) reported by (Wang et al., 2006). This work exploits case information both from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model. A preparatory study on the capitalization of Portuguese BN has been performed by (Batista et al., 2007). One important aspect related with capitalization concerns the language dynamics: new words are introduced everyday in our vocabularies and the usage of some other words decays with time. Concerning this subject, (Mota, 2008) shows that, as the time gap between training and test data increases, the performance of a named tagger based on co-training (Collins and Singer, 1999) decreases. This paper studies and evaluates the effects of language dynamics in the capitalization of newspaper Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1–4, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics corpora. Section 2 describes the corpus and presents a short analysis on the lexicon variation. Section 3 presents experiments concerning the capitalization task, either using isolat</context>
</contexts>
<marker>Mota, 2008</marker>
<rawString>Cristina Mota. 2008. How to keep up with language dynamics? A case study on Named Entity Recognition. Ph.D. thesis, IST / UTL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Capitalizing machine translation.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>1--8</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2757" citStr="Wang et al., 2006" startWordPosition="413" endWordPosition="416">a Maximum Entropy Markov Model (MEMM) based approach, which allows to combine different features. A large written newspaper corpora is used for training and the test data consists of Broadcast News (BN) data. (Lita et al., 2003) builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with case information, and then uses dynamic programming to disambiguate over all possible tag assignments on a sentence. Other related work includes a bilingual capitalization model for capitalizing machine translation (MT) outputs, using conditional random fields (CRFs) reported by (Wang et al., 2006). This work exploits case information both from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model. A preparatory study on the capitalization of Portuguese BN has been performed by (Batista et al., 2007). One important aspect related with capitalization concerns the language dynamics: new words are introduced everyday in our vocabularies and the usage of some other words decays with time. Concerning this subject, (Mota, 2008) shows that, as the time gap between training and test data increases, the performance o</context>
</contexts>
<marker>Wang, Knight, Marcu, 2006</marker>
<rawString>Wei Wang, Kevin Knight, and Daniel Marcu. 2006. Capitalizing machine translation. In HLT-NAACL, pages 1–8, Morristown, NJ, USA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>