<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.995721">
Solving the “Who’s Mark Johnson” Puzzle:
Information Extraction Based Cross Document Coreference
</title>
<author confidence="0.969431">
Jian Huang†* Sarah M. Taylor$ Jonathan L. Smith$ Konstantinos A. Fotiadis§ C. Lee Giles††Information Sciences and Technology
</author>
<affiliation confidence="0.9820425">
Pennsylvania State University, University Park, PA 16802, USA
$§Advanced Technology Office, Lockheed Martin IS&amp;GS
</affiliation>
<address confidence="0.6752305">
$4350 N. Fairfax Drive, Suite 470, Arlington, VA 22203, USA
§230 Mall Blvd, King of Prussia, PA 19406, USA
</address>
<sectionHeader confidence="0.969729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.986046611111111">
Cross Document Coreference (CDC) is the
problem of resolving the underlying identity
of entities across multiple documents and is a
major step for document understanding.
We develop a framework to efficiently
determine the identity of a person based on
extracted information, which includes unary
properties such as gender and title, as well as
binary relationships with other named entities
such as co-occurrence and geo-locations.
At the heart of our approach is a suite of
similarity functions (specialists) for matching
relationships and a relational density-based
clustering algorithm that delineates name
clusters based on pairwise similarity. We
demonstrate the effectiveness of our methods
on the WePS benchmark datasets and point
out future research directions.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997078">
The explosive growth of web data offers users both
the opportunity and the challenge to discover and
integrate information from disparate sources. As
alluded to in the title, a search query of the common
name “Mark Johnson” refers to as many as 70
namesakes in the top 100 search results from the
Yahoo! search engine, only one of whom is the
Brown University professor and co-author of an
ACL 2006 paper (see experiments). Cross document
coreference (CDC) (Bagga and Baldwin, 1998) is a
distinct technology that consolidates named entities
across documents according to their real referents.
Despite the variety of styles and content in different
text, CDC can break the boundaries of documents
and cluster those mentions referring to the same
</bodyText>
<email confidence="0.469993">
&amp;quot;Contactauthor: jhuang@ist.psu.edu
</email>
<page confidence="0.983567">
7
</page>
<bodyText confidence="0.998166727272727">
Mark Johnson. As unambiguous person references
are key to many tasks, e.g. social network analysis,
this work focuses on person named entities. The
method can be later extended to organizations.
We highlight the key differences between our
proposed CDC system with past person name
search systems. First, we seek to transcend the
simple bag of words approaches in earlier CDC
work by leveraging state-of-the-art information
extraction (IE) tools for disambiguation. The
main advantage is that our IE based approach has
access to accurate information such as a person’s
work titles, geo-locations, relationships and other
attributes. Traditional IR approaches, on the other
hand, may naively use the terms in a document
which can significantly hamper accuracy. For
instance, an article about Hillary Clinton may
contain references to journalists, politicians who
make comments about her. Even with careful word
selection, such textual features may still confuse the
disambiguation system about the true identity of the
person. The information extraction process in our
work can thus be regarded as an intelligent feature
selection step for disambiguation. Second, after
coreferencing, our system not only yields clusters
of documents, but also structured information
which is highly useful for automated document
understanding and data mining.
We review related work on CDC next and
describe our approach in Section 3. The methods
are evaluated on benchmark datasets in Section 4.
We discuss directions for future improvement in
Section 5 and conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.99962" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9845275">
There is a long tradition of work on the within
document coreference (WDC) problem in NLP,
</bodyText>
<note confidence="0.673038">
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7–12,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999951862068965">
which links named entities with the same referent
within a document into a WDC chain. State-of-
the-art WDC systems, e.g. (Ng and Cardie, 2001),
leverage rich lexical features and use supervised
and unsupervised machine learning methods.
Research on cross document coreference began
more recently. (Bagga and Baldwin, 1998) proposed
a CDC system to merge the WDC chains using the
Vector Space Model on the summary sentences.
(Gooi and Allan, 2004) simplified this approach by
eliminating the WDC module without significant
deterioration in performance. Clustering approaches
(e.g. hierarchical agglomerative clustering (Mann
and Yarowsky, 2003)) have been commonly used
for CDC due to the variety of data distributions
of different names. Our work goes beyond the
simple co-occurrence features (Bagga and Baldwin,
1998) and the limited extracted information (e.g.
biographical information in (Mann and Yarowsky,
2003) that is relatively scarce in web data) using
the broad range of relational information with the
support of information extraction tools. There
are also other related research problems. (Li et
al., 2004) solved the robust reading problem by
adopting a probabilistic view on how documents are
generated and how names are sprinkled into them.
Our previous work (Huang et al., 2006) resolved
the author name ambiguity problem based on the
metadata records extracted from academic papers.
</bodyText>
<sectionHeader confidence="0.998535" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999823916666667">
The overall framework of our CDC system works
as follows. Given a document, the information
extraction tool first extracts named entities and
constructs WDC chains. It also creates linkages
(relationships) between entities. The similarity
between a pair of relationships in WDC chains
is measured by an awakened similarity specialist
and the similarity between two WDC chains is
determined by the mixture of awakened specialists’
predictions. Finally, a density-based clustering
method generates clusters corresponding to real
world entities. We describe these steps in detail.
</bodyText>
<subsectionHeader confidence="0.991374">
3.1 Entity and Relationship Extraction
</subsectionHeader>
<bodyText confidence="0.999980578947368">
Given a document, an information extraction
tool is first used to extract named entities and
perform within document coreference. Hence,
named entities in each document are divided into
a set of WDC chains, each chain corresponding
to one real world entity. In addition, state-of-
the-art IE tools are capable of creating relational
information between named entities. We use an
IE tool AeroText1 (Taylor, 2004) for this purpose.
Besides the attribute information about the person
named entity (first/middle/last names, gender,
mention, etc), AeroText also extracts relationship
information between named entities, such as
Family, List, Employment, Ownership, Citizen-
Resident-Religion-Ethnicity, etc, as specified in the
Automatic Content Extraction (ACE) evaluation.
The input to the CDC system is a set of WDC chains
(with relationship information stored in them) and
the CDC task is to merge these WDC chains2.
</bodyText>
<subsectionHeader confidence="0.99942">
3.2 Similarity Features
</subsectionHeader>
<bodyText confidence="0.999897454545455">
We design a suite of similarity functions to
determine whether the relationships in a pair of
WDC chains match, divided into three groups:
Text similarity. To decide whether two names
in the co-occurrence or family relationship match,
we use SoftTFIDF (Cohen et al., 2003), which has
shown best performance among various similarity
schemes tested for name matching. SoftTFIDF is
a hybrid matching scheme that combines the token-
based TFIDF with the Jaro-Winkler string distance
metric. This permits inexact matching of named
entities due to name variations, spelling errors, etc.
Semantic similarity. Text or syntactic similarity is
not always sufficient for matching relationships. For
instance, although the mentions “U.S. President”
and “Commander-in-chief” have no textual overlap,
they are semantically highly related as they can be
synonyms. We use WordNet and the information
theory based JC semantic distance (Jiang and
Conrath, 1997) to measure the semantic similarity
between concepts in relationships such as mention,
employment, ownership and so on.
</bodyText>
<footnote confidence="0.539953625">
1AeroText is a text mining application for content
analysis, with main focus on information extraction
including entity extraction and intrasource link analysis
(seehttp://en.wikipedia.org/wiki/AeroText).
2We make no distinctions whether WDC chains are
extracted from the same document. Indeed, the CDC system
can correct the WDC errors due to lack of information for
merging named entities within a document.
</footnote>
<page confidence="0.993042">
8
</page>
<bodyText confidence="0.998957625">
Other rule-based similarity. Several other
cases require special treatment. For example, the
employment relationships of Senator and D-N.Y.
should match based on domain knowledge. Also,
we design rule-based similarity functions to handle
nicknames (Bill and William), acronyms (COLING
for International Conference on Computational
Linguistics), and geographical locations3.
</bodyText>
<subsectionHeader confidence="0.999914">
3.3 Learning a Similarity Matrix
</subsectionHeader>
<bodyText confidence="0.999649264705882">
After the similarity features between a pair of
WDC chains are computed, we need to compute
the pairwise distance metric for clustering. (Cohen
et al., 2003) trained a binary SVM model and
interpreted its confidence in predicting the negative
class as the distance metric. In our case of using
information extraction results for disambiguation,
however, only some of the similarity features are
present based on the availability of relationships
in two WDC chains. Therefore, we treat each
similarity function as a subordinate predicting
algorithm (called specialist) and utilize the
specialist learning framework (Freund et al., 1997)
to combine the predictions. Here, a specialist is
awake only when the same relationships are present
in two WDC chains. Also, a specialist can refrain
from making a prediction for an instance if it is
not confident enough. In addition to the similarity
scores, specialists have different weights, e.g. a
match in a family relationship is considered more
important than in a co-occurrence relationship.
The Specialist Exponentiated Gradient (SEG)
(Freund et al., 1997) algorithm is adopted to learn
to mix the specialists’ prediction. Given a set
of T training instances {xt} (xt,i denotes the
i-th specialist’s prediction), the SEG algorithm
minimizes the square loss of the outcome y� in an
online manner (Algorithm 1). In each learning
iteration, SEG first predict yt using the set of awake
experts Et with respect to instance xt. The true
outcome yt (1 for coreference and 0 otherwise) is
then revealed and square loss L is incurred. SEG
then updates the weight distribution p accordingly.
To sum up, the similarity between a pair of
</bodyText>
<footnote confidence="0.99490525">
3Though a rich set of similarity features has been built for
matching the relationships, they may not encompass all possible
cases in real world documents. The goal of this work, however,
is to focus on the algorithms instead of knowledge engineering.
</footnote>
<figure confidence="0.320102133333333">
Algorithm 1 SEG (Freund et al., 1997)
Input: Initial weight distribution p1;
learning rate q &gt; 0; training set {xt}
1: for t=1 to T do
2: Predict using:
yt = Ei∈Et pit
xt,i (1)
A Ei∈Et pi
3: Observe true label yt and incur square loss
L(yt, yt) = (yt − yt)2
4: Update weight distribution: for i E Et
pt+1 Ej∈Et ptj
i = pt −2ηxt,i(�yt−yt) Pt e-2ηxt,i(yt−yt)
ie Ej∈Et
pt+1
</figure>
<bodyText confidence="0.360427">
i = pit, otherwise
</bodyText>
<listItem confidence="0.5418375">
5: end for
Output: Model p
</listItem>
<bodyText confidence="0.9983228">
WDC chains wi and wj can be represented in a
similarity matrix R, with ri,j computed by the SEG
prediction step using the learned weight distribution
p (Equation 1). A relational clustering algorithm
then clusters entities using R, as we introduce next.
</bodyText>
<subsectionHeader confidence="0.879161">
3.4 Relational Clustering
</subsectionHeader>
<bodyText confidence="0.999851380952381">
The set of WDC chains to be clustered are
represented by a relational similarity matrix. Most
of the work in clustering, however, is only capable
of clustering numerical object data (e.g. K-means).
Relational clustering algorithms, on the other hand,
cluster objects based on the less direct measurement
of similarity between object pairs. We choose to
use a density based clustering algorithm DBSCAN
(Ester et al., 1996) mainly for two reasons.
First, most clustering algorithm require the
number of clusters K as an input parameter. The
optimal K can apparently vary greatly for names
with different frequency and thus is a sensitive
parameter. Even if a cluster validity index is used
to determine K, it usually requires running the
underlying clustering algorithm multiple times
and hence is inefficient for large scale data.
DBSCAN, as a density based clustering method,
only requires density parameters such as the
radius of the neighborhood 2 that are universal for
different datasets. As we show in the experiment,
</bodyText>
<page confidence="0.989361">
9
</page>
<bodyText confidence="0.999842761904762">
density parameters are relatively insensitive for
disambiguation performance.
Second, the distance metric in relational space
may be non-Euclidean, rendering many clustering
algorithms ineffective (e.g. single linkage clustering
algorithm is known to generate chain-shaped
clusters). Density-based clustering, on the other
hand, can generate clusters of arbitrary shapes since
only objects in dense areas are placed in a cluster.
DBSCAN induces a density-based cluster by
the core objects, i.e. objects having more than
a specified number of other data objects in their
neighborhood of size E. In each clustering step, a
seed object is checked to determine whether it’s a
core object and if so, it induces other points of the
same cluster using breadth first search (otherwise
it’s considered as a noise point). In interest of
space, we refer readers to (Ester et al., 1996) for
algorithmic details of DBSCAN and now turn
our attention to evaluating the disambiguation
performance of our methods.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999603333333333">
We first formally define the evaluation metrics,
followed by the introduction to the benchmark test
sets and the system’s performance.
</bodyText>
<subsectionHeader confidence="0.98841">
4.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.998501">
We evaluate the performance of our method using
the standard purity and inverse purity clustering
metrics. Let a set of clusters C = {C1, ..., Cs}
denote the system’s output and a set of categories
</bodyText>
<equation confidence="0.856396833333333">
D = {D1, ..., Dt} be the gold standard. Both C and
D are partitions of the WDC chains {w1, ..., wn}
(n = &amp; |Ci |= Ej |Dj|). First, the precision of
a cluster Ci wrt. a category Dj is defined as,
Precision(Ci, Dj) = |Ci n Dj|
|Ci|
</equation>
<bodyText confidence="0.996274666666667">
Purity is defined as the weighted average of the
maximum precision achieved by the clusters on one
of the categories,
</bodyText>
<sectionHeader confidence="0.535456" genericHeader="method">
Precision(Ci, Dj)
</sectionHeader>
<bodyText confidence="0.997708733333333">
Hence purity penalizes putting noise WDC chains in
a cluster. Trivially, the maximum purity (i.e. 1) can
be achieved by making one cluster per WDC chain
(referred to as the one-in-one baseline).
Reversing the role of clusters and categories,
Inverse purity(C, D) def= Purity(D, C). Inverse
Purity penalizes splitting WDC chains belonging
to the same category into different clusters. The
maximum inverse purity can be achieved by putting
all chain in one cluster (all-in-one baseline).
Purity and inverse purity are similar to the
precision and recall measures commonly used
in information retrieval. There is a tradeoff
relationship between the two and their harmonic
mean F0.5 is used for performance evaluation.
</bodyText>
<subsectionHeader confidence="0.905502">
4.2 Datasets
</subsectionHeader>
<bodyText confidence="0.999984678571429">
We evaluate our methods using the benchmark
test collection from the ACL SemEval-2007 web
person search task (WePS hereafter) (Artiles et al.,
2007). The test collection consists of three sets of
documents for 10 different names, sampled from
the English Wikipedia (famous people), participants
of the ACL 2006 conference (computer scientists)
and common names from the US Census data,
respectively. For each ambiguous name, the top 100
documents retrieved from the Yahoo! Search API
were annotated by human annotators according to
the actual entity of the name. This yields on average
45 different real world entities per set and about 3k
documents in total.
We note that the annotation in WePS makes the
simplifying assumption that each document refers to
only one real world person among the namesakes
in question. The CDC task in the perspective of
this paper, however, is to merge the WDC chains
rather than documents. Hence in our evaluation,
we adopt the document label to annotate the WDC
chain from the document that corresponds to the
person name search query. Despite the difference,
the results of the one-in-one and all-in-one baselines
are almost identical to those reported in the WePS
evaluation (F0.5 = 0.61, 0.40 respectively). Hence
the performance reported here is comparable to the
official evaluation results (Artiles et al., 2007).
</bodyText>
<subsectionHeader confidence="0.999801">
4.3 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999892333333333">
We computed the similarity features from the WDC
chains extracted from the WePS training data and
subsampled the non-coreferent pairs to generate a
</bodyText>
<equation confidence="0.992343666666667">
Purity(C, D) =
s |Ci |max
i=1 n j
</equation>
<page confidence="0.998248">
10
</page>
<tableCaption confidence="0.9080005">
Table 1: Cross document coreference performance
(macro-averaged scores, I-Pur denotes inverse purity).
</tableCaption>
<table confidence="0.999452">
Test set Method Purity I-Pur F0.5
Wikipedia AT-CDC 0.684 0.725 0.687
ACL-06 AT-CDC 0.792 0.657 0.712
US Census AT-CDC 0.772 0.700 0.722
AT-CDC 0.749 0.695 0.708
F05
Global
One-in-one 1.000 0.482 0.618
Average
All-in-one 0.279 1.000 0.389
</table>
<bodyText confidence="0.989050684210526">
training set of around 32k pairwise instances. We
then used the SEG algorithm to learn the weight
distribution model. The macro-averaged cross
document coreference results on the WePS test
sets are reported in Table 1. The F0.5 score of our
CDC system (AT-CDC) is 0.708, comparable to the
test results of the first tier systems in the official
evaluation. The two baselines are also included.
Because the test set is very ambiguous (on average
only two documents per real world entity), the
one-in-one baseline has relatively high F0.5 score.
The Wikipedia, ACL06 and US Census sets
have on average 56, 31 and 50 entities per name
respectively. We notice that as the data set becomes
more ambiguous, purity decreases implying
it’s harder for the system to discard irrelevant
documents from a cluster. The other case is true
for inverse purity. In particular, we are interested in
how the coreference performance changes with the
number of entities per name (which can be viewed
as the ambiguity level of a data set). This is shown
in Figure 1. We observe that in general the harmonic
mean of the purity is fairly stable across different
number of entities per dataset (generally within
the band between 0.6 and 0.8). This is important
because the system’s performance does not vary
greatly with the underlying data characteristics.
There is a particular name (with only one underlying
referent) that appears to be an outlier in performance
in Figure 1. After examining the extraction results,
we notice that the extracted relationships refer to
the same person’s employment, coauthors and geo-
locations. The generated CDC clusters correctly
reflect the different aspects of the person but the
system is unable to link them together due to the
lack of information for merging. This motivates us
to further improve performance in future work.
Figure 2 shows how the coreference performance
</bodyText>
<figure confidence="0.983806">
0 10 20 30 40 50 60 70 80 90 100
Number of entities per name
</figure>
<figureCaption confidence="0.9984515">
Figure 1: Coreference performance for names with
different number of real world entities.
</figureCaption>
<bodyText confidence="0.9997454">
changes with different density parameter c. We
observe that as we increase the size of the c
neighborhood, inverse purity increases indicating
that more correct coreference decisions are made.
On the other hand, purity decreases as more noise
WDC chains appear in clusters. Due to this tradeoff
relationship, the F score is fairly stable with a wide
range of c values and hence the density parameter is
rather insensitive (compared to, say, the number of
clusters K).
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.984306125">
We see several opportunities to improve the
coreference performance of the proposed methods.
First, though the system’s performance compares
favorably with the WePS submissions, we observe
that purity is higher than inverse purity, indicating
that the system finds it more difficult to link
coreferent documents than to discard noise from
DBSCAN with different parameter settings
</bodyText>
<figureCaption confidence="0.997718">
Figure 2: Coreference performance with different c.
</figureCaption>
<figure confidence="0.987587266666667">
Purity
Inverse Purity
F(0.5)
1
0.8
0.6
0.4
0.2
0
1
0.8
0.6
0.4
0.2
0
</figure>
<page confidence="0.995395">
11
</page>
<bodyText confidence="0.92598005">
clusters. Thus coreferencing based solely on the
information generated by an information extraction
tool may not always be sufficient. For one, it
remains a huge challenge to develop a general
purpose information extraction tool capable of
applying to web documents with widely different
formats, styles, content, etc. Also, even if the
extraction results are perfect, relationships extracted
from different documents may be of different
types (family memberships vs. geo-locations) and
cannot be directly matched against one another. We
are exploring several methods to complement the
extracted relationships using other information:
• Context-aided CDC. The context where an named
entity is extracted can be leveraged for coreference.
The bag of words in the context tend to be less noisy
than that from the entire document. Moreover, we
can use noun phrase chunkers to extract base noun
phrases from the context. These word or phrase level
features can serve as a safenet when the IE tool fails.
• Topic-based CDC. Similar to (Li et al., 2004),
document topics can be used to ameliorate the
sparsity problem. For example, the topics Sport
and Education are important cues for differentiating
mentions of “Michael Jordan”, which may refer to a
basketball player, a computer science professor, etc.
Second, as noted in the top WePS run (Chen and
Martin, 2007), feature development is important in
achieving good coreference performance. We aim
to improve the set of similarity specialists in our
system by leveraging large knowledge bases.
Moreover, although the CDC system is developed
in the web person search context, the methods are
also applicable to other scenarios. For instance,
there is tremendous interest in building structured
databases from unstructured text such as enterprise
documents and news articles for data mining, where
CDC is a key step for “understanding” documents
from disparate sources. We plan to continue our
investigations along these lines.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999934846153846">
We have presented and implemented an information
extraction based Cross Document Coreference
(CDC) system that employs supervised and
unsupervised learning methods. We evaluated
the proposed methods with experiments on a
large benchmark disambiguation collection, which
demonstrate that the proposed methods compare
favorably with the top runs in the SemEval
evaluation. We believe that by incorporating
information such as context and topic, besides the
extracted relationships, the performance of the CDC
can be further improved. We have outlined research
plans to address this and several other issues.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856142857143">
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The SemEval-2007 WePS evaluation: Establishing a
benchmark for the web people search task. In Proc 4th
Int’l Workshop on Semantic Evaluations (SemEval).
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proc. of 36th ACL and 17th COLING.
Ying Chen and James Martin. 2007. Towards robust
unsupervised personal name disambiguation. In
Proceedings of EMNLP and CoNLL, pages 190–198.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A comparison of string distance
metrics for name-matching tasks. In Proc. of IJCAI
Workshop on Information Integration on the Web.
Martin Ester, Hans-Peter Kriegel, Jorg Sander, and
Xiaowei Xu. 1996. A density-based algorithm for
discovering clusters in large spatial databases with
noise. In Proceedings of 2nd KDD, pages 226 – 231.
Yoav Freund, Robert E. Schapire, Yoram Singer, and
Manfred K. Warmuth. 1997. Using and combining
predictors that specialize. In Proceedings of 29th ACM
symposium on Theory of computing (STOC).
Chung H. Gooi and James Allan. 2004. Cross-document
coreference on a large scale corpus. In HLT-NAACL.
Jian Huang, Seyda Ertekin, and C. Lee Giles.
2006. Efficient name disambiguation for large scale
databases. In Proc. of 10th PKDD, pages 536 – 544.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical
taxonomy. In Proceedings of ROCLING X.
Xin Li, Paul Morie, and Dan Roth. 2004. Robust
reading: Identification and tracing of ambiguous
names. In Proceedings of HLT-NAACL, pages 17–24.
Gideon S. Mann and David Yarowsky. 2003.
Unsupervised personal name disambiguation. In
Proceedings of HLT-NAACL, pages 33–40.
Vincent Ng and Claire Cardie. 2001. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40th ACL, pages 104–111.
Sarah M. Taylor. 2004. Information extraction
tools: Deciphering human language. IT Professional,
6(6):28 – 34.
</reference>
<page confidence="0.998453">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335239">
<title confidence="0.9953725">Solving the “Who’s Mark Johnson” Information Extraction Based Cross Document Coreference</title>
<author confidence="0.994045">Sarah M L A Lee Sciences</author>
<affiliation confidence="0.5651635">Pennsylvania State University, University Park, PA 16802, Technology Office, Lockheed Martin</affiliation>
<address confidence="0.974534">N. Fairfax Drive, Suite 470, Arlington, VA 22203, Mall Blvd, King of Prussia, PA 19406, USA</address>
<abstract confidence="0.999765789473684">Cross Document Coreference (CDC) is the problem of resolving the underlying identity of entities across multiple documents and is a major step for document understanding. We develop a framework to efficiently determine the identity of a person based on extracted information, which includes unary properties such as gender and title, as well as binary relationships with other named entities such as co-occurrence and geo-locations. At the heart of our approach is a suite of similarity functions (specialists) for matching relationships and a relational density-based clustering algorithm that delineates name clusters based on pairwise similarity. We demonstrate the effectiveness of our methods on the WePS benchmark datasets and point out future research directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>The SemEval-2007 WePS evaluation: Establishing a benchmark for the web people search task.</title>
<date>2007</date>
<booktitle>In Proc 4th Int’l Workshop on Semantic Evaluations (SemEval).</booktitle>
<contexts>
<context position="14962" citStr="Artiles et al., 2007" startWordPosition="2297" endWordPosition="2300">y(C, D) def= Purity(D, C). Inverse Purity penalizes splitting WDC chains belonging to the same category into different clusters. The maximum inverse purity can be achieved by putting all chain in one cluster (all-in-one baseline). Purity and inverse purity are similar to the precision and recall measures commonly used in information retrieval. There is a tradeoff relationship between the two and their harmonic mean F0.5 is used for performance evaluation. 4.2 Datasets We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (Artiles et al., 2007). The test collection consists of three sets of documents for 10 different names, sampled from the English Wikipedia (famous people), participants of the ACL 2006 conference (computer scientists) and common names from the US Census data, respectively. For each ambiguous name, the top 100 documents retrieved from the Yahoo! Search API were annotated by human annotators according to the actual entity of the name. This yields on average 45 different real world entities per set and about 3k documents in total. We note that the annotation in WePS makes the simplifying assumption that each document </context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007. The SemEval-2007 WePS evaluation: Establishing a benchmark for the web people search task. In Proc 4th Int’l Workshop on Semantic Evaluations (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Entity-based cross-document coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proc. of 36th ACL and 17th COLING.</booktitle>
<contexts>
<context position="1720" citStr="Bagga and Baldwin, 1998" startWordPosition="254" endWordPosition="257">rity. We demonstrate the effectiveness of our methods on the WePS benchmark datasets and point out future research directions. 1 Introduction The explosive growth of web data offers users both the opportunity and the challenge to discover and integrate information from disparate sources. As alluded to in the title, a search query of the common name “Mark Johnson” refers to as many as 70 namesakes in the top 100 search results from the Yahoo! search engine, only one of whom is the Brown University professor and co-author of an ACL 2006 paper (see experiments). Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities across documents according to their real referents. Despite the variety of styles and content in different text, CDC can break the boundaries of documents and cluster those mentions referring to the same &amp;quot;Contactauthor: jhuang@ist.psu.edu 7 Mark Johnson. As unambiguous person references are key to many tasks, e.g. social network analysis, this work focuses on person named entities. The method can be later extended to organizations. We highlight the key differences between our proposed CDC system with past person name search systems. Fi</context>
<context position="4183" citStr="Bagga and Baldwin, 1998" startWordPosition="623" endWordPosition="626">nd conclude in Section 6. 2 Related Work There is a long tradition of work on the within document coreference (WDC) problem in NLP, Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7–12, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics which links named entities with the same referent within a document into a WDC chain. State-ofthe-art WDC systems, e.g. (Ng and Cardie, 2001), leverage rich lexical features and use supervised and unsupervised machine learning methods. Research on cross document coreference began more recently. (Bagga and Baldwin, 1998) proposed a CDC system to merge the WDC chains using the Vector Space Model on the summary sentences. (Gooi and Allan, 2004) simplified this approach by eliminating the WDC module without significant deterioration in performance. Clustering approaches (e.g. hierarchical agglomerative clustering (Mann and Yarowsky, 2003)) have been commonly used for CDC due to the variety of data distributions of different names. Our work goes beyond the simple co-occurrence features (Bagga and Baldwin, 1998) and the limited extracted information (e.g. biographical information in (Mann and Yarowsky, 2003) that </context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Entity-based cross-document coreferencing using the vector space model. In Proc. of 36th ACL and 17th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Chen</author>
<author>James Martin</author>
</authors>
<title>Towards robust unsupervised personal name disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP and CoNLL,</booktitle>
<pages>190--198</pages>
<contexts>
<context position="21102" citStr="Chen and Martin, 2007" startWordPosition="3286" endWordPosition="3289">he bag of words in the context tend to be less noisy than that from the entire document. Moreover, we can use noun phrase chunkers to extract base noun phrases from the context. These word or phrase level features can serve as a safenet when the IE tool fails. • Topic-based CDC. Similar to (Li et al., 2004), document topics can be used to ameliorate the sparsity problem. For example, the topics Sport and Education are important cues for differentiating mentions of “Michael Jordan”, which may refer to a basketball player, a computer science professor, etc. Second, as noted in the top WePS run (Chen and Martin, 2007), feature development is important in achieving good coreference performance. We aim to improve the set of similarity specialists in our system by leveraging large knowledge bases. Moreover, although the CDC system is developed in the web person search context, the methods are also applicable to other scenarios. For instance, there is tremendous interest in building structured databases from unstructured text such as enterprise documents and news articles for data mining, where CDC is a key step for “understanding” documents from disparate sources. We plan to continue our investigations along </context>
</contexts>
<marker>Chen, Martin, 2007</marker>
<rawString>Ying Chen and James Martin. 2007. Towards robust unsupervised personal name disambiguation. In Proceedings of EMNLP and CoNLL, pages 190–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Pradeep Ravikumar</author>
<author>Stephen E Fienberg</author>
</authors>
<title>A comparison of string distance metrics for name-matching tasks.</title>
<date>2003</date>
<booktitle>In Proc. of IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="7097" citStr="Cohen et al., 2003" startWordPosition="1057" endWordPosition="1060">between named entities, such as Family, List, Employment, Ownership, CitizenResident-Religion-Ethnicity, etc, as specified in the Automatic Content Extraction (ACE) evaluation. The input to the CDC system is a set of WDC chains (with relationship information stored in them) and the CDC task is to merge these WDC chains2. 3.2 Similarity Features We design a suite of similarity functions to determine whether the relationships in a pair of WDC chains match, divided into three groups: Text similarity. To decide whether two names in the co-occurrence or family relationship match, we use SoftTFIDF (Cohen et al., 2003), which has shown best performance among various similarity schemes tested for name matching. SoftTFIDF is a hybrid matching scheme that combines the tokenbased TFIDF with the Jaro-Winkler string distance metric. This permits inexact matching of named entities due to name variations, spelling errors, etc. Semantic similarity. Text or syntactic similarity is not always sufficient for matching relationships. For instance, although the mentions “U.S. President” and “Commander-in-chief” have no textual overlap, they are semantically highly related as they can be synonyms. We use WordNet and the in</context>
<context position="8862" citStr="Cohen et al., 2003" startWordPosition="1306" endWordPosition="1309">mation for merging named entities within a document. 8 Other rule-based similarity. Several other cases require special treatment. For example, the employment relationships of Senator and D-N.Y. should match based on domain knowledge. Also, we design rule-based similarity functions to handle nicknames (Bill and William), acronyms (COLING for International Conference on Computational Linguistics), and geographical locations3. 3.3 Learning a Similarity Matrix After the similarity features between a pair of WDC chains are computed, we need to compute the pairwise distance metric for clustering. (Cohen et al., 2003) trained a binary SVM model and interpreted its confidence in predicting the negative class as the distance metric. In our case of using information extraction results for disambiguation, however, only some of the similarity features are present based on the availability of relationships in two WDC chains. Therefore, we treat each similarity function as a subordinate predicting algorithm (called specialist) and utilize the specialist learning framework (Freund et al., 1997) to combine the predictions. Here, a specialist is awake only when the same relationships are present in two WDC chains. A</context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>William W. Cohen, Pradeep Ravikumar, and Stephen E. Fienberg. 2003. A comparison of string distance metrics for name-matching tasks. In Proc. of IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Ester</author>
<author>Hans-Peter Kriegel</author>
<author>Jorg Sander</author>
<author>Xiaowei Xu</author>
</authors>
<title>A density-based algorithm for discovering clusters in large spatial databases with noise.</title>
<date>1996</date>
<booktitle>In Proceedings of 2nd KDD,</booktitle>
<pages>226--231</pages>
<contexts>
<context position="11748" citStr="Ester et al., 1996" startWordPosition="1781" endWordPosition="1784">ed by the SEG prediction step using the learned weight distribution p (Equation 1). A relational clustering algorithm then clusters entities using R, as we introduce next. 3.4 Relational Clustering The set of WDC chains to be clustered are represented by a relational similarity matrix. Most of the work in clustering, however, is only capable of clustering numerical object data (e.g. K-means). Relational clustering algorithms, on the other hand, cluster objects based on the less direct measurement of similarity between object pairs. We choose to use a density based clustering algorithm DBSCAN (Ester et al., 1996) mainly for two reasons. First, most clustering algorithm require the number of clusters K as an input parameter. The optimal K can apparently vary greatly for names with different frequency and thus is a sensitive parameter. Even if a cluster validity index is used to determine K, it usually requires running the underlying clustering algorithm multiple times and hence is inefficient for large scale data. DBSCAN, as a density based clustering method, only requires density parameters such as the radius of the neighborhood 2 that are universal for different datasets. As we show in the experiment</context>
<context position="13226" citStr="Ester et al., 1996" startWordPosition="2012" endWordPosition="2015">rate chain-shaped clusters). Density-based clustering, on the other hand, can generate clusters of arbitrary shapes since only objects in dense areas are placed in a cluster. DBSCAN induces a density-based cluster by the core objects, i.e. objects having more than a specified number of other data objects in their neighborhood of size E. In each clustering step, a seed object is checked to determine whether it’s a core object and if so, it induces other points of the same cluster using breadth first search (otherwise it’s considered as a noise point). In interest of space, we refer readers to (Ester et al., 1996) for algorithmic details of DBSCAN and now turn our attention to evaluating the disambiguation performance of our methods. 4 Experiments We first formally define the evaluation metrics, followed by the introduction to the benchmark test sets and the system’s performance. 4.1 Evaluation Measures We evaluate the performance of our method using the standard purity and inverse purity clustering metrics. Let a set of clusters C = {C1, ..., Cs} denote the system’s output and a set of categories D = {D1, ..., Dt} be the gold standard. Both C and D are partitions of the WDC chains {w1, ..., wn} (n = &amp;</context>
</contexts>
<marker>Ester, Kriegel, Sander, Xu, 1996</marker>
<rawString>Martin Ester, Hans-Peter Kriegel, Jorg Sander, and Xiaowei Xu. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of 2nd KDD, pages 226 – 231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
<author>Manfred K Warmuth</author>
</authors>
<title>Using and combining predictors that specialize.</title>
<date>1997</date>
<booktitle>In Proceedings of 29th ACM symposium on Theory of computing (STOC).</booktitle>
<contexts>
<context position="9340" citStr="Freund et al., 1997" startWordPosition="1376" endWordPosition="1379">arity features between a pair of WDC chains are computed, we need to compute the pairwise distance metric for clustering. (Cohen et al., 2003) trained a binary SVM model and interpreted its confidence in predicting the negative class as the distance metric. In our case of using information extraction results for disambiguation, however, only some of the similarity features are present based on the availability of relationships in two WDC chains. Therefore, we treat each similarity function as a subordinate predicting algorithm (called specialist) and utilize the specialist learning framework (Freund et al., 1997) to combine the predictions. Here, a specialist is awake only when the same relationships are present in two WDC chains. Also, a specialist can refrain from making a prediction for an instance if it is not confident enough. In addition to the similarity scores, specialists have different weights, e.g. a match in a family relationship is considered more important than in a co-occurrence relationship. The Specialist Exponentiated Gradient (SEG) (Freund et al., 1997) algorithm is adopted to learn to mix the specialists’ prediction. Given a set of T training instances {xt} (xt,i denotes the i-th s</context>
<context position="10667" citStr="Freund et al., 1997" startWordPosition="1594" endWordPosition="1597">(Algorithm 1). In each learning iteration, SEG first predict yt using the set of awake experts Et with respect to instance xt. The true outcome yt (1 for coreference and 0 otherwise) is then revealed and square loss L is incurred. SEG then updates the weight distribution p accordingly. To sum up, the similarity between a pair of 3Though a rich set of similarity features has been built for matching the relationships, they may not encompass all possible cases in real world documents. The goal of this work, however, is to focus on the algorithms instead of knowledge engineering. Algorithm 1 SEG (Freund et al., 1997) Input: Initial weight distribution p1; learning rate q &gt; 0; training set {xt} 1: for t=1 to T do 2: Predict using: yt = Ei∈Et pit xt,i (1) A Ei∈Et pi 3: Observe true label yt and incur square loss L(yt, yt) = (yt − yt)2 4: Update weight distribution: for i E Et pt+1 Ej∈Et ptj i = pt −2ηxt,i(�yt−yt) Pt e-2ηxt,i(yt−yt) ie Ej∈Et pt+1 i = pit, otherwise 5: end for Output: Model p WDC chains wi and wj can be represented in a similarity matrix R, with ri,j computed by the SEG prediction step using the learned weight distribution p (Equation 1). A relational clustering algorithm then clusters entiti</context>
</contexts>
<marker>Freund, Schapire, Singer, Warmuth, 1997</marker>
<rawString>Yoav Freund, Robert E. Schapire, Yoram Singer, and Manfred K. Warmuth. 1997. Using and combining predictors that specialize. In Proceedings of 29th ACM symposium on Theory of computing (STOC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung H Gooi</author>
<author>James Allan</author>
</authors>
<title>Cross-document coreference on a large scale corpus.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4307" citStr="Gooi and Allan, 2004" startWordPosition="645" endWordPosition="648">NLP, Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7–12, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics which links named entities with the same referent within a document into a WDC chain. State-ofthe-art WDC systems, e.g. (Ng and Cardie, 2001), leverage rich lexical features and use supervised and unsupervised machine learning methods. Research on cross document coreference began more recently. (Bagga and Baldwin, 1998) proposed a CDC system to merge the WDC chains using the Vector Space Model on the summary sentences. (Gooi and Allan, 2004) simplified this approach by eliminating the WDC module without significant deterioration in performance. Clustering approaches (e.g. hierarchical agglomerative clustering (Mann and Yarowsky, 2003)) have been commonly used for CDC due to the variety of data distributions of different names. Our work goes beyond the simple co-occurrence features (Bagga and Baldwin, 1998) and the limited extracted information (e.g. biographical information in (Mann and Yarowsky, 2003) that is relatively scarce in web data) using the broad range of relational information with the support of information extraction</context>
</contexts>
<marker>Gooi, Allan, 2004</marker>
<rawString>Chung H. Gooi and James Allan. 2004. Cross-document coreference on a large scale corpus. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Huang</author>
<author>Seyda Ertekin</author>
<author>C Lee Giles</author>
</authors>
<title>Efficient name disambiguation for large scale databases.</title>
<date>2006</date>
<booktitle>In Proc. of 10th PKDD,</booktitle>
<pages>536--544</pages>
<contexts>
<context position="5156" citStr="Huang et al., 2006" startWordPosition="770" endWordPosition="773"> due to the variety of data distributions of different names. Our work goes beyond the simple co-occurrence features (Bagga and Baldwin, 1998) and the limited extracted information (e.g. biographical information in (Mann and Yarowsky, 2003) that is relatively scarce in web data) using the broad range of relational information with the support of information extraction tools. There are also other related research problems. (Li et al., 2004) solved the robust reading problem by adopting a probabilistic view on how documents are generated and how names are sprinkled into them. Our previous work (Huang et al., 2006) resolved the author name ambiguity problem based on the metadata records extracted from academic papers. 3 Methods The overall framework of our CDC system works as follows. Given a document, the information extraction tool first extracts named entities and constructs WDC chains. It also creates linkages (relationships) between entities. The similarity between a pair of relationships in WDC chains is measured by an awakened similarity specialist and the similarity between two WDC chains is determined by the mixture of awakened specialists’ predictions. Finally, a density-based clustering metho</context>
</contexts>
<marker>Huang, Ertekin, Giles, 2006</marker>
<rawString>Jian Huang, Seyda Ertekin, and C. Lee Giles. 2006. Efficient name disambiguation for large scale databases. In Proc. of 10th PKDD, pages 536 – 544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of ROCLING X.</booktitle>
<contexts>
<context position="7766" citStr="Jiang and Conrath, 1997" startWordPosition="1153" endWordPosition="1156">ous similarity schemes tested for name matching. SoftTFIDF is a hybrid matching scheme that combines the tokenbased TFIDF with the Jaro-Winkler string distance metric. This permits inexact matching of named entities due to name variations, spelling errors, etc. Semantic similarity. Text or syntactic similarity is not always sufficient for matching relationships. For instance, although the mentions “U.S. President” and “Commander-in-chief” have no textual overlap, they are semantically highly related as they can be synonyms. We use WordNet and the information theory based JC semantic distance (Jiang and Conrath, 1997) to measure the semantic similarity between concepts in relationships such as mention, employment, ownership and so on. 1AeroText is a text mining application for content analysis, with main focus on information extraction including entity extraction and intrasource link analysis (seehttp://en.wikipedia.org/wiki/AeroText). 2We make no distinctions whether WDC chains are extracted from the same document. Indeed, the CDC system can correct the WDC errors due to lack of information for merging named entities within a document. 8 Other rule-based similarity. Several other cases require special tre</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of ROCLING X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Paul Morie</author>
<author>Dan Roth</author>
</authors>
<title>Robust reading: Identification and tracing of ambiguous names.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="4980" citStr="Li et al., 2004" startWordPosition="741" endWordPosition="744">ithout significant deterioration in performance. Clustering approaches (e.g. hierarchical agglomerative clustering (Mann and Yarowsky, 2003)) have been commonly used for CDC due to the variety of data distributions of different names. Our work goes beyond the simple co-occurrence features (Bagga and Baldwin, 1998) and the limited extracted information (e.g. biographical information in (Mann and Yarowsky, 2003) that is relatively scarce in web data) using the broad range of relational information with the support of information extraction tools. There are also other related research problems. (Li et al., 2004) solved the robust reading problem by adopting a probabilistic view on how documents are generated and how names are sprinkled into them. Our previous work (Huang et al., 2006) resolved the author name ambiguity problem based on the metadata records extracted from academic papers. 3 Methods The overall framework of our CDC system works as follows. Given a document, the information extraction tool first extracts named entities and constructs WDC chains. It also creates linkages (relationships) between entities. The similarity between a pair of relationships in WDC chains is measured by an awake</context>
<context position="20788" citStr="Li et al., 2004" startWordPosition="3236" endWordPosition="3239">fferent types (family memberships vs. geo-locations) and cannot be directly matched against one another. We are exploring several methods to complement the extracted relationships using other information: • Context-aided CDC. The context where an named entity is extracted can be leveraged for coreference. The bag of words in the context tend to be less noisy than that from the entire document. Moreover, we can use noun phrase chunkers to extract base noun phrases from the context. These word or phrase level features can serve as a safenet when the IE tool fails. • Topic-based CDC. Similar to (Li et al., 2004), document topics can be used to ameliorate the sparsity problem. For example, the topics Sport and Education are important cues for differentiating mentions of “Michael Jordan”, which may refer to a basketball player, a computer science professor, etc. Second, as noted in the top WePS run (Chen and Martin, 2007), feature development is important in achieving good coreference performance. We aim to improve the set of similarity specialists in our system by leveraging large knowledge bases. Moreover, although the CDC system is developed in the web person search context, the methods are also app</context>
</contexts>
<marker>Li, Morie, Roth, 2004</marker>
<rawString>Xin Li, Paul Morie, and Dan Roth. 2004. Robust reading: Identification and tracing of ambiguous names. In Proceedings of HLT-NAACL, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="4504" citStr="Mann and Yarowsky, 2003" startWordPosition="668" endWordPosition="671">entities with the same referent within a document into a WDC chain. State-ofthe-art WDC systems, e.g. (Ng and Cardie, 2001), leverage rich lexical features and use supervised and unsupervised machine learning methods. Research on cross document coreference began more recently. (Bagga and Baldwin, 1998) proposed a CDC system to merge the WDC chains using the Vector Space Model on the summary sentences. (Gooi and Allan, 2004) simplified this approach by eliminating the WDC module without significant deterioration in performance. Clustering approaches (e.g. hierarchical agglomerative clustering (Mann and Yarowsky, 2003)) have been commonly used for CDC due to the variety of data distributions of different names. Our work goes beyond the simple co-occurrence features (Bagga and Baldwin, 1998) and the limited extracted information (e.g. biographical information in (Mann and Yarowsky, 2003) that is relatively scarce in web data) using the broad range of relational information with the support of information extraction tools. There are also other related research problems. (Li et al., 2004) solved the robust reading problem by adopting a probabilistic view on how documents are generated and how names are sprinkl</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2003. Unsupervised personal name disambiguation. In Proceedings of HLT-NAACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="4003" citStr="Ng and Cardie, 2001" startWordPosition="599" endWordPosition="602">rk on CDC next and describe our approach in Section 3. The methods are evaluated on benchmark datasets in Section 4. We discuss directions for future improvement in Section 5 and conclude in Section 6. 2 Related Work There is a long tradition of work on the within document coreference (WDC) problem in NLP, Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 7–12, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics which links named entities with the same referent within a document into a WDC chain. State-ofthe-art WDC systems, e.g. (Ng and Cardie, 2001), leverage rich lexical features and use supervised and unsupervised machine learning methods. Research on cross document coreference began more recently. (Bagga and Baldwin, 1998) proposed a CDC system to merge the WDC chains using the Vector Space Model on the summary sentences. (Gooi and Allan, 2004) simplified this approach by eliminating the WDC module without significant deterioration in performance. Clustering approaches (e.g. hierarchical agglomerative clustering (Mann and Yarowsky, 2003)) have been commonly used for CDC due to the variety of data distributions of different names. Our </context>
</contexts>
<marker>Ng, Cardie, 2001</marker>
<rawString>Vincent Ng and Claire Cardie. 2001. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah M Taylor</author>
</authors>
<title>Information extraction tools: Deciphering human language.</title>
<date>2004</date>
<journal>IT Professional,</journal>
<volume>6</volume>
<issue>6</issue>
<contexts>
<context position="6298" citStr="Taylor, 2004" startWordPosition="941" endWordPosition="942">ned specialists’ predictions. Finally, a density-based clustering method generates clusters corresponding to real world entities. We describe these steps in detail. 3.1 Entity and Relationship Extraction Given a document, an information extraction tool is first used to extract named entities and perform within document coreference. Hence, named entities in each document are divided into a set of WDC chains, each chain corresponding to one real world entity. In addition, state-ofthe-art IE tools are capable of creating relational information between named entities. We use an IE tool AeroText1 (Taylor, 2004) for this purpose. Besides the attribute information about the person named entity (first/middle/last names, gender, mention, etc), AeroText also extracts relationship information between named entities, such as Family, List, Employment, Ownership, CitizenResident-Religion-Ethnicity, etc, as specified in the Automatic Content Extraction (ACE) evaluation. The input to the CDC system is a set of WDC chains (with relationship information stored in them) and the CDC task is to merge these WDC chains2. 3.2 Similarity Features We design a suite of similarity functions to determine whether the relati</context>
</contexts>
<marker>Taylor, 2004</marker>
<rawString>Sarah M. Taylor. 2004. Information extraction tools: Deciphering human language. IT Professional, 6(6):28 – 34.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>