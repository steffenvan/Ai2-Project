<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017949">
<title confidence="0.979552">
Lost in Translation: Authorship Attribution using Frame Semantics
</title>
<author confidence="0.99937">
Steffen Hedegaard
</author>
<affiliation confidence="0.998845">
Department of Computer Science,
University of Copenhagen
</affiliation>
<address confidence="0.912942">
Njalsgade 128,
2300 Copenhagen S, Denmark
</address>
<email confidence="0.998914">
steffenh@diku.dk
</email>
<author confidence="0.987871">
Jakob Grue Simonsen
</author>
<affiliation confidence="0.9988045">
Department of Computer Science,
University of Copenhagen
</affiliation>
<address confidence="0.91296">
Njalsgade 128,
2300 Copenhagen S, Denmark
</address>
<email confidence="0.99903">
simonsen@diku.dk
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940631578947">
We investigate authorship attribution using
classifiers based on frame semantics. The pur-
pose is to discover whether adding semantic
information to lexical and syntactic methods
for authorship attribution will improve them,
specifically to address the difficult problem of
authorship attribution of translated texts. Our
results suggest (i) that frame-based classifiers
are usable for author attribution of both trans-
lated and untranslated texts; (ii) that frame-
based classifiers generally perform worse than
the baseline classifiers for untranslated texts,
but (iii) perform as well as, or superior to
the baseline classifiers on translated texts; (iv)
that—contrary to current belief—naïve clas-
sifiers based on lexical markers may perform
tolerably on translated texts if the combination
of author and translator is present in the train-
ing set of a classifier.
</bodyText>
<sectionHeader confidence="0.999465" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999203615384615">
Authorship attribution is the following problem: For
a given text, determine the author of said text among
a list of candidate authors. Determining author-
ship is difficult, and a host of methods have been
proposed: As of 1998 Rudman estimated the num-
ber of metrics used in such methods to be at least
1000 (Rudman, 1997). For comprehensive recent
surveys see e.g. (Juola, 2006; Koppel et al., 2008;
Stamatatos, 2009). The process of authorship at-
tribution consists of selecting markers (features that
provide an indication of the author), and classifying
a text by assigning it to an author using some appro-
priate machine learning technique.
</bodyText>
<page confidence="0.954037">
65
</page>
<subsectionHeader confidence="0.995742">
1.1 Attribution of translated texts
</subsectionHeader>
<bodyText confidence="0.99744035">
In contrast to the general authorship attribution
problem, the specific problem of attributing trans-
lated texts to their original author has received little
attention. Conceivably, this is due to the common
intuition that the impact of the translator may add
enough noise that proper attribution to the original
author will be very difficult; for example, in (Arun
et al., 2009) it was found that the imprint of the
translator was significantly greater than that of the
original author. The volume of resources for nat-
ural language processing in English appears to be
much larger than for any other language, and it is
thus, conceivably, convenient to use the resources at
hand for a translated version of the text, rather than
the original.
To appreciate the difficulty of purely lexical or
syntactic characterization of authors based on trans-
lation, consider the following excerpts from three
different translations of the first few paragraphs of
Turgenev’s ,jsopxHcxoe rHe3,4o:
</bodyText>
<equation confidence="0.4717535">
Liza &amp;quot;A nest of nobles&amp;quot; Translated by W. R. Shedden-
Ralston
</equation>
<bodyText confidence="0.9842808125">
A beautiful spring day was drawing to a close. High
aloft in the clear sky floated small rosy clouds,
which seemed never to drift past, but to be slowly
absorbed into the blue depths beyond.
At an open window, in a handsome mansion situ-
ated in one of the outlying streets of O., the chief
town of the government of that name–it was in the
year 1842–there were sitting two ladies, the one
about fifty years old, the other an old woman of
seventy.
A Nobleman’s Nest Translated by I. F. Hapgood
The brilliant, spring day was inclining toward the
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 65–70,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
evening, tiny rose-tinted cloudlets hung high in the language examples. In this paper, we combine the
heavens, and seemed not to be floating past, but re- data from FrameNet with the LTH semantic parser
treating into the very depths of the azure. (Johansson and Nugues, 2007), until very recently
In front of the open window of a handsome house, (Das et al., 2010) the semantic parser with best ex-
in one of the outlying streets of O * * * the capital perimental performance (note that the performance
of a Government, sat two women; one fifty years of of LTH on our corpora is unknown and may dif-
age, the other seventy years old, and already aged. fer from the numbers reported in (Johansson and
A House of Gentlefolk Translated by C. Garnett Nugues, 2007)).
A bright spring day was fading into evening. High 1.2 Related work
overhead in the clear heavens small rosy clouds The research on authorship attribution is too volu-
seemed hardly to move across the sky but to be minous to include; see the excellent surveys (Juola,
sinking into its depths of blue. 2006; Koppel et al., 2008; Stamatatos, 2009) for
In a handsome house in one of the outlying streets an overview of the plethora of lexical and syntac-
of the government town of O—- (it was in the year tic markers used. The literature on the use of se-
1842) two women were sitting at an open window; mantic markers is much scarcer: Gamon (Gamon,
one was about fifty, the other an old lady of seventy. 2004) developed a tool for producing semantic de-
As translators express the same semantic content pendency graphs and using the resulting information
in different ways the syntax and style of different in conjunction with lexical and syntactic markers to
translations of the same text will differ greatly due improve the accuracy of classification. McCarthy
to the footprint of the translators; this footprint may et al. (McCarthy et al., 2006) employed WordNet
affect the classification process in different ways de- and latent semantic analysis to lexical features with
pending on the features. the purpose of finding semantic similarities between
For markers based on language structure such as words; it is not clear whether the use of semantic
grammar or function words it is to be expected that features improved the classification. Argamon et
the footprint of the translator has such a high im- al. (Argamon, 2007) used systemic functional gram-
pact on the resulting text that attribution to the au- mars to define a feature set associating single words
thor may not be possible. However, it is possi- or phrases with semantic information (an approach
ble that a specific author/translator combination has reminiscent of frames); Experiments of authorship
its own unique footprint discernible from other au- identification on a corpus of English novels of the
thor/translator combinations: A specific translator 19th century showed that the features could improve
may often translate often used phrases in the same the classification results when combined with tra-
way. Ideally, the footprint of the author is (more or ditional function word features. Apart from a few
less) unaffected by the process of translation, for ex- studies (Arun et al., 2009; Holmes, 1992; Archer et
ample if the languages are very similar or the marker al., 1997), the problem of attributing translated texts
is not based solely on lexical or syntactic features. appears to be fairly untouched.
In contrast to purely lexical or syntactic features, 2 Corpus and resource selection
the semantic content is expected to be, roughly, the As pointed out in (Luyckx and Daelemans, 2010) the
same in translations and originals. This leads us to size of data set and number of authors may crucially
hypothesize that a marker based on semantic frames affect the efficiency of author attribution methods,
such as found in the FrameNet database (Ruppen- and evaluation of the method on some standard cor-
hofer et al., 2006), will be largely unaffected by pus is essential (Stamatatos, 2009).
translations, whereas traditional lexical markers will Closest to a standard corpus for author attribu-
be severely impacted by the footprint of the transla- tion is The Federalist Papers (Juola, 2006), origi-
tor. nally used by Mosteller and Wallace (Mosteller and
The FrameNet project is a database of annotated Wallace, 1964), and we employ the subset of this
exemplar frames, their relations to other frames and
obligatory as well as optional frame elements for
each frame. FrameNet currently numbers approxi-
mately 1000 different frames annotated with natural
66
corpus consisting of the 71 undisputed single-author
documents as our Corpus I.
For translated texts, a mix of authors and transla-
tors across authors is needed to ensure that the at-
tribution methods do not attribute to the translator
instead of the author. However, there does not ap-
pear to be a large corpus of texts publicly available
that satisfy this demand.
Based on this, we elected to compile a fresh cor-
pus of translated texts; our Corpus II consists of En-
glish translations of 19th century Russian romantic
literature chosen from Project Gutenberg for which
a number of different versions, with different trans-
lators existed. The corpus primarily consists of nov-
els, but is slightly polluted by a few collections of
short stories and two nonfiction works by Tolstoy
due to the necessity of including a reasonable mix
of authors and translators. The corpus consists of 30
texts by 4 different authors and 12 different transla-
tors of which some have translated several different
authors. The texts range in size from 200 (Turgenev:
The Rendezvous) to 33000 (Tolstoy: War and Peace)
sentences.
The option of splitting the corpus into an artifi-
cially larger corpus by sampling sentences for each
author and collating these into a large number of new
documents was discarded; we deemed that the sam-
pling could inadvertently both smooth differences
between the original texts and smooth differences in
the translators’ footprints. This could have resulted
in an inaccurate positive bias in the evaluation re-
sults.
</bodyText>
<sectionHeader confidence="0.995074" genericHeader="method">
3 Experiment design
</sectionHeader>
<bodyText confidence="0.998670528301887">
For both corpora, authorship attribution experiments
were performed using six classifiers, each employ-
ing a distinct feature set. For each feature set the
markers were counted in the text and their relative
frequencies calculated. Feature selection was based
solely on training data in the inner loop of the cross-
validation cycle. Two sets of experiments were per-
formed, each with with X = 200 and X = 400
features; the size of the feature vector was kept con-
stant across comparison of methods, due to space
constraints only results for 400 features are reported.
The feature sets were:
Frequent Words (FW): Frequencies in the text of
the X most frequent words1. Classification
with this feature set is used as baseline.
Character N-grams: The X most frequent N-
grams for N = 3, 4, 5.
Frames: The relative frequencies of the X most
frequently occurring semantic frames.
Frequent Words and Frames (FWaF): The X/2
most frequent features; words and frames resp.
combined to a single feature vector of size X.
In order to gauge the impact of translation upon an
author’s footprint, three different experiments were
performed on subsets of Corpus II:
The full corpus of 30 texts [Corpus IIa] was used
for authorship attribution with an ample mix of au-
thors an translators, several translators having trans-
lated texts by more than one author. To ascertain
how heavily each marker is influenced by translation
we also performed translator attribution on a sub-
set of 11 texts [Corpus IIb] with 3 different transla-
tors each having translated 3 different authors. If the
translator leaves a heavy footprint on the marker, the
marker is expected to score better when attributing
to translator than to author. Finally, we reduced the
corpus to a set of 18 texts [Corpus IIc] that only in-
cludes unique author/translator combinations to see
if each marker could attribute correctly to an author
if the translator/author combination was not present
in the training set.
All classification experiments were conducted
using a multi-class winner-takes-all (Duan and
Keerthi, 2005) support vector machine (SVM). For
cross-validation, all experiments used leave-one-out
(i.e. N-fold for N texts in the corpus) validation.
All features were scaled to lie in the range [0, 1] be-
fore different types of features were combined. In
each step of the cross-validation process, the most
frequently occurring features were selected from the
training data, and to minimize the effect of skewed
training data on the results, oversampling with sub-
stitution was used on the training data.
</bodyText>
<footnote confidence="0.9928165">
1The most frequent words, is from a list of word frequencies
in the BNC compiled by (Leech et al., 2001)
</footnote>
<page confidence="0.999651">
67
</page>
<sectionHeader confidence="0.999466" genericHeader="evaluation">
4 Results and evaluation
</sectionHeader>
<bodyText confidence="0.999822666666667">
We tested our results for statistical significance us-
ing McNemar’s test (McNemar, 1947) with Yates’
correction for continuity (Yates, 1934) against the
null hypothesis that the classifier is indistinguishable
from a random attribution weighted by the number
of author texts in the corpus.
</bodyText>
<table confidence="0.753824666666667">
Random Weighted Attribution
Corpus I IIa IIb IIc
Accuracy 57.6 28.7 33.9 26.5
</table>
<tableCaption confidence="0.995349">
Table 1: Accuracy of a random weighted attribution.
</tableCaption>
<bodyText confidence="0.991440636363636">
FWaF performed better than FW for attribution of
author on translated texts. However, the difference
failed to be statistically significant.
Results of the experiments are reported in the ta-
ble below. For each corpus results are given for
experiments with 400 features. We report macro2
precision/recall, and the corresponding F1 and ac-
curacy scores; the best scoring result in each row is
shown in boldface. For each corpus the bottom row
indicates whether each classifier is significantly dis-
cernible from a weighted random attribution.
</bodyText>
<table confidence="0.976816">
400 Features
Corpus Measure FW 3-grams 4-grams 5-grams Frames FWaF
I precision 96.4 97.0 97.0 99.4 80.7 92.0
recall 90.3 97.0 91.0 97.6 66.8 93.3
F1 93.3 97.0 93.9 98.5 73.1 92.7
Accuracy 95.8 97.2 97.2 98.6 80.3 93.0
p&lt;0.05: ✓ ✓ ✓ ✓ ✓ ✓
IIa precision 63.8 61.9 59.1 57.9 82.7 81.9
recall 66.4 60.4 60.4 60.4 70.8 80.8
F1 65.1 61.1 59.7 59.1 76.3 81.3
Accuracy 80.0 73.3 73.3 73.3 76.7 90.0
p&lt;0.05: ✓ ✓ ✓ ✓ ✓ ✓
IIb precision 91.7 47.2 47.2 38.9 70.0 70.0
recall 91.7 58.3 58.3 50.0 63.9 63.9
F1 91.7 52.2 52.2 43.8 66.8 66.8
Accuracy 90.9 63.6 63.6 54.5 63.6 63.6
p&lt;0.05: ✓
IIc precision 42.9 43.8 42.4 51.0 60.1 75.0
recall 52.1 42.1 42.1 50.4 59.6 75.0
F1 47.0 42.9 42.2 50.7 59.8 75.0
Accuracy 55.6 50.0 44.4 55.6 61.1 72.2
p&lt;0.05: ✓
</table>
<tableCaption confidence="0.994882">
Table 2: Authorship attribution results
</tableCaption>
<footnote confidence="0.975711">
2each author is given equal weight, regardless of the number
of documents
</footnote>
<subsectionHeader confidence="0.903529">
4.1 Corpus I: The Federalist Papers
</subsectionHeader>
<bodyText confidence="0.999529625">
For the Federalist Papers the traditional authorship
attribution markers all lie in the 95+ range in accu-
racy as expected. However, the frame-based mark-
ers achieved statistically significant results, and can
hence be used for authorship attribution on untrans-
lated documents (but performs worse than the base-
line). FWaF did not result in an improvement over
FW.
</bodyText>
<subsectionHeader confidence="0.998198">
4.2 Corpus II: Attribution of translated texts
</subsectionHeader>
<bodyText confidence="0.99996825">
For Corpus IIa–the entire corpus of translated texts–
all methods achieve results significantly better than
random, and FWaF is the best-scoring method, fol-
lowed by FW.
The results for Corpus IIb (three authors, three
translators) clearly suggest that the footprint of the
translator is evident in the translated texts, and that
the FW (function word) classifier is particularly sen-
sitive to the footprint. In fact, FW was the only one
achieving a significant result over random assign-
ment, giving an indication that this marker may be
particularly vulnerable to translator influence when
attempting to attribute authors.
For Corpus IIc (unique author/translator combina-
tions) decreased performance of all methods is evi-
dent. Some of this can be attributed to a smaller
(training) corpus, but we also suspect the lack of
several instances of the same author/translator com-
binations in the corpus.
Observe that the FWaF classifier is the only
classifier with significantly better performance than
weighted random assignment, and outperforms the
other methods. Frames alone also outperform tradi-
tional markers, albeit not by much.
The experiments on the collected corpora strongly
suggest the feasibility of using Frames as markers
for authorship attribution, in particular in combina-
tion with traditional lexical approaches.
Our inability to obtain demonstrably significant
improvement of FWaF over the approach based on
Frequent Words is likely an artifact of the fairly
small corpus we employ. However, computation of
significance is generally woefully absent from stud-
ies of automated author attribution, so it is conceiv-
able that the apparent improvement shown in many
such studies fail to be statistically significant under
</bodyText>
<page confidence="0.988873">
68
</page>
<bodyText confidence="0.94604326">
closer scrutiny (note that the exact tests to employ mon in a corpus may fail to occur in the training
for statistical significance in information retrieval– material of a given author; it is thus conceivable that
including text categorization–is a subject of con- smoothing would improve classification by frames
tention (Smucker et al., 2007)). more than by words or N-grams.
5 Conclusions, caveats, and future work References
We have investigated the use of semantic frames as John B. Archer, John L. Hilton, and G. Bruce Schaalje.
markers for author attribution and tested their appli- 1997. Comparative power of three author-attribution
cability to attribution of translated texts. Our results techniques for differentiating authors. Journal of Book
show that frames are potentially useful, especially of Mormon Studies, 6(1):47–63.
so for translated texts, and suggest that a combined Shlomo Argamon. 2007. Interpreting Burrows’ Delta:
method of frequent words and frames can outper- Geometric and probabilistic foundations. Literary and
form methods based solely on traditional markers, Linguistic Computing, 23(2):131–147.
on translated texts. For attribution of untranslated R. Arun, V. Suresh, and C. E. Veni Madhaven. 2009.
texts and attribution to translator traditional markers Stopword graphs and authorship attribution in text cor-
such as frequent words and n-grams are still to be pora. In Proceedings of the 3rd IEEE International
preferred. Conference on Semantic Computing (ICSC 2009),
Our test corpora consist of a limited number of pages 192–196, Berkeley, CA, USA, sep. IEEE Com-
authors, from a limited time period, with translators puter Society Press.
from a similar limited time period and cultural con- Dipanjan Das, Nathan Schneider, Desai Chen, and
text. Furthermore, our translations are all from a sin- Noah A. Smith. 2010. Probabilistic frame-semantic
gle language. Thus, further work is needed before parsing. In Proceedings of the North American Chap-
firm conclusions regarding the general applicability ter of the Association for Compututional Linguistics
of the methods can be made. Human Language Technologies Conference (NAACL
It is well known that effectiveness of authorship HLT ’10).
markers may be influenced by topics (Stein et al., Kai-Bo Duan and S. Sathiya Keerthi. 2005. Which is
2007; Schein et al., 2010); while we have endeav- the best multiclass svm method? an empirical study.
ored to design our corpora to minimize such influ- In Proceedings of the Sixth International Workshop on
ence, we do not currently know the quantitative im- Multiple Classifier Systems, pages 278–285.
pact on topicality on the attribution methods in this Michael Gamon. 2004. Linguistic correlates of style:
paper. Furthermore, traditional investigations of au- Authorship classification with deep linguistic analy-
thorship attribution have focused on the case of at- sis features. In Proceedings of the 20th International
tributing texts among a small (N &lt; 10) class of Conference on Computational Linguistics (COLING
authors at the time, albeit with recent, notable ex- ’04), pages 611–617.
ceptions (Luyckx and Daelemans, 2010; Koppel et David I. Holmes. 1992. A stylometric analysis of mor-
al., 2010). We test our methods on similarly re- mon scripture and related texts. Journal of the Royal
stricted sets of authors; the scalability of the meth- Statistical Society, Series A, 155(1):91–120.
ods to larger numbers of authors is currently un- Richard Johansson and Pierre Nugues. 2007. Semantic
known. Combining several classification methods structure extraction using nonprojective dependency
into an ensemble method may yield improvements trees. In Proceedings of SemEval-2007, Prague, Czech
in precision (Raghavan et al., 2010); it would be Republic, June 23-24.
interesting to see whether a classifier using frames Patrick Juola. 2006. Authorship attribution. Found.
yields significant improvements in ensemble with Trends Inf. Retr., 1(3):233–334.
other methods. Finally, the distribution of frames in Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
texts is distinctly different from the distribution of 2008. Computational methods for authorship attribu-
words: While there are function words, there are no tion. Journal of the American Society for Information
‘function frames’, and certain frames that are com- Sciences and Technology, 60(1):9–25.
69 Moshe Koppel, Jonathan Schler, and Shlomo Arga-
mon. 2010. Authorship attribution in the wild.
Language Resources and Evaluation, pages 1–12.
10.1007/s10579-009-9111-2.
</bodyText>
<reference confidence="0.998316210526316">
Geoffrey Leech, Paul Rayson, and Andrew Wilson.
2001. Word Frequencies in Written and Spoken En-
glish: Based on the British National Corpus. Long-
man, London.
Kim Luyckx and Walter Daelemans. 2010. The effect of
author set size and data size in authorship attribution.
Literary and Linguistic Computing. To appear.
Philip M. McCarthy, Gwyneth A. Lewis, David F. Dufty,
and Danielle S. McNamara. 2006. Analyzing writing
styles with coh-metrix. In Proceedings of the Interna-
tional Conference of the Florida Artificial Intelligence
Research Society, pages 764–769.
Quinn McNemar. 1947. Note on the sampling error of
the difference between correlated proportions or per-
centages. Psychometrika, 12:153–157.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and Disputed Authorship: The Federalist.
Springer-Verlag, New York. 2nd Edition appeared in
1984 and was called Applied Bayesian and Classical
Inference.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proceedings of the
ACL 2010 Conference Short Papers, pages 38–42. As-
sociation for Computational Linguistics.
Joseph Rudman. 1997. The state of authorship attribu-
tion studies: Some problems and solutions. Comput-
ers and the Humanities, 31(4):351–365.
Joseph Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Scheffczyk.
2006. FrameNet II: Extended Theory and Practice.
The Framenet Project.
Andrew I. Schein, Johnnie F. Caver, Randale J. Honaker,
and Craig H. Martell. 2010. Author attribution evalua-
tion with novel topic cross-validation. In Proceedings
of the 2010 International Conference on Knowledge
Discovery and Information Retrieval (KDIR ’10).
Mark D. Smucker, James Allan, and Ben Carterette.
2007. A comparison of statistical significance tests
for information retrieval evaluation. In Proceedings of
the sixteenth ACM conference on Conference on infor-
mation and knowledge management, CIKM ’07, pages
623–632, New York, NY, USA. ACM.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538–556.
Benno Stein, Moshe Koppel, and Efstathios Stamatatos,
editors. 2007. Proceedings of the SIGIR 2007 In-
ternational Workshop on Plagiarism Analysis, Au-
thorship Identification, and Near-Duplicate Detection,
PAN 2007, Amsterdam, Netherlands, July 27, 2007,
volume 276 of CEUR Workshop Proceedings. CEUR-
WS.org.
Frank Yates. 1934. Contingency tables involving small
numbers and the x2 test. Supplement to the Journal of
the Royal Statistical Society, 1(2):pp. 217–235.
</reference>
<page confidence="0.998453">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.367298">
<title confidence="0.998192">Lost in Translation: Authorship Attribution using Frame Semantics</title>
<author confidence="0.9978">Steffen</author>
<affiliation confidence="0.875085">Department of Computer University of Njalsgade</affiliation>
<address confidence="0.993637">2300 Copenhagen S,</address>
<email confidence="0.982295">steffenh@diku.dk</email>
<author confidence="0.999558">Jakob Grue</author>
<affiliation confidence="0.875093">Department of Computer University of Njalsgade</affiliation>
<address confidence="0.993544">2300 Copenhagen S,</address>
<email confidence="0.992764">simonsen@diku.dk</email>
<abstract confidence="0.998819">We investigate authorship attribution using classifiers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that framebased classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that—contrary to current belief—naïve classifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
<author>Paul Rayson</author>
<author>Andrew Wilson</author>
</authors>
<date>2001</date>
<booktitle>Word Frequencies in Written and Spoken English: Based on the British National Corpus.</booktitle>
<location>Longman, London.</location>
<contexts>
<context position="12460" citStr="Leech et al., 2001" startWordPosition="2011" endWordPosition="2014">an and Keerthi, 2005) support vector machine (SVM). For cross-validation, all experiments used leave-one-out (i.e. N-fold for N texts in the corpus) validation. All features were scaled to lie in the range [0, 1] before different types of features were combined. In each step of the cross-validation process, the most frequently occurring features were selected from the training data, and to minimize the effect of skewed training data on the results, oversampling with substitution was used on the training data. 1The most frequent words, is from a list of word frequencies in the BNC compiled by (Leech et al., 2001) 67 4 Results and evaluation We tested our results for statistical significance using McNemar’s test (McNemar, 1947) with Yates’ correction for continuity (Yates, 1934) against the null hypothesis that the classifier is indistinguishable from a random attribution weighted by the number of author texts in the corpus. Random Weighted Attribution Corpus I IIa IIb IIc Accuracy 57.6 28.7 33.9 26.5 Table 1: Accuracy of a random weighted attribution. FWaF performed better than FW for attribution of author on translated texts. However, the difference failed to be statistically significant. Results of </context>
</contexts>
<marker>Leech, Rayson, Wilson, 2001</marker>
<rawString>Geoffrey Leech, Paul Rayson, and Andrew Wilson. 2001. Word Frequencies in Written and Spoken English: Based on the British National Corpus. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Luyckx</author>
<author>Walter Daelemans</author>
</authors>
<title>The effect of author set size and data size in authorship attribution. Literary and Linguistic Computing.</title>
<date>2010</date>
<note>To appear.</note>
<contexts>
<context position="7301" citStr="Luyckx and Daelemans, 2010" startWordPosition="1170" endWordPosition="1173">tion results when combined with traway. Ideally, the footprint of the author is (more or ditional function word features. Apart from a few less) unaffected by the process of translation, for ex- studies (Arun et al., 2009; Holmes, 1992; Archer et ample if the languages are very similar or the marker al., 1997), the problem of attributing translated texts is not based solely on lexical or syntactic features. appears to be fairly untouched. In contrast to purely lexical or syntactic features, 2 Corpus and resource selection the semantic content is expected to be, roughly, the As pointed out in (Luyckx and Daelemans, 2010) the same in translations and originals. This leads us to size of data set and number of authors may crucially hypothesize that a marker based on semantic frames affect the efficiency of author attribution methods, such as found in the FrameNet database (Ruppen- and evaluation of the method on some standard corhofer et al., 2006), will be largely unaffected by pus is essential (Stamatatos, 2009). translations, whereas traditional lexical markers will Closest to a standard corpus for author attribube severely impacted by the footprint of the transla- tion is The Federalist Papers (Juola, 2006),</context>
<context position="19627" citStr="Luyckx and Daelemans, 2010" startWordPosition="3132" endWordPosition="3135">on ence, we do not currently know the quantitative im- Multiple Classifier Systems, pages 278–285. pact on topicality on the attribution methods in this Michael Gamon. 2004. Linguistic correlates of style: paper. Furthermore, traditional investigations of au- Authorship classification with deep linguistic analythorship attribution have focused on the case of at- sis features. In Proceedings of the 20th International tributing texts among a small (N &lt; 10) class of Conference on Computational Linguistics (COLING authors at the time, albeit with recent, notable ex- ’04), pages 611–617. ceptions (Luyckx and Daelemans, 2010; Koppel et David I. Holmes. 1992. A stylometric analysis of moral., 2010). We test our methods on similarly re- mon scripture and related texts. Journal of the Royal stricted sets of authors; the scalability of the meth- Statistical Society, Series A, 155(1):91–120. ods to larger numbers of authors is currently un- Richard Johansson and Pierre Nugues. 2007. Semantic known. Combining several classification methods structure extraction using nonprojective dependency into an ensemble method may yield improvements trees. In Proceedings of SemEval-2007, Prague, Czech in precision (Raghavan et al.,</context>
</contexts>
<marker>Luyckx, Daelemans, 2010</marker>
<rawString>Kim Luyckx and Walter Daelemans. 2010. The effect of author set size and data size in authorship attribution. Literary and Linguistic Computing. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M McCarthy</author>
<author>Gwyneth A Lewis</author>
<author>David F Dufty</author>
<author>Danielle S McNamara</author>
</authors>
<title>Analyzing writing styles with coh-metrix.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference of the Florida Artificial Intelligence Research Society,</booktitle>
<pages>764--769</pages>
<contexts>
<context position="5583" citStr="McCarthy et al., 2006" startWordPosition="898" endWordPosition="901">he literature on the use of se1842) two women were sitting at an open window; mantic markers is much scarcer: Gamon (Gamon, one was about fifty, the other an old lady of seventy. 2004) developed a tool for producing semantic deAs translators express the same semantic content pendency graphs and using the resulting information in different ways the syntax and style of different in conjunction with lexical and syntactic markers to translations of the same text will differ greatly due improve the accuracy of classification. McCarthy to the footprint of the translators; this footprint may et al. (McCarthy et al., 2006) employed WordNet affect the classification process in different ways de- and latent semantic analysis to lexical features with pending on the features. the purpose of finding semantic similarities between For markers based on language structure such as words; it is not clear whether the use of semantic grammar or function words it is to be expected that features improved the classification. Argamon et the footprint of the translator has such a high im- al. (Argamon, 2007) used systemic functional grampact on the resulting text that attribution to the au- mars to define a feature set associati</context>
</contexts>
<marker>McCarthy, Lewis, Dufty, McNamara, 2006</marker>
<rawString>Philip M. McCarthy, Gwyneth A. Lewis, David F. Dufty, and Danielle S. McNamara. 2006. Analyzing writing styles with coh-metrix. In Proceedings of the International Conference of the Florida Artificial Intelligence Research Society, pages 764–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<tech>Psychometrika,</tech>
<pages>12--153</pages>
<contexts>
<context position="12576" citStr="McNemar, 1947" startWordPosition="2031" endWordPosition="2032"> for N texts in the corpus) validation. All features were scaled to lie in the range [0, 1] before different types of features were combined. In each step of the cross-validation process, the most frequently occurring features were selected from the training data, and to minimize the effect of skewed training data on the results, oversampling with substitution was used on the training data. 1The most frequent words, is from a list of word frequencies in the BNC compiled by (Leech et al., 2001) 67 4 Results and evaluation We tested our results for statistical significance using McNemar’s test (McNemar, 1947) with Yates’ correction for continuity (Yates, 1934) against the null hypothesis that the classifier is indistinguishable from a random attribution weighted by the number of author texts in the corpus. Random Weighted Attribution Corpus I IIa IIb IIc Accuracy 57.6 28.7 33.9 26.5 Table 1: Accuracy of a random weighted attribution. FWaF performed better than FW for attribution of author on translated texts. However, the difference failed to be statistically significant. Results of the experiments are reported in the table below. For each corpus results are given for experiments with 400 features</context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12:153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Inference and Disputed Authorship: The Federalist.</title>
<date>1964</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<note>2nd Edition appeared in</note>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1964. Inference and Disputed Authorship: The Federalist. Springer-Verlag, New York. 2nd Edition appeared in 1984 and was called Applied Bayesian and Classical Inference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sindhu Raghavan</author>
<author>Adriana Kovashka</author>
<author>Raymond Mooney</author>
</authors>
<title>Authorship attribution using probabilistic context-free grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>38--42</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20233" citStr="Raghavan et al., 2010" startWordPosition="3221" endWordPosition="3224"> Daelemans, 2010; Koppel et David I. Holmes. 1992. A stylometric analysis of moral., 2010). We test our methods on similarly re- mon scripture and related texts. Journal of the Royal stricted sets of authors; the scalability of the meth- Statistical Society, Series A, 155(1):91–120. ods to larger numbers of authors is currently un- Richard Johansson and Pierre Nugues. 2007. Semantic known. Combining several classification methods structure extraction using nonprojective dependency into an ensemble method may yield improvements trees. In Proceedings of SemEval-2007, Prague, Czech in precision (Raghavan et al., 2010); it would be Republic, June 23-24. interesting to see whether a classifier using frames Patrick Juola. 2006. Authorship attribution. Found. yields significant improvements in ensemble with Trends Inf. Retr., 1(3):233–334. other methods. Finally, the distribution of frames in Moshe Koppel, Jonathan Schler, and Shlomo Argamon. texts is distinctly different from the distribution of 2008. Computational methods for authorship attribuwords: While there are function words, there are no tion. Journal of the American Society for Information ‘function frames’, and certain frames that are com- Sciences </context>
</contexts>
<marker>Raghavan, Kovashka, Mooney, 2010</marker>
<rawString>Sindhu Raghavan, Adriana Kovashka, and Raymond Mooney. 2010. Authorship attribution using probabilistic context-free grammars. In Proceedings of the ACL 2010 Conference Short Papers, pages 38–42. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Rudman</author>
</authors>
<title>The state of authorship attribution studies: Some problems and solutions.</title>
<date>1997</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>31--4</pages>
<contexts>
<context position="1543" citStr="Rudman, 1997" startWordPosition="224" endWordPosition="225">s, or superior to the baseline classifiers on translated texts; (iv) that—contrary to current belief—naïve classifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier. 1 Introduction Authorship attribution is the following problem: For a given text, determine the author of said text among a list of candidate authors. Determining authorship is difficult, and a host of methods have been proposed: As of 1998 Rudman estimated the number of metrics used in such methods to be at least 1000 (Rudman, 1997). For comprehensive recent surveys see e.g. (Juola, 2006; Koppel et al., 2008; Stamatatos, 2009). The process of authorship attribution consists of selecting markers (features that provide an indication of the author), and classifying a text by assigning it to an author using some appropriate machine learning technique. 65 1.1 Attribution of translated texts In contrast to the general authorship attribution problem, the specific problem of attributing translated texts to their original author has received little attention. Conceivably, this is due to the common intuition that the impact of the</context>
</contexts>
<marker>Rudman, 1997</marker>
<rawString>Joseph Rudman. 1997. The state of authorship attribution studies: Some problems and solutions. Computers and the Humanities, 31(4):351–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice. The Framenet Project.</title>
<date>2006</date>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Joseph Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2006. FrameNet II: Extended Theory and Practice. The Framenet Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew I Schein</author>
<author>Johnnie F Caver</author>
<author>Randale J Honaker</author>
<author>Craig H Martell</author>
</authors>
<title>Author attribution evaluation with novel topic cross-validation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 International Conference on Knowledge Discovery and Information Retrieval (KDIR ’10).</booktitle>
<contexts>
<context position="18823" citStr="Schein et al., 2010" startWordPosition="3012" endWordPosition="3015">panjan Das, Nathan Schneider, Desai Chen, and text. Furthermore, our translations are all from a sin- Noah A. Smith. 2010. Probabilistic frame-semantic gle language. Thus, further work is needed before parsing. In Proceedings of the North American Chapfirm conclusions regarding the general applicability ter of the Association for Compututional Linguistics of the methods can be made. Human Language Technologies Conference (NAACL It is well known that effectiveness of authorship HLT ’10). markers may be influenced by topics (Stein et al., Kai-Bo Duan and S. Sathiya Keerthi. 2005. Which is 2007; Schein et al., 2010); while we have endeav- the best multiclass svm method? an empirical study. ored to design our corpora to minimize such influ- In Proceedings of the Sixth International Workshop on ence, we do not currently know the quantitative im- Multiple Classifier Systems, pages 278–285. pact on topicality on the attribution methods in this Michael Gamon. 2004. Linguistic correlates of style: paper. Furthermore, traditional investigations of au- Authorship classification with deep linguistic analythorship attribution have focused on the case of at- sis features. In Proceedings of the 20th International tr</context>
</contexts>
<marker>Schein, Caver, Honaker, Martell, 2010</marker>
<rawString>Andrew I. Schein, Johnnie F. Caver, Randale J. Honaker, and Craig H. Martell. 2010. Author attribution evaluation with novel topic cross-validation. In Proceedings of the 2010 International Conference on Knowledge Discovery and Information Retrieval (KDIR ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Smucker</author>
<author>James Allan</author>
<author>Ben Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM ’07,</booktitle>
<pages>623--632</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="16818" citStr="Smucker et al., 2007" startWordPosition="2712" endWordPosition="2715">ct of the fairly small corpus we employ. However, computation of significance is generally woefully absent from studies of automated author attribution, so it is conceivable that the apparent improvement shown in many such studies fail to be statistically significant under 68 closer scrutiny (note that the exact tests to employ mon in a corpus may fail to occur in the training for statistical significance in information retrieval– material of a given author; it is thus conceivable that including text categorization–is a subject of con- smoothing would improve classification by frames tention (Smucker et al., 2007)). more than by words or N-grams. 5 Conclusions, caveats, and future work References We have investigated the use of semantic frames as John B. Archer, John L. Hilton, and G. Bruce Schaalje. markers for author attribution and tested their appli- 1997. Comparative power of three author-attribution cability to attribution of translated texts. Our results techniques for differentiating authors. Journal of Book show that frames are potentially useful, especially of Mormon Studies, 6(1):47–63. so for translated texts, and suggest that a combined Shlomo Argamon. 2007. Interpreting Burrows’ Delta: me</context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>Mark D. Smucker, James Allan, and Ben Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM ’07, pages 623–632, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>A survey of modern authorship attribution methods.</title>
<date>2009</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>60</volume>
<issue>3</issue>
<contexts>
<context position="1639" citStr="Stamatatos, 2009" startWordPosition="238" endWordPosition="239">t belief—naïve classifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier. 1 Introduction Authorship attribution is the following problem: For a given text, determine the author of said text among a list of candidate authors. Determining authorship is difficult, and a host of methods have been proposed: As of 1998 Rudman estimated the number of metrics used in such methods to be at least 1000 (Rudman, 1997). For comprehensive recent surveys see e.g. (Juola, 2006; Koppel et al., 2008; Stamatatos, 2009). The process of authorship attribution consists of selecting markers (features that provide an indication of the author), and classifying a text by assigning it to an author using some appropriate machine learning technique. 65 1.1 Attribution of translated texts In contrast to the general authorship attribution problem, the specific problem of attributing translated texts to their original author has received little attention. Conceivably, this is due to the common intuition that the impact of the translator may add enough noise that proper attribution to the original author will be very dif</context>
<context position="4787" citStr="Stamatatos, 2009" startWordPosition="763" endWordPosition="764"> the performance of a Government, sat two women; one fifty years of of LTH on our corpora is unknown and may difage, the other seventy years old, and already aged. fer from the numbers reported in (Johansson and A House of Gentlefolk Translated by C. Garnett Nugues, 2007)). A bright spring day was fading into evening. High 1.2 Related work overhead in the clear heavens small rosy clouds The research on authorship attribution is too voluseemed hardly to move across the sky but to be minous to include; see the excellent surveys (Juola, sinking into its depths of blue. 2006; Koppel et al., 2008; Stamatatos, 2009) for In a handsome house in one of the outlying streets an overview of the plethora of lexical and syntacof the government town of O—- (it was in the year tic markers used. The literature on the use of se1842) two women were sitting at an open window; mantic markers is much scarcer: Gamon (Gamon, one was about fifty, the other an old lady of seventy. 2004) developed a tool for producing semantic deAs translators express the same semantic content pendency graphs and using the resulting information in different ways the syntax and style of different in conjunction with lexical and syntactic mark</context>
<context position="7699" citStr="Stamatatos, 2009" startWordPosition="1238" endWordPosition="1239">s. appears to be fairly untouched. In contrast to purely lexical or syntactic features, 2 Corpus and resource selection the semantic content is expected to be, roughly, the As pointed out in (Luyckx and Daelemans, 2010) the same in translations and originals. This leads us to size of data set and number of authors may crucially hypothesize that a marker based on semantic frames affect the efficiency of author attribution methods, such as found in the FrameNet database (Ruppen- and evaluation of the method on some standard corhofer et al., 2006), will be largely unaffected by pus is essential (Stamatatos, 2009). translations, whereas traditional lexical markers will Closest to a standard corpus for author attribube severely impacted by the footprint of the transla- tion is The Federalist Papers (Juola, 2006), origitor. nally used by Mosteller and Wallace (Mosteller and The FrameNet project is a database of annotated Wallace, 1964), and we employ the subset of this exemplar frames, their relations to other frames and obligatory as well as optional frame elements for each frame. FrameNet currently numbers approximately 1000 different frames annotated with natural 66 corpus consisting of the 71 undispu</context>
</contexts>
<marker>Stamatatos, 2009</marker>
<rawString>Efstathios Stamatatos. 2009. A survey of modern authorship attribution methods. Journal of the American Society for Information Science and Technology, 60(3):538–556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benno Stein</author>
<author>Moshe Koppel</author>
</authors>
<title>and Efstathios Stamatatos, editors.</title>
<date>2007</date>
<booktitle>Proceedings of the SIGIR 2007 International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection, PAN 2007,</booktitle>
<volume>276</volume>
<location>Amsterdam, Netherlands,</location>
<marker>Stein, Koppel, 2007</marker>
<rawString>Benno Stein, Moshe Koppel, and Efstathios Stamatatos, editors. 2007. Proceedings of the SIGIR 2007 International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection, PAN 2007, Amsterdam, Netherlands, July 27, 2007, volume 276 of CEUR Workshop Proceedings. CEURWS.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Yates</author>
</authors>
<title>Contingency tables involving small numbers and the x2 test. Supplement to the</title>
<date>1934</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>217--235</pages>
<contexts>
<context position="12628" citStr="Yates, 1934" startWordPosition="2038" endWordPosition="2039">ere scaled to lie in the range [0, 1] before different types of features were combined. In each step of the cross-validation process, the most frequently occurring features were selected from the training data, and to minimize the effect of skewed training data on the results, oversampling with substitution was used on the training data. 1The most frequent words, is from a list of word frequencies in the BNC compiled by (Leech et al., 2001) 67 4 Results and evaluation We tested our results for statistical significance using McNemar’s test (McNemar, 1947) with Yates’ correction for continuity (Yates, 1934) against the null hypothesis that the classifier is indistinguishable from a random attribution weighted by the number of author texts in the corpus. Random Weighted Attribution Corpus I IIa IIb IIc Accuracy 57.6 28.7 33.9 26.5 Table 1: Accuracy of a random weighted attribution. FWaF performed better than FW for attribution of author on translated texts. However, the difference failed to be statistically significant. Results of the experiments are reported in the table below. For each corpus results are given for experiments with 400 features. We report macro2 precision/recall, and the corresp</context>
</contexts>
<marker>Yates, 1934</marker>
<rawString>Frank Yates. 1934. Contingency tables involving small numbers and the x2 test. Supplement to the Journal of the Royal Statistical Society, 1(2):pp. 217–235.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>