<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001732">
<note confidence="0.742749">
Computational Linguistics Volume 26, Number 2
</note>
<title confidence="0.967776">
Optimality Theory
</title>
<author confidence="0.982135">
René Kager
</author>
<affiliation confidence="0.9940645">
(Utrecht University)
Cambridge University Press
</affiliation>
<address confidence="0.7800488">
(Cambridge textbooks in linguistics,
edited by S.R. Anderson et al.), 1999,
xiii+452 pp; hardbound, ISBN
0-521-58019-6, $64.95; paperbound,
ISBN 0-521-58980-0, $24.95
</address>
<figure confidence="0.2528785">
Reviewed by
Jason Eisner
</figure>
<affiliation confidence="0.963005">
University of Rochester
</affiliation>
<sectionHeader confidence="0.985572" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999329315789474">
René Kager&apos;s textbook is one of the first to cover Optimality Theory (OT), a declarative
grammar framework that swiftly took over phonology after it was introduced by
Prince, Smolensky, and McCarthy in 1993.
OT reclaims traditional grammar&apos;s ability to express surface generalizations (&amp;quot;syl-
lables have onsets,&amp;quot; &amp;quot;no nasal+voiceless obstruent clusters&amp;quot;). Empirically, some surface
generalizations are robust within a language, or—perhaps for functionalist reasons—
widespread across languages. Derivational theories were forced to posit diverse rules
that rescued these robust generalizations from other phonological processes. An OT
grammar avoids such &amp;quot;conspiracies&amp;quot; by stating the generalizations directly, as in Two-
Level Morphology (Koskenniemi 1983) or Declarative Phonology (Bird 1995).
In OT, the processes that try but fail to disrupt a robust generalization are described
not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a general-
ization may fail in contexts where it is overruled by a higher-ranked requirement of
the language (or of the underlying form). As Kager emphasizes, this interaction of
violable constraints can yield complex surface patterns.
OT therefore holds out the promise of simplifying grammars, by factoring all com-
plex phenomena into simple surface-level constraints that partially mask one another.1
Whether this is always possible under an appropriate definition of &amp;quot;simple constraints&amp;quot;
(e.g., Eisner 1997b) is of course an empirical question.
</bodyText>
<sectionHeader confidence="0.993712" genericHeader="keywords">
2. Relevance
</sectionHeader>
<bodyText confidence="0.999122166666667">
Before looking at Kager &apos;s textbook in detail, it is worth pausing to ask what broader
implications Optimality Theory might have for computational linguistics. If you are
an academic phonologist, you already know OT by now. If you are not, should you
take the time to learn?
So far, OT has served CL mainly as a source of interesting new problems—both
theoretical and (assuming a lucrative market for phonology workbench utilities) prac-
</bodyText>
<footnote confidence="0.992092">
1 This style of analysis is shared by Autolexical Grammar (Sadock 1985), which has focused more on
(morpho)syntax than phonology.
</footnote>
<page confidence="0.995943">
286
</page>
<subsectionHeader confidence="0.949041">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.99980972">
tical. To wit: Given constraints of a certain computational power (e.g., finite-state),
how expressive is the class of OT grammars? How to generate the optimal surface
form for a given underlying form? Or conversely, how to reconstruct an underlying
form for which a given surface form is optimal? How can one learn a grammar and
lexicon? Should we rethink our phonological representations? And how about vari-
ants of the OT framework? Many of the relevant papers are listed in ACL SIGPHON&apos;s
computational OT bibliography at http://www.cogsci.ed.ac.uk/sigphon/.
Within phonology, the obvious applications of OT are in speech recognition and
synthesis. Given a lexicon, any phonological grammar serves as a compact pronounc-
ing dictionary that generalizes to novel inputs (compound and inflected forms) as well
as novel outputs (free and dialectal variants). OT is strong on the latter point, since it
offers a plausible account of variation in terms of constraint reranking. Unfortunately,
complete grammars are still in short supply.
Looking beyond phonology, OT actually parallels a recent trend in statistical NLP:
to describe natural language at all levels by specifying the relative importance of
many conflicting surface features. This approach characterizes the family of proba-
bility distributions known variously as maximum-entropy models, log-linear mod-
els, Markov random fields, or Gibbs distributions. Indeed, such models were well
known to one of the architects of OT (Smolensky 1986), and it is possible to regard
an OT grammar as a limit case of a Gibbs distribution whose conditional probabilities
p(surface form I underlying form) approach 1.2 Johnson (2000) has recently learned
simple OT constraint rankings by fitting Gibbs distributions to unambiguous data.
Gibbs distributions are broadly useful in NLP when their features are chosen
well. So one might study OT simply to develop better intuitions about useful types of
linguistic features and their patterns of interaction, and about the usefulness of positing
hidden structure (e.g., prosodic constituency) to which multiple features may refer.
For example, consider the relevance to hidden Markov models (HMMs), another
restricted class of Gibbs distributions used in speech recognition or part-of-speech
tagging. Just like OT grammars, HMM Viterbi decoders are functions that pick the
optimal output from E*, based on criteria of well-formedness (transition probabilities)
and faithfulness to the input (emission probabilities). But typical OT grammars offer
much richer finite-state models of left context (Eisner 1997a) than provided by the
traditional HMM finite-state topologies.
Now, among methods that use a Gibbs distribution to choose among linguistic
forms, OT generation is special in that the distribution ranks the features strictly, rather
than weighting them in a gentler way that allows tradeoffs. When is this appropriate?
It seems to me that there are three possible uses.
First, there are categorical phenomena for which strict feature ranking may gen-
uinely suffice. As Kager demonstrates in this textbook, phonology may well fall into
this class—although the claim depends on what features are allowed, and Kager aptly
notes that some phonologists have tried to sneak gang effects in the back door by
allowing high-ranked conjunctions of low-ranked features. Several syntacticians have
also been experimenting with OT; Kager devotes a chapter to Grimshaw&apos;s seminal
paper (1997) on verb movement and English do-support. Orthography (i.e., text-to-
speech) and punctuation may also be suited to OT analysis.
2 Each constraint/feature is weighted so highly that it can overwhelm the total of all lower-ranked
constraints, and even the lowest-ranked constraint is weighted very highly. Recall that the
incompatibility of some feature combinations (i.e., nonorthogonality of features) is always what makes
it nontrivial to normalize or sample a Gibbs distribution, just as it makes it nontrivial to find optimal
forms in OT.
</bodyText>
<page confidence="0.952031">
287
</page>
<note confidence="0.412044">
Computational Linguistics Volume 26, Number 2
</note>
<bodyText confidence="0.999555117647059">
Second, weights are an annoyance when writing grammars by hand. In some cases
rankings may work well enough. Samuelsson and Voutilainen (1997) report excellent
part-of-speech tagging results using a handcrafted approach that is close to OT.3 More
speculatively, imagine an OT grammar for stylistic revision of parsed sentences. The
tension between preserving the original author&apos;s text (faithfulness to the underlying
form) and making it readable in various ways (well-formedness) is right up OT&apos;s alley.
The same applies to document layout: I have often wished I could write OT-style TeX
macros!
Third, even in statistical corpus-based NLP, estimating a full Gibbs distribution
is not always feasible. Even if strict ranking is not quite accurate, sparse data or the
complexity of parameter estimation may make it easier to learn a good OT grammar
than a good arbitrary Gibbs model. A well-known example is Yarowsky&apos;s (1996) work
on word sense disambiguation using decision lists (a kind of OT grammar). Although
decision lists are not very powerful because of their simple output space, they have
the characteristic OT property that each generalization partially masks lower-ranked
generalizations.
Having established a context, we now return to phonology and the subject at hand.
</bodyText>
<listItem confidence="0.553893">
3. Goals and Strengths
</listItem>
<bodyText confidence="0.99937972">
Kager &apos;s textbook addresses linguists who are new to OT but who have a good work-
ing knowledge of phonological terminology and representations (preferably of work
through the mid-1980&apos;s, but the book ignores autosegmentalism and is careful to
review its assumptions about prosodic structure). This is a shrinking audience, as
phonology courses are increasingly integrating OT from week one. But there are
surely many nonphonologists—computational linguists and others—who learned their
phonology years ago and would like to come up to date. In a graduate linguistics pro-
gram, the text might profitably be used in tandem with a derivational textbook such
as Kenstowicz (1993), or postponed until a second-semester course that explores OT
in more detail.
The book begins with a lucid introduction to the optimality-theoretic perspective
and its relation to other ideas. It even explains, deftly, why optimization over an infinite
candidate set is computationally feasible. It then proceeds through a series of themati-
cally grouped case studies that concern larger and larger phonological units. Chapter 2
focuses on segmental and featural effects, using Joe Pater &apos;s elegant demonstration of
how the *NC constraint is satisfied differently in different languages. Correspondence
Theory makes its first appearance here. Chapter 3 considers some effects of syllable
structure constraints. Chapter 4—the most ambitious in the book—discusses Kager &apos;s
own specialty, the optimization of metrical structure, whose effects on word shape are
not limited to stress. Chapter 5 moves up to morphological structure with the redu-
plicative facts that inspired Correspondence Theory; chapter 6 extends Correspondence
to entire morphological paradigms.
The remaining three chapters touch more frequently on open architectural is-
sues, e.g., the nature of the lexical input. Chapter 7 discusses Tesar and Smolensky&apos;s
constraint-ranking algorithms, with some preliminary suggestions by Kager about how
</bodyText>
<footnote confidence="0.983682">
3 Voutilainen&apos;s tagger follows OT in applying a succession of violable constraints to winnow the set of
possible tag sequences. But his constraints are only partially ranked (into five strata), and rather than
manage nondeterminism, as OT does, he waits to apply a constraint until the context it specifies has
been sufficiently disambiguated.
</footnote>
<page confidence="0.992352">
288
</page>
<subsectionHeader confidence="0.851249">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999941142857143">
to learn the lexicon. Chapter 8 reviews the previously mentioned syntax work by
Grimshaw. Finally, the thoughtful Chapter 9 evaluates proposals for some residual
formal issues in OT phonology. These include opacity, free variation, and the possibil-
ity of eliminating underlying forms altogether.
Kager is always clear and orderly in his presentation—the main strength of the
book. The discussion is organized around concrete examples from the pre- and post-
OT secondary literature. Each example is carefully selected to add a new constraint
or two to the soup. By the end of Chapter 8, the reader will have been exposed to a
judicious sampling of the best-known ideas in OT, and will be well-prepared to read
additional papers on their own.
The text keeps up a running discussion of the constraints used, how they interact
to produce the desired result, and—most usefully—the advantages and predictions
of the OT analysis. In several cases Kager even provides a rule-based analysis for
comparison.
</bodyText>
<sectionHeader confidence="0.921448" genericHeader="introduction">
4. Weaknesses
</sectionHeader>
<bodyText confidence="0.9999114">
The book contains a few minor editing errors (duplication of text) and technical errors
(in the analyses). On the computational front, it confuses the names of two learning
algorithms, and misrepresents the state of the art in OT generation: the crucial property
is that constraints be finite-state, not that they have bounded violations. (The latter
property is helpful but neither necessary nor sufficient by itself.)
A more serious concern is that reading this textbook feels quite a lot like reading
OT research papers. Of course, the book provides a much more efficient (though highly
selective) tour of OT, together with a small number of exercises. But does it do a good
job of training future researchers?
At a basic level, one would wish an OT textbook for derivational phonologists—
like a Prolog textbook for C programmers—to inculcate standards of accuracy and
good taste for the new paradigm. This is difficult to do without discussing examples
of poor analyses (and offering rules of thumb). Unfortunately, Kager tends to pull
perfect constraints out of his pocket as needed. The book therefore ignores two crucial
activities of the OT phonologist: searching for data that will distinguish among dif-
ferent precise formulations of a constraint (or different representational assumptions),
and proving that each attested form really beats all of its infinitely many competi-
tors.
At a more advanced level, OT is a living framework that we are still working out.
Empiricists as well as formalists need to understand what (tentative) choices were
made in getting us to this point, and what questions remain unresolved. Kager treats
a few such issues but only at the end of the book. Other volumes tend to highlight
these issues as they arise: the ur-text of OT (Prince and Smolensky 1993), and to
some extent, the undergraduate &amp;quot;textbook&amp;quot; edited by Archangeli and Langendoen
(1997).
</bodyText>
<sectionHeader confidence="0.927" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.9998335">
This well-written and organized if sometimes conservative textbook provides a view
of the current state of OT. Kager repeatedly shows how OT grammars can succeed in
motivating and unifying phenomena. This makes the book a good starting point if one
wishes to get a feel for constraint interaction in OT by looking at some real, exemplary
analyses, as suggested in Section 2 above. For classroom use, the book would ideally be
supplemented with in-class data analysis and discussion, and perhaps other readings.
</bodyText>
<page confidence="0.995308">
289
</page>
<note confidence="0.693748">
Computational Linguistics Volume 26, Number 2
</note>
<sectionHeader confidence="0.298057" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98833890140845">
Archangeli, Diana and D. Terence
Langendoen, editors. 1997. Optimality
Theory: An Overview. Explaining
Linguistics. Blackwell, Oxford.
Bird, Steven. 1995. Computational Phonology: A
Constraint-Based Approach. Cambridge
University Press.
Eisner, Jason. 1997a. Efficient generation in
primitive Optimality Theory. In Proceedings
of the 35th Annual Meeting of the Association
for Computational Linguistics and the 8th
Conference of the European Chapter of the
Association for Computational Linguistics,
Madrid, July.
Eisner, Jason. 1997b. What constraints should
OT allow? Talk handout, Linguistic Society
of America, Chicago, January. Available on
the Rutgers Optimality Archive,
http://ruccs.rutgers.edu/roa.html.
Grimshaw, Jane. 1997. Projection, heads, and
optimality. Linguistic Inquiry, 28:373-422.
Johnson, Mark. 2000. Context-sensitivity and
stochastic &amp;quot;unification-based&amp;quot; grammars.
Talk presented at the CLSP Seminar Series,
The Johns Hopkins University
Kenstowicz, Michael. 1993. Phonology in
Generative Grammar. Blackwell Textbooks
in Linguistics. Blackwell, Oxford.
Koskenniemi, Kimmo. 1983. Two-level
morphology: A general computational
model for word-form recognition and
production. Publication 11, Department of
General Linguistics, University of
Helsinki.
Paradis, Carole. 1988. On constraints and
repair strategies. Linguistic Review,
6:71-97.
Prince, Alan and Paul Smolensky. 1993.
Optimality theory: Constraint interaction
in generative grammar. Manuscript,
Rutgers University and University of
Colorado at Boulder.
Sadock, Jerrold M. 1985. Autolexical syntax:
A proposal for the treatment of noun
incorporation and similar phenomena.
Natural Language and Linguistic Theory,
3:379-439.
Samuelsson, Christer and Atro Voutilainen.
1997. Comparing a linguistic and a
stochastic tagger. In Proceedings of the 35th
Annual Meeting of the Association for
Computational Linguistics and the 8th
Conference of the European Chapter of the
Association for Computational Linguistics,
Madrid, July.
Smolensky, P. 1986. Information processing
in dynamical systems: Foundations of
harmony theory. In David E. Rumelhart
and James L. McClelland, editors, Parallel
Distributed Processing: Explorations in the
Microstructure of Cognition, volume 1. MIT
Press, pages 194-281.
Yarowsky, David. 1996. Three Machine
Learning Algorithms for Lexical Ambiguity
Resolution. Ph.D. thesis, University of
Pennsylvania.
Jason Eisner is an assistant professor of computer science at the University of Rochester, where he
works on statistical parsing and computational phonology. He is the architect of the Primitive
Optimality Theory (OTP) formalism for phonological constraints and representations. Eisner&apos;s
address is: Department of Computer Science, University of Rochester, P.O. Box 270226, Rochester,
NY, 14627-0226; e-mail: jason@cs.rochester.edu
</reference>
<page confidence="0.997075">
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358166">
<title confidence="0.875748">Computational Linguistics Volume 26, Number 2 Optimality Theory</title>
<author confidence="0.996852">René Kager</author>
<affiliation confidence="0.9964225">(Utrecht University) Cambridge University Press</affiliation>
<note confidence="0.902002333333333">(Cambridge textbooks in linguistics, edited by S.R. Anderson et al.), 1999, xiii+452 pp; hardbound, ISBN 0-521-58019-6, $64.95; paperbound, ISBN 0-521-58980-0, $24.95 Reviewed by</note>
<author confidence="0.999421">Jason Eisner</author>
<affiliation confidence="0.995699">University of Rochester</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Optimality Theory: An Overview. Explaining Linguistics.</title>
<date>1997</date>
<editor>Archangeli, Diana and D. Terence Langendoen, editors.</editor>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="5920" citStr="(1997)" startWordPosition="874" endWordPosition="874">that allows tradeoffs. When is this appropriate? It seems to me that there are three possible uses. First, there are categorical phenomena for which strict feature ranking may genuinely suffice. As Kager demonstrates in this textbook, phonology may well fall into this class—although the claim depends on what features are allowed, and Kager aptly notes that some phonologists have tried to sneak gang effects in the back door by allowing high-ranked conjunctions of low-ranked features. Several syntacticians have also been experimenting with OT; Kager devotes a chapter to Grimshaw&apos;s seminal paper (1997) on verb movement and English do-support. Orthography (i.e., text-tospeech) and punctuation may also be suited to OT analysis. 2 Each constraint/feature is weighted so highly that it can overwhelm the total of all lower-ranked constraints, and even the lowest-ranked constraint is weighted very highly. Recall that the incompatibility of some feature combinations (i.e., nonorthogonality of features) is always what makes it nontrivial to normalize or sample a Gibbs distribution, just as it makes it nontrivial to find optimal forms in OT. 287 Computational Linguistics Volume 26, Number 2 Second, w</context>
</contexts>
<marker>1997</marker>
<rawString>Archangeli, Diana and D. Terence Langendoen, editors. 1997. Optimality Theory: An Overview. Explaining Linguistics. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>Computational Phonology: A Constraint-Based Approach.</title>
<date>1995</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1144" citStr="Bird 1995" startWordPosition="149" endWordPosition="150">ensky, and McCarthy in 1993. OT reclaims traditional grammar&apos;s ability to express surface generalizations (&amp;quot;syllables have onsets,&amp;quot; &amp;quot;no nasal+voiceless obstruent clusters&amp;quot;). Empirically, some surface generalizations are robust within a language, or—perhaps for functionalist reasons— widespread across languages. Derivational theories were forced to posit diverse rules that rescued these robust generalizations from other phonological processes. An OT grammar avoids such &amp;quot;conspiracies&amp;quot; by stating the generalizations directly, as in TwoLevel Morphology (Koskenniemi 1983) or Declarative Phonology (Bird 1995). In OT, the processes that try but fail to disrupt a robust generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.1 Whether this is always possible un</context>
</contexts>
<marker>Bird, 1995</marker>
<rawString>Bird, Steven. 1995. Computational Phonology: A Constraint-Based Approach. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient generation in primitive Optimality Theory.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="1816" citStr="Eisner 1997" startWordPosition="250" endWordPosition="251">generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.1 Whether this is always possible under an appropriate definition of &amp;quot;simple constraints&amp;quot; (e.g., Eisner 1997b) is of course an empirical question. 2. Relevance Before looking at Kager &apos;s textbook in detail, it is worth pausing to ask what broader implications Optimality Theory might have for computational linguistics. If you are an academic phonologist, you already know OT by now. If you are not, should you take the time to learn? So far, OT has served CL mainly as a source of interesting new problems—both theoretical and (assuming a lucrative market for phonology workbench utilities) prac1 This style of analysis is shared by Autolexical Grammar (Sadock 1985), which has focused more on (morpho)synta</context>
<context position="5043" citStr="Eisner 1997" startWordPosition="739" endWordPosition="740">interaction, and about the usefulness of positing hidden structure (e.g., prosodic constituency) to which multiple features may refer. For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging. Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from E*, based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities). But typical OT grammars offer much richer finite-state models of left context (Eisner 1997a) than provided by the traditional HMM finite-state topologies. Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs. When is this appropriate? It seems to me that there are three possible uses. First, there are categorical phenomena for which strict feature ranking may genuinely suffice. As Kager demonstrates in this textbook, phonology may well fall into this class—although the claim depends on what features are allowe</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Eisner, Jason. 1997a. Efficient generation in primitive Optimality Theory. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics, Madrid, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>What constraints should OT allow? Talk handout, Linguistic Society of America,</title>
<date>1997</date>
<booktitle>Available on the Rutgers Optimality Archive,</booktitle>
<location>Chicago,</location>
<contexts>
<context position="1816" citStr="Eisner 1997" startWordPosition="250" endWordPosition="251">generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.1 Whether this is always possible under an appropriate definition of &amp;quot;simple constraints&amp;quot; (e.g., Eisner 1997b) is of course an empirical question. 2. Relevance Before looking at Kager &apos;s textbook in detail, it is worth pausing to ask what broader implications Optimality Theory might have for computational linguistics. If you are an academic phonologist, you already know OT by now. If you are not, should you take the time to learn? So far, OT has served CL mainly as a source of interesting new problems—both theoretical and (assuming a lucrative market for phonology workbench utilities) prac1 This style of analysis is shared by Autolexical Grammar (Sadock 1985), which has focused more on (morpho)synta</context>
<context position="5043" citStr="Eisner 1997" startWordPosition="739" endWordPosition="740">interaction, and about the usefulness of positing hidden structure (e.g., prosodic constituency) to which multiple features may refer. For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-speech tagging. Just like OT grammars, HMM Viterbi decoders are functions that pick the optimal output from E*, based on criteria of well-formedness (transition probabilities) and faithfulness to the input (emission probabilities). But typical OT grammars offer much richer finite-state models of left context (Eisner 1997a) than provided by the traditional HMM finite-state topologies. Now, among methods that use a Gibbs distribution to choose among linguistic forms, OT generation is special in that the distribution ranks the features strictly, rather than weighting them in a gentler way that allows tradeoffs. When is this appropriate? It seems to me that there are three possible uses. First, there are categorical phenomena for which strict feature ranking may genuinely suffice. As Kager demonstrates in this textbook, phonology may well fall into this class—although the claim depends on what features are allowe</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Eisner, Jason. 1997b. What constraints should OT allow? Talk handout, Linguistic Society of America, Chicago, January. Available on the Rutgers Optimality Archive, http://ruccs.rutgers.edu/roa.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<date>1997</date>
<booktitle>Projection, heads, and optimality. Linguistic Inquiry,</booktitle>
<pages>28--373</pages>
<marker>Grimshaw, 1997</marker>
<rawString>Grimshaw, Jane. 1997. Projection, heads, and optimality. Linguistic Inquiry, 28:373-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Context-sensitivity and stochastic &amp;quot;unification-based&amp;quot; grammars. Talk presented at the CLSP Seminar Series, The Johns Hopkins</title>
<date>2000</date>
<publisher>University</publisher>
<contexts>
<context position="4123" citStr="Johnson (2000)" startWordPosition="607" endWordPosition="608"> OT actually parallels a recent trend in statistical NLP: to describe natural language at all levels by specifying the relative importance of many conflicting surface features. This approach characterizes the family of probability distributions known variously as maximum-entropy models, log-linear models, Markov random fields, or Gibbs distributions. Indeed, such models were well known to one of the architects of OT (Smolensky 1986), and it is possible to regard an OT grammar as a limit case of a Gibbs distribution whose conditional probabilities p(surface form I underlying form) approach 1.2 Johnson (2000) has recently learned simple OT constraint rankings by fitting Gibbs distributions to unambiguous data. Gibbs distributions are broadly useful in NLP when their features are chosen well. So one might study OT simply to develop better intuitions about useful types of linguistic features and their patterns of interaction, and about the usefulness of positing hidden structure (e.g., prosodic constituency) to which multiple features may refer. For example, consider the relevance to hidden Markov models (HMMs), another restricted class of Gibbs distributions used in speech recognition or part-of-sp</context>
</contexts>
<marker>Johnson, 2000</marker>
<rawString>Johnson, Mark. 2000. Context-sensitivity and stochastic &amp;quot;unification-based&amp;quot; grammars. Talk presented at the CLSP Seminar Series, The Johns Hopkins University</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Kenstowicz</author>
</authors>
<title>Phonology in Generative Grammar. Blackwell Textbooks in Linguistics.</title>
<date>1993</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="8490" citStr="Kenstowicz (1993)" startWordPosition="1265" endWordPosition="1266">but who have a good working knowledge of phonological terminology and representations (preferably of work through the mid-1980&apos;s, but the book ignores autosegmentalism and is careful to review its assumptions about prosodic structure). This is a shrinking audience, as phonology courses are increasingly integrating OT from week one. But there are surely many nonphonologists—computational linguists and others—who learned their phonology years ago and would like to come up to date. In a graduate linguistics program, the text might profitably be used in tandem with a derivational textbook such as Kenstowicz (1993), or postponed until a second-semester course that explores OT in more detail. The book begins with a lucid introduction to the optimality-theoretic perspective and its relation to other ideas. It even explains, deftly, why optimization over an infinite candidate set is computationally feasible. It then proceeds through a series of thematically grouped case studies that concern larger and larger phonological units. Chapter 2 focuses on segmental and featural effects, using Joe Pater &apos;s elegant demonstration of how the *NC constraint is satisfied differently in different languages. Corresponden</context>
</contexts>
<marker>Kenstowicz, 1993</marker>
<rawString>Kenstowicz, Michael. 1993. Phonology in Generative Grammar. Blackwell Textbooks in Linguistics. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level morphology: A general computational model for word-form recognition and production.</title>
<date>1983</date>
<journal>Publication</journal>
<volume>11</volume>
<institution>Department of General Linguistics, University of Helsinki.</institution>
<contexts>
<context position="1107" citStr="Koskenniemi 1983" startWordPosition="144" endWordPosition="145">logy after it was introduced by Prince, Smolensky, and McCarthy in 1993. OT reclaims traditional grammar&apos;s ability to express surface generalizations (&amp;quot;syllables have onsets,&amp;quot; &amp;quot;no nasal+voiceless obstruent clusters&amp;quot;). Empirically, some surface generalizations are robust within a language, or—perhaps for functionalist reasons— widespread across languages. Derivational theories were forced to posit diverse rules that rescued these robust generalizations from other phonological processes. An OT grammar avoids such &amp;quot;conspiracies&amp;quot; by stating the generalizations directly, as in TwoLevel Morphology (Koskenniemi 1983) or Declarative Phonology (Bird 1995). In OT, the processes that try but fail to disrupt a robust generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Koskenniemi, Kimmo. 1983. Two-level morphology: A general computational model for word-form recognition and production. Publication 11, Department of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carole Paradis</author>
</authors>
<title>On constraints and repair strategies. Linguistic Review,</title>
<date>1988</date>
<pages>6--71</pages>
<contexts>
<context position="1265" citStr="Paradis 1988" startWordPosition="170" endWordPosition="171">have onsets,&amp;quot; &amp;quot;no nasal+voiceless obstruent clusters&amp;quot;). Empirically, some surface generalizations are robust within a language, or—perhaps for functionalist reasons— widespread across languages. Derivational theories were forced to posit diverse rules that rescued these robust generalizations from other phonological processes. An OT grammar avoids such &amp;quot;conspiracies&amp;quot; by stating the generalizations directly, as in TwoLevel Morphology (Koskenniemi 1983) or Declarative Phonology (Bird 1995). In OT, the processes that try but fail to disrupt a robust generalization are described not as rules (cf. Paradis 1988), but as lower-ranked generalizations. Such a generalization may fail in contexts where it is overruled by a higher-ranked requirement of the language (or of the underlying form). As Kager emphasizes, this interaction of violable constraints can yield complex surface patterns. OT therefore holds out the promise of simplifying grammars, by factoring all complex phenomena into simple surface-level constraints that partially mask one another.1 Whether this is always possible under an appropriate definition of &amp;quot;simple constraints&amp;quot; (e.g., Eisner 1997b) is of course an empirical question. 2. Relevan</context>
</contexts>
<marker>Paradis, 1988</marker>
<rawString>Paradis, Carole. 1988. On constraints and repair strategies. Linguistic Review, 6:71-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality theory: Constraint interaction in generative grammar.</title>
<date>1993</date>
<tech>Manuscript,</tech>
<institution>Rutgers University and University of Colorado at Boulder.</institution>
<contexts>
<context position="12966" citStr="Prince and Smolensky 1993" startWordPosition="1960" endWordPosition="1963">g for data that will distinguish among different precise formulations of a constraint (or different representational assumptions), and proving that each attested form really beats all of its infinitely many competitors. At a more advanced level, OT is a living framework that we are still working out. Empiricists as well as formalists need to understand what (tentative) choices were made in getting us to this point, and what questions remain unresolved. Kager treats a few such issues but only at the end of the book. Other volumes tend to highlight these issues as they arise: the ur-text of OT (Prince and Smolensky 1993), and to some extent, the undergraduate &amp;quot;textbook&amp;quot; edited by Archangeli and Langendoen (1997). 5. Conclusions This well-written and organized if sometimes conservative textbook provides a view of the current state of OT. Kager repeatedly shows how OT grammars can succeed in motivating and unifying phenomena. This makes the book a good starting point if one wishes to get a feel for constraint interaction in OT by looking at some real, exemplary analyses, as suggested in Section 2 above. For classroom use, the book would ideally be supplemented with in-class data analysis and discussion, and per</context>
</contexts>
<marker>Prince, Smolensky, 1993</marker>
<rawString>Prince, Alan and Paul Smolensky. 1993. Optimality theory: Constraint interaction in generative grammar. Manuscript, Rutgers University and University of Colorado at Boulder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerrold M Sadock</author>
</authors>
<title>Autolexical syntax: A proposal for the treatment of noun incorporation and similar phenomena. Natural Language and Linguistic Theory,</title>
<date>1985</date>
<pages>3--379</pages>
<contexts>
<context position="2375" citStr="Sadock 1985" startWordPosition="342" endWordPosition="343">definition of &amp;quot;simple constraints&amp;quot; (e.g., Eisner 1997b) is of course an empirical question. 2. Relevance Before looking at Kager &apos;s textbook in detail, it is worth pausing to ask what broader implications Optimality Theory might have for computational linguistics. If you are an academic phonologist, you already know OT by now. If you are not, should you take the time to learn? So far, OT has served CL mainly as a source of interesting new problems—both theoretical and (assuming a lucrative market for phonology workbench utilities) prac1 This style of analysis is shared by Autolexical Grammar (Sadock 1985), which has focused more on (morpho)syntax than phonology. 286 Book Reviews tical. To wit: Given constraints of a certain computational power (e.g., finite-state), how expressive is the class of OT grammars? How to generate the optimal surface form for a given underlying form? Or conversely, how to reconstruct an underlying form for which a given surface form is optimal? How can one learn a grammar and lexicon? Should we rethink our phonological representations? And how about variants of the OT framework? Many of the relevant papers are listed in ACL SIGPHON&apos;s computational OT bibliography at </context>
</contexts>
<marker>Sadock, 1985</marker>
<rawString>Sadock, Jerrold M. 1985. Autolexical syntax: A proposal for the treatment of noun incorporation and similar phenomena. Natural Language and Linguistic Theory, 3:379-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
<author>Atro Voutilainen</author>
</authors>
<title>Comparing a linguistic and a stochastic tagger.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="6653" citStr="Samuelsson and Voutilainen (1997)" startWordPosition="982" endWordPosition="985"> be suited to OT analysis. 2 Each constraint/feature is weighted so highly that it can overwhelm the total of all lower-ranked constraints, and even the lowest-ranked constraint is weighted very highly. Recall that the incompatibility of some feature combinations (i.e., nonorthogonality of features) is always what makes it nontrivial to normalize or sample a Gibbs distribution, just as it makes it nontrivial to find optimal forms in OT. 287 Computational Linguistics Volume 26, Number 2 Second, weights are an annoyance when writing grammars by hand. In some cases rankings may work well enough. Samuelsson and Voutilainen (1997) report excellent part-of-speech tagging results using a handcrafted approach that is close to OT.3 More speculatively, imagine an OT grammar for stylistic revision of parsed sentences. The tension between preserving the original author&apos;s text (faithfulness to the underlying form) and making it readable in various ways (well-formedness) is right up OT&apos;s alley. The same applies to document layout: I have often wished I could write OT-style TeX macros! Third, even in statistical corpus-based NLP, estimating a full Gibbs distribution is not always feasible. Even if strict ranking is not quite acc</context>
</contexts>
<marker>Samuelsson, Voutilainen, 1997</marker>
<rawString>Samuelsson, Christer and Atro Voutilainen. 1997. Comparing a linguistic and a stochastic tagger. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics, Madrid, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Smolensky</author>
</authors>
<title>Information processing in dynamical systems: Foundations of harmony theory.</title>
<date>1986</date>
<booktitle>Parallel Distributed Processing: Explorations in the Microstructure of Cognition,</booktitle>
<volume>1</volume>
<pages>194--281</pages>
<editor>In David E. Rumelhart and James L. McClelland, editors,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="3945" citStr="Smolensky 1986" startWordPosition="578" endWordPosition="579">ter point, since it offers a plausible account of variation in terms of constraint reranking. Unfortunately, complete grammars are still in short supply. Looking beyond phonology, OT actually parallels a recent trend in statistical NLP: to describe natural language at all levels by specifying the relative importance of many conflicting surface features. This approach characterizes the family of probability distributions known variously as maximum-entropy models, log-linear models, Markov random fields, or Gibbs distributions. Indeed, such models were well known to one of the architects of OT (Smolensky 1986), and it is possible to regard an OT grammar as a limit case of a Gibbs distribution whose conditional probabilities p(surface form I underlying form) approach 1.2 Johnson (2000) has recently learned simple OT constraint rankings by fitting Gibbs distributions to unambiguous data. Gibbs distributions are broadly useful in NLP when their features are chosen well. So one might study OT simply to develop better intuitions about useful types of linguistic features and their patterns of interaction, and about the usefulness of positing hidden structure (e.g., prosodic constituency) to which multipl</context>
</contexts>
<marker>Smolensky, 1986</marker>
<rawString>Smolensky, P. 1986. Information processing in dynamical systems: Foundations of harmony theory. In David E. Rumelhart and James L. McClelland, editors, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1. MIT Press, pages 194-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Three Machine Learning Algorithms for Lexical Ambiguity Resolution.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Yarowsky, 1996</marker>
<rawString>Yarowsky, David. 1996. Three Machine Learning Algorithms for Lexical Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jason</author>
</authors>
<title>Eisner is an assistant professor of computer science at the University of Rochester, where he works on statistical parsing and computational phonology. He is the architect of the Primitive Optimality Theory (OTP) formalism for phonological constraints and representations. Eisner&apos;s address is:</title>
<date>1462</date>
<institution>Department of Computer Science, University of Rochester,</institution>
<location>P.O. Box 270226, Rochester, NY,</location>
<note>e-mail: jason@cs.rochester.edu</note>
<marker>Jason, 1462</marker>
<rawString>Jason Eisner is an assistant professor of computer science at the University of Rochester, where he works on statistical parsing and computational phonology. He is the architect of the Primitive Optimality Theory (OTP) formalism for phonological constraints and representations. Eisner&apos;s address is: Department of Computer Science, University of Rochester, P.O. Box 270226, Rochester, NY, 14627-0226; e-mail: jason@cs.rochester.edu</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>