<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000035">
<title confidence="0.990805">
Representing Topics Using Images
</title>
<author confidence="0.987943">
Nikolaos Aletras and Mark Stevenson
</author>
<affiliation confidence="0.99807">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.962427">
Regent Court, 211 Portobello
Sheffield, S1 4DP, UK
</address>
<email confidence="0.999135">
{n.aletras, m.stevenson}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999333055555556">
Topics generated automatically, e.g. using
LDA, are now widely used in Computational
Linguistics. Topics are normally represented
as a set of keywords, often the n terms in a
topic with the highest marginal probabilities.
We introduce an alternative approach in which
topics are represented using images. Candi-
date images for each topic are retrieved from
the web by querying a search engine using the
top n terms. The most suitable image is se-
lected from this set using a graph-based al-
gorithm which makes use of textual informa-
tion from the metadata associated with each
image and features extracted from the images
themselves. We show that the proposed ap-
proach significantly outperforms several base-
lines and can provide images that are useful to
represent a topic.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999852083333333">
Topic models are statistical methods for summaris-
ing the content of a document collection using latent
variables known as topics (Hofmann, 1999; Blei et
al., 2003). Within a model, each topic is a multino-
mial distribution over words in the collection while
documents are represented as distributions over top-
ics. Topic modelling is now widely used in Natural
Language Processing (NLP) and has been applied to
a range of tasks including word sense disambigua-
tion (Boyd-Graber et al., 2007), multi-document
summarisation (Haghighi and Vanderwende, 2009),
information retrieval (Wei and Croft, 2006), image
labelling (Feng and Lapata, 2010a) and visualisation
of document collections (Chaney and Blei, 2012).
Topics are often represented by using the n terms
with the highest marginal probabilities in the topic to
generate a set of keywords. For example, wine, bot-
tle, grape, flavour, dry. Interpreting such lists may
not be straightforward, particularly since there may
be no access to the source collection used to train the
model. Therefore, researchers have recently begun
developing automatic methods to generate meaning-
ful and representative labels for topics. These tech-
niques have focussed on the creation of textual la-
bels (Mei et al., 2007; Lau et al., 2010; Lau et al.,
2011).
An alternative approach is to represent a topic us-
ing an illustrative image (or set of images). Im-
ages have the advantage that they can be under-
stood quickly and are language independent. This is
particularly important for applications in which the
topics are used to provide an overview of a collec-
tion with many topics being shown simultaneously
(Chaney and Blei, 2012; Gretarsson et al., 2012;
Hinneburg et al., 2012).
This paper explores the problem of selecting im-
ages to illustrate automatically generated topics.
Our approach generates a set of candidate images for
each topic by querying an image search engine with
the top n topic terms. The most suitable image is
selected using a graph-based method that makes use
of both textual and visual information. Textual in-
formation is obtained from the metadata associated
with each image while visual features are extracted
from the images themselves. Our approach is evalu-
ated using a data set created for this study that was
annotated by crowdsourcing. Results of the evalu-
ation show that the proposed method significantly
</bodyText>
<page confidence="0.969426">
158
</page>
<note confidence="0.490939">
Proceedings of NAACL-HLT 2013, pages 158–167,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.95194875">
outperforms three baselines.
The contributions of this paper are as follows: (1)
introduces the problem of labelling topics using im-
ages; (2) describes an approach to this problem that
makes use of multimodal information to select im-
ages from a set of candidates; (3) introduces a data
set to evaluate image labelling; and (4) evaluates the
proposed approach using this data set.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999956891566265">
In early research on topic modelling, labels were
manually assigned to topics for convenient presen-
tation of research results (Mei and Zhai, 2005; Teh
et al., 2006).
The first attempt at automatically assigning la-
bels to topics is described by Mei et al. (2007).
In their approach, a set of candidate labels are ex-
tracted from a reference collection using chunking
and statistically important bigrams. Then, a rele-
vance scoring function is defined which minimises
the Kullback-Leibler divergence between word dis-
tribution in a topic and word distribution in candi-
date labels. Candidate labels are ranked according
to their relevance and the top ranked label chosen to
represent the topic.
Magatti et al. (2009) introduced an approach
for labelling topics that relied on two hierarchi-
cal knowledge resources labelled by humans, the
Google Directory and the OpenOffice English The-
saurus. A topics tree is a pre-existing hierarchi-
cal structure of labelled topics. The Automatic La-
belling Of Topics algorithm computes the similarity
between LDA inferred topics and topics in a topics
tree by computing scores using six standard similar-
ity measures. The label for the most similar topic in
the topic tree is assigned to the LDA topic.
Lau et al. (2010) proposed selecting the most rep-
resentative word from a topic as its label. A la-
bel is selected by computing the similarity between
each word and all the others in the topic. Sev-
eral sources of information are used to identify the
best label including Pointwise Mutual Information
scores, WordNet hypernymy relations and distribu-
tional similarity. These features are combined in a
reranking model to achieve results above a baseline
(the most probable word in the topic).
In more recent work, Lau et al. (2011) proposed
a method for automatically labelling topics by mak-
ing use of Wikipedia article titles as candidate la-
bels. The candidate labels are ranked using infor-
mation from word association measures, lexical fea-
tures and an Information Retrieval technique. Re-
sults showed that this ranking method achieves bet-
ter performance than a previous approach (Mei et al.,
2007).
Mao et al. (2012) introduced a method for la-
belling hierarchical topics which makes use of sib-
ling and parent-child relations of topics. Candidate
labels are generated using a similar approach to the
one used by Mei et al. (2007). Each candidate la-
bel is then assigned a score by creating a distribu-
tion based on the words it contains and measuring
the Jensen-Shannon divergence between this and a
reference corpus.
Hulpus et al. (2013) make use of the structured
data in DBpedia1 to label topics. Their approach
maps topic words to DBpedia concepts. The best
concepts are identified by applying graph central-
ity measures which assume that words that co-
occurring in text are likely to refer to concepts that
are close in the DBpedia graph.
Our own work differs from the approaches de-
scribed above since, to our knowledge, it is the first
to propose labelling topics with images rather than
text.
Recent advances in computer vision has lead to
the development of reliable techniques for exploit-
ing information available in images (Datta et al.,
2008; Szeliski, 2010) and these have been combined
with NLP (Feng and Lapata, 2010a; Feng and Lap-
ata, 2010b; Agrawal et al., 2011; Bruni et al., 2011).
The closest work to our own is the text illustration
techniques which have been proposed for story pic-
turing (Joshi et al., 2006) and news articles illustra-
tion (Feng and Lapata, 2010b). The input to text il-
lustration models is a textual document and a set of
image candidates. The goal of the models is to as-
sociate the document with the correct image. More-
over, the problem of ranking images returned from
a text query is related to, but different from, the
one explored in our paper. Those approaches used
queries that were much smaller (e.g. between one
and three words) and more focussed than the ones
</bodyText>
<footnote confidence="0.99827">
1http://dbpedia.org
</footnote>
<page confidence="0.99864">
159
</page>
<bodyText confidence="0.9994345">
we use (Jing and Baluja, 2008). In our work, the in-
put is a topic and the aim is to associate it with an
image, or images, denoting the main thematic sub-
ject.
</bodyText>
<sectionHeader confidence="0.904642" genericHeader="method">
3 Labelling Topics
</sectionHeader>
<bodyText confidence="0.999934333333333">
In this section we propose an approach to identify-
ing images to illustrate automatically generated top-
ics. It is assumed that there are no candidate images
available so the first step (Section 3.1) is to generate
a set of candidate images. However, when a candi-
date set is available the first step can be skipped.
</bodyText>
<subsectionHeader confidence="0.999923">
3.1 Selecting Candidate Images
</subsectionHeader>
<bodyText confidence="0.999983555555555">
For the experiments presented here we restrict our-
selves to using images from Wikipedia available un-
der the Creative Commons licence, since this allows
us to make the data available. The top-5 terms from
a topic are used to query Google using its Custom
Search API2. The search is restricted to the English
Wikipedia3 with image search enabled. The top-20
images retrieved for each search are used as candi-
dates for the topic.
</bodyText>
<subsectionHeader confidence="0.99977">
3.2 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.9998615">
Candidate images are represented by two modalities
(textual and visual) and features extracted for each.
</bodyText>
<subsectionHeader confidence="0.516087">
3.2.1 Textual Information
</subsectionHeader>
<bodyText confidence="0.9999818">
Each image’s textual information consists of the
metadata retrieved by the search. The assumption
here is that image’s metadata is indicative of the im-
age’s content and (at least to some extent) related to
the topic. The textual information is formed by con-
catenating the title and the link fields of the search
result. These represent, respectively, the web page
title containing the image and the image file name.
The textual information is preprocessed by tokeniz-
ing and removing stop words.
</bodyText>
<subsubsectionHeader confidence="0.721691">
3.2.2 Visual Information
</subsubsectionHeader>
<bodyText confidence="0.999658">
Visual information is extracted using low-level
image keypoint descriptors, i.e. SIFT features
</bodyText>
<footnote confidence="0.999872333333333">
2https://developers.google.com/
apis-explorer/#s/customsearch/v1
3http://en.wikipedia.org
</footnote>
<bodyText confidence="0.996726615384616">
(Lowe, 1999; Lowe, 2004) sensitive to colour in-
formation. SIFT features denote “interesting” ar-
eas in an image. Image features are extracted us-
ing dense sampling and described using Opponent
colour SIFT descriptors provided by the colorde-
scriptor4 software. Opponent colour SIFT descrip-
tors have been found to give the best performance
in object scene and face recognition (Sande et al.,
2008). The SIFT features are clustered to form a vi-
sual codebook of 1,000 visual words using K-Means
such that each feature is mapped to a visual word.
Each image is represented as a bag-of-visual words
(BOVW).
</bodyText>
<subsectionHeader confidence="0.999819">
3.3 Ranking Candidate Images
</subsectionHeader>
<bodyText confidence="0.9999605">
We rank images in the candidates set using graph-
based algorithms. The graph is created by treating
images as nodes and using similarity scores (textual
or visual) between images to weight the edges.
</bodyText>
<subsectionHeader confidence="0.888592">
3.3.1 PageRank
</subsectionHeader>
<bodyText confidence="0.999982785714286">
PageRank (Page et al., 1999) is a graph-based al-
gorithm for identifying important nodes in a graph
that was originally developed for assigning impor-
tance to web pages. It has been used for a range
of NLP tasks including word sense disambiguation
(Agirre and Soroa, 2009) and keyword extraction
(Mihalcea and Tarau, 2004).
Let G = (V, E) be a graph with a set of ver-
tices, V , denoting image candidates and a set of
edges, E, denoting similarity scores between two
images. For example, sim(Vi, Vj) indicates the sim-
ilarity between images Vi and Vj. The PageRank
score (Pr) over G for an image (Vi) can be com-
puted by the following equation:
</bodyText>
<equation confidence="0.98252375">
�
Pr(Vi) = d ·
Vj EC(Vi)
(1)
</equation>
<bodyText confidence="0.999940333333333">
where QVi) denotes the set of vertices which are
connected to the vertex Vi. d is the damping factor
which is set to the default value of d = 0.85 (Page et
al., 1999). In standard PageRank all elements of the
vector v are the same, 1 � where N is the number of
nodes in the graph.
</bodyText>
<footnote confidence="0.6783965">
4http://koen.me/research/
colordescriptors
</footnote>
<equation confidence="0.9914762">
sim(Vi, Vj)
Pr(Vj) + (1 − d)v
E
VkEC(Vj )
sim(Vj, Vk)
</equation>
<page confidence="0.977143">
160
</page>
<subsectionHeader confidence="0.477785">
3.3.2 Personalised PageRank
</subsectionHeader>
<bodyText confidence="0.999584">
Personalised PageRank (PPR) (Haveliwala et al.,
2003) is a variant of the PageRank algorithm in
which extra importance is assigned to certain ver-
tices in the graph. This is achieved by adjusting the
values of the vector v in equation 1 to prefer certain
nodes. Nodes that are assigned high values in v are
more likely to also be assigned a high PPR score.
We make use of PPR to prefer images with textual
information that is similar to the terms in the topic.
</bodyText>
<subsectionHeader confidence="0.993749">
3.3.3 Weighting Graph Edges
</subsectionHeader>
<bodyText confidence="0.999944424242424">
Three approaches were compared for computing
the values of sim(V�,Vj) in equation 1 used to
weight the edges of the graph. Two of these make
use of the textual information associated with each
image while the final one relies on visual features.
The first approach is Pointwise Mutual Infor-
mation (PMI). The similarity between a pair of
images (vertices in the graph) is computed as the
average PMI between the terms in their metadata.
PMI is computed using word co-occurrence counts
over Wikipedia identified using a sliding window of
length 20. We also experimented with other word
association measures but these did not perform as
well. The PageRank over the graph weighted using
PMI is denoted as PRPMI.
The second approach, Explicit Semantic Anal-
ysis (ESA) (Gabrilovich and Markovitch, 2007), is
a knowledge-based similarity measure. ESA trans-
forms the text from the image metadata into vectors
that consist of Wikipedia article titles weighted by
their relevance. The similarity score between these
vectors is computed as the cosine of the angle be-
tween them. This similarity measure is used to cre-
ate the graph and its PageRank is denoted as PRESA.
The final approach uses the visual features ex-
tracted from the images themselves. The visual
words extracted from the images are used to form
feature vectors and the similarity between a pair of
images computed as the cosine of the angle between
them. The PageRank of the graph created using this
approach is PRvis and it is similar to the approach
proposed by Jing and Baluja (2008) for associating
images to text queries.
</bodyText>
<subsectionHeader confidence="0.903472">
3.3.4 Initialising the Personalisation Vector
</subsectionHeader>
<bodyText confidence="0.999968125">
The personalisation vector (see above) is
weighted using the similarity scores computed be-
tween the topic and its image candidates. Similarity
is computed using PMI and ESA (see above). When
PMI and ESA are used to weight the personalisation
vector they compute the similarity between the
top 10 terms for a topic and the textual metadata
associated with each image in the set of candidates.
We refer to the personalisation vectors created
using PMI and ESA as Per(PMI) and Per(ESA)
respectively.
Using PPR allows information about the simi-
larity between the images’ metadata and the topics
themselves to be considered when identifying a suit-
able image label. The situation is different when
PageRank is used since this only considers the sim-
ilarity between the images in the candidate set.
The personalisation vector used by PPR is em-
ployed in combination with a graph created us-
ing one of the approaches described above. For
example, the graph may be weighted using vi-
sual features and the personalisation vector created
using PMI scores. This approach is denoted as
PRvis+Per(PMI).
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999907857142857">
This section discusses the experimental design for
evaluating the proposed approaches to labelling top-
ics with images. To our knowledge no data set for
evaluating these approaches is currently available
and consequently we developed one for this study5.
Human judgements about the suitability of images
are obtained through crowdsourcing.
</bodyText>
<subsectionHeader confidence="0.968357">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.998164">
We created a data set of topics from two collections
which cover a broad thematic range:
</bodyText>
<listItem confidence="0.997955833333333">
• NYT 47,229 New York Times news articles
(included in the GigaWord corpus) that were
published between May and December 2010.
• WIKI A set of Wikipedia categories randomly
selected by browsing its hierarchy in a breadth-
first-search manner starting from a few seed
</listItem>
<footnote confidence="0.991479">
5Data set can be downloaded from http://staffwww.
dcs.shef.ac.uk/people/N.Aletras/resources.
html.
</footnote>
<page confidence="0.992206">
161
</page>
<bodyText confidence="0.473347">
police, officer, crime, street, man, city, gang, suspect, arrested, violence
game, season, team, patriot, bowl, nfl, quarterback, week, play, jet
</bodyText>
<figureCaption confidence="0.994105">
Figure 1: A sample of topics and their top-3 image candidates (i.e. with the highest average human annota-
tions).
</figureCaption>
<bodyText confidence="0.704013307692308">
military, afghanistan, force, official, afghan, defense, pentagon, american, war, gates
categories (e.g. SPORTS, POLITICS, COMPUT-
ING). Categories that have more that 80 articles
associated with them are considered. These
articles are collected to produce a corpus of
approximately 60,000 articles generated from
1,461 categories.
Documents in the two collections are tokenised
and stop words removed. LDA was applied to learn
200 topics from NYT and 400 topics from WIKI.
The gensim package6 was used to implement and
compute LDA. The hyperparameters (α, Q) were set
to num of topics. Incoherent topics are filtered out
</bodyText>
<equation confidence="0.685765">
1
</equation>
<bodyText confidence="0.999873428571429">
by applying the method proposed by Aletras and
Stevenson (2013).
We randomly selected 100 topics from NYT and
200 topics from WIKI resulting in a data set of 300
topics. Candidate images for these topics were gen-
erated using the approach described in Section 3.1,
producing a total of 6,000 candidate images (20 for
</bodyText>
<footnote confidence="0.886342">
6http://pypi.python.org/pypi/gensim
</footnote>
<bodyText confidence="0.698826">
each topic).
</bodyText>
<subsectionHeader confidence="0.985554">
4.2 Human Judgements of Image Relevance
</subsectionHeader>
<bodyText confidence="0.9999106875">
Human judgements of the suitability of each im-
age were obtained using an online crowdsourcing
platform, Crowdflower7. Annotators were provided
with a topic (represented as a set of 10 keywords)
and a candidate image. They were asked to judge
how appropriate the image was as a representation
of the main subject of the topic and provide a rating
on a scale of 0 (completely unsuitable) to 3 (very
suitable).
Quality control is important in crowdscourcing
experiments to ensure reliability (Kazai, 2011). To
avoid random answers, control questions with obvi-
ous answer were included in the survey. Annotations
by participants that failed to answer these questions
correctly or participants that gave the same rating for
all pairs were removed.
</bodyText>
<footnote confidence="0.991146">
7http://crowdflower.com
</footnote>
<page confidence="0.997168">
162
</page>
<bodyText confidence="0.9818996875">
The total number of filtered responses obtained
was 62,221 from 273 participants. Each topic-
image pair was rated at least by 10 subjects. The
average response for each pair was calculated in or-
der to create the final similarity judgement for use as
a gold-standard. The average variance across judges
(excluding control questions) is 0.88.
Inter-Annotator agreement (IAA) is computed as
the average Spearman’s p between the ratings given
by an annotator and the average ratings given by all
other annotators. The average IAA across all topics
was 0.50 which indicates the difficulty of the task,
even for humans.
Figure 1 shows three example topics from the data
set together with the images that received the highest
average score from the annotators.
</bodyText>
<subsectionHeader confidence="0.990917">
4.3 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999903421052631">
Evaluation of the topic labelling methods is carried
out using a similar approach to the framework pro-
posed by Lau et al. (2011) for labelling topics using
textual labels.
Top-1 average rating is the average human rating
assigned to the top-ranked label proposed by the sys-
tem. This provides an indication of the overall qual-
ity of the image the system judges as the best one.
The highest possible score averaged across all top-
ics is 2.68, since for many topics the average score
obtained from the human judgements is lower than
3.
The second evaluation measure is the normalized
discounted cumulative gain (nDCG) (Jarvelin and
Kekalainen, 2002; Croft et al., 2009) which com-
pares the label ranking proposed by the system to
the optimal ranking provided by humans. The dis-
counted cumulative gain at position p (DCGp) is
computed using the following equation:
</bodyText>
<equation confidence="0.9970755">
reli (2)
lo92(i)
</equation>
<bodyText confidence="0.9999535">
where reli is the relevance of the label to the topic
in position i. Then nDCG is computed as:
</bodyText>
<equation confidence="0.860906666666667">
DCGp
nDCGp = (3)
IDCGp
</equation>
<bodyText confidence="0.9999715">
where IDCGp is the optimal ranking of the image
labels, in our experiments this is the ranking pro-
vided by the scores in the human annotated data set.
We follow Lau et al. (2011) in computing nDCG-1,
nDCG-3 and nDCG-5 for the top 1, 3 and 5 ranked
system image labels respectively.
</bodyText>
<subsectionHeader confidence="0.995345">
4.4 Baselines
</subsectionHeader>
<bodyText confidence="0.9999625">
Since there are no previous methods for labelling
topics using images, we compare our proposed mod-
els against three baselines.
The Random baseline randomly selects a label
for the topic from the 20 image candidates. The pro-
cess is repeated 10,000 times and the average score
of the selected labels is computed for each topic.
The more informed Word Overlap baseline se-
lects the image that is most similar to the topic terms
by applying a Lesk-style algorithm (Lesk, 1986) to
compare metadata for each image against the topic
terms. It is defined as the number of common terms
between a topic and image candidate normalised by
the total number of terms in the topic and image’s
metadata.
We also compared our approach with the ranking
returned by the Google Image Search for the top-20
images for a specific topic.
</bodyText>
<subsectionHeader confidence="0.995759">
4.5 User Study
</subsectionHeader>
<bodyText confidence="0.999996888888889">
A user study was conducted to estimate human per-
formance on the image selection task. Three annota-
tors were recruited and asked to select the best image
for each of the 300 topics in the data set. The anno-
tators were provided with the topic (in the form of a
set of keywords) and shown all candidate images for
that topic before being asked to select exactly one.
The Average Top-1 Rating was computed for each
annotator and the mean of these values was 2.24.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.9999015">
Table 1 presents the results obtained for each of the
methods on the collection of 300 topics. Results are
shown for both Top-1 Average rating and nDCG.
We begin by discussing the results obtained us-
ing the standard PageRank algorithm applied to
graphs weighted using PMI, ESA and visual features
(PRPMI, PRESA and PRvis respectively). Results us-
ing PMI consistently outperform all baselines and
those obtained using ESA. This suggests that distri-
butional word association measures are more suit-
able for identifying useful images than knowledge-
based similarity measures. The best results using
</bodyText>
<equation confidence="0.880625">
p
DCGp = rel1 + L
i=2
</equation>
<page confidence="0.995994">
163
</page>
<table confidence="0.996541588235294">
Model Top-1 Av. Rating nDCG-1 nDCG-3 nDCG-5
Baselines
Random 1.79 - - -
Word Overlap 1.85 0.69 0.72 0.74
Google Image Search 1.89 0.73 0.75 0.77
PageRank
PRPMI 1.87 0.70 0.73 0.75
PRESA 1.81 0.67 0.68 0.70
PRvis 1.96 0.73 0.75 0.76
Personalised PageRank
PRPMI+Per(PMI) 1.98 0.74 0.76 0.77
PRPMI+Per(ESA) 1.92 0.70 0.72 0.74
PRESA+Per(PMI) 1.91 0.70 0.72 0.73
PRESA+Per(ESA) 1.88 0.69 0.72 0.74
PRvis+Per(PMI) 2.00 0.74 0.75 0.76
PRvis+Per(ESA) 1.94 0.72 0.75 0.76
User Study 2.24 – – –
</table>
<tableCaption confidence="0.999534">
Table 1: Results for various approaches to topic labelling.
</tableCaption>
<bodyText confidence="0.9998982">
standard PageRank are obtained when the visual
similarity measures are used to weight the graph,
with performance that significantly outperforms the
word overlap baseline (paired t-test, p &lt; 0.05). This
demonstrates that visual features are a useful source
of information for deciding which images are suit-
able topic labels.
The Personalised version of PageRank produces
consistently higher results compared to standard
PageRank, demonstrating that the additional infor-
mation provided by comparing the image metadata
with the topics is useful for this task. The best
results are obtained when the personalisation vec-
tor is weighted using PMI (i.e. Per(PMI)). The
best overall result for the top-1 average rating (2.00)
is obtained when the graph is weighted using vi-
sual features and the personalisation vector using the
PMI scores (PRvis+Per(PMI)) while the best results
for the various DCG metrics are produced when
both the graph and the personalisation vector are
weighted using PMI scores (PRPMI+Per(PMI)). In
addition, these two methods, PRvis+Per(PMI) and
PRPMI+Per(PMI), perform significantly better than
the word overlap and the Google Image Search base-
lines (p &lt; 0.01 and p &lt; 0.05 respectively). Weight-
ing the personalisation vector using ESA consis-
tently produces lower performance compared to
PMI. These results indicate that graph-based meth-
ods for ranking images are useful for illustrating top-
ics.
</bodyText>
<sectionHeader confidence="0.998823" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999809909090909">
Figure 2 shows a sample of three topics together
with the top-3 candidates (left-to-right) selected by
applying the PRvis+Per(PMI) approach. Reasonable
labels have been selected for the first two topics. On
the other hand, the images selected for the third topic
do not seem to be as appropriate.
We observed that inappropriate labels can be gen-
erated for two reasons. Firstly, the topic may be ab-
stract and difficult to illustrate. For example, one of
the topics in our data set refers to the subject AL-
GEBRAIC NUMBER THEORY and contains the terms
number, ideal, group, field, theory, algebraic, class,
ring, prime, theorem. It is difficult to find a represen-
tative image for topics such as this one. Secondly,
there are topics for which none of the candidate im-
ages returned by the search engine is relevant. An
example of a topic like this in our data set is one
that refers to PLANTS and contains the terms family,
sources, plants, familia, order, plant, species, taxon-
omy, classification, genera. The images returned by
the search engine include pictures of the Sagrada Fa-
milia cathedral in Barcelona, a car called “Familia”
</bodyText>
<page confidence="0.997837">
164
</page>
<figureCaption confidence="0.681918333333333">
Figure 2: A sample of topics and their top-3 images selected by applying the the PRvis+Per(PMI) approach
(left side) and the ones with the highest average human annotations (right side). The number under each
image represents its average human annotations score.
</figureCaption>
<figure confidence="0.989378">
2.3 2.7 2.5
2.8 2.8 2.73
2.1 2.6 2.7
2.83 2.8 2.8
1.91 1.7 1.6
1.0 1.2 0.2
</figure>
<bodyText confidence="0.9094595">
dance, ballet, dancer, swan, company, dancing, nutcracker, balanchine, ballerina, choreographer
wine, bottle, grape, flavor, dry, vineyard, curtis, winery, sweet, champagne
haiti, haitian, earthquake, paterson, jean, prince, governor, au, cholera, country
and pictures of families but no pictures of plants.
</bodyText>
<sectionHeader confidence="0.997875" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999987076923077">
This paper explores the use of images to represent
automatically generated topics. An approach to se-
lecting appropriate images was described. This be-
gins by identifying a set of candidate images us-
ing a search engine and then attempts to select the
most suitable. Images are ranked using a graph-
based method that makes use of both textual and
visual information. Evaluation is carried out on a
data set created for this study. The results show that
the visual features are a useful source of information
for this task while the proposed graph-based method
significantly outperforms several baselines.
This paper demonstrates that it is possible to iden-
tify images to illustrate topics. A possible applica-
tion for this technique is to represent the contents
of large document collections in a way that supports
rapid interpretation and can be used to enable nav-
igation (Chaney and Blei, 2012; Gretarsson et al.,
2012; Hinneburg et al., 2012). We plan to explore
this possibility in future work. Other possible exten-
sions to this work include exploring alternative ap-
proaches to generating candidate images and devel-
oping techniques to automatically identify abstract
topics for which suitable images are unlikely to be
found, thereby avoiding the problem cases described
in Section 6.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998655833333333">
The research leading to these results was
carried out as part of the PATHS project
(http://paths-project.eu) funded by
the European Community’s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 270082.
</bodyText>
<page confidence="0.998291">
165
</page>
<sectionHeader confidence="0.991004" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.5832172">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL ’09), pages 33–41, Athens, Greece.
</bodyText>
<reference confidence="0.999324396039604">
Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan,
and Krishnaram Kenthapadi. 2011. Enriching text-
books with images. In Proceedings of the 20th ACM
International Conference on Information and Knowl-
edge Management (CIKM ’11), pages 1847–1856,
Glasgow, Scotland, UK.
Nikolaos Aletras and Mark Stevenson. 2013. Evaluat-
ing topic coherence using distributional semantics. In
Proceedings of the 10th International Conference on
Computational Semantics (IWCS ’13) – Long Papers,
pages 13–22, Potsdam, Germany.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007.
A topic model for word sense disambiguation. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
’07), pages 1024–1033, Prague, Czech Republic.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the Workshop on GEometrical Models of
Natural Language Semantics (GEMS ’11), pages 22–
32, Edinburgh, UK.
Allison June-Barlow Chaney and David M. Blei. 2012.
Visualizing topic models. In Proceedings of the Sixth
International AAAI Conference on Weblogs and Social
Media, Dublin, Ireland.
Bruce W. Croft, Donald Metzler, and Trevor Strohman.
2009. Search engines: Information retrieval in prac-
tice. Addison-Wesley.
Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang.
2008. Image Retrieval: Ideas, Influences, and Trends
of the New Age. ACM Computing Surveys, 40(2):1–
60.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? Automatic caption gener-
ation for news images. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1239–1249, Uppsala, Sweden.
Yansong Feng and Mirella Lapata. 2010b. Topic Models
for Image Annotation and Text Illustration. In Pro-
ceedings of Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
831–839, Los Angeles, California.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the In-
ternational Joint Conference on Artificial Intelligence
(IJCAI ’07), pages 1606–1611, Hyberabad, India.
Brynjar Gretarsson, John O’Donovan, Svetlin Bostand-
jiev, Tobias H¨ollerer, Arthur Asuncion, David New-
man, and Padhraic Smyth. 2012. TopicNets: Vi-
sual analysis of large text corpora with topic modeling.
ACM Trans. Intell. Syst. Technol., 3(2):23:1–23:26.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362–370, Boulder, Colorado.
Taher Haveliwala, Sepandar Kamvar, and Glen Jeh.
2003. An analytical comparison of approaches to
personalizing PageRank. Technical Report 2003-35,
Stanford InfoLab.
Alexander Hinneburg, Rico Preiss, and Ren´e Schr¨oder.
2012. TopicExplorer: Exploring document collec-
tions with topic models. In Peter A. Flach, Tijl Bie,
and Nello Cristianini, editors, Machine Learning and
Knowledge Discovery in Databases, volume 7524 of
Lecture Notes in Computer Science, pages 838–841.
Springer Berlin Heidelberg.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR ’99), pages
50–57, Berkeley, California, United States.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. 2013. Unsupervised graph-based topic
labelling using DBpedia. In Proceedings of the 6th
ACM International Conference on Web Search and
Data Mining (WSDM ’13), pages 465–474, Rome,
Italy.
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Trans. Inf. Syst., 20(4):422–446.
Yushi Jing and Shumeet Baluja. 2008. PageRank for
product image search. In Proceedings of the 17th In-
ternational Conference on World Wide Web (WWW
’08), pages 307–316, Beijing, China.
Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The Story
Picturing Engine—A system for automatic text illus-
tration. ACM Trans. Multimedia Comput. Commun.
Appl., 2(1):68–89.
Gabriella Kazai. 2011. In search of quality in crowd-
sourcing for search engine evaluation. Advances in In-
formation Retrieval, pages 165–176.
Jey Han Lau, David Newman, Sarvnaz Karimi, and Tim-
othy Baldwin. 2010. Best topic word selection for
</reference>
<page confidence="0.985346">
166
</page>
<reference confidence="0.999627788732395">
topic labelling. In The 23rd International Conference
on Computational Linguistics (COLING ’10), pages
605–613, Beijing, China.
Jey Han Lau, Karl Grieser, David Newman, and Timothy
Baldwin. 2011. Automatic labelling of topic models.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1536–1545, Portland, Ore-
gon, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
5th Annual International Conference on Systems Doc-
umentation (SIGDOC ’86), pages 24–26, Toronto, On-
tario, Canada.
David G. Lowe. 1999. Object Recognition from Local
Scale-invariant Features. In Proceedings of the Sev-
enth IEEE International Conference on Computer Vi-
sion, pages 1150–1157, Kerkyra, Greece.
David G. Lowe. 2004. Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of
Computer Vision, 60(2):91–110.
Davide Magatti, Silvia Calegari, Davide Ciucci, and
Fabio Stella. 2009. Automatic Labeling of Topics.
In Proceedings of the 9th International Conference on
Intelligent Systems Design and Applications (ICSDA
’09), pages 1227–1232, Pisa, Italy.
Xian-Li Mao, Zhao-Yan Ming, Zheng-Jun Zha, Tat-Seng
Chua, Hongfei Yan, and Xiaoming Li. 2012. Auto-
matic labeling hierarchical topics. In Proceedings of
the 21st ACM International Conference on Informa-
tion and Knowledge Management (CIKM ’12), Shera-
ton, Maui Hawai.
Qiaozhu Mei and ChengXiang Zhai. 2005. Discovering
evolutionary theme patterns from text: an exploration
of temporal text mining. In Proceedings of the 11th
ACM International Conference on Knowledge Discov-
ery in Data Mining (SIGKDD ’05), pages 198–207,
Chicago, Illinois, USA.
Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai.
2007. Automatic Labeling of Multinomial Topic Mod-
els. In Proceedings of the 13th ACM International
Conference on Knowledge Discovery and Data Mining
(SIGKDD ’07), pages 490–499, San Jose, California.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of Interna-
tional Conference on Empirical Methods in Natural
Language Processing (EMNLP ’04), pages 404–411,
Barcelona, Spain.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The PageRank citation ranking:
Bringing order to the web. Technical Report 1999-66,
Stanford InfoLab.
Koen E.A. Sande, Theo Gevers, and Cees G. M. Snoek.
2008. Evaluation of Color Descriptors for Object and
Scene Recognition. In Proceedings of the IEEE Com-
puter Society Conference on Computer Vision and Pat-
tern Recognition (CVPR ’08), pages 1–8, Anchorage,
Alaska, USA.
Richard Szeliski. 2010. Computer Vision: Algorithms
and Applications. Springer-Verlag Inc.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and
David M. Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
Xing Wei and W. Bruce Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and Development in Information Re-
trieval (SIGIR ’06), pages 178–185, Seattle, Washing-
ton, USA.
</reference>
<page confidence="0.99775">
167
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416304">
<title confidence="0.999764">Representing Topics Using Images</title>
<author confidence="0.869598">Aletras</author>
<affiliation confidence="0.991854">Department of Computer University of</affiliation>
<address confidence="0.625823">Regent Court, 211 Sheffield, S1 4DP,</address>
<abstract confidence="0.999628526315789">Topics generated automatically, e.g. using LDA, are now widely used in Computational Linguistics. Topics are normally represented a set of keywords, often the in a topic with the highest marginal probabilities. We introduce an alternative approach in which topics are represented using images. Candidate images for each topic are retrieved from the web by querying a search engine using the The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extracted from the images themselves. We show that the proposed approach significantly outperforms several baselines and can provide images that are useful to represent a topic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Sreenivas Gollapudi</author>
<author>Anitha Kannan</author>
<author>Krishnaram Kenthapadi</author>
</authors>
<title>Enriching textbooks with images.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM ’11),</booktitle>
<pages>1847--1856</pages>
<location>Glasgow, Scotland, UK.</location>
<contexts>
<context position="7201" citStr="Agrawal et al., 2011" startWordPosition="1156" endWordPosition="1159">e best concepts are identified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were much smaller (e.g. between one and three words) an</context>
</contexts>
<marker>Agrawal, Gollapudi, Kannan, Kenthapadi, 2011</marker>
<rawString>Rakesh Agrawal, Sreenivas Gollapudi, Anitha Kannan, and Krishnaram Kenthapadi. 2011. Enriching textbooks with images. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management (CIKM ’11), pages 1847–1856, Glasgow, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Aletras</author>
<author>Mark Stevenson</author>
</authors>
<title>Evaluating topic coherence using distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS ’13) – Long Papers,</booktitle>
<pages>13--22</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="16546" citStr="Aletras and Stevenson (2013)" startWordPosition="2694" endWordPosition="2697">pentagon, american, war, gates categories (e.g. SPORTS, POLITICS, COMPUTING). Categories that have more that 80 articles associated with them are considered. These articles are collected to produce a corpus of approximately 60,000 articles generated from 1,461 categories. Documents in the two collections are tokenised and stop words removed. LDA was applied to learn 200 topics from NYT and 400 topics from WIKI. The gensim package6 was used to implement and compute LDA. The hyperparameters (α, Q) were set to num of topics. Incoherent topics are filtered out 1 by applying the method proposed by Aletras and Stevenson (2013). We randomly selected 100 topics from NYT and 200 topics from WIKI resulting in a data set of 300 topics. Candidate images for these topics were generated using the approach described in Section 3.1, producing a total of 6,000 candidate images (20 for 6http://pypi.python.org/pypi/gensim each topic). 4.2 Human Judgements of Image Relevance Human judgements of the suitability of each image were obtained using an online crowdsourcing platform, Crowdflower7. Annotators were provided with a topic (represented as a set of 10 keywords) and a candidate image. They were asked to judge how appropriate </context>
</contexts>
<marker>Aletras, Stevenson, 2013</marker>
<rawString>Nikolaos Aletras and Mark Stevenson. 2013. Evaluating topic coherence using distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS ’13) – Long Papers, pages 13–22, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="1172" citStr="Blei et al., 2003" startWordPosition="177" endWordPosition="180"> topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extracted from the images themselves. We show that the proposed approach significantly outperforms several baselines and can provide images that are useful to represent a topic. 1 Introduction Topic models are statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with th</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL ’07),</booktitle>
<pages>1024--1033</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1497" citStr="Boyd-Graber et al., 2007" startWordPosition="230" endWordPosition="233">roposed approach significantly outperforms several baselines and can provide images that are useful to represent a topic. 1 Introduction Topic models are statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing au</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL ’07), pages 1024–1033, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Giang Binh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Distributional semantics from text and images.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on GEometrical Models of Natural Language Semantics (GEMS ’11),</booktitle>
<pages>22--32</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="7222" citStr="Bruni et al., 2011" startWordPosition="1160" endWordPosition="1163">entified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were much smaller (e.g. between one and three words) and more focussed than </context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011. Distributional semantics from text and images. In Proceedings of the Workshop on GEometrical Models of Natural Language Semantics (GEMS ’11), pages 22– 32, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allison June-Barlow Chaney</author>
<author>David M Blei</author>
</authors>
<title>Visualizing topic models.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International AAAI Conference on Weblogs and Social</booktitle>
<location>Media, Dublin, Ireland.</location>
<contexts>
<context position="1713" citStr="Chaney and Blei, 2012" startWordPosition="257" endWordPosition="260">lection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). An alternative app</context>
<context position="26299" citStr="Chaney and Blei, 2012" startWordPosition="4291" endWordPosition="4294">s are ranked using a graphbased method that makes use of both textual and visual information. Evaluation is carried out on a data set created for this study. The results show that the visual features are a useful source of information for this task while the proposed graph-based method significantly outperforms several baselines. This paper demonstrates that it is possible to identify images to illustrate topics. A possible application for this technique is to represent the contents of large document collections in a way that supports rapid interpretation and can be used to enable navigation (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). We plan to explore this possibility in future work. Other possible extensions to this work include exploring alternative approaches to generating candidate images and developing techniques to automatically identify abstract topics for which suitable images are unlikely to be found, thereby avoiding the problem cases described in Section 6. Acknowledgments The research leading to these results was carried out as part of the PATHS project (http://paths-project.eu) funded by the European Community’s Seventh Framework Programme (FP7/2007-2013) un</context>
</contexts>
<marker>Chaney, Blei, 2012</marker>
<rawString>Allison June-Barlow Chaney and David M. Blei. 2012. Visualizing topic models. In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce W Croft</author>
<author>Donald Metzler</author>
<author>Trevor Strohman</author>
</authors>
<title>Search engines: Information retrieval in practice.</title>
<date>2009</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="19099" citStr="Croft et al., 2009" startWordPosition="3104" endWordPosition="3107">arried out using a similar approach to the framework proposed by Lau et al. (2011) for labelling topics using textual labels. Top-1 average rating is the average human rating assigned to the top-ranked label proposed by the system. This provides an indication of the overall quality of the image the system judges as the best one. The highest possible score averaged across all topics is 2.68, since for many topics the average score obtained from the human judgements is lower than 3. The second evaluation measure is the normalized discounted cumulative gain (nDCG) (Jarvelin and Kekalainen, 2002; Croft et al., 2009) which compares the label ranking proposed by the system to the optimal ranking provided by humans. The discounted cumulative gain at position p (DCGp) is computed using the following equation: reli (2) lo92(i) where reli is the relevance of the label to the topic in position i. Then nDCG is computed as: DCGp nDCGp = (3) IDCGp where IDCGp is the optimal ranking of the image labels, in our experiments this is the ranking provided by the scores in the human annotated data set. We follow Lau et al. (2011) in computing nDCG-1, nDCG-3 and nDCG-5 for the top 1, 3 and 5 ranked system image labels res</context>
</contexts>
<marker>Croft, Metzler, Strohman, 2009</marker>
<rawString>Bruce W. Croft, Donald Metzler, and Trevor Strohman. 2009. Search engines: Information retrieval in practice. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ritendra Datta</author>
<author>Dhiraj Joshi</author>
<author>Jia Li</author>
<author>James Z Wang</author>
</authors>
<title>Image Retrieval: Ideas, Influences, and Trends of the New Age.</title>
<date>2008</date>
<journal>ACM Computing Surveys,</journal>
<volume>40</volume>
<issue>2</issue>
<pages>60</pages>
<contexts>
<context position="7076" citStr="Datta et al., 2008" startWordPosition="1134" endWordPosition="1137">(2013) make use of the structured data in DBpedia1 to label topics. Their approach maps topic words to DBpedia concepts. The best concepts are identified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different fr</context>
</contexts>
<marker>Datta, Joshi, Li, Wang, 2008</marker>
<rawString>Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z. Wang. 2008. Image Retrieval: Ideas, Influences, and Trends of the New Age. ACM Computing Surveys, 40(2):1– 60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? Automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1239--1249</pages>
<location>Uppsala,</location>
<contexts>
<context position="1645" citStr="Feng and Lapata, 2010" startWordPosition="248" endWordPosition="251">e statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei e</context>
<context position="7154" citStr="Feng and Lapata, 2010" startWordPosition="1147" endWordPosition="1150">pproach maps topic words to DBpedia concepts. The best concepts are identified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were muc</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010a. How many words is a picture worth? Automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1239–1249, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic Models for Image Annotation and Text Illustration.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>831--839</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="1645" citStr="Feng and Lapata, 2010" startWordPosition="248" endWordPosition="251">e statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei e</context>
<context position="7154" citStr="Feng and Lapata, 2010" startWordPosition="1147" endWordPosition="1150">pproach maps topic words to DBpedia concepts. The best concepts are identified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were muc</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010b. Topic Models for Image Annotation and Text Illustration. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 831–839, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI ’07),</booktitle>
<pages>1606--1611</pages>
<location>Hyberabad, India.</location>
<contexts>
<context position="12869" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2112" endWordPosition="2115">ormation associated with each image while the final one relies on visual features. The first approach is Pointwise Mutual Information (PMI). The similarity between a pair of images (vertices in the graph) is computed as the average PMI between the terms in their metadata. PMI is computed using word co-occurrence counts over Wikipedia identified using a sliding window of length 20. We also experimented with other word association measures but these did not perform as well. The PageRank over the graph weighted using PMI is denoted as PRPMI. The second approach, Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), is a knowledge-based similarity measure. ESA transforms the text from the image metadata into vectors that consist of Wikipedia article titles weighted by their relevance. The similarity score between these vectors is computed as the cosine of the angle between them. This similarity measure is used to create the graph and its PageRank is denoted as PRESA. The final approach uses the visual features extracted from the images themselves. The visual words extracted from the images are used to form feature vectors and the similarity between a pair of images computed as the cosine of the angle be</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI ’07), pages 1606–1611, Hyberabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brynjar Gretarsson</author>
<author>John O’Donovan</author>
<author>Svetlin Bostandjiev</author>
<author>Tobias H¨ollerer</author>
<author>Arthur Asuncion</author>
<author>David Newman</author>
<author>Padhraic Smyth</author>
</authors>
<title>TopicNets: Visual analysis of large text corpora with topic modeling.</title>
<date>2012</date>
<journal>ACM Trans. Intell. Syst. Technol.,</journal>
<volume>3</volume>
<issue>2</issue>
<marker>Gretarsson, O’Donovan, Bostandjiev, H¨ollerer, Asuncion, Newman, Smyth, 2012</marker>
<rawString>Brynjar Gretarsson, John O’Donovan, Svetlin Bostandjiev, Tobias H¨ollerer, Arthur Asuncion, David Newman, and Padhraic Smyth. 2012. TopicNets: Visual analysis of large text corpora with topic modeling. ACM Trans. Intell. Syst. Technol., 3(2):23:1–23:26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="1560" citStr="Haghighi and Vanderwende, 2009" startWordPosition="236" endWordPosition="239">nes and can provide images that are useful to represent a topic. 1 Introduction Topic models are statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative label</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taher Haveliwala</author>
<author>Sepandar Kamvar</author>
<author>Glen Jeh</author>
</authors>
<title>An analytical comparison of approaches to personalizing PageRank.</title>
<date>2003</date>
<tech>Technical Report 2003-35,</tech>
<institution>Stanford InfoLab.</institution>
<contexts>
<context position="11638" citStr="Haveliwala et al., 2003" startWordPosition="1901" endWordPosition="1904">he similarity between images Vi and Vj. The PageRank score (Pr) over G for an image (Vi) can be computed by the following equation: � Pr(Vi) = d · Vj EC(Vi) (1) where QVi) denotes the set of vertices which are connected to the vertex Vi. d is the damping factor which is set to the default value of d = 0.85 (Page et al., 1999). In standard PageRank all elements of the vector v are the same, 1 � where N is the number of nodes in the graph. 4http://koen.me/research/ colordescriptors sim(Vi, Vj) Pr(Vj) + (1 − d)v E VkEC(Vj ) sim(Vj, Vk) 160 3.3.2 Personalised PageRank Personalised PageRank (PPR) (Haveliwala et al., 2003) is a variant of the PageRank algorithm in which extra importance is assigned to certain vertices in the graph. This is achieved by adjusting the values of the vector v in equation 1 to prefer certain nodes. Nodes that are assigned high values in v are more likely to also be assigned a high PPR score. We make use of PPR to prefer images with textual information that is similar to the terms in the topic. 3.3.3 Weighting Graph Edges Three approaches were compared for computing the values of sim(V�,Vj) in equation 1 used to weight the edges of the graph. Two of these make use of the textual infor</context>
</contexts>
<marker>Haveliwala, Kamvar, Jeh, 2003</marker>
<rawString>Taher Haveliwala, Sepandar Kamvar, and Glen Jeh. 2003. An analytical comparison of approaches to personalizing PageRank. Technical Report 2003-35, Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Hinneburg</author>
<author>Rico Preiss</author>
<author>Ren´e Schr¨oder</author>
</authors>
<title>TopicExplorer: Exploring document collections with topic models.</title>
<date>2012</date>
<booktitle>Machine Learning and Knowledge Discovery in Databases,</booktitle>
<volume>7524</volume>
<pages>838--841</pages>
<editor>In Peter A. Flach, Tijl Bie, and Nello Cristianini, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Hinneburg, Preiss, Schr¨oder, 2012</marker>
<rawString>Alexander Hinneburg, Rico Preiss, and Ren´e Schr¨oder. 2012. TopicExplorer: Exploring document collections with topic models. In Peter A. Flach, Tijl Bie, and Nello Cristianini, editors, Machine Learning and Knowledge Discovery in Databases, volume 7524 of Lecture Notes in Computer Science, pages 838–841. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’99),</booktitle>
<pages>50--57</pages>
<location>Berkeley, California, United States.</location>
<contexts>
<context position="1152" citStr="Hofmann, 1999" startWordPosition="175" endWordPosition="176">images for each topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extracted from the images themselves. We show that the proposed approach significantly outperforms several baselines and can provide images that are useful to represent a topic. 1 Introduction Topic models are statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’99), pages 50–57, Berkeley, California, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioana Hulpus</author>
<author>Conor Hayes</author>
<author>Marcel Karnstedt</author>
<author>Derek Greene</author>
</authors>
<title>Unsupervised graph-based topic labelling using DBpedia.</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM ’13),</booktitle>
<pages>465--474</pages>
<location>Rome, Italy.</location>
<contexts>
<context position="6464" citStr="Hulpus et al. (2013)" startWordPosition="1031" endWordPosition="1034">ion measures, lexical features and an Information Retrieval technique. Results showed that this ranking method achieves better performance than a previous approach (Mei et al., 2007). Mao et al. (2012) introduced a method for labelling hierarchical topics which makes use of sibling and parent-child relations of topics. Candidate labels are generated using a similar approach to the one used by Mei et al. (2007). Each candidate label is then assigned a score by creating a distribution based on the words it contains and measuring the Jensen-Shannon divergence between this and a reference corpus. Hulpus et al. (2013) make use of the structured data in DBpedia1 to label topics. Their approach maps topic words to DBpedia concepts. The best concepts are identified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta </context>
</contexts>
<marker>Hulpus, Hayes, Karnstedt, Greene, 2013</marker>
<rawString>Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and Derek Greene. 2013. Unsupervised graph-based topic labelling using DBpedia. In Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM ’13), pages 465–474, Rome, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>20</volume>
<issue>4</issue>
<marker>J¨arvelin, Kek¨al¨ainen, 2002</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yushi Jing</author>
<author>Shumeet Baluja</author>
</authors>
<title>PageRank for product image search.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web (WWW ’08),</booktitle>
<pages>307--316</pages>
<location>Beijing, China.</location>
<contexts>
<context position="7885" citStr="Jing and Baluja, 2008" startWordPosition="1276" endWordPosition="1279"> illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were much smaller (e.g. between one and three words) and more focussed than the ones 1http://dbpedia.org 159 we use (Jing and Baluja, 2008). In our work, the input is a topic and the aim is to associate it with an image, or images, denoting the main thematic subject. 3 Labelling Topics In this section we propose an approach to identifying images to illustrate automatically generated topics. It is assumed that there are no candidate images available so the first step (Section 3.1) is to generate a set of candidate images. However, when a candidate set is available the first step can be skipped. 3.1 Selecting Candidate Images For the experiments presented here we restrict ourselves to using images from Wikipedia available under the</context>
<context position="13612" citStr="Jing and Baluja (2008)" startWordPosition="2239" endWordPosition="2242">ikipedia article titles weighted by their relevance. The similarity score between these vectors is computed as the cosine of the angle between them. This similarity measure is used to create the graph and its PageRank is denoted as PRESA. The final approach uses the visual features extracted from the images themselves. The visual words extracted from the images are used to form feature vectors and the similarity between a pair of images computed as the cosine of the angle between them. The PageRank of the graph created using this approach is PRvis and it is similar to the approach proposed by Jing and Baluja (2008) for associating images to text queries. 3.3.4 Initialising the Personalisation Vector The personalisation vector (see above) is weighted using the similarity scores computed between the topic and its image candidates. Similarity is computed using PMI and ESA (see above). When PMI and ESA are used to weight the personalisation vector they compute the similarity between the top 10 terms for a topic and the textual metadata associated with each image in the set of candidates. We refer to the personalisation vectors created using PMI and ESA as Per(PMI) and Per(ESA) respectively. Using PPR allows</context>
</contexts>
<marker>Jing, Baluja, 2008</marker>
<rawString>Yushi Jing and Shumeet Baluja. 2008. PageRank for product image search. In Proceedings of the 17th International Conference on World Wide Web (WWW ’08), pages 307–316, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dhiraj Joshi</author>
<author>James Z Wang</author>
<author>Jia Li</author>
</authors>
<title>The Story Picturing Engine—A system for automatic text illustration.</title>
<date>2006</date>
<journal>ACM Trans. Multimedia Comput. Commun. Appl.,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="7353" citStr="Joshi et al., 2006" startWordPosition="1183" endWordPosition="1186">t are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were much smaller (e.g. between one and three words) and more focussed than the ones 1http://dbpedia.org 159 we use (Jing and Baluja, 2008). In our work, the input is a topic and the aim is to associate it w</context>
</contexts>
<marker>Joshi, Wang, Li, 2006</marker>
<rawString>Dhiraj Joshi, James Z. Wang, and Jia Li. 2006. The Story Picturing Engine—A system for automatic text illustration. ACM Trans. Multimedia Comput. Commun. Appl., 2(1):68–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Kazai</author>
</authors>
<title>In search of quality in crowdsourcing for search engine evaluation. Advances in Information Retrieval,</title>
<date>2011</date>
<pages>165--176</pages>
<contexts>
<context position="17390" citStr="Kazai, 2011" startWordPosition="2830" endWordPosition="2831">idate images (20 for 6http://pypi.python.org/pypi/gensim each topic). 4.2 Human Judgements of Image Relevance Human judgements of the suitability of each image were obtained using an online crowdsourcing platform, Crowdflower7. Annotators were provided with a topic (represented as a set of 10 keywords) and a candidate image. They were asked to judge how appropriate the image was as a representation of the main subject of the topic and provide a rating on a scale of 0 (completely unsuitable) to 3 (very suitable). Quality control is important in crowdscourcing experiments to ensure reliability (Kazai, 2011). To avoid random answers, control questions with obvious answer were included in the survey. Annotations by participants that failed to answer these questions correctly or participants that gave the same rating for all pairs were removed. 7http://crowdflower.com 162 The total number of filtered responses obtained was 62,221 from 273 participants. Each topicimage pair was rated at least by 10 subjects. The average response for each pair was calculated in order to create the final similarity judgement for use as a gold-standard. The average variance across judges (excluding control questions) i</context>
</contexts>
<marker>Kazai, 2011</marker>
<rawString>Gabriella Kazai. 2011. In search of quality in crowdsourcing for search engine evaluation. Advances in Information Retrieval, pages 165–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>David Newman</author>
<author>Sarvnaz Karimi</author>
<author>Timothy Baldwin</author>
</authors>
<title>Best topic word selection for topic labelling.</title>
<date>2010</date>
<booktitle>In The 23rd International Conference on Computational Linguistics (COLING ’10),</booktitle>
<pages>605--613</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2274" citStr="Lau et al., 2010" startWordPosition="348" endWordPosition="351">isation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the problem of selecting images to illustrate automatically generated topics. Our approach generates a set of candidate images for each t</context>
<context position="5149" citStr="Lau et al. (2010)" startWordPosition="813" endWordPosition="816">ce and the top ranked label chosen to represent the topic. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, the Google Directory and the OpenOffice English Thesaurus. A topics tree is a pre-existing hierarchical structure of labelled topics. The Automatic Labelling Of Topics algorithm computes the similarity between LDA inferred topics and topics in a topics tree by computing scores using six standard similarity measures. The label for the most similar topic in the topic tree is assigned to the LDA topic. Lau et al. (2010) proposed selecting the most representative word from a topic as its label. A label is selected by computing the similarity between each word and all the others in the topic. Several sources of information are used to identify the best label including Pointwise Mutual Information scores, WordNet hypernymy relations and distributional similarity. These features are combined in a reranking model to achieve results above a baseline (the most probable word in the topic). In more recent work, Lau et al. (2011) proposed a method for automatically labelling topics by making use of Wikipedia article t</context>
</contexts>
<marker>Lau, Newman, Karimi, Baldwin, 2010</marker>
<rawString>Jey Han Lau, David Newman, Sarvnaz Karimi, and Timothy Baldwin. 2010. Best topic word selection for topic labelling. In The 23rd International Conference on Computational Linguistics (COLING ’10), pages 605–613, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic labelling of topic models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1536--1545</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="2293" citStr="Lau et al., 2011" startWordPosition="352" endWordPosition="355">t collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the problem of selecting images to illustrate automatically generated topics. Our approach generates a set of candidate images for each topic by querying an</context>
<context position="5659" citStr="Lau et al. (2011)" startWordPosition="898" endWordPosition="901">res. The label for the most similar topic in the topic tree is assigned to the LDA topic. Lau et al. (2010) proposed selecting the most representative word from a topic as its label. A label is selected by computing the similarity between each word and all the others in the topic. Several sources of information are used to identify the best label including Pointwise Mutual Information scores, WordNet hypernymy relations and distributional similarity. These features are combined in a reranking model to achieve results above a baseline (the most probable word in the topic). In more recent work, Lau et al. (2011) proposed a method for automatically labelling topics by making use of Wikipedia article titles as candidate labels. The candidate labels are ranked using information from word association measures, lexical features and an Information Retrieval technique. Results showed that this ranking method achieves better performance than a previous approach (Mei et al., 2007). Mao et al. (2012) introduced a method for labelling hierarchical topics which makes use of sibling and parent-child relations of topics. Candidate labels are generated using a similar approach to the one used by Mei et al. (2007). </context>
<context position="18562" citStr="Lau et al. (2011)" startWordPosition="3015" endWordPosition="3018">across judges (excluding control questions) is 0.88. Inter-Annotator agreement (IAA) is computed as the average Spearman’s p between the ratings given by an annotator and the average ratings given by all other annotators. The average IAA across all topics was 0.50 which indicates the difficulty of the task, even for humans. Figure 1 shows three example topics from the data set together with the images that received the highest average score from the annotators. 4.3 Evaluation Metrics Evaluation of the topic labelling methods is carried out using a similar approach to the framework proposed by Lau et al. (2011) for labelling topics using textual labels. Top-1 average rating is the average human rating assigned to the top-ranked label proposed by the system. This provides an indication of the overall quality of the image the system judges as the best one. The highest possible score averaged across all topics is 2.68, since for many topics the average score obtained from the human judgements is lower than 3. The second evaluation measure is the normalized discounted cumulative gain (nDCG) (Jarvelin and Kekalainen, 2002; Croft et al., 2009) which compares the label ranking proposed by the system to the</context>
</contexts>
<marker>Lau, Grieser, Newman, Baldwin, 2011</marker>
<rawString>Jey Han Lau, Karl Grieser, David Newman, and Timothy Baldwin. 2011. Automatic labelling of topic models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1536–1545, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual International Conference on Systems Documentation (SIGDOC ’86),</booktitle>
<pages>24--26</pages>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="20195" citStr="Lesk, 1986" startWordPosition="3299" endWordPosition="3300">llow Lau et al. (2011) in computing nDCG-1, nDCG-3 and nDCG-5 for the top 1, 3 and 5 ranked system image labels respectively. 4.4 Baselines Since there are no previous methods for labelling topics using images, we compare our proposed models against three baselines. The Random baseline randomly selects a label for the topic from the 20 image candidates. The process is repeated 10,000 times and the average score of the selected labels is computed for each topic. The more informed Word Overlap baseline selects the image that is most similar to the topic terms by applying a Lesk-style algorithm (Lesk, 1986) to compare metadata for each image against the topic terms. It is defined as the number of common terms between a topic and image candidate normalised by the total number of terms in the topic and image’s metadata. We also compared our approach with the ranking returned by the Google Image Search for the top-20 images for a specific topic. 4.5 User Study A user study was conducted to estimate human performance on the image selection task. Three annotators were recruited and asked to select the best image for each of the 300 topics in the data set. The annotators were provided with the topic (</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the 5th Annual International Conference on Systems Documentation (SIGDOC ’86), pages 24–26, Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Object Recognition from Local Scale-invariant Features.</title>
<date>1999</date>
<booktitle>In Proceedings of the Seventh IEEE International Conference on Computer Vision,</booktitle>
<pages>1150--1157</pages>
<location>Kerkyra, Greece.</location>
<contexts>
<context position="9675" citStr="Lowe, 1999" startWordPosition="1562" endWordPosition="1563">that image’s metadata is indicative of the image’s content and (at least to some extent) related to the topic. The textual information is formed by concatenating the title and the link fields of the search result. These represent, respectively, the web page title containing the image and the image file name. The textual information is preprocessed by tokenizing and removing stop words. 3.2.2 Visual Information Visual information is extracted using low-level image keypoint descriptors, i.e. SIFT features 2https://developers.google.com/ apis-explorer/#s/customsearch/v1 3http://en.wikipedia.org (Lowe, 1999; Lowe, 2004) sensitive to colour information. SIFT features denote “interesting” areas in an image. Image features are extracted using dense sampling and described using Opponent colour SIFT descriptors provided by the colordescriptor4 software. Opponent colour SIFT descriptors have been found to give the best performance in object scene and face recognition (Sande et al., 2008). The SIFT features are clustered to form a visual codebook of 1,000 visual words using K-Means such that each feature is mapped to a visual word. Each image is represented as a bag-of-visual words (BOVW). 3.3 Ranking </context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>David G. Lowe. 1999. Object Recognition from Local Scale-invariant Features. In Proceedings of the Seventh IEEE International Conference on Computer Vision, pages 1150–1157, Kerkyra, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive Image Features from Scale-Invariant Keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="9688" citStr="Lowe, 2004" startWordPosition="1564" endWordPosition="1565"> metadata is indicative of the image’s content and (at least to some extent) related to the topic. The textual information is formed by concatenating the title and the link fields of the search result. These represent, respectively, the web page title containing the image and the image file name. The textual information is preprocessed by tokenizing and removing stop words. 3.2.2 Visual Information Visual information is extracted using low-level image keypoint descriptors, i.e. SIFT features 2https://developers.google.com/ apis-explorer/#s/customsearch/v1 3http://en.wikipedia.org (Lowe, 1999; Lowe, 2004) sensitive to colour information. SIFT features denote “interesting” areas in an image. Image features are extracted using dense sampling and described using Opponent colour SIFT descriptors provided by the colordescriptor4 software. Opponent colour SIFT descriptors have been found to give the best performance in object scene and face recognition (Sande et al., 2008). The SIFT features are clustered to form a visual codebook of 1,000 visual words using K-Means such that each feature is mapped to a visual word. Each image is represented as a bag-of-visual words (BOVW). 3.3 Ranking Candidate Ima</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G. Lowe. 2004. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision, 60(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Magatti</author>
<author>Silvia Calegari</author>
<author>Davide Ciucci</author>
<author>Fabio Stella</author>
</authors>
<title>Automatic Labeling of Topics.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Systems Design and Applications (ICSDA ’09),</booktitle>
<pages>1227--1232</pages>
<location>Pisa, Italy.</location>
<contexts>
<context position="4612" citStr="Magatti et al. (2009)" startWordPosition="725" endWordPosition="728">ent presentation of research results (Mei and Zhai, 2005; Teh et al., 2006). The first attempt at automatically assigning labels to topics is described by Mei et al. (2007). In their approach, a set of candidate labels are extracted from a reference collection using chunking and statistically important bigrams. Then, a relevance scoring function is defined which minimises the Kullback-Leibler divergence between word distribution in a topic and word distribution in candidate labels. Candidate labels are ranked according to their relevance and the top ranked label chosen to represent the topic. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, the Google Directory and the OpenOffice English Thesaurus. A topics tree is a pre-existing hierarchical structure of labelled topics. The Automatic Labelling Of Topics algorithm computes the similarity between LDA inferred topics and topics in a topics tree by computing scores using six standard similarity measures. The label for the most similar topic in the topic tree is assigned to the LDA topic. Lau et al. (2010) proposed selecting the most representative word from a topic a</context>
</contexts>
<marker>Magatti, Calegari, Ciucci, Stella, 2009</marker>
<rawString>Davide Magatti, Silvia Calegari, Davide Ciucci, and Fabio Stella. 2009. Automatic Labeling of Topics. In Proceedings of the 9th International Conference on Intelligent Systems Design and Applications (ICSDA ’09), pages 1227–1232, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian-Li Mao</author>
<author>Zhao-Yan Ming</author>
<author>Zheng-Jun Zha</author>
<author>Tat-Seng Chua</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Automatic labeling hierarchical topics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM ’12), Sheraton, Maui Hawai.</booktitle>
<contexts>
<context position="6045" citStr="Mao et al. (2012)" startWordPosition="960" endWordPosition="963">n scores, WordNet hypernymy relations and distributional similarity. These features are combined in a reranking model to achieve results above a baseline (the most probable word in the topic). In more recent work, Lau et al. (2011) proposed a method for automatically labelling topics by making use of Wikipedia article titles as candidate labels. The candidate labels are ranked using information from word association measures, lexical features and an Information Retrieval technique. Results showed that this ranking method achieves better performance than a previous approach (Mei et al., 2007). Mao et al. (2012) introduced a method for labelling hierarchical topics which makes use of sibling and parent-child relations of topics. Candidate labels are generated using a similar approach to the one used by Mei et al. (2007). Each candidate label is then assigned a score by creating a distribution based on the words it contains and measuring the Jensen-Shannon divergence between this and a reference corpus. Hulpus et al. (2013) make use of the structured data in DBpedia1 to label topics. Their approach maps topic words to DBpedia concepts. The best concepts are identified by applying graph centrality meas</context>
</contexts>
<marker>Mao, Ming, Zha, Chua, Yan, Li, 2012</marker>
<rawString>Xian-Li Mao, Zhao-Yan Ming, Zheng-Jun Zha, Tat-Seng Chua, Hongfei Yan, and Xiaoming Li. 2012. Automatic labeling hierarchical topics. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM ’12), Sheraton, Maui Hawai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Discovering evolutionary theme patterns from text: an exploration of temporal text mining.</title>
<date>2005</date>
<booktitle>In Proceedings of the 11th ACM International Conference on Knowledge Discovery in Data Mining (SIGKDD ’05),</booktitle>
<pages>198--207</pages>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="4047" citStr="Mei and Zhai, 2005" startWordPosition="634" endWordPosition="637">ta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics outperforms three baselines. The contributions of this paper are as follows: (1) introduces the problem of labelling topics using images; (2) describes an approach to this problem that makes use of multimodal information to select images from a set of candidates; (3) introduces a data set to evaluate image labelling; and (4) evaluates the proposed approach using this data set. 2 Related work In early research on topic modelling, labels were manually assigned to topics for convenient presentation of research results (Mei and Zhai, 2005; Teh et al., 2006). The first attempt at automatically assigning labels to topics is described by Mei et al. (2007). In their approach, a set of candidate labels are extracted from a reference collection using chunking and statistically important bigrams. Then, a relevance scoring function is defined which minimises the Kullback-Leibler divergence between word distribution in a topic and word distribution in candidate labels. Candidate labels are ranked according to their relevance and the top ranked label chosen to represent the topic. Magatti et al. (2009) introduced an approach for labelli</context>
</contexts>
<marker>Mei, Zhai, 2005</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2005. Discovering evolutionary theme patterns from text: an exploration of temporal text mining. In Proceedings of the 11th ACM International Conference on Knowledge Discovery in Data Mining (SIGKDD ’05), pages 198–207, Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xuehua Shen</author>
<author>Cheng Xiang Zhai</author>
</authors>
<title>Automatic Labeling of Multinomial Topic Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 13th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD ’07),</booktitle>
<pages>490--499</pages>
<location>San Jose, California.</location>
<contexts>
<context position="2256" citStr="Mei et al., 2007" startWordPosition="344" endWordPosition="347"> 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the problem of selecting images to illustrate automatically generated topics. Our approach generates a set of candidate</context>
<context position="4163" citStr="Mei et al. (2007)" startWordPosition="655" endWordPosition="658">butions of this paper are as follows: (1) introduces the problem of labelling topics using images; (2) describes an approach to this problem that makes use of multimodal information to select images from a set of candidates; (3) introduces a data set to evaluate image labelling; and (4) evaluates the proposed approach using this data set. 2 Related work In early research on topic modelling, labels were manually assigned to topics for convenient presentation of research results (Mei and Zhai, 2005; Teh et al., 2006). The first attempt at automatically assigning labels to topics is described by Mei et al. (2007). In their approach, a set of candidate labels are extracted from a reference collection using chunking and statistically important bigrams. Then, a relevance scoring function is defined which minimises the Kullback-Leibler divergence between word distribution in a topic and word distribution in candidate labels. Candidate labels are ranked according to their relevance and the top ranked label chosen to represent the topic. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, the Google Directory and the OpenO</context>
<context position="6026" citStr="Mei et al., 2007" startWordPosition="956" endWordPosition="959">e Mutual Information scores, WordNet hypernymy relations and distributional similarity. These features are combined in a reranking model to achieve results above a baseline (the most probable word in the topic). In more recent work, Lau et al. (2011) proposed a method for automatically labelling topics by making use of Wikipedia article titles as candidate labels. The candidate labels are ranked using information from word association measures, lexical features and an Information Retrieval technique. Results showed that this ranking method achieves better performance than a previous approach (Mei et al., 2007). Mao et al. (2012) introduced a method for labelling hierarchical topics which makes use of sibling and parent-child relations of topics. Candidate labels are generated using a similar approach to the one used by Mei et al. (2007). Each candidate label is then assigned a score by creating a distribution based on the words it contains and measuring the Jensen-Shannon divergence between this and a reference corpus. Hulpus et al. (2013) make use of the structured data in DBpedia1 to label topics. Their approach maps topic words to DBpedia concepts. The best concepts are identified by applying gr</context>
</contexts>
<marker>Mei, Shen, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xuehua Shen, and Cheng Xiang Zhai. 2007. Automatic Labeling of Multinomial Topic Models. In Proceedings of the 13th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD ’07), pages 490–499, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of International Conference on Empirical Methods in Natural Language Processing (EMNLP ’04),</booktitle>
<pages>404--411</pages>
<location>Barcelona,</location>
<contexts>
<context position="10826" citStr="Mihalcea and Tarau, 2004" startWordPosition="1748" endWordPosition="1751">d. Each image is represented as a bag-of-visual words (BOVW). 3.3 Ranking Candidate Images We rank images in the candidates set using graphbased algorithms. The graph is created by treating images as nodes and using similarity scores (textual or visual) between images to weight the edges. 3.3.1 PageRank PageRank (Page et al., 1999) is a graph-based algorithm for identifying important nodes in a graph that was originally developed for assigning importance to web pages. It has been used for a range of NLP tasks including word sense disambiguation (Agirre and Soroa, 2009) and keyword extraction (Mihalcea and Tarau, 2004). Let G = (V, E) be a graph with a set of vertices, V , denoting image candidates and a set of edges, E, denoting similarity scores between two images. For example, sim(Vi, Vj) indicates the similarity between images Vi and Vj. The PageRank score (Pr) over G for an image (Vi) can be computed by the following equation: � Pr(Vi) = d · Vj EC(Vi) (1) where QVi) denotes the set of vertices which are connected to the vertex Vi. d is the damping factor which is set to the default value of d = 0.85 (Page et al., 1999). In standard PageRank all elements of the vector v are the same, 1 � where N is the </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of International Conference on Empirical Methods in Natural Language Processing (EMNLP ’04), pages 404–411, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The PageRank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<tech>Technical Report 1999-66,</tech>
<institution>Stanford InfoLab.</institution>
<contexts>
<context position="10534" citStr="Page et al., 1999" startWordPosition="1700" endWordPosition="1703">ware. Opponent colour SIFT descriptors have been found to give the best performance in object scene and face recognition (Sande et al., 2008). The SIFT features are clustered to form a visual codebook of 1,000 visual words using K-Means such that each feature is mapped to a visual word. Each image is represented as a bag-of-visual words (BOVW). 3.3 Ranking Candidate Images We rank images in the candidates set using graphbased algorithms. The graph is created by treating images as nodes and using similarity scores (textual or visual) between images to weight the edges. 3.3.1 PageRank PageRank (Page et al., 1999) is a graph-based algorithm for identifying important nodes in a graph that was originally developed for assigning importance to web pages. It has been used for a range of NLP tasks including word sense disambiguation (Agirre and Soroa, 2009) and keyword extraction (Mihalcea and Tarau, 2004). Let G = (V, E) be a graph with a set of vertices, V , denoting image candidates and a set of edges, E, denoting similarity scores between two images. For example, sim(Vi, Vj) indicates the similarity between images Vi and Vj. The PageRank score (Pr) over G for an image (Vi) can be computed by the followin</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The PageRank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen E A Sande</author>
<author>Theo Gevers</author>
<author>Cees G M Snoek</author>
</authors>
<title>Evaluation of Color Descriptors for Object and Scene Recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR ’08),</booktitle>
<pages>1--8</pages>
<location>Anchorage, Alaska, USA.</location>
<contexts>
<context position="10057" citStr="Sande et al., 2008" startWordPosition="1620" endWordPosition="1623">ing stop words. 3.2.2 Visual Information Visual information is extracted using low-level image keypoint descriptors, i.e. SIFT features 2https://developers.google.com/ apis-explorer/#s/customsearch/v1 3http://en.wikipedia.org (Lowe, 1999; Lowe, 2004) sensitive to colour information. SIFT features denote “interesting” areas in an image. Image features are extracted using dense sampling and described using Opponent colour SIFT descriptors provided by the colordescriptor4 software. Opponent colour SIFT descriptors have been found to give the best performance in object scene and face recognition (Sande et al., 2008). The SIFT features are clustered to form a visual codebook of 1,000 visual words using K-Means such that each feature is mapped to a visual word. Each image is represented as a bag-of-visual words (BOVW). 3.3 Ranking Candidate Images We rank images in the candidates set using graphbased algorithms. The graph is created by treating images as nodes and using similarity scores (textual or visual) between images to weight the edges. 3.3.1 PageRank PageRank (Page et al., 1999) is a graph-based algorithm for identifying important nodes in a graph that was originally developed for assigning importan</context>
</contexts>
<marker>Sande, Gevers, Snoek, 2008</marker>
<rawString>Koen E.A. Sande, Theo Gevers, and Cees G. M. Snoek. 2008. Evaluation of Color Descriptors for Object and Scene Recognition. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR ’08), pages 1–8, Anchorage, Alaska, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Szeliski</author>
</authors>
<title>Computer Vision: Algorithms and Applications.</title>
<date>2010</date>
<publisher>Springer-Verlag Inc.</publisher>
<contexts>
<context position="7093" citStr="Szeliski, 2010" startWordPosition="1138" endWordPosition="1139">he structured data in DBpedia1 to label topics. Their approach maps topic words to DBpedia concepts. The best concepts are identified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explo</context>
</contexts>
<marker>Szeliski, 2010</marker>
<rawString>Richard Szeliski. 2010. Computer Vision: Algorithms and Applications. Springer-Verlag Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="4066" citStr="Teh et al., 2006" startWordPosition="638" endWordPosition="641">ne 2013. c�2013 Association for Computational Linguistics outperforms three baselines. The contributions of this paper are as follows: (1) introduces the problem of labelling topics using images; (2) describes an approach to this problem that makes use of multimodal information to select images from a set of candidates; (3) introduces a data set to evaluate image labelling; and (4) evaluates the proposed approach using this data set. 2 Related work In early research on topic modelling, labels were manually assigned to topics for convenient presentation of research results (Mei and Zhai, 2005; Teh et al., 2006). The first attempt at automatically assigning labels to topics is described by Mei et al. (2007). In their approach, a set of candidate labels are extracted from a reference collection using chunking and statistically important bigrams. Then, a relevance scoring function is defined which minimises the Kullback-Leibler divergence between word distribution in a topic and word distribution in candidate labels. Candidate labels are ranked according to their relevance and the top ranked label chosen to represent the topic. Magatti et al. (2009) introduced an approach for labelling topics that reli</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xing Wei</author>
<author>W Bruce Croft</author>
</authors>
<title>LDA-based Document Models for Ad-hoc Retrieval.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR ’06),</booktitle>
<pages>178--185</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="1605" citStr="Wei and Croft, 2006" startWordPosition="242" endWordPosition="245">a topic. 1 Introduction Topic models are statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed </context>
</contexts>
<marker>Wei, Croft, 2006</marker>
<rawString>Xing Wei and W. Bruce Croft. 2006. LDA-based Document Models for Ad-hoc Retrieval. In Proceedings of the 29th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR ’06), pages 178–185, Seattle, Washington, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>