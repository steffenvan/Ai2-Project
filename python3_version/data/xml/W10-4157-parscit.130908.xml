<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000527">
<title confidence="0.987735">
Chinese Personal Name Disambiguation Based on Person Modeling
</title>
<author confidence="0.980329">
Hua-Ping ZHANG1 Zhi-Hua LIU2 Qian MO3 He-Yan HUANG1
</author>
<affiliation confidence="0.993695333333333">
1 Beijing Institute of Technology, Beijing, P.R.C 100081
2 North China University of Technology, P.R.C 100041
3 Beijing Technology and Business University, Beijing, P.R.C 100048
</affiliation>
<email confidence="0.998599">
Email: kevinzhang@bit.edu.cn
</email>
<sectionHeader confidence="0.994746" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922583333333">
This document presents the bakeoff re-
sults of Chinese personal name in the
First CIPS-SIGHAN Joint Conference
on Chinese Language Processing. The
authors introduce the frame of person
disambiguation system LJPD, which
uses a new person model. LJPD was
built in short time, and it is not given
enough training and adjustment. Evalua-
tion on LJPD shows that the precision is
competitive, but the recall is very low. It
has more space for further improvement.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999175272727273">
We participated in the First CIPS-SIGHAN Joint
Conference on Chinese Language Processing.
And have taken task 3: Chinese Personal Name
disambiguation.
Chinese personal name disambiguation in-
cludes two stages: words are segmented to rec-
ognize Chinese personal name, and documents
are clustered to disambiguate different person
with the same personal name.
In our system, it involves the following
steps:
</bodyText>
<listItem confidence="0.899397833333333">
1) Segmenting words and tagging the part-of-
speech, and then recognizing Chinese personal
name using ICTCLAS 2010 system1.
2) Extracting personal feature to create the per-
son attribution model on each document.
3) Generating initial clusters according to fea-
</listItem>
<bodyText confidence="0.943911333333333">
tures in person model, and then clustering the
initial clusters until the stop criteria is reached.
The processing flow is illustrated in figure 1.
</bodyText>
<footnote confidence="0.8543495">
1 It can be downloaded from
http://hi.baidu.com/drkevinzhang
</footnote>
<figureCaption confidence="0.99658">
Figure 1 Step of Person Disambiguation
</figureCaption>
<bodyText confidence="0.99902275">
As illustrated in figure 1, the whole system
addresses four problems: personal name recogni-
tion, anaphora resolution of personal name, per-
son model creation and clustering.
</bodyText>
<sectionHeader confidence="0.958448" genericHeader="method">
2 Personal Name Recognition
</sectionHeader>
<bodyText confidence="0.999837692307692">
Chinese personal name recognition is more dif-
ficult than English. Such difficulties usually
combine with Chinese word segmentation. The
set of Chinese personal name is infinite, and the
rule of name construction is varied. Chinese per-
sonal name is often made up of a usual word,
and has ambiguity with its context.
To solve the difficulties mentioned above,
Chinese personal name recognition based on role
tagging is given in [Zhang etc., 2002]. The ap-
proach is: tokens after segmentation are tagged
using Viterbi algorithm with different roles ac-
cording to their functions in the generation of
Chinese personal name; the possible names are
recognized after maximum pattern matching on
the roles sequence [ZHANG, etc., 2002]. With
this approach, the precision of ICTCLAS
reaches 95.57% and the recall is 95.23% in an
opening corpus which contains 1,108,049 words.
In the corpus, the count of the personal name is
15,888. And ICTCLAS is a Chinese lexical
analysis system witch combines part-of-speech
tagging, word segmentation, unknown words
recognition. It can meet our requirements, so
ICTCLAS provides personal name recognition
in our system.
</bodyText>
<sectionHeader confidence="0.8663165" genericHeader="method">
3 Anaphora Resolution of Personal
Name
</sectionHeader>
<bodyText confidence="0.999857">
Anaphora is very common in natural language.
Resolve this problem can help us get more in-
formation of the person from a document.
Anaphora resolution of personal name is an
important part of anaphora resolution. At present,
much advancement in anaphora resolution have
occurred [Saliha 1998]. Anaphora resolution of
personal pronouns is an especially complicate
problem in anaphora resolution of personal name.
In our system, we don’t process this problem.
The reason is that anaphora resolution of per-
sonal name will take side effect to personal
name disambiguation unless its precision is defi-
nitely high. So we just process the anaphora of
the first name or the second name. For example,
“Jianmin Wang” in above context and “Profes-
sor Wang” will be resolved in our system.
</bodyText>
<sectionHeader confidence="0.998636" genericHeader="method">
4 Personal Model
</sectionHeader>
<bodyText confidence="0.997314833333334">
We propose a person model to represent the
person in the document:
Person = {N, P, Q, R}
where:
N is the collection of appellation of person,
such as name, nickname, alias, and so on
P is the collection of the basic attributes of
person
Q is the collection of the other attributes of
person
R is the collection of the terms co-
occurrence with person name, witch is called
term field
In the system, we focused on seven attrib-
utes such as sex, nationality, birthday, native
place, address, profession, family members and
personal name, co-occurrence terms. In these
features, nameEN, {sex, nationality, birthday,
native place}EP, {address, profession, family
members}EQ, {co-occurrence term}ER. Table
1 is the examples of person model.
In view of the co-occurrence personal name
is especially important for person disambigua-
tion. We separate it as another field in R.
</bodyText>
<subsectionHeader confidence="0.999176">
4.1 Attributes Feature
</subsectionHeader>
<bodyText confidence="0.996202428571428">
The components N, P and Q of person model are
attributes feature. The dimension of these fea-
tures for a person is different. For example, the
sex of a person is constant in life, while his or
her address may be different in different time.
Take DOM to represent the dimension of the
attributes features. Then:
</bodyText>
<equation confidence="0.997881666666667">
DOM(Ni) = 1; (1&lt;i&lt;n)
DOM(Pi) = 1; (1&lt;i&lt;k)
DOM(Qi) % 1; (1&lt;i&lt;m)
</equation>
<bodyText confidence="0.9607045">
For a person, N and P are constant in life. If
one attribute of N or P between two persons is
different, they are not the same person.
To get the attributes feature, we have three
steps: First, segment word and tag part-of-speech
for the input document. Second, we identify the
triggering word which is defined as attributes
value and the Max-Noun Phrase. The triggering
words are identified by their POS and a hand-
built triggering word thesaurus. At last, a classi-
fier determines the attribute belongs to the left
personal name or the right to the attribute. The
classifier is trained by the corpus which is hand-
tagged documents from internet.
</bodyText>
<figureCaption confidence="0.97364">
Figure 2 Step of Person Attributes Extraction
</figureCaption>
<subsectionHeader confidence="0.958478">
4.2 Term Field
</subsectionHeader>
<bodyText confidence="0.983513774193548">
In person model, R is the collection of the terms
co-occurrence within person. We adopt Vector
Space Model to represent these terms. The i-th
term is represented by ti, and its weight is repre-
sented by wi, and the weight shows the impor-
tance of the term for the person.
R = (t1, w1; t2, w2; ... ; tH, wH)
To get the person’s term field, we identify
a scope witch these terms occurred. We con-
sider three kinds of scope for term field: the
total document, the paragraph where the per-
sonal name is present, sentence where the per-
sonal name is present. And then segment words
and tag part-of-speech for these fragments. Next,
filter out the attribute terms and filter by part-
of-speech and leave only nouns, verb, adjective,
adverb and name entry. Third, we make a stop
word list, and filter out these stop terms. Last,
according to the term’s DF, filter out high fre-
quency and low frequency terms, and only the
terms witch DF is not lower than 2 and not
higher than N/3(N is the total count of docu-
ments) are left.
In collection R, we have separated term
field to co-occurrence personal name vector and
co-occurrence common term vector. Because
the two vectors have different affect to person
disambiguation. This difference manifests in the
different method to compute these weight. The
common term’s weight is computed by tf-idf
algorithm:
</bodyText>
<equation confidence="0.9416875">
w t d
( , ) log( ( , ) 1) log( / 1
= tf t d + x N nt +
where:
w(t, d) is the weight of term t in document
d
tf (t, d) is the frequency of occurrence of t
in d
</equation>
<bodyText confidence="0.973166">
N is the total count of documents
nt is the count of documents which contain
term t
</bodyText>
<table confidence="0.970747333333333">
)
sex nationality birthday Native address Family profession Co-occurrence Co-occurrence
place members personal name terms field
Name1 N a 1949 北京 演员 ...
Name2 女 LSI东 王红 教师 ... ......
Name3 N 蒙 安徽 书记 ... ••••••
</table>
<tableCaption confidence="0.999115">
Table 1 Examples of Person Model
</tableCaption>
<bodyText confidence="0.9408975">
The co-occurrence personal name’s weight is
computed below:
</bodyText>
<equation confidence="0.9395245">
w name p
( , ) log( ( , ) 1) log( &apos; /
</equation>
<bodyText confidence="0.893986818181818">
= nf name p + x N nname + 1
where:
w(name, p) is the weight of co-occurrence
name name
nf (name, p) is the frequency of co-
occurrence of name and person p
name is the count of the co-occurrence of
name and the other personal name
The similarity of term field between two persons
is calculated by the angle cosine:
∑
</bodyText>
<equation confidence="0.976758666666667">
i
∑xi *∑
i i
</equation>
<sectionHeader confidence="0.988685" genericHeader="method">
5 Clustering
</sectionHeader>
<bodyText confidence="0.999672388888889">
Person model “Person = {N, P, Q, R}” is multi-
dimensional. First, we adopted two rules to gen-
erate original clusters:
Rule 1: For two persons whose name is same, if
one of the birthday (accurate to month) or rela-
tive is matched, these two persons are the same
person.
Rule 2: For two persons whose name is same, if
one of the sex, nationality, native place or birth-
day is not matched, these two persons are differ-
ent.
There are profession, co-occurrence per-
sonal name and co-occurrence common terms
left. For two persons whose name is same, we
apply rule 1 and 2 first. If both of the two rules
are not activating, compute the similarity Simposi-
tion(X, Y), cosname(X, Y), costerm(X, Y). And then
synthesize these three similarities.
</bodyText>
<equation confidence="0.985762666666667">
)
*
yi
xi
Y)
(
X
Sim
,Y) = cos(X,
</equation>
<page confidence="0.612456">
2
yi
</page>
<bodyText confidence="0.9999726">
Assume the three factors profession, co-
occurrence personal name and co-occurrence
common terms are independent, and adopt Stan-
ford certainty theory to synthesize the three
similarities. The Stanford certainty theory cre-
ates confidence measures and some simple rules
for combing these confidences. Assume E1, E2,
E2 are the Stanford certainty factors of event B,
and CF represent the confidence, then the confi-
dence of event B is :
</bodyText>
<equation confidence="0.995816666666667">
CF B CF E CF E CF E CF E CF E CF E
( ) (1) ( 2) ( 3) ( 1) ( 2) ( 1)
= + + − × − ×
CF E CF E CF E CF E CF E CF E
( 3) ( 2) ( 3) ( 1) ( 2) ( 3
− × + × ×
</equation>
<bodyText confidence="0.9739608">
For example, if the confidence of the three
factors for event B is respectively: 88%, 74%,
66%, then the confidence for event B is 88%+
74%+66%-88%x74%-88%x66%-76%x
66%+88%x74%x66%=98.93%.
To compute the confidence of the factors,
we should get the threshold (represented by ui)
of the similarity for factors. If the similarity of
the factor reaches the threshold, its confidence is
100%:
</bodyText>
<figure confidence="0.908117923076923">
simE
CF(E) = i CF(Ei)∈ [0,1]
iui
)
the original clusters
2) Adopt agglomerative hierarchical cluster-
ing algorithm to clustering these original
clusters.
(1) Take each original cluster as a single
cluster
(2) Select two clusters which are most
likelihood and merge to one cluster
(3) If there is only one cluster or reaches
</figure>
<bodyText confidence="0.967133857142857">
stop criteria, exit. Else, go to step (2).
In the process of merging the clusters, we
should merge the fragment of person. For term
field vector, we simply compute the average of
the term weights. For att
ribute feature, we adopt
rule method to merge two clusters.
</bodyText>
<sectionHeader confidence="0.851444" genericHeader="conclusions">
6 Task
</sectionHeader>
<bodyText confidence="0.980286666666667">
element of clustering is chan
ged to the subfolder
of a real name. When all the subfolders are clus-
tered for a query name, we merge the results to
one xml file.
d 2 to group the persons to
</bodyText>
<table confidence="0.99453625">
B-Cubed P-IP
precision recall F score P IP F score
Formal test 80.2 68.75 68.4 86.12 76.37 77.54
Diagnosis test 94.62 63.32 72.48 96.44 72.78 80.85
</table>
<tableCaption confidence="0.998916">
Table 2 Evaluation result of Personal Disambiguation
</tableCaption>
<sectionHeader confidence="0.997319" genericHeader="references">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.981900125">
hcoming bakeoff. The training method is: clustering training
References data according to the single factor, select the
threshold with which the recall is higher with the
premise that the precision is not lower than 98%.
We get three thresholds 3, 0.5, 0.25 respectively
for factor profession, co-occurrence personal
name and co-occurrence common terms.
Overall, the algorithm takes two steps:
1) Adopt rule 1 an
We would introduce the operation of some dif-
ferent track in this section.
In formal test, we first get a query name
and its all files. Then we segment these files and
extract the related information of our person
model and output to files. At last, we cluster
these person models and output to result xml.
In the diagnosis test, the basic process is
same to the formal test. The difference is that the
Through the first bakeoff, we have learned
much about the development in Chinese per-
sonal name recognition and person disambigua-
tion. At the same time, we really find our prob-
lems during the evaluation. The bakeoff is inter-
esting and helpful. We look forward to partici-
</bodyText>
<reference confidence="0.990987">
pate in fort
ZHANG Hua-Ping, LIU Qun, YU Hong-Kui,
CHENG Xue-Qi, BAI Shuo. Chinese
En-
tity Recognition Using Role Model. International
Journal of Computational Linguistics and Chinese
language processing,
8 (2)
Azzam
Kevin Humphreys
Named
2003,Vol.
Saliha,
&amp; Robert Gai-
zauskas. Coreference resolution in a multilingual
information extraction. In the Proc. of the Work-
shop on Linguistic Coference. Granada,
Spain.1998.
Yu Manquan. Research on Knowledge Mining in
Person Tracking. Ph.D.Thesis of GUCAS. 2006
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.730738">
<title confidence="0.999562">Chinese Personal Name Disambiguation Based on Person Modeling</title>
<author confidence="0.999819">Zhi-Hua Qian He-Yan</author>
<affiliation confidence="0.970425">1Beijing Institute of Technology, Beijing, P.R.C 100081</affiliation>
<address confidence="0.8851095">2North China University of Technology, P.R.C 100041 3Beijing Technology and Business University, Beijing, P.R.C</address>
<email confidence="0.955596">kevinzhang@bit.edu.cn</email>
<abstract confidence="0.998352692307693">This document presents the bakeoff results of Chinese personal name in the First CIPS-SIGHAN Joint Conference on Chinese Language Processing. The authors introduce the frame of person disambiguation system LJPD, which uses a new person model. LJPD was built in short time, and it is not given enough training and adjustment. Evaluation on LJPD shows that the precision is competitive, but the recall is very low. It has more space for further improvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>