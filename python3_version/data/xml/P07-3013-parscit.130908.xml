<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011417">
<title confidence="0.991452">
Annotating and Learning Compound Noun Semantics
</title>
<author confidence="0.970638">
Diarmuid O´ S´eaghdha
</author>
<affiliation confidence="0.998895">
University of Cambridge Computer Laboratory
</affiliation>
<address confidence="0.977198333333333">
15 JJ Thomson Avenue
Cambridge CB3 0FD
United Kingdom
</address>
<email confidence="0.998446">
do242@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951181818182">
There is little consensus on a standard ex-
perimental design for the compound inter-
pretation task. This paper introduces well-
motivated general desiderata for semantic
annotation schemes, and describes such a
scheme for in-context compound annotation
accompanied by detailed publicly available
guidelines. Classification experiments on an
open-text dataset compare favourably with
previously reported results and provide a
solid baseline for future research.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990592592593">
There are a number of reasons why the interpreta-
tion of noun-noun compounds has long been a topic
of interest for NLP researchers. Compounds oc-
cur very frequently in English and many other lan-
guages, so they cannot be avoided by a robust se-
mantic processing system. Compounding is a very
productive process with a highly skewed type fre-
quency spectrum, and corpus information may be
very sparse. Compounds are often highly ambigu-
ous and a large degree of “world knowledge” seems
necessary to understand them. For example, know-
ing that a cheese knife is (probably) a knife for
cutting cheese and (probably) not a knife made of
cheese (cf. plastic knife) does not just require an
ability to identify the senses of cheese and knife but
also knowledge about what one usually does with
cheese and knives. These factors combine to yield
a difficult problem that exhibits many of the chal-
lenges characteristic of lexical semantic process-
ing in general. Recent research has made signifi-
cant progress on solving the problem with statisti-
cal methods and often without the need for manu-
ally created lexical resources (Lauer, 1995; Lapata
and Keller, 2004; Girju, 2006; Turney, 2006). The
work presented here is part of an ongoing project
that treats compound interpretation as a classifica-
tion problem to be solved using machine learning.
</bodyText>
<sectionHeader confidence="0.840576" genericHeader="method">
2 Selecting an Annotation Scheme
</sectionHeader>
<bodyText confidence="0.999650041666667">
For many classification tasks, such as part-of-speech
tagging or word sense disambiguation, there is gen-
eral agreement on a standard set of categories that
is used by most researchers. For the compound
interpretation task, on the other hand, there is lit-
tle agreement and numerous classification schemes
have been proposed. This hinders meaningful com-
parison of different methods and results. One must
therefore consider how an appropriate annotation
scheme should be chosen.
One of the problems is that it is not immedi-
ately clear what level of granularity is desirable, or
even what kind of units the categories should be.
Lauer (1995) proposes a set of 8 prepositions that
can be used to paraphrase compounds: a cheese
knife is a knife FOR cheese but a kitchen knife is
a knife (used) IN a kitchen. An advantage of this
approach is that preposition-noun co-occurrences
can efficiently be mined from large corpora using
shallow techniques. On the other hand, interpret-
ing a paraphrase requires further disambiguation as
one preposition can map onto many semantic rela-
tions.1 Girju et al. (2005) and Nastase and Szpakow-
icz (2003) both present large inventories of seman-
</bodyText>
<footnote confidence="0.986806">
1The interpretation of prepositions is itself the focus of a
Semeval task in 2007.
</footnote>
<page confidence="0.99303">
73
</page>
<bodyText confidence="0.961384470588236">
Proceedings of the ACL 2007 Student Research Workshop, pages 73–78,
Prague, June 2007. c�2007 Association for Computational Linguistics
tic relations that describe noun-noun dependencies.
Such relations provide richer semantic information,
but it is harder for both humans and machines to
identify their occurrence in text. Larger invento-
ries can also suffer from class sparsity; for exam-
ple, 14 of Girju et al.’s 35 relations do not occur in
their dataset and 7 more occur in less than 1% of
the data. Nastase and Szpakowicz’ scheme mitigates
this problem by the presence of 5 supercategories.
Each of these proposals has its own advantages
and drawbacks, and there is a need for principled cri-
teria for choosing one. As the literature on semantic
annotation “best practice” is rather small,2 I devised
a novel set of design principles based on empirical
and theoretical considerations:
</bodyText>
<listItem confidence="0.972155833333333">
1. The inventory of informative categories should
account for as many compounds as possible
2. The category boundaries should be clear and
categories should describe a coherent concept
3. The class distribution should not be overly
skewed or sparse
4. The concepts underlying the categories should
generalise to other linguistic phenomena
5. The guidelines should make the annotation pro-
cess as simple as possible
6. The categories should provide useful semantic
information
</listItem>
<bodyText confidence="0.999756470588235">
These intuitively appear to be desirable principles
for any semantic annotation scheme. The require-
ment of class distribution balance is motivated by
the classification task. Where one category domi-
nates, the most-frequent-class baseline can be diffi-
cult to exceed and care must be taken in evaluation
to consider macro-averaged performance as well as
raw accuracy. It has been suggested that classifiers
trained on skewed data may perform poorly on mi-
nority classes (Zhang and Oles, 2001). Of course,
this is not a justification for conflating concepts with
little in common, and it may well be that the natural
distribution of data is inherently skewed.
There is clearly a tension between these criteria,
and only a best-fit solution is possible. However, it
was felt that a new scheme might satisfy them more
optimally than existing schemes. Such a proposal
</bodyText>
<footnote confidence="0.94937">
2One relevant work is Wilson and Thomas (1997).
</footnote>
<table confidence="0.999722916666667">
Relation Distribution Example
BE 191 (9.55%) steel knife
HAVE 199 (9.95%) street name
IN 308 (15.40%) forest hut
INST 266 (13.30%) rice cooker
ACTOR 236 (11.80%) honey bee
ABOUT 243 (12.15%) fairy tale
REL 81 (4.05%) camera gear
LEX 35 (1.75%) home secretary
UNKNOWN 9 (0.45%) simularity crystal
MISTAG 220 (11.00%) blazing fire
NONCOMP 212 (10.60%) [real tennis] club
</table>
<tableCaption confidence="0.999829">
Table 1: Sample class frequencies
</tableCaption>
<bodyText confidence="0.999344387096774">
necessitates a method of evaluation. Not all the cri-
teria are easily evaluable. It is difficult to prove gen-
eralisability and usefulness conclusively, but it can
be maximised by building on more general work on
semantic representation; for example, the guidelines
introduced here use a conception of events and par-
ticipants compatible with that of FrameNet (Baker
et al., 1998). Good results on agreement and base-
line classification will provide positive evidence for
the coherence and balance of the classes; agreement
measures can confirm ease of annotation.
In choosing an appropriate level of granularity, I
wished to avoid positing a large number of detailed
but rare categories. Levi’s (1978) set of nine se-
mantic relations was used as a starting point. The
development process involved a series of revisions
over six months, aimed at satisfying the six criteria
above and maximising interannotator agreement in
annotation trials. The nature of the decisions which
had to be made is exemplified by the compound car
factory, whose standard referent seems to qualify as
FOR, CAUSE, FROM and IN in Levi’s scheme (and
causes similar problems for the other schemes I am
aware of). Likewise there seems to be no princi-
pled way to choose between a locative or purposive
label for dining room. Such examples led to both
redefinition of category boundaries and changes in
the category set; for example, FOR was replaced by
INST and AGENT, which are independent of purpo-
sivity. This resulted in the class inventory shown in
Table 1 and a detailed set of annotation guidelines.3
</bodyText>
<footnote confidence="0.989581">
3The guidelines are publicly available at http://www.
cl.cam.ac.uk/˜do242/guidelines.pdf.
</footnote>
<page confidence="0.998662">
74
</page>
<bodyText confidence="0.999984722222222">
The scheme’s development is described at length in
O´ S´eaghdha (2007b).
Many of the labels are self-explanatory. AGENT
and INST(rument) apply to sentient and non-
sentient participants in an event respectively, with
ties (e.g., stamp collector) being broken by a hier-
archy of coarse semantic roles. REL is an OTHER-
style category for compounds encoding non-specific
association. LEX(icalised) applies to compounds
which are semantically opaque without prior knowl-
edge of their meanings. MISTAG and NON-
COMP(ound) labels are required to deal with se-
quences that are not valid two-noun compounds but
have been identified as such due to tagging errors
and the simple data extraction heuristic described in
Section 3.1. Coverage is good, as 92% of valid com-
pounds in the dataset described below were assigned
one of the six main semantic relations.
</bodyText>
<sectionHeader confidence="0.978859" genericHeader="method">
3 Annotation Experiment
</sectionHeader>
<subsectionHeader confidence="0.994312">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9999783">
A simple heuristic was used to extract noun se-
quences from the 90 million word written part of the
British National Corpus.4 The corpus was parsed
using the RASP parser5 and all sequences of two
common nouns were extracted except those adjacent
to another noun and those containing non-alphabetic
characters. This yielded almost 1.6 million tokens
with 430,555 types. 2,000 unique tokens were ran-
domly drawn for use in annotation and classification
experiments.
</bodyText>
<subsectionHeader confidence="0.999828">
3.2 Method
</subsectionHeader>
<bodyText confidence="0.999977454545455">
Two annotators were used: the current author and
an annotator experienced in lexicography but with-
out any special knowledge of compounds or any role
in the development of the annotation scheme. In all
the trials described here, each compound was pre-
sented alongside the sentence in which it was found
in the BNC. The annotators had to assign one of the
labels in Table 1 and the rule that licensed that la-
bel in the annotation guidelines. For example, the
compound forest hut in its usual sense would be an-
notated TN,2,2.1.3.1 to indicate the semantic
</bodyText>
<footnote confidence="0.988633">
4http://www.natcorp.ox.ac.uk/
5http://www.informatics.susx.ac.uk/
research/nlp/rasp/
</footnote>
<bodyText confidence="0.999843533333333">
relation, the direction of the relation (it is a hut in
a forest, not a forest in a hut) and that the label is
licensed by rule 2.1.3.1 in the guidelines (N1/N2 is
an object spatially located in or near N2/N1).6 Two
trial batches of 100 compounds were annotated to
familiarise the second annotator with the guidelines
and to confirm that the guidelines were indeed us-
able for others. The first trial resulted in agreement
of 52% and the second in agreement of 73%. The
result of the second trial, corresponding to a Kappa
beyond-chance agreement estimate (Cohen, 1960)
of n� = 0.693, was very impressive and it was de-
cided to proceed to a larger-scale task. 500 com-
pounds not used in the trial runs were drawn from
the 2,000-item set and annotated.
</bodyText>
<subsectionHeader confidence="0.909014">
3.3 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999983541666667">
Agreement on the test set was 66.2% with n� = 0.62.
This is less than the score achieved in the second
trial run, but may be a more accurate estimator of the
true population n due to the larger sample size. On
the other hand, the larger dataset may have caused
annotator fatigue. Pearson standardised residuals
(Haberman, 1973) were calculated to identify the
main sources of disagreement.7 In the context of
inter-annotator agreement one expects these residu-
als to have large positive values on the agreement di-
agonal and negative values in all other cells. Among
the six main relations listed at the top of Table 1,
a small positive association was observed between
INST and ABOUT, indicating that borderline topics
such as assessment task and gas alarm were likely
to be annotated as INST by the first annotator and
ABOUT by the second. It seems that the guidelines
might need to clarify this category boundary.
It is clear from analysis of the data that the REL,
LEX and UNKNOWN categories show very low
agreement. They all have low residuals on the agree-
ment diagonal (that for UNKNOWN is negative) and
numerous positive entries off it. REL and LEX are
also the categories for which it is most difficult to
</bodyText>
<footnote confidence="0.696641333333333">
6The additional information provided by the direction and
rule annotations could be used to give a richer classification
scheme but has not yet been used in this way in my experiments.
7The standardised residual of cell ij is calculated as
nij − �pi+P+j
y Pi+p+j(1 − Pi+)(1 − P+j)
where nij is the observed value of cell ij and Pi+, P+j are row
and column marginal probabilities estimated from the data.
eij _
</footnote>
<page confidence="0.993091">
75
</page>
<bodyText confidence="0.9995758">
provide clear guidelines. On the other hand, the
MISTAG and NONCOMP categories showed good
agreement, with slightly higher agreement residu-
als than the other categories. To get a rough idea
of agreement on the six categories used in the clas-
sification experiments described below, agreement
was calculated for all items which neither annota-
tor annotated with any of REL, LEX, UNKNOWN,
MISTAG and NONCOMP. This left 343 items with
agreement of 73.6% and k = 0.683.
</bodyText>
<subsectionHeader confidence="0.820309">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999990204081633">
This is the first work I am aware of where com-
pounds were annotated in their sentential context.
This aspect is significant, as compound meaning is
often context dependent (compare school manage-
ment decided... and principles of school manage-
ment) and in-context interpretation is closer to the
dynamic of real-world language use. Context can
both help and hinder agreement, and it is not clear
whether in- or out-of-context annotation is easier.
Previous work has given out-of-context agree-
ment figures for corpus data. Kim and Bald-
win (2005) report an experiment using 2,169 com-
pounds taken from newspaper text and the categories
of Nastase and Szpakowicz (2003). Their annota-
tors could assign multiple labels in case of doubt
and were judged to agree on an item if their anno-
tations had any label in common. This less strin-
gent measure yielded agreement of 52.31%. Girju
et al. (2005) report agreement for annotation using
both Lauer’s 8 prepositional labels (h = 0.8) and
their own 35 semantic relations (h = 0.58). These
figures are difficult to interpret as annotators were
again allowed assign multiple labels (for the prepo-
sitions this occurred in “almost all” cases) and the
multiply-labelled items were excluded from the cal-
culation of Kappa. This entails discarding the items
which are hardest to classify and thus most likely to
cause disagreement.
Girju (2006) has recently published impressive
agreement results on a related task. This involved
annotating 2,200 compounds extracted from an on-
line dictionary, each presented in five languages, and
resulted in a Kappa score of 0.67. This task may
have been facilitated by the data source and its mul-
tilingual nature. It seems plausible that dictionary
entries are more likely to refer to familiar concepts
than compounds extracted from a balanced corpus,
which are frequently context-dependent coinages or
rare specialist terms. Furthermore, the translations
of compounds in Romance languages often pro-
vide information that disambiguates the compound
meaning (this aspect was the main motivation for the
work) and translations from a dictionary are likely
to correspond to an item’s most frequent meaning.
A qualitative analysis of the experiment described
above suggests that about 30% of the disagreements
can confidently be attributed to disagreement about
the semantics of a given compound (as opposed to
how a given meaning should be annotated).8
</bodyText>
<sectionHeader confidence="0.995606" genericHeader="method">
4 SVM Learning with Co-occurrence Data
</sectionHeader>
<subsectionHeader confidence="0.987169">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.97371652">
The data used for classification was taken from the
2,000 items used for the annotation experiment, an-
notated by a single annotator. Due to time con-
straints, this annotation was done before the second
annotator had been used and was not changed af-
terwards. All compounds annotated as BE, HAVE,
IN, INST, AGENT and ABOUT were used, giving a
dataset of 1,443 items. All experiments were run us-
ing Support Vector Machine classifiers implemented
in LIBSVM.9 Performance was measured via 5-fold
cross-validation. Best performance was achieved
with a linear kernel and one-against-all classifica-
tion. The single SVM parameter C was estimated
for each fold by cross-validating on the training set.
Due to the efficiency of the linear kernel the optimi-
sation, training and testing steps for each fold could
be performed in under an hour.
I investigated what level of performance could
be achieved using only corpus information. Feature
vectors were extracted from the written BNC for
each modifier and head in the dataset under the
following conditions:
w5, w10: Each word within a window of 5 or 10
words on either side of the item is a feature.
Rbasic, Rmod, Rverb, Rconj: These feature sets
</bodyText>
<footnote confidence="0.9984852">
8For example, one annotator thought peat boy referred to a
boy who sells peat (AGENT) while the other thought it referred
to a boy buried in peat (IN).
9http://www.csie.ntu.edu.tw/˜cjlin/
libsvm
</footnote>
<page confidence="0.989367">
76
</page>
<bodyText confidence="0.999991842105263">
use the grammatical relation output of the RASP
parser run over the written BNC. The Rbasic feature
set conflates information about 25 grammatical
relations; Rmod counts only prepositional, nominal
and adjectival noun modification; Rverb counts
only relations among subjects, objects and verbs;
Rconj counts only conjunctions of nouns. In each
case, each word entering into one of the target
relations with the item is a feature and only the
target relations contribute to the feature values.
Each feature vector counts the target word’s co-
occurrences with the 10,000 words that most fre-
quently appear in the context of interest over the en-
tire corpus. Each compound in the dataset is rep-
resented by the concatenation of the feature vectors
for its head and modifier. To model aspects of co-
occurrence association that might be obscured by
raw frequency, the log-likelihood ratio G2 was used
to transform the feature space.10
</bodyText>
<subsectionHeader confidence="0.946699">
4.2 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999931066666667">
Results for these feature sets are given in Table 2.
The simple word-counting conditions w5 and w10
perform relatively well, but the highest accuracy is
achieved by Rconj. The general effect of the log-
likelihood transformation cannot be stated categor-
ically, as it causes some conditions to improve and
others to worsen, but the G2-transformed Rconj fea-
tures give the best results of all with 54.95% ac-
curacy (53.42% macro-average). Analysis of per-
formance across categories shows that in all cases
accuracy is lower (usually below 30%) on the BE
and HAVE relations than on the others (often above
50%). These two relations are least common in the
dataset, which is why the macro-averaged figures are
slightly lower than the micro-averaged accuracy.
</bodyText>
<subsectionHeader confidence="0.996484">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999851333333333">
It is interesting that the conjunction-based features
give the best performance, as these features are also
the most sparse. This may be explained by the fact
that words appearing in conjunctions are often tax-
onomically similar (Roark and Charniak, 1998) and
that taxonomic information is particularly useful for
</bodyText>
<footnote confidence="0.991417666666667">
10This measure is relatively robust where frequency counts
are low and consistently outperformed other association mea-
sures in the empirical evaluation of Evert (2004).
</footnote>
<table confidence="0.998211625">
Raw G2
Accuracy Macro Accuracy Macro
w5 52.60% 51.07% 51.35% 49.93%
w10 51.84% 50.32% 50.10% 48.60%
Rbasic 51.28% 49.92% 51.83% 50.26%
Rmod 51.35% 50.06% 48.51% 47.03%
Rverb 48.79% 47.13% 48.58% 47.07%
Rconj 54.12% 52.44% 54.95% 53.42%
</table>
<tableCaption confidence="0.999604">
Table 2: Performance of BNC co-occurrence data
</tableCaption>
<bodyText confidence="0.99628424">
compound interpretation, as evidenced by the suc-
cess of WordNet-based methods (see Section 5).
In comparing reported classification results, it is
difficult to disentangle the effects of different data,
annotation schemes and classification methods. The
results described here should above all be taken to
demonstrate the feasibility of learning using a well-
motivated annotation scheme and to provide a base-
line for future work on the same data. In terms of
methodology, Turney’s (2006) Vector Space Model
experiments are most similar. Using feature vec-
tors derived from lexical patterns and frequencies re-
turned by a Web search engine, a nearest-neighbour
classifier achieves 45.7% accuracy on compounds
annotated with 5 semantic classes. Turney improves
accuracy to 58% with a combination of query ex-
pansion and linear dimensionality reduction. This
method trades off efficiency for accuracy, requiring
many times more resources in terms of time, stor-
age and corpus size than that described here. Lap-
ata and Keller (2004) obtain accuracy of 55.71% on
Lauer’s (1995) prepositionally annotated data using
simple search engine queries. Their method has the
advantage of not requiring supervision, but it cannot
be used with deep semantic relations.
</bodyText>
<sectionHeader confidence="0.997249" genericHeader="method">
5 SVM Classification with WordNet
</sectionHeader>
<subsectionHeader confidence="0.982836">
5.1 Method
</subsectionHeader>
<bodyText confidence="0.998348142857143">
The experiments reported in this section make a ba-
sic use of the WordNet11 hierarchy. Binary feature
vectors are used whereby a vector entry is 1 if the
item belongs to or is a hyponym of the synset corre-
sponding to that feature, and 0 otherwise. Each com-
pound is represented by the concatenation of two
such vectors, for the head and modifier. The same
</bodyText>
<footnote confidence="0.87192">
11http://wordnet.princeton.edu/
</footnote>
<page confidence="0.997542">
77
</page>
<bodyText confidence="0.859032">
classification method is used as in Section 4.
</bodyText>
<subsectionHeader confidence="0.768796">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.9985238125">
This method achieves accuracy of 56.76% and
macro-averaged accuracy of 54.59%, slightly higher
than that achieved by the co-occurrence features.
Combining WordNet and co-occurrence vectors by
simply concatenating the G2-transformed Rconj
vector and WordNet feature vector for each com-
pound gives a further boost to 58.35% accuracy
(56.70% macro-average).
These results are higher than those reported for
similar approaches on open-text data (Kim and
Baldwin, 2005; Girju et al., 2005), though the same
caveat applies about comparison. The best results
(over 70%) reported so far for compound inter-
pretation use a combination of multiple lexical re-
sources and detailed additional annotation (Girju et
al., 2005; Girju, 2006).
</bodyText>
<sectionHeader confidence="0.996104" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999984">
The annotation scheme described above has been
tested on a rigorous multiple-annotator task and
achieved superior agreement to comparable results
in the literature. Further refinement should be possi-
ble but would most likely yield diminishing returns.
In the classification experiments, my goal was to
see what level of performance could be gained by
using straightforward techniques so as to provide
a meaningful baseline for future research. Good
results were achieved with methods that rely nei-
ther on massive corpora or broad-coverage lexical
resources, though slightly better performance was
achieved using WordNet. An advantage of resource-
poor methods is that they can be used for the many
languages where compounding is common but such
resources are limited.
The learning approach described here only cap-
tures the lexical semantics of the individual con-
situents. It seems intuitive that other kinds of corpus
information would be useful; in particular, contexts
in which the head and modifier of a compound both
occur may make explicit the relations that typically
hold between their referents. Kernel methods for us-
ing such relational information are investigated in O´
S´eaghdha (2007a) with promising results, and I am
continuing my research in this area.
</bodyText>
<sectionHeader confidence="0.996466" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935730769231">
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Proc. ACL-
COLING-98, pages 86–90, Montreal, Canada.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20:37–46.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
Universit¨at Stuttgart.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19(4):479–496.
Roxana Girju. 2006. Out-of-context noun phrase seman-
tic interpretation with cross-linguistic evidence. In
Proc. CIKM-06, pages 268–276, Arlington, VA.
Shelby J. Haberman. 1973. The analysis of residuals in
cross-classified tables. Biometrics, 29(1):205–220.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCNLP-05, pages 945–956, Jeju Is-
land, Korea.
Mirella Lapata and Frank Keller. 2004. The Web as a
baseline: Evaluating the performance of unsupervised
Web-based models for a range of NLP tasks. In Proc.
HLT-NAACL-04, pages 121–128, Boston, MA.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Compound Nouns. Ph.D.
thesis, Macquarie University.
Judith N. Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proc. IWCS-5,
Tilburg, Netherlands.
Brian Roark and Eugene Charniak. 1998. Noun-
phrase co-occurrence statistics for semi-automatic se-
mantic lexicon construction. In Proc. ACL-COLING-
98, pages 1110–1106, Montreal, Canada.
Diarmuid O´ S´eaghdha. 2007a. Co-occurrence contexts
for corpus-based noun compound interpretation. In
Proc. of the ACL Workshop A Broader Perspective on
Multiword Expressions, Prague, Czech Republic.
Diarmuid O´ S´eaghdha. 2007b. Designing and evaluating
a semantic annotation scheme for compound nouns. In
Proc. Corpus Linguistics 2007, Birmingham, UK.
Peter D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Andrew Wilson and Jenny Thomas. 1997. Semantic an-
notation. In R. Garside, G. Leech, and A. McEnery,
editors, Corpus Annotation. Longman, London.
Tong Zhang and Frank J. Oles. 2001. Text categorization
based on regularized linear classification methods. In-
formation Retrieval, 4(1):5–31.
</reference>
<page confidence="0.998827">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775166">
<title confidence="0.99968">Annotating and Learning Compound Noun Semantics</title>
<author confidence="0.993406">Diarmuid O´ S´eaghdha</author>
<affiliation confidence="0.999999">University of Cambridge Computer Laboratory</affiliation>
<address confidence="0.927808333333333">15 JJ Thomson Avenue Cambridge CB3 0FD United Kingdom</address>
<email confidence="0.985176">do242@cl.cam.ac.uk</email>
<abstract confidence="0.999122333333333">There is little consensus on a standard experimental design for the compound interpretation task. This paper introduces wellmotivated general desiderata for semantic annotation schemes, and describes such a scheme for in-context compound annotation accompanied by detailed publicly available guidelines. Classification experiments on an open-text dataset compare favourably with previously reported results and provide a solid baseline for future research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proc. ACLCOLING-98,</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="6337" citStr="Baker et al., 1998" startWordPosition="1000" endWordPosition="1003"> honey bee ABOUT 243 (12.15%) fairy tale REL 81 (4.05%) camera gear LEX 35 (1.75%) home secretary UNKNOWN 9 (0.45%) simularity crystal MISTAG 220 (11.00%) blazing fire NONCOMP 212 (10.60%) [real tennis] club Table 1: Sample class frequencies necessitates a method of evaluation. Not all the criteria are easily evaluable. It is difficult to prove generalisability and usefulness conclusively, but it can be maximised by building on more general work on semantic representation; for example, the guidelines introduced here use a conception of events and participants compatible with that of FrameNet (Baker et al., 1998). Good results on agreement and baseline classification will provide positive evidence for the coherence and balance of the classes; agreement measures can confirm ease of annotation. In choosing an appropriate level of granularity, I wished to avoid positing a large number of detailed but rare categories. Levi’s (1978) set of nine semantic relations was used as a starting point. The development process involved a series of revisions over six months, aimed at satisfying the six criteria above and maximising interannotator agreement in annotation trials. The nature of the decisions which had to</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles Fillmore, and John Lowe. 1998. The Berkeley FrameNet project. In Proc. ACLCOLING-98, pages 86–90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="10180" citStr="Cohen, 1960" startWordPosition="1619" endWordPosition="1620">.informatics.susx.ac.uk/ research/nlp/rasp/ relation, the direction of the relation (it is a hut in a forest, not a forest in a hut) and that the label is licensed by rule 2.1.3.1 in the guidelines (N1/N2 is an object spatially located in or near N2/N1).6 Two trial batches of 100 compounds were annotated to familiarise the second annotator with the guidelines and to confirm that the guidelines were indeed usable for others. The first trial resulted in agreement of 52% and the second in agreement of 73%. The result of the second trial, corresponding to a Kappa beyond-chance agreement estimate (Cohen, 1960) of n� = 0.693, was very impressive and it was decided to proceed to a larger-scale task. 500 compounds not used in the trial runs were drawn from the 2,000-item set and annotated. 3.3 Results and Analysis Agreement on the test set was 66.2% with n� = 0.62. This is less than the score achieved in the second trial run, but may be a more accurate estimator of the true population n due to the larger sample size. On the other hand, the larger dataset may have caused annotator fatigue. Pearson standardised residuals (Haberman, 1973) were calculated to identify the main sources of disagreement.7 In </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences: Word Pairs and Collocations.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at Stuttgart.</institution>
<contexts>
<context position="18533" citStr="Evert (2004)" startWordPosition="2986" endWordPosition="2987"> common in the dataset, which is why the macro-averaged figures are slightly lower than the micro-averaged accuracy. 4.3 Discussion It is interesting that the conjunction-based features give the best performance, as these features are also the most sparse. This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for 10This measure is relatively robust where frequency counts are low and consistently outperformed other association measures in the empirical evaluation of Evert (2004). Raw G2 Accuracy Macro Accuracy Macro w5 52.60% 51.07% 51.35% 49.93% w10 51.84% 50.32% 50.10% 48.60% Rbasic 51.28% 49.92% 51.83% 50.26% Rmod 51.35% 50.06% 48.51% 47.03% Rverb 48.79% 47.13% 48.58% 47.07% Rconj 54.12% 52.44% 54.95% 53.42% Table 2: Performance of BNC co-occurrence data compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5). In comparing reported classification results, it is difficult to disentangle the effects of different data, annotation schemes and classification methods. The results described here should above all be taken to demonstra</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>Stefan Evert. 2004. The Statistics of Word Cooccurrences: Word Pairs and Collocations. Ph.D. thesis, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="3125" citStr="Girju et al. (2005)" startWordPosition="490" endWordPosition="493">ne of the problems is that it is not immediately clear what level of granularity is desirable, or even what kind of units the categories should be. Lauer (1995) proposes a set of 8 prepositions that can be used to paraphrase compounds: a cheese knife is a knife FOR cheese but a kitchen knife is a knife (used) IN a kitchen. An advantage of this approach is that preposition-noun co-occurrences can efficiently be mined from large corpora using shallow techniques. On the other hand, interpreting a paraphrase requires further disambiguation as one preposition can map onto many semantic relations.1 Girju et al. (2005) and Nastase and Szpakowicz (2003) both present large inventories of seman1The interpretation of prepositions is itself the focus of a Semeval task in 2007. 73 Proceedings of the ACL 2007 Student Research Workshop, pages 73–78, Prague, June 2007. c�2007 Association for Computational Linguistics tic relations that describe noun-noun dependencies. Such relations provide richer semantic information, but it is harder for both humans and machines to identify their occurrence in text. Larger inventories can also suffer from class sparsity; for example, 14 of Girju et al.’s 35 relations do not occur </context>
<context position="13376" citStr="Girju et al. (2005)" startWordPosition="2165" endWordPosition="2168">pretation is closer to the dynamic of real-world language use. Context can both help and hinder agreement, and it is not clear whether in- or out-of-context annotation is easier. Previous work has given out-of-context agreement figures for corpus data. Kim and Baldwin (2005) report an experiment using 2,169 compounds taken from newspaper text and the categories of Nastase and Szpakowicz (2003). Their annotators could assign multiple labels in case of doubt and were judged to agree on an item if their annotations had any label in common. This less stringent measure yielded agreement of 52.31%. Girju et al. (2005) report agreement for annotation using both Lauer’s 8 prepositional labels (h = 0.8) and their own 35 semantic relations (h = 0.58). These figures are difficult to interpret as annotators were again allowed assign multiple labels (for the prepositions this occurred in “almost all” cases) and the multiply-labelled items were excluded from the calculation of Kappa. This entails discarding the items which are hardest to classify and thus most likely to cause disagreement. Girju (2006) has recently published impressive agreement results on a related task. This involved annotating 2,200 compounds e</context>
<context position="21058" citStr="Girju et al., 2005" startWordPosition="3373" endWordPosition="3376">ad and modifier. The same 11http://wordnet.princeton.edu/ 77 classification method is used as in Section 4. 5.2 Results and Discussion This method achieves accuracy of 56.76% and macro-averaged accuracy of 54.59%, slightly higher than that achieved by the co-occurrence features. Combining WordNet and co-occurrence vectors by simply concatenating the G2-transformed Rconj vector and WordNet feature vector for each compound gives a further boost to 58.35% accuracy (56.70% macro-average). These results are higher than those reported for similar approaches on open-text data (Kim and Baldwin, 2005; Girju et al., 2005), though the same caveat applies about comparison. The best results (over 70%) reported so far for compound interpretation use a combination of multiple lexical resources and detailed additional annotation (Girju et al., 2005; Girju, 2006). 6 Conclusion and Future Directions The annotation scheme described above has been tested on a rigorous multiple-annotator task and achieved superior agreement to comparable results in the literature. Further refinement should be possible but would most likely yield diminishing returns. In the classification experiments, my goal was to see what level of perf</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19(4):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Out-of-context noun phrase semantic interpretation with cross-linguistic evidence.</title>
<date>2006</date>
<booktitle>In Proc. CIKM-06,</booktitle>
<pages>268--276</pages>
<location>Arlington, VA.</location>
<contexts>
<context position="1824" citStr="Girju, 2006" startWordPosition="282" endWordPosition="283">nife is (probably) a knife for cutting cheese and (probably) not a knife made of cheese (cf. plastic knife) does not just require an ability to identify the senses of cheese and knife but also knowledge about what one usually does with cheese and knives. These factors combine to yield a difficult problem that exhibits many of the challenges characteristic of lexical semantic processing in general. Recent research has made significant progress on solving the problem with statistical methods and often without the need for manually created lexical resources (Lauer, 1995; Lapata and Keller, 2004; Girju, 2006; Turney, 2006). The work presented here is part of an ongoing project that treats compound interpretation as a classification problem to be solved using machine learning. 2 Selecting an Annotation Scheme For many classification tasks, such as part-of-speech tagging or word sense disambiguation, there is general agreement on a standard set of categories that is used by most researchers. For the compound interpretation task, on the other hand, there is little agreement and numerous classification schemes have been proposed. This hinders meaningful comparison of different methods and results. On</context>
<context position="13862" citStr="Girju (2006)" startWordPosition="2244" endWordPosition="2245">tem if their annotations had any label in common. This less stringent measure yielded agreement of 52.31%. Girju et al. (2005) report agreement for annotation using both Lauer’s 8 prepositional labels (h = 0.8) and their own 35 semantic relations (h = 0.58). These figures are difficult to interpret as annotators were again allowed assign multiple labels (for the prepositions this occurred in “almost all” cases) and the multiply-labelled items were excluded from the calculation of Kappa. This entails discarding the items which are hardest to classify and thus most likely to cause disagreement. Girju (2006) has recently published impressive agreement results on a related task. This involved annotating 2,200 compounds extracted from an online dictionary, each presented in five languages, and resulted in a Kappa score of 0.67. This task may have been facilitated by the data source and its multilingual nature. It seems plausible that dictionary entries are more likely to refer to familiar concepts than compounds extracted from a balanced corpus, which are frequently context-dependent coinages or rare specialist terms. Furthermore, the translations of compounds in Romance languages often provide inf</context>
<context position="21297" citStr="Girju, 2006" startWordPosition="3412" endWordPosition="3413">ed by the co-occurrence features. Combining WordNet and co-occurrence vectors by simply concatenating the G2-transformed Rconj vector and WordNet feature vector for each compound gives a further boost to 58.35% accuracy (56.70% macro-average). These results are higher than those reported for similar approaches on open-text data (Kim and Baldwin, 2005; Girju et al., 2005), though the same caveat applies about comparison. The best results (over 70%) reported so far for compound interpretation use a combination of multiple lexical resources and detailed additional annotation (Girju et al., 2005; Girju, 2006). 6 Conclusion and Future Directions The annotation scheme described above has been tested on a rigorous multiple-annotator task and achieved superior agreement to comparable results in the literature. Further refinement should be possible but would most likely yield diminishing returns. In the classification experiments, my goal was to see what level of performance could be gained by using straightforward techniques so as to provide a meaningful baseline for future research. Good results were achieved with methods that rely neither on massive corpora or broad-coverage lexical resources, thoug</context>
</contexts>
<marker>Girju, 2006</marker>
<rawString>Roxana Girju. 2006. Out-of-context noun phrase semantic interpretation with cross-linguistic evidence. In Proc. CIKM-06, pages 268–276, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shelby J Haberman</author>
</authors>
<title>The analysis of residuals in cross-classified tables.</title>
<date>1973</date>
<journal>Biometrics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10713" citStr="Haberman, 1973" startWordPosition="1716" endWordPosition="1717">ond trial, corresponding to a Kappa beyond-chance agreement estimate (Cohen, 1960) of n� = 0.693, was very impressive and it was decided to proceed to a larger-scale task. 500 compounds not used in the trial runs were drawn from the 2,000-item set and annotated. 3.3 Results and Analysis Agreement on the test set was 66.2% with n� = 0.62. This is less than the score achieved in the second trial run, but may be a more accurate estimator of the true population n due to the larger sample size. On the other hand, the larger dataset may have caused annotator fatigue. Pearson standardised residuals (Haberman, 1973) were calculated to identify the main sources of disagreement.7 In the context of inter-annotator agreement one expects these residuals to have large positive values on the agreement diagonal and negative values in all other cells. Among the six main relations listed at the top of Table 1, a small positive association was observed between INST and ABOUT, indicating that borderline topics such as assessment task and gas alarm were likely to be annotated as INST by the first annotator and ABOUT by the second. It seems that the guidelines might need to clarify this category boundary. It is clear </context>
</contexts>
<marker>Haberman, 1973</marker>
<rawString>Shelby J. Haberman. 1973. The analysis of residuals in cross-classified tables. Biometrics, 29(1):205–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic interpretation of noun compounds using WordNet similarity.</title>
<date>2005</date>
<booktitle>In Proc. IJCNLP-05,</booktitle>
<pages>945--956</pages>
<location>Jeju Island,</location>
<contexts>
<context position="13032" citStr="Kim and Baldwin (2005)" startWordPosition="2104" endWordPosition="2108">COMP. This left 343 items with agreement of 73.6% and k = 0.683. 3.4 Discussion This is the first work I am aware of where compounds were annotated in their sentential context. This aspect is significant, as compound meaning is often context dependent (compare school management decided... and principles of school management) and in-context interpretation is closer to the dynamic of real-world language use. Context can both help and hinder agreement, and it is not clear whether in- or out-of-context annotation is easier. Previous work has given out-of-context agreement figures for corpus data. Kim and Baldwin (2005) report an experiment using 2,169 compounds taken from newspaper text and the categories of Nastase and Szpakowicz (2003). Their annotators could assign multiple labels in case of doubt and were judged to agree on an item if their annotations had any label in common. This less stringent measure yielded agreement of 52.31%. Girju et al. (2005) report agreement for annotation using both Lauer’s 8 prepositional labels (h = 0.8) and their own 35 semantic relations (h = 0.58). These figures are difficult to interpret as annotators were again allowed assign multiple labels (for the prepositions this</context>
<context position="21037" citStr="Kim and Baldwin, 2005" startWordPosition="3369" endWordPosition="3372">uch vectors, for the head and modifier. The same 11http://wordnet.princeton.edu/ 77 classification method is used as in Section 4. 5.2 Results and Discussion This method achieves accuracy of 56.76% and macro-averaged accuracy of 54.59%, slightly higher than that achieved by the co-occurrence features. Combining WordNet and co-occurrence vectors by simply concatenating the G2-transformed Rconj vector and WordNet feature vector for each compound gives a further boost to 58.35% accuracy (56.70% macro-average). These results are higher than those reported for similar approaches on open-text data (Kim and Baldwin, 2005; Girju et al., 2005), though the same caveat applies about comparison. The best results (over 70%) reported so far for compound interpretation use a combination of multiple lexical resources and detailed additional annotation (Girju et al., 2005; Girju, 2006). 6 Conclusion and Future Directions The annotation scheme described above has been tested on a rigorous multiple-annotator task and achieved superior agreement to comparable results in the literature. Further refinement should be possible but would most likely yield diminishing returns. In the classification experiments, my goal was to s</context>
</contexts>
<marker>Kim, Baldwin, 2005</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2005. Automatic interpretation of noun compounds using WordNet similarity. In Proc. IJCNLP-05, pages 945–956, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL-04,</booktitle>
<pages>121--128</pages>
<location>Boston, MA.</location>
<contexts>
<context position="1811" citStr="Lapata and Keller, 2004" startWordPosition="278" endWordPosition="281">, knowing that a cheese knife is (probably) a knife for cutting cheese and (probably) not a knife made of cheese (cf. plastic knife) does not just require an ability to identify the senses of cheese and knife but also knowledge about what one usually does with cheese and knives. These factors combine to yield a difficult problem that exhibits many of the challenges characteristic of lexical semantic processing in general. Recent research has made significant progress on solving the problem with statistical methods and often without the need for manually created lexical resources (Lauer, 1995; Lapata and Keller, 2004; Girju, 2006; Turney, 2006). The work presented here is part of an ongoing project that treats compound interpretation as a classification problem to be solved using machine learning. 2 Selecting an Annotation Scheme For many classification tasks, such as part-of-speech tagging or word sense disambiguation, there is general agreement on a standard set of categories that is used by most researchers. For the compound interpretation task, on the other hand, there is little agreement and numerous classification schemes have been proposed. This hinders meaningful comparison of different methods an</context>
<context position="19841" citStr="Lapata and Keller (2004)" startWordPosition="3183" endWordPosition="3187">e a baseline for future work on the same data. In terms of methodology, Turney’s (2006) Vector Space Model experiments are most similar. Using feature vectors derived from lexical patterns and frequencies returned by a Web search engine, a nearest-neighbour classifier achieves 45.7% accuracy on compounds annotated with 5 semantic classes. Turney improves accuracy to 58% with a combination of query expansion and linear dimensionality reduction. This method trades off efficiency for accuracy, requiring many times more resources in terms of time, storage and corpus size than that described here. Lapata and Keller (2004) obtain accuracy of 55.71% on Lauer’s (1995) prepositionally annotated data using simple search engine queries. Their method has the advantage of not requiring supervision, but it cannot be used with deep semantic relations. 5 SVM Classification with WordNet 5.1 Method The experiments reported in this section make a basic use of the WordNet11 hierarchy. Binary feature vectors are used whereby a vector entry is 1 if the item belongs to or is a hyponym of the synset corresponding to that feature, and 0 otherwise. Each compound is represented by the concatenation of two such vectors, for the head</context>
</contexts>
<marker>Lapata, Keller, 2004</marker>
<rawString>Mirella Lapata and Frank Keller. 2004. The Web as a baseline: Evaluating the performance of unsupervised Web-based models for a range of NLP tasks. In Proc. HLT-NAACL-04, pages 121–128, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Compound Nouns.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University.</institution>
<contexts>
<context position="1786" citStr="Lauer, 1995" startWordPosition="276" endWordPosition="277">. For example, knowing that a cheese knife is (probably) a knife for cutting cheese and (probably) not a knife made of cheese (cf. plastic knife) does not just require an ability to identify the senses of cheese and knife but also knowledge about what one usually does with cheese and knives. These factors combine to yield a difficult problem that exhibits many of the challenges characteristic of lexical semantic processing in general. Recent research has made significant progress on solving the problem with statistical methods and often without the need for manually created lexical resources (Lauer, 1995; Lapata and Keller, 2004; Girju, 2006; Turney, 2006). The work presented here is part of an ongoing project that treats compound interpretation as a classification problem to be solved using machine learning. 2 Selecting an Annotation Scheme For many classification tasks, such as part-of-speech tagging or word sense disambiguation, there is general agreement on a standard set of categories that is used by most researchers. For the compound interpretation task, on the other hand, there is little agreement and numerous classification schemes have been proposed. This hinders meaningful compariso</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Compound Nouns. Ph.D. thesis, Macquarie University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith N Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Levi, 1978</marker>
<rawString>Judith N. Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring noun-modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Proc. IWCS-5,</booktitle>
<location>Tilburg, Netherlands.</location>
<contexts>
<context position="3159" citStr="Nastase and Szpakowicz (2003)" startWordPosition="495" endWordPosition="499">at it is not immediately clear what level of granularity is desirable, or even what kind of units the categories should be. Lauer (1995) proposes a set of 8 prepositions that can be used to paraphrase compounds: a cheese knife is a knife FOR cheese but a kitchen knife is a knife (used) IN a kitchen. An advantage of this approach is that preposition-noun co-occurrences can efficiently be mined from large corpora using shallow techniques. On the other hand, interpreting a paraphrase requires further disambiguation as one preposition can map onto many semantic relations.1 Girju et al. (2005) and Nastase and Szpakowicz (2003) both present large inventories of seman1The interpretation of prepositions is itself the focus of a Semeval task in 2007. 73 Proceedings of the ACL 2007 Student Research Workshop, pages 73–78, Prague, June 2007. c�2007 Association for Computational Linguistics tic relations that describe noun-noun dependencies. Such relations provide richer semantic information, but it is harder for both humans and machines to identify their occurrence in text. Larger inventories can also suffer from class sparsity; for example, 14 of Girju et al.’s 35 relations do not occur in their dataset and 7 more occur </context>
<context position="13153" citStr="Nastase and Szpakowicz (2003)" startWordPosition="2124" endWordPosition="2127">of where compounds were annotated in their sentential context. This aspect is significant, as compound meaning is often context dependent (compare school management decided... and principles of school management) and in-context interpretation is closer to the dynamic of real-world language use. Context can both help and hinder agreement, and it is not clear whether in- or out-of-context annotation is easier. Previous work has given out-of-context agreement figures for corpus data. Kim and Baldwin (2005) report an experiment using 2,169 compounds taken from newspaper text and the categories of Nastase and Szpakowicz (2003). Their annotators could assign multiple labels in case of doubt and were judged to agree on an item if their annotations had any label in common. This less stringent measure yielded agreement of 52.31%. Girju et al. (2005) report agreement for annotation using both Lauer’s 8 prepositional labels (h = 0.8) and their own 35 semantic relations (h = 0.58). These figures are difficult to interpret as annotators were again allowed assign multiple labels (for the prepositions this occurred in “almost all” cases) and the multiply-labelled items were excluded from the calculation of Kappa. This entail</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2003. Exploring noun-modifier semantic relations. In Proc. IWCS-5, Tilburg, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Eugene Charniak</author>
</authors>
<title>Nounphrase co-occurrence statistics for semi-automatic semantic lexicon construction.</title>
<date>1998</date>
<booktitle>In Proc. ACL-COLING98,</booktitle>
<pages>1110--1106</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="18307" citStr="Roark and Charniak, 1998" startWordPosition="2952" endWordPosition="2955">4.95% accuracy (53.42% macro-average). Analysis of performance across categories shows that in all cases accuracy is lower (usually below 30%) on the BE and HAVE relations than on the others (often above 50%). These two relations are least common in the dataset, which is why the macro-averaged figures are slightly lower than the micro-averaged accuracy. 4.3 Discussion It is interesting that the conjunction-based features give the best performance, as these features are also the most sparse. This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for 10This measure is relatively robust where frequency counts are low and consistently outperformed other association measures in the empirical evaluation of Evert (2004). Raw G2 Accuracy Macro Accuracy Macro w5 52.60% 51.07% 51.35% 49.93% w10 51.84% 50.32% 50.10% 48.60% Rbasic 51.28% 49.92% 51.83% 50.26% Rmod 51.35% 50.06% 48.51% 47.03% Rverb 48.79% 47.13% 48.58% 47.07% Rconj 54.12% 52.44% 54.95% 53.42% Table 2: Performance of BNC co-occurrence data compound interpretation, as evidenced by the success of WordNet-based methods (see Sectio</context>
</contexts>
<marker>Roark, Charniak, 1998</marker>
<rawString>Brian Roark and Eugene Charniak. 1998. Nounphrase co-occurrence statistics for semi-automatic semantic lexicon construction. In Proc. ACL-COLING98, pages 1110–1106, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Co-occurrence contexts for corpus-based noun compound interpretation.</title>
<date>2007</date>
<booktitle>In Proc. of the ACL Workshop A Broader Perspective on Multiword Expressions,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>S´eaghdha, 2007</marker>
<rawString>Diarmuid O´ S´eaghdha. 2007a. Co-occurrence contexts for corpus-based noun compound interpretation. In Proc. of the ACL Workshop A Broader Perspective on Multiword Expressions, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Designing and evaluating a semantic annotation scheme for compound nouns.</title>
<date>2007</date>
<booktitle>In Proc. Corpus Linguistics</booktitle>
<location>Birmingham, UK.</location>
<marker>S´eaghdha, 2007</marker>
<rawString>Diarmuid O´ S´eaghdha. 2007b. Designing and evaluating a semantic annotation scheme for compound nouns. In Proc. Corpus Linguistics 2007, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1839" citStr="Turney, 2006" startWordPosition="284" endWordPosition="285">ably) a knife for cutting cheese and (probably) not a knife made of cheese (cf. plastic knife) does not just require an ability to identify the senses of cheese and knife but also knowledge about what one usually does with cheese and knives. These factors combine to yield a difficult problem that exhibits many of the challenges characteristic of lexical semantic processing in general. Recent research has made significant progress on solving the problem with statistical methods and often without the need for manually created lexical resources (Lauer, 1995; Lapata and Keller, 2004; Girju, 2006; Turney, 2006). The work presented here is part of an ongoing project that treats compound interpretation as a classification problem to be solved using machine learning. 2 Selecting an Annotation Scheme For many classification tasks, such as part-of-speech tagging or word sense disambiguation, there is general agreement on a standard set of categories that is used by most researchers. For the compound interpretation task, on the other hand, there is little agreement and numerous classification schemes have been proposed. This hinders meaningful comparison of different methods and results. One must therefor</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Wilson</author>
<author>Jenny Thomas</author>
</authors>
<title>Semantic annotation. In</title>
<date>1997</date>
<editor>R. Garside, G. Leech, and A. McEnery, editors, Corpus Annotation.</editor>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="5555" citStr="Wilson and Thomas (1997)" startWordPosition="876" endWordPosition="879">aken in evaluation to consider macro-averaged performance as well as raw accuracy. It has been suggested that classifiers trained on skewed data may perform poorly on minority classes (Zhang and Oles, 2001). Of course, this is not a justification for conflating concepts with little in common, and it may well be that the natural distribution of data is inherently skewed. There is clearly a tension between these criteria, and only a best-fit solution is possible. However, it was felt that a new scheme might satisfy them more optimally than existing schemes. Such a proposal 2One relevant work is Wilson and Thomas (1997). Relation Distribution Example BE 191 (9.55%) steel knife HAVE 199 (9.95%) street name IN 308 (15.40%) forest hut INST 266 (13.30%) rice cooker ACTOR 236 (11.80%) honey bee ABOUT 243 (12.15%) fairy tale REL 81 (4.05%) camera gear LEX 35 (1.75%) home secretary UNKNOWN 9 (0.45%) simularity crystal MISTAG 220 (11.00%) blazing fire NONCOMP 212 (10.60%) [real tennis] club Table 1: Sample class frequencies necessitates a method of evaluation. Not all the criteria are easily evaluable. It is difficult to prove generalisability and usefulness conclusively, but it can be maximised by building on more </context>
</contexts>
<marker>Wilson, Thomas, 1997</marker>
<rawString>Andrew Wilson and Jenny Thomas. 1997. Semantic annotation. In R. Garside, G. Leech, and A. McEnery, editors, Corpus Annotation. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Zhang</author>
<author>Frank J Oles</author>
</authors>
<title>Text categorization based on regularized linear classification methods.</title>
<date>2001</date>
<journal>Information Retrieval,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="5137" citStr="Zhang and Oles, 2001" startWordPosition="806" endWordPosition="809">he guidelines should make the annotation process as simple as possible 6. The categories should provide useful semantic information These intuitively appear to be desirable principles for any semantic annotation scheme. The requirement of class distribution balance is motivated by the classification task. Where one category dominates, the most-frequent-class baseline can be difficult to exceed and care must be taken in evaluation to consider macro-averaged performance as well as raw accuracy. It has been suggested that classifiers trained on skewed data may perform poorly on minority classes (Zhang and Oles, 2001). Of course, this is not a justification for conflating concepts with little in common, and it may well be that the natural distribution of data is inherently skewed. There is clearly a tension between these criteria, and only a best-fit solution is possible. However, it was felt that a new scheme might satisfy them more optimally than existing schemes. Such a proposal 2One relevant work is Wilson and Thomas (1997). Relation Distribution Example BE 191 (9.55%) steel knife HAVE 199 (9.95%) street name IN 308 (15.40%) forest hut INST 266 (13.30%) rice cooker ACTOR 236 (11.80%) honey bee ABOUT 24</context>
</contexts>
<marker>Zhang, Oles, 2001</marker>
<rawString>Tong Zhang and Frank J. Oles. 2001. Text categorization based on regularized linear classification methods. Information Retrieval, 4(1):5–31.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>