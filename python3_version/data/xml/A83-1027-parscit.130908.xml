<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.860504">
AN APPLICATION OF MONTAGUE GRAMMAR TO ENGLISH-JAPANESE MACHINE TRANSLATION
Toyoaki NISHIDA and Shuji DOSHITA
</note>
<affiliation confidence="0.948066">
Dept. of Information Science
Faculty of Engineering, Kyoto University
</affiliation>
<address confidence="0.722001">
Sakyo-ku, Kyoto 606, JAPAN
</address>
<email confidence="0.853471">
ABSTRACT
</email>
<bodyText confidence="0.796261769230769">
English-Japanese machine translation
requires a large amount of structural transfor-
mations in both grammatical and conceptual level.
In order to make its control structure clearer
and more understandable, this paper proposes a
model based on Montague Gramamr. Translation
process is modeled as a data flow computation
process. Formal description tools are developed
and a prototype system is constructed. Various
problems which arise in this modeling and their
solutions are described. Results of experiments
are shown and it is discussed how far initial
goals are achieved.
</bodyText>
<listItem confidence="0.390345">
1. GOAL OF INTERMEDIATE REPRESENTATION DESIGN
</listItem>
<figureCaption confidence="0.73392">
Differences between English and Japanese
exist not only in grammatical level but also in
conceptual level. Examples are illustrated in
Fig.l. Accordingly, a large amount of transfor-
</figureCaption>
<bodyText confidence="0.934755833333333">
mations in various levels are required in order
to obtain high quality translation. The goal of
this research is to provide a good framework for
carrying out those operations systematically.
The solution depends on the design of intermedi-
ate representation (IR). Basic requirements to
intermediate representation design are listed
below.
a) Accuracy: IR should retain logical conclu-
sion of natural language expression. The follow-
ing distinctions, for example, should be made in
IR level:
- partial/total negation
- any-exist/exist-any
active/passive
- restrictive use/ nonrestrictive use, etc.
In other words, scope of operators should be
represented precisely.
</bodyText>
<figure confidence="0.940256703703704">
GRAMMATICAL difference
a) Case Marking:
&lt;E&gt;: (relative position) + preposition
&lt;J&gt;: postposition (called JOSHI)
b) Word Order
i) simple sentence
&lt;E&gt;: S+V+0 : te le
(J&gt;: S+0+V : WAf-ASHI-TW RINGO 46 TABETR
ii) preposition vs postposition
CE&gt;: PREP+NP : Ljherefrigator
&lt;J&gt;: NP+JOSHI : TEIZOUFT6551 NAKA-W01,0
iii) order of modification
&lt;E&gt;: NP+POSTMODEFEER: 2T apple, on the box,
&lt;J&gt;: PRENOMINAL MODIFIER+NP:TaK0 NO UE NO RINGO&apos;
translate HONYAKU SURU
KAISHAKU SURU
interpret
understand RIKAI SURU
grasp TSUKAMU
hold TAMOTSU
keep KAMORU
CONCEPTUAL difference
&lt;E&gt; her arrival makes him happy
i.. paraphrasing is needed
&lt;J&gt; KARE WA KANOJO GA TOUCHAKU SHITA NODE
URESHIL.
(he becomes happy because she has arrived)
</figure>
<figureCaption confidence="0.6582885">
Fig.l. Examples of Differences between English
and Japanese.
</figureCaption>
<figure confidence="0.697674">
&lt;E&gt;: English; KJ&gt;: Japanese.
LEXICAL difference
&lt;E&gt; &lt;J&gt;
</figure>
<page confidence="0.994699">
156
</page>
<bodyText confidence="0.911871642857143">
b) Ability of representing semantic relations:
In English-Japanese translation, it is often the
case that a given English word must be translated
into different Japanese words or phrases if it
has more than one word meanings. But it is not
reasonable to capture this problem solely as a
problem of word meaning disambiguation in analy-
sis phase; the needed depth of disambiguation
depends on target language. So it is also
handled in transfer phase. In general, meaning
of a given word is recognized based on the rela-
tion to other constituents in the sentence or
text which is semantically related to the given
word. To make this possible in transfer phase,
IR must provide a link to semantically related
constituents of a given item. For example, an
object of a verb should be accessible in IR level
from the verb, even if the relation is implicit
in the surface structure (eg., passives, relative
clauses, and their combinations, etc.)
c) Prediction of control: given an IR expres-
sion, the model should be able to predict
explicitly what operations are to be done in what
order.
d) Lexicon driven: some sort of transforma-
tion rules are word specific. The IR interpreta-
tion system should be designed to deal with those
word specific rules easily.
</bodyText>
<listItem confidence="0.997495333333333">
e) Computability: All processings should be
effectively computable. Any IR is useless if it
is not computable.
</listItem>
<sectionHeader confidence="0.919793" genericHeader="method">
2. PRINCIPLE OF TRANSLATION
</sectionHeader>
<bodyText confidence="0.971427375">
This section outlines our solution to the
requirements posed in the preceding section. We
employ Montague Grammar (Montague 1974, Dowty
1981) as a theoretical basis of translation model.
Intermediate representation is designed based on
intensional logic. Intermediate representation
for a given natural language expression is
obtained by what we call functional analysis.
</bodyText>
<subsectionHeader confidence="0.7258915">
2.1 Functional Analysis
In functional analysis, input sentence is
decomposed into groups of constituents and
interrelationship among those groups are analyzed
in terms of function-argument relationships.
Suppose a sentence:
</subsectionHeader>
<construct confidence="0.3384512">
I don&apos;t have a book. (1)
The functional analysis makes following two
points:
a) (1) is decomposed as:
&amp;quot;I have a book&amp;quot; + &amp;quot;not&amp;quot;. (2)
</construct>
<bodyText confidence="0.878527">
b) In the decomposition (2), &amp;quot;not&amp;quot; is an
operator or function to &amp;quot;I have a book.&amp;quot;
The result of this analysis can be depicted as
follows:
</bodyText>
<equation confidence="0.8817">
&amp;quot;not&amp;quot;? &amp;quot;I have a book&amp;quot;i (3)
</equation>
<bodyText confidence="0.997692636363636">
where CD&apos; denotes a function and 11111 denotes
an argument. The role of &amp;quot;not&amp;quot; as a function is:
&amp;quot;not&amp;quot; as a semantic operator:
it negates a given proposition;
&amp;quot;not&amp;quot; as a syntactic operator:
it inserts an appropriate auxiliary verb
and a lexical item &amp;quot;not&amp;quot; into appropriate
position of its argument. (4)
This kind of analysis goes on further with
embedded sentence until it is decomposed into
lexical units or even morphemes.
</bodyText>
<subsectionHeader confidence="0.998211">
2.2 Montague Grammar as a Basic Theory
</subsectionHeader>
<bodyText confidence="0.945218133333333">
Montague Grammar (MG) gives a basis of func-
tional analysis. One of the advantages of MG
consists in its interpretation system of function
form (or intensional logical form). In MG, inter-
pretation of an intensional logical formula is a
mapping I from intensional logical formulas to
set theoretical domain. Important property is
that this mapping I is defined under the cons-
traint of compositionality, that is, I satisfies:
...), (5)
without regard to what f, a, b, etc. are. This
property simplifies control structure and it also
specifies what operations are done in what order.
For example, suppose input data has a structure
like:
</bodyText>
<equation confidence="0.694247">
(6)
</equation>
<bodyText confidence="0.722699818181818">
For the sake of property (5), the interpretation
of (6) is done as a data flow computation process
as follows:
By this property, we can easily grasp the process-
ing stream. In particular, we can easily shoot
trouble and source of abnormality when debugging
a system.
Due to the above property and others, in
particular due to its rigorous framework based 0,1
logic, MG has been studied in information science
field (Hobbs 1978, Friedman 1978, Yonezaki 1980,
</bodyText>
<page confidence="0.99709">
157
</page>
<bodyText confidence="0.999163">
Nishida 1980, Landsbergen 1980, Moran 1982, Moore
1981, Rosenschein 1982, ...). Application of MG
to machine translation was also attempted
(Hauenschild 1979, Landsbergen 1982), but those
systems have only partially utilized the power of
MG. Our approach attempts to utilize the full
power of MG.
</bodyText>
<subsectionHeader confidence="0.998421">
2.3 Application of Montague Grammar to
Machine Translation
</subsectionHeader>
<bodyText confidence="0.999404923076923">
In order to obtain the syntactic structure
in Japanese from an intensional logical form, in
the same way as interpretation process of MG, we
change the semantic domain from set theoretical
domain to conceptual domain for Japanese. Each
conceptual unit contains its syntactic expression
in Japanese. Syntactic aspect is stressed for
generating syntactic structure in Japanese.
Conceptual information is utilized for semantic
based word choice and paraphrasing.
For example, the following function in
Japanese syntactic domain is assigned to a
logical item &amp;quot;not&amp;quot;:
</bodyText>
<equation confidence="0.965716">
(LAMBDA (x) (SENTENCE x [AUX &amp;quot;NAI&amp;quot;])). (8)
</equation>
<bodyText confidence="0.516401">
Transfer-generation process for the sentence (1)
looks like:
&amp;quot;I don&apos;t have a book&amp;quot;
</bodyText>
<subsectionHeader confidence="0.997781">
3.1 Definition of Formal Tools
</subsectionHeader>
<bodyText confidence="0.941385818181818">
a) English oriented Formal Representation (EFR)
is a version of intensional logic, and gives a
rigorous formalism for describing the results of
functional analysis. It is based on Cresswell&apos;s
lambda deep structure (Cresawell 1973). Each
expression has a uniquely defined type. Lambda
form is employed to denote function itself.
b) Conceptual Phrase Structure (CPS) is a data
structure in which syntactic and semantic informa-
tion of a Japanese lexical unit or phrase struc-
ture are packed.
</bodyText>
<listItem confidence="0.862178714285714">
0 example of CPS for a lexical item:
EIGO:[NP &amp;quot;EIGO&amp;quot; with ISA=LANGUAGE; ...,] (9)
category; lexical item; conceptual info.
; &amp;quot;EIGO&amp;quot; means English language.
ii) example of CPS for phrase structure:
[NP [ADJ &amp;quot;AKAI&amp;quot; with ... ]
[NOUN &amp;quot;RINGO&amp;quot; with ... ] with ... ] (10)
; &amp;quot;AKAI&amp;quot; means red, and &amp;quot;RINGO&amp;quot; means apple.
c) CPS Form (CPSF) is a form which denotes
operation or function on CPS domain. It is used
to give descriptions to mappings from EFR to CPS.
Constituents of CPSF are:
0 Constants: CPS.
ii) Variables: x, y, .
</listItem>
<bodyText confidence="0.59399">
(indicated by lower case strings).
iii) Variables with constraints:
e.g., (! SENTENCE x).
; variable x which must be
of category SENTENCE.
</bodyText>
<figure confidence="0.975165083333333">
&amp;quot;not&amp;quot; &gt; &amp;quot;I have a book&amp;quot;
Ii
.TRANSFER
TRANSFER,GENERATION
(LAMBDA (x)
(SENTENCE x (AUX &amp;quot;NAI&amp;quot;]})
WATASHI-WA HON-WO MOTSU
Nirs■einesanumummob./
AUX
I
WATASHI-WA HON-WO MOTSU NAI
MOTANAI
</figure>
<sectionHeader confidence="0.991433" genericHeader="method">
3. FORMAL TOOLS
</sectionHeader>
<bodyText confidence="0.999768">
Formal description tools have been developed
to provide a precise description of the idea men-
tioned in the last section.
</bodyText>
<equation confidence="0.7792035">
iv) Transformations:
e.g., (+ TENSE (TENSE..PAST), x).
indicator; operator-name; PARAMs; argument
v) CPS construction:
e.g., (SENTENCE (x y) with ... ).
new category; descendents
vi) Conditionals:
[ &lt;condition&gt;1 -&gt; &lt;CPSF&gt;i; ... 1.
</equation>
<bodyText confidence="0.875366">
vii) Lambda form:
e.g., (LAMBDA (x) (+ PASSIVE 0 x))
Using those description tools, translation
process is modeled as a three staged process:
stage 1 (analysis): anlyzes English
sentence and extracts EFR form,
stage 2 (transfer): substitutes CPSF to
each lexical item in the EFR form,
</bodyText>
<page confidence="0.972142">
158
</page>
<figure confidence="0.992962897435897">
He does not always came late.
not ( always ( he late C comes )))) EFR
(TRANSFER)
+NEG I Aq SE+NEG-4-1-0t/17CItZVI] P NP
ADV S he
I I
*it 9
always 4•14
■
S.
ADIV ,r)}
id&lt;ttr
late
/ S
Ax NP VP
x tfi5,51
I 1
Z\ )
[
\ x comes ) ji
,\\111,1
CPSF
) +NEG--)-11-Mtr114;VI]
ADV
/10: tiEtsid&lt;rt-C3(EZ
always he comes Late
AUX
atilts&lt;tvcAz bo-c4atatia
always he comes late it is not the case that
Xx
NP
ADV VP
I I
x ZS&lt; tvC5EZ
x comes late -
(ENERATION)
tlEtisia&lt;tvCNEZ
he comes tate
.. CPS ..
</figure>
<figureCaption confidence="0.298859">
Fig.2. Example of Translation Process // Prefix notation is used for CPSF,
</figureCaption>
<bodyText confidence="0.9319762">
/
described using Formal Tools. and syntactic aspect is emphasized.
stage 3 (generation): evaluates the CPSF to
get CPS; generation of surface structure
from CPS is straightforward.
In order to give readers an overall pers-
pective, we illustrate an example in Fig.2.
Note that the example illustrated includes
partial negation. Thus operator &amp;quot;not&amp;quot; is
given a wider scope than &amp;quot;always&amp;quot;.
In the remaining part of this section
we will describe how to extract EFR expression
from a given sentence. Then we will discuss the
problem which arises in evaluating CPSF, and give
its possible solution.
</bodyText>
<subsectionHeader confidence="0.817054333333333">
3.2 Extracting EFR Expression from Input Sentence
Rules for translating English into EFR form
L$ •associated with each phrase structure rules.
</subsectionHeader>
<bodyText confidence="0.922763625">
For example, the rule looks like:
NP -&gt; DET+NOUN where &lt;NP)=&lt;DET&gt;(&lt;NOUN&gt;) (11)
where, &lt;NP&gt; stands for an EFR form assigned to
the NP node, etc. Rule (11) says that EFR for an
NP is a form whose function section is EFR for a
DET node and whose argument section is EFR for a
NOUN node. This rule can be incorporated into
conventional natural language parser.
</bodyText>
<subsectionHeader confidence="0.999912">
3.3 Evaluation of CPSF
</subsectionHeader>
<bodyText confidence="0.959389833333333">
Evaluation process of CPSF is a sequence of
lambda conversions and tree transformations.
Evaluation of CPSF is done by a LISP interpreter-
like algorithm. A problem which we call higher
order problem arose in designing the evaluation
algorithm.
</bodyText>
<page confidence="0.9254975">
159
160
</page>
<figure confidence="0.695123833333333">
(21)
(it is a block on the table)
large &lt;= [ADJ &amp;quot;LARGE&amp;quot;].
Since this decision cuases a form:
[ADJ &amp;quot;LARGE&amp;quot;]([NOUN &amp;quot;DATABASE&amp;quot;]),
Higher Order Problem
</figure>
<bodyText confidence="0.820165352941177">
By higher order property we mean that there
exist functions which take other functions as
arguments (Henderson 1980). CPSF in fact has
this property. For example, an adjective &amp;quot;large&amp;quot;
is modeled as a function which takes a noun as
its argument. For example,
large(database).
&amp;quot;large database&amp;quot; (12)
On the other hand, adverbs are modeled as
functions to adjectives. For example,
very(large), extremely(large),
comparatively(large), etc. (13)
The difficulty with higher order functions
consists in modifiction to function. For explana-
tion, let our temporal goal be regeneration of
English from EFR. Suppose we assign to &amp;quot;large&amp;quot; a
lambda form like:
</bodyText>
<equation confidence="0.477694">
(LAMBDA (x) (NOUN (ADJ &amp;quot;LARGE&amp;quot;) x)) (14)
</equation>
<bodyText confidence="0.722716">
which takes a noun and returns a complex noun by
attaching an adjective &amp;quot;large&amp;quot;. If the adjective
is modified by an adverb, say &amp;quot;very&amp;quot;, we have to
modify (14); we have to transform (14) into a
lambda form like:
</bodyText>
<equation confidence="0.500010666666667">
(LAMBDA (x)
(NOUN [ADJ [ADV &amp;quot;VERY&amp;quot;]
[ADJ &amp;quot;LARGE&amp;quot;)] x)), (15)
</equation>
<bodyText confidence="0.960897888888889">
which attaches a complex adjective &amp;quot;very large&amp;quot;
to a given noun. As is easily expected, it is
too tedious or even impossible to do this task
in general. Accordingly, we take an alternative
assignment instead of (14), namely:
(22)
to be created in the course of evaluation, we
specify what to do in such case. The rule is
defiend as follows:
</bodyText>
<equation confidence="0.911683">
[ADJMNOUND = [NOUN [ADJ1 [NOUN]). (18)
</equation>
<bodyText confidence="0.954760642857143">
This rule is called an application rule.
In general, evaluation of lambda form
itself results in a function value (function as a
value). This causes difficulty as mentioned
above. Unfortunately, we can&apos;t dispense with
lambda forms; lambda variables are needed to link
gap and its antecedent in relative clause, verb
and its dependants (subject, object, etc), pre-
position and its object, etc. For example, in
our model, an complex noun modified by a PP:
&amp;quot;block on the table&amp;quot; (19)
is assigned a following EFR:
The extraction rule can be written as a pattern
matching rule like:
</bodyText>
<equation confidence="0.5352874">
SENTENCE
NP 1
NP PRED
SORE WA x:NOUN DEARU
(It is an x) (23)
</equation>
<bodyText confidence="0.839211833333333">
Of course, this way of processing is not
desirable; it introduces extra complexity. But
this is a trade off of employing formal seman-
tics; the same sort of processing is also done
rather opaque procedures in conventional MT
system.
</bodyText>
<sectionHeader confidence="0.399837" genericHeader="method">
4. MODELING TRANSLATION PROCESS
</sectionHeader>
<bodyText confidence="0.9827462">
This section illustrates how English-
Japanese translation process is modeled using
formal tools. Firstly, how several basic
linguistic constructions are treated is described
and then mechanism for word choice is presented.
</bodyText>
<equation confidence="0.841787">
jkY[(the(table))
Chx[(((*ap(on))(y))(block))(x)1)], (20)
</equation>
<bodyText confidence="0.99272">
; which may read: is y:[there is a uniquely
specified object y referred to by an NP &amp;quot;the
table&amp;quot;, such that y is a block which is
restricted to be located on x.)
This lambda form is too complicated for tree
transformation procedure to manipulate. So it
should be transformed into equivalent CPS if it
exists. The type of the lambda form is known
from the context, namely one-place predicate. So
if we apply the lambda form (20) to &amp;quot;known&amp;quot;
entity, say &amp;quot;it&amp;quot;, we can obtain sentence struc-
ture like:
</bodyText>
<figure confidence="0.988764294117647">
SENTENCE
NP
NOUN PRED
MODIFIER NOUN
&amp;quot;-■
J
NP NP OSHI
SORE WA TSUKUE NO UE NO BLOCK DEARU
it a block on the table is
From this result, we can infer that the lambda
form (20) is equivalent to a noun:
NOUN
MODIFIER NOUN
NP &amp;quot;öSHI
4!f.
TSUKUE NO UE NO BLOCK
(block on the table)
</figure>
<subsectionHeader confidence="0.890541">
4.1 Translating Basic Constructions of English
</subsectionHeader>
<bodyText confidence="0.992718857142857">
a) Sentence: sentence consists of an NP and a
VP. VP is analyzed as a one-place predicate,
which constructs a proposition out of an indivi-
dual referred to by the subject. VP is further
decomposed into intransitive verb or transitive
verb + object. Intransitive verbs and transitive
verbs are analyzed as one-place predicates and
two-place predicate, respectively. One-place
predicate and two-place predicate are assigned a
CPSF function which generates a sentence out of
an individual and that which generates a sentence
out of a pair of individuals, respectively. Thus,
a transitive verb &amp;quot;constructs&amp;quot; is assigned a CPSF
form:
</bodyText>
<equation confidence="0.9412562">
(LAMBDA (x y)
(SENTENCE
(+ CASE-MARKER (CASE-AGENT) x)
(+ CASE-MARKER (CASEsOBJ) y)
(PREDICATE (VERB &amp;quot;SAKUSEI-SURC&amp;quot;11)), (24)
</equation>
<bodyText confidence="0.999311157894737">
; given two individuals, this function attaches
to each argument a case marker (corresponding
to JOSHI or Japanese postfix) and then gener-
ates a sentence structure.
The assignment (24) may be extended later to
incorporate word choice mechanism.
Treatment of NP in Montague-based semantics
is significant in that EFR expression for an NP is
given a wider scope than that for a VP. Thus the
EFR form for an NP-VP construction looks like:
In English-Japanese machine translation,
this treatment gives an elegant solution to the
translation of prenominal negation, partial nega-
tion, etc. Since Japanese language does not have
a syntactic device for prenominal negation, &amp;quot;no&amp;quot;
must be translated into mainly two separate
constituents: one is a RENTAISHI (Japanese deter-
miner) and another is an auxiliary verb of nega-
tion. One possible assignment of CPSF looks like:
</bodyText>
<equation confidence="0.3080004">
no &lt;= (LAMBDA (p)
(LAMBDA (q)
NEC
(q (NP &amp;quot;DONNA&amp;quot; (I NOUN p) &amp;quot;MO&amp;quot;))))).
(30)
</equation>
<bodyText confidence="0.672340571428571">
In general, correspondence of NP and indivi-
dual is indirect in EFR. The association of an
NP with its referent x is indicated as follows:
&lt;NP&gt;(Ax( x J). (31)
sentence type
one-place predicate type
; &lt;NP&gt; stands for EFR expression for NP.
</bodyText>
<subsectionHeader confidence="0.673513">
Most of other NP&apos;s correspond Co its
</subsectionHeader>
<bodyText confidence="0.99322">
referent more directly. The application rule
reflecting this fact is:
</bodyText>
<equation confidence="0.968102">
[NPMONE-PLACE-PREDI) = [ONE-PLACE-PREDJUNP*1),
(32)
</equation>
<bodyText confidence="0.992248">
where, (xi stands for a CPS for x.
</bodyText>
<subsectionHeader confidence="0.527189">
b) Internal structure of NP: the below illus-
</subsectionHeader>
<bodyText confidence="0.992313375">
(25) trates the structure of EFR expression assigned
to an NP:
where &lt;x&gt; means EFR form for x, x=NP,... .
The reason is to provide an appropriate model for
English quantifier which is syntactically local
but semantically global. For example, first
order logical form for a sentence:
&amp;quot;this command needs no operand&amp;quot; (26)
</bodyText>
<equation confidence="0.838493">
looks like:
not(there-exists x
(needs(&amp;quot;this-command&amp;quot;,x)
operand(x)I), (27)
</equation>
<bodyText confidence="0.99758125">
where operator &amp;quot;not&amp;quot;, which comes from a deter-
miner &amp;quot;no&amp;quot;, is given a wider scope than &amp;quot;needs&amp;quot;.
This translation is straightforward in our model;
the following EFR is extracted from (26):
</bodyText>
<figure confidence="0.4426367">
(this(command))
Nx((no(operand))(Xy(needs(x,y)1)1). (28)
If we make appropriate assignment including:
no &lt; (LAMBDA (P)
(LAMBDA (g)
&amp;quot;not(there exists x
(p(x) &amp; q(x)p&amp;quot;)), (29)
we can get (27) from (28).
&lt;DETWMODIFIER&gt;(...(&lt;MODIFIERX&lt;NOUN&gt;)) ...)).
(33)
</figure>
<bodyText confidence="0.9175302">
By &lt;MODIFIER&gt; we mean modification to noun by
adjectives, prepositional phrases, infinitives,
present/past particles, etc. The translation
process is determined by a CPSF assigned to &lt;DET&gt;.
In cases of &amp;quot;the&amp;quot; or &amp;quot;a/an&amp;quot;, translation process
is a bit complicated. It is almost the same as
the process described in detail in section 3:
firstly the &lt;MODIFIER&gt;s and &lt;NOUN&gt; are applied to
an individual like &amp;quot;the thing&amp;quot; (the) or &amp;quot;some-
thing&amp;quot; (a/an) and a sentence will be obtained;
then a.noun structure is extracted and appro-
priate RENTAISRI or Japanese determiner is
attached.
c) Other cases: some other cases are illust-
rated by examples in Fig.).
</bodyText>
<subsectionHeader confidence="0.997055">
4.2 Word Choice Mechanism
</subsectionHeader>
<bodyText confidence="0.997509333333333">
.In order to obtain high quality translation,
word choice mechanism must be incorporated at
least for handling the cases like:
</bodyText>
<page confidence="0.988057">
161
</page>
<figure confidence="0.883331333333333">
1) subordinate clause:
&amp;quot;When Sl, S2&amp;quot;
4
(when(&lt;S1&gt;))(&lt;S2&gt;)
4 4
&amp;quot;TOKI&amp;quot; [Si] \I
[[Si] &amp;quot;TOKI&amp;quot;] [S2]
[[Si] &amp;quot;TOKI&amp;quot; [S2]]
2) tense, aspect, modal:
&amp;quot;I bought a car&amp;quot;
4
did((I buy a car&gt;)
4 4
&amp;quot;TA&amp;quot; &amp;quot;WATASHI-WA JIDOUSHA-WO KAU&amp;quot;
&amp;quot;WATASHI-WA JIDOUSHA-WO KAU TA&amp;quot;
KATTA
3) passive:
&amp;quot;
is broken ... &amp;quot;
en(break)
(Axy{! &amp;quot;-GA&amp;quot; y &amp;quot;-WO&amp;quot; &amp;quot;KOWASU&amp;quot;}
Xy(y &amp;quot;-GA&amp;quot; &amp;quot;KOWA SARERU&amp;quot;)
; function &amp;quot;en&amp;quot; transforms a CPSF for
a transitive verb into intransitive.
4) interrogative:
&amp;quot;Do you have a car?&amp;quot;
4
#gues(whether((you have a car&gt;))
; ; i
+MKSENTENCE &amp;quot;KADOUKA&amp;quot; &amp;quot;ANATA-WA JIDOUSHA-WO MOTSU&amp;quot;
&amp;quot;ANATA-WA JIDOUSHA-WO MOTSU-KADOUKA&amp;quot;
&amp;quot;ANATA-WA JIDOUSHA-WO MOTSU-KA&amp;quot;
&amp;quot;Which car do you have?&amp;quot;
#ques((which(car))(?1[&lt;you have y&gt;])))
OKSENTENCE 1
AP{P(&amp;quot;DONO-JIDOUSHA&amp;quot;) &amp;quot;KA&amp;quot;}
4{&amp;quot;ANATA-WA&amp;quot; y &amp;quot;-WO MOTSU&amp;quot;}
TA-WA DONO-JIDOUSHA-WO MOTSU-KA&amp;quot;
&amp;quot;ANATA-WA DONO-JIDOUSHA-WO MOTSU-KA&amp;quot;
</figure>
<tableCaption confidence="0.7722794">
; indirect question is generated first, then it is
transformed into a sentence.
Fig.3. Examples of Translation of Basic English
Construction. &lt;x&gt;, {x}, [x] and &amp;quot;x&amp;quot; stand
for EFR for x, CPSF for x, CPS for x, and
CPS for Japanese string x, respectively.
verb in accordance with its object or its agent,
adjective-noun,
adverb-verb, and
preposition.
Word choice is partially solved in the analysis
phase as a word meaning disambiguation. So the
design problem is to determine to what degree
word sense is disambiguated in the analysis phase
and what kind of ambiguities is left until
transfer-generation phase. Suppose we are to
translate a given preposition. The occurence of
a preposition is classified as:
(a) when it is governed by verbs or nouns:
(a-1) when government is strong:
e.g., study on, belong to, provide for;
(a-2) when government is weak:
e.g., buy ... at score;
(b) otherwise:
(b-I) idiomatic:
e.g., in particular, in addition;
(b-2) related to its object:
e.g., by bus, with high probability,
without+ING.
We treat (a) and (b-1) as an analysis problem and
handle them in the analysis phase. (b-2) is more
difficult and is treated in the transfer-
generation phase where partial semantic interpre-
tation is done.
Word choice in transfer-generation phase is
done by using conditional expression and attri-
butive information included in CPS. For example,
a transitive verb &amp;quot;develop&amp;quot; is translated differ-
ently according to its object:
develop [0. system) ... KAIHATSU-SURU
</tableCaption>
<table confidence="0.942735">
(+ film) ... GENZOU-SURU. (34)
The following assignment of CPSF makes this choice
possible:
develop
&lt;= (LAMBDA (x y)
[(CLASS Y)-SYSTEM -&gt;
(&amp;quot;x-GA y-WO KAIHATSU-SURU&amp;quot;);
(CLASS y)-FILM -&gt;
(&amp;quot;x-GA y-WO CENZOU-SURU&amp;quot;);
... 1), (35)
operating-system
&lt;= [NOUN &amp;quot;OS&amp;quot; with CLASS=system; 1, (36)
film
&lt;= [NOUN &amp;quot;FUILUMU&amp;quot; with CLASS-film; 1.
</table>
<page confidence="0.684193">
(37)
</page>
<bodyText confidence="0.97976">
To make this type of processing possible in the
- cases where the deep object is moved from surface
object position by transformations, link infor-
mation between verb and its (deep) object should
</bodyText>
<page confidence="0.995053">
162
</page>
<bodyText confidence="0.968343">
be represented explicitly. The below shows how
it is done in the case of relative clause.
</bodyText>
<equation confidence="0.898507">
(WhiCh(AX[ x ]))(&lt;noun&gt;)
</equation>
<bodyText confidence="0.8748055">
link from head noun to
place holder
</bodyText>
<equation confidence="0.9268642">
CPSF assignment:
which sm. (LAMBDA (P) (LAMBDA (Q)
(NOUN (+ MK-MODIFIER ()
(P (+ MK-NULL-NP Q)))
DM.
</equation>
<bodyText confidence="0.9653543125">
; In EFR level, lambda variable x is explicit-
ly used as a place holder for the gap.
A functor &amp;quot;which&amp;quot; dominates both the EFR
for the embedded sentence and that for
the head noun. A CPSF assigned to the
functor &amp;quot;which&amp;quot; sends conceptual informa-
tion of the head noun to the gap as
follows: firstly it creates a null NP
out of the head noun, then the null NP
is substituted into the lambda variable
for the gap.
In word choice or semantic based translation
in general, various kinds of transformations are
carried out on target language structure. For
example,
her arrival mikes him happy, (38)
</bodyText>
<subsectionHeader confidence="0.465527">
must be paraphrased into:
</subsectionHeader>
<bodyText confidence="0.97636525">
he becomes happy because she has arrived (39)
since inanimate agent is unnatural in Japanese.
In order to retrieve appropriate lexical item of
target language for transformation, mutual rela-
tions among lexical items are organized using
network formalism (lexical net). The node repre-
sents a lexical item and a link represents an
association with specification of what operation
causes that link to be passed through. It also
contains description of case transformation
needed in order to map case structure appropriate-
ly. The below illustrate a part of lexical net:
</bodyText>
<figure confidence="0.722665">
TOUCHAKU TOUCHAKU
arrival
kfagent:NO
location:E-NO
</figure>
<sectionHeader confidence="0.995449" genericHeader="method">
5. EXPERIMENTS
</sectionHeader>
<bodyText confidence="0.990528176470588">
We have constructed a prototype system.
It is simplified than practical system in:
- it has only limited vocabulary,
- interactive disambiguation is done instead
of automatic disambiguation, and
- word choice mechanism is limited to typical
cases since overall definition of rules
have not yet been completed.
Sample texts are taken from real computer
manuals or abstracts of computer journals.
Initially, four sample texts (40 sentences) are
chosen. Currently it is extended to 10 texts (72
sentences).
Additional features are introduced in order
to make the system more practical.
a) Parser: declarative rules are inefficient
for dealing with sentences in real texts. The
parser uses production type rules each of which
is classified according to its invocation condi-
tion. Declarative rules are manually converted
into this rule type.
b) Automatic posteditor: transfer process
defined so far concentrates on local processings.
Even if certain kinds of ambiguities are re-
solved in this phase, there still remains a
possibility that new ambiguity is introduced in
generation phase. Instead of incorporating into
the transfer-generation phase a sophisticated
mechanism for filtering out ambiguities, we
attach a postprocessor which will &amp;quot;reform&apos; a
phrase structure yielding ambiguous output. Tree-
tree transformation rules are utilized here.
Current result of our machine translation
system is shown in Appendix.
</bodyText>
<figure confidence="0.997014666666666">
Phrase Structure (for restrictive use):
NP
4.
DET
&amp;quot;which&amp;quot;
EFR:
agent:NO/NIYORU
obj:NO
source:KARANO
dest:ENO
I)
HONYAKU KANOU NA
translatable
subj:WA
source:KARA
dest:E/NI
t*
LCHONYAKU SURU
trameLate
obj:WO
&apos;agent:GA activity
source:KARA (agent.-ON9
obj.-subj
,dest:E/NI
arrive activity
agent GA
location:NI
</figure>
<page confidence="0.9949">
163
</page>
<sectionHeader confidence="0.996415" genericHeader="method">
6. DISCUSSION
</sectionHeader>
<bodyText confidence="0.998976727272727">
Having completed initial experiments, it is
shown that our framework is applicable to real
texts under plausible assumption. The prototype
system has a clear architecture. Central rule
interpreter contains no complicated parts.
Although several errors occured in the implementa-
tion of translation rules, they were easily
detected and eliminated for the sake of data flow
property.
The initial requirement for intermediate
representation are filled in the following way:
</bodyText>
<sectionHeader confidence="0.920431" genericHeader="method">
REFERENCES
</sectionHeader>
<reference confidence="0.882832">
Cresswell, M.J. (1973): Logic and Languages,
Methuen and Company.
Dowty, R. et al (1981): Introduction to Montague
Semantics, Reidel.
Friedman, J. (1978): Evaluating English Sentences
in a Logical Model, Abstract 16, COLING 78.
Hauenschild, C., et al. (1979): SALAT: Machine
Translation Via Semantic Representation,
Bauerle et al.(eds.): Semantics From Different
Points of View, Springer-Verlag, 324-352.
</reference>
<figure confidence="0.912892866666667">
Requirement a:
Requirement b:
Requirement c:
Requirement d:
Requirement e:
precise representation based
on intensional logic,
using lambda variables and
scope rules,
data flow computing model
based on compositionality,
any CPSF can be assigned
to a given lexical item
if type is agreed,
fact that computer model
</figure>
<reference confidence="0.98923795">
has been implemented.
Henderson, P.(1980): Functional Programming --
Application and Implementation, Prentice/Hall.
Hobbs, J.R. and Rosenschein, S.J. (1978): Making .
Computational Sense of Montague&apos;s Intensional
Logic, Al 9, 287-306.
Landsbergen, J. (1980): Adaptation of Montague
Grammar to the Requirement of Question Answer-
ing, Proc. COLING 80, 211-212.
Some essential problem are left unsolved.
1) Scope analysis: correct analysis of scope of
words are crucial but difficult. For example,
scope relation of auxiliary and &amp;quot;not&amp;quot; differs
case by case:
Landsbergen, J. (1982): Machine Translation based
on Logically Isomorphic Montague Grammars,
Proc. COLING 82.
Montague, R. (1974): Proper Treatment of Quantifi-
cation in Ordinary English, Thompson (ed.)
Formal Philosophy, Yale University, 247-270.
</reference>
<bodyText confidence="0.932777666666667">
he can&apos;t swim
-&gt; not(can(‹he&gt;,&lt;swim&gt;))
you should not eat the banana
-&gt; should(not(&lt;eat the banana&gt;))
it may not be him.
-&gt; may(not( &lt;it=he&gt; ))
</bodyText>
<reference confidence="0.987626838709677">
(40) Moore, R.C. (1981): Problems in Logical Form,
Proc. 19th Annual Meeting of the ACL, 117-124.
Moran, D.B. (1982): The Representation of
Inconsistent Information in a Dynamic Model-
(42) Theoretic Semantics, Proc. 20th Annual Meeting
of the ACL, 16-18.
you may not eat the banana
-&gt; not(may( &lt;you eat banana&gt;)) (43)
2) Logic vs machine translation: The sentence
(44) is logically equivalent to (45), but
that paraphrasing is bad in machine translation.
he reads and writes English. (44)
he reads English and he writes English. (45)
7. CONCLUSION
Application of formal semantics to machine
translation brings about new phase of machine
translation. It makes the translation process
clearer than conventional systems. The theory
has been tested by implementing a prototype,
which can translate real texts with plausible
human assist.
Nishida, T. and Doshita, S. (1980): Hierarchical
Meaning Representation and Analysis of
Natural Language Documents, Proc. COLING 80,
85-92.
Rosenschein, S.J. and Shieber, S.M.(1982): Trans-
lating English into Logical Form, Proc. 20th
Annual Meeting of the. ACL, 1-8.
Yonezaki, H. and Enomoto, H. (1980): Database
System Based on Intensional Logic, Proc. COLING
80, 220-227.
</reference>
<page confidence="0.997469">
164
</page>
<note confidence="0.505318">
APPENDIX: Translation of a Sample Text.
</note>
<sectionHeader confidence="0.780829" genericHeader="method">
INPUT TEXT
</sectionHeader>
<bodyText confidence="0.9163795625">
htherne: is ai stem for local communication among computing stations Our experimental
F.therne: ;Amex coaxial cables to carry %dnable-length digital data packets among. for example.
persona: minicomputers. printing facilities. large file storage devices. magnetic tape backup stations.
larger central computers. and longer-haul communication equipment.
The shared communication facility. a branching Ether. is passive. A station&apos;s Ethernet Interface
connects bu-senallv through an. interface cable to a transceiver which in turn taps into the passing
Ether. A packet is broadcast onto the Ether. is heard by all stations. and is copied from the Ether
desunauons which select it according to the packet&apos;s leading address bits. This is broadcast
packc: switching and should be distinguished from store-and-forward packet switching in which
rouuna i Performed h■ intermediate processing elements. To handle the demands of growth. an
Ethernet can be extended using packet repeaters for signal regeneration, packet filters for traffic
localization, and packet gateways for internetwork address extension.
Control is completely distriouteci among stations with packet transmissions coordinated Enroug.h
statistical arbitration. Transmissions Initiated by a station defer to any which may already be in
progress. Once started. if interference with other packets is detected. a transmission is aborted and
rescheduled by its source station. After a certain period Of interference-free transmission, a packet
is heard by all stations and will run to completion without interference. Ethernet controllers in
colliding stations each generate random retransmission intervals to avoid repeated collisions. The
mean of a packet&apos;s retransmission intervals is adjusted as a function of collision history to keep
Ether utilizauon near the optimum with changing network load.
Even when transmitted without source-detected interference, a packet may still not reach its
destination without error: thus, packets are delivered only wish high probability. Stations requinng a
residual error rate lower than that provided by the bare Ethernet packet transport mechanism must
follow mutually agreed upon packet protocols.
(cited from: Metcalfe, R.M. and Boggs, D.R. (1975): Ethernet: Distributed Packet)
Switching for Local Computer Networks, CSL-75-7, Xerox.
E THERNE - 3 % OrgliqAPrrirtgrafgO.V: #50) X 1- 314C&apos;)
OUTPUT TEXT *Um* E THE RN E h 0/*&amp;quot;..‘S, (IA0) &gt;
translation is carried %./ , r43340111140, &amp;u:AE.11110121813tigagifIts:f4let 6 t: I.: 9
out sentence by sentence; htz= 7 4. -%•• tt,*
the result is assembled 0) E THE R ) It141/010 0)--C1) z,--&apos;&gt; 3 E &apos;r
by hand. ERNET(&apos; 1 x. - it CIAT ETHERt.:JOItz si .f t /9,1 f ; &apos;7 7
</bodyText>
<figure confidence="0.598629153846154">
7.)1&apos; &amp;quot;C e *tIVIP.:tit*it t1,6„ /11&apos; GOETHE R (b±c:
- g -C1117), -E.0)/1 F ofAIST 6*itkL&amp;quot; h
[MAW OE-CHER/5, 0111 ti-50 tti-.arsil, or 7 l• • trwl
014/11%01: ti z is 7 • -r • 7 -I- 7 le/ 7%:111%,!:.Pigil
tt*ithall 7!•‘;‘,., 111.)co)%31Wantst 31- I&apos; L &apos;7
to)tz ow)/ b &apos;7 4 lt• g1.0: *. fula)*Itto.0,-jgcryt y-
cipp,•-cETI-IERNETUbath.6::.:
%
i-C.:.g-,-CALeih-$a:Kil-:,..-12EltoMeEtzififft:.tsit6 0-&amp;quot;Cs,t, 6 7). 4, I.
711.&apos; t 1:-IX 3. ---)teItheN1116 t_g t ftlio,/1 .7 7: 6D1rNZ.754*it&apos;. tt -F-.
4:.k
0.40)016XIIMOlti:/lAr 7 h T x % ISVP Ma:* 7-LE T 1.: 5
/.‘1:6 30 .eitfsho)-16:gt - a •./ ts-:vit6ETIIERNET=: - -,114./0)Z41
</figure>
<footnote confidence="0.381926">
61111;ttialt 6 abc. 7&apos;10 7144i011141*1.filit 6. /11&apos; &apos;7 I- ogfill$111411091snlk.ft-r
</footnote>
<reference confidence="0.8545165">
.*-7 I- 7 - ift-r 41. -c E THE R $1.11fl es #51.: 111.5°:IfEt orNItt
&apos;CDR&amp; tt6.,
ts: INT* 71 I y- 7 ;Veit JAM/ i•
(ifattiO:PIV &amp;quot;,:t.■;)*1)I. ; :Lo).:: )1:At qtit311:0=
no) THE RNET, t/r .1••• -&amp;quot;C /IVA t j.; ;) fit,,qt/r,r)
V7ret 7.) A •Y- % at1151:Irla Ale.1M1:Vt ; r
</reference>
<page confidence="0.998412">
165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000043">
<title confidence="0.998926">AN APPLICATION OF MONTAGUE GRAMMAR TO ENGLISH-JAPANESE MACHINE TRANSLATION</title>
<author confidence="0.994186">Toyoaki NISHIDA</author>
<author confidence="0.994186">Shuji DOSHITA</author>
<affiliation confidence="0.999946">Dept. of Information Science Faculty of Engineering, Kyoto University</affiliation>
<address confidence="0.99721">Sakyo-ku, Kyoto 606, JAPAN</address>
<abstract confidence="0.973096668304668">English-Japanese machine translation requires a large amount of structural transformations in both grammatical and conceptual level. In order to make its control structure clearer and more understandable, this paper proposes a model based on Montague Gramamr. Translation process is modeled as a data flow computation process. Formal description tools are developed and a prototype system is constructed. Various problems which arise in this modeling and their solutions are described. Results of experiments are shown and it is discussed how far initial goals are achieved. 1. GOAL OF INTERMEDIATE REPRESENTATION DESIGN Differences between English and Japanese exist not only in grammatical level but also in conceptual level. Examples are illustrated in Accordingly, a large amount of transformations in various levels are required in order to obtain high quality translation. The goal of this research is to provide a good framework for carrying out those operations systematically. The solution depends on the design of intermediate representation (IR). Basic requirements to intermediate representation design are listed below. a) Accuracy: IR should retain logical conclusion of natural language expression. The following distinctions, for example, should be made in IR level: partial/total negation any-exist/exist-any active/passive restrictive use/ nonrestrictive use, etc. In other words, scope of operators should be represented precisely. GRAMMATICAL difference a) Case Marking: &lt;E&gt;: (relative position) + preposition &lt;J&gt;: postposition (called JOSHI) b) Word Order i) simple sentence S+V+0 : le S+0+V : RINGO ii) preposition vs postposition PREP+NP : NP+JOSHI : TEIZOUFT6551 iii) order of modification NP+POSTMODEFEER: on the box, &lt;J&gt;: PRENOMINAL MODIFIER+NP:TaK0 NO UE NO RINGO&apos; translate HONYAKU SURU KAISHAKU SURU interpret understand RIKAI SURU grasp TSUKAMU hold TAMOTSU keep KAMORU CONCEPTUAL difference &lt;E&gt; her arrival makes him happy paraphrasing is needed &lt;J&gt; KARE WA KANOJO GA TOUCHAKU SHITA NODE URESHIL. happy because she has arrived) Fig.l. Examples of Differences between English and Japanese. English; LEXICAL difference &lt;E&gt; &lt;J&gt; 156 Ability semantic relations: In English-Japanese translation, it is often the case that a given English word must be translated into different Japanese words or phrases if it has more than one word meanings. But it is not reasonable to capture this problem solely as a problem of word meaning disambiguation in analysis phase; the needed depth of disambiguation depends on target language. So it is also handled in transfer phase. In general, meaning of a given word is recognized based on the relation to other constituents in the sentence or text which is semantically related to the given word. To make this possible in transfer phase, IR must provide a link to semantically related constituents of a given item. For example, an object of a verb should be accessible in IR level from the verb, even if the relation is implicit in the surface structure (eg., passives, relative clauses, and their combinations, etc.) c) Prediction of control: given an IR expression, the model should be able to predict what operations are to be what order. d) Lexicon driven: some sort of transformation rules are word specific. The IR interpretation system should be designed to deal with those word specific rules easily. e) Computability: All processings should be effectively computable. Any IR is useless if it is not computable. 2. PRINCIPLE OF TRANSLATION This section outlines our solution to the requirements posed in the preceding section. We employ Montague Grammar (Montague 1974, Dowty 1981) as a theoretical basis of translation model. Intermediate representation is designed based on intensional logic. Intermediate representation for a given natural language expression is by what call functional analysis. 2.1 Functional Analysis In functional analysis, input sentence is decomposed into groups of constituents and among those groups are terms function-argument Suppose a sentence: I don&apos;t have a book. (1) The functional analysis makes following two points: (1) is decomposed &amp;quot;I have a book&amp;quot; + &amp;quot;not&amp;quot;. (2) b) In the decomposition (2), &amp;quot;not&amp;quot; is an operator or function to &amp;quot;I have a book.&amp;quot; this analysis can be depicted as follows: &amp;quot;not&amp;quot;? &amp;quot;I have a book&amp;quot;i (3) where CD&apos; denotes a function and 11111 denotes argument. The role of &amp;quot;not&amp;quot; as a is: &amp;quot;not&amp;quot; as a semantic operator: it negates a given proposition; &amp;quot;not&amp;quot; as a syntactic operator: it inserts an appropriate auxiliary verb a lexical item &amp;quot;not&amp;quot; position of its argument. (4) kind of analysis goes on further embedded sentence until it is decomposed into lexical units or even morphemes. Montague Grammar as a Basic Grammar a basis func- One of the advantages of MG in its interpretation of function (or intensional form). MG, interof an intensional formula is a I intensional formulas to domain. Important is this mapping I is defined under conscompositionality, that is, I regard to what f, a, b, etc. are. property simplifies control structure and it also what operations are done in what example, suppose input data has structure like: (6) the sake of (5), the interpretation (6) is done as a data computation process as follows: property, we can grasp the process- In particular, we shoot and source of abnormality debugging a system. to the above property and in due to its rigorous based MG has been studied information science (Hobbs 1978, 1978, Yonezaki 1980, 157 Nishida 1980, Landsbergen 1980, Moran 1982, Moore 1981, Rosenschein 1982, ...). Application of MG to machine translation was also attempted (Hauenschild 1979, Landsbergen 1982), but those systems have only partially utilized the power of MG. Our approach attempts to utilize the full power of MG. 2.3 Application of Montague Grammar to Machine Translation In order to obtain the syntactic structure in Japanese from an intensional logical form, in the same way as interpretation process of MG, we change the semantic domain from set theoretical domain to conceptual domain for Japanese. Each conceptual unit contains its syntactic expression Syntactic aspect is stressed for generating syntactic structure in Japanese. information for semantic based word choice and paraphrasing. For example, the following function in Japanese syntactic domain is assigned to a logical item &amp;quot;not&amp;quot;: (x) (SENTENCE x [AUX &amp;quot;NAI&amp;quot;])). Transfer-generation process for the sentence (1) looks like: &amp;quot;I don&apos;t have a book&amp;quot; 3.1 Definition of Formal Tools a) English oriented Formal Representation (EFR) is a version of intensional logic, and gives a rigorous formalism for describing the results of functional analysis. It is based on Cresswell&apos;s lambda deep structure (Cresawell 1973). Each expression has a uniquely defined type. Lambda form is employed to denote function itself. b) Conceptual Phrase Structure (CPS) is a data structure in which syntactic and semantic information of a Japanese lexical unit or phrase structure are packed. 0 example of CPS for a lexical item: &amp;quot;EIGO&amp;quot; with ISA=LANGUAGE; ...,](9) category; lexical item; conceptual info. ; &amp;quot;EIGO&amp;quot; means English language. ii) example of CPS for phrase structure: [NP [ADJ &amp;quot;AKAI&amp;quot; with ... ] [NOUN &amp;quot;RINGO&amp;quot; with ... ] with ... ] (10) ; &amp;quot;AKAI&amp;quot; means red, and &amp;quot;RINGO&amp;quot; means apple. c) CPS Form (CPSF) is a form which denotes operation or function on CPS domain. It is used to give descriptions to mappings from EFR to CPS. Constituents of CPSF are: 0 Constants: CPS. ii) Variables: x, y, . (indicated by lower case strings). iii) Variables with constraints: e.g., (! SENTENCE x). variable x which must of category SENTENCE. &amp;quot;I have a book&amp;quot; Ii .TRANSFER TRANSFER,GENERATION (LAMBDA (x) (SENTENCE x (AUX &amp;quot;NAI&amp;quot;]}) WATASHI-WA HON-WO MOTSU AUX I WATASHI-WA HON-WO MOTSU NAI MOTANAI 3. FORMAL TOOLS description tools have developed to provide a precise description of the idea mentioned in the last section. iv) Transformations: (+ TENSE (TENSE..PAST),x). indicator; operator-name; PARAMs; argument v) CPS construction: (SENTENCE (x y) ... ). descendents vi) Conditionals: -&gt; ... 1. vii) Lambda form: (LAMBDA (+ 0 x)) Using those description tools, translation process is modeled as a three staged process: 1 (analysis): anlyzes sentence and extracts EFR form, stage 2 (transfer): substitutes CPSF to each lexical item in the EFR form, 158 He does not always came late. ( he late C comes )))) (TRANSFER) IAq ADV S P NP I I he *it 9 ■ S. id&lt;ttr late / S Ax NP VP 1 Z\ ) [ comes CPSF ADV always he comes Late AUX he comes late it is not the case Xx NP ADV VP I I tvC5EZ x comes late - (ENERATION) tlEtisia&lt;tvCNEZ he comes tate .. CPS .. Example of Translation Process notation is used for CPSF, / described using Formal Tools. and syntactic aspect is emphasized. 3 evaluates the CPSF to get CPS; generation of surface structure from CPS is straightforward. In order to give readers an overall perspective, we illustrate an example in Fig.2. Note that the example illustrated includes partial negation. Thus operator &amp;quot;not&amp;quot; is given a wider scope than &amp;quot;always&amp;quot;. In the remaining part of this section we will describe how to extract EFR expression a Then we will discuss the problem which arises in evaluating CPSF, and give its possible solution. 3.2 Extracting EFR Expression from Input Sentence Rules for translating English into EFR form •associated with each structure rules. For example, the rule looks like: NP -&gt; DET+NOUN where &lt;NP)=&lt;DET&gt;(&lt;NOUN&gt;) (11) where, &lt;NP&gt; stands for an EFR form assigned to the NP node, etc. Rule (11) says that EFR for an NP is a form whose function section is EFR for a DET node and whose argument section is EFR for a node. This rule can into conventional natural language parser. 3.3 Evaluation of CPSF Evaluation process of CPSF is a sequence of lambda conversions and tree transformations. of CPSF is done by a LISP interpreterlike algorithm. A problem which we call higher order problem arose in designing the evaluation algorithm. 159 160 (21) is a block table) large &lt;= [ADJ &amp;quot;LARGE&amp;quot;]. Since this decision cuases a form: [ADJ &amp;quot;LARGE&amp;quot;]([NOUN &amp;quot;DATABASE&amp;quot;]), Higher Order Problem By higher order property we mean that there exist functions which take other functions as arguments (Henderson 1980). CPSF in fact has this property. For example, an adjective &amp;quot;large&amp;quot; is modeled as a function which takes a noun as its argument. For example, large(database). &amp;quot;large database&amp;quot; (12) On the other hand, adverbs are modeled as functions to adjectives. For example, very(large), extremely(large), comparatively(large), etc. (13) The difficulty with higher order functions consists in modifiction to function. For explanation, let our temporal goal be regeneration of English from EFR. Suppose we assign to &amp;quot;large&amp;quot; a lambda form like: (LAMBDA (x) (NOUN (ADJ &amp;quot;LARGE&amp;quot;) x)) (14) which takes a noun and returns a complex noun by attaching an adjective &amp;quot;large&amp;quot;. If the adjective is modified by an adverb, say &amp;quot;very&amp;quot;, we have to modify (14); we have to transform (14) into a lambda form like: (LAMBDA (x) (NOUN [ADJ [ADV &amp;quot;VERY&amp;quot;] [ADJ &amp;quot;LARGE&amp;quot;)] x)), (15) which attaches a complex adjective &amp;quot;very large&amp;quot; to a given noun. As is easily expected, it is too tedious or even impossible to do this task in general. Accordingly, we take an alternative assignment instead of (14), namely: (22) to be created in the course of evaluation, we specify what to do in such case. The rule is defiend as follows: [ADJMNOUND = [NOUN [ADJ1 [NOUN]). (18) rule is an application rule. In general, evaluation of lambda form itself results in a function value (function as a value). This causes difficulty as mentioned above. Unfortunately, we can&apos;t dispense with lambda forms; lambda variables are needed to link gap and its antecedent in relative clause, verb (subject, object, etc), preposition and its object, etc. For example, in our model, an complex noun modified by a PP: on the table&amp;quot; assigned a following extraction can be written as a pattern rule SENTENCE NP 1 NP PRED SORE WA x:NOUN DEARU is x) (23) course, this way of processing is it extra complexity. is trade off of employing formal semantics; the same sort of processing is also done rather opaque procedures in conventional MT system. 4. MODELING TRANSLATION PROCESS This section illustrates how Englishtranslation process is modeled tools. Firstly, how basic linguistic constructions are treated is described and then mechanism for word choice is presented. jkY[(the(table)) ; which may read: is y:[there is a uniquely specified object y referred to by an NP &amp;quot;the table&amp;quot;, such that y is a block which is restricted to be located on x.) This lambda form is too complicated for tree transformation procedure to manipulate. So it should be transformed into equivalent CPS if it exists. The type of the lambda form is known from the context, namely one-place predicate. So if we apply the lambda form (20) to &amp;quot;known&amp;quot; entity, say &amp;quot;it&amp;quot;, we can obtain sentence structure like:</abstract>
<title confidence="0.723825375">SENTENCE NP NOUN PRED MODIFIER NOUN &amp;quot;-■ J NP NP OSHI SORE WA TSUKUE NO UE NO BLOCK DEARU</title>
<abstract confidence="0.986888472222222">a table is From this result, we can infer that the lambda form (20) is equivalent to a noun: NOUN MODIFIER NOUN 4!f. NO BLOCK (block on the table) 4.1 Translating Basic Constructions of English a) Sentence: sentence consists of an NP and a VP. VP is analyzed as a one-place predicate, which constructs a proposition out of an individual referred to by the subject. VP is further decomposed into intransitive verb or transitive verb + object. Intransitive verbs and transitive verbs are analyzed as one-place predicates and two-place predicate, respectively. One-place predicate and two-place predicate are assigned a CPSF function which generates a sentence out of an individual and that which generates a sentence out of a pair of individuals, respectively. Thus, a transitive verb &amp;quot;constructs&amp;quot; is assigned a CPSF form: (LAMBDA (x y) (SENTENCE (+ CASE-MARKER (CASE-AGENT) x) (+ CASE-MARKER (CASEsOBJ) y) (PREDICATE (VERB &amp;quot;SAKUSEI-SURC&amp;quot;11)), (24) ; given two individuals, this function attaches to each argument a case marker (corresponding to JOSHI or Japanese postfix) and then generates a sentence structure. assignment be extended later to incorporate word choice mechanism. Treatment of NP in Montague-based semantics is significant in that EFR expression for an NP is a wider scope than that VP. Thus the EFR form for an NP-VP construction looks like: In English-Japanese machine translation, treatment gives an elegant solution to translation of prenominal negation, partial negation, etc. Since Japanese language does not have a syntactic device for prenominal negation, &amp;quot;no&amp;quot; must be translated into mainly two separate constituents: one is a RENTAISHI (Japanese determiner) and another is an auxiliary verb of negation. One possible assignment of CPSF looks like: no &lt;= (LAMBDA (p) (LAMBDA (q) NEC (NP &amp;quot;DONNA&amp;quot; (I &amp;quot;MO&amp;quot;))))). (30) general, correspondence of NP and indiviindirect in EFR. The association of an NP with its referent x is indicated as follows: &lt;NP&gt;(Ax( x J). (31) sentence type one-place predicate type for EFR expression for NP. of other Co referent more directly. The application rule this fact [NPMONE-PLACE-PREDI) = [ONE-PLACE-PREDJUNP*1), (32) where, (xi stands for a CPS for x. Internal structure of NP: the below illus- (25) trates the structure of EFR expression assigned to an NP: EFR form for x, x=NP,... . The reason is to provide an appropriate model for English quantifier which is syntactically local but semantically global. For example, first order logical form for a sentence: command needs no operand&amp;quot; looks like: not(there-exists x (needs(&amp;quot;this-command&amp;quot;,x) where operator &amp;quot;not&amp;quot;, which comes from a determiner &amp;quot;no&amp;quot;, is given a wider scope than &amp;quot;needs&amp;quot;. This translation is straightforward in our model; following EFR is extracted from (this(command)) make appropriate assignment no &lt; (LAMBDA (P) &amp;quot;not(there exists x (p(x) &amp; q(x)p&amp;quot;)), (29) we can get (27) from (28). &lt;DETWMODIFIER&gt;(...(&lt;MODIFIERX&lt;NOUN&gt;)) ...)). (33) By &lt;MODIFIER&gt; we mean modification to noun by prepositional infinitives, particles, The translation is determined by assigned &lt;DET&gt;. &amp;quot;the&amp;quot; or translation process bit complicated. It is almost same as the process described in detail in section 3: &lt;MODIFIER&gt;s and &lt;NOUN&gt; to individual like (the) or &amp;quot;someand a sentence obtained; then a.noun structure is extracted and appropriate RENTAISRI or Japanese determiner is attached. Other cases: some other cases are illustexamples in Fig.). 4.2 Word Choice Mechanism .In order to obtain high quality translation, word choice mechanism must be incorporated at for the cases like:</abstract>
<note confidence="0.924072461538462">161 1) subordinate clause: &amp;quot;When Sl, S2&amp;quot; 4 (when(&lt;S1&gt;))(&lt;S2&gt;) 4 4 [Si] [[Si] &amp;quot;TOKI&amp;quot;] [S2] [[Si] &amp;quot;TOKI&amp;quot; [S2]] 2) tense, aspect, modal: &amp;quot;I bought a car&amp;quot; 4 did((I buy a car&gt;)</note>
<phone confidence="0.427089">4 4</phone>
<abstract confidence="0.978471819095478">amp;quot;TA&amp;quot; &amp;quot;WATASHI-WA JIDOUSHA-WO KAU&amp;quot; &amp;quot;WATASHI-WA JIDOUSHA-WO KAU TA&amp;quot; KATTA 3) passive: &amp;quot; ... &amp;quot; en(break) &amp;quot;-GA&amp;quot; y &amp;quot;-WO&amp;quot; &amp;quot;KOWASU&amp;quot;} &amp;quot;KOWA SARERU&amp;quot;) ; function &amp;quot;en&amp;quot; transforms a CPSF for a transitive verb into intransitive. 4) interrogative: &amp;quot;Do you have a car?&amp;quot; 4 ; ; i +MKSENTENCE &amp;quot;KADOUKA&amp;quot; &amp;quot;ANATA-WA JIDOUSHA-WO MOTSU&amp;quot; &amp;quot;ANATA-WA JIDOUSHA-WO MOTSU-KADOUKA&amp;quot; &amp;quot;ANATA-WA JIDOUSHA-WO MOTSU-KA&amp;quot; &amp;quot;Which car do you have?&amp;quot; have AP{P(&amp;quot;DONO-JIDOUSHA&amp;quot;) &amp;quot;KA&amp;quot;} y MOTSU&amp;quot;} TA-WA DONO-JIDOUSHA-WO MOTSU-KA&amp;quot; &amp;quot;ANATA-WA DONO-JIDOUSHA-WO MOTSU-KA&amp;quot; ; indirect question is generated first, then it is transformed into a sentence. Fig.3. Examples of Translation of Basic English Construction. &lt;x&gt;, {x}, [x] and &amp;quot;x&amp;quot; stand for EFR for x, CPSF for x, CPS for x, and CPS for Japanese string x, respectively. verb in accordance with its object or its agent, adjective-noun, adverb-verb, and preposition. Word choice is partially solved in the analysis phase as a word meaning disambiguation. So the design problem is to determine to what degree sense is disambiguated in the analysis and what kind of ambiguities is left until transfer-generation phase. Suppose we are to translate a given preposition. The occurence of a preposition is classified as: (a) when it is governed by verbs or nouns: (a-1) when government is strong: e.g., study on, belong to, provide for; (a-2) when government is weak: e.g., buy ... at score; (b) otherwise: (b-I) idiomatic: e.g., in particular, in addition; to its object: e.g., by bus, with high probability, without+ING. (a) an analysis problem and handle them in the analysis phase. (b-2) is more difficult and is treated in the transfergeneration phase where partial semantic interpretation is done. Word choice in transfer-generation phase is done by using conditional expression and attributive information included in CPS. For example, a transitive verb &amp;quot;develop&amp;quot; is translated differaccording its system) ... ... (34) following of CPSF makes this choice possible: develop &lt;= (LAMBDA (x y) [(CLASS Y)-SYSTEM -&gt; (&amp;quot;x-GA y-WO KAIHATSU-SURU&amp;quot;); (CLASS y)-FILM -&gt; (&amp;quot;x-GA y-WO CENZOU-SURU&amp;quot;); ... 1), (35) operating-system &lt;= [NOUN &amp;quot;OS&amp;quot; with CLASS=system; 1, (36) film [NOUN &amp;quot;FUILUMU&amp;quot; 1. (37) make this type of processing possible the cases where the object is moved surface position by transformations, verb and should 162 be represented explicitly. The below shows how it is done in the case of relative clause. x link from head noun to place holder CPSF assignment: which sm. (LAMBDA (P) (LAMBDA (Q) (NOUN (+ MK-MODIFIER () (P (+ MK-NULL-NP Q))) DM. In EFR level, lambda variable x is explicitly used as a place holder for the gap. A functor &amp;quot;which&amp;quot; dominates both the EFR for the embedded sentence and that for the head noun. A CPSF assigned to the &amp;quot;which&amp;quot; sends conceptual information of the head noun to the gap as follows: firstly it creates a null NP out of the head noun, then the null NP is substituted into the lambda variable for the gap. In word choice or semantic based translation in general, various kinds of transformations are carried out on target language structure. For example, her arrival mikes him happy, (38) be paraphrased he becomes happy because she has arrived (39) since inanimate agent is unnatural in Japanese. In order to retrieve appropriate lexical item of target language for transformation, mutual relations among lexical items are organized using network formalism (lexical net). The node represents a lexical item and a link represents an association with specification of what operation causes that link to be passed through. It also contains description of case transformation needed in order to map case structure appropriatebelow a part of lexical net: TOUCHAKU TOUCHAKU arrival location:E-NO 5. EXPERIMENTS have constructed system. is simplified than practical system it has only limited vocabulary, interactive disambiguation is done instead of automatic disambiguation, and word choice mechanism is limited to typical cases since overall definition of rules have not yet been completed. texts are taken from real manuals or abstracts of computer journals. four sample texts (40 are Currently it is extended to 10 (72 sentences). features introduced in order to make the system more practical. Parser: declarative are inefficient dealing with sentences in real The parser uses production type rules each of which classified according to invocation condi- Declarative rules are manually rule type. b) Automatic posteditor: transfer process concentrates local processings. if certain kinds of are rein this phase, there remains a that new ambiguity is generation phase. Instead of incorporating into the transfer-generation phase a sophisticated for filtering out a postprocessor will a structure ambiguous output. Treeare utilized here. Current result of our machine translation system is shown in Appendix. Phrase Structure (for restrictive use): NP 4. DET &amp;quot;which&amp;quot; EFR: agent:NO/NIYORU obj:NO source:KARANO dest:ENO I) HONYAKU KANOU NA translatable subj:WA source:KARA dest:E/NI SURU trameLate obj:WO &apos;agent:GA activity source:KARA (agent.-ON9 obj.-subj ,dest:E/NI arrive activity agent GA location:NI 163 6. DISCUSSION Having completed initial experiments, it is shown that our framework is applicable to real texts under plausible assumption. The prototype system has a clear architecture. Central rule interpreter contains no complicated parts. Although several errors occured in the implementation of translation rules, they were easily detected and eliminated for the sake of data flow property. The initial requirement for intermediate representation are filled in the following way:</abstract>
<note confidence="0.643408333333333">REFERENCES Cresswell, M.J. (1973): Logic and Languages, Methuen and Company.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M J Cresswell</author>
</authors>
<title>Logic and Languages,</title>
<date>1973</date>
<publisher>Methuen and Company.</publisher>
<marker>Cresswell, 1973</marker>
<rawString>Cresswell, M.J. (1973): Logic and Languages, Methuen and Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dowty</author>
</authors>
<title>Introduction to Montague Semantics,</title>
<date>1981</date>
<location>Reidel.</location>
<contexts>
<context position="4037" citStr="Dowty 1981" startWordPosition="617" endWordPosition="618"> combinations, etc.) c) Prediction of control: given an IR expression, the model should be able to predict explicitly what operations are to be done in what order. d) Lexicon driven: some sort of transformation rules are word specific. The IR interpretation system should be designed to deal with those word specific rules easily. e) Computability: All processings should be effectively computable. Any IR is useless if it is not computable. 2. PRINCIPLE OF TRANSLATION This section outlines our solution to the requirements posed in the preceding section. We employ Montague Grammar (Montague 1974, Dowty 1981) as a theoretical basis of translation model. Intermediate representation is designed based on intensional logic. Intermediate representation for a given natural language expression is obtained by what we call functional analysis. 2.1 Functional Analysis In functional analysis, input sentence is decomposed into groups of constituents and interrelationship among those groups are analyzed in terms of function-argument relationships. Suppose a sentence: I don&apos;t have a book. (1) The functional analysis makes following two points: a) (1) is decomposed as: &amp;quot;I have a book&amp;quot; + &amp;quot;not&amp;quot;. (2) b) In the deco</context>
</contexts>
<marker>Dowty, 1981</marker>
<rawString>Dowty, R. et al (1981): Introduction to Montague Semantics, Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
</authors>
<title>Evaluating English Sentences in a Logical Model, Abstract 16,</title>
<date>1978</date>
<journal>COLING</journal>
<volume>78</volume>
<contexts>
<context position="6330" citStr="Friedman 1978" startWordPosition="992" endWordPosition="993">etc. are. This property simplifies control structure and it also specifies what operations are done in what order. For example, suppose input data has a structure like: (6) For the sake of property (5), the interpretation of (6) is done as a data flow computation process as follows: By this property, we can easily grasp the processing stream. In particular, we can easily shoot trouble and source of abnormality when debugging a system. Due to the above property and others, in particular due to its rigorous framework based 0,1 logic, MG has been studied in information science field (Hobbs 1978, Friedman 1978, Yonezaki 1980, 157 Nishida 1980, Landsbergen 1980, Moran 1982, Moore 1981, Rosenschein 1982, ...). Application of MG to machine translation was also attempted (Hauenschild 1979, Landsbergen 1982), but those systems have only partially utilized the power of MG. Our approach attempts to utilize the full power of MG. 2.3 Application of Montague Grammar to Machine Translation In order to obtain the syntactic structure in Japanese from an intensional logical form, in the same way as interpretation process of MG, we change the semantic domain from set theoretical domain to conceptual domain for Ja</context>
</contexts>
<marker>Friedman, 1978</marker>
<rawString>Friedman, J. (1978): Evaluating English Sentences in a Logical Model, Abstract 16, COLING 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hauenschild</author>
</authors>
<title>SALAT: Machine Translation Via Semantic Representation, Bauerle et al.(eds.): Semantics From Different Points of View,</title>
<date>1979</date>
<pages>324--352</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="6508" citStr="Hauenschild 1979" startWordPosition="1017" endWordPosition="1018">r the sake of property (5), the interpretation of (6) is done as a data flow computation process as follows: By this property, we can easily grasp the processing stream. In particular, we can easily shoot trouble and source of abnormality when debugging a system. Due to the above property and others, in particular due to its rigorous framework based 0,1 logic, MG has been studied in information science field (Hobbs 1978, Friedman 1978, Yonezaki 1980, 157 Nishida 1980, Landsbergen 1980, Moran 1982, Moore 1981, Rosenschein 1982, ...). Application of MG to machine translation was also attempted (Hauenschild 1979, Landsbergen 1982), but those systems have only partially utilized the power of MG. Our approach attempts to utilize the full power of MG. 2.3 Application of Montague Grammar to Machine Translation In order to obtain the syntactic structure in Japanese from an intensional logical form, in the same way as interpretation process of MG, we change the semantic domain from set theoretical domain to conceptual domain for Japanese. Each conceptual unit contains its syntactic expression in Japanese. Syntactic aspect is stressed for generating syntactic structure in Japanese. Conceptual information is</context>
</contexts>
<marker>Hauenschild, 1979</marker>
<rawString>Hauenschild, C., et al. (1979): SALAT: Machine Translation Via Semantic Representation, Bauerle et al.(eds.): Semantics From Different Points of View, Springer-Verlag, 324-352.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Henderson</author>
</authors>
<title>P.(1980): Functional Programming --Application and Implementation,</title>
<publisher>Prentice/Hall.</publisher>
<marker>Henderson, </marker>
<rawString>has been implemented. Henderson, P.(1980): Functional Programming --Application and Implementation, Prentice/Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>S J Rosenschein</author>
</authors>
<title>Making .</title>
<date>1978</date>
<journal>Computational Sense of Montague&apos;s Intensional Logic, Al</journal>
<volume>9</volume>
<pages>287--306</pages>
<marker>Hobbs, Rosenschein, 1978</marker>
<rawString>Hobbs, J.R. and Rosenschein, S.J. (1978): Making . Computational Sense of Montague&apos;s Intensional Logic, Al 9, 287-306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Landsbergen</author>
</authors>
<title>Adaptation of Montague Grammar to the Requirement of Question Answering,</title>
<date>1980</date>
<booktitle>Proc. COLING</booktitle>
<volume>80</volume>
<pages>211--212</pages>
<contexts>
<context position="6381" citStr="Landsbergen 1980" startWordPosition="999" endWordPosition="1000">ture and it also specifies what operations are done in what order. For example, suppose input data has a structure like: (6) For the sake of property (5), the interpretation of (6) is done as a data flow computation process as follows: By this property, we can easily grasp the processing stream. In particular, we can easily shoot trouble and source of abnormality when debugging a system. Due to the above property and others, in particular due to its rigorous framework based 0,1 logic, MG has been studied in information science field (Hobbs 1978, Friedman 1978, Yonezaki 1980, 157 Nishida 1980, Landsbergen 1980, Moran 1982, Moore 1981, Rosenschein 1982, ...). Application of MG to machine translation was also attempted (Hauenschild 1979, Landsbergen 1982), but those systems have only partially utilized the power of MG. Our approach attempts to utilize the full power of MG. 2.3 Application of Montague Grammar to Machine Translation In order to obtain the syntactic structure in Japanese from an intensional logical form, in the same way as interpretation process of MG, we change the semantic domain from set theoretical domain to conceptual domain for Japanese. Each conceptual unit contains its syntactic</context>
</contexts>
<marker>Landsbergen, 1980</marker>
<rawString>Landsbergen, J. (1980): Adaptation of Montague Grammar to the Requirement of Question Answering, Proc. COLING 80, 211-212.</rawString>
</citation>
<citation valid="false">
<title>Some essential problem are left unsolved. 1) Scope analysis: correct analysis of scope of words are crucial but difficult. For example, scope relation of auxiliary and &amp;quot;not&amp;quot; differs case by case:</title>
<marker></marker>
<rawString>Some essential problem are left unsolved. 1) Scope analysis: correct analysis of scope of words are crucial but difficult. For example, scope relation of auxiliary and &amp;quot;not&amp;quot; differs case by case:</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Landsbergen</author>
</authors>
<title>Machine Translation based on Logically Isomorphic Montague Grammars,</title>
<date>1982</date>
<booktitle>Proc. COLING 82.</booktitle>
<contexts>
<context position="6527" citStr="Landsbergen 1982" startWordPosition="1019" endWordPosition="1020">erty (5), the interpretation of (6) is done as a data flow computation process as follows: By this property, we can easily grasp the processing stream. In particular, we can easily shoot trouble and source of abnormality when debugging a system. Due to the above property and others, in particular due to its rigorous framework based 0,1 logic, MG has been studied in information science field (Hobbs 1978, Friedman 1978, Yonezaki 1980, 157 Nishida 1980, Landsbergen 1980, Moran 1982, Moore 1981, Rosenschein 1982, ...). Application of MG to machine translation was also attempted (Hauenschild 1979, Landsbergen 1982), but those systems have only partially utilized the power of MG. Our approach attempts to utilize the full power of MG. 2.3 Application of Montague Grammar to Machine Translation In order to obtain the syntactic structure in Japanese from an intensional logical form, in the same way as interpretation process of MG, we change the semantic domain from set theoretical domain to conceptual domain for Japanese. Each conceptual unit contains its syntactic expression in Japanese. Syntactic aspect is stressed for generating syntactic structure in Japanese. Conceptual information is utilized for seman</context>
</contexts>
<marker>Landsbergen, 1982</marker>
<rawString>Landsbergen, J. (1982): Machine Translation based on Logically Isomorphic Montague Grammars, Proc. COLING 82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Montague</author>
</authors>
<title>Proper Treatment of Quantification in Ordinary English,</title>
<date>1974</date>
<pages>247--270</pages>
<editor>(ed.)</editor>
<institution>Formal Philosophy, Yale University,</institution>
<location>Thompson</location>
<contexts>
<context position="4024" citStr="Montague 1974" startWordPosition="615" endWordPosition="616">uses, and their combinations, etc.) c) Prediction of control: given an IR expression, the model should be able to predict explicitly what operations are to be done in what order. d) Lexicon driven: some sort of transformation rules are word specific. The IR interpretation system should be designed to deal with those word specific rules easily. e) Computability: All processings should be effectively computable. Any IR is useless if it is not computable. 2. PRINCIPLE OF TRANSLATION This section outlines our solution to the requirements posed in the preceding section. We employ Montague Grammar (Montague 1974, Dowty 1981) as a theoretical basis of translation model. Intermediate representation is designed based on intensional logic. Intermediate representation for a given natural language expression is obtained by what we call functional analysis. 2.1 Functional Analysis In functional analysis, input sentence is decomposed into groups of constituents and interrelationship among those groups are analyzed in terms of function-argument relationships. Suppose a sentence: I don&apos;t have a book. (1) The functional analysis makes following two points: a) (1) is decomposed as: &amp;quot;I have a book&amp;quot; + &amp;quot;not&amp;quot;. (2) b</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Montague, R. (1974): Proper Treatment of Quantification in Ordinary English, Thompson (ed.) Formal Philosophy, Yale University, 247-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
</authors>
<title>Problems in Logical Form,</title>
<date>1981</date>
<booktitle>Proc. 19th Annual Meeting of the ACL,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="6405" citStr="Moore 1981" startWordPosition="1003" endWordPosition="1004">t operations are done in what order. For example, suppose input data has a structure like: (6) For the sake of property (5), the interpretation of (6) is done as a data flow computation process as follows: By this property, we can easily grasp the processing stream. In particular, we can easily shoot trouble and source of abnormality when debugging a system. Due to the above property and others, in particular due to its rigorous framework based 0,1 logic, MG has been studied in information science field (Hobbs 1978, Friedman 1978, Yonezaki 1980, 157 Nishida 1980, Landsbergen 1980, Moran 1982, Moore 1981, Rosenschein 1982, ...). Application of MG to machine translation was also attempted (Hauenschild 1979, Landsbergen 1982), but those systems have only partially utilized the power of MG. Our approach attempts to utilize the full power of MG. 2.3 Application of Montague Grammar to Machine Translation In order to obtain the syntactic structure in Japanese from an intensional logical form, in the same way as interpretation process of MG, we change the semantic domain from set theoretical domain to conceptual domain for Japanese. Each conceptual unit contains its syntactic expression in Japanese.</context>
</contexts>
<marker>Moore, 1981</marker>
<rawString>(40) Moore, R.C. (1981): Problems in Logical Form, Proc. 19th Annual Meeting of the ACL, 117-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Moran</author>
</authors>
<title>The Representation of Inconsistent Information in a Dynamic Model(42) Theoretic Semantics,</title>
<date>1982</date>
<booktitle>Proc. 20th Annual Meeting of the ACL,</booktitle>
<pages>16--18</pages>
<contexts>
<context position="6393" citStr="Moran 1982" startWordPosition="1001" endWordPosition="1002">pecifies what operations are done in what order. For example, suppose input data has a structure like: (6) For the sake of property (5), the interpretation of (6) is done as a data flow computation process as follows: By this property, we can easily grasp the processing stream. In particular, we can easily shoot trouble and source of abnormality when debugging a system. Due to the above property and others, in particular due to its rigorous framework based 0,1 logic, MG has been studied in information science field (Hobbs 1978, Friedman 1978, Yonezaki 1980, 157 Nishida 1980, Landsbergen 1980, Moran 1982, Moore 1981, Rosenschein 1982, ...). Application of MG to machine translation was also attempted (Hauenschild 1979, Landsbergen 1982), but those systems have only partially utilized the power of MG. Our approach attempts to utilize the full power of MG. 2.3 Application of Montague Grammar to Machine Translation In order to obtain the syntactic structure in Japanese from an intensional logical form, in the same way as interpretation process of MG, we change the semantic domain from set theoretical domain to conceptual domain for Japanese. Each conceptual unit contains its syntactic expression </context>
</contexts>
<marker>Moran, 1982</marker>
<rawString>Moran, D.B. (1982): The Representation of Inconsistent Information in a Dynamic Model(42) Theoretic Semantics, Proc. 20th Annual Meeting of the ACL, 16-18.</rawString>
</citation>
<citation valid="false">
<title>you may not eat the banana -&gt; not(may( eat banana&gt;)) (43) 2) Logic vs machine translation: The sentence (44) is logically equivalent to (45), but that paraphrasing is bad in machine translation. he reads and writes English. (44) he reads English and he writes English. (45) 7. CONCLUSION Application of formal semantics to machine translation brings about new phase of machine translation. It makes the translation process clearer than conventional systems. The theory has been tested by implementing a prototype, which can translate real texts with plausible human assist.</title>
<marker></marker>
<rawString>you may not eat the banana -&gt; not(may( &lt;you eat banana&gt;)) (43) 2) Logic vs machine translation: The sentence (44) is logically equivalent to (45), but that paraphrasing is bad in machine translation. he reads and writes English. (44) he reads English and he writes English. (45) 7. CONCLUSION Application of formal semantics to machine translation brings about new phase of machine translation. It makes the translation process clearer than conventional systems. The theory has been tested by implementing a prototype, which can translate real texts with plausible human assist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nishida</author>
<author>S Doshita</author>
</authors>
<title>Hierarchical Meaning Representation and Analysis of Natural Language Documents,</title>
<date>1980</date>
<booktitle>Proc. COLING</booktitle>
<volume>80</volume>
<pages>85--92</pages>
<marker>Nishida, Doshita, 1980</marker>
<rawString>Nishida, T. and Doshita, S. (1980): Hierarchical Meaning Representation and Analysis of Natural Language Documents, Proc. COLING 80, 85-92.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S J Rosenschein</author>
</authors>
<title>and Shieber, S.M.(1982): Translating English into Logical Form,</title>
<booktitle>Proc. 20th Annual Meeting of the. ACL,</booktitle>
<pages>1--8</pages>
<marker>Rosenschein, </marker>
<rawString>Rosenschein, S.J. and Shieber, S.M.(1982): Translating English into Logical Form, Proc. 20th Annual Meeting of the. ACL, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yonezaki</author>
<author>H Enomoto</author>
</authors>
<date>1980</date>
<booktitle>Database System Based on Intensional Logic, Proc. COLING</booktitle>
<volume>80</volume>
<pages>220--227</pages>
<marker>Yonezaki, Enomoto, 1980</marker>
<rawString>Yonezaki, H. and Enomoto, H. (1980): Database System Based on Intensional Logic, Proc. COLING 80, 220-227.</rawString>
</citation>
<citation valid="false">
<authors>
<author>I-</author>
</authors>
<title>7 - ift-r 41. -c E THE R $1.11fl es #51.: 111.5°:IfEt orNItt &apos;CDR&amp;</title>
<booktitle>tt6., ts: INT* 71 I y- 7 ;Veit JAM/ i• (ifattiO:PIV &amp;quot;,:t.■;)*1)I. ; :Lo).:: )1:At qtit311:0= no) THE RNET, t/r .1••• -&amp;quot;C /IVA t j.; ;) fit,,qt/r,r) V7ret 7.) A •Y- % at1151:Irla Ale.1M1:Vt ; r</booktitle>
<marker>I-, </marker>
<rawString>.*-7 I- 7 - ift-r 41. -c E THE R $1.11fl es #51.: 111.5°:IfEt orNItt &apos;CDR&amp; tt6., ts: INT* 71 I y- 7 ;Veit JAM/ i• (ifattiO:PIV &amp;quot;,:t.■;)*1)I. ; :Lo).:: )1:At qtit311:0= no) THE RNET, t/r .1••• -&amp;quot;C /IVA t j.; ;) fit,,qt/r,r) V7ret 7.) A •Y- % at1151:Irla Ale.1M1:Vt ; r</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>