<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001866">
<title confidence="0.995972">
Learning Adjective Meanings with a Tensor-Based Skip-Gram Model
</title>
<author confidence="0.999404">
Jean Maillard Stephen Clark
</author>
<affiliation confidence="0.9983915">
University of Cambridge University of Cambridge
Computer Laboratory Computer Laboratory
</affiliation>
<email confidence="0.989305">
jean@maillard.it sc609@cam.ac.uk
</email>
<sectionHeader confidence="0.993595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820785714286">
We present a compositional distributional
semantic model which is an implementa-
tion of the tensor-based framework of Co-
ecke et al. (2011). It is an extended skip-
gram model (Mikolov et al., 2013) which
we apply to adjective-noun combinations,
learning nouns as vectors and adjectives
as matrices. We also propose a novel
measure of adjective similarity, and show
that adjective matrix representations lead
to improved performance in adjective and
adjective-noun similarity tasks, as well as
in the detection of semantically anomalous
adjective-noun pairs.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960476190476">
A number of approaches have emerged for com-
bining compositional and distributional seman-
tics. Some approaches assume that all words and
phrases are represented by vectors living in the
same semantic space, and use mathematical op-
erations such as vector addition and element-wise
multiplication to combine the constituent vectors
(Mitchell and Lapata, 2008). In these relatively
simple methods, the composition function does
not typically depend on its arguments or their syn-
tactic role in the sentence.
An alternative which makes more use of gram-
matical structure is the recursive neural network
approach of Socher et al. (2010). Constituent vec-
tors in a phrase are combined using a matrix and
non-linearity, with the resulting vector living in the
same vector space as the inputs. The matrices can
be parameterised by the syntactic type of the com-
bining words or phrases (Socher et al., 2013; Her-
mann and Blunsom, 2013). Socher et al. (2012)
extend this idea by representing the meanings of
words and phrases as both a vector and a matrix,
introducing a form of lexicalisation into the model.
A further extension, which moves us closer to
formal semantics (Dowty et al., 1981), is to build
a semantic representation in step with the syntac-
tic derivation, and have the embeddings of words
be determined by their syntactic type. Coecke et
al. (2011) achieve this by treating relational words
such as verbs and adjectives as functions in the
semantic space. The functions are assumed to
be multilinear maps, and are therefore realised as
tensors, with composition being achieved through
tensor contraction.1 While the framework speci-
fies the “shape” or semantic type of these tensors,
it makes no assumption about how the values of
these tensors should be interpreted (nor how they
can be learned).
A proposal for the case of adjective-noun com-
binations is given by Baroni and Zamparelli
(2010) (and also Guevara (2010)). Their model
represents adjectives as matrices over noun space,
trained via linear regression to approximate the
“holistic” adjective-noun vectors from the corpus.
In this paper we propose a new solution to the
problem of learning adjective meaning represen-
tations. The model is an implementation of the
tensor framework of Coecke et al. (2011), here
applied to adjective-noun combinations as a start-
ing point. Like Baroni and Zamparelli (2010), our
model also learn nouns as vectors and adjectives as
matrices, but uses a skip-gram approach with neg-
ative sampling (Mikolov et al., 2013), extended to
learn matrices.
We also propose a new way of quantifying ad-
jective similarity, based on the action of adjec-
tives on nouns (consistent with the view that ad-
jectives are functions). We use this new measure
instead of the naive cosine similarity function ap-
plied to matrices, and obtain competitive perfor-
mance compared to the baseline skip-gram vec-
tors (Mikolov et al., 2013) on an adjective simi-
larity task. We also perform competitively on the
</bodyText>
<footnote confidence="0.935734">
1Baroni et al. (2014) have developed a similar approach.
</footnote>
<page confidence="0.941629">
327
</page>
<note confidence="0.7679085">
Proceedings of the 19th Conference on Computational Language Learning, pages 327–331,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.999170454545455">
SEXTANT
CICERO
DEFER
TREE
THE
APPLE
FROM
PICK
RIPE
ONE
APPLE
UNRIPE
UNRIPE APPLE
BRAILLE
OMEN
SOUR
VERY
SAYS
TASTED
GREEN
SMALL
THE
</figure>
<figureCaption confidence="0.807857666666667">
Figure 1: Learning the vector for apple in the con-
text pick one ripe apple from the tree. The vector
for apple is updated in order to increase the inner
product with green vectors and decrease it with red
ones, which are negatively sampled.
Figure 2: Learning the matrix for unripe in the
</figureCaption>
<bodyText confidence="0.985356777777778">
context the green small unripe apple tasted very
sour. The matrix for unripe is updated to increase
the inner product of the vector for unripe apple
with green vectors and decrease it with red ones.
adjective-noun similarity dataset from Mitchell
and Lapata (2010). Finally, the tensor-based skip-
gram model also leads to improved performance in
the detection of semantically anomalous adjective-
noun phrases, compared to previous work.
</bodyText>
<sectionHeader confidence="0.814537" genericHeader="method">
2 A tensor-based skip-gram model
</sectionHeader>
<bodyText confidence="0.999976833333333">
Our model treats adjectives as linear maps over the
vector space of noun meanings, encoded as matri-
ces. The algorithm works in two stages: the first
stage learns the noun vectors, as in a standard skip-
gram model, and the second stage learns the adjec-
tive matrices, given fixed noun vectors.
</bodyText>
<subsectionHeader confidence="0.999115">
2.1 Training of nouns
</subsectionHeader>
<bodyText confidence="0.9999655">
To learn noun vectors, we use a skip-gram model
with negative sampling (Mikolov et al., 2013).
Each noun n in the vocabulary is assigned two d-
dimensional vectors: a content vector n, which
constitutes the embedding, and a context vector
n&apos;. For every occurrence of a noun n in the corpus,
the embeddings are updated in order to maximise
the objective function
</bodyText>
<equation confidence="0.981234">
E log Q(n • c&apos;) + E log Q(—n • c&apos;), (1)
CLEC C1EC
</equation>
<bodyText confidence="0.999958">
where C is a set of contexts for the current noun,
and C is a set of negative contexts. The contexts
are taken to be the vectors of words in a fixed win-
dow around the noun, while the negative contexts
are vectors for k words sampled from a unigram
distribution raised to the power of 3/4 (Goldberg,
2014). In our experiments, we have set k = 5.
After each step, both content and context vec-
tors are updated via back-propagation. This pro-
cedure leads to noun embeddings (content vectors)
which have a high inner product with the vectors
of words in the context of the noun, and a low
inner product with vectors of negatively sampled
words. Fig. 1 shows this intuition.
</bodyText>
<subsectionHeader confidence="0.999862">
2.2 Training of adjectives
</subsectionHeader>
<bodyText confidence="0.999980375">
Each adjective a in the vocabulary is assigned a
matrix A, initialised to the identity plus uniform
noise. First, all adjective-noun phrases (a, n) are
extracted from the corpus. For each (a, n) pair, the
corresponding adjective matrix A and noun vec-
tor n are multiplied to compute the adjective-noun
vector An. The matrix A is then updated to max-
imise the objective function
</bodyText>
<equation confidence="0.8813975">
E log Q(An • c&apos;) + E log Q(—An • c&apos;). (2)
CIEC C1EC
</equation>
<bodyText confidence="0.999451733333334">
The contexts C are taken to be the vectors of words
in a window around the adjective-noun phrase,
while the negative contexts C are again vectors of
randomly sampled words. Matrices are initialised
to the identity, while the context vectors C are the
results of Section 2.1.
Finally, the matrix A is updated via back-
propagation. Equation 2 means that the induced
matrices will have the following property: when
multiplying the matrix with a noun vector, the re-
sulting adjective-noun vector will have a high in-
ner product with words in the context window of
the adjective-noun phrase, and low inner product
for negatively sampled words. This is exemplified
in Figure 2.
</bodyText>
<page confidence="0.992992">
328
</page>
<subsectionHeader confidence="0.996285">
2.3 Similarity measure
</subsectionHeader>
<bodyText confidence="0.999541333333333">
The similarity of two vectors n and m is gener-
ally measured using the cosine similarity function
(Turney and Pantel, 2010; Baroni et al., 2014),
</bodyText>
<equation confidence="0.646718">
MODEL CORRELATION
SKIPGRAM-300 0.776
TBSG-100 0.769
n • m
vecsim(n, m) =
Table 1: Spearman rank correlation on noun simi-
larity task.
JJnJJ JJmJJ .
</equation>
<bodyText confidence="0.99846580952381">
Based on tests using a development set, we found
that using cosine to measure the similarity of ad-
jective matrices leads to no correlation with gold-
standard similarity judgements. Cosine similarity,
while suitable for vectors, does not capture any in-
formation about the function of matrices as linear
maps. We postulate that a suitable measure of the
similarity of two adjectives should be related to
how similarly they transform nouns.
Consider two adjective matrices A and B. If
An and Bn are similar vectors for every noun
vector n, then we deem the adjectives to be simi-
lar. Therefore, one possible measure involves cal-
culating the cosine distance between the images of
all nouns under the two adjectives, and taking the
average or median of these distances. Rather than
working on every noun in the vocabulary, which
is expensive, we instead take the most frequent
nouns, cluster them, and use the cluster centroids
(obtained in our case using k-means). The result-
ing distance function is given by
</bodyText>
<equation confidence="0.754698">
vecsim(An, Bn),
(3)
</equation>
<bodyText confidence="0.999199">
where the median is taken over the set of cluster
centroids ,N.2
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999924181818182">
The model is trained on a dump of the English
Wikipedia, automatically parsed with the C&amp;C
parser (Clark and Curran, 2007). The corpus con-
tains around 200 million noun examples, and 30
million adjective-noun examples. For every con-
text word in the corpus, 5 negative words are sam-
pled from the unigram distribution. Subsampling
is used to decrease the number of frequent words
(Mikolov et al., 2013). We train 100-dimensional
noun vectors and 100x100-dimensional adjective
matrices.
</bodyText>
<subsectionHeader confidence="0.999484">
3.1 Word Similarity
</subsectionHeader>
<bodyText confidence="0.974862">
First we test word, rather than phrase, similarity
on the MEN test collection (Bruni et al., 2014),
</bodyText>
<footnote confidence="0.7626115">
2We chose the median instead of the average as it is more
resistant to outliers in the data.
</footnote>
<figure confidence="0.716561666666667">
MODEL CORRELATION
TBSG-100x100 0.645
SKIPGRAM-300 0.638
</figure>
<tableCaption confidence="0.869221">
Table 2: Spearman rank correlation on adjective
similarity task.
</tableCaption>
<bodyText confidence="0.999627862068965">
which contains a set of POS-tagged word pairs to-
gether with gold-standard human similarity judge-
ments. We use the POS tags to select all noun-
noun and adjective-adjective pairs, leaving us with
a set of 643 noun-noun pairs and 96 adjective-
adjective pairs. For the noun-noun dataset, we are
testing the quality of the 100-dimensional noun
vectors from the first stage of the tensor-based
skip-gram model (TBSG), which is essentially
word2vec applied to just learning noun vectors.
These are compared to the 300-dimensional SKIP-
GRAM vectors available from the word2vec
page (which have been trained on a very large
news corpus).3
The adjective-adjective pairs are used to test
the 100 x 100 matrices obtained from our TBSG
model, again compared to the 300-dimensional
SKIPGRAM vectors. The Spearman correlations
between human judgements and the similarity of
vectors are reported in Tables 1 and 2. Note that
for adjectives we used the similarity measure de-
scribed in Section 2.3. Table 1 shows that the
noun vectors we use are of a high quality, perform-
ing comparably to the SKIPGRAM noun vectors
on the noun-noun similarity data. Table 2 shows
our TBSG adjective matrices, plus new similarity
measure, to also perform comparably to the SKIP-
GRAM adjective vectors on the adjective-adjective
similarity data.
</bodyText>
<subsectionHeader confidence="0.999674">
3.2 Phrase Similarity
</subsectionHeader>
<bodyText confidence="0.999961">
The TBSG model aims to learn matrices that act
in a compositional manner. Therefore, a more in-
teresting evaluation of its performance is to test
how well the matrices combine with noun vectors.
</bodyText>
<equation confidence="0.721407454545455">
3http://word2vec.googlecode.com/
matsim(A, B) = median
neN
329
MODEL CORRELATION
TBSG-100 0.50
SKIPGRAM-300 (add) 0.48
SKIPGRAM-300 (N only) 0.43
TBSG-100 (N only) 0.42
REG-600 0.37
humans 0.52
</equation>
<tableCaption confidence="0.9756655">
Table 3: Spearman rank correlation on adjective-
noun similarity task.
</tableCaption>
<table confidence="0.992493166666667">
COSINE DENSITY
MODEL t sig. t sig.
TBSG-100 5.16 *** 5.72 ***
ADD-300 0.31 2.63 **
MUL-300 -0.56 2.68 **
REG-300 0.48 3.12 **
</table>
<tableCaption confidence="0.781382333333333">
Table 4: Correlation on test data for semantic
anomalies. Significance levels are marked *** for
p &lt; 0.001, ** for p &lt; 0.01.
</tableCaption>
<bodyText confidence="0.999985807692308">
We use the Mitchell and Lapata (2010) adjective-
noun similarity dataset, which contains pairs of
adjective-noun phrases such as last number – vast
majority together with gold-standard human simi-
larity judgements. For the evaluation, we calculate
the Spearman correlation between non-averaged
human similarity judgements and the cosine simi-
larity of the vectors produced by various composi-
tional models.
The results in Table 3 show that TBSG has
the best correlation with human judgements of
the other models tested. It outperforms SKIP-
GRAM vectors with both addition and element-
wise multiplication as composition functions (the
latter not shown in that table, as it is worse than
addition). Also reported is the baseline perfor-
mance of SKIPGRAM and TBSG when using only
nouns to compute similarity (ignoring the adjec-
tives). It is interesting to note that TBSG also out-
performs the result of the matrix-vector linear re-
gression method (REG-600) of Baroni and Zam-
parelli (2010) as reported by Vecchi et al. (2015)
on the same dataset. Their method trains a matrix
for every adjective via linear regression to approx-
imate corpus-extracted “holistic” adjective-noun
vectors, and is therefore similar in spirit to TBSG.
</bodyText>
<subsectionHeader confidence="0.999853">
3.3 Semantic Anomaly
</subsectionHeader>
<bodyText confidence="0.999964814814815">
Finally, we use the model to distinguish between
semantically acceptable and anomalous adjective-
noun phrases, using the data from Vecchi et al.
(2011). The data consists of two sets: a set of un-
observed acceptable phrases (e.g. ethical statute)
and one of deviant phrases (e.g. cultural acne).
Following Vecchi et al. (2011) we use two indices
of semantic anomaly. The first, denoted COSINE,
is the cosine between the adjective-noun vector
and the noun vector. This is based on the hypothe-
sis that deviant adjective-noun vectors will form a
wider angle with the noun vector. The second in-
dex, denoted DENSITY, is the average cosine dis-
tance between the adjective-noun vector and its 10
nearest noun neighbours. This measure is based
on the hypothesis that nonsensical adjective-nouns
should not have many neighbours in the space
of (meaningful) nouns.4 These two measures are
computed for the acceptable and deviant sets, and
compared using a two-tailed Welch’s t-test.
Table 4 compares the performance of TBSG
with the results of count-based vectors using
addition (ADD) and element-wise multiplication
(MUL) reported by Vecchi et al. (2011), as well as
the matrix-vector linear regression method (REG-
300) of Baroni and Zamparelli (2010). TBSG ob-
tains the highest scores with both measures.
</bodyText>
<sectionHeader confidence="0.999686" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999645368421053">
In this paper we have implemented the tensor-
based framework of Coecke et al. (2011) in the
form of a skip-gram model extended to learn
higher-order embeddings, in this case adjectives as
matrices. While adjectives and nouns are learned
separately in this study, an obvious extension is
to learn embeddings jointly. We find the tensor-
based skip-gram model particularly attractive for
the obvious ways in which it can be extended to
other parts-of-speech (Maillard et al., 2014). For
example, in this framework transitive verbs are
third-order tensors which yield a sentence vector
when contracted with subject and object vectors.
Assuming contextual representations of sentences,
these could be learned by the tensor-based skip-
gram as a straghtforward extension from second-
order (matrices) to third-order tensors (and poten-
tially beyond for words requiring even higher or-
der tensors).
</bodyText>
<footnote confidence="0.97041425">
4Vecchi et al. (2011) also use a third index of semantic
anomaly, based on the length of adjective-noun vectors. We
omit this measure as we deem it unsuitable for models not
based on context counts and elementwise vector operations.
</footnote>
<page confidence="0.996885">
330
</page>
<sectionHeader confidence="0.996783" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.853295714285714">
Jean Maillard is supported by an EPSRC Doc-
toral Training Grant and a St John’s Scholar-
ship. Stephen Clark is supported by ERC Start-
ing Grant DisCoTex (306920) and EPSRC grant
EP/I037512/1. We would like to thank Tamara
Polajnar, Laura Rimell, and Eva Vecchi for useful
discussion.
</bodyText>
<sectionHeader confidence="0.99329" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999139513157895">
Marco Baroni, Roberto Zamparelli 2010. Nouns
are vectors, adjectives are matrices: represent-
ing adjective-noun constructions in semantic space.
Proceedings of EMNLP, 1183–1193.
Marco Baroni, Raffaela Bernardi, Roberto Zamparelli
2014 Frege in space: A program for compositional
distributional semantics. Linguistic Issues in Lan-
guage Technologies 9(6), 5–110.
Marco Baroni, Georgiana Dinu, Germ´an Kruszewski
2014 Don’t count, predict! A systematic compar-
ison of context-counting vs. context-predicting se-
mantic vectors. Proceedings of ACL, 238–247.
Elia Bruni, Nam Khanh Tran, Marco Baroni 2014
Multimodal Distributional Semantics. Journal of
Artificial Intelligence Research 49, 1–47.
Stephen Clark, James R. Curran 2007 Wide-coverage
efficient statistical parsing with CCG and log-linear
models. Computational Linguistics 33(4), 493–552.
Bob Coecke, Mehrnoosh Sadrzadeh, Stephen Clark
2011. Mathematical Foundations for a Composi-
tional Distributional Model of Meaning. Linguistic
Analysis 36 (Lambek Festschrift), 345–384.
David R. Dowty, Robert E. Wall, Stanley Peters 1981
Introduction to Montague semantics. Dordrecht.
Yoav Goldberg, Omer Levy 2014 word2vec
Explained: deriving Mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
1402.3722.
Emiliano Guevara 2010 A regression model of
adjective-noun compositionality in distributional se-
mantics. Proceedings of the ACL GEMS workshop,
33–37.
Karl Moritz Hermann and Phil Blunsom. 2013 The
role of syntax in vector space models of composi-
tional semantics. Proceedings ofACL, 894–904.
Jean Maillard, Stephen Clark, Edward Grefenstette
2014. A Type-Driven Tensor-Based Semantics for
CCG. Proceedings of the EACL 2014 Type Theory
and Natural Language Semantics Workshop, 46–54.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, Jeff Dean 2013. Distributed representations
of words and phrases and their compositionality.
Proceedings of NIPS, 3111–3119.
Jeff Mitchell, Mirella Lapata 2008 Vector-based mod-
els of semantic composition. Proceedings of ACL
08, 263–244.
Jeff Mitchell, Mirella Lapata 2010 Composition in
Distributional Models of Semantics. Cognitive sci-
ence 34(8), 1388–1439.
Richard Socher, Christopher D. Manning, Andrew Y.
Ng 2010 Learning continuous phrase represen-
tations and syntactic parsing with recursive neural
networks. Proceedings of the NIPS Deep Learning
and Unsupervised Feature Learning Workshop, 1–9.
Richard Socher, Brody Huval, Christopher D. Man-
ning, Andrew Y. Ng 2012 Semantic composition-
ality through recursive matrix-vector spaces. Pro-
ceedings of EMNLP, 1201–1211.
Richard Socher, John Bauer, Christopher D. Manning,
Andrew Y. Ng 2013 Parsing with Compositional
Vector Grammars. Proceedings of ACL, 455-465.
Mark Steedman 2000 The Syntactic Process. MIT
Press.
Peter D. Turney, Patrick Pantel 2010. From fre-
quency to meaning: vector space models of seman-
tics. Journal of Artificial Intelligence Research 37,
141–188.
Eva M. Vecchi, Marco Baroni, Roberto Zamparelli
2011 (Linear) maps of the impossible: Capturing
semantic anomalies in distributional space. Pro-
ceedings of ACL DISCo Workshop, 1–9.
Eva M. Vecchi, Marco Marelli, Roberto Zamparelli,
Marco Baroni 2015 Spicy adjectives and nominal
donkeys: Capturing semantic deviance using com-
positionality in distributional spaces. Accepted for
publication.
</reference>
<page confidence="0.998785">
331
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578706">
<title confidence="0.999918">Learning Adjective Meanings with a Tensor-Based Skip-Gram Model</title>
<author confidence="0.999785">Jean Maillard Stephen Clark</author>
<affiliation confidence="0.999719">University of Cambridge University of Cambridge Computer Laboratory Computer Laboratory</affiliation>
<email confidence="0.584617">jean@maillard.itsc609@cam.ac.uk</email>
<abstract confidence="0.999385533333333">We present a compositional distributional semantic model which is an implementation of the tensor-based framework of Coecke et al. (2011). It is an extended skipgram model (Mikolov et al., 2013) which we apply to adjective-noun combinations, learning nouns as vectors and adjectives as matrices. We also propose a novel measure of adjective similarity, and show that adjective matrix representations lead to improved performance in adjective and adjective-noun similarity tasks, as well as in the detection of semantically anomalous adjective-noun pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Marco Baroni</author>
</authors>
<title>Roberto Zamparelli 2010. Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space.</title>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<marker>Baroni, </marker>
<rawString>Marco Baroni, Roberto Zamparelli 2010. Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space. Proceedings of EMNLP, 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaela Bernardi</author>
</authors>
<title>Roberto Zamparelli</title>
<date>2014</date>
<journal>Linguistic Issues in Language Technologies</journal>
<volume>9</volume>
<issue>6</issue>
<pages>5--110</pages>
<marker>Baroni, Bernardi, 2014</marker>
<rawString>Marco Baroni, Raffaela Bernardi, Roberto Zamparelli 2014 Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technologies 9(6), 5–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>238--247</pages>
<location>Germ´an Kruszewski</location>
<marker>Baroni, Dinu, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, Germ´an Kruszewski 2014 Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. Proceedings of ACL, 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
</authors>
<title>Nam Khanh Tran,</title>
<date>2014</date>
<journal>Journal of Artificial Intelligence Research</journal>
<volume>49</volume>
<pages>1--47</pages>
<location>Marco Baroni</location>
<marker>Bruni, 2014</marker>
<rawString>Elia Bruni, Nam Khanh Tran, Marco Baroni 2014 Multimodal Distributional Semantics. Journal of Artificial Intelligence Research 49, 1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<volume>33</volume>
<issue>4</issue>
<pages>493--552</pages>
<contexts>
<context position="8932" citStr="Clark and Curran, 2007" startWordPosition="1473" endWordPosition="1476">le measure involves calculating the cosine distance between the images of all nouns under the two adjectives, and taking the average or median of these distances. Rather than working on every noun in the vocabulary, which is expensive, we instead take the most frequent nouns, cluster them, and use the cluster centroids (obtained in our case using k-means). The resulting distance function is given by vecsim(An, Bn), (3) where the median is taken over the set of cluster centroids ,N.2 3 Evaluation The model is trained on a dump of the English Wikipedia, automatically parsed with the C&amp;C parser (Clark and Curran, 2007). The corpus contains around 200 million noun examples, and 30 million adjective-noun examples. For every context word in the corpus, 5 negative words are sampled from the unigram distribution. Subsampling is used to decrease the number of frequent words (Mikolov et al., 2013). We train 100-dimensional noun vectors and 100x100-dimensional adjective matrices. 3.1 Word Similarity First we test word, rather than phrase, similarity on the MEN test collection (Bruni et al., 2014), 2We chose the median instead of the average as it is more resistant to outliers in the data. MODEL CORRELATION TBSG-100</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark, James R. Curran 2007 Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics 33(4), 493–552.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bob Coecke</author>
</authors>
<title>Mehrnoosh Sadrzadeh, Stephen Clark 2011. Mathematical Foundations for a Compositional Distributional Model of Meaning. Linguistic Analysis 36 (Lambek Festschrift),</title>
<pages>345--384</pages>
<marker>Coecke, </marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, Stephen Clark 2011. Mathematical Foundations for a Compositional Distributional Model of Meaning. Linguistic Analysis 36 (Lambek Festschrift), 345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
<author>Robert E Wall</author>
</authors>
<title>Introduction to Montague semantics.</title>
<date>1981</date>
<location>Stanley Peters</location>
<marker>Dowty, Wall, 1981</marker>
<rawString>David R. Dowty, Robert E. Wall, Stanley Peters 1981 Introduction to Montague semantics. Dordrecht.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yoav Goldberg</author>
</authors>
<title>Omer Levy 2014 word2vec Explained: deriving Mikolov et al.’s negativesampling word-embedding method. arXiv preprint 1402.3722.</title>
<marker>Goldberg, </marker>
<rawString>Yoav Goldberg, Omer Levy 2014 word2vec Explained: deriving Mikolov et al.’s negativesampling word-embedding method. arXiv preprint 1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>Proceedings of the ACL GEMS workshop,</booktitle>
<pages>33--37</pages>
<contexts>
<context position="2709" citStr="Guevara (2010)" startWordPosition="420" endWordPosition="421">by their syntactic type. Coecke et al. (2011) achieve this by treating relational words such as verbs and adjectives as functions in the semantic space. The functions are assumed to be multilinear maps, and are therefore realised as tensors, with composition being achieved through tensor contraction.1 While the framework specifies the “shape” or semantic type of these tensors, it makes no assumption about how the values of these tensors should be interpreted (nor how they can be learned). A proposal for the case of adjective-noun combinations is given by Baroni and Zamparelli (2010) (and also Guevara (2010)). Their model represents adjectives as matrices over noun space, trained via linear regression to approximate the “holistic” adjective-noun vectors from the corpus. In this paper we propose a new solution to the problem of learning adjective meaning representations. The model is an implementation of the tensor framework of Coecke et al. (2011), here applied to adjective-noun combinations as a starting point. Like Baroni and Zamparelli (2010), our model also learn nouns as vectors and adjectives as matrices, but uses a skip-gram approach with negative sampling (Mikolov et al., 2013), extended </context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara 2010 A regression model of adjective-noun compositionality in distributional semantics. Proceedings of the ACL GEMS workshop, 33–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>Proceedings ofACL,</booktitle>
<pages>894--904</pages>
<contexts>
<context position="1712" citStr="Hermann and Blunsom, 2013" startWordPosition="253" endWordPosition="257">to combine the constituent vectors (Mitchell and Lapata, 2008). In these relatively simple methods, the composition function does not typically depend on its arguments or their syntactic role in the sentence. An alternative which makes more use of grammatical structure is the recursive neural network approach of Socher et al. (2010). Constituent vectors in a phrase are combined using a matrix and non-linearity, with the resulting vector living in the same vector space as the inputs. The matrices can be parameterised by the syntactic type of the combining words or phrases (Socher et al., 2013; Hermann and Blunsom, 2013). Socher et al. (2012) extend this idea by representing the meanings of words and phrases as both a vector and a matrix, introducing a form of lexicalisation into the model. A further extension, which moves us closer to formal semantics (Dowty et al., 1981), is to build a semantic representation in step with the syntactic derivation, and have the embeddings of words be determined by their syntactic type. Coecke et al. (2011) achieve this by treating relational words such as verbs and adjectives as functions in the semantic space. The functions are assumed to be multilinear maps, and are theref</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013 The role of syntax in vector space models of compositional semantics. Proceedings ofACL, 894–904.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jean Maillard</author>
<author>Stephen Clark</author>
</authors>
<title>Edward Grefenstette 2014. A Type-Driven Tensor-Based Semantics for CCG.</title>
<booktitle>Proceedings of the EACL 2014 Type Theory and Natural Language Semantics Workshop,</booktitle>
<pages>46--54</pages>
<marker>Maillard, Clark, </marker>
<rawString>Jean Maillard, Stephen Clark, Edward Grefenstette 2014. A Type-Driven Tensor-Based Semantics for CCG. Proceedings of the EACL 2014 Type Theory and Natural Language Semantics Workshop, 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="3298" citStr="Mikolov et al., 2013" startWordPosition="510" endWordPosition="513"> (2010) (and also Guevara (2010)). Their model represents adjectives as matrices over noun space, trained via linear regression to approximate the “holistic” adjective-noun vectors from the corpus. In this paper we propose a new solution to the problem of learning adjective meaning representations. The model is an implementation of the tensor framework of Coecke et al. (2011), here applied to adjective-noun combinations as a starting point. Like Baroni and Zamparelli (2010), our model also learn nouns as vectors and adjectives as matrices, but uses a skip-gram approach with negative sampling (Mikolov et al., 2013), extended to learn matrices. We also propose a new way of quantifying adjective similarity, based on the action of adjectives on nouns (consistent with the view that adjectives are functions). We use this new measure instead of the naive cosine similarity function applied to matrices, and obtain competitive performance compared to the baseline skip-gram vectors (Mikolov et al., 2013) on an adjective similarity task. We also perform competitively on the 1Baroni et al. (2014) have developed a similar approach. 327 Proceedings of the 19th Conference on Computational Language Learning, pages 327–</context>
<context position="5281" citStr="Mikolov et al., 2013" startWordPosition="837" endWordPosition="840">pata (2010). Finally, the tensor-based skipgram model also leads to improved performance in the detection of semantically anomalous adjectivenoun phrases, compared to previous work. 2 A tensor-based skip-gram model Our model treats adjectives as linear maps over the vector space of noun meanings, encoded as matrices. The algorithm works in two stages: the first stage learns the noun vectors, as in a standard skipgram model, and the second stage learns the adjective matrices, given fixed noun vectors. 2.1 Training of nouns To learn noun vectors, we use a skip-gram model with negative sampling (Mikolov et al., 2013). Each noun n in the vocabulary is assigned two ddimensional vectors: a content vector n, which constitutes the embedding, and a context vector n&apos;. For every occurrence of a noun n in the corpus, the embeddings are updated in order to maximise the objective function E log Q(n • c&apos;) + E log Q(—n • c&apos;), (1) CLEC C1EC where C is a set of contexts for the current noun, and C is a set of negative contexts. The contexts are taken to be the vectors of words in a fixed window around the noun, while the negative contexts are vectors for k words sampled from a unigram distribution raised to the power of</context>
<context position="9209" citStr="Mikolov et al., 2013" startWordPosition="1519" endWordPosition="1522">uster them, and use the cluster centroids (obtained in our case using k-means). The resulting distance function is given by vecsim(An, Bn), (3) where the median is taken over the set of cluster centroids ,N.2 3 Evaluation The model is trained on a dump of the English Wikipedia, automatically parsed with the C&amp;C parser (Clark and Curran, 2007). The corpus contains around 200 million noun examples, and 30 million adjective-noun examples. For every context word in the corpus, 5 negative words are sampled from the unigram distribution. Subsampling is used to decrease the number of frequent words (Mikolov et al., 2013). We train 100-dimensional noun vectors and 100x100-dimensional adjective matrices. 3.1 Word Similarity First we test word, rather than phrase, similarity on the MEN test collection (Bruni et al., 2014), 2We chose the median instead of the average as it is more resistant to outliers in the data. MODEL CORRELATION TBSG-100x100 0.645 SKIPGRAM-300 0.638 Table 2: Spearman rank correlation on adjective similarity task. which contains a set of POS-tagged word pairs together with gold-standard human similarity judgements. We use the POS tags to select all nounnoun and adjective-adjective pairs, leavi</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeff Dean 2013. Distributed representations of words and phrases and their compositionality. Proceedings of NIPS, 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
</authors>
<title>Mirella Lapata</title>
<date>2008</date>
<booktitle>Proceedings of ACL 08,</booktitle>
<pages>263--244</pages>
<marker>Mitchell, 2008</marker>
<rawString>Jeff Mitchell, Mirella Lapata 2008 Vector-based models of semantic composition. Proceedings of ACL 08, 263–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
</authors>
<title>Mirella Lapata</title>
<date>2010</date>
<journal>Cognitive science</journal>
<volume>34</volume>
<issue>8</issue>
<pages>1388--1439</pages>
<marker>Mitchell, 2010</marker>
<rawString>Jeff Mitchell, Mirella Lapata 2010 Composition in Distributional Models of Semantics. Cognitive science 34(8), 1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>Proceedings of the NIPS Deep Learning and Unsupervised Feature Learning Workshop,</booktitle>
<volume>1</volume>
<contexts>
<context position="1420" citStr="Socher et al. (2010)" startWordPosition="203" endWordPosition="206"> of approaches have emerged for combining compositional and distributional semantics. Some approaches assume that all words and phrases are represented by vectors living in the same semantic space, and use mathematical operations such as vector addition and element-wise multiplication to combine the constituent vectors (Mitchell and Lapata, 2008). In these relatively simple methods, the composition function does not typically depend on its arguments or their syntactic role in the sentence. An alternative which makes more use of grammatical structure is the recursive neural network approach of Socher et al. (2010). Constituent vectors in a phrase are combined using a matrix and non-linearity, with the resulting vector living in the same vector space as the inputs. The matrices can be parameterised by the syntactic type of the combining words or phrases (Socher et al., 2013; Hermann and Blunsom, 2013). Socher et al. (2012) extend this idea by representing the meanings of words and phrases as both a vector and a matrix, introducing a form of lexicalisation into the model. A further extension, which moves us closer to formal semantics (Dowty et al., 1981), is to build a semantic representation in step wit</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, Andrew Y. Ng 2010 Learning continuous phrase representations and syntactic parsing with recursive neural networks. Proceedings of the NIPS Deep Learning and Unsupervised Feature Learning Workshop, 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<contexts>
<context position="1734" citStr="Socher et al. (2012)" startWordPosition="258" endWordPosition="261">ectors (Mitchell and Lapata, 2008). In these relatively simple methods, the composition function does not typically depend on its arguments or their syntactic role in the sentence. An alternative which makes more use of grammatical structure is the recursive neural network approach of Socher et al. (2010). Constituent vectors in a phrase are combined using a matrix and non-linearity, with the resulting vector living in the same vector space as the inputs. The matrices can be parameterised by the syntactic type of the combining words or phrases (Socher et al., 2013; Hermann and Blunsom, 2013). Socher et al. (2012) extend this idea by representing the meanings of words and phrases as both a vector and a matrix, introducing a form of lexicalisation into the model. A further extension, which moves us closer to formal semantics (Dowty et al., 1981), is to build a semantic representation in step with the syntactic derivation, and have the embeddings of words be determined by their syntactic type. Coecke et al. (2011) achieve this by treating relational words such as verbs and adjectives as functions in the semantic space. The functions are assumed to be multilinear maps, and are therefore realised as tensor</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, Andrew Y. Ng 2012 Semantic compositionality through recursive matrix-vector spaces. Proceedings of EMNLP, 1201–1211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Y Andrew</author>
</authors>
<title>Ng</title>
<date>2013</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="1684" citStr="Socher et al., 2013" startWordPosition="249" endWordPosition="252">-wise multiplication to combine the constituent vectors (Mitchell and Lapata, 2008). In these relatively simple methods, the composition function does not typically depend on its arguments or their syntactic role in the sentence. An alternative which makes more use of grammatical structure is the recursive neural network approach of Socher et al. (2010). Constituent vectors in a phrase are combined using a matrix and non-linearity, with the resulting vector living in the same vector space as the inputs. The matrices can be parameterised by the syntactic type of the combining words or phrases (Socher et al., 2013; Hermann and Blunsom, 2013). Socher et al. (2012) extend this idea by representing the meanings of words and phrases as both a vector and a matrix, introducing a form of lexicalisation into the model. A further extension, which moves us closer to formal semantics (Dowty et al., 1981), is to build a semantic representation in step with the syntactic derivation, and have the embeddings of words be determined by their syntactic type. Coecke et al. (2011) achieve this by treating relational words such as verbs and adjectives as functions in the semantic space. The functions are assumed to be mult</context>
</contexts>
<marker>Socher, Bauer, Manning, Andrew, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, Andrew Y. Ng 2013 Parsing with Compositional Vector Grammars. Proceedings of ACL, 455-465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman 2000 The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Patrick Pantel 2010. From frequency to meaning: vector space models of semantics.</title>
<journal>Journal of Artificial Intelligence Research</journal>
<volume>37</volume>
<pages>141--188</pages>
<marker>Turney, </marker>
<rawString>Peter D. Turney, Patrick Pantel 2010. From frequency to meaning: vector space models of semantics. Journal of Artificial Intelligence Research 37, 141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva M Vecchi</author>
<author>Marco Baroni</author>
</authors>
<title>Roberto Zamparelli</title>
<date>2011</date>
<booktitle>Proceedings of ACL DISCo Workshop,</booktitle>
<volume>1</volume>
<marker>Vecchi, Baroni, 2011</marker>
<rawString>Eva M. Vecchi, Marco Baroni, Roberto Zamparelli 2011 (Linear) maps of the impossible: Capturing semantic anomalies in distributional space. Proceedings of ACL DISCo Workshop, 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva M Vecchi</author>
<author>Marco Marelli</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Spicy adjectives and nominal donkeys: Capturing semantic deviance using compositionality in distributional spaces. Accepted for publication.</title>
<date>2015</date>
<location>Marco Baroni</location>
<contexts>
<context position="12674" citStr="Vecchi et al. (2015)" startWordPosition="2071" endWordPosition="2074">al models. The results in Table 3 show that TBSG has the best correlation with human judgements of the other models tested. It outperforms SKIPGRAM vectors with both addition and elementwise multiplication as composition functions (the latter not shown in that table, as it is worse than addition). Also reported is the baseline performance of SKIPGRAM and TBSG when using only nouns to compute similarity (ignoring the adjectives). It is interesting to note that TBSG also outperforms the result of the matrix-vector linear regression method (REG-600) of Baroni and Zamparelli (2010) as reported by Vecchi et al. (2015) on the same dataset. Their method trains a matrix for every adjective via linear regression to approximate corpus-extracted “holistic” adjective-noun vectors, and is therefore similar in spirit to TBSG. 3.3 Semantic Anomaly Finally, we use the model to distinguish between semantically acceptable and anomalous adjectivenoun phrases, using the data from Vecchi et al. (2011). The data consists of two sets: a set of unobserved acceptable phrases (e.g. ethical statute) and one of deviant phrases (e.g. cultural acne). Following Vecchi et al. (2011) we use two indices of semantic anomaly. The first,</context>
</contexts>
<marker>Vecchi, Marelli, Zamparelli, 2015</marker>
<rawString>Eva M. Vecchi, Marco Marelli, Roberto Zamparelli, Marco Baroni 2015 Spicy adjectives and nominal donkeys: Capturing semantic deviance using compositionality in distributional spaces. Accepted for publication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>