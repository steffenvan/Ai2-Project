<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9973495">
Prefix Probability
for Probabilistic Synchronous Context-Free Grammars
</title>
<author confidence="0.985291">
Mark-Jan Nederhof
</author>
<affiliation confidence="0.999353">
School of Computer Science
University of St Andrews
</affiliation>
<address confidence="0.839690333333333">
North Haugh, St Andrews, Fife
KY16 9SX
United Kingdom
</address>
<email confidence="0.998869">
markjan.nederhof@googlemail.com
</email>
<author confidence="0.994976">
Giorgio Satta
</author>
<affiliation confidence="0.998509">
Dept. of Information Engineering
University of Padua
</affiliation>
<address confidence="0.877351666666667">
via Gradenigo, 6/A
I-35131 Padova
Italy
</address>
<email confidence="0.995106">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.997319" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999573">
We present a method for the computation of
prefix probabilities for synchronous context-
free grammars. Our framework is fairly gen-
eral and relies on the combination of a sim-
ple, novel grammar transformation and stan-
dard techniques to bring grammars into nor-
mal forms.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999996316666667">
Within the area of statistical machine translation,
there has been a growing interest in so-called syntax-
based translation models, that is, models that de-
fine mappings between languages through hierar-
chical sentence structures. Several such statistical
models that have been investigated in the literature
are based on synchronous rewriting or tree transduc-
tion. Probabilistic synchronous context-free gram-
mars (PSCFGs) are one among the most popular ex-
amples of such models. PSCFGs subsume several
syntax-based statistical translation models, as for in-
stance the stochastic inversion transduction gram-
mars of Wu (1997), the statistical model used by the
Hiero system of Chiang (2007), and systems which
extract rules from parsed text, as in Galley et al.
(2004).
Despite the widespread usage of models related to
PSCFGs, our theoretical understanding of this class
is quite limited. In contrast to the closely related
class of probabilistic context-free grammars, a syn-
tax model for which several interesting mathemati-
cal and statistical properties have been investigated,
as for instance by Chi (1999), many theoretical prob-
lems are still unsolved for the class of PSCFGs.
This paper considers a parsing problem that is
well understood for probabilistic context-free gram-
mars but that has never been investigated in the con-
text of PSCFGs, viz. the computation of prefix prob-
abilities. In the case of a probabilistic context-free
grammar, this problem is defined as follows. We
are asked to compute the probability that a sentence
generated by our model starts with a prefix string v
given as input. This quantity is defined as the (pos-
sibly infinite) sum of the probabilities of all strings
of the form vw, for any string w over the alphabet
of the model. This problem has been studied by
Jelinek and Lafferty (1991) and by Stolcke (1995).
Prefix probabilities can be used to compute probabil-
ity distributions for the next word or part-of-speech.
This has applications in incremental processing of
text or speech from left to right; see again (Jelinek
and Lafferty, 1991). Prefix probabilities can also be
exploited in speech understanding systems to score
partial hypotheses in beam search (Corazza et al.,
1991).
This paper investigates the problem of computing
prefix probabilities for PSCFGs. In this context, a
pair of strings v1 and v2 is given as input, and we are
asked to compute the probability that any string in
the source language starting with prefix v1 is trans-
lated into any string in the target language starting
with prefix v2. This probability is more precisely
defined as the sum of the probabilities of translation
pairs of the form [v1w1, v2w21, for any strings w1
and w2.
A special case of prefix probability for PSCFGs
is the right prefix probability. This is defined as the
probability that some (complete) input string w in
the source language is translated into a string in the
target language starting with an input prefix v.
</bodyText>
<page confidence="0.98789">
460
</page>
<note confidence="0.979518">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 460–469,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999855108108108">
Prefix probabilities and right prefix probabilities
for PSCFGs can be exploited to compute probabil-
ity distributions for the next word or part-of-speech
in left-to-right incremental translation, essentially in
the same way as described by Jelinek and Lafferty
(1991) for probabilistic context-free grammars, as
discussed later in this paper.
Our solution to the problem of computing prefix
probabilities is formulated in quite different terms
from the solutions by Jelinek and Lafferty (1991)
and by Stolcke (1995) for probabilistic context-free
grammars. In this paper we reduce the computation
of prefix probabilities for PSCFGs to the computa-
tion of inside probabilities under the same model.
Computation of inside probabilities for PSCFGs is
a well-known problem that can be solved using off-
the-shelf algorithms that extend basic parsing algo-
rithms. Our reduction is a novel grammar trans-
formation, and the proof of correctness proceeds
by fairly conventional techniques from formal lan-
guage theory, relying on the correctness of standard
methods for the computation of inside probabilities
for PSCFG. This contrasts with the techniques pro-
posed by Jelinek and Lafferty (1991) and by Stolcke
(1995), which are extensions of parsing algorithms
for probabilistic context-free grammars, and require
considerably more involved proofs of correctness.
Our method for computing the prefix probabili-
ties for PSCFGs runs in exponential time, since that
is the running time of existing methods for comput-
ing the inside probabilities for PSCFGs. It is un-
likely this can be improved, because the recogni-
tion problem for PSCFG is NP-complete, as estab-
lished by Satta and Peserico (2005), and there is a
straightforward reduction from the recognition prob-
lem for PSCFGs to the problem of computing the
prefix probabilities for PSCFGs.
</bodyText>
<sectionHeader confidence="0.993802" genericHeader="introduction">
2 Definitions
</sectionHeader>
<bodyText confidence="0.999657294117647">
In this section we introduce basic definitions re-
lated to synchronous context-free grammars and
their probabilistic extension; our notation follows
Satta and Peserico (2005).
Let N and E be sets of nonterminal and terminal
symbols, respectively. In what follows we need to
represent bijections between the occurrences of non-
terminals in two strings over N UE. This is realized
by annotating nonterminals with indices from an in-
finite set. We define Z(N) = {A t �A E N, t E
N} and VI = Z(N) U E. For a string -y E VI∗ , we
write index(-y) to denote the set of all indices that
appear in symbols in -y.
Two strings -y1, -y2 E VI∗ are synchronous if each
index from N occurs at most once in -y1 and at most
once in -y2, and index(-y1) = index(-y2). Therefore
-y1, -y2 have the general form:
</bodyText>
<equation confidence="0.9484574">
-y1 = u10A t1
11 u11A t2 12 u12 &apos; &apos; &apos; u1r−1A tr
1r u1r
tπ(2)
22 u22 &apos; &apos; &apos; u2r−1A
where r &gt;_ 0, u1i, u2i E E∗, A ti
1i , A tπ(i)
2i E Z(N),
ti =� tj for i =� j, and 7r is a permutation of the set
{1,...,r}.
</equation>
<bodyText confidence="0.999908045454545">
A synchronous context-free grammar (SCFG)
is a tuple G = (N, E, P, 5), where N and E are fi-
nite, disjoint sets of nonterminal and terminal sym-
bols, respectively, 5 E N is the start symbol and
P is a finite set of synchronous rules. Each syn-
chronous rule has the form s : [A1 —* α1, A2 —*
α2], where A1, A2 E N and where α1, α2 E VI∗ are
synchronous strings. The symbol s is the label of
the rule, and each rule is uniquely identified by its
label. For technical reasons, we allow the existence
of multiple rules that are identical apart from their
labels. We refer to A1 —* α1 and A2 —* α2, respec-
tively, as the left and right components of rule s.
Example 1 The following synchronous rules im-
plicitly define a SCFG:
In each step of the derivation process of a SCFG
G, two nonterminals with the same index in a pair of
synchronous strings are rewritten by a synchronous
rule. This is done in such a way that the result is once
more a pair of synchronous strings. An auxiliary
notion is that of reindexing, which is an injective
function f from N to N. We extend f to VI by letting
</bodyText>
<equation confidence="0.920416">
f(A t ) = A f(t) for A t E Z(N) and f(a) = a
for a E E. We also extend f to strings in VI∗ by
-y2 = u20A 21 u21A
tπ(1)
tπ(r)
2r u2r
s1 : [5 —* A 1 B 2 , 5 — *B 2 A 1 ]
s2 : [A —* aA 1 b, A —* bA 1 a]
s3 : [A —* ab, A —* ba]
s4 : [B —* cB 1 d, B —* dB 1 c]
s5 : [B —* cd, B —* dc]
�
461
letting f(ε) = ε and f(Xγ) = f(X)f(γ), for each
X ∈VIand γ∈VI*.
</equation>
<bodyText confidence="0.9997144">
Let γ1, γ2 be synchronous strings in VI* . The de-
rive relation [γ1, γ2] ⇒G [δ1, δ2] holds whenever
there exist an index t in index(γ1) = index(γ2), a
synchronous rule s : [A1 → α1, A2 → α2] in P
and some reindexing f such that:
</bodyText>
<equation confidence="0.9934508">
(i) index(f(α1)) ∩ (index(γ1) \ {t}) = ∅;
(ii) γ1 = γiA1� γi, γ2 = γ�2A2 γ&amp;quot;2; and
(iii) δ1 = γif(α1)γ&amp;quot;, δ2 = γ�2f (a2)-Y2&amp;quot;-
We
a2)-Y2-
</equation>
<bodyText confidence="0.999971379310345">
We also write [γ1, γ2] ⇒sG [δ1, δ2] to explicitly
indicate that the derive relation holds through rule s.
Note that δ1, δ2 above are guaranteed to be syn-
chronous strings, because α1 and α2 are syn-
chronous strings and because of (i) above. Note
also that, for a given pair [γ1, γ2] of synchronous
strings, an index t and a rule s, there may be in-
finitely many choices of reindexing f such that the
above constraints are satisfied. In this paper we will
not further specify the choice of f.
We say the pair [A1, A2] of nonterminals is linked
(in G) if there is a rule of the form s : [A1 →
α1, A2 → α2]. The set of linked nonterminal pairs
is denoted by N[2].
A derivation is a sequence σ = s1s2 · · · sd of syn-
chronous rules si ∈ P with d ≥ 0 (σ = ε for
d = 0) such that [γ1i−1, γ2i−1] ⇒s�G [γ1i, γ2i] for
every i with 1 ≤ i ≤ d and synchronous strings
[γ1i, γ2i] with 0 ≤ i ≤ d. Throughout this paper,
we always implicitly assume some canonical form
for derivations in G, by demanding for instance that
each step rewrites a pair of nonterminal occurrences
of which the first is leftmost in the left component.
When we want to focus on the specific synchronous
strings being derived, we also write derivations in
the form [γ10, γ20] ⇒σG [γ1d, γ2d], and we write
[γ10, γ20] ⇒*G [γ1d, γ2d] when σ is not further
specified. The translation generated by a SCFG G
is defined as:
</bodyText>
<equation confidence="0.995844">
T(G) = {[w1, w2]  |[S 1 , S 1 ] ⇒*G [w1, w2],
w1, w2 ∈ Σ*}
</equation>
<bodyText confidence="0.999162357142857">
For w1, w2 ∈ Σ*, we write D(G, [w1, w2]) to de-
note the set of all (canonical) derivations σ such that
[S 1 , S 1 ] ⇒σG [w1, w2].
Analogously to standard terminology for context-
free grammars, we call a SCFG reduced if ev-
ery rule occurs in at least one derivation σ ∈
D(G, [w1, w2]), for some w1, w2 ∈ Σ*. We as-
sume without loss of generality that the start sym-
bol S does not occur in the right-hand side of either
component of any rule.
Example 2 Consider the SCFG G from example 1.
The following is a canonical derivation in G, since it
is always the leftmost nonterminal occurrence in the
left component that is involved in a derivation step:
</bodyText>
<equation confidence="0.991859625">
[S 1 , S 1 ] ⇒G [A 1 B 2 ,B 2 A 1 ]
⇒G [aA 3 bB 2 , B 2 bA 3 a]
⇒G [aaA 4 bbB 2 , B 2 bbA 4 aa]
⇒G [aaabbbB 2 , B 2 bbbaaa]
⇒G [aaabbbcB 5 d, dB 5 cbbbaaa]
⇒G [aaabbbccdd, ddccbbbaaa]
It is not difficult to see that the generated translation
isT(G) = {[apbpcqdq, dqcqbpap]  |p,q ≥ 1}. ❑
</equation>
<bodyText confidence="0.998357875">
The size of a synchronous rule s : [A1 → α1,
A2 → α2], is defined as |s |= |A1α1A2α2|. The
size of G is defined as |G |= EsEP |s|.
A probabilistic SCFG (PSCFG) is a pair G =
(G, pG) where G = (N, Σ, P, S) is a SCFG and pG
is a function from P to real numbers in [0, 1]. We
say that G is proper if for each pair [A1, A2] ∈ N[2]
we have:
</bodyText>
<equation confidence="0.771354">
� pG(s) = 1
s:[A1→α1, A2→α2]
</equation>
<bodyText confidence="0.998879285714286">
Intuitively, properness ensures that where a pair
of nonterminals in two synchronous strings can be
rewritten, there is a probability distribution over the
applicable rules.
For a (canonical) derivation σ = s1s2 · · · sd, we
define pG(σ) = Hdi�1 pG(si). For w1, w2 ∈Σ*,
we also define:
</bodyText>
<equation confidence="0.992854">
�pG([w1, w2]) = pG(σ) (1)
σED(G,[w1,w2])
</equation>
<bodyText confidence="0.997829">
We say a PSCFG is consistent if pG defines a prob-
ability distribution over the translation, or formally:
</bodyText>
<equation confidence="0.728808">
� pG([w1, w2]) = 1
w1,w2
</equation>
<page confidence="0.989485">
462
</page>
<bodyText confidence="0.9990928">
If the grammar is reduced, proper and consistent,
then also:
for every pair [A1, A2] E N[2]. The proof is identi-
cal to that of the corresponding fact for probabilistic
context-free grammars.
</bodyText>
<sectionHeader confidence="0.996682" genericHeader="method">
3 Effective PSCFG parsing
</sectionHeader>
<bodyText confidence="0.998060012820513">
If w = a1 · · · an then the expression w[i, j], with
0 &lt; i &lt; j &lt; n, denotes the substring ai+1 · · · aj (if
i = j then w[i, j] = E). In this section, we assume
the input is the pair [w1, w2] of terminal strings.
The task of a recognizer for SCFG G is to decide
whether [w1,w2] E T(G).
We present a general algorithm for solving the
above problem in terms of the specification of a de-
duction system, following Shieber et al. (1995). The
items that are constructed by the system have the
form [m1, A1, m01; m2, A2, m02], where [A1, A2] E
N[2] and where m1, m01, m2, m02 are non-negative
integers such that 0 &lt; m1 &lt; m01 &lt; |w1 |and
0 &lt; m2 &lt; m02 &lt; |w2|. Such an item can be de-
rived by the deduction system if and only if:
[A 1 1 , A21 ] ==�,∗G [w1[m1, m01], w2[m2, m02]]
The deduction system has one inference rule,
shown in figure 1. One of its side conditions has
a synchronous rule in P of the form:
Observe that, in the right-hand side of the two rule
components above, nonterminals A1i and A2π−1(i),
1 &lt; i &lt; r, have both the same index. More pre-
cisely, A1i has index ti and A2π−1(i) has index tip
with i0 = 7r(7r−1(i)) = i. Thus the nonterminals in
each antecedent item in figure 1 form a linked pair.
We now turn to a computational analysis of the
above algorithm. In the inference rule in figure 1
there are 2(r + 1) variables that can be bound to
positions in w1, and as many that can be bound to
positions in w2. However, the side conditions imply
m0ij = mij + |uij|, for i E 11, 21 and 0 &lt; j &lt; r,
and therefore the number of free variables is only
r + 1 for each component. By standard complex-
ity analysis of deduction systems, for example fol-
lowing McAllester (2002), the time complexity of
a straightforward implementation of the recogni-
tion algorithm is O(|P |· |w1|rmax+1 ·|w2|rmax+1),
where rmax is the maximum number of right-hand
side nonterminals in either component of a syn-
chronous rule. The algorithm therefore runs in ex-
ponential time, when the grammar G is considered
as part of the input. Such computational behavior
seems unavoidable, since the recognition problem
for SCFG is NP-complete, as reported by Satta and
Peserico (2005). See also Gildea and Stefankovic
(2007) and Hopkins and Langmead (2010) for fur-
ther analysis of the upper bound above.
The recognition algorithm above can easily be
turned into a parsing algorithm by letting an imple-
mentation keep track of which items were derived
from which other items, as instantiations of the con-
sequent and the antecedents, respectively, of the in-
ference rule in figure 1.
A probabilistic parsing algorithm that computes
PG([w1, w2]), defined in (1), can also be obtained
from the recognition algorithm above, by associat-
ing each item with a probability. To explain the ba-
sic idea, let us first assume that each item can be
inferred in finitely many ways by the inference rule
in figure 1. Each instantiation of the inference rule
should be associated with a term that is computed
by multiplying the probability of the involved rule
s and the product of all probabilities previously as-
sociated with the instantiations of the antecedents.
The probability associated with an item is then
computed as the sum of each term resulting from
some instantiation of an inference rule deriving that
item. This is a generalization to PSCFG of the in-
side algorithm defined for probabilistic context-free
grammars (Manning and Sch¨utze, 1999), and we
can show that the probability associated with item
[0, 5, |w1 |; 0, 5, |w2|] provides the desired value
PG([w1, w2]). We refer to the procedure sketched
above as the inside algorithm for PSCFGs.
However, this simple procedure fails if there are
cyclic dependencies, whereby the derivation of an
item involves a proper subderivation of the same
item. Cyclic dependencies can be excluded if it can
</bodyText>
<equation confidence="0.99967025">
� PG(a) = 1
w1,w2∈Σ*, σ∈P*
s.t. [A11 , A21 ]⇒°c[w1, w2]
s : [A1 —* u10A t�
11 u11 · · · u1r−1A t�
1r u1r,
t� t�(�)
A2 —* u20A21a) u21 ··· u2r−1A 2r u2r] (2)
</equation>
<page confidence="0.537004">
463
</page>
<equation confidence="0.991206052631579">
[m010, A11, m11; m02π−1(1)−1, A2π−1(1), m2π−1(1)]
..
.
[m01r−1, A1r, m1r; m02π−1(r)−1, A2π−1(r), m2π−1(r)]
[m10, A1, m01r; m20,A2,m02r]
��(�)
2r u2r] ∈ P,
A2 → u20A ��(�)
21 u21 · · · u2r−1A
w1[m10, m010] = u10,
...
w1[m1r, m01r] = u1r,
{
w2[m20, m020] = u20,
...
w2[m2r, m02r] = u2r
s:[A1 → u10A ��
11 u11 ··· u1r−1A ��
1r u1r,
</equation>
<figureCaption confidence="0.999152">
Figure 1: SCFG recognition, by a deduction system consisting of a single inference rule.
</figureCaption>
<bodyText confidence="0.999881281690141">
be guaranteed that, in figure 1, m01r − m10 is greater
than m1j − m01j−1 for each j (1 ≤ j ≤ r), or
m02r − m20 is greater than m2j − m02j−1 for each
j (1 ≤ j ≤ r).
Consider again a synchronous rule s of the form
in (2). We say s is an epsilon rule if r = 0 and
u10 = u20 = E. We say s is a unit rule if r = 1
and u10 = u11 = u20 = u21 = E. Similarly to
context-free grammars, absence of epsilon rules and
unit rules guarantees that there are no cyclic depen-
dencies between items and in this case the inside al-
gorithm correctly computes pG([w1, w2]).
Epsilon rules can be eliminated from PSCFGs
by a grammar transformation that is very similar
to the transformation eliminating epsilon rules from
a probabilistic context-free grammar (Abney et al.,
1999). This is sketched in what follows. We first
compute the set of all nullable linked pairs of non-
terminals of the underlying SCFG, that is, the set of
all [A1, A2] ∈ N[2] such that [A11 , A21 ] ⇒∗G [ε, ε].
This can be done in linear time O(|G|) using essen-
tially the same algorithm that identifies nullable non-
terminals in a context-free grammar, as presented for
instance by Sippu and Soisalon-Soininen (1988).
Next, we identify all occurrences of nullable pairs
[A1, A2] in the right-hand side components of a rule
s, such that A1 and A2 have the same index. For
every possible choice of a subset U of these occur-
rences, we add to our grammar a new rule sU con-
structed by omitting all of the nullable occurrences
in U. The probability of sU is computed as the prob-
ability of s multiplied by terms of the form:
for every pair [A1, A2] in U. After adding these extra
rules, which in effect circumvents the use of epsilon-
generating subderivations, we can safely remove all
epsilon rules, with the only exception of a possible
rule of the form [S → c, S → c]. The translation and
the associated probability distribution in the result-
ing grammar will be the same as those in the source
grammar.
One problem with the above construction is that
we have to create new synchronous rules sU for each
possible choice of subset U. In the worst case, this
may result in an exponential blow-up of the source
grammar. In the case of context-free grammars, this
is usually circumvented by casting the rules in bi-
nary form prior to epsilon rule elimination. How-
ever, this is not possible in our case, since SCFGs
do not allow normal forms with a constant bound
on the length of the right-hand side of each compo-
nent. This follows from a result due to Aho and Ull-
man (1969) for a formalism called syntax directed
translation schemata, which is a syntactic variant of
SCFGs.
An additional complication with our construction
is that finding any of the values in (3) may involve
solving a system of non-linear equations, similarly
to the case of probabilistic context-free grammars;
see again Abney et al. (1999), and Stolcke (1995).
Approximate solution of such systems might take
exponential time, as pointed out by Kiefer et al.
(2007).
Notwithstanding the worst cases mentioned
above, there is a special case that can be easily dealt
with. Assume that, for each nullable pair [A1, A2] in
G we have that [A11 , A21 ] ⇒∗G [w1, w2] does not
hold for any w1 and w2 with w1 =6 ε or w2 =6 ε.
Then each of the values in (3) is guaranteed to be 1,
and furthermore we can remove the instances of the
nullable pairs in the source rule s all at the same
time. This means that the overall construction of
</bodyText>
<equation confidence="0.900233">
E pG(σ) (3)
σ s.t. [A 11 ,A21 ]⇒G[ε, ε]
</equation>
<page confidence="0.993749">
464
</page>
<bodyText confidence="0.999042846153846">
elimination of nullable rules from G can be imple-
mented in linear time |G|. It is this special case that
we will encounter in section 4.
After elimination of epsilon rules, one can elimi-
nate unit rules. We define Cunit([A1, A2], [B1, B2])
as the sum of the probabilities of all derivations de-
riving [B1, B2] from [A1, A2] with arbitrary indices,
or more precisely:
Note that [A1, A2] may be equal to [B1, B2] and σ
may be ε, in which case Cunit([A1, A2], [B1, B2]) is
at least 1, but it may be larger if there are unit rules.
Therefore Cunit([A1, A2], [B1, B2]) should not be
seen as a probability.
</bodyText>
<equation confidence="0.929206769230769">
Consider a pair [A1, A2] ∈ N[2] and let all unit
rules with left-hand sides A1 and A2 be:
s1 : [A1, A2] → [A ��
11 , A ��
21 ]
...
sm : [A1, A2] → [A��
1m , A��
2m ]
The values of Cunit(·, ·) are related by the following:
Cunit([A1, A2], [B1, B2]) = δ([A1, A2] = [B1, B2]) +
� pG(si) · Cunit([A1i, A2i], [B1, B2])
i
</equation>
<bodyText confidence="0.999949227272727">
where δ([A1, A2] = [B1, B2]) is defined to be 1 if
[A1, A2] = [B1, B2] and 0 otherwise. This forms a
system of linear equations in the unknown variables
Cunit(·, ·). Such a system can be solved in polyno-
mial time in the number of variables, for example
using Gaussian elimination.
The elimination of unit rules starts with adding
a rule s&apos; : [A1 → α1, A2 → α2] for each non-
unit rule s : [B1 → α1, B2 → α2] and pair
[A1, A2] such that Cunit([A1, A2], [B1, B2]) &gt; 0.
We assign to the new rule s&apos; the probability pG(s) ·
Cunit([A1, A2], [B1, B2]). The unit rules can now
be removed from the grammar. Again, in the re-
sulting grammar the translation and the associated
probability distribution will be the same as those in
the source grammar. The new grammar has size
O(|G|2), where G is the input grammar. The time
complexity is dominated by the computation of the
solution of the linear system of equations. This com-
putation takes cubic time in the number of variables.
The number of variables in this case is O(|G|2),
which makes the running time O(|G|6).
</bodyText>
<sectionHeader confidence="0.984432" genericHeader="method">
4 Prefix probabilities
</sectionHeader>
<bodyText confidence="0.9733998">
The joint prefix probability pprefix
G ([v1,v2]) of a
pair [v1, v2] of terminal strings is the sum of the
probabilities of all pairs of strings that have v1 and
v2, respectively, as their prefixes. Formally:
</bodyText>
<equation confidence="0.967161333333333">
pprefix
G ([v1, v2]) = � pG([v1w1, v2w2])
w1,w2EΣ*
</equation>
<bodyText confidence="0.992948304347826">
At first sight, it is not clear this quantity can be ef-
fectively computed, as it involves a sum over in-
finitely many choices of w1 and w2. However, anal-
ogously to the case of context-free prefix probabili-
ties (Jelinek and Lafferty, 1991), we can isolate two
parts in the computation. One part involves infinite
sums, which are independent of the input strings v1
and v2, and can be precomputed by solving a sys-
tem of linear equations. The second part does rely
on v1 and v2, and involves the actual evaluation of
pprefix
G ([v1, v2]). This second part can be realized
effectively, on the basis of the precomputed values
from the first part.
In order to keep the presentation simple, and
to allow for simple proofs of correctness, we
solve the problem in a modular fashion. First,
we present a transformation from a PSCFG
G = (G, pG), with G = (N, Σ, P, S), to a
PSCFG Gprefix = (Gprefix,pGprefix), with Gprefix =
(Nprefix, Σ, Pprefix, Si). The latter grammar derives
all possible pairs [v1, v2] such that [v1w1, v2w2] can
be derived from G, for some w1 and w2. Moreover,
</bodyText>
<equation confidence="0.7064685">
pGprefix([v1,v2]) = pprefix
G ([v1,v2]), as will be veri-
</equation>
<bodyText confidence="0.981117222222222">
fied later.
Computing pGprefix([v1, v2]) directly using a
generic probabilistic parsing algorithm for PSCFGs
is difficult, due to the presence of epsilon rules and
unit rules. The next step will be to transform Gprefix
into a third grammar G&apos;prefix by eliminating epsilon
rules and unit rules from the underlying SCFG,
and preserving the probability distribution over pairs
of strings. Using G&apos;prefix one can then effectively
</bodyText>
<equation confidence="0.879814">
� pG(σ)
σEP* s.t. �ItEN,
[A11 , A21 ]�°c[B1t , B2t ]
</equation>
<page confidence="0.99528">
465
</page>
<bodyText confidence="0.980327681818182">
apply generic probabilistic parsing algorithms for
PSCFGs, such as the inside algorithm discussed in
section 3, in order to compute the desired prefix
probabilities for the source PSCFG !9.
For each nonterminal A in the source SCFG G,
the grammar Gprefix contains three nonterminals,
namely A itself, A↓ and Aε. The meaning of A re-
mains unchanged, whereas A↓ is intended to gen-
erate a string that is a suffix of a known prefix v1 or
v2. Nonterminals Aε generate only the empty string,
and are used to simulate the generation by G of in-
fixes of the unknown suffix w1 or w2. The two left-
hand sides of a synchronous rule in Gprefix can con-
tain different combinations of nonterminals of the
forms A, A↓, or Aε. The start symbol of Gprefix is
S↓. The structure of the rules from the source gram-
mar is largely retained, except that some terminal
symbols are omitted in order to obtain the intended
interpretation of A↓ and Aε.
In more detail, let us consider a synchronous rule
s : [A1 —* α1, A2 —* α2] from the source gram-
mar, where for i E 11, 21 we have:
</bodyText>
<equation confidence="0.976708">
αi = ui0A ti1
i1 ui1 ··· uir−1A tir
ir uir
</equation>
<bodyText confidence="0.999515769230769">
The transformed grammar then contains a large
number of rules, each of which is of the form s0 :
[B1 —* 01, B2 —* 02], where Bi —* 0i is of
one of three forms, namely Ai —* αi, A↓i —* α↓i
or Aεi —* αεi, where α↓i and αεi are explained below.
The choices for i = 1 and for i = 2 are independent,
so that we can have 3 * 3 = 9 kinds of synchronous
rules, to be further subdivided in what follows. A
unique label s0 is produced for each new rule, and
the probability of each new rule equals that of s.
The right-hand side αεi is constructed by omitting
all terminals and propagating downwards the E su-
perscript, resulting in:
</bodyText>
<equation confidence="0.996369">
αε = Aε ti1 ··· Aε tir
i i1 ir
</equation>
<bodyText confidence="0.9990528">
It is more difficult to define α↓i . In fact, there can
be a number of choices for α↓i and, for each choice,
the transformed grammar contains an instance of the
synchronous rule s0 : [B1 —* 01, B2 —* 02] as de-
fined above. The reason why different choices need
to be considered is because the boundary between
the known prefix vi and the unknown suffix wi can
occur at different positions, either within a terminal
string uij or else further down in a subderivation in-
volving Aij. In the first case, we have for some j
</bodyText>
<equation confidence="0.991048333333333">
(0 &lt; j &lt; r):
α↓i = ui0A ti1
i1 ui1A ti2
i2 ···
uij−1A tij
ij u0 ijAε tij+1
ij+1 Aε tij+2
ij+2 · · · Aε tir
ir
</equation>
<bodyText confidence="0.999785571428571">
where u0ij is a choice of a prefix of uij. In words,
the known prefix ends after u0ij and, thereafter, no
more terminals are generated. We demand that u0ij
must not be the empty string, unless Ai = S and
j = 0. The reason for this restriction is that we want
to avoid an overlap with the second case. In this
second case, we have for some j (1 &lt; j &lt; r):
</bodyText>
<equation confidence="0.99163525">
α↓ i = ui0A ti1
i1 ui1A ti2
i2 ···
uij−1A↓ tij
ij Aε tij+1
ij+1 Aε tij+2
ij+2 · · · Aε tir
ir
</equation>
<bodyText confidence="0.951174142857143">
Here the known prefix of the input ends within a sub-
derivation involving Aij, and further to the right no
more terminals are generated.
Example 3 Consider the synchronous rule s :
[A —* aB 1 bc C 2 d, D —* ef E 2 F 1 ]. The first
component of a synchronous rule derived from this
can be one of the following eight:
</bodyText>
<equation confidence="0.999905125">
Aε —* Bε 1 Cε 2
A↓ —* aBε 1 Cε 2
A↓ —* aB↓ 1 Cε 2
A↓ —* aB 1 b Cε 2
A↓ —* aB 1 bc Cε 2
A↓ —* aB 1 bc C↓ 2
A↓ —* aB 1 bc C 2 d
A —* aB 1 bcC2d
</equation>
<bodyText confidence="0.7547715">
The second component can be one of the following
six:
</bodyText>
<equation confidence="0.992513333333333">
Dε —* Eε 2 Fε 1
D↓ —* eEε 2 Fε 1
D↓ —* ef Eε 2 Fε 1
D↓ —* ef E↓ 2 Fε 1
D↓ —* ef E 2 F↓ 1
D —* efE2F1
</equation>
<page confidence="0.99552">
466
</page>
<bodyText confidence="0.991287">
In total, the transformed grammar will contain 8 ∗
</bodyText>
<equation confidence="0.787605">
6 = 48 synchronous rules derived from s. ❑
</equation>
<bodyText confidence="0.982312023809524">
For each synchronous rule s, the above gram-
mar transformation produces O(|s|) left rule com-
ponents and as many right rule components. This
means the number of new synchronous rules is
O(|s|2), and the size of each such rule is O(|s|). If
we sum O(|s|3) for every rule s we obtain a time
and space complexity of O(|G|3).
We now investigate formal properties of our
grammar transformation, in order to relate it to pre-
fix probabilities. We define the relation ` between P
and Pprefix such that s ` s0 if and only if s0 was ob-
tained from s by the transformation described above.
This is extended in a natural way to derivations, such
that s1 ··· sd ` s01 · · · s0d0 if and only if d = d0 and
si ` s0i for each i (1 ≤ i ≤ d).
The formal relation between G and Gprefix is re-
vealed by the following two lemmas.
Lemma 1 For each v1, v2, w1, w2 ∈ Σ∗ and
σ ∈ P∗ such that [S, S] ⇒σG [v1w1, v2w2], there
is a unique σ0 ∈ P∗ prefix such that [S↓, S↓] ⇒σ0 Gprefix
[v1, v2] and σ ` σ0. ❑
Lemma 2 For each v1, v2 ∈ Σ∗ and derivation
σ0 ∈ P∗prefix such that [S↓, S↓] ⇒σ0Gprefix [v1, v2],
there is a unique σ ∈ P∗ and unique w1, w2 ∈ Σ∗
such that [S, S] ⇒σG [v1w1, v2w2] and σ ` σ0. ❑
The only non-trivial issue in the proof of Lemma 1
is the uniqueness of σ0. This follows from the obser-
vation that the length of v1 in v1w1 uniquely deter-
mines how occurrences of left components of rules
in P found in σ are mapped to occurrences of left
components of rules in Pprefix found in σ0. The same
applies to the length of v2 in v2w2 and the right com-
ponents.
Lemma 2 is easy to prove as the structure of the
transformation ensures that the terminals that are in
rules from P but not in the corresponding rules from
Pprefix occur at the end of a string v1 (and v2) to form
the longer string v1w1 (and v2w2, respectively).
The transformation also ensures that s ` s0 im-
plies pG(s) = pGprefix(s0). Therefore σ ` σ0 implies
pG(σ) = pGprefix(σ0). By this and Lemmas 1 and 2
we may conclude:
</bodyText>
<equation confidence="0.8464275">
Theorem 1 pGprefix([v1, v2]) = pprefix
G ([v1, v2]). ❑
</equation>
<bodyText confidence="0.999968">
Because of the introduction of rules with left-hand
sides of the form Aε in both the left and right compo-
nents of synchronous rules, it is not straightforward
to do effective probabilistic parsing with the gram-
mar Gprefix. We can however apply the transforma-
tions from section 3 to eliminate epsilon rules and
thereafter eliminate unit rules, in a way that leaves
the derived string pairs and their probabilities un-
changed.
The simplest case is when the source grammar G
is reduced, proper and consistent, and has no epsilon
rules. The only nullable pairs of nonterminals in
Gprefix will then be of the form [Aε1, Aε2]. Consider
such a pair [Aε1, Aε2]. Because of reduction, proper-
ness and consistency of G we have:
</bodyText>
<equation confidence="0.936297">
pG(σ) = 1
w1,w2∈Σ∗, σ∈P∗ s.t.
[A11 , A2 1 ]⇒σG[w1, w2]
</equation>
<bodyText confidence="0.998397666666667">
Because of the structure of the grammar transforma-
tion by which Gprefix was obtained from G, we also
have:
</bodyText>
<equation confidence="0.989688">
E
σ∈P∗ s.t.
[Aε 1
1 , Aε 1
2 ]⇒σ Gprefix[ε, ε]
</equation>
<bodyText confidence="0.99998925">
Therefore pairs of occurrences of Aε1 and Aε2 with
the same index in synchronous rules of Gprefix
can be systematically removed without affecting the
probability of the resulting rule, as outlined in sec-
tion 3. Thereafter, unit rules can be removed to allow
parsing by the inside algorithm for PSCFGs.
Following the computational analyses for all of
the constructions presented in section 3, and for the
grammar transformation discussed in this section,
we can conclude that the running time of the pro-
posed algorithm for the computation of prefix prob-
abilities is dominated by the running time of the in-
side algorithm, which in the worst case is exponen-
tial in |G|. This result is not unexpected, as already
pointed out in the introduction, since the recogni-
tion problem for PSCFGs is NP-complete, as estab-
lished by Satta and Peserico (2005), and there is a
straightforward reduction from the recognition prob-
lem for PSCFGs to the problem of computing the
prefix probabilities for PSCFGs.
</bodyText>
<equation confidence="0.97431">
pGprefix(σ) = 1
</equation>
<page confidence="0.997052">
467
</page>
<bodyText confidence="0.999946428571429">
One should add that, in real world machine trans-
lation applications, it has been observed that recog-
nition (and computation of inside probabilities) for
SCFGs can typically be carried out in low-degree
polynomial time, and the worst cases mentioned
above are not observed with real data. Further dis-
cussion on this issue is due to Zhang et al. (2006).
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999065777777778">
We have shown that the computation of joint prefix
probabilities for PSCFGs can be reduced to the com-
putation of inside probabilities for the same model.
Our reduction relies on a novel grammar transfor-
mation, followed by elimination of epsilon rules and
unit rules.
Next to the joint prefix probability, we can also
consider the right prefix probability, which is de-
fined by:
</bodyText>
<equation confidence="0.920435">
�
pr�prefix
G ([v1, v2]) =
W
</equation>
<bodyText confidence="0.999854777777778">
In words, the entire left string is given, along with a
prefix of the right string, and the task is to sum the
probabilities of all string pairs for different suffixes
following the given right prefix. This can be com-
puted as a special case of the joint prefix probability.
Concretely, one can extend the input and the gram-
mar by introducing an end-of-sentence marker $.
Let G&apos; be the underlying SCFG grammar after the
extension. Then:
</bodyText>
<equation confidence="0.994283">
�G
prefix([v1,v2]) = pprefix
G� ([v1$,v2])
</equation>
<bodyText confidence="0.998292666666667">
Prefix probabilities and right prefix probabilities
for PSCFGs can be exploited to compute probability
distributions for the next word or part-of-speech in
left-to-right incremental translation of speech, or al-
ternatively as a predictive tool in applications of in-
teractive machine translation, of the kind described
by Foster et al. (2002). We provide some technical
details here, generalizing to PSCFGs the approach
by Jelinek and Lafferty (1991).
Let !9 = (G, pG) be a PSCFG, with Σ the alpha-
bet of terminal symbols. We are interested in the
probability that the next terminal in the target trans-
lation is a E Σ, after having processed a prefix v1 of
the source sentence and having produced a prefix v2
of the target translation. This can be computed as:
</bodyText>
<equation confidence="0.863701333333333">
prefix (
pGword (a  |[v1 , v2 ]) = pG prefix [v1,v2 a] )
pG ([v1,v2])
</equation>
<bodyText confidence="0.935652333333333">
Two considerations are relevant when applying
the above formula in practice. First, the computa-
tion of pprefix
</bodyText>
<equation confidence="0.882608666666667">
G ([v1, v2a]) need not be computed from
scratch if pprefix
G ([v1, v2]) has been computed al-
</equation>
<bodyText confidence="0.9258325">
ready. Because of the tabular nature of the inside al-
gorithm, one can extend the table for pprefix
</bodyText>
<equation confidence="0.933363">
G ([v1, v2])
</equation>
<bodyText confidence="0.86502">
by adding new entries to obtain the table for
</bodyText>
<equation confidence="0.647933">
prefix
pG ([v1, v2a]). The same holds for the compu-
tation of pprefix
G ([v1b, v2]).
Secondly, the computation of pprefix
G ([v1, v2a]) for
</equation>
<bodyText confidence="0.9958577">
all possible a E Σ may be impractical. However,
one may also compute the probability that the next
part-of-speech in the target translation is A. This can
be realised by adding a rule s&apos; : [B —* b, A —* cA]
for each rule s : [B —* b, A —* a] from the source
grammar, where A is a nonterminal representing a
part-of-speech and cA is a (pre-)terminal specific to
A. The probability of s&apos; is the same as that of s. If
G&apos; is the underlying SCFG after adding such rules,
then the required value is pprefix
</bodyText>
<equation confidence="0.788363">
G� ([v1, v2 cA]).
</equation>
<bodyText confidence="0.9999864">
One variant of the definitions presented in this pa-
per is the notion of infix probability, which is use-
ful in island-driven speech translation. Here we are
interested in the probability that any string in the
source language with infix v1 is translated into any
string in the target language with infix v2. However,
just as infix probabilities are difficult to compute
for probabilistic context-free grammars (Corazza et
al., 1991; Nederhof and Satta, 2008) so (joint) infix
probabilities are difficult to compute for PSCFGs.
The problem lies in the possibility that a given in-
fix may occur more than once in a string in the lan-
guage. The computation of infix probabilities can
be reduced to that of solving non-linear systems of
equations, which can be approximated using for in-
stance Newton’s algorithm. However, such a system
of equations is built from the input strings, which en-
tails that the computational effort of solving the sys-
tem primarily affects parse time rather than parser-
generation time.
</bodyText>
<equation confidence="0.680885">
pG([v1, v2w])
</equation>
<page confidence="0.998275">
468
</page>
<sectionHeader confidence="0.99834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923386666666">
S. Abney, D. McAllester, and F. Pereira. 1999. Relating
probabilistic grammars and automata. In 37th Annual
Meeting of the Association for Computational Linguis-
tics, Proceedings of the Conference, pages 542–549,
Maryland, USA, June.
A.V. Aho and J.D. Ullman. 1969. Syntax directed trans-
lations and the pushdown assembler. Journal of Com-
puter and System Sciences, 3:37–56.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131–160.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
A. Corazza, R. De Mori, R. Gretter, and G. Satta.
1991. Computation of probabilities for an island-
driven parser. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 13(9):936–950.
G. Foster, P. Langlais, and G. Lapalme. 2002. User-
friendly text prediction for translators. In Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 148–155, University of Pennsylvania,
Philadelphia, PA, USA, July.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What’s in a translation rule? In HLT-NAACL 2004,
Proceedings of the Main Conference, Boston, Mas-
sachusetts, USA, May.
D. Gildea and D. Stefankovic. 2007. Worst-case syn-
chronous grammar rules. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Proceedings of the Main Conference, pages 147–
154, Rochester, New York, USA, April.
M. Hopkins and G. Langmead. 2010. SCFG decod-
ing without binarization. In Conference on Empirical
Methods in Natural Language Processing, Proceed-
ings of the Conference, pages 646–655, October.
F. Jelinek and J.D. Lafferty. 1991. Computation of the
probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315–323.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the
convergence of Newton’s method for monotone sys-
tems of polynomial equations. In Proceedings of the
39th ACM Symposium on Theory of Computing, pages
217–266.
C.D. Manning and H. Sch¨utze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
D. McAllester. 2002. On the complexity analysis of
static analyses. Journal of the ACM, 49(4):512–537.
M.-J. Nederhof and G. Satta. 2008. Computing parti-
tion functions of PCFGs. Research on Language and
Computation, 6(2):139–162.
G. Satta and E. Peserico. 2005. Some computational
complexity results for synchronous context-free gram-
mars. In Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 803–810.
S.M. Shieber, Y. Schabes, and F.C.N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24:3–36.
S. Sippu and E. Soisalon-Soininen. 1988. Parsing
Theory, Vol. I: Languages and Parsing, volume 15
of EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
A. Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities.
Computational Linguistics, 21(2):167–201.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–404.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Confer-
ence, pages 256–263, New York, USA, June.
</reference>
<page confidence="0.999626">
469
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310128">
<title confidence="0.9997435">Prefix Probability for Probabilistic Synchronous Context-Free Grammars</title>
<author confidence="0.978945">Mark-Jan</author>
<affiliation confidence="0.940095333333333">School of Computer University of St North Haugh, St Andrews,</affiliation>
<address confidence="0.79534">KY16</address>
<affiliation confidence="0.560486">United</affiliation>
<email confidence="0.991386">markjan.nederhof@googlemail.com</email>
<author confidence="0.989185">Giorgio</author>
<affiliation confidence="0.983119666666667">Dept. of Information University of via Gradenigo,</affiliation>
<address confidence="0.924756">I-35131</address>
<email confidence="0.997327">satta@dei.unipd.it</email>
<abstract confidence="0.9832425">We present a method for the computation of prefix probabilities for synchronous contextfree grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>D McAllester</author>
<author>F Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>542--549</pages>
<location>Maryland, USA,</location>
<contexts>
<context position="16962" citStr="Abney et al., 1999" startWordPosition="3119" endWordPosition="3122">02j−1 for each j (1 ≤ j ≤ r). Consider again a synchronous rule s of the form in (2). We say s is an epsilon rule if r = 0 and u10 = u20 = E. We say s is a unit rule if r = 1 and u10 = u11 = u20 = u21 = E. Similarly to context-free grammars, absence of epsilon rules and unit rules guarantees that there are no cyclic dependencies between items and in this case the inside algorithm correctly computes pG([w1, w2]). Epsilon rules can be eliminated from PSCFGs by a grammar transformation that is very similar to the transformation eliminating epsilon rules from a probabilistic context-free grammar (Abney et al., 1999). This is sketched in what follows. We first compute the set of all nullable linked pairs of nonterminals of the underlying SCFG, that is, the set of all [A1, A2] ∈ N[2] such that [A11 , A21 ] ⇒∗G [ε, ε]. This can be done in linear time O(|G|) using essentially the same algorithm that identifies nullable nonterminals in a context-free grammar, as presented for instance by Sippu and Soisalon-Soininen (1988). Next, we identify all occurrences of nullable pairs [A1, A2] in the right-hand side components of a rule s, such that A1 and A2 have the same index. For every possible choice of a subset U </context>
<context position="19054" citStr="Abney et al. (1999)" startWordPosition="3495" endWordPosition="3498"> casting the rules in binary form prior to epsilon rule elimination. However, this is not possible in our case, since SCFGs do not allow normal forms with a constant bound on the length of the right-hand side of each component. This follows from a result due to Aho and Ullman (1969) for a formalism called syntax directed translation schemata, which is a syntactic variant of SCFGs. An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al. (1999), and Stolcke (1995). Approximate solution of such systems might take exponential time, as pointed out by Kiefer et al. (2007). Notwithstanding the worst cases mentioned above, there is a special case that can be easily dealt with. Assume that, for each nullable pair [A1, A2] in G we have that [A11 , A21 ] ⇒∗G [w1, w2] does not hold for any w1 and w2 with w1 =6 ε or w2 =6 ε. Then each of the values in (3) is guaranteed to be 1, and furthermore we can remove the instances of the nullable pairs in the source rule s all at the same time. This means that the overall construction of E pG(σ) (3) σ s</context>
</contexts>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>S. Abney, D. McAllester, and F. Pereira. 1999. Relating probabilistic grammars and automata. In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 542–549, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>Syntax directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>3--37</pages>
<contexts>
<context position="18718" citStr="Aho and Ullman (1969)" startWordPosition="3442" endWordPosition="3446">rammar will be the same as those in the source grammar. One problem with the above construction is that we have to create new synchronous rules sU for each possible choice of subset U. In the worst case, this may result in an exponential blow-up of the source grammar. In the case of context-free grammars, this is usually circumvented by casting the rules in binary form prior to epsilon rule elimination. However, this is not possible in our case, since SCFGs do not allow normal forms with a constant bound on the length of the right-hand side of each component. This follows from a result due to Aho and Ullman (1969) for a formalism called syntax directed translation schemata, which is a syntactic variant of SCFGs. An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al. (1999), and Stolcke (1995). Approximate solution of such systems might take exponential time, as pointed out by Kiefer et al. (2007). Notwithstanding the worst cases mentioned above, there is a special case that can be easily dealt with. Assume that, for each nullable p</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>A.V. Aho and J.D. Ullman. 1969. Syntax directed translations and the pushdown assembler. Journal of Computer and System Sciences, 3:37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="1747" citStr="Chi (1999)" startWordPosition="254" endWordPosition="255">veral syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. This paper considers a parsing problem that is well understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input. This quantity is defined as the (possibly infinite) sum of the probabilities of all strings of the for</context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Z. Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1328" citStr="Chiang (2007)" startWordPosition="189" endWordPosition="190">owing interest in so-called syntaxbased translation models, that is, models that define mappings between languages through hierarchical sentence structures. Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction. Probabilistic synchronous context-free grammars (PSCFGs) are one among the most popular examples of such models. PSCFGs subsume several syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. This paper considers a parsing problem that is well understood for probabilistic context-free grammars but th</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corazza</author>
<author>R De Mori</author>
<author>R Gretter</author>
<author>G Satta</author>
</authors>
<title>Computation of probabilities for an islanddriven parser.</title>
<date>1991</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>13</volume>
<issue>9</issue>
<marker>Corazza, De Mori, Gretter, Satta, 1991</marker>
<rawString>A. Corazza, R. De Mori, R. Gretter, and G. Satta. 1991. Computation of probabilities for an islanddriven parser. IEEE Transactions on Pattern Analysis and Machine Intelligence, 13(9):936–950.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Foster</author>
<author>P Langlais</author>
<author>G Lapalme</author>
</authors>
<title>Userfriendly text prediction for translators.</title>
<date>2002</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>148--155</pages>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA, USA,</location>
<contexts>
<context position="32624" citStr="Foster et al. (2002)" startWordPosition="6093" endWordPosition="6096">x. This can be computed as a special case of the joint prefix probability. Concretely, one can extend the input and the grammar by introducing an end-of-sentence marker $. Let G&apos; be the underlying SCFG grammar after the extension. Then: �G prefix([v1,v2]) = pprefix G� ([v1$,v2]) Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation of speech, or alternatively as a predictive tool in applications of interactive machine translation, of the kind described by Foster et al. (2002). We provide some technical details here, generalizing to PSCFGs the approach by Jelinek and Lafferty (1991). Let !9 = (G, pG) be a PSCFG, with Σ the alphabet of terminal symbols. We are interested in the probability that the next terminal in the target translation is a E Σ, after having processed a prefix v1 of the source sentence and having produced a prefix v2 of the target translation. This can be computed as: prefix ( pGword (a |[v1 , v2 ]) = pG prefix [v1,v2 a] ) pG ([v1,v2]) Two considerations are relevant when applying the above formula in practice. First, the computation of pprefix G </context>
</contexts>
<marker>Foster, Langlais, Lapalme, 2002</marker>
<rawString>G. Foster, P. Langlais, and G. Lapalme. 2002. Userfriendly text prediction for translators. In Conference on Empirical Methods in Natural Language Processing, pages 148–155, University of Pennsylvania, Philadelphia, PA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004, Proceedings of the Main Conference,</booktitle>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="1406" citStr="Galley et al. (2004)" startWordPosition="201" endWordPosition="204">dels that define mappings between languages through hierarchical sentence structures. Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction. Probabilistic synchronous context-free grammars (PSCFGs) are one among the most popular examples of such models. PSCFGs subsume several syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. This paper considers a parsing problem that is well understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. What’s in a translation rule? In HLT-NAACL 2004, Proceedings of the Main Conference, Boston, Massachusetts, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Stefankovic</author>
</authors>
<title>Worst-case synchronous grammar rules.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference,</booktitle>
<pages>147--154</pages>
<location>Rochester, New York, USA,</location>
<contexts>
<context position="14014" citStr="Gildea and Stefankovic (2007)" startWordPosition="2582" endWordPosition="2585">r each component. By standard complexity analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1|rmax+1 ·|w2|rmax+1), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items, as instantiations of the consequent and the antecedents, respectively, of the inference rule in figure 1. A probabilistic parsing algorithm that computes PG([w1, w2]), defined in (1), can also be obtained from the recognition algorithm above, by associating each item with a probability. To explain the basic idea, let us first assume that each item </context>
</contexts>
<marker>Gildea, Stefankovic, 2007</marker>
<rawString>D. Gildea and D. Stefankovic. 2007. Worst-case synchronous grammar rules. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference, pages 147– 154, Rochester, New York, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hopkins</author>
<author>G Langmead</author>
</authors>
<title>SCFG decoding without binarization.</title>
<date>2010</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference,</booktitle>
<pages>646--655</pages>
<contexts>
<context position="14046" citStr="Hopkins and Langmead (2010)" startWordPosition="2587" endWordPosition="2590">lexity analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1|rmax+1 ·|w2|rmax+1), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items, as instantiations of the consequent and the antecedents, respectively, of the inference rule in figure 1. A probabilistic parsing algorithm that computes PG([w1, w2]), defined in (1), can also be obtained from the recognition algorithm above, by associating each item with a probability. To explain the basic idea, let us first assume that each item can be inferred in finitely many</context>
</contexts>
<marker>Hopkins, Langmead, 2010</marker>
<rawString>M. Hopkins and G. Langmead. 2010. SCFG decoding without binarization. In Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, pages 646–655, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="2462" citStr="Jelinek and Lafferty (1991)" startWordPosition="375" endWordPosition="378">ders a parsing problem that is well understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input. This quantity is defined as the (possibly infinite) sum of the probabilities of all strings of the form vw, for any string w over the alphabet of the model. This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech. This has applications in incremental processing of text or speech from left to right; see again (Jelinek and Lafferty, 1991). Prefix probabilities can also be exploited in speech understanding systems to score partial hypotheses in beam search (Corazza et al., 1991). This paper investigates the problem of computing prefix probabilities for PSCFGs. In this context, a pair of strings v1 and v2 is given as input, and we are asked to compute the probability that any st</context>
<context position="4059" citStr="Jelinek and Lafferty (1991)" startWordPosition="627" endWordPosition="630">fined as the probability that some (complete) input string w in the source language is translated into a string in the target language starting with an input prefix v. 460 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 460–469, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation, essentially in the same way as described by Jelinek and Lafferty (1991) for probabilistic context-free grammars, as discussed later in this paper. Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. In this paper we reduce the computation of prefix probabilities for PSCFGs to the computation of inside probabilities under the same model. Computation of inside probabilities for PSCFGs is a well-known problem that can be solved using offthe-shelf algorithms that extend basic parsing algorithms. Our reductio</context>
<context position="22171" citStr="Jelinek and Lafferty, 1991" startWordPosition="4095" endWordPosition="4098">number of variables. The number of variables in this case is O(|G|2), which makes the running time O(|G|6). 4 Prefix probabilities The joint prefix probability pprefix G ([v1,v2]) of a pair [v1, v2] of terminal strings is the sum of the probabilities of all pairs of strings that have v1 and v2, respectively, as their prefixes. Formally: pprefix G ([v1, v2]) = � pG([v1w1, v2w2]) w1,w2EΣ* At first sight, it is not clear this quantity can be effectively computed, as it involves a sum over infinitely many choices of w1 and w2. However, analogously to the case of context-free prefix probabilities (Jelinek and Lafferty, 1991), we can isolate two parts in the computation. One part involves infinite sums, which are independent of the input strings v1 and v2, and can be precomputed by solving a system of linear equations. The second part does rely on v1 and v2, and involves the actual evaluation of pprefix G ([v1, v2]). This second part can be realized effectively, on the basis of the precomputed values from the first part. In order to keep the presentation simple, and to allow for simple proofs of correctness, we solve the problem in a modular fashion. First, we present a transformation from a PSCFG G = (G, pG), wit</context>
<context position="32732" citStr="Jelinek and Lafferty (1991)" startWordPosition="6109" endWordPosition="6112"> the input and the grammar by introducing an end-of-sentence marker $. Let G&apos; be the underlying SCFG grammar after the extension. Then: �G prefix([v1,v2]) = pprefix G� ([v1$,v2]) Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation of speech, or alternatively as a predictive tool in applications of interactive machine translation, of the kind described by Foster et al. (2002). We provide some technical details here, generalizing to PSCFGs the approach by Jelinek and Lafferty (1991). Let !9 = (G, pG) be a PSCFG, with Σ the alphabet of terminal symbols. We are interested in the probability that the next terminal in the target translation is a E Σ, after having processed a prefix v1 of the source sentence and having produced a prefix v2 of the target translation. This can be computed as: prefix ( pGword (a |[v1 , v2 ]) = pG prefix [v1,v2 a] ) pG ([v1,v2]) Two considerations are relevant when applying the above formula in practice. First, the computation of pprefix G ([v1, v2a]) need not be computed from scratch if pprefix G ([v1, v2]) has been computed already. Because of </context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>F. Jelinek and J.D. Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kiefer</author>
<author>M Luttenberger</author>
<author>J Esparza</author>
</authors>
<title>On the convergence of Newton’s method for monotone systems of polynomial equations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 39th ACM Symposium on Theory of Computing,</booktitle>
<pages>217--266</pages>
<contexts>
<context position="19180" citStr="Kiefer et al. (2007)" startWordPosition="3515" endWordPosition="3518">o not allow normal forms with a constant bound on the length of the right-hand side of each component. This follows from a result due to Aho and Ullman (1969) for a formalism called syntax directed translation schemata, which is a syntactic variant of SCFGs. An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al. (1999), and Stolcke (1995). Approximate solution of such systems might take exponential time, as pointed out by Kiefer et al. (2007). Notwithstanding the worst cases mentioned above, there is a special case that can be easily dealt with. Assume that, for each nullable pair [A1, A2] in G we have that [A11 , A21 ] ⇒∗G [w1, w2] does not hold for any w1 and w2 with w1 =6 ε or w2 =6 ε. Then each of the values in (3) is guaranteed to be 1, and furthermore we can remove the instances of the nullable pairs in the source rule s all at the same time. This means that the overall construction of E pG(σ) (3) σ s.t. [A 11 ,A21 ]⇒G[ε, ε] 464 elimination of nullable rules from G can be implemented in linear time |G|. It is this special ca</context>
</contexts>
<marker>Kiefer, Luttenberger, Esparza, 2007</marker>
<rawString>S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the convergence of Newton’s method for monotone systems of polynomial equations. In Proceedings of the 39th ACM Symposium on Theory of Computing, pages 217–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C.D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McAllester</author>
</authors>
<title>On the complexity analysis of static analyses.</title>
<date>2002</date>
<journal>Journal of the ACM,</journal>
<volume>49</volume>
<issue>4</issue>
<contexts>
<context position="13496" citStr="McAllester (2002)" startWordPosition="2504" endWordPosition="2505">i has index ti and A2π−1(i) has index tip with i0 = 7r(7r−1(i)) = i. Thus the nonterminals in each antecedent item in figure 1 form a linked pair. We now turn to a computational analysis of the above algorithm. In the inference rule in figure 1 there are 2(r + 1) variables that can be bound to positions in w1, and as many that can be bound to positions in w2. However, the side conditions imply m0ij = mij + |uij|, for i E 11, 21 and 0 &lt; j &lt; r, and therefore the number of free variables is only r + 1 for each component. By standard complexity analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1|rmax+1 ·|w2|rmax+1), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. Th</context>
</contexts>
<marker>McAllester, 2002</marker>
<rawString>D. McAllester. 2002. On the complexity analysis of static analyses. Journal of the ACM, 49(4):512–537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Computing partition functions of PCFGs.</title>
<date>2008</date>
<journal>Research on Language and Computation,</journal>
<volume>6</volume>
<issue>2</issue>
<marker>Nederhof, Satta, 2008</marker>
<rawString>M.-J. Nederhof and G. Satta. 2008. Computing partition functions of PCFGs. Research on Language and Computation, 6(2):139–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
<author>E Peserico</author>
</authors>
<title>Some computational complexity results for synchronous context-free grammars.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>803--810</pages>
<contexts>
<context position="5470" citStr="Satta and Peserico (2005)" startWordPosition="843" endWordPosition="846">or the computation of inside probabilities for PSCFG. This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. Our method for computing the prefix probabilities for PSCFGs runs in exponential time, since that is the running time of existing methods for computing the inside probabilities for PSCFGs. It is unlikely this can be improved, because the recognition problem for PSCFG is NP-complete, as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs. 2 Definitions In this section we introduce basic definitions related to synchronous context-free grammars and their probabilistic extension; our notation follows Satta and Peserico (2005). Let N and E be sets of nonterminal and terminal symbols, respectively. In what follows we need to represent bijections between the occurrences of nonterminals in two strings over N UE. This is realized by annotating nonterminals with indices from an infinite set.</context>
<context position="13974" citStr="Satta and Peserico (2005)" startWordPosition="2576" endWordPosition="2579">r of free variables is only r + 1 for each component. By standard complexity analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1|rmax+1 ·|w2|rmax+1), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items, as instantiations of the consequent and the antecedents, respectively, of the inference rule in figure 1. A probabilistic parsing algorithm that computes PG([w1, w2]), defined in (1), can also be obtained from the recognition algorithm above, by associating each item with a probability. To explain the basic i</context>
<context position="30866" citStr="Satta and Peserico (2005)" startWordPosition="5802" endWordPosition="5805">fter, unit rules can be removed to allow parsing by the inside algorithm for PSCFGs. Following the computational analyses for all of the constructions presented in section 3, and for the grammar transformation discussed in this section, we can conclude that the running time of the proposed algorithm for the computation of prefix probabilities is dominated by the running time of the inside algorithm, which in the worst case is exponential in |G|. This result is not unexpected, as already pointed out in the introduction, since the recognition problem for PSCFGs is NP-complete, as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs. pGprefix(σ) = 1 467 One should add that, in real world machine translation applications, it has been observed that recognition (and computation of inside probabilities) for SCFGs can typically be carried out in low-degree polynomial time, and the worst cases mentioned above are not observed with real data. Further discussion on this issue is due to Zhang et al. (2006). 5 Discussion We have shown that the computation of joint prefix probabilities fo</context>
</contexts>
<marker>Satta, Peserico, 2005</marker>
<rawString>G. Satta and E. Peserico. 2005. Some computational complexity results for synchronous context-free grammars. In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 803–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
<author>Y Schabes</author>
<author>F C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--3</pages>
<contexts>
<context position="12254" citStr="Shieber et al. (1995)" startWordPosition="2252" endWordPosition="2255"> proper and consistent, then also: for every pair [A1, A2] E N[2]. The proof is identical to that of the corresponding fact for probabilistic context-free grammars. 3 Effective PSCFG parsing If w = a1 · · · an then the expression w[i, j], with 0 &lt; i &lt; j &lt; n, denotes the substring ai+1 · · · aj (if i = j then w[i, j] = E). In this section, we assume the input is the pair [w1, w2] of terminal strings. The task of a recognizer for SCFG G is to decide whether [w1,w2] E T(G). We present a general algorithm for solving the above problem in terms of the specification of a deduction system, following Shieber et al. (1995). The items that are constructed by the system have the form [m1, A1, m01; m2, A2, m02], where [A1, A2] E N[2] and where m1, m01, m2, m02 are non-negative integers such that 0 &lt; m1 &lt; m01 &lt; |w1 |and 0 &lt; m2 &lt; m02 &lt; |w2|. Such an item can be derived by the deduction system if and only if: [A 1 1 , A21 ] ==�,∗G [w1[m1, m01], w2[m2, m02]] The deduction system has one inference rule, shown in figure 1. One of its side conditions has a synchronous rule in P of the form: Observe that, in the right-hand side of the two rule components above, nonterminals A1i and A2π−1(i), 1 &lt; i &lt; r, have both the same </context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>S.M. Shieber, Y. Schabes, and F.C.N. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24:3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sippu</author>
<author>E Soisalon-Soininen</author>
</authors>
<title>Parsing Theory, Vol. I: Languages and Parsing,</title>
<date>1988</date>
<journal>of EATCS Monographs on Theoretical Computer Science.</journal>
<volume>15</volume>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="17371" citStr="Sippu and Soisalon-Soininen (1988)" startWordPosition="3193" endWordPosition="3196">omputes pG([w1, w2]). Epsilon rules can be eliminated from PSCFGs by a grammar transformation that is very similar to the transformation eliminating epsilon rules from a probabilistic context-free grammar (Abney et al., 1999). This is sketched in what follows. We first compute the set of all nullable linked pairs of nonterminals of the underlying SCFG, that is, the set of all [A1, A2] ∈ N[2] such that [A11 , A21 ] ⇒∗G [ε, ε]. This can be done in linear time O(|G|) using essentially the same algorithm that identifies nullable nonterminals in a context-free grammar, as presented for instance by Sippu and Soisalon-Soininen (1988). Next, we identify all occurrences of nullable pairs [A1, A2] in the right-hand side components of a rule s, such that A1 and A2 have the same index. For every possible choice of a subset U of these occurrences, we add to our grammar a new rule sU constructed by omitting all of the nullable occurrences in U. The probability of sU is computed as the probability of s multiplied by terms of the form: for every pair [A1, A2] in U. After adding these extra rules, which in effect circumvents the use of epsilongenerating subderivations, we can safely remove all epsilon rules, with the only exception</context>
</contexts>
<marker>Sippu, Soisalon-Soininen, 1988</marker>
<rawString>S. Sippu and E. Soisalon-Soininen. 1988. Parsing Theory, Vol. I: Languages and Parsing, volume 15 of EATCS Monographs on Theoretical Computer Science. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2484" citStr="Stolcke (1995)" startWordPosition="381" endWordPosition="382"> understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input. This quantity is defined as the (possibly infinite) sum of the probabilities of all strings of the form vw, for any string w over the alphabet of the model. This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech. This has applications in incremental processing of text or speech from left to right; see again (Jelinek and Lafferty, 1991). Prefix probabilities can also be exploited in speech understanding systems to score partial hypotheses in beam search (Corazza et al., 1991). This paper investigates the problem of computing prefix probabilities for PSCFGs. In this context, a pair of strings v1 and v2 is given as input, and we are asked to compute the probability that any string in the source lan</context>
<context position="4307" citStr="Stolcke (1995)" startWordPosition="666" endWordPosition="667">pages 460–469, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation, essentially in the same way as described by Jelinek and Lafferty (1991) for probabilistic context-free grammars, as discussed later in this paper. Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. In this paper we reduce the computation of prefix probabilities for PSCFGs to the computation of inside probabilities under the same model. Computation of inside probabilities for PSCFGs is a well-known problem that can be solved using offthe-shelf algorithms that extend basic parsing algorithms. Our reduction is a novel grammar transformation, and the proof of correctness proceeds by fairly conventional techniques from formal language theory, relying on the correctness of standard methods for the computation of inside probabilities for PSCFG. This con</context>
<context position="19074" citStr="Stolcke (1995)" startWordPosition="3500" endWordPosition="3501">ary form prior to epsilon rule elimination. However, this is not possible in our case, since SCFGs do not allow normal forms with a constant bound on the length of the right-hand side of each component. This follows from a result due to Aho and Ullman (1969) for a formalism called syntax directed translation schemata, which is a syntactic variant of SCFGs. An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al. (1999), and Stolcke (1995). Approximate solution of such systems might take exponential time, as pointed out by Kiefer et al. (2007). Notwithstanding the worst cases mentioned above, there is a special case that can be easily dealt with. Assume that, for each nullable pair [A1, A2] in G we have that [A11 , A21 ] ⇒∗G [w1, w2] does not hold for any w1 and w2 with w1 =6 ε or w2 =6 ε. Then each of the values in (3) is guaranteed to be 1, and furthermore we can remove the instances of the nullable pairs in the source rule s all at the same time. This means that the overall construction of E pG(σ) (3) σ s.t. [A 11 ,A21 ]⇒G[ε</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):167–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1263" citStr="Wu (1997)" startWordPosition="178" endWordPosition="179"> area of statistical machine translation, there has been a growing interest in so-called syntaxbased translation models, that is, models that define mappings between languages through hierarchical sentence structures. Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction. Probabilistic synchronous context-free grammars (PSCFGs) are one among the most popular examples of such models. PSCFGs subsume several syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. This paper considers a parsing problem that </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>256--263</pages>
<location>New York, USA,</location>
<contexts>
<context position="31384" citStr="Zhang et al. (2006)" startWordPosition="5888" endWordPosition="5891"> since the recognition problem for PSCFGs is NP-complete, as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs. pGprefix(σ) = 1 467 One should add that, in real world machine translation applications, it has been observed that recognition (and computation of inside probabilities) for SCFGs can typically be carried out in low-degree polynomial time, and the worst cases mentioned above are not observed with real data. Further discussion on this issue is due to Zhang et al. (2006). 5 Discussion We have shown that the computation of joint prefix probabilities for PSCFGs can be reduced to the computation of inside probabilities for the same model. Our reduction relies on a novel grammar transformation, followed by elimination of epsilon rules and unit rules. Next to the joint prefix probability, we can also consider the right prefix probability, which is defined by: � pr�prefix G ([v1, v2]) = W In words, the entire left string is given, along with a prefix of the right string, and the task is to sum the probabilities of all string pairs for different suffixes following t</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 256–263, New York, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>