<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000857">
<note confidence="0.712658">
Proceedings of EACL &apos;99
</note>
<title confidence="0.775701666666667">
Robust and Flexible Mixed-Initiative Dialogue
for Telephone Services
Relario Gil, José and Tapias, Daniel and Gancedo, Maria C.
</title>
<author confidence="0.690363">
Charfuelan, Marcela and Hernández, Luis A. Ll
</author>
<affiliation confidence="0.572572">
Speech Technology Group, Telefonica InvestigaciOn y Desarrollo, S.A.
C. Emilio Vargas, 6 28043 - Madrid (Spain)
Te1:34.1.549500. Fax:34.1.3367350. e-mail:jrelanioOgaps.ssr.upm.es
</affiliation>
<sectionHeader confidence="0.932174" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995695">
In this work, we present an experimental
analysis of a Dialogue System for the au-
tomatization of simple telephone services.
Starting from the evaluation of a preliminar
version of the system wei conclude the ne-
cessity to desing a robust and flexible system
suitable to have to have different dialogue
control strategies depending on the charac-
teristics of the user and the performance of
the speech recognition module. Experimen-
tal results following the PARADISE frame-
work show an important improvement both
in terms of task success and dialogue cost
for the proposed system.
</bodyText>
<sectionHeader confidence="0.997687" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999969583333333">
In this contribution we present some improve-
ments on the design of a Dialogue Management
System for the automatization of simple telephone
tasks in a PABX environment (automatic name
dialing, voice messaging, ... ). From the point
of view of its functionality, our system is a very
simple one because there is no need of advanced
Plan Recognition strategies or General Problem
Solving methods. However we think that even for
these kind of dialogue sytems there is still a long
way to demonstrate their usability in real situa-
tions by the &amp;quot;general public&amp;quot;.
In our work we will concentrate on systems
designed for the telephone line and for a wide
range of potential users. Therefore our evalua-
tions will be done taking into account different lev-
els of speech recognition performance and user be-
haviours. In particular we will propose and eval-
uate strategies directed to increase the robustness
against recognition errors and flexibility to deal
with a wide range of users. We will use the PAR-
ADISE evaluation framework (Walker et al., 1998)
to analyze both task success and agent dialogue
behaviour related to subjective user satisfaction.
</bodyText>
<page confidence="0.969645">
111 Dep. SSR ETSIT-UPM Spain
</page>
<sectionHeader confidence="0.9944355" genericHeader="method">
2 ROBUST AND FLEXIBLE
SYSTEM
</sectionHeader>
<bodyText confidence="0.998613571428571">
Following the classification of Dialogue Systems
proposed by Allen (Allen, 1997), our baseline dia-
logue system could be described as a system with
topic-based performance capabilities, adaptive
single task, a minimal pair clarification/correction
dialogue manager and fixed mixed-initiative.
One of the most important objectives of our di-
alogue manager has been the implementation of a
collaborative dialogue model. So the system has
to be able to understand all the user actions, in
whatever order they appear, and even if the focus
of the dialogue has been changed by the user. In
order to achieve this, we organize the information
in an information tree, controlled by a task knowl-
edge interpreter and we let the data to partici-
pate in driving the dialogue. However, to control
a mixed-initiative strategy we use three separate
sources of information: the user data, the world
knowledge embedded in the task structure and the
general dialogue acts.
Therefore, from this preliminar evaluation of
the system we found that in order to increase
its permormance two major points should be ad-
dressed: a) robustness against recognition and
parser errors, and b) more flexibility to be able
to deal with different user models. We designed
four complementary strategies to improve its per-
formance:
</bodyText>
<listItem confidence="0.917559333333333">
1. To estimate the performance of the speech recog-
nition module. This was done from a count on
the number of corrections during previous inter-
actions with the same user.
2. To classify each user as belonging to group A or B
that will be described later in the Experimental
Results section. This was done combining a nor-
malized average number of utterances per task
and the amount of information in each utterance,
especially at some particular dialogue points (for
example when answering to the question of our
previous example).
</listItem>
<page confidence="0.974779">
287
</page>
<bodyText confidence="0.9622504">
Proceedings of EACL &apos;99
3. To include a control module that from the re-
sults of steps 1 and 2 defines two different kinds
of control management allowing a flexible mixed-
initiative strategy: more user initiative for Group
A users and high recognition rates, and more
restictive strategies for Group 13 users and/or low
recognition performance.
All of these strategies have been included in our
system as it is depicted in Figure 1.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="evaluation">
3 EXPERIMENTAL RESULTS
</sectionHeader>
<bodyText confidence="0.995485035714286">
In order to test the improvements over our original
system (described in (Alvarez et al., 1996)) we de-
signed a simulated evaluation environment where
the performance of the Speech Recognition Mod-
ule (recognition rate) was artificially controlled.
A Wizard of Oz simulation environment was de-
signed to obtain different levels of recognition per-
formance for a vocabulary of 1170 words: 96.4%
word recognition rate for high performance and
80% for low performance. A pre-defined single
fixed mixed-initiative strategy was used in all the
cases.
We used an annotated data base composed of
50 dialogues with 50 different novice users and 6
different simple telephone tasks in each dialogue:
25 dialogues were simulated using 94.6% recogni-
tion rate and 25 with 80%. Performance results
were obtained using the PARADISE evaluation
framework (Walker et al., 1998), determining the
contributions of task success and dialogue cost to
user satisfaction. Therefore as task success mea-
sure me obtained the Kappa coefficient while dia-
logue cost measures were based on the number of
users turns. In this case it is important to point
out that as each tested dialogue is composed of a
set of six different tasks which have quantify differ-
ent number of turns, the number of turns for each
task was normalized to it&apos;s N(x) = score
</bodyText>
<table confidence="0.948963166666667">
cr
Both Group High ASR
Lo ASR Hi ASR Gr. A Gr. B
K 0.68 0.81 1 0.61
User Turn 7.3 5.4 4.2 6.9
Satisf 26.4 30.1 35.4 25.2
</table>
<tableCaption confidence="0.818158666666667">
Table 1: Shows means results for both group in low
and high ASR. And separately for each Group A and
B, only in high ASR situation
</tableCaption>
<bodyText confidence="0.999923175">
User satisfaction in Table 1 was obtained as a
cumulative satisfaction score for each dialogue by
summing the scores of a set of questions similar
to those proposed in (Walker et al., 1998). The
ANOVA for Kappa, the cost measure and user sat-
isfaction demostrated a significant effect of ASR
performance. As it could be predicted, we found
that in all cases a low recognition rate corresponds
to a dramatical decrease in the absolute number
of suscessfully completed tasks and an important
increase in the average number of utterances.
However we also found that in high ASR situ-
ation the task success measure of Kappa was sur-
prisingly low.
A closer inspection of the dialogues in Table 1
revealed that this low performance under high
ASR situations was due to the presence of two
groups of users. A first group, Group A, showed
a &amp;quot;fluent&amp;quot; interaction with the system, similar to
the one supposed by the mixed-initiative strategy
(for example, as an answer to the question of the
system &amp;quot;do you want to do any other task?&amp;quot;, these
users could answer something like &amp;quot;yes, I would
like to send a message to John Smith&amp;quot;). While
the other group of users, Group B, e)dbited a very
restrictive interaction with the system (for exam-
ple, a short answer &amp;quot;yes&amp;quot; for the same question).
As a conclusion of this first evaluation we found
that in order to increase the permormance of our
baseline system, two major points should be ad-
dressed: a) robustness against recognition and
parser errors, and b) more flexibility to be able
to deal with different user models.
Therefore we designed an adaptive strategy to
adapt our dialogue manager to Group A or B of
users and to High and Low ASR situations. The
adaptation was done based on linear discrimina-
tion, as it is ilustrated in Figure 2, using both the
average number of turns and recognition errors
from the two first tasks in each dialogue.
</bodyText>
<table confidence="0.997051">
Low ASR High ASR
Both Gr. Gr. A Gr. B
k 0.71 1 0.83
User Turn 7.2 5.3 6.1
Satisfaction 26.9 32.1 29.4
</table>
<tableCaption confidence="0.887768">
Table 2: Shows means results for each Group in high
ASR situations and for both in low ASR.
</tableCaption>
<bodyText confidence="0.991224285714286">
Table 2 shows mean results for each Group A
and B of users for High ASR performance, and
for all users in Low ASR situations. These results
show a more stable behaviour of the system, that
is, less difference in performance between users of
Group A and Group B and, although to a lower
extend, between high and low recognition rates.
</bodyText>
<sectionHeader confidence="0.999904" genericHeader="conclusions">
4 CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.99986925">
The main conclusion of the work is the necessity
to design adaptive dialogue management strate-
gies to make the system robust against recogniton
performance and different user behaviours.
</bodyText>
<page confidence="0.992747">
288
</page>
<bodyText confidence="0.345077">
Proceedings of EACL &apos;99
</bodyText>
<sectionHeader confidence="0.779871" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988312083333334">
James Allen. 1997. Tutorial: Dialogue Modeling.
uno, ACL/ERACL Workshop on Spoken Dia-
logue System, Madrid, Spain.
J. Alvarez, J. Caminero, C. Crespo, and
D. Tapias. 1996. The Natural Language Pro-
cessing Module for a Voice Asisted Operator at
Telefonica I+D. uno, ICSLP &apos;96, Philadelphia,
USA.
M. Walker, D. Litman, C. Kamm, and A. Abella.
1998. Evaluating spoken dialog agents with
PARADISE: Two case studies. uno, Computer
speech and language.
</reference>
<page confidence="0.997568">
289
</page>
<figureCaption confidence="0.811097">
Proceedings of EACL &apos;99
Figure I: Modules of Robust and Flexible Mixed-Iniciative Dialogue
</figureCaption>
<figure confidence="0.9932945">
0 5 10 15 20 % ERROR
RATE
</figure>
<figureCaption confidence="0.87753">
Figure 2: User clasification
</figureCaption>
<figure confidence="0.996981354166667">
BACKWARD USER INTENTIONS,
CO-REFERENCE PROCESSOR
PROCESSOR
PARSER
USERS GROUPS
SELECTOR
BEHAVIOUR USER
ACTS
DIALOG
GROUPS STRATEGY
SELECTOR
TASK ACTS
DIALOG INTERPRETER
ACTS
DIALOG ACTS
CORRECTION
DETECTOR
SEMANTIC
GATHERINGS
PROCESSOR
TRAKER
BASIC ACTS
BASIC ACTS
TASK
KNOWLEDGE
INTERPRETER
BEHAVIOUR
SYSTEM
DEFINED
OUTPUT GENERATOR
SYNTHESIS MODULE
&gt;( RECOGNIZER )
DEIXIS
• REQUEST-REPLY INFORMATION
• ACT ALIZATION OF
DIA OG&apos;S INFORMATION,
• REQUEST-REPLY DATA INFORMATION
• STORE DATA INFORMATION
TELEPHONE
APLICATION
ac
z
2 CO
12
10
8
6
4
</figure>
<page confidence="0.842427">
2
290
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291978">
<note confidence="0.699574">Proceedings of EACL &apos;99</note>
<title confidence="0.997948">Robust and Flexible Mixed-Initiative Dialogue for Telephone Services</title>
<author confidence="0.9437555">José Daniel Maria C Marcela Gil</author>
<author confidence="0.9437555">Luis A Ll Hernández</author>
<note confidence="0.615364">Speech Technology Group, Telefonica InvestigaciOn y Desarrollo, S.A. C. Emilio Vargas, 6 28043 - Madrid (Spain) Te1:34.1.549500. Fax:34.1.3367350. e-mail:jrelanioOgaps.ssr.upm.es</note>
<abstract confidence="0.999577066666667">In this work, we present an experimental analysis of a Dialogue System for the automatization of simple telephone services. Starting from the evaluation of a preliminar of the system conclude the necessity to desing a robust and flexible system suitable to have to have different dialogue control strategies depending on the characteristics of the user and the performance of the speech recognition module. Experimental results following the PARADISE framework show an important improvement both in terms of task success and dialogue cost for the proposed system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Tutorial: Dialogue Modeling. uno,</title>
<date>1997</date>
<booktitle>ACL/ERACL Workshop on Spoken Dialogue System,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="2254" citStr="Allen, 1997" startWordPosition="351" endWordPosition="352">users. Therefore our evaluations will be done taking into account different levels of speech recognition performance and user behaviours. In particular we will propose and evaluate strategies directed to increase the robustness against recognition errors and flexibility to deal with a wide range of users. We will use the PARADISE evaluation framework (Walker et al., 1998) to analyze both task success and agent dialogue behaviour related to subjective user satisfaction. 111 Dep. SSR ETSIT-UPM Spain 2 ROBUST AND FLEXIBLE SYSTEM Following the classification of Dialogue Systems proposed by Allen (Allen, 1997), our baseline dialogue system could be described as a system with topic-based performance capabilities, adaptive single task, a minimal pair clarification/correction dialogue manager and fixed mixed-initiative. One of the most important objectives of our dialogue manager has been the implementation of a collaborative dialogue model. So the system has to be able to understand all the user actions, in whatever order they appear, and even if the focus of the dialogue has been changed by the user. In order to achieve this, we organize the information in an information tree, controlled by a task k</context>
</contexts>
<marker>Allen, 1997</marker>
<rawString>James Allen. 1997. Tutorial: Dialogue Modeling. uno, ACL/ERACL Workshop on Spoken Dialogue System, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Alvarez</author>
<author>J Caminero</author>
<author>C Crespo</author>
<author>D Tapias</author>
</authors>
<title>The Natural Language Processing Module for a Voice Asisted Operator at</title>
<date>1996</date>
<booktitle>Telefonica I+D. uno, ICSLP &apos;96,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="4549" citStr="Alvarez et al., 1996" startWordPosition="726" endWordPosition="729">or example when answering to the question of our previous example). 287 Proceedings of EACL &apos;99 3. To include a control module that from the results of steps 1 and 2 defines two different kinds of control management allowing a flexible mixedinitiative strategy: more user initiative for Group A users and high recognition rates, and more restictive strategies for Group 13 users and/or low recognition performance. All of these strategies have been included in our system as it is depicted in Figure 1. 3 EXPERIMENTAL RESULTS In order to test the improvements over our original system (described in (Alvarez et al., 1996)) we designed a simulated evaluation environment where the performance of the Speech Recognition Module (recognition rate) was artificially controlled. A Wizard of Oz simulation environment was designed to obtain different levels of recognition performance for a vocabulary of 1170 words: 96.4% word recognition rate for high performance and 80% for low performance. A pre-defined single fixed mixed-initiative strategy was used in all the cases. We used an annotated data base composed of 50 dialogues with 50 different novice users and 6 different simple telephone tasks in each dialogue: 25 dialog</context>
</contexts>
<marker>Alvarez, Caminero, Crespo, Tapias, 1996</marker>
<rawString>J. Alvarez, J. Caminero, C. Crespo, and D. Tapias. 1996. The Natural Language Processing Module for a Voice Asisted Operator at Telefonica I+D. uno, ICSLP &apos;96, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>D Litman</author>
<author>C Kamm</author>
<author>A Abella</author>
</authors>
<title>Evaluating spoken dialog agents with PARADISE: Two case studies. uno, Computer speech and language.</title>
<date>1998</date>
<contexts>
<context position="2016" citStr="Walker et al., 1998" startWordPosition="314" endWordPosition="317">n for these kind of dialogue sytems there is still a long way to demonstrate their usability in real situations by the &amp;quot;general public&amp;quot;. In our work we will concentrate on systems designed for the telephone line and for a wide range of potential users. Therefore our evaluations will be done taking into account different levels of speech recognition performance and user behaviours. In particular we will propose and evaluate strategies directed to increase the robustness against recognition errors and flexibility to deal with a wide range of users. We will use the PARADISE evaluation framework (Walker et al., 1998) to analyze both task success and agent dialogue behaviour related to subjective user satisfaction. 111 Dep. SSR ETSIT-UPM Spain 2 ROBUST AND FLEXIBLE SYSTEM Following the classification of Dialogue Systems proposed by Allen (Allen, 1997), our baseline dialogue system could be described as a system with topic-based performance capabilities, adaptive single task, a minimal pair clarification/correction dialogue manager and fixed mixed-initiative. One of the most important objectives of our dialogue manager has been the implementation of a collaborative dialogue model. So the system has to be ab</context>
<context position="5309" citStr="Walker et al., 1998" startWordPosition="844" endWordPosition="847">controlled. A Wizard of Oz simulation environment was designed to obtain different levels of recognition performance for a vocabulary of 1170 words: 96.4% word recognition rate for high performance and 80% for low performance. A pre-defined single fixed mixed-initiative strategy was used in all the cases. We used an annotated data base composed of 50 dialogues with 50 different novice users and 6 different simple telephone tasks in each dialogue: 25 dialogues were simulated using 94.6% recognition rate and 25 with 80%. Performance results were obtained using the PARADISE evaluation framework (Walker et al., 1998), determining the contributions of task success and dialogue cost to user satisfaction. Therefore as task success measure me obtained the Kappa coefficient while dialogue cost measures were based on the number of users turns. In this case it is important to point out that as each tested dialogue is composed of a set of six different tasks which have quantify different number of turns, the number of turns for each task was normalized to it&apos;s N(x) = score cr Both Group High ASR Lo ASR Hi ASR Gr. A Gr. B K 0.68 0.81 1 0.61 User Turn 7.3 5.4 4.2 6.9 Satisf 26.4 30.1 35.4 25.2 Table 1: Shows means </context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1998</marker>
<rawString>M. Walker, D. Litman, C. Kamm, and A. Abella. 1998. Evaluating spoken dialog agents with PARADISE: Two case studies. uno, Computer speech and language.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>