<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.691167" genericHeader="abstract">
ABSTRACTS OF CURRENT LITERATURE
</sectionHeader>
<bodyText confidence="0.956687896551724">
Recent memoranda in computer and cognitive science related to natural language processing.
For copies of the technical reports listed below write to:
Memoranda Series
Computing Research Laboratory
Box 30001
New Mexico State University
Las Cruces, New Mexico 88003
USA
On Psychological Plausibility in Artificial
Intelligence
Fowler, R. H., Slator, B. M. &amp; Balogh, I.
CRL, MCCS-89-150
Neural-Net Implementation of Complex
Symbol-Processing in a Mental Model
Approach to Syllogistic Reasoning
Barnden, J.
CRL, MCCS-89-154
Belief, Metaphorically Speaking*
Barnden, J.
CRL, MCCS-89-155
The Artificial Intelligence literature is liberally laced with claims about
cognitive reality. Sometimes these are strong claims that cite empirical
psychological evidence; but more often these claims take the form of
weak appeals to &amp;quot;psychological plausibility.&amp;quot; To examine the nature and
scientific status of these claims, AI research is characterized along a
particular dimension, cast as the &amp;quot;psychological evidence line.&amp;quot; Then,
some ideas about theory in AI are examined, especially the thorny notion
of &amp;quot;models&amp;quot; in Al theories: what does it mean to use human intellect as a
model in an Al theory, or in an Al program? Then, as is so often the
case, further light is shed by an historical characterization, giving
evidence for a particular grouping of &amp;quot;camps&amp;quot; in Al, according to how
psychological evidence &amp;quot;matters&amp;quot; to them. These discussions set the
scene, finally, for an examination of the role (really, roles) that
psychological plausibility actually plays in Al; and this leads naturally
into a discussion, and some conclusions, about which of these roles are
appropriate, and which are not.
A neural net system called &amp;quot;Conposit&amp;quot; is described. Conposit performs
rule-based manipulation of very short-term, complex symbolic data
structures. This paper concentrates on a simulated version of Conposit
that embodies core aspects of Johnson-Laird&apos;s mental model theory of
syllogistic reasoning. This Conposit version is not intended to be a
psychological theory, but rather to act as a test and demonstration of the
power and flexibility of Conposit&apos;s unusual connectionist techniques for
encoding the structure of data.
The central claim of the paper concerns AT systems that attempt to
represent propositional attitudes in realistic situations, and particularly
in situations portrayed in natural language discourse. The claim is that
the system, to achieve a coherent, useful view of a situation, must often
ascribe, to outer agents, views of inner agents&apos; attitudes that are based on
rich explications in terms of commonsense metaphorical views of mind.
This elevates the emasculated metaphors based on notions of world,
situation, container, and so on that underlie propositional attitude
representation proposals to the status of explicitly used, rich metaphors.
A system can adopt different patterns of commonsense inference about
attitudes by choosing different metaphors. The current stage of
development of a detailed representation scheme based on the claim is
described. The scheme allows different metaphors to be used for the
explication of attitudes at different levels in a nested-attitude situation.
</bodyText>
<note confidence="0.874087142857143">
60 Computational Linguistics Volume 16, Number 1, March 1990
Abstracts of Current Literature
Constructing A Machine Tractable
Dictionary From Longman Dictionary of
Contemporary English
Guo. C.-M.
CRL, MCCS-89-156 Dissertation
</note>
<bodyText confidence="0.981182941176471">
It is the purpose of this research to design a machine-tractable
dictionary (henceforth MTD) from Longman Dictionary of
Contemporary English (henceforth LDOCE). The MTD is intended to
be a basic facility for a whole spectrum of natural language processing
tasks. The research adopts a compositional-reduction approach to obtain
a formalized set of definitions of sense entries in a nested predicate form,
where the predicates are a set of &amp;quot;seed sense.&amp;quot; The focus of this research
is on the derivation of these &amp;quot;seed senses&amp;quot; and their utilization in the
construction of the MTD.
The construction of the proposed MTD involves the following four
steps; including step 1: determine the &amp;quot;defining senses&amp;quot; of LDOCE, i.e.
those world senses that are used in the definition of the meaning of 2,137
&amp;quot;controlled words&amp;quot; of LDOCE; step 2: derive the &amp;quot;seed senses&amp;quot; of
LDOCE. The &amp;quot;seed senses&amp;quot; are a subset of the defining sense that are
sufficient to define senses of step 1. The seen senses are taken as a
natural set semantic primitives derived from LDOCE; step 3: hand-code
the initial knowledge base for the natural set of semantic primitives
derived from LDOCE; step 4: construct a MTD for the controlled words
and the rest of LDOCE words by means of bootstrapping process, a
process of knowledge acquisition from dictionary definition text.
Step 1 of the construction process has been completed. A total of
3,860 defining senses have been determined. Step 2 of the construction
process has also been completed. A total of 3,280 word senses are found
to be the seed senses of LDOCE. These seed senses are taken as a
natural set of semantic primitives derived from the dictionary. The
feasibility of Steps 3 and 4 of the coprocess have been demonstrated with
implemented examples. What remains to be accomplished is to complete
Steps 3 and 4 to build a full-sized MTD from LDOCE using as
guidelines the results of initial implementation of the two steps.
Selected Dissertation Abstracts
Compiled by: Susanne M. Humphrey
National Library of Medicine
Bethesda, MD 20209
Bob Krovetz
University of Massachusetts
Amherst, MA 01002
The following are citations selected by title and abstract as being related to computational linguistics or knowledge
representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the
Dissertation Abstracts International (DAI) database produced by University Microfilms International.
Included are the UM order number and year-month of entry into the database; author; university, degree, and, if
available, number of pages; title; DAI subject category chosen by the author of the dissertation; and abstract. References
are sorted first by DAI subject category and second by author. Citations denoted by an MAI reference do not yet have
abstracts in the database and refer to abstracts in the published Masters Abstracts International.
Unless otherwise specified, paper or microform copies of dissertations may be ordered from University Microfilms
International, Dissertation Copies, Post Office Box 1764, Ann Arbor, MI 48106; telephone for U.S. (except Michigan,
Hawaii, Alaska): 1-800-521-3042, for Canada: 1-800-268-6090. Price lists and other ordering and shipping information
are in the introduction to the published DAI. An alternate source for copies is sometiems provided at the end of the abstract.
The dissertation titles and abstracts contained here are published with permission of University Microfilms Interna-
tional, publishers of Dissertation Abstracts International (copyright by University Microfilms International), and may not
be reproduced without their prior permission.
Computational Linguistics Volume 16, Number 1, March 1990 61
</bodyText>
<subsectionHeader confidence="0.358851">
Abstracts of Current Literature
</subsectionHeader>
<figure confidence="0.3580497">
An Intelligent Simulation Generator with a
Natural Language Interface
DAI V50(04), SecA, pp1012
Ford, Donnie Ray
The University of Alabama Ph.D. 1988, 153
pages
Business Administration, Marketing.
Computer Science
University Microfilms International
ADG89-13853
</figure>
<bodyText confidence="0.997683671875">
With increasing frequency, scientists and engineers are turning to
computer-based modeling of systems they hope to understand, build, or
modify. Many tools have been produced to carry out the modeling
process itself, e.g. statistical and numerical analysis packages, simulation
languages, and the like. A number of studies have found simulation to be
one of the most effective methods in use today. However, there are
problems with using simulation. What is apparent from past research is
that simulation is a power tool and one that would be more widely used if
it were cheaper and easier to use.
What the decision maker needs is the ability to construct computer
simulation models in less time and with less resources. In order to
accomplish this, more intelligence needs to be added to programs that
construct simulation programs. This is where knowledge-based systems
techniques can contribute greatly In this research, a knowledge-based
systems approach to generating simulation code is suggested. Also, a
prototype system called the Intelligent Simulation Generator (ISG) is
built. This system uses knowledge of how simulation models are
formulated and knowledge about the SIMAN simulation language in
producing computer simulation models from a constrained, natural
language description.
The methodology is detailed, and two test scenarios are given to
substantiate the ability of the ISG. While this approach has proven
fruitful, there are some severe limitations that must be overcome to
advance the capabilities of the system. In essence, a better
understanding of model formulation is necessary, and then this
knowledge must be captured and incorporated into the ISG.
Syntactic Analysis of English with Respect
to Government-Binding Grammar
DAI V50(05), SecB, pp 2020
Correa, Nelson
Syracuse University Ph.D. 1988, 296 pages
Computer Science. Engineering, Electronics
and Electrical. Language, Linguistics
University Microfilms International
ADG89-14562
An attribute-grammar formulation of the Government-binding (GB)
theory of natural language is proposed in which attribute definitions
capture the substantive aspects of transformations and most component
subtheories in Government-binding. Attributed definitions are used in
place of transformational rewriting to formulate an interpretive Chain
rule, with similar empirical effects to the transformation move-$a$ in
the theory.
The principal claim associated with this thesis is that the attribute
grammar (AG) formalism is sufficient and of major interest for the
description of natural language. First, it is shown that under even a
trivial kind of attribution domains and functions, attribute grammars
have the expressive power of Type-0 grammars. Hence, the languages
that can be described by the kind of linguistic devices assumed in current
GB theory can also be described within the simpler and more uniform
framework of attribute grammar. Although attribute grammars have the
same generative power as Turing machines, they are of major linguistic
interest, in the sense that generation of a string automatically defines
relevant structural information.
The second and more substantive part of the defense of our claim
involves an AG specification of English, assuming the notions and
principles of the Government-binding theory. We provide, for each
component of the GB theory, its counterpart in the AG specification. Of
particular interest is the abandonment of transformations and the use
instead of the interpretive Chain rule for the definition of
trace-antecedent relations in derivation trees. We show that the various
linguistic constraints that movement structures are subject to, including
the Subjacency and Path Containment conditions, are easily embodied
in the formulation of the interpretive rule.
The feasibility of the use of attribute grammar for models of linguistic
</bodyText>
<page confidence="0.893976">
62 Computational Linguistics Volume 16, Number 1, March 1990
</page>
<subsectionHeader confidence="0.51057">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.999811375">
performance is shown by producing an extended LL(1) parser and
attribute evaluator that constitute a practical analysis procedure for a
subset of English. The analysis procedure is attribute-directed,
effectively combining LL(1) derivation steps with attribute evaluation.
The procedure is thus able to cope with the extreme ambiguity of the
underlying context-free syntax. The procedure by which the attribute
evaluator is derived from the grammer is described in detail. (Abstract
shortened with permission of author.)
</bodyText>
<table confidence="0.877789909090909">
Word Sense Disambiguation in Descriptive
Text Interpretation: A Dual-Route
Parsimonious Covering Model
DAI V50(04), SecB, pp 1550
Dasigi, Venugopala Rao
University of Maryland College Park Ph.D.
1988, 259 pages
Computer Science. Language, Linguistics.
Information Science
University Microfilms International
ADG89-1228I
</table>
<bodyText confidence="0.99995272972973">
There has recently been growing awareness that natural language
processing can be viewed as abductive inference. This research extends
parsimonious covering theory, a formal model of abductive inference
originally developed for diagnostic problem solving, to automate
descriptive text interpretation. A mapping is identified between the
concepts of natural language processing and the concepts of diagnostic
parsimonious covering theory. Both syntactic and semantic aspects of
language are addressed in an extended theory, and a dual-route
parsimonious covering algorithm is developed for word sense
disambiguation. In the space of irredundant covers, search is focused on
plausible syntactic and semantic covers. The two routes of covering are
integrated by attributing both syntactic and semantic facets to each
&amp;quot;open class&amp;quot; concept.
An experimental prototype has been developed to test these ideas in
the context of expert system interfaces. A natural language interface is
generated by extracting vocabulary from a domain-specific knowledge
base, augmenting it with domain-independent linguistic information,
and superimposing a parsimonious covering procedure on this
knowledge. The prototype has at least some ability to handle
ungrammatical sentences, to revise inferences in the wake of new
information, to perform cross-sentence inferences, etc. Using a different
application-specific knowledge base, the same procedure can generate an
interface for a different domain, opening the way to automated interface
generation for certain classes of expert systems.
This work is significant for two reasons. First, it uses a nondeductive
inference method for word sense disambiguation that exploits associative
linguistic knowledge. This approach contrasts sharply with others, where
knowledge has usually been laboriously encoded into pattern-action
rules. The present work strives to use nondeductive inference for both
syntactic and semantic processing, and to integrate these two aspects
cleanly. It differes from other research on abductive inference in
language in focusing on word sense disambiguation rather than
pragmatics. Second, this work is significant because it extends
parsimonious covering theory to an entirely new class of applications.
This suggests that some basically similar abductive inferences underlie
linguistic and diagnostic problem-solving activities, and it is hoped that
further research will enable their unification.
</bodyText>
<table confidence="0.84569">
Step-Logic: Reasoning Situated in Time
DAI V50(04), SecB, pp 1501
Elgot-Drapkin, Jennifer Jill
University of Maryland College Park Ph.D.
1988, 168 pages
Computer Science
University Microfilms International
</table>
<page confidence="0.833935">
ADG89-12283
</page>
<bodyText confidence="0.963402222222222">
The world in which a commonsense reasoning agent reasons, that is, the
everyday world, is continually changing. These changes occur as the
agent proceeds, and must be taken into account as the agent reasons. An
often overlooked, but extremely important, change that occurs is simply
the passage of time as the agent reasons.
A commonsense reasoner is frequently limited in the amount of time it
has to reason. Conclusions that may be logically (or otherwise) entailed
by the agent&apos;s information take time to be derived. But time spent in
Computational Linguistics Volume 16, Number 1, March 1990 63
</bodyText>
<subsectionHeader confidence="0.884327">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.999920565217392">
such derivations is concurrent with changes in the world. This limitation
must be recognized by the reasoner; that is, the agent should be able to
reason about its ongoing reasoning efforts themselves. To do this, the
agent&apos;s reasoning must be &amp;quot;situated&amp;quot; in a temporal environment.
The problem that I address is that of defining a formalism in which
the ongoing process of deduction itself is part of that very same
reasoning. This involves focusing on individual deductive steps, rather
than the collection of all conclusions ever reached. This has led to the
formulation of step (or situated) logic, an approach to reasoning in
which the formalism has a kind of real-time self-reference that affects
the course of deduction itself. Such a notion of logic deviates in a crucial
way from traditional formal deductive mechanisms, for the proof process
becomes part of the available information used in forming proofs.
A precise characterization of step-logic is given, with details of two
particular step-logics. Two commonsense reasoning problems, the
Brother problem and the Three-wise-men problem, are modeled using
step-logic, providing real-time formal solutions to these commonsense
reasoning problems. These solutions were then implemented on an IBM
PC-AT.
It appears that step-logic is a promising formalism for modeling the
fact that that reasoning takes time. Contradictions can arise and be
subsequently resolved within the logic itself, permitting a genuinely
computational solution to certain types of default reasoning.
</bodyText>
<table confidence="0.9698395">
Learning from Physical Analogies: A
Study in Analogy and the Explanation
Process
DAI V50(05), SecB, pp 2021
Falkenhainer, Brian Carl
University of Illinois at Urbana-Champaign
Ph.D. 1989, 256 pages
Computer Science
University Microfilms International
ADG89-16244
</table>
<bodyText confidence="0.999853833333333">
To make programs that understand and interact with the world as well
as people do, we must duplicate the kind of flexibility people exhibit
when conjecturing plausible explanations of the diverse physical
phenomena they encounter. This process often involves drawing upon
physical analogies--viewing the situation and its behavior as similar to
familiar phenomena, conjecturing that they share analogous underlying
causes, and using the plausible interpretation as a foothold to further
understanding, analysis, and hypothesis refinement.
This thesis investigates analogical reasoning and learning applied to
the task of constructing qualitative explanations for observed physical
phenomena. Primary emphasis is placed on two central questions. First,
how are analogies elaborated to sanction new inferences about a novel
situation? This problem is addressed by contextual structure-mapping, a
knowledge-intensive adaptation of Gentner&apos;s structure-mapping theory.
It presents analogy elaboration as a map and analyze cycle, in which two
situations are placed in correspondence, followed by problem solving and
inference production focused on correspondence inadequacies. Second,
how is the quality of a proposed analogy evaluated and used for some
performance task? A theory of verification-based analogical learning is
presented that addresses the tenuous nature of analogically inferred
concepts and describes procedures that can be used to increase
confidence in the inferred knowledge. Specifically, it relies on analogical
inference to hypothesize new theories and simulation of those theories to
analyze their validity. It represents a view of analogy as an iterative
process of hypothesis formation, testing, and revision.
These ideas are illustrated via PHINEAS, a program that uses
similarity to posit qualitative explanations for time-varying descriptions
of physical behaviors. It builds upon existing work in qualitative physics
to provide a rich environment in which to describe and reason with
theories of the physical world.
</bodyText>
<page confidence="0.912998">
64 Computational Linguistics Volume 16, Number 1, March 1990
</page>
<table confidence="0.8427772">
Abstracts of Current Literature
A Computational Treatment of the
Comparative
DAL V50(04), SecB, pp 1502
Friedman, Carol
New York University Ph.D. 1989, 296 pages
Computer Science. Information Science.
Language, Linguistics
University Microfilms International
ADG89-16068
</table>
<bodyText confidence="0.999680916666667">
This thesis develops a computational treatment of the comparative in
English that is general, efficient, and relatively easy to implement, while
not unduly complicating the natural language processing system.
Implementation was accomplished using the Proteus Question
Answering System, which translates natural language questions into
database queries.
The comparative is a particularly difficult language structure to
process, and presently only a few natural language systems handle it in
limited ways. However, the comparative is an essential component of
language that frequently occurs in discourse. The comparative is difficult
to process because it corresponds to an amazingly diverse range of
syntactic forms, such as coordinate and subordinate conjunctions and
relative clauses, which are also very complex and often contain missing
elements. Semantically, the comparative is cross-categorical: adjectives,
quantifiers, and adverbs can have the comparative feature. The
semantics of the comparative has to be consistent with that of different
linguistic categories while retaining its own unique characteristics.
The computational approach of this thesis is based on a language
model that contains functionally independent syntactic, semantic, and
pragmatic components. Although the comparative relates to all the
components, the syntactic component is the one that is mainly affected.
The syntactic stage of processing analyzes and regularizes the
comparative structures. The analysis process utilizes existing
mechanisms that handle structures similar to the comparative. The
regularization process transforms all the different comparative
structures into one standard form consisting of a comparative operator
and two complete clauses. This process consists of two phases: the first
uses a compositional approach based on Montague-style translation
rules. The subsequent phase uses specialized procedures to complete the
regularization process by expanding the comparative, filling in missing
elements, and providing the appropriate quantified terms associated with
the comparated elements.
After the comparative is regularized, the remaining stages of
processing are hardly affected. Each clause of the comparative is
processed using the same procedures as usual, and only minor
modifications are required specifically for the comparative.
</bodyText>
<table confidence="0.768498875">
A Generative Taxonomy of Application
Domains Based on Interaction Semantics
DAI V50(03), SecB, pp 1021
Hurley, William David
The George Washington University D.Sc.
1989, 343 pages
Computer Science
University Microfilms International
</table>
<page confidence="0.385648">
ADG89-10833
</page>
<bodyText confidence="0.98349952631579">
User interface designers need tools that actively encourage good user
interface design, provide sophisticated software development support,
and are easy to learn and use. Three goals for the next generation of user
interface tools are helping designers to separate user interface from
application, enhancing software reuse, and generating run-time user
interface software from high-level specifications. Achieving these goals
requires a better understanding of the interface between user interface
and application—the UI—AP interface.
This study defines a model of the UI—AP interface and describes a
methodology for generating a taxonomy of application domain
descriptions based on the model. The UI—AP interface model consists of
a set of modeling primitives with more descriptive power than those in
traditional object-centered models, and serves as a template for
describing interaction semantics: interactions between user interface and
application and knowledge about the application required by the user
interface to process these interactions. The model provides a high-level
representation for describing interactive systems, and supports
specification techniques that enhance opportunities for reusable software
Computational Linguistics Volume 16, Number 1, March 1990 65
</bodyText>
<subsectionHeader confidence="0.913767">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.999948166666667">
components. The model is incorporated into a generative taxonomy
system that defines a methodology for generating new application
domain descriptions from existing descriptions, starting with descriptions
of interaction semantics common across a broad range of applications.
The application descriptions provide a framework for resolving trade-offs
involving separation, facilitate identifying opportunities for reusable
software, and serve as more detailed templates for design and
implementation of interactive systems. The model and the generative
taxonomy system are implemented as a knowledge-based tool that
demonstrates the feasibility of the model and the generative taxonomy
system as an architectural base for specifying interactive software and
for creating an exploratory research environment.
</bodyText>
<figure confidence="0.533594625">
A Computational Theory of Metaphor
DAI V50(04), SecB, pp 1507
Martin, James Hugh
University of California at Berkeley Ph.D.
1988, 309 pages
Computer Science
University Microfilms International
ADG89-16782
</figure>
<bodyText confidence="0.999326953488372">
Metaphor is a conventional and ordinary part of language. A theory
attempting to explain metaphor must account for the ease with which
conventional metaphors are understood, and with the ability to
understand novel metaphors as they are encountered. An approach to
metaphor, based on the explicit representation of knowledge about
metaphors, has been developed to address these issues. This approach
asserts that the interpretation of conventional metaphoric language
should proceed through the direct application of specific knowledge
about the metaphors in the language. Correspondingly, the
interpretation of novel metaphors can be accomplished through the
systematic extension, elaboration, and combination of knowledge about
already well-understood metaphors.
MIDAS (Metaphor Interpretation, Denotation, and Acquisition
System) is a computer program that embodies this approach. MIDAS
can be used to perform the following tasks: represent knowledge about
conventional metaphors, interpret metaphoric language by applying this
knowledge, and dynamically learn new metaphors as they are
encountered during normal processing.
Knowledge about conventional metaphors is represented in the form of
coherent sets of associations between disparate conceptual domains. The
representation captures both the details of individual metaphors and the
systematicities exhibited by the set of metaphors in the language as a
whole. These systematic sets of associations were implemented using the
KODIAK knowledge representation language.
MIDAS is capable of using this metaphoric knowledge to interpret
conventional metaphoric language. The main thrust of this approach is
that normal processing of metaphoric language proceeds through the
direct application of specific knowledge about the metaphors in the
language. This approach gives equal status to all conventional
metaphoric and literal interpretations. Moreover, the mechanisms used
to arrive at metaphoric and literal interpretations are fundamentally the
same.
When a metaphor is encountered for which MIDAS has no applicable
knowledge, MIDAS calls upon its learning component—the Metaphor
Extension System (MES). The approach embodied in the MES asserts
that a novel metaphor can best be understood through the systematic
extension of an already well-understood metaphor.
MIDAS has been integrated as a part of the UNIX Consultant
system. UC is a natural language consultant system that provides naive
computer users with advice on how to use the UNIX operating system.
By calling upon MIDAS, UC can successfully interpret and lean
conventional UNIX domain metaphors as they are encountered during
the course of UC&apos;s normal processing.
</bodyText>
<page confidence="0.63444">
66 Computational Linguistics Volume 16, Number 1, March 1990
</page>
<table confidence="0.975614142857143">
Abstracts of Current Literature
Semantic Constraints in First Order
Theories: A Definition and Its Applicability
DAI V50(04), SecB, pp 1509
Noel, P. A. J.
Victoria University of Manchester (U.K.)
Ph.D. 1988, 202 pages
Computer Science
Available from UMI in association with The
British Library. Requires signed TDF.
University Microfilms International
ADGD-85821
Control of Mixed-Initiative Discourse
through Meta-Locutionary Acts: A
Computational Model
DAI V50(03), SecB, pp 1025
Novick, David Graham
University of Oregon Ph.D. 1988, 237 pages
Computer Science. Language, Linguistics
University Microfilms International
ADG89-11322
</table>
<bodyText confidence="0.992171771929825">
The form and content of conventional databases are usually restricted by
syntactic and semantic constraints. Both kinds of constraints are
contained in the schemas of the databases. It is likely that such
constraints will also be required if the more expressive formalisms, such
as first order predicate logic, are to replace the formalisms of
conventional databases. Under the name of &amp;quot;integrity constraints,&amp;quot;
several definitions of semantic constraints are provided in the literature
for deductive databases. However, these definitions apply only to
restricted forms of first order theories. For instance, a definition due to
Lloyd (1985), which has sometimes been referred to as &amp;quot;standard,&amp;quot;
concerns hierarchical databases, in which recursive definitions do not
occur and negation is interpreted as nonprovability. The aim of this
thesis is to propose a definition of semantic constraints that is more
expressive than the current definitions and applies to a larger class of
first order theories, and to investigate the possibility of implementing
semantic checkers based on this definition.
The proposed definition may be stated informally as follows: a
semantic constraint for a first order theory T is a first order sentence
about the logical implication in T of formulae of the language of T. A
constraint is said to be satisfied if it is true in some Herbrand model of a
metatheory concerning the logical implication of formulae in T (the
conditions that the suitable metatheories and Herbrand models must
satisfy are identified in the thesis).
Semantic checkers have been implemented in Prolog for some
restricted forms of theories. Some of the implementations concern
theories under nonmonotonic assumptions.
Human—computer interaction typically displays single-initiative
interaction in which either the computer or the human controls the
conversation. The interaction is largely preplanned and depends on
well-formed language. In contrast, human—human conversations are
characterized by unpredictability, ungrammatical utterances, nonverbal
expression, and mixed-initiative control in which the conversants take
independent actions. Traditional natural language systems are largely
unable to handle these aspects of &amp;quot;feral&amp;quot; language. Yet human—human
interaction is coherent for the participants; the conversants take turns,
make interruptions, detect and cure misunderstandings, and resolve
ambiguous references. How can these processes of control be modeled
formally in a manner sufficient for use in computers?
Nonsentential aspects of conversation such as nods, fragmentary
utterances, and correction can be seen reflecting control information for
interaction. Such actions by the conversants, based on the context of
their interaction, determine the form of the conversation. In this view
ungrammaticality, for example, is not a problem but a guide to these
&amp;quot;meta&amp;quot; acts. This dissertation develops a theory of &amp;quot;meta-locutionary&amp;quot;
acts that explains these control processes. The theory extends speech-act
theory to real-world conversational control and encompasses a taxonomy
of meta-locutionary acts.
The theory of meta-locutionary acts was refined and validated by a
protocol study and computational simulation. In the protocol study,
subjects were given cooperative problem-solving tasks. The conversants&apos;
interaction, both verbal and nonverbal, was transcribed as illocutionary
and meta-locutionary acts. The computational model was developed
using a rule-based system written in Prolog. The system represents the
independent conversational knowledge of both conversants
simultaneously, and can simulate their simultaneous action. Simulations
of the protocol conversations using the computational model showed that
Computational Linguistics Volume 16, Number 1, March 1990 67
</bodyText>
<subsectionHeader confidence="0.645372">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.9621031">
meta-locutionar y acts are capable of providing control of
mixed-initiative discourse. The model agents can, for example, take and
give turns. A single agent can simultaneously take multiple acts of
differing control. The simulations also confirmed that conversations need
not be strictly planned. Rather, mixed-initiative interaction can be
plausibly controlled by contextually determined operators.
This research has application to natural language processing, user
interface design and multiple-agent artificial intelligence systems. The
theory of meta-locutionary acts will integrate well with existing speech
act—based natural language systems.
</bodyText>
<figure confidence="0.654427263157895">
Explaining Knowledge Systems: Justifying
Diagnostic Conclusions
DAI V50(04), SecB, pp 1512
Tanner, Michael Clay
The Ohio State University Ph.D. 1989, 293
pages
Computer Science
University Microfilms International
ADG89-13705
The Design and Implementation of an
Intelligent Interface for Information
Retrieval
DAI V50(05), SecB, pp 2030
Thompson, Roger Howard
University of Massachusetts Ph.D. 1989, 227
pages
Computer Science. Library Science.
Information Science
University Microfilms International
</figure>
<page confidence="0.928136">
ADG89-17411
</page>
<bodyText confidence="0.999966">
We would like problem-solving systems to explain themselves for a
variety of reasons, including convincing users that problem solutions are
correct and showing that solutions follow from appropriate methods. An
important aspect of explanation is showing that the actions and
conclusions of a problem-solving system are related to the logical
structure of the task that the system performs. That is, the system and
its users must share an understanding of what the task is. Explanations
can then relate the goal-subgoal structure of the problem solver to this
shared understanding. The shared understanding, or shared model, of
the task represents the logical structure of the task, i.e. the features that
characterize correct answers and correct problem-solving. One thing
that a shared model for any task will show is the ways in which users
might be puzzled, that is, the issues they will be concerned about and
about which they may ask questions. In this dissertation I develop this
idea in the concrete domain of diagnosis. In common with many others, I
consider diagnosis to be an abduction problem—determine the disease,
or set of diseases, that best explain a given set of symptoms. The logical
structure of this task can be used to derive the questions that a
diagnostic system can be asked; the questions are those that arise solely
because the system does diagnosis. I give an architecture based on
generic tasks for a system that can do diagnosis. Parts of this system
serve parts of the diagnostic task; from this mapping of architecture to
task it is possible to derive answers to the diagnostic questions. I
illustrate these ideas by describing the development of explanation for
RED, which is not actually a diagnostic system, but what it does is
enough like diagnosis to be informative. There are many aspects to the
problem of explanation, including the problem of how to present
explanations to users. But central to any explanation is its content. This
dissertation is about the content of explanations and how the content can
be derived from the structure and memory of problem solvers by
reference to the logical structure of their problem-solving task. The main
ideas should transfer to tasks other than diagnosis.
Commercial information (text) retrieval systems have been available
since the early 1960s. While they have provided a service allowing
individuals to find useful documents out of the millions of documents
contained in online databases, there are a number of problems that
prevent the user from being more effective. The primary problems are an
inadequate means for specifying information needs, a single way of
responding to all users and their information needs, and an inadequate
user interface.
This thesis describes the design and implementation of IR, an
intelligent interface for information retrieval the purpose of which is to
overcome the limitations of current information retrieval systems by
providing multiple ways of assisting the user to precisely specify his or
her information need and to search for information. The system
organization is based on a blackboard architecture and consists of a
</bodyText>
<page confidence="0.937474">
68 Computational Linguistics Volume 16, Number 1, March 1990
</page>
<figure confidence="0.758456">
Abstracts of Current Literature
Application of an Intelligent CAI Tutoring
System to Spelling Instruction for
Learning-Disabled Students
DAL V50(04), SecA, pp 909
Stricker, Andrew Gerald
Texas A&amp;M University Ph.D. 1988, 331 pages
Education, Psychology. Education, Language
and Literature. Education, Technology
University Microfilms International
ADG89-I 3447
</figure>
<bodyText confidence="0.999949852941176">
number of &amp;quot;experts&amp;quot; that work cooperatively to assist the user. The
operation of the experts is coordinated by a control expert that makes its
decisions based on a plan derived from the analysis of human search
intermediaries, end user dialogues, and user models. The experts provide
multiple formal search strategies, the use and collection of domain
knowledge, and browsing assistance. The operation of the system is
demonstrated by four scenarios.
The purpose of this study was to investigate the impact of an intelligent
computer-assisted instructional (ICAI) tutoring system on the spelling
knowledge of students with learning problems. To do this, four pairs (2
male and 2 female) of learning-disabled students who were matched for
age, sex, aptitude, and reading and spelling achievement were instructed
in spelling for four weeks (45 minutes a day, five days a week) on two
versions of a microcomputer instructional program (SPELLDOWN and
LOBO). After becoming familiarized with the instructional
environment, one subject from each matched pair was randomly
assigned to the instructional treatment condition to begin the study.
Corresponding matched subjects began instruction using the control
version of the program. All subjects then alternated treatments on a
weekly basis for the remaining three weeks of the study. At the
beginning of each week, each pair was presented with a new 10 word
spelling list that contained regular words and highlighted specific
phonetic features. Pre-instructional assessment indicated the phonetic
features that were problematic for students in each matched pair. One
version of the instructional program (SPELLDOWN) included
elaborated correction subroutines featuring scaffolded and faded cues
customized to fit the students&apos; individual responses. The control version
of the program (LOBO) was designed to mimic drill and practice
routines typical of classroom spelling instruction.
Analysis of the results may be interpreted to suggest that the
instructionally enhanced version of SPELLDOWN produced improved
quality in the students&apos; spelling errors as well as increases in the number
of words correctly spelled relative to performance attained using the
control version of the program.
</bodyText>
<footnote confidence="0.333215">
Computational Linguistics Volume 16, Number 1, March 1990 69
</footnote>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000377">
<title confidence="0.88772675">ABSTRACTS OF CURRENT LITERATURE Recent memoranda in computer and cognitive science related to natural language processing. For copies of the technical reports listed below write to: Memoranda Series</title>
<affiliation confidence="0.999327">Computing Research Laboratory</affiliation>
<address confidence="0.999933">Box 30001</address>
<affiliation confidence="0.995151">New Mexico State University</affiliation>
<address confidence="0.9924545">Las Cruces, New Mexico 88003 USA</address>
<note confidence="0.452346666666667">On Psychological Plausibility in Artificial Intelligence Fowler, R. H., Slator, B. M. &amp; Balogh, I. CRL, MCCS-89-150 Neural-Net Implementation of Complex Symbol-Processing in a Mental Model Approach to Syllogistic Reasoning Barnden, J. CRL, MCCS-89-154 Belief, Metaphorically Speaking* Barnden, J. CRL, MCCS-89-155</note>
<abstract confidence="0.996669710526316">The Artificial Intelligence literature is liberally laced with claims about cognitive reality. Sometimes these are strong claims that cite empirical psychological evidence; but more often these claims take the form of weak appeals to &amp;quot;psychological plausibility.&amp;quot; To examine the nature and scientific status of these claims, AI research is characterized along a particular dimension, cast as the &amp;quot;psychological evidence line.&amp;quot; Then, some ideas about theory in AI are examined, especially the thorny notion of &amp;quot;models&amp;quot; in Al theories: what does it mean to use human intellect as a model in an Al theory, or in an Al program? Then, as is so often the case, further light is shed by an historical characterization, giving evidence for a particular grouping of &amp;quot;camps&amp;quot; in Al, according to how psychological evidence &amp;quot;matters&amp;quot; to them. These discussions set the scene, finally, for an examination of the role (really, roles) that psychological plausibility actually plays in Al; and this leads naturally into a discussion, and some conclusions, about which of these roles are appropriate, and which are not. A neural net system called &amp;quot;Conposit&amp;quot; is described. Conposit performs rule-based manipulation of very short-term, complex symbolic data structures. This paper concentrates on a simulated version of Conposit that embodies core aspects of Johnson-Laird&apos;s mental model theory of syllogistic reasoning. This Conposit version is not intended to be a psychological theory, but rather to act as a test and demonstration of the power and flexibility of Conposit&apos;s unusual connectionist techniques for encoding the structure of data. central claim of the paper concerns that attempt to represent propositional attitudes in realistic situations, and particularly in situations portrayed in natural language discourse. The claim is that the system, to achieve a coherent, useful view of a situation, must often ascribe, to outer agents, views of inner agents&apos; attitudes that are based on rich explications in terms of commonsense metaphorical views of mind. This elevates the emasculated metaphors based on notions of world, situation, container, and so on that underlie propositional attitude representation proposals to the status of explicitly used, rich metaphors. A system can adopt different patterns of commonsense inference about attitudes by choosing different metaphors. The current stage of development of a detailed representation scheme based on the claim is described. The scheme allows different metaphors to be used for the explication of attitudes at different levels in a nested-attitude situation.</abstract>
<note confidence="0.65512">60 Computational Linguistics Volume 16, Number 1, March 1990</note>
<title confidence="0.970113">Abstracts of Current Literature Constructing A Machine Tractable Dictionary From Longman Dictionary of Contemporary English</title>
<author confidence="0.770747">C-M</author>
<keyword confidence="0.210006">Dissertation</keyword>
<abstract confidence="0.998615655172414">It is the purpose of this research to design a machine-tractable (henceforth MTD) from Dictionary of English LDOCE). The MTD is intended to be a basic facility for a whole spectrum of natural language processing tasks. The research adopts a compositional-reduction approach to obtain a formalized set of definitions of sense entries in a nested predicate form, where the predicates are a set of &amp;quot;seed sense.&amp;quot; The focus of this research is on the derivation of these &amp;quot;seed senses&amp;quot; and their utilization in the of the construction of the proposed the following four including step 1: determine the &amp;quot;defining senses&amp;quot; of those world senses that are used in the definition of the meaning of 2,137 &amp;quot;controlled words&amp;quot; of LDOCE; step 2: derive the &amp;quot;seed senses&amp;quot; of LDOCE. The &amp;quot;seed senses&amp;quot; are a subset of the defining sense that are sufficient to define senses of step 1. The seen senses are taken as a natural set semantic primitives derived from LDOCE; step 3: hand-code the initial knowledge base for the natural set of semantic primitives from LDOCE; step 4: construct a the controlled words and the rest of LDOCE words by means of bootstrapping process, a process of knowledge acquisition from dictionary definition text. Step 1 of the construction process has been completed. A total of 3,860 defining senses have been determined. Step 2 of the construction process has also been completed. A total of 3,280 word senses are found to be the seed senses of LDOCE. These seed senses are taken as a natural set of semantic primitives derived from the dictionary. The feasibility of Steps 3 and 4 of the coprocess have been demonstrated with implemented examples. What remains to be accomplished is to complete Steps 3 and 4 to build a full-sized MTD from LDOCE using as guidelines the results of initial implementation of the two steps.</abstract>
<title confidence="0.637322">Selected Dissertation Abstracts</title>
<author confidence="0.734037">Compiled by Susanne M Humphrey</author>
<affiliation confidence="0.995104">National Library of Medicine</affiliation>
<address confidence="0.998826">Bethesda, MD 20209</address>
<author confidence="0.997844">Bob Krovetz</author>
<affiliation confidence="0.999982">University of Massachusetts</affiliation>
<address confidence="0.999877">Amherst, MA 01002</address>
<abstract confidence="0.708776">The following are citations selected by title and abstract as being related to computational linguistics or knowledge resulting from a computer search, using the Technologies retrieval service, of the Dissertation Abstracts International (DAI) database produced by University Microfilms International. Included are the UM order number and year-month of entry into the database; author; university, degree, and, if available, number of pages; title; DAI subject category chosen by the author of the dissertation; and abstract. References sorted first by DAI subject category and second by author. Citations denoted by an do not yet have abstracts in the database and refer to abstracts in the published Masters Abstracts International.</abstract>
<affiliation confidence="0.663011">Unless otherwise specified, paper or microform copies of dissertations may be ordered from University Microfilms International, Dissertation Copies, Post Office Box 1764, Ann Arbor, MI 48106; telephone for U.S. (except Michigan,</affiliation>
<address confidence="0.704172">Hawaii, Alaska): 1-800-521-3042, for Canada: 1-800-268-6090. Price lists and other ordering and shipping information</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>