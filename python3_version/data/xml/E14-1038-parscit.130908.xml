<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000129">
<title confidence="0.990156">
Correcting Grammatical Verb Errors
</title>
<author confidence="0.996757">
Alla Rozovskaya Dan Roth Vivek Srikumar
</author>
<affiliation confidence="0.999929">
Columbia University University of Illinois Stanford University
</affiliation>
<address confidence="0.94887">
New York, NY 10115 Urbana, IL 61801 Stanford, CA 94305
</address>
<email confidence="0.998837">
ar3366@columbia.edu danr@illinois.edu svivek@cs.stanford.edu
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878892857143">
Verb errors are some of the most com-
mon mistakes made by non-native writers
of English but some of the least studied.
The reason is that dealing with verb er-
rors requires a new paradigm; essentially
all research done on correcting grammat-
ical errors assumes a closed set of trig-
gers – e.g., correcting the use of prepo-
sitions or articles – but identifying mis-
takes in verbs necessitates identifying po-
tentially ambiguous triggers first, and then
determining the type of mistake made and
correcting it. Moreover, once the verb is
identified, modeling verb errors is chal-
lenging because verbs fulfill many gram-
matical functions, resulting in a variety of
mistakes. Consequently, the little earlier
work done on verb errors assumed that the
error type is known in advance.
We propose a linguistically-motivated ap-
proach to verb error correction that makes
use of the notion of verb finiteness to iden-
tify triggers and types of mistakes, before
using a statistical machine learning ap-
proach to correct these mistakes. We show
that the linguistically-informed model sig-
nificantly improves the accuracy of the
verb correction approach.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991059655172414">
We address the problem of correcting grammati-
cal verb mistakes made by English as a Second
Language (ESL) learners. Recent work in ESL er-
ror correction has focused on errors in article and
preposition usage (Han et al., 2006; Felice and
Pulman, 2008; Gamon et al., 2008; Tetreault et
al., 2010; Gamon, 2010; Rozovskaya and Roth,
2010b; Dahlmeier and Ng, 2011).
While verb errors occur as often as article and
preposition mistakes, with a few exceptions (Lee
and Seneff, 2008; Gamon et al., 2009; Tajiri et al.,
2012), there has been little work on verbs. There
are two reasons for why it is difficult to deal with
verb mistakes. First, in contrast to articles and
prepositions, verbs are more difficult to identify
in text, as they can often be confused with other
parts of speech, and processing tools are known to
make more errors on noisy ESL data (Nagata et al.,
2011). Second, verbs are more complex linguisti-
cally: they fulfill several grammatical functions,
and these different roles imply different types of
errors.
These difficulties have led all previous work
on verb mistakes to assume prior knowledge of
the mistake type; however, identifying the specific
category of a verb error is nontrivial, since the sur-
face form of the verb may be ambiguous, espe-
cially when that verb is used incorrectly. Consider
the following examples of verb mistakes:
</bodyText>
<listItem confidence="0.997776714285714">
1. “We discusses*/discuss this every time.”
2. “I will be lucky if I {will find}*/find something that
fits.”
3. “They wanted to visit many places without
spend*/spending a lot of money.”
4. “They arrived early to organized*/organize every-
thing”.
</listItem>
<bodyText confidence="0.999070777777778">
These examples illustrate three grammatical
verb properties: Agreement, Tense, and non-finite
Form choice that encompass the most common
grammatical verb problems for ESL learners. The
first two examples show mistakes on verbs that
function as main verbs in a clause: sentence (1)
shows an example of subject-verb Agreement er-
ror; (2) is an example of a Tense mistake where
the ambiguity is between {will find} (Future tense)
</bodyText>
<page confidence="0.971746">
358
</page>
<note confidence="0.993029">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 358–367,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999816961538462">
and find (Present tense). Examples (3) and (4) dis-
play Form mistakes: confusing the infinitive and
gerund forms in (3) and including an inflection on
an infinitive verb in (4).
This paper addresses the specific challenges of
verb error correction that have not been addressed
previously – identifying candidates for mistakes
and determining which class of errors is present,
before proceeding to correct the error. The ex-
perimental results show that our linguistically-
motivated approach benefits verb error correction.
In particular, in order to determine the error type,
we build on the notion of verb finiteness to distin-
guish between finite and non-finite verbs (Quirk et
al., 1985), that correspond to Agreement and Tense
mistakes (examples (1) and (2) above) and Form
mistakes (examples (3) and (4) above), respec-
tively (see Sec. 3). The approach presented in this
work was evaluated empirically and competitively
in the context of the CoNLL shared task on error
correction (Ng et al., 2013) where it was imple-
mented as part of the highest-scoring University
of Illinois system (Rozovskaya et al., 2013) and
demonstrated superior performance on the verb er-
ror correction sub-task.
This paper makes the following contributions:
</bodyText>
<listItem confidence="0.873650066666667">
• We present a holistic, linguistically-motivated
framework for correcting grammatical verb mis-
takes; our approach “starts from scratch” with-
out any knowledge of which mistakes should be
corrected or of the mistake type; in doing that
we show that the specific challenges of verb error
correction are better addressed by first identifying
the finiteness of the verb in the error identification
stage.
• Within the proposed model, we describe and
evaluate several methods of selecting verb candi-
dates, an algorithm for determining the verb type,
and a type-driven verb error correction system.
• We annotate a subset of the FCE data set with
gold verb candidates and gold verb type.1
</listItem>
<sectionHeader confidence="0.999343" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999630166666667">
Earlier work in ESL error correction follows the
methodology of the context-sensitive spelling cor-
rection task (Golding and Roth, 1996; Golding
and Roth, 1999; Banko and Brill, 2001; Carlson
et al., 2001; Carlson and Fette, 2007). Most of
the effort in ESL error correction so far has been
</bodyText>
<footnote confidence="0.9937605">
1The annotation is available at http://cogcomp.cs.illinois.
edu/page/publication view/743
</footnote>
<bodyText confidence="0.999928725490197">
on article and preposition usage errors, as these
are some of the most common mistakes among
non-native English speakers (Dalgish, 1985; Lea-
cock et al., 2010). These phenomena are generally
modeled as multiclass classification problems: a
single classifier is trained for a given error type
where the set of classes includes all articles or the
top n most frequent English prepositions (Izumi
et al., 2003; Han et al., 2006; Felice and Pul-
man, 2008; Gamon et al., 2008; Tetreault et al.,
2010; Rozovskaya and Roth, 2010b; Rozovskaya
and Roth, 2011; Dahlmeier and Ng, 2011).
Mistakes on verbs have attracted significantly
less attention in the error correction literature.
Moreover, the little earlier work done on verb er-
rors only considered subsets of these errors and
assumed the error sub-type is known in advance.
Gamon et al. (2009) mentioned a model for learn-
ing gerund/infinitive confusions and auxiliary verb
presence/choice. Lee and Seneff (2008) proposed
an approach based on pattern matching on trees
combined with word n-gram counts for correcting
agreement misuse and some types of verb form
errors. However, they excluded tense mistakes,
which is the most common error category for ESL
learners (40% of all verb errors, Sec. 3). Tajiri
et al. (2012) considered only tense mistakes. In
the above studies, it was assumed that the type of
mistake that needs to be corrected is known, and
irrelevant verb errors were excluded (e.g., Tajiri
et al. (2012) addressed only tense mistakes and
excluded from the evaluation other kinds of verb
errors). In other words, it was assumed that part
of the task was solved. But, unlike in article and
preposition error correction where the type of mis-
take is known based on the surface form of the
word, in verb error correction, it is not obvious.
The key distinction of our work is that we pro-
pose a holistic approach that starts from “scratch”
and, given an instance, first detects a mistake and
identifies its type, and then proceeds to correct
it. We also evaluate several methods for select-
ing verb candidates and show the significance of
this step for improving verb error correction per-
formance, while earlier studies do not discuss this
aspect of the problem. In the CoNLL shared task
(Ng et al., 2013) that included verb errors in agree-
ment and form, the participating teams did not pro-
vide details on how specific challenges were han-
dled, but the University of Illinois system obtained
the highest score on the verb sub-task, even though
</bodyText>
<page confidence="0.998198">
359
</page>
<table confidence="0.999371222222222">
Tag Error type Rel. freq. (%)
TV Tense 40.0
FV Form 22.3
AGV Verb-subject agreement 11.5
MV Missing verb 11.7
UV Unneccesary verb 7.3
IV Inflection 5.4
DV Derivation 1.8
Total 6640
</table>
<tableCaption confidence="0.99976">
Table 1: Grammatical verb errors in FCE.
</tableCaption>
<bodyText confidence="0.50309">
all teams used similar resources (Ng et al., 2013).
</bodyText>
<sectionHeader confidence="0.775282" genericHeader="method">
3 Verb Errors in ESL Writing
</sectionHeader>
<bodyText confidence="0.9996708">
Verb-related errors are very prominent among
non-native English speakers: grammatical mis-
use of verbs constitutes one of the most com-
mon errors in several learner corpora, including
those previously used (Izumi et al., 2003; Lee
and Seneff, 2008) and the one employed in this
work. We study verb errors using the FCE cor-
pus (Yannakoudakis et al., 2011). The corpus
possesses several desirable characteristics: it is
large (500,000 words), has been annotated by na-
tive English speakers, and contains data by learn-
ers of multiple first-language backgrounds. The
FCE corpus contains 5056 determiner errors, 5347
preposition errors, and 6640 grammatical verb
mistakes (Table 1).
</bodyText>
<subsectionHeader confidence="0.998121">
3.1 Verb Finiteness
</subsectionHeader>
<bodyText confidence="0.999700958333333">
There are many grammatical categories for which
English verbs can be marked. The linguistic no-
tion of verb finiteness or verb type (Radford, 1988;
Quirk et al., 1985) distinguishes between verbs
that function on their own in a clause as main verbs
(finite) and those that do not (non-finite). Gram-
matical properties associated with each group are
mutually exclusive: tense and agreement markers,
for example, do not apply to non-finite verbs; non-
finite verbs are not marked for many grammatical
functions but may appear in several forms.
The most common verb problems for ESL
learners – Tense, Agreement, non-finite Form –
involve verbs both in finite and non-finite roles.
Table 2 illustrates contexts that license finite and
non-finite verbs.
Our intuition is that, because properties associ-
ated with each verb type are mutually exclusive,
verb finiteness should benefit verb error correc-
tion models: an observed verb error may be due
to several grammatical phenomena, and knowing
which phenomena are active depends on the func-
tion of the verb in the current context. Note that
Agreement, Tense, and Form errors account for
</bodyText>
<table confidence="0.998600333333333">
Category Agreement Kappa Random
Correct verbs 0.97 0.95 0.51
Erroneous verbs 0.88 0.81 0.41
</table>
<tableCaption confidence="0.9978915">
Table 3: Inter-annotator agreement based on 250 verb
errors and 250 correct verbs, randomly selected.
</tableCaption>
<bodyText confidence="0.999238333333333">
about 74% of all grammatical verb errors in Ta-
ble 1 but the finiteness distinction applies to all
English verbs – every verb is either finite or non-
finite in a specific syntactic context – and is also
relevant for the remaining mistakes not addressed
here.2
</bodyText>
<sectionHeader confidence="0.997549" genericHeader="method">
4 Annotation for Verb Finiteness
</sectionHeader>
<bodyText confidence="0.999965966666667">
In order to evaluate the quality of the algorithm
for verb finiteness and of the candidate selection
methods, we annotated all verbs – correct and er-
roneous – in a random set of 124 documents from
our corpus with the information about verb finite-
ness. We refer to these 124 documents as gold sub-
set. We also annotated erroneous verbs in the re-
maining 1120 documents of the corpus. The anno-
tation was performed by two students with back-
ground in Linguistics. The inter-annotator agree-
ment is shown in Table 3 and is high.
Annotating Verb Errors For each verb error that
was tagged as Tense (TV), Agreement (AGV), and
Form (FV), the annotators marked verb finiteness.
Additionally, the annotators also specified the type
of error (Tense, Agreement, or Form) (Table 4),
since the FCE tags do not always correspond to
the three error types we study here. For exam-
ple, the FV tag may mark errors on finite verbs.
Overall, about 7% of verb errors have to do with
phenomena different from the three verb proper-
ties considered in this work and thus are excluded
from the present study.
Annotating Correct Verbs Correct verbs were
identified in text using an automated proce-
dure that relies on part-of-speech information
(Sec. 5.1). Valid candidates were specified for
verb finiteness. The candidates that were iden-
tified incorrectly due to mistakes by the part-of-
speech tagger were marked as invalid.
</bodyText>
<sectionHeader confidence="0.993406" genericHeader="method">
5 The Computational Model
</sectionHeader>
<bodyText confidence="0.999479">
The verb error correction problem is formulated
as a classification task in the spirit of the learn-
</bodyText>
<footnote confidence="0.63267725">
2For instance, the missing verb errors (MV, 11.7%) re-
quire an additional step to identify contexts for missing verbs,
and then appropriate verb properties need to be determined
based on verb finiteness.
</footnote>
<page confidence="0.983129">
360
</page>
<table confidence="0.947018">
Verb type Example Agreement Verb Form
properties
Tense
Finite “He discussed this with me last week” - Past Simple -
“He discusses this with me every week.” 3rd person,Sing. Present Simple -
Non-finite “He left without discussing it with me.” - - Gerund
“They let him discuss this with me.” - - Infinitive
“To discuss this now would be ill-advised.” - - to-Infinitive
</table>
<tableCaption confidence="0.992148">
Table 2: Contexts that license finite and non-finite verbs and the corresponding active properties.
</tableCaption>
<table confidence="0.999686">
Error on Verb Type Subcategory Example
Finite (67.7%) Agreement (20%) “We discusses*/discuss this every time.”
Tense (80%) “If you buy something, you {would be}*/{will be} happy.”
Non-finite (25.3%) “If one is famous he has to accept the disadvantages of be*/being famous.” “I am very
glad {for receiving}*/{to receive} it.”
“They arrived early to organized*/organize everything.”
Other errors (7.0%) Passive/Active(42.3%) “Our end-of-conference party {is included}*/includes dinner and dancing.”
Compound (40.7%) “You ask me for some informations*/information- here they*/it are*/is.”
Other (16.8%) “Nobody {has to be}*/{should be} late.”
</table>
<tableCaption confidence="0.999846">
Table 4: Verb error classification based on 4864 mistakes marked as TV, AGV, and FV errors in the FCE corpus.
</tableCaption>
<bodyText confidence="0.96960525">
ing paradigm commonly used for correcting other
ESL errors (Sec. 2), with the exception that the
verb model includes additional components. All
of the components are listed below:
</bodyText>
<listItem confidence="0.999418">
1. Candidate selection (5.1)
2. Verb finiteness prediction (5.2)
3. Feature generation (5.3)
4. Error identification (5.4)
5. Error correction (5.5)
</listItem>
<bodyText confidence="0.999290769230769">
After verb candidates are selected, verb finite-
ness is determined and features are generated for
each candidate. The finiteness prediction is used
in the error identification component. Given the
output of the error identification stage, the corre-
sponding classifiers for each error type are invoked
to propose an appropriate correction.
We split the corpus documents into two equal
parts – training and test. We chose a train-test split
and not cross-validation, since the FCE data set is
quite large to allow for such a split. The training
data is also used to develop the components for
candidate selection and verb finiteness prediction.
</bodyText>
<subsectionHeader confidence="0.997738">
5.1 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.999972534883721">
This stage selects the set of verb instances that
are presented as input to the classifier. A verb in-
stance refers to the verb, including its auxiliaries
or the infinitive marker (e.g. “found”, “will find”,
“to find”). Candidate selection is a crucial step for
models that correct mistakes on open-class words
because those errors that are missed at this stage
have no chance of being detected. We implement
four candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as identi-
fied by a shallow parser (Punyakanok and Roth,
2001).3 Method (2) also includes words tagged
with one of the verb tags: {VB, VBN, VBG,
VBD, VBP, VBZ} predicted by the POS tagger.4
However, relying on the POS information is not
good enough, since the POS tagger performance
on ESL data is known to be suboptimal (Nagata et
al., 2011). For example, verbs lacking agreement
markers are likely to be mistagged as nouns (Lee
and Seneff, 2008). Methods (3) and (4) address
the problem of pre-processing errors. Method (3)
adds words that are on the list of valid English
verb lemmas; the lemma list is constructed us-
ing a POS-tagged version of the NYT section of
the Gigaword corpus and contains about 2,600 of
frequently-occurring words tagged as VB; for ex-
ample, (3) will add shop but not shopping, but (4)
will add both.
For methods (3) and (4), we developed verb-
Morph,5 a tool that performs morphological anal-
ysis on verbs and is used to lemmatize verbs and
to generate morphological variants. The module
makes uses of (1) the verb lemma list and (2) a list
of irregular English verbs.
The quality of the candidate selection methods
is evaluated in Table 5 on the gold subset by com-
puting the recall, i.e. the percentage of erroneous
verbs that have been selected as candidates. Meth-
ods that address pre-processing mistakes are able
to recover more erroneous verb candidates in text.
It is also interesting to note that across all methods,
the highest recall is obtained for tense errors. This
suggests that the POS tagger is more prone to fail-
</bodyText>
<footnote confidence="0.999896">
3http://cogcomp.cs.illinois.edu/demo/shallowparse
4http://cogcomp.cs.illinois.edu/page/software view/POS
5The tool and more detail about it can be found at
http://cogcomp.cs.illinois.edu/page/publication view/743
</footnote>
<page confidence="0.972429">
361
</page>
<table confidence="0.99954875">
Method Recall Recall by error group (%)
(%) Agr. Tense Form
(1) All verb phrases 83.00 86.62 93.55 59.08
(2) + tokens tagged as verbs 91.96 90.30 94.33 87.79
(3) + tokens that are valid 95.50 95.99 96.46 93.23
verb lemmas
(4) + tokens with inflections 96.09 96.32 96.62 94.84
that are valid verb lemmas
</table>
<tableCaption confidence="0.99951">
Table 5: Candidate selection methods performance.
</tableCaption>
<bodyText confidence="0.9999518">
ure due to errors in agreement and form. The eval-
uation in Table 5 uses recall, as the goal is to assess
the ability of the methods to select erroneous verbs
as candidates. In Sec. 6.1, the contribution of each
method to error identification is evaluated.
</bodyText>
<subsectionHeader confidence="0.998151">
5.2 Predicting Verb Finiteness
</subsectionHeader>
<bodyText confidence="0.9999298125">
Predicting verb finiteness is not trivial, as almost
all English verbs can occur in both finite and non-
finite form and the surface forms of a verb in finite
and non-finite form may be the same (see Table 2).
While we cannot learn verb type automatically
due to lack of annotation, we show, however, that,
for the majority of verbs, finiteness can be reliably
predicted using linguistic knowledge. We imple-
ment a decision-list classifier that makes use of
linguistically-motivated rules (Table 6). The algo-
rithm covers about 92% of all verb candidates, ab-
staining on the remaining highly-ambiguous 8%.
The evaluation of the method on the gold sub-
set (last column in Table 6) shows that despite its
simplicity, this method is highly effective: 98% on
correct verbs and over 89% on errors.
</bodyText>
<subsectionHeader confidence="0.8199">
5.3 Features
</subsectionHeader>
<bodyText confidence="0.9999845">
The baseline features are word n-grams in the 4-
word window around the verb instance. Addi-
tional features are intended to characterize a given
error type and are selected based on previous stud-
ies: for Agreement and Form errors, we use a
parser (Klein and Manning, 2003) and define fea-
tures that reflect dependency relations between the
verb and its neighbors. We denote these features
by syntax. Syntactic knowledge via tree patterns
has been shown useful for Agreement mistakes
(Lee and Seneff, 2008). Features for Tense in-
clude temporal adverbs in the sentence and tenses
of other verbs in the sentence and are similar to
the features used in other verb classification tasks
(Reichart and Rappoport, 2010; Lee, 2011; Tajiri
et al., 2012). The features are shown in Table 7.
</bodyText>
<subsectionHeader confidence="0.569204">
5.4 Error Identification
</subsectionHeader>
<bodyText confidence="0.999909454545454">
The goal of this stage is to identify errors and to
predict their type. We define a linear model where,
given a verb, a weight vector w assigns a score
to each label in the label space {Correct, Form,
Agreement, Tense}. The prediction of the classi-
fier is the label with the highest score.
The baseline error identification model, called
combined, is agnostic to the type of the verb. In
the combined model, for each verb v and label l,
we generate a feature vector, O(v, l) and the best
label is predicted as
</bodyText>
<equation confidence="0.456366">
arg max wT O(v,l).
l
</equation>
<bodyText confidence="0.95098105882353">
The combined model makes use of all the fea-
tures we have defined earlier for each verb.
The type-based model uses the verb finiteness
prediction made by the verb finiteness classifier.
A soft way to use the finiteness prediction is to
add the predicted finiteness value as a feature. The
other – hard-decision approach – is to use only
a subset of the features depending on the pre-
dicted finiteness: Agreement and Tense for the fi-
nite verbs, and Form features for non-finite. The
hard-decision type-driven approach defines a fea-
ture vector for a verb based on its type. Thus,
given the verb v and its type t, we define fea-
tures O(v, t, l) for each label l. Thus, the label is
predicted as
arg max
l wT O(v, t, l).
</bodyText>
<sectionHeader confidence="0.591534" genericHeader="method">
5.5 Error Correction
</sectionHeader>
<bodyText confidence="0.999979533333333">
The correction module consists of three compo-
nents, one for each type of mistake. Given the
output of the error identification model, the ap-
propriate correction component is run for each in-
stance predicted to be a mistake.6 The verb finite-
ness prediction is used to select finite instances for
training the Agreement and Tense components and
non-finite – for the Form component. The label
space for Tense specifies tense and aspect prop-
erties of the English verbs (see Tajiri et al., 2012
for more detail), the Agreement component spec-
ifies the person and number properties, while the
Form component includes the commonly confus-
able non-finite English forms (see Table 2). These
components are trained as multiclass classifiers.
</bodyText>
<footnote confidence="0.930575333333333">
6We assume that each verb contains at most one mistake.
Less than 1% of all erroneous verbs have more than one error
present.
</footnote>
<page confidence="0.992626">
362
</page>
<figure confidence="0.664331666666667">
A verb is Non-Finite if any of the following hold: A verb is Finite if any of the following hold Accuracy on
Correct Erroneous
verbs verbs
</figure>
<listItem confidence="0.9246824">
(1) All verbs identified by shallow parser
(1) [numTokens = 2] ∧ [firstToken = to] (2) can; could
(2) firstToken = be (3) [numTokens = 1] ∧ [pos ∈ {V BD, VBP, V BZ}] 98.01 89.4
(3) [numTokens = 1] ∧ [pos = VBG] (4) [numTokens = 2] ∧ [firstToken! = to]
(5) numTokens &gt; 2
</listItem>
<tableCaption confidence="0.989604">
Table 6: Algorithm for determining verb type. numTokens denotes the number of tokens in the verb instance, e.g., for the
verb instance “to go”, numTokens = 2. Verbs not covered by the rules, e.g. those that are not tagged with a verb-related POS
in methods (3) and (4), are not assigned any verb type. The last column shows algorithm accuracy on the gold subset separately
for correct and incorrect verbs.
</tableCaption>
<table confidence="0.941519809523809">
Agreement Description
subjHead, subjPOS The surface form and the POS tag of the subject head
subjDet {those,this,..} Determiner of the subject phrase
subjDistance Distance between the verb and the subject head
subjNumber {Sing, Pl} Sing – singular pronouns and nouns; Pl – plural pronouns and nouns
subjPerson {3rdSing, Not3rdSing, 1stSing} 3rdSing – she,he,it,singular nouns; Not3rdSing – we,you,they, plural nouns; 1stSing – “I”
conjunctions (1)&amp;(3);(4)&amp;(5)
Tense Description
verb phrase (VP) verb lemma, negation, surface forms and POS tags of all words in the verb phrase
verbs in sentence(4 features) tenses and lemmas of the finite verbs preceding and following the verb instance
time adverbs (2 features) temporal adverb before and after the verb instance
bag-of-words (BOW) (8 features) Includes the following words in the sentence: {if, when, since, then, wish, hope, when, since,
after}
Form Description
closest word surface form, lemma, POS tag, and distance of the closest open-class word to the left of the
verb
governor surface form, POS tag and dependency type of the target
preposition if the verb is preceded by a preposition: preposition itself and the surface form, POS tag and
dependency of the governor of the preposition
pos and lemma POS tag and lemma of the verb and their conjunctions with features in (2) and (3) and word
ngrams
</table>
<tableCaption confidence="0.99419">
Table 7: Features used, grouped by error type.
</tableCaption>
<sectionHeader confidence="0.997884" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.892736714285715">
The main goal of this work is to propose a uni-
fied framework for correcting verb mistakes and
to address the specific challenges of the problem.
We thus do not focus on features or on the spe-
cific learning algorithm. Our experimental study
addresses the following research questions:
I. Linguistic questions: (i) candidate selection
methods; (ii) verb finiteness contribution to
error identification
II. Computational Framework: error identifi-
cation vs. correction
III. Gold annotation: (i) using gold candidates
and verb type vs. automatic; (ii) performance
comparison by error type
Learning Framework There is a lot of under-
standing for which algorithmic methods work
best for ESL correction tasks, how they compare
among themselves, and how they compare to n-
gram based methods. Specifically, despite their in-
tuitive appeal, language models were shown to not
work well on these tasks, while the discriminative
learning framework has been shown to be superior
to other approaches and thus is commonly used
for error correction tasks (see Sec. 2). Since we
do not address the algorithmic aspect of the prob-
lem, we refer the reader to Rozovskaya and Roth
(2011) for a discussion of these issues. We train
all our models with the SVM learning algorithm
implemented in JLIS (Chang et al., 2010).
Evaluation We report both Precision/Recall
curves and AAUC (as a summary). Error cor-
rection is generally evaluated using F1 (Dale et
al., 2012); Precision and Recall (Gamon, 2010;
Tajiri et al., 2012); or Average Area Under Curve
(AAUC) (Rozovskaya and Roth, 2011). For a dis-
cussion on these metrics with respect to error cor-
rection tasks, we refer the reader to Rozovskaya
(2013). AAUC (Hanley and McNeil, 1983)) is a
measure commonly used to generate a summary
statistic, computed as an average precision value
over a range of recall points. In this paper, AAUC
is computed over the first 15 recall points:
</bodyText>
<equation confidence="0.9394915">
1
AAUC = 15 ·
</equation>
<subsectionHeader confidence="0.990715">
6.1 Linguistic Questions
</subsectionHeader>
<bodyText confidence="0.98520375">
Candidate Selection Methods The contribution
of the candidate selection component with respect
to error identification is evaluated in Table 8, us-
ing the methods presented in Sec. 5.1. Overall,
</bodyText>
<equation confidence="0.810876">
Precision(i).
i=1
1 5
</equation>
<page confidence="0.987419">
363
</page>
<table confidence="0.933706833333333">
Recall of candidate Combined AAUC
selection method (%) Type-based
(1) (83.00) 73.38 79.49
(2) (91.96) 80.36 86.48
(3) (95.50) 81.39 87.05
(4) (96.09) 81.27 86.81
</table>
<tableCaption confidence="0.9364">
Table 8: Impact of candidate selection methods on error
identification performance. The first column shows the per-
centage of erroneous verbs selected by each method. Type-
based models are discussed in Sec. 6.1.
</tableCaption>
<table confidence="0.998788">
Correct verbs Erroneous verbs Error rate
Training 41721 1981 4.75%
Test 41836 2014 4.81%
</table>
<tableCaption confidence="0.9918555">
Table 9: Training and test data statistics. Candidates are
selected using method (3).
</tableCaption>
<bodyText confidence="0.9994731875">
better performance is achieved by methods with
higher recall, with the exception of method (4); its
performance on error identification is behind that
of method (3), perhaps due to the amount of noise
that is also added. While the difference is small,
method (3) is also simpler than method (4). We
thus use method (3) in the rest of the paper. Table
9 shows the number of verb instances in training
and test selected with this method.
Verb Finiteness Sec. 5.4 presented two ways of
adding verb finiteness: (1) adding the predicted
verb type as a feature and (2) selecting only the
relevant features depending on the finiteness of the
verb. Table 10 shows the results of using verb type
in the error identification stage. While the first
approach does not provide improvement over the
combined model, the second method is very ef-
fective. We conjecture that because verb type pre-
diction is quite accurate, the second, hard-decision
approach is preferred, as it provides knowledge in
a direct way. Henceforth, we will use the second
method in the type-based model.
Fig. 1 compares the performance of the com-
bined and the hard-decision type-based models
shown in Table 10. Precision/Recall curves are
generated by varying the threshold on the confi-
dence of the classifier. This graph reveals the be-
havior of the systems at multiple recall points: we
observe that at every recall point the type-based
classifier has higher precision.
So far, the models used all features defined in
Sec. 5.3. Table 11 reveals that the type-driven
</bodyText>
<table confidence="0.927128">
Model AAUC
Combined 81.39
Type-based I (soft) 81.11
Type-based II (hard) 87.05
</table>
<tableCaption confidence="0.769626">
Table 10: Verb finiteness contribution to error identifi-
cation.
Figure 1: Verb finiteness contribution to error identifi-
cation: key result. AAUC shown in Table 10. The combined
model uses no verb type information. In the hard-decision
type-based model, each verb uses the features according to
its finiteness. The differences are statistically significant (Mc-
Nemar’s test, p &lt; 0.0001).
</tableCaption>
<table confidence="0.9997184">
Feature set Combined AAUC
Type-based
Baseline 46.62 49.72
All−Syntax 79.47 84.88
Full feature set 81.39 87.05
</table>
<tableCaption confidence="0.998108">
Table 11: Verb finiteness contribution to error identifi-
cation for different features.
</tableCaption>
<bodyText confidence="0.999870666666667">
approach is superior to the combined approach
across different feature sets, and the performance
gap increases with more sophisticated feature sets,
which is to be expected, since more complex fea-
tures are tailored toward relevant verb errors. Fur-
thermore, adding features specific to each error
type significantly improves the performance over
the word n-gram features. The rest of the experi-
ments use all features (denoted Full feature set).
</bodyText>
<subsectionHeader confidence="0.999259">
6.2 Identification vs. Correction
</subsectionHeader>
<bodyText confidence="0.999893">
After running the error identification component,
we apply the appropriate correction models to
those instances identified as errors. The results
for identification and correction are shown in Ta-
ble 12. The correction models are also finiteness-
aware models trained on the relevant verb in-
stances (finite or non-finite), as predicted by the
verb finiteness classifier.
We evaluate the correction components by fix-
ing a recall point in the error identification stage.7
We observe the relatively low recall obtained by
the models. Error correction models tend to have
low recall (see, for example, the recent shared
tasks on ESL error correction (Dale and Kilgar-
riff, 2011; Dale et al., 2012; Ng et al., 2013)). The
key reason for the low recall is the error sparsity:
over 95% of verbs are correct, as shown in Table 9.
</bodyText>
<footnote confidence="0.9959475">
7We can increase recall using a different threshold but
higher precision is preferred in error correction tasks.
</footnote>
<page confidence="0.991742">
364
</page>
<table confidence="0.9999215">
Error type P Correction F1 P Identification F1
R R
Agreement 90.62 9.70 17.52 90.62 9.70 17.52
Tense 60.51 7.47 13.31 86.62 10.70 19.05
Form 81.82 16.34 27.24 83.47 16.67 27.79
Total 71.94 10.24 17.94 85.81 12.22 21.20
</table>
<tableCaption confidence="0.998272">
Table 12: Performance of the complete model after the
</tableCaption>
<bodyText confidence="0.9903487">
correction stage. The results on Agreement mistakes are the
same, since Agreement errors are always binary decisions,
unlike Tense and Form mistakes.
The only way to improve over this 95% baseline is
by forcing the system to have very good precision
(at the expense of recall). The performance shown
in Table 12 corresponds to an accuracy of 95.60%
in identification (error reduction of 8.7%) and
95.40% in correction (error reduction of 4.5%)
over the baseline of 95.19%.
</bodyText>
<subsectionHeader confidence="0.999753">
6.3 Analysis on Gold Data
</subsectionHeader>
<bodyText confidence="0.999962705882353">
To further study the impact of each step of the sys-
tem, we analyze our model on the gold subset of
the data. The gold subset contains two additional
pieces of information not available for the rest of
the corpus: gold verb candidates and gold verb
finiteness (Sec. 4). The set contains 7784 gold
verbs, including 464 errors. Experiments are run
in 10-fold cross-validation where on each run 90%
of the documents are used for training and the re-
maining 10% are used for evaluation. The gold
annotation can be used instead of automatic pre-
dictions in two system components: (1) candidate
selection and (2) verb finiteness.
Table 13 shows the performance on error identi-
fication when gold vs. automatic settings are used.
As expected, using the gold verb type is more ef-
fective than using the automatic one, both with au-
tomatic and gold candidates. The same is true for
candidate selection. For instance, the combined
model improves by 14 AAUC points (from 55.90
to 69.86) with gold candidates. These results indi-
cate that candidate selection is an important com-
ponent of the verb error correction system.
Note that compared to the performance on the
entire data set (Table 10), the performance of the
models shown here that use automatic components
is lower, since the training size is smaller. On the
other hand, because of the smaller training size,
the gain due to the type-based approach is larger
on the gold subset (19 vs. 6 AAUC points).
Finally, in Table 14, we evaluate the contribu-
tion of verb finiteness to error identification by er-
ror type. While performance varies by error, it is
clear that all errors benefit from verb typing.
</bodyText>
<table confidence="0.996239857142857">
Candidate selection Verb type prediction AAUC
None 55.90
Automatic Automatic 74.72
Gold 89.45
None 69.86
Gold Automatic 90.89
Gold 96.42
</table>
<tableCaption confidence="0.9952085">
Table 13: Gold subset: error identification with gold vs.
automatic candidates and finiteness information. Value
</tableCaption>
<table confidence="0.969831714285714">
None for verb type prediction denotes the combined model.
Error type Combined AAUC Type-based
Type-based Gold
Automatic
Agreement 86.80 88.43 89.21
Tense 18.07 25.62 26.87
Form 97.08 98.23 98.36
</table>
<tableCaption confidence="0.988928">
Table 14: Gold subset: gold vs. automatic finiteness con-
tribution to error identification by error type.
</tableCaption>
<sectionHeader confidence="0.996358" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999970047619048">
Verb errors are commonly made by ESL writers
but difficult to address due to to their diversity
and the fact that identifying verbs in (noisy) text
may itself be difficult. We develop a linguistically-
inspired approach that first identifies verb candi-
dates in noisy learner text and then makes use
of verb finiteness to identify errors and character-
ize the type of mistake. This is important, since
most errors made by non-native speakers cannot
be identified by considering only closed classes
(e.g., prepositions and articles). Our model inte-
grates a statistical machine learning approach with
a rule-based system that encodes linguistic knowl-
edge to yield the first general correction approach
to verb errors (that is, one that does not assume
prior knowledge of which mistake was made).
This work thus provides a first step in consider-
ing more general algorithmic paradigms for cor-
recting grammatical errors and paves the way for
developing models to address other “open-class”
mistakes.
</bodyText>
<sectionHeader confidence="0.997486" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996347">
The authors thank Graeme Hirst, Julia Hockenmaier, Mark
Sammons, and the anonymous reviewers for their helpful
feedback. This work was done while the first and the third
authors were at the University of Illinois. This material is
based on research sponsored by DARPA under agreement
number FA8750-13-2-0008 and by the Army Research Lab-
oratory (ARL) under agreement W911NF-09-2-0053. Any
opinions, findings, conclusions or recommendations are those
of the authors and do not necessarily reflect the view of the
agencies.
</bodyText>
<page confidence="0.998466">
365
</page>
<sectionHeader confidence="0.990123" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999602916666667">
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of 39th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 26–33,
Toulouse, France, July.
A. Carlson and I. Fette. 2007. Memory-based context-
sensitive spelling correction at web scale. In Pro-
ceedings of the IEEE International Conference on
Machine Learning and Applications (ICMLA).
A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In Proceedings of
the National Conference on Innovative Applications
of Artificial Intelligence (IAAI), pages 45–50.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect su-
pervision. In Proc. of the International Conference
on Machine Learning (ICML).
D. Dahlmeier and H. T. Ng. 2011. Grammatical er-
ror correction with alternating structure optimiza-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 915–923, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
G. Dalgish. 1985. Computer-assisted ESL research.
CALICO Journal, 2(2).
R. De Felice and S. Pulman. 2008. A classifier-based
approach to preposition and determiner error correc-
tion in L2 English. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 169–176, Manchester, UK,
August.
M. Gamon, J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings
of IJCNLP.
M. Gamon, C. Leacock, C. Brockett, W. B. Dolan,
J. Gao, D. Belenko, and A. Klementiev. 2009. Us-
ing statistical techniques and web search to correct
ESL errors. CALICO Journal, Special Issue on Au-
tomatic Analysis of Learner Language, 26(3):491–
511.
M. Gamon. 2010. Using mostly native data to correct
errors in learners’ writing. In NAACL, pages 163–
171, Los Angeles, California, June.
A. R. Golding and D. Roth. 1996. Applying Winnow
to context-sensitive spelling correction. In Proc. of
the International Conference on Machine Learning
(ICML), pages 182–190.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107–130.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115–129.
J. Hanley and B. McNeil. 1983. A method of com-
paring the areas under receiver operating character-
istic curves derived from the same cases. Radiology,
148(3):839–843.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in
the Japanese learners’ English spoken data. In The
Companion Volume to the Proceedings of 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 145–148, Sapporo, Japan, July.
T.-H. Kao, Y.-W. Chang, H. w. Chiu, T-.H. Yen, J. Bois-
son, J. c. Wu, and J.S. Chang. 2013. Conll-2013
shared task: Grammatical error correction nthu sys-
tem description. In Proceedings of the Seventeenth
Conference on Computational Natural Language
Learning: Shared Task, pages 20–25, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Pro-
cessing Systems 15 NIPS, pages 3–10. MIT Press.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan and Claypool Publish-
ers.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174–182, Columbus, Ohio,
June. Association for Computational Linguistics.
J. Lee. 2011. Verb tense generation. Social and Be-
havioral Sciences, 27:122–130.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210–1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
</reference>
<page confidence="0.987978">
366
</page>
<reference confidence="0.9996044">
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS), pages 995–1001. MIT Press.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A Comprehensive Grammar of the English
Language. Longman, New York.
A. Radford. 1988. Transformational Grammar. Cam-
bridge University Press.
R. Reichart and A. Rappoport. 2010. Tense sense
disambiguation: A new syntactic polysemy task.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
325–334, Cambridge, MA, October. Association for
Computational Linguistics.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Training
paradigms for correcting errors in grammar and us-
age. In Proceedings of the NAACL-HLT.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL, Portland, Oregon, 6. Association for Com-
putational Linguistics.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya. 2013. Automated Methods for Text
Correction. Ph.D. thesis.
T. Tajiri, M. Komachi, and Y. Matsumoto. 2012. Tense
and aspect error correction for esl learners using
global context. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 198–202,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Us-
ing parse features for preposition selection and error
detection. In ACL.
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011.
A new dataset and method for automatically grading
esol texts. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 180–189, Portland, Oregon, USA, June.
Association for Computational Linguistics.
</reference>
<page confidence="0.998361">
367
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.943882">
<title confidence="0.9995">Correcting Grammatical Verb Errors</title>
<author confidence="0.997112">Alla Rozovskaya Dan Roth Vivek Srikumar</author>
<affiliation confidence="0.999903">Columbia University University of Illinois Stanford University</affiliation>
<address confidence="0.992074">New York, NY 10115 Urbana, IL 61801 Stanford, CA</address>
<email confidence="0.995445">ar3366@columbia.edudanr@illinois.edusvivek@cs.stanford.edu</email>
<abstract confidence="0.998460896551724">Verb errors are some of the most common mistakes made by non-native writers of English but some of the least studied. The reason is that dealing with verb errors requires a new paradigm; essentially all research done on correcting grammatical errors assumes a closed set of triggers – e.g., correcting the use of prepositions or articles – but identifying mistakes in verbs necessitates identifying potentially ambiguous triggers first, and then determining the type of mistake made and correcting it. Moreover, once the verb is identified, modeling verb errors is challenging because verbs fulfill many grammatical functions, resulting in a variety of mistakes. Consequently, the little earlier work done on verb errors assumed that the error type is known in advance. We propose a linguistically-motivated approach to verb error correction that makes of the notion of finiteness identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="5764" citStr="Banko and Brill, 2001" startWordPosition="910" endWordPosition="913">ic challenges of verb error correction are better addressed by first identifying the finiteness of the verb in the error identification stage. • Within the proposed model, we describe and evaluate several methods of selecting verb candidates, an algorithm for determining the verb type, and a type-driven verb error correction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 Related Work Earlier work in ESL error correction follows the methodology of the context-sensitive spelling correction task (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., </context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 26–33, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>I Fette</author>
</authors>
<title>Memory-based contextsensitive spelling correction at web scale.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Machine Learning and Applications (ICMLA).</booktitle>
<contexts>
<context position="5812" citStr="Carlson and Fette, 2007" startWordPosition="918" endWordPosition="921">tter addressed by first identifying the finiteness of the verb in the error identification stage. • Within the proposed model, we describe and evaluate several methods of selecting verb candidates, an algorithm for determining the verb type, and a type-driven verb error correction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 Related Work Earlier work in ESL error correction follows the methodology of the context-sensitive spelling correction task (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008;</context>
</contexts>
<marker>Carlson, Fette, 2007</marker>
<rawString>A. Carlson and I. Fette. 2007. Memory-based contextsensitive spelling correction at web scale. In Proceedings of the IEEE International Conference on Machine Learning and Applications (ICMLA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>Scaling up context sensitive text correction.</title>
<date>2001</date>
<booktitle>In Proceedings of the National Conference on Innovative Applications of Artificial Intelligence (IAAI),</booktitle>
<pages>45--50</pages>
<contexts>
<context position="5786" citStr="Carlson et al., 2001" startWordPosition="914" endWordPosition="917">rror correction are better addressed by first identifying the finiteness of the verb in the error identification stage. • Within the proposed model, we describe and evaluate several methods of selecting verb candidates, an algorithm for determining the verb type, and a type-driven verb error correction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 Related Work Earlier work in ESL error correction follows the methodology of the context-sensitive spelling correction task (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>A. Carlson, J. Rosen, and D. Roth. 2001. Scaling up context sensitive text correction. In Proceedings of the National Conference on Innovative Applications of Artificial Intelligence (IAAI), pages 45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>V Srikumar</author>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Structured output learning with indirect supervision.</title>
<date>2010</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="25162" citStr="Chang et al., 2010" startWordPosition="4112" endWordPosition="4115">st for ESL correction tasks, how they compare among themselves, and how they compare to ngram based methods. Specifically, despite their intuitive appeal, language models were shown to not work well on these tasks, while the discriminative learning framework has been shown to be superior to other approaches and thus is commonly used for error correction tasks (see Sec. 2). Since we do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 reca</context>
</contexts>
<marker>Chang, Srikumar, Goldwasser, Roth, 2010</marker>
<rawString>M. Chang, V. Srikumar, D. Goldwasser, and D. Roth. 2010. Structured output learning with indirect supervision. In Proc. of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Grammatical error correction with alternating structure optimization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>915--923</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1767" citStr="Dahlmeier and Ng, 2011" startWordPosition="274" endWordPosition="277">niteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical funct</context>
<context position="6535" citStr="Dahlmeier and Ng, 2011" startWordPosition="1032" endWordPosition="1035">cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mistakes, which is the most co</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>D. Dahlmeier and H. T. Ng. 2011. Grammatical error correction with alternating structure optimization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 915–923, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>A Kilgarriff</author>
</authors>
<title>Helping Our Own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="29914" citStr="Dale and Kilgarriff, 2011" startWordPosition="4871" endWordPosition="4875">nent, we apply the appropriate correction models to those instances identified as errors. The results for identification and correction are shown in Table 12. The correction models are also finitenessaware models trained on the relevant verb instances (finite or non-finite), as predicted by the verb finiteness classifier. We evaluate the correction components by fixing a recall point in the error identification stage.7 We observe the relatively low recall obtained by the models. Error correction models tend to have low recall (see, for example, the recent shared tasks on ESL error correction (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013)). The key reason for the low recall is the error sparsity: over 95% of verbs are correct, as shown in Table 9. 7We can increase recall using a different threshold but higher precision is preferred in error correction tasks. 364 Error type P Correction F1 P Identification F1 R R Agreement 90.62 9.70 17.52 90.62 9.70 17.52 Tense 60.51 7.47 13.31 86.62 10.70 19.05 Form 81.82 16.34 27.24 83.47 16.67 27.79 Total 71.94 10.24 17.94 85.81 12.22 21.20 Table 12: Performance of the complete model after the correction stage. The results on Agreement mistakes are the s</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>R. Dale and A. Kilgarriff. 2011. Helping Our Own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>I Anisimoff</author>
<author>G Narroway</author>
</authors>
<title>A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proc. of the NAACL HLT 2012 Seventh Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Canada,</location>
<contexts>
<context position="25307" citStr="Dale et al., 2012" startWordPosition="4135" endWordPosition="4138"> appeal, language models were shown to not work well on these tasks, while the discriminative learning framework has been shown to be superior to other approaches and thus is commonly used for error correction tasks (see Sec. 2). Since we do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: 1 AAUC = 15 · 6.1 Linguistic Questions Candidate Selection Methods The contribution of the candidate selection component with respect </context>
<context position="29933" citStr="Dale et al., 2012" startWordPosition="4876" endWordPosition="4879">ate correction models to those instances identified as errors. The results for identification and correction are shown in Table 12. The correction models are also finitenessaware models trained on the relevant verb instances (finite or non-finite), as predicted by the verb finiteness classifier. We evaluate the correction components by fixing a recall point in the error identification stage.7 We observe the relatively low recall obtained by the models. Error correction models tend to have low recall (see, for example, the recent shared tasks on ESL error correction (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013)). The key reason for the low recall is the error sparsity: over 95% of verbs are correct, as shown in Table 9. 7We can increase recall using a different threshold but higher precision is preferred in error correction tasks. 364 Error type P Correction F1 P Identification F1 R R Agreement 90.62 9.70 17.52 90.62 9.70 17.52 Tense 60.51 7.47 13.31 86.62 10.70 19.05 Form 81.82 16.34 27.24 83.47 16.67 27.79 Total 71.94 10.24 17.94 85.81 12.22 21.20 Table 12: Performance of the complete model after the correction stage. The results on Agreement mistakes are the same, since Agreemen</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>R. Dale, I. Anisimoff, and G. Narroway. 2012. A report on the preposition and determiner error correction shared task. In Proc. of the NAACL HLT 2012 Seventh Workshop on Innovative Use of NLP for Building Educational Applications, Montreal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dalgish</author>
</authors>
<date>1985</date>
<journal>Computer-assisted ESL research. CALICO Journal,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="6098" citStr="Dalgish, 1985" startWordPosition="961" endWordPosition="962">notate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 Related Work Earlier work in ESL error correction follows the methodology of the context-sensitive spelling correction task (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only cons</context>
</contexts>
<marker>Dalgish, 1985</marker>
<rawString>G. Dalgish. 1985. Computer-assisted ESL research. CALICO Journal, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
<author>S Pulman</author>
</authors>
<title>A classifier-based approach to preposition and determiner error correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>169--176</pages>
<location>Manchester, UK,</location>
<marker>De Felice, Pulman, 2008</marker>
<rawString>R. De Felice and S. Pulman. 2008. A classifier-based approach to preposition and determiner error correction in L2 English. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 169–176, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for ESL error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="1677" citStr="Gamon et al., 2008" startWordPosition="260" endWordPosition="263">y-motivated approach to verb error correction that makes use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 20</context>
<context position="6431" citStr="Gamon et al., 2008" startWordPosition="1016" endWordPosition="1019"> Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreemen</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. Dolan, D. Belenko, and L. Vanderwende. 2008. Using contextual speller techniques and language modeling for ESL error correction. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>C Leacock</author>
<author>C Brockett</author>
<author>W B Dolan</author>
<author>J Gao</author>
<author>D Belenko</author>
<author>A Klementiev</author>
</authors>
<title>Using statistical techniques and web search to correct ESL errors.</title>
<date>2009</date>
<journal>CALICO Journal, Special Issue on Automatic Analysis of Learner Language,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>511</pages>
<contexts>
<context position="1902" citStr="Gamon et al., 2009" startWordPosition="297" endWordPosition="300"> that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors. These difficulties have led all previous work on verb mistakes to assu</context>
<context position="6800" citStr="Gamon et al. (2009)" startWordPosition="1074" endWordPosition="1077">fication problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mistakes, which is the most common error category for ESL learners (40% of all verb errors, Sec. 3). Tajiri et al. (2012) considered only tense mistakes. In the above studies, it was assumed that the type of mistake that needs to be corrected is known, and irrelevant verb errors were excluded (</context>
</contexts>
<marker>Gamon, Leacock, Brockett, Dolan, Gao, Belenko, Klementiev, 2009</marker>
<rawString>M. Gamon, C. Leacock, C. Brockett, W. B. Dolan, J. Gao, D. Belenko, and A. Klementiev. 2009. Using statistical techniques and web search to correct ESL errors. CALICO Journal, Special Issue on Automatic Analysis of Learner Language, 26(3):491– 511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing.</title>
<date>2010</date>
<booktitle>In NAACL,</booktitle>
<pages>163--171</pages>
<location>Los Angeles, California,</location>
<contexts>
<context position="1714" citStr="Gamon, 2010" startWordPosition="268" endWordPosition="269">n that makes use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex l</context>
<context position="25342" citStr="Gamon, 2010" startWordPosition="4142" endWordPosition="4143"> work well on these tasks, while the discriminative learning framework has been shown to be superior to other approaches and thus is commonly used for error correction tasks (see Sec. 2). Since we do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: 1 AAUC = 15 · 6.1 Linguistic Questions Candidate Selection Methods The contribution of the candidate selection component with respect to error identification is evaluate</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>M. Gamon. 2010. Using mostly native data to correct errors in learners’ writing. In NAACL, pages 163– 171, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>Applying Winnow to context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Proc. of the International Conference on Machine Learning (ICML),</booktitle>
<pages>182--190</pages>
<contexts>
<context position="5717" citStr="Golding and Roth, 1996" startWordPosition="902" endWordPosition="905">take type; in doing that we show that the specific challenges of verb error correction are better addressed by first identifying the finiteness of the verb in the error identification stage. • Within the proposed model, we describe and evaluate several methods of selecting verb candidates, an algorithm for determining the verb type, and a type-driven verb error correction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 Related Work Earlier work in ESL error correction follows the methodology of the context-sensitive spelling correction task (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n mos</context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>A. R. Golding and D. Roth. 1996. Applying Winnow to context-sensitive spelling correction. In Proc. of the International Conference on Machine Learning (ICML), pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow based approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="5741" citStr="Golding and Roth, 1999" startWordPosition="906" endWordPosition="909"> we show that the specific challenges of verb error correction are better addressed by first identifying the finiteness of the verb in the error identification stage. • Within the proposed model, we describe and evaluate several methods of selecting verb candidates, an algorithm for determining the verb type, and a type-driven verb error correction system. • We annotate a subset of the FCE data set with gold verb candidates and gold verb type.1 2 Related Work Earlier work in ESL error correction follows the methodology of the context-sensitive spelling correction task (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepo</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow based approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="1632" citStr="Han et al., 2006" startWordPosition="252" endWordPosition="255">nown in advance. We propose a linguistically-motivated approach to verb error correction that makes use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make mor</context>
<context position="6386" citStr="Han et al., 2006" startWordPosition="1007" endWordPosition="1010">son et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined wi</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N. Han, M. Chodorow, and C. Leacock. 2006. Detecting errors in English article usage by non-native speakers. Journal of Natural Language Engineering, 12(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hanley</author>
<author>B McNeil</author>
</authors>
<title>A method of comparing the areas under receiver operating characteristic curves derived from the same cases.</title>
<date>1983</date>
<journal>Radiology,</journal>
<volume>148</volume>
<issue>3</issue>
<contexts>
<context position="25576" citStr="Hanley and McNeil, 1983" startWordPosition="4180" endWordPosition="4183">ithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: 1 AAUC = 15 · 6.1 Linguistic Questions Candidate Selection Methods The contribution of the candidate selection component with respect to error identification is evaluated in Table 8, using the methods presented in Sec. 5.1. Overall, Precision(i). i=1 1 5 363 Recall of candidate Combined AAUC selection method (%) Type-based (1) (83.00) 73.38 79.49 (2) (91.96) 80.36 86.48 (3) (95.50) 81.39 87.05 (4) (9</context>
</contexts>
<marker>Hanley, McNeil, 1983</marker>
<rawString>J. Hanley and B. McNeil. 1983. A method of comparing the areas under receiver operating characteristic curves derived from the same cases. Radiology, 148(3):839–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<title>Automatic error detection in the Japanese learners’ English spoken data.</title>
<date>2003</date>
<booktitle>In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--148</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="6368" citStr="Izumi et al., 2003" startWordPosition="1003" endWordPosition="1006">nd Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on</context>
<context position="8989" citStr="Izumi et al., 2003" startWordPosition="1443" endWordPosition="1446">versity of Illinois system obtained the highest score on the verb sub-task, even though 359 Tag Error type Rel. freq. (%) TV Tense 40.0 FV Form 22.3 AGV Verb-subject agreement 11.5 MV Missing verb 11.7 UV Unneccesary verb 7.3 IV Inflection 5.4 DV Derivation 1.8 Total 6640 Table 1: Grammatical verb errors in FCE. all teams used similar resources (Ng et al., 2013). 3 Verb Errors in ESL Writing Verb-related errors are very prominent among non-native English speakers: grammatical misuse of verbs constitutes one of the most common errors in several learner corpora, including those previously used (Izumi et al., 2003; Lee and Seneff, 2008) and the one employed in this work. We study verb errors using the FCE corpus (Yannakoudakis et al., 2011). The corpus possesses several desirable characteristics: it is large (500,000 words), has been annotated by native English speakers, and contains data by learners of multiple first-language backgrounds. The FCE corpus contains 5056 determiner errors, 5347 preposition errors, and 6640 grammatical verb mistakes (Table 1). 3.1 Verb Finiteness There are many grammatical categories for which English verbs can be marked. The linguistic notion of verb finiteness or verb ty</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic error detection in the Japanese learners’ English spoken data. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 145–148, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T-H Kao</author>
<author>Y-W Chang</author>
<author>H w Chiu</author>
<author>T- H Yen</author>
<author>J Boisson</author>
<author>J c Wu</author>
<author>J S Chang</author>
</authors>
<title>Conll-2013 shared task: Grammatical error correction nthu system description.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Kao, Chang, Chiu, Yen, Boisson, Wu, Chang, 2013</marker>
<rawString>T.-H. Kao, Y.-W. Chang, H. w. Chiu, T-.H. Yen, J. Boisson, J. c. Wu, and J.S. Chang. 2013. Conll-2013 shared task: Grammatical error correction nthu system description. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 20–25, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15 NIPS,</booktitle>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18983" citStr="Klein and Manning, 2003" startWordPosition="3067" endWordPosition="3070"> use of linguistically-motivated rules (Table 6). The algorithm covers about 92% of all verb candidates, abstaining on the remaining highly-ambiguous 8%. The evaluation of the method on the gold subset (last column in Table 6) shows that despite its simplicity, this method is highly effective: 98% on correct verbs and over 89% on errors. 5.3 Features The baseline features are word n-grams in the 4- word window around the verb instance. Additional features are intended to characterize a given error type and are selected based on previous studies: for Agreement and Form errors, we use a parser (Klein and Manning, 2003) and define features that reflect dependency relations between the verb and its neighbors. We denote these features by syntax. Syntactic knowledge via tree patterns has been shown useful for Agreement mistakes (Lee and Seneff, 2008). Features for Tense include temporal adverbs in the sentence and tenses of other verbs in the sentence and are similar to the features used in other verb classification tasks (Reichart and Rappoport, 2010; Lee, 2011; Tajiri et al., 2012). The features are shown in Table 7. 5.4 Error Identification The goal of this stage is to identify errors and to predict their ty</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15 NIPS, pages 3–10. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>M Gamon</author>
<author>J Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners.</title>
<date>2010</date>
<publisher>Morgan and Claypool Publishers.</publisher>
<contexts>
<context position="6121" citStr="Leacock et al., 2010" startWordPosition="963" endWordPosition="967"> of the FCE data set with gold verb candidates and gold verb type.1 2 Related Work Earlier work in ESL error correction follows the methodology of the context-sensitive spelling correction task (Golding and Roth, 1996; Golding and Roth, 1999; Banko and Brill, 2001; Carlson et al., 2001; Carlson and Fette, 2007). Most of the effort in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Morgan and Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>S Seneff</author>
</authors>
<title>Correcting misuse of verb forms.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>174--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1882" citStr="Lee and Seneff, 2008" startWordPosition="293" endWordPosition="296">hese mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors. These difficulties have led all previous work on v</context>
<context position="6918" citStr="Lee and Seneff (2008)" startWordPosition="1090" endWordPosition="1093">les or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mistakes, which is the most common error category for ESL learners (40% of all verb errors, Sec. 3). Tajiri et al. (2012) considered only tense mistakes. In the above studies, it was assumed that the type of mistake that needs to be corrected is known, and irrelevant verb errors were excluded (e.g., Tajiri et al. (2012) addressed only tense mistakes and excluded from the evaluation other kinds of verb errors).</context>
<context position="9012" citStr="Lee and Seneff, 2008" startWordPosition="1447" endWordPosition="1450">system obtained the highest score on the verb sub-task, even though 359 Tag Error type Rel. freq. (%) TV Tense 40.0 FV Form 22.3 AGV Verb-subject agreement 11.5 MV Missing verb 11.7 UV Unneccesary verb 7.3 IV Inflection 5.4 DV Derivation 1.8 Total 6640 Table 1: Grammatical verb errors in FCE. all teams used similar resources (Ng et al., 2013). 3 Verb Errors in ESL Writing Verb-related errors are very prominent among non-native English speakers: grammatical misuse of verbs constitutes one of the most common errors in several learner corpora, including those previously used (Izumi et al., 2003; Lee and Seneff, 2008) and the one employed in this work. We study verb errors using the FCE corpus (Yannakoudakis et al., 2011). The corpus possesses several desirable characteristics: it is large (500,000 words), has been annotated by native English speakers, and contains data by learners of multiple first-language backgrounds. The FCE corpus contains 5056 determiner errors, 5347 preposition errors, and 6640 grammatical verb mistakes (Table 1). 3.1 Verb Finiteness There are many grammatical categories for which English verbs can be marked. The linguistic notion of verb finiteness or verb type (Radford, 1988; Quir</context>
<context position="15950" citStr="Lee and Seneff, 2008" startWordPosition="2565" endWordPosition="2568">ose errors that are missed at this stage have no chance of being detected. We implement four candidate selection methods. Method (1) extracts all verbs heading a verb phrase, as identified by a shallow parser (Punyakanok and Roth, 2001).3 Method (2) also includes words tagged with one of the verb tags: {VB, VBN, VBG, VBD, VBP, VBZ} predicted by the POS tagger.4 However, relying on the POS information is not good enough, since the POS tagger performance on ESL data is known to be suboptimal (Nagata et al., 2011). For example, verbs lacking agreement markers are likely to be mistagged as nouns (Lee and Seneff, 2008). Methods (3) and (4) address the problem of pre-processing errors. Method (3) adds words that are on the list of valid English verb lemmas; the lemma list is constructed using a POS-tagged version of the NYT section of the Gigaword corpus and contains about 2,600 of frequently-occurring words tagged as VB; for example, (3) will add shop but not shopping, but (4) will add both. For methods (3) and (4), we developed verbMorph,5 a tool that performs morphological analysis on verbs and is used to lemmatize verbs and to generate morphological variants. The module makes uses of (1) the verb lemma l</context>
<context position="19215" citStr="Lee and Seneff, 2008" startWordPosition="3103" endWordPosition="3106">at despite its simplicity, this method is highly effective: 98% on correct verbs and over 89% on errors. 5.3 Features The baseline features are word n-grams in the 4- word window around the verb instance. Additional features are intended to characterize a given error type and are selected based on previous studies: for Agreement and Form errors, we use a parser (Klein and Manning, 2003) and define features that reflect dependency relations between the verb and its neighbors. We denote these features by syntax. Syntactic knowledge via tree patterns has been shown useful for Agreement mistakes (Lee and Seneff, 2008). Features for Tense include temporal adverbs in the sentence and tenses of other verbs in the sentence and are similar to the features used in other verb classification tasks (Reichart and Rappoport, 2010; Lee, 2011; Tajiri et al., 2012). The features are shown in Table 7. 5.4 Error Identification The goal of this stage is to identify errors and to predict their type. We define a linear model where, given a verb, a weight vector w assigns a score to each label in the label space {Correct, Form, Agreement, Tense}. The prediction of the classifier is the label with the highest score. The baseli</context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>J. Lee and S. Seneff. 2008. Correcting misuse of verb forms. In ACL, pages 174–182, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
</authors>
<title>Verb tense generation. Social and Behavioral Sciences,</title>
<date>2011</date>
<pages>27--122</pages>
<contexts>
<context position="19431" citStr="Lee, 2011" startWordPosition="3141" endWordPosition="3142"> intended to characterize a given error type and are selected based on previous studies: for Agreement and Form errors, we use a parser (Klein and Manning, 2003) and define features that reflect dependency relations between the verb and its neighbors. We denote these features by syntax. Syntactic knowledge via tree patterns has been shown useful for Agreement mistakes (Lee and Seneff, 2008). Features for Tense include temporal adverbs in the sentence and tenses of other verbs in the sentence and are similar to the features used in other verb classification tasks (Reichart and Rappoport, 2010; Lee, 2011; Tajiri et al., 2012). The features are shown in Table 7. 5.4 Error Identification The goal of this stage is to identify errors and to predict their type. We define a linear model where, given a verb, a weight vector w assigns a score to each label in the label space {Correct, Form, Agreement, Tense}. The prediction of the classifier is the label with the highest score. The baseline error identification model, called combined, is agnostic to the type of the verb. In the combined model, for each verb v and label l, we generate a feature vector, O(v, l) and the best label is predicted as arg ma</context>
</contexts>
<marker>Lee, 2011</marker>
<rawString>J. Lee. 2011. Verb tense generation. Social and Behavioral Sciences, 27:122–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>E Whittaker</author>
<author>V Sheinman</author>
</authors>
<title>Creating a manually error-tagged and shallow-parsed learner corpus.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>1210--1219</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2280" citStr="Nagata et al., 2011" startWordPosition="365" endWordPosition="368">amon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors. These difficulties have led all previous work on verb mistakes to assume prior knowledge of the mistake type; however, identifying the specific category of a verb error is nontrivial, since the surface form of the verb may be ambiguous, especially when that verb is used incorrectly. Consider the following examples of verb mistakes: 1. “We discusses*/discuss this every time.” 2. “I will be lucky if I {will find}*/find something that fits.” 3. “T</context>
<context position="15845" citStr="Nagata et al., 2011" startWordPosition="2548" endWordPosition="2551">). Candidate selection is a crucial step for models that correct mistakes on open-class words because those errors that are missed at this stage have no chance of being detected. We implement four candidate selection methods. Method (1) extracts all verbs heading a verb phrase, as identified by a shallow parser (Punyakanok and Roth, 2001).3 Method (2) also includes words tagged with one of the verb tags: {VB, VBN, VBG, VBD, VBP, VBZ} predicted by the POS tagger.4 However, relying on the POS information is not good enough, since the POS tagger performance on ESL data is known to be suboptimal (Nagata et al., 2011). For example, verbs lacking agreement markers are likely to be mistagged as nouns (Lee and Seneff, 2008). Methods (3) and (4) address the problem of pre-processing errors. Method (3) adds words that are on the list of valid English verb lemmas; the lemma list is constructed using a POS-tagged version of the NYT section of the Gigaword corpus and contains about 2,600 of frequently-occurring words tagged as VB; for example, (3) will add shop but not shopping, but (4) will add both. For methods (3) and (4), we developed verbMorph,5 a tool that performs morphological analysis on verbs and is used</context>
</contexts>
<marker>Nagata, Whittaker, Sheinman, 2011</marker>
<rawString>R. Nagata, E. Whittaker, and V. Sheinman. 2011. Creating a manually error-tagged and shallow-parsed learner corpus. In ACL, pages 1210–1219, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hadiwinoto</author>
<author>J Tetreault</author>
</authors>
<title>The CoNLL-2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proc. of the Seventeenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<marker>Hadiwinoto, Tetreault, 2013</marker>
<rawString>H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and J. Tetreault. 2013. The CoNLL-2013 shared task on grammatical error correction. In Proc. of the Seventeenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS),</booktitle>
<pages>995--1001</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="15565" citStr="Punyakanok and Roth, 2001" startWordPosition="2498" endWordPosition="2501">ate selection and verb finiteness prediction. 5.1 Candidate Selection This stage selects the set of verb instances that are presented as input to the classifier. A verb instance refers to the verb, including its auxiliaries or the infinitive marker (e.g. “found”, “will find”, “to find”). Candidate selection is a crucial step for models that correct mistakes on open-class words because those errors that are missed at this stage have no chance of being detected. We implement four candidate selection methods. Method (1) extracts all verbs heading a verb phrase, as identified by a shallow parser (Punyakanok and Roth, 2001).3 Method (2) also includes words tagged with one of the verb tags: {VB, VBN, VBG, VBD, VBP, VBZ} predicted by the POS tagger.4 However, relying on the POS information is not good enough, since the POS tagger performance on ESL data is known to be suboptimal (Nagata et al., 2011). For example, verbs lacking agreement markers are likely to be mistagged as nouns (Lee and Seneff, 2008). Methods (3) and (4) address the problem of pre-processing errors. Method (3) adds words that are on the list of valid English verb lemmas; the lemma list is constructed using a POS-tagged version of the NYT sectio</context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In The Conference on Advances in Neural Information Processing Systems (NIPS), pages 995–1001. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="4338" citStr="Quirk et al., 1985" startWordPosition="683" endWordPosition="686">es: confusing the infinitive and gerund forms in (3) and including an inflection on an infinitive verb in (4). This paper addresses the specific challenges of verb error correction that have not been addressed previously – identifying candidates for mistakes and determining which class of errors is present, before proceeding to correct the error. The experimental results show that our linguisticallymotivated approach benefits verb error correction. In particular, in order to determine the error type, we build on the notion of verb finiteness to distinguish between finite and non-finite verbs (Quirk et al., 1985), that correspond to Agreement and Tense mistakes (examples (1) and (2) above) and Form mistakes (examples (3) and (4) above), respectively (see Sec. 3). The approach presented in this work was evaluated empirically and competitively in the context of the CoNLL shared task on error correction (Ng et al., 2013) where it was implemented as part of the highest-scoring University of Illinois system (Rozovskaya et al., 2013) and demonstrated superior performance on the verb error correction sub-task. This paper makes the following contributions: • We present a holistic, linguistically-motivated fra</context>
<context position="9627" citStr="Quirk et al., 1985" startWordPosition="1544" endWordPosition="1547">008) and the one employed in this work. We study verb errors using the FCE corpus (Yannakoudakis et al., 2011). The corpus possesses several desirable characteristics: it is large (500,000 words), has been annotated by native English speakers, and contains data by learners of multiple first-language backgrounds. The FCE corpus contains 5056 determiner errors, 5347 preposition errors, and 6640 grammatical verb mistakes (Table 1). 3.1 Verb Finiteness There are many grammatical categories for which English verbs can be marked. The linguistic notion of verb finiteness or verb type (Radford, 1988; Quirk et al., 1985) distinguishes between verbs that function on their own in a clause as main verbs (finite) and those that do not (non-finite). Grammatical properties associated with each group are mutually exclusive: tense and agreement markers, for example, do not apply to non-finite verbs; nonfinite verbs are not marked for many grammatical functions but may appear in several forms. The most common verb problems for ESL learners – Tense, Agreement, non-finite Form – involve verbs both in finite and non-finite roles. Table 2 illustrates contexts that license finite and non-finite verbs. Our intuition is that</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Radford</author>
</authors>
<title>Transformational Grammar.</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9606" citStr="Radford, 1988" startWordPosition="1542" endWordPosition="1543">e and Seneff, 2008) and the one employed in this work. We study verb errors using the FCE corpus (Yannakoudakis et al., 2011). The corpus possesses several desirable characteristics: it is large (500,000 words), has been annotated by native English speakers, and contains data by learners of multiple first-language backgrounds. The FCE corpus contains 5056 determiner errors, 5347 preposition errors, and 6640 grammatical verb mistakes (Table 1). 3.1 Verb Finiteness There are many grammatical categories for which English verbs can be marked. The linguistic notion of verb finiteness or verb type (Radford, 1988; Quirk et al., 1985) distinguishes between verbs that function on their own in a clause as main verbs (finite) and those that do not (non-finite). Grammatical properties associated with each group are mutually exclusive: tense and agreement markers, for example, do not apply to non-finite verbs; nonfinite verbs are not marked for many grammatical functions but may appear in several forms. The most common verb problems for ESL learners – Tense, Agreement, non-finite Form – involve verbs both in finite and non-finite roles. Table 2 illustrates contexts that license finite and non-finite verbs. </context>
</contexts>
<marker>Radford, 1988</marker>
<rawString>A. Radford. 1988. Transformational Grammar. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichart</author>
<author>A Rappoport</author>
</authors>
<title>Tense sense disambiguation: A new syntactic polysemy task.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>325--334</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="19420" citStr="Reichart and Rappoport, 2010" startWordPosition="3137" endWordPosition="3140">tance. Additional features are intended to characterize a given error type and are selected based on previous studies: for Agreement and Form errors, we use a parser (Klein and Manning, 2003) and define features that reflect dependency relations between the verb and its neighbors. We denote these features by syntax. Syntactic knowledge via tree patterns has been shown useful for Agreement mistakes (Lee and Seneff, 2008). Features for Tense include temporal adverbs in the sentence and tenses of other verbs in the sentence and are similar to the features used in other verb classification tasks (Reichart and Rappoport, 2010; Lee, 2011; Tajiri et al., 2012). The features are shown in Table 7. 5.4 Error Identification The goal of this stage is to identify errors and to predict their type. We define a linear model where, given a verb, a weight vector w assigns a score to each label in the label space {Correct, Form, Agreement, Tense}. The prediction of the classifier is the label with the highest score. The baseline error identification model, called combined, is agnostic to the type of the verb. In the combined model, for each verb v and label l, we generate a feature vector, O(v, l) and the best label is predicte</context>
</contexts>
<marker>Reichart, Rappoport, 2010</marker>
<rawString>R. Reichart and A. Rappoport. 2010. Tense sense disambiguation: A new syntactic polysemy task. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 325–334, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Annotating ESL errors: Challenges and rewards.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="1741" citStr="Rozovskaya and Roth, 2010" startWordPosition="270" endWordPosition="273">use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill</context>
<context position="6482" citStr="Rozovskaya and Roth, 2010" startWordPosition="1024" endWordPosition="1027">so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010a. Annotating ESL errors: Challenges and rewards. In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Training paradigms for correcting errors in grammar and usage.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT.</booktitle>
<contexts>
<context position="1741" citStr="Rozovskaya and Roth, 2010" startWordPosition="270" endWordPosition="273">use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill</context>
<context position="6482" citStr="Rozovskaya and Roth, 2010" startWordPosition="1024" endWordPosition="1027">so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. Howeve</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010b. Training paradigms for correcting errors in grammar and usage. In Proceedings of the NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Algorithm selection and model adaptation for esl correction tasks.</title>
<date>2011</date>
<booktitle>In ACL, Portland, Oregon, 6. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6510" citStr="Rozovskaya and Roth, 2011" startWordPosition="1028" endWordPosition="1031">ion is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mista</context>
<context position="25031" citStr="Rozovskaya and Roth (2011)" startWordPosition="4089" endWordPosition="4092">omatic; (ii) performance comparison by error type Learning Framework There is a lot of understanding for which algorithmic methods work best for ESL correction tasks, how they compare among themselves, and how they compare to ngram based methods. Specifically, despite their intuitive appeal, language models were shown to not work well on these tasks, while the discriminative learning framework has been shown to be superior to other approaches and thus is commonly used for error correction tasks (see Sec. 2). Since we do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary stat</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>A. Rozovskaya and D. Roth. 2011. Algorithm selection and model adaptation for esl correction tasks. In ACL, Portland, Oregon, 6. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>K-W Chang</author>
<author>M Sammons</author>
<author>D Roth</author>
</authors>
<date>2013</date>
<booktitle>The University of Illinois system in the CoNLL-2013 shared task. In CoNLL Shared Task.</booktitle>
<contexts>
<context position="4761" citStr="Rozovskaya et al., 2013" startWordPosition="752" endWordPosition="755">ch benefits verb error correction. In particular, in order to determine the error type, we build on the notion of verb finiteness to distinguish between finite and non-finite verbs (Quirk et al., 1985), that correspond to Agreement and Tense mistakes (examples (1) and (2) above) and Form mistakes (examples (3) and (4) above), respectively (see Sec. 3). The approach presented in this work was evaluated empirically and competitively in the context of the CoNLL shared task on error correction (Ng et al., 2013) where it was implemented as part of the highest-scoring University of Illinois system (Rozovskaya et al., 2013) and demonstrated superior performance on the verb error correction sub-task. This paper makes the following contributions: • We present a holistic, linguistically-motivated framework for correcting grammatical verb mistakes; our approach “starts from scratch” without any knowledge of which mistakes should be corrected or of the mistake type; in doing that we show that the specific challenges of verb error correction are better addressed by first identifying the finiteness of the verb in the error identification stage. • Within the proposed model, we describe and evaluate several methods of se</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>A. Rozovskaya, K.-W. Chang, M. Sammons, and D. Roth. 2013. The University of Illinois system in the CoNLL-2013 shared task. In CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
</authors>
<title>Automated Methods for Text Correction.</title>
<date>2013</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="25544" citStr="Rozovskaya (2013)" startWordPosition="4177" endWordPosition="4178"> do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: 1 AAUC = 15 · 6.1 Linguistic Questions Candidate Selection Methods The contribution of the candidate selection component with respect to error identification is evaluated in Table 8, using the methods presented in Sec. 5.1. Overall, Precision(i). i=1 1 5 363 Recall of candidate Combined AAUC selection method (%) Type-based (1) (83.00) 73.38 79.49 (2) (91.96) 80.36 86.4</context>
</contexts>
<marker>Rozovskaya, 2013</marker>
<rawString>A. Rozovskaya. 2013. Automated Methods for Text Correction. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tajiri</author>
<author>M Komachi</author>
<author>Y Matsumoto</author>
</authors>
<title>Tense and aspect error correction for esl learners using global context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>198--202</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="1924" citStr="Tajiri et al., 2012" startWordPosition="301" endWordPosition="304">ally-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are more complex linguistically: they fulfill several grammatical functions, and these different roles imply different types of errors. These difficulties have led all previous work on verb mistakes to assume prior knowledge of </context>
<context position="7226" citStr="Tajiri et al. (2012)" startWordPosition="1141" endWordPosition="1144">error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types of verb form errors. However, they excluded tense mistakes, which is the most common error category for ESL learners (40% of all verb errors, Sec. 3). Tajiri et al. (2012) considered only tense mistakes. In the above studies, it was assumed that the type of mistake that needs to be corrected is known, and irrelevant verb errors were excluded (e.g., Tajiri et al. (2012) addressed only tense mistakes and excluded from the evaluation other kinds of verb errors). In other words, it was assumed that part of the task was solved. But, unlike in article and preposition error correction where the type of mistake is known based on the surface form of the word, in verb error correction, it is not obvious. The key distinction of our work is that we propose a holistic appro</context>
<context position="19453" citStr="Tajiri et al., 2012" startWordPosition="3143" endWordPosition="3146">o characterize a given error type and are selected based on previous studies: for Agreement and Form errors, we use a parser (Klein and Manning, 2003) and define features that reflect dependency relations between the verb and its neighbors. We denote these features by syntax. Syntactic knowledge via tree patterns has been shown useful for Agreement mistakes (Lee and Seneff, 2008). Features for Tense include temporal adverbs in the sentence and tenses of other verbs in the sentence and are similar to the features used in other verb classification tasks (Reichart and Rappoport, 2010; Lee, 2011; Tajiri et al., 2012). The features are shown in Table 7. 5.4 Error Identification The goal of this stage is to identify errors and to predict their type. We define a linear model where, given a verb, a weight vector w assigns a score to each label in the label space {Correct, Form, Agreement, Tense}. The prediction of the classifier is the label with the highest score. The baseline error identification model, called combined, is agnostic to the type of the verb. In the combined model, for each verb v and label l, we generate a feature vector, O(v, l) and the best label is predicted as arg max wT O(v,l). l The com</context>
<context position="21269" citStr="Tajiri et al., 2012" startWordPosition="3470" endWordPosition="3473">e t, we define features O(v, t, l) for each label l. Thus, the label is predicted as arg max l wT O(v, t, l). 5.5 Error Correction The correction module consists of three components, one for each type of mistake. Given the output of the error identification model, the appropriate correction component is run for each instance predicted to be a mistake.6 The verb finiteness prediction is used to select finite instances for training the Agreement and Tense components and non-finite – for the Form component. The label space for Tense specifies tense and aspect properties of the English verbs (see Tajiri et al., 2012 for more detail), the Agreement component specifies the person and number properties, while the Form component includes the commonly confusable non-finite English forms (see Table 2). These components are trained as multiclass classifiers. 6We assume that each verb contains at most one mistake. Less than 1% of all erroneous verbs have more than one error present. 362 A verb is Non-Finite if any of the following hold: A verb is Finite if any of the following hold Accuracy on Correct Erroneous verbs verbs (1) All verbs identified by shallow parser (1) [numTokens = 2] ∧ [firstToken = to] (2) can</context>
<context position="25364" citStr="Tajiri et al., 2012" startWordPosition="4144" endWordPosition="4147"> these tasks, while the discriminative learning framework has been shown to be superior to other approaches and thus is commonly used for error correction tasks (see Sec. 2). Since we do not address the algorithmic aspect of the problem, we refer the reader to Rozovskaya and Roth (2011) for a discussion of these issues. We train all our models with the SVM learning algorithm implemented in JLIS (Chang et al., 2010). Evaluation We report both Precision/Recall curves and AAUC (as a summary). Error correction is generally evaluated using F1 (Dale et al., 2012); Precision and Recall (Gamon, 2010; Tajiri et al., 2012); or Average Area Under Curve (AAUC) (Rozovskaya and Roth, 2011). For a discussion on these metrics with respect to error correction tasks, we refer the reader to Rozovskaya (2013). AAUC (Hanley and McNeil, 1983)) is a measure commonly used to generate a summary statistic, computed as an average precision value over a range of recall points. In this paper, AAUC is computed over the first 15 recall points: 1 AAUC = 15 · 6.1 Linguistic Questions Candidate Selection Methods The contribution of the candidate selection component with respect to error identification is evaluated in Table 8, using th</context>
</contexts>
<marker>Tajiri, Komachi, Matsumoto, 2012</marker>
<rawString>T. Tajiri, M. Komachi, and Y. Matsumoto. 2012. Tense and aspect error correction for esl learners using global context. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 198–202, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>J Foster</author>
<author>M Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1701" citStr="Tetreault et al., 2010" startWordPosition="264" endWordPosition="267"> to verb error correction that makes use of the notion of verb finiteness to identify triggers and types of mistakes, before using a statistical machine learning approach to correct these mistakes. We show that the linguistically-informed model significantly improves the accuracy of the verb correction approach. 1 Introduction We address the problem of correcting grammatical verb mistakes made by English as a Second Language (ESL) learners. Recent work in ESL error correction has focused on errors in article and preposition usage (Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Gamon, 2010; Rozovskaya and Roth, 2010b; Dahlmeier and Ng, 2011). While verb errors occur as often as article and preposition mistakes, with a few exceptions (Lee and Seneff, 2008; Gamon et al., 2009; Tajiri et al., 2012), there has been little work on verbs. There are two reasons for why it is difficult to deal with verb mistakes. First, in contrast to articles and prepositions, verbs are more difficult to identify in text, as they can often be confused with other parts of speech, and processing tools are known to make more errors on noisy ESL data (Nagata et al., 2011). Second, verbs are m</context>
<context position="6455" citStr="Tetreault et al., 2010" startWordPosition="1020" endWordPosition="1023">in ESL error correction so far has been 1The annotation is available at http://cogcomp.cs.illinois. edu/page/publication view/743 on article and preposition usage errors, as these are some of the most common mistakes among non-native English speakers (Dalgish, 1985; Leacock et al., 2010). These phenomena are generally modeled as multiclass classification problems: a single classifier is trained for a given error type where the set of classes includes all articles or the top n most frequent English prepositions (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Gamon et al., 2008; Tetreault et al., 2010; Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011). Mistakes on verbs have attracted significantly less attention in the error correction literature. Moreover, the little earlier work done on verb errors only considered subsets of these errors and assumed the error sub-type is known in advance. Gamon et al. (2009) mentioned a model for learning gerund/infinitive confusions and auxiliary verb presence/choice. Lee and Seneff (2008) proposed an approach based on pattern matching on trees combined with word n-gram counts for correcting agreement misuse and some types </context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse features for preposition selection and error detection. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yannakoudakis</author>
<author>T Briscoe</author>
<author>B Medlock</author>
</authors>
<title>A new dataset and method for automatically grading esol texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>180--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="9118" citStr="Yannakoudakis et al., 2011" startWordPosition="1467" endWordPosition="1470">) TV Tense 40.0 FV Form 22.3 AGV Verb-subject agreement 11.5 MV Missing verb 11.7 UV Unneccesary verb 7.3 IV Inflection 5.4 DV Derivation 1.8 Total 6640 Table 1: Grammatical verb errors in FCE. all teams used similar resources (Ng et al., 2013). 3 Verb Errors in ESL Writing Verb-related errors are very prominent among non-native English speakers: grammatical misuse of verbs constitutes one of the most common errors in several learner corpora, including those previously used (Izumi et al., 2003; Lee and Seneff, 2008) and the one employed in this work. We study verb errors using the FCE corpus (Yannakoudakis et al., 2011). The corpus possesses several desirable characteristics: it is large (500,000 words), has been annotated by native English speakers, and contains data by learners of multiple first-language backgrounds. The FCE corpus contains 5056 determiner errors, 5347 preposition errors, and 6640 grammatical verb mistakes (Table 1). 3.1 Verb Finiteness There are many grammatical categories for which English verbs can be marked. The linguistic notion of verb finiteness or verb type (Radford, 1988; Quirk et al., 1985) distinguishes between verbs that function on their own in a clause as main verbs (finite) </context>
</contexts>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011. A new dataset and method for automatically grading esol texts. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 180–189, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>