<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.626297">
Performance Prediction for Exponential Language Models
</title>
<note confidence="0.92477">
Stanley F. Chen
IBM T.J. Watson Research Center
P.O. Box 218, Yorktown Heights, NY 10598
</note>
<email confidence="0.993749">
stanchen@watson.ibm.com
</email>
<sectionHeader confidence="0.99471" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956263157895">
We investigate the task of performance pre-
diction for language models belonging to the
exponential family. First, we attempt to em-
pirically discover a formula for predicting test
set cross-entropy for n-gram language mod-
els. We build models over varying domains,
data set sizes, and n-gram orders, and perform
linear regression to see whether we can model
test set performance as a simple function of
training set performance and various model
statistics. Remarkably, we find a simple rela-
tionship that predicts test set performance with
a correlation of 0.9997. We analyze why this
relationship holds and show that it holds for
other exponential language models as well, in-
cluding class-based models and minimum dis-
crimination information models. Finally, we
discuss how this relationship can be applied to
improve language model performance.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944533333333">
In this paper, we investigate the following question
for language models belonging to the exponential
family: given some training data and test data drawn
from the same distribution, can we accurately pre-
dict the test set performance of a model estimated
from the training data? This problem is known as
performance prediction and is relevant for model se-
lection, the task of selecting the best model from a
set of candidate models given data.1
Let us first define some notation. Events have the
form (x, y), where we attempt to predict the cur-
rent word y given previous words x. We denote the
training data as D = (x1, y1), ... , (xD, yD) and de-
fine ˜p(x, y) = countD(x, y)/D to be the empirical
distribution of the training data. Similarly, we have
</bodyText>
<footnote confidence="0.660763">
1A long version of this paper can be found at (Chen, 2008).
</footnote>
<bodyText confidence="0.999621">
a test set D* and an associated empirical distribu-
tion p*(x, y). We take the performance of a condi-
tional language model p(y|x) to be the cross-entropy
H(p*, p) between the empirical test distribution p*
and the model p(y|x):
</bodyText>
<equation confidence="0.9117185">
H(p*,p) = − � p*(x, y) log p(y|x) (1)
x,y
</equation>
<bodyText confidence="0.999796">
This is equivalent to the negative mean log-
likelihood per event, as well as to log perplexity.
We only consider models in the exponential fam-
ily. An exponential model pΛ(y|x) is a model with
a set offeatures {f1(x, y), ... , fF(x, y)} and equal
number of parameters A = {λ1, ... , λF} where
</bodyText>
<equation confidence="0.997332">
exp(�F i�1 λifi(x, y))
pA(y|x) = (2)
ZA(x)
</equation>
<bodyText confidence="0.999932428571428">
and where ZΛ(x) is a normalization factor.
One of the seminal methods for performance pre-
diction is the Akaike Information Criterion (AIC)
(Akaike, 1973). For a model, let Aˆ be the maxi-
mum likelihood estimate of A on some training data.
Akaike derived the following estimate for the ex-
pected value of the test set cross-entropy H(p*, pˆΛ):
</bodyText>
<equation confidence="0.975768">
F
H(p*,pn) ≈ H(˜p, pA) + (3)
D
</equation>
<bodyText confidence="0.993246307692308">
H(˜p, pˆΛ) is the cross-entropy of the training set, F
is the number of parameters in the model, and D is
the number of events in the training data. However,
maximum likelihood estimates for language mod-
els typically yield infinite cross-entropy on test data,
and thus AIC behaves poorly for these domains.
In this work, instead of deriving a performance
prediction relationship theoretically, we attempt to
empirically discover a formula for predicting test
performance. Initially, we consider only n-gram lan-
guage models, and build models over varying do-
mains, data set sizes, and n-gram orders. We per-
form linear regression to discover whether we can
</bodyText>
<page confidence="0.969244">
450
</page>
<note confidence="0.889989">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 450–458,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99987">
model test set cross-entropy as a simple function of
training set cross-entropy and other model statistics.
For the 200+ n-gram models we evaluate, we find
that the empirical relationship
</bodyText>
<equation confidence="0.965637">
R(p∗,p˜Λ) ;::L1 R(˜p,p˜Λ) + γ
D
</equation>
<bodyText confidence="0.99751859375">
holds with a correlation of 0.9997 where γ is a con-
stant and where Λ˜ = J˜λij are regularized parameter
estimates; i.e., rather than estimating performance
for maximum likelihood models as in AIC, we do
this for regularized models. In other words, test set
cross-entropy can be approximated by the sum of the
training set cross-entropy and the scaled sum of the
magnitudes of the model parameters.
To maximize the correlation achieved by eq. (4),
we find that it is necessary to use the same regular-
ization method and regularization hyperparameters
across models and that the optimal value of γ de-
pends on the values of the hyperparameters. Con-
sequently, we first evaluate several types of regu-
larization and find which of these (and which hy-
perparameter values) work best across all domains,
and use these values in all subsequent experiments.
While `22 regularization gives the best performance
reported in the literature for n-gram models, we find
here that `1 + `2 2 regularization works even better.
The organization of this paper is as follows: In
Section 2, we evaluate various regularization tech-
niques for n-gram models and select the method and
hyperparameter values that give the best overall per-
formance. In Section 3, we discuss experiments to
find a formula for predicting n-gram model perfor-
mance, and provide an explanation for why eq. (4)
works so well. In Section 4, we evaluate how well
eq. (4) holds for several class-based language mod-
els and minimum discrimination information mod-
els. Finally, in Sections 5 and 6 we discuss related
work and conclusions.
</bodyText>
<sectionHeader confidence="0.803655" genericHeader="method">
2 Selecting Regularization Settings
</sectionHeader>
<bodyText confidence="0.999347166666667">
In this section, we address the issue of how to per-
form regularization in our later experiments. Fol-
lowing the terminology of Dudik and Schapire
(2006), the most widely-used and effective methods
for regularizing exponential models are `1 regular-
ization (Tibshirani, 1994; Kazama and Tsujii, 2003;
</bodyText>
<table confidence="0.895743111111111">
data token range training voc.
source type of n sents. size
A RH letter 2–7 100–75k 27
B WSJ POS 2–7 100–30k 45
C WSJ word 2–5 100–100k 300
D WSJ word 2–5 100–100k 3k
E WSJ word 2–5 100–100k 21k
F BN word 2–5 100–100k 84k
G SWB word 2–5 100–100k 19k
</table>
<tableCaption confidence="0.877132666666667">
Table 1: Statistics of data sets. RH = Random House
dictionary; WSJ = Wall Street Journal; BN = Broadcast
News; SWB = Switchboard.
</tableCaption>
<bodyText confidence="0.968265151515151">
Goodman, 2004) and `2 2 regularization (Lau, 1994;
Chen and Rosenfeld, 2000; Lebanon and Lafferty,
2001). While not as popular, another regularization
scheme that has been shown to be effective is 2-norm
inequality regularization (Kazama and Tsujii, 2003)
which is an instance of `1+`22 regularization as noted
by Dudik and Schapire (2006). Under `1 + `2 2 regu-
larization, the regularized parameter estimates Λ˜ are
chosen to optimize the objective function
λ2i
(5)
Note that `1 regularization can be considered a spe-
cial case of this (by taking σ = oc) as can `2 2 regu-
larization (by taking α = 0).
Here, we evaluate `1, `22, and `1 + `2 2 regulariza-
tion for exponential n-gram models. An exponen-
tial n-gram model contains a binary feature fω for
each n&apos;-gram ω occurring in the training data for
n&apos; &lt; n, where fω(x, y) = 1 iff xy ends in ω. We
would like to find the regularization method and as-
sociated hyperparameters that work best across dif-
ferent domains, training set sizes, and n-gram or-
ders. As it is computationally expensive to evalu-
ate a large number of hyperparameter settings over
a large collection of models, we divide this search
into two phases. First, we evaluate a large set of hy-
perparameters on a limited set of models to come up
with a short list of candidate hyperparameters. We
then evaluate these candidates on our full model set
to find the best one.
We build n-gram models over data from five dif-
ferent sources and consider three different vocabu-
lary sizes for one source, giving us seven “domains”
</bodyText>
<equation confidence="0.999114285714286">
F
|˜λi |(4)
i=1
α F �F
O�1+�2 2(Λ) = R(˜p, pΛ) + i=1 1
D |λi |+ 2σ2D
i=1
</equation>
<page confidence="0.98672">
451
</page>
<bodyText confidence="0.998203939393939">
in total. We refer to these domains by the letters A–
G; summary statistics for each domain are given in
Table 1. The domains C–G consist of regular word
data, while domains A and B consist of letter and
part-of-speech (POS) sequences, respectively. Do-
mains C–E differ only in vocabulary.
For each domain, we first randomize the order of
sentences in that data. We partition off two devel-
opment sets and an evaluation set (5000 “sentences”
each in domain A and 2500 sentences elsewhere) and
use the remaining data as training data. In this way,
we assure that our training and test data are drawn
from the same distribution as is assumed in our later
experiments. Training set sizes in sentences are 100,
300, 1000, 3000, etc., up to the maximums given in
Table 1. Building models for each training set size
and n-gram order in Table 1 gives us a total of 218
models over the seven domains.
In the first phase of hyperparameter search, we
choose a subset of these models (57 total) and evalu-
ate many different values for (α, σ2) with `1+`22 reg-
ularization on each. We perform a grid search, trying
each value α ∈ {0.0, 0.1, 0.2, ... ,1.2} with each
value σ2 ∈ {1,1.2,1.5,2,2.5,3,4,5,6,7,8, 10, ∞}
where σ = ∞ corresponds to `1 regularization and
α = 0 corresponds to `2 2 regularization. We use
a variant of iterative scaling for parameter estima-
tion. For each model and each (α, σ2), we denote
the cross-entropy of the development data as Ha�Q
for the mth model, m ∈ {1, ... , 57}. Then, for each
m and (α, σ2), we can compute how much worse the
settings (α, σ2) perform with model m as compared
to the best hyperparameter settings for that model:
</bodyText>
<table confidence="0.999176714285714">
statistic RMSE coeff.
PF 0.043 0.938
1 i=1 |˜λi |0.044 0.939
D 0.047 0.940
1 0.162 0.755
D Pi:˜λ &gt;0 λi 0.234 0.669
D Pi=1 λi 0.429 0.443
1 0.709 1.289
0.783 0.129
0.910 1.109
0.952 0.112
1.487 1.698
2.232 -0.028
2.236 -0.023
i=1 |˜λi |3
D PF
1 i=1 |˜λi|3 2
PF
D
1 i=1 ˜λ2
PF
D i
F6=0
D
F6=0 log D
D
F
D
F log D
D
1
F
D−F−1
F6=0
D−F6=0−1
</table>
<tableCaption confidence="0.997757">
Table 2: Root mean squared error (RMSE) in nats when
</tableCaption>
<bodyText confidence="0.988438125">
predicting difference in development set and training set
cross-entropy as linear function of a single statistic. The
last column is the optimal coefficient for that statistic.
On the development sets, the (α, σ2) value with
the lowest squared error is (0.5, 6), and these are
the hyperparameter settings we use in all later ex-
periments unless otherwise noted. The RMS error,
mean error, and maximum error for these hyperpa-
rameters on the evaluation sets are 0.011, 0.007, and
0.033 nats, respectively.2 An error of 0.011 nats cor-
responds to a 1.1% difference in perplexity which
is generally considered insignificant. Thus, we can
achieve good performance across domains, data set
sizes, and n-gram orders using a single set of hyper-
parameters as compared to optimizing hyperparam-
eters separately for each model.
</bodyText>
<equation confidence="0.978227555555556">
ˆHm = Hm
α,σ
α,
Hm(6)
α0,σ0
3 N-Gram Model Performance Prediction
− min
α0,σ0
σ
</equation>
<bodyText confidence="0.999948333333333">
We would like to select (α, σ2) for which ˆHa�Q tends
to be small; in particular, we choose (α, σ2) that
minimizes the root mean squared (RMS) error
</bodyText>
<equation confidence="0.972302">
(ˆHmα,σ)2 (7)
</equation>
<bodyText confidence="0.979926888888889">
For each of `1, `22, and `1 + `22 regularization, we re-
tain the 6–8 best hyperparameter settings. To choose
the best single hyperparameter setting from within
this candidate set, we repeat the same analysis ex-
cept over the full set of 218 models.
Now that we have established which regularization
method and hyperparameters to use, we attempt to
empirically discover a simple formula for predict-
ing the test set cross-entropy of regularized n-gram
models. The basic strategy is as follows: We first
build a large number of n-gram models over differ-
ent domains, training set sizes, and n-gram orders.
Then, we come up with a set of candidate statistics,
e.g., training set cross-entropy, number of features,
etc., and do linear regression to try to best model test
2All cross-entropy values are reported in hats, or natural
bits, equivalent to loge e regular bits. This will let us directly
compare γ values with average discounts in Section 3.1.
</bodyText>
<figure confidence="0.974884263157895">
1
5 7
=
ˆHRMS
α,σ
X
m=1
tu u v
57
452
7
6
5
4
3
2
1
0
0
</figure>
<figureCaption confidence="0.999923">
Figure 1: Graph of optimism on evaluation data vs.
</figureCaption>
<equation confidence="0.927623">
PF
1 i=1 |˜λi |for various n-gram models under `1 + `2
D 2
</equation>
<bodyText confidence="0.938440117647059">
regularization, α = 0.5 and σ2 = 6. The line repre-
sents the predicted optimism according to eq. (9) with
γ = 0.938.
set cross-entropy as a linear function of these statis-
tics. We assume that training and test data come
from the same distribution; otherwise, it would be
difficult to predict test performance.
We use the same 218 n-gram models as in Sec-
tion 2. For each model, we compute training set
cross-entropy H(˜p, p˜Λ) as well as all of the statis-
tics listed on the left in Table 2. The statistics FD,
D−F −1, and F log D
F D are motivated by AIC, AICc
(Hurvich and Tsai, 1989), and the Bayesian Infor-
mation Criterion (Schwarz, 1978), respectively. As
features fi with ˜λi = 0 have no effect, instead of
F we also consider using Fl0, the number of fea-
</bodyText>
<equation confidence="0.950704666666667">
PF
tures fi with ˜λi =6 0. The statistics 1 i=1 |˜λi |and
D
PF
1 i=1 ˜λ2 i are motivated by eq. (5). The statistics
D
</equation>
<bodyText confidence="0.999740363636364">
with fractional exponents are suggested by Figure 2.
The value 1 is present to handle constant offsets.
After some initial investigation, it became clear
that training set cross-entropy is a very good (par-
tial) predictor of test set cross-entropy with coeffi-
cient 1. As there is ample theoretical support for
this, instead of fitting test set performance directly,
we chose to model the difference between test and
training performance as a function of the remaining
statistics. This difference is sometimes referred to as
the optimism of a model:
</bodyText>
<equation confidence="0.992962">
optimism(p˜Λ) ≡ H(p∗,p˜Λ) − H(˜p,p˜Λ) (8)
</equation>
<bodyText confidence="0.946576583333333">
First, we attempt to model optimism as a lin-
ear function of a single statistic. For each statis-
tic listed previously, we perform linear regression
to minimize root mean squared error when predict-
ing development set optimism. In Table 2, we dis-
play the RMSE and best coefficient for each statis-
tic. We see that three statistics have by far the lowest
error: D PF 1 |˜λi|, D Pi:˜λi&gt;0 λi, and D PFi= 1 λi.
In practice, most ˜λi in n-gram models are positive,
so these statistics tend to have similar values. We
PF
choose the best ranked of these, 1 i=1 |˜λi|, and
</bodyText>
<sectionHeader confidence="0.54565" genericHeader="method">
D
</sectionHeader>
<bodyText confidence="0.999912375">
show in Section 3.1 why this statistic is more appeal-
ing than the others. Next, we investigate modeling
optimism as a linear function of a pair of statistics.
We find that the best RMSE for two variables (0.042)
is only slightly lower than that for one (0.043), so it
is doubtful that a second variable helps.
Thus, our analysis suggests that among our candi-
dates, the best predictor of optimism is simply
</bodyText>
<equation confidence="0.98492">
|˜λi |(9)
</equation>
<bodyText confidence="0.999979888888889">
where γ = 0.938, with this value being independent
of domain, training set size, and n-gram order. In
other words, the difference between test and train-
ing cross-entropy is a linear function of the sum of
parameter magnitudes scaled per event. Substituting
into eq. (8) and rearranging, we get eq. (4).
To assess the accuracy of eq. (4), we compute var-
ious statistics on our evaluation sets using the best
γ from our development data, i.e., γ = 0.938. In
</bodyText>
<figureCaption confidence="0.834046">
Figure 1, we graph optimism for the evaluation data
</figureCaption>
<equation confidence="0.404960333333333">
PF
against 1 i=1 |˜λi |for each of our models; we see
D
</equation>
<bodyText confidence="0.9998235">
that the linear correlation is very good. The correla-
tion between the actual and predicted cross-entropy
on the evaluation data is 0.9997; the mean absolute
prediction error is 0.030 nats; the RMSE is 0.043
nats; and the maximum absolute error is 0.166 nats.
Thus, on average we can predict test performance to
within 3% in perplexity, though in the worst case we
may be off by as much as 18%.3
</bodyText>
<footnote confidence="0.974587833333333">
3The sampling variation in our test set selection limits the
measured accuracy of our performance prediction. To give
some idea of the size of this effect, we randomly selected 100
test sets in domain D of 2500 sentences each (as in our other
experiments). We evaluated their cross-entropies using mod-
els trained on 100, 1k, 10k, and 100k sentences. The empiri-
</footnote>
<equation confidence="0.82863525">
optimism ≈ γ
D
XF
i=1
</equation>
<page confidence="0.930141">
453
</page>
<figure confidence="0.998465333333333">
discount
2.5
2
1.5
1
0.5
0
-0.5
-1
-1.5
-1 0 1 2 3 4
X
</figure>
<figureCaption confidence="0.99831525">
Figure 2: Smoothed graph of discount versus ˜λi for all
features in ten different models built on domains A and
E. Each smoothed point represents the average of at least
512 raw data points.
</figureCaption>
<bodyText confidence="0.999994909090909">
If we compute the prediction error of eq. (4) over
the same models except using `1 or `2 2 regulariza-
tion (with the best corresponding hyperparameter
values found in Section 2), the prediction RMSE is
0.054 and 0.139 nats, respectively. Thus, we find
that choosing hyperparameters carefully in Section 2
was important in doing well in performance predic-
tion. While hyperparameters were chosen to opti-
mize test performance rather than prediction accu-
racy, we find that the chosen hyperparameters are
favorable for the latter task as well.
</bodyText>
<subsectionHeader confidence="0.998367">
3.1 Why Does Prediction Work So Well?
</subsectionHeader>
<bodyText confidence="0.9989808">
The correlation in Figure 1 is remarkably high, and
thus it begs for an explanation. First, let us express
the difference in test and training cross-entropy for
a model in terms of its parameters Λ. Substituting
eq. (2) into eq. (1), we get
</bodyText>
<equation confidence="0.999519333333333">
H(p*,pA) = − XF λiEp*[fi] + X p*(x) log ZA(x)
i=1 x
(10)
</equation>
<bodyText confidence="0.999727">
where Ep.[fi] = Px,y p∗(x, y)fi(x, y). Then, we
can express the difference in test and training per-
formance as
</bodyText>
<equation confidence="0.9953295">
H(p*,pA) − H(˜p,pA) = PFi=1 λi(Ep[fi] − Ep*[fi])+
Px(p*(x) − ˜p(x))log ZA(x) (11)
</equation>
<bodyText confidence="0.99497525">
cal standard deviation across test sets was found to be 0.0123,
0.0144, 0.0167, and 0.0174 nats, respectively. This effect can
be mitigated by simply using larger test sets.
Ignoring the last term on the right, we see that opti-
mism for exponential models is a linear function of
the λi’s with coefficients E˜p[fi] − Ep.[fi].
Then, we can ask what E˜p[fi] − Ep.[fi] values
would let us satisfy eq. (4). Consider the relationship
</bodyText>
<equation confidence="0.884955">
(EP[fi] − Ep*[fi]) × D ≈ γ sgn ˜λi (12)
</equation>
<bodyText confidence="0.99994058974359">
If we substitute this into eq. (11) and ignore the last
term on the right again, this gives us exactly eq. (4).
We refer to the value (E˜p[fi] − Ep.[fi]) × D as the
discount of a feature. It can be thought of as rep-
resenting how many times less the feature occurs in
the test data as opposed to the training data, if the
test data were normalized to be the same size as the
training data. Discounts for n-grams have been stud-
ied extensively, e.g., (Good, 1953; Church and Gale,
1991; Chen and Goodman, 1998), and tend not to
vary much across training set sizes.
We can check how well eq. (12) holds for actual
regularized n-gram models. We construct a total of
ten n-gram models on domains A and E. We build
four letter 5-gram models on domain A on training
sets ranging in size from 100 words to 30k words,
and six models (either trigram or 5-gram) on do-
main E on training sets ranging from 100 sentences
to 30k sentences. We create large development test
sets (45k words for domain A and 70k sentences for
domain E) to better estimate Ep.[fi].
Because graphs of discounts as a function of ˜λi
are very noisy, we smooth the data before plotting.
We partition data points into buckets containing at
least 512 points. We average all of the points in
each bucket to get a “smoothed” data point, and plot
this single point for each bucket. In Figure 2, we
plot smoothed discounts as a function of ˜λi over the
range ˜λi ∈ [−1, 4] for all ten models.
We see that eq. (12) holds at a very rough level
over the ˜λi range displayed. If we examine how
much different ranges of ˜λi contribute to the over-
all value of PFi=1 ˜λi(E˜p[fi] − Ep.[fi]), we find that
the great majority of the mass (90–95%+) is concen-
trated in the range ˜λi ∈ [0, 4] for all ten models un-
der consideration. Thus, to a first approximation, the
reason that eq. (4) holds with γ = 0.938 is because
on average, feature expectations have a discount of
about this value for ˜λi in this range.4
</bodyText>
<footnote confidence="0.987088">
4This analysis provides some insight as to when eq. (4)
</footnote>
<page confidence="0.99869">
454
</page>
<figure confidence="0.998655928571429">
test cross-entropy - training cross-entropy (nats)
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
0 1 2 3 4 5
E |� |/ c
</figure>
<figureCaption confidence="0.986891714285714">
Figure 3: Graph of optimism on evaluation data vs.
D �j=1 |˜λi |for various models. The ‘+’ marks corre-
spond to models S, M, and L over different training set
sizes, n-gram orders, and numbers of classes. The ‘×’
marks correspond to MDI models over different n-gram
orders and in-domain training set sizes. The line and
small points are taken from Figure 1.
</figureCaption>
<bodyText confidence="0.99997525">
Due to space considerations, we only summarize
our other findings; a longer discussion is provided
in (Chen, 2008). We find that the absolute error in
cross-entropy tends to be quite small across models
for several reasons. For non-sparse models, there
is significant variation in average discounts, but be-
cause D �F 1 |˜λi |is low, the overall error is low.
In contrast, sparse models are dominated by single-
count n-grams with features whose average discount
is quite close to γ = 0.938. Finally, the last term on
the right in eq. (11) also plays a small but significant
role in keeping the prediction error low.
</bodyText>
<sectionHeader confidence="0.986158" genericHeader="method">
4 Other Exponential Language Models
</sectionHeader>
<bodyText confidence="0.999631181818182">
In (Chen, 2009), we show how eq. (4) can be used
to motivate a novel class-based language model and
a regularized version of minimum discrimination in-
formation (MDI) models (Della Pietra et al., 1992).
In this section, we analyze whether in addition to
word n-gram models, eq. (4) holds for these other
exponential language models as well.
won’t hold. For example, if a feature function fi is doubled, its
expectations and discount will also double. Thus, eq. (4) won’t
hold in general for models with continuous feature values, as
average discounts may vary widely.
</bodyText>
<subsectionHeader confidence="0.997372">
4.1 Class-Based Language Models
</subsectionHeader>
<bodyText confidence="0.9998015">
We assume a word w is always mapped to the same
class c(w). For a sentence w1 · · · wl, we have
</bodyText>
<equation confidence="0.998577666666667">
p(w1 ··· wl) 7= 11�+1 p(cj  |c1 ··· cj−1, w1 ··· wj−1) ×
7
ijl=1 p( wj|c1 ···cj w1 ··· wj−1) (13)
</equation>
<bodyText confidence="0.999964666666667">
where cj = c(wj) and where cl+1 is an end-of-
sentence token. We use the notation png(y|ω) to
denote an exponential n-gram model as defined in
Section 2, where we have features for each suffix of
each ωy occurring in the training set. We use the
notation png(y|ω1, ω2) to denote a model containing
all features in the models png(y|ω1) and png(y|ω2).
We consider three class models, models S, M, and
L, defined as
</bodyText>
<equation confidence="0.999477">
pS(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1)
pS(wj|c1···cj,w1···wj−1)=png(wj|cj)
pM(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1,wj−2wj−1)
pM(wj|c1···cj,w1···wj−1)=png(wj|wj−2wj−1cj)
pL(cj|c1···cj−1,w1···wj−1)=png(cj|wj−2cj−2wj−1cj−1)
pL(wj|c1···cj,w1···wj−1)=png(wj|wj−2cj−2wj−1cj−1cj)
</equation>
<bodyText confidence="0.9981713125">
Model S is an exponential version of the class-based
n-gram model from (Brown et al., 1992); model M
is a novel model introduced in (Chen, 2009); and
model L is an exponential version of the model ind-
expredict from (Goodman, 2001).
To evaluate whether eq. (4) can accurately pre-
dict test performance for these class-based models,
we use the WSJ data and vocabulary from domain
E and consider training set sizes of 1k, 10k, 100k,
and 900k sentences. We create three different word
classings containing 50, 150, and 500 classes using
the algorithm of Brown et al. (1992) on the largest
training set. For each training set and number of
classes, we build both 3-gram and 4-gram versions
of each of our three class models.
In Figure 3, we plot optimism (i.e., test minus
</bodyText>
<equation confidence="0.864209">
�F
training cross-entropy) versus 1 i=1 |˜λi |for these
D
</equation>
<bodyText confidence="0.9223296">
models (66 in total) on our WSJ evaluation set. The
‘+’ marks correspond to our class n-gram models,
while the small points replicate the points for word
n-gram models from Figure 1. Remarkably, eq. (4)
appears to accurately predict performance for our
</bodyText>
<page confidence="0.998145">
455
</page>
<bodyText confidence="0.999947117647059">
class n-gram models using the same -y = 0.938
value found for word n-gram models. The mean ab-
solute prediction error is 0.029 nats, comparable to
that found for word n-gram models.
It is interesting that eq. (4) works for class-based
models despite their being composed of two sub-
models, one for word prediction and one for class
prediction. However, taking the log of eq. (13), we
note that the cross-entropy of text can be expressed
as the sum of the cross-entropy of its word tokens
and the cross-entropy of its class tokens. It would
not be surprising if eq. (4) holds separately for the
class prediction model predicting class data and the
word prediction model predicting word data, since
all of these component models are basically n-gram
models. Summing, this explains why eq. (4) holds
for the whole class model.
</bodyText>
<subsectionHeader confidence="0.999742">
4.2 Models with Prior Distributions
</subsectionHeader>
<bodyText confidence="0.990343666666667">
Minimum discrimination information models (Della
Pietra et al., 1992) are exponential models with a
prior distribution q(y|x):
</bodyText>
<equation confidence="0.999407">
pA(y|x) = q(y|x)exp(EFi=1 aifi(x, y)) (14)
ZA(x)
</equation>
<bodyText confidence="0.999980518518519">
The central issue in performance prediction for MDI
models is whether q(y|x) needs to be accounted for.
That is, if we assume q is an exponential model,
should its parameters aqi be included in the sum in
eq. (4)? From eq. (11), we note that if E˜p[fi] −
Ep.[fi] = 0 for a feature fi, then the feature does
not affect the difference between test and training
cross-entropy (ignoring its impact on the last term).
By assumption, the training and test set for p come
from the same distribution while q is derived from
an independent data set. It follows that we expect
E˜p[fqi ]−Ep.[fqi ] to be zero for features in q, and we
should ignore q when applying eq. (4).
To evaluate whether eq. (4) holds for MDI mod-
els, we use the same WSJ training and evaluation
sets from domain E as in Section 4.1. We consider
three different training set sizes: 1k, 10k, and 100k
sentences. To train q, we use the 100k sentence BN
training set from domain F. We build both trigram
and 4-gram versions of each model.
In Figure 3, we plot test minus training cross-
entropy versus D �F 1  |ai  |for these models on our
WSJ evaluation set; the ‘x’ marks correspond to
the MDI models. As expected, eq. (4) appears to
work quite well for MDI models using the same
-y = 0.938 value as before; the mean absolute pre-
diction error is 0.077 nats.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999974">
We group existing performance prediction methods
into two categories: non-data-splitting methods and
data-splitting methods. In non-data-splitting meth-
ods, test performance is directly estimated from
training set performance and/or other statistics of a
model. Data-splitting methods involve partitioning
training data into a truncated training set and a surro-
gate test set and using surrogate test set performance
to estimate true performance.
The most popular non-data-splitting methods for
predicting test set cross-entropy (or likelihood) are
AIC and variants such as AICc, quasi-AIC (QAIC),
and QAICc (Akaike, 1973; Hurvich and Tsai, 1989;
Lebreton et al., 1992). In Section 3, we consid-
ered performance prediction formulae with the same
form as AIC and AICc (except using regularized pa-
rameter estimates), and neither performed as well as
eq. (4); e.g., see Table 2.
There are many techniques for bounding test
set classification error including the Occam’s Ra-
zor bound (Blumer et al., 1987; McAllester, 1999),
PAC-Bayes bound (McAllester, 1999), and the sam-
ple compression bound (Littlestone and Warmuth,
1986; Floyd and Warmuth, 1995). These methods
derive theoretical guarantees that the true error rate
of a classifier will be below (or above) some value
with a certain probability. Langford (2005) evalu-
ates these techniques over many data sets; while the
bounds can sometimes be fairly tight, in many data
sets the bounds are quite loose.
When learning an element from a set of target
classifiers, the Vapnik-Chervonenkis (VC) dimen-
sion of the set can be used to bound the true error rate
relative to the training error rate with some probabil-
ity (Vapnik, 1998); this technique has been used to
compute error bounds for many types of classifiers.
For example, Bartlett (1998) shows that for a neural
network with small weights and small training set
squared error, the true error depends on the size of
its weights rather than the number of weights; this
finding is similar in spirit to eq. (4).
</bodyText>
<page confidence="0.99722">
456
</page>
<bodyText confidence="0.9999548">
In practice, the most accurate methods for perfor-
mance prediction in many contexts are data-splitting
methods (Guyon et al., 2006). These techniques in-
clude the hold-out method; leave-one-out and k-fold
cross-validation; and bootstrapping (Allen, 1974;
Stone, 1974; Geisser, 1975; Craven and Wahba,
1979; Efron, 1983). However, unlike non-data-
splitting methods, these methods do not lend them-
selves well to providing insight into model design as
discussed in Section 6.
</bodyText>
<sectionHeader confidence="0.999663" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999972432432433">
We show that for several types of exponential lan-
guage models, it is possible to accurately predict the
cross-entropy of test data using the simple relation-
ship given in eq. (4). When using E1 + E2 2 regulariza-
tion with (α = 0.5, σ2 = 6), the value γ = 0.938
works well across varying model types, domains,
vocabulary sizes, training set sizes, and n-gram or-
ders, yielding a mean absolute error of about 0.03
nats (3% in perplexity). We evaluate ∼300 language
models in total, including word and class n-gram
models and n-gram models with prior distributions.
While there has been a great deal of work in
performance prediction, the vast majority of work
on non-data-splitting methods has focused on find-
ing theoretically-motivated approximations or prob-
abilistic bounds on test performance. In contrast, we
developed eq. (4) on a purely empirical basis, and
there has been little, if any, existing work that has
shown comparable performance prediction accuracy
over such a large number of models and data sets. In
addition, there has been little, if any, previous work
on performance prediction for language modeling.5
While eq. (4) performs well as compared to other
non-data-splitting methods for performance predic-
tion, the prediction error can be several percent in
perplexity, which means we cannot reliably rank
models that are close in quality. In addition, in
speech recognition and many other applications, an
external test set is typically provided, which means
we can measure test set performance directly. Thus,
in practice, eq. (4) is not terribly useful for the task
5Here, we refer to predicting test set performance from
training set performance and other model statistics. However,
there has been a good deal of work on predicting speech recog-
nition word-error rate from test set perplexity and other statis-
tics, e.g., (Klakow and Peters, 2002).
of model selection; instead, what eq. (4) gives us is
insight into model design. That is, instead of select-
ing between candidate models once they have been
built as in model selection, it is desirable to be able
to select between models at the model design stage.
Being able to intelligently compare models (with-
out implementation) requires that we know which
aspects of a model impact test performance, and this
is exactly what eq. (4) tells us.
Intuitively, simpler models should perform better
on test data given equivalent training performance,
and model structure (as opposed to parameter val-
ues) is an important aspect of the complexity of a
model. Accordingly, there have been many meth-
ods for model selection that measure the size of a
model in terms of the number of features or param-
eters in the model, e.g., (Akaike, 1973; Rissanen,
1978; Schwarz, 1978). Surprisingly, for exponential
language models, the number of model parameters
seems to matter not at all; all that matters are the
magnitudes of the parameter values. Consequently,
one can improve such models by adding features (or
a prior model) that reduce parameter values while
maintaining training performance.
In (Chen, 2009), we show how these ideas can be
used to motivate heuristics for improving the perfor-
mance of existing language models, and use these
heuristics to develop a novel class-based model and
a regularized version of MDI models that outper-
form comparable methods in both perplexity and
speech recognition word-error rate on WSJ data. In
addition, we show how the tradeoff between train-
ing set performance and model size impacts aspects
of language modeling as diverse as backoff n-gram
features, class-based models, and domain adapta-
tion. In sum, eq. (4) provides a new and valuable
framework for characterizing, analyzing, and de-
signing statistical models.
</bodyText>
<sectionHeader confidence="0.996177" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999824666666667">
We thank Bhuvana Ramabhadran and the anony-
mous reviewers for their comments on this and ear-
lier versions of the paper.
</bodyText>
<sectionHeader confidence="0.994273" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.548042">
Hirotugu Akaike. 1973. Information theory and an ex-
tension of the maximum likelihood principle. In Sec-
</bodyText>
<page confidence="0.990091">
457
</page>
<reference confidence="0.999667333333334">
ond Intl. Symp. on Information Theory, pp. 267–281.
David M. Allen. 1974. The relationship between vari-
able selection and data augmentation and a method for
prediction. Technometrics, 16(1):125–127.
Peter L. Bartlett. 1998. The sample complexity of pat-
tern classification with neural networks: the size of the
weights is more important than the size of the network.
IEEE Trans. on Information Theory, 44(2):525–536.
Alselm Blumer, Andrzej Ehrenfeucht, David Haussler,
and Manfred K. Warmuth. 1987. Occam’s razor. In-
formation Processing Letters, 24(6):377–380.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479, December.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Tech. Report TR-10-98, Harvard Univ.
Stanley F. Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for maximum entropy mod-
els. IEEE Trans. Speech and Aud. Proc., 8(1):37–50.
Stanley F. Chen. 2008. Performance prediction for ex-
ponential language models. Tech. Report RC 24671,
IBM Research Division, October.
Stanley F. Chen. 2009. Shrinking exponential language
models. In Proc. ofHLT-NAACL.
Kenneth W. Church and William A. Gale. 1991. A com-
parison of the enhanced Good-Turing and deleted esti-
mation methods for estimating probabilities of English
bigrams. Computer Speech and Language, 5:19–54.
Peter Craven and Grace Wahba. 1979. Smoothing noisy
data with spline functions: estimating the correct de-
gree of smoothing by the method of generalized cross-
validation. Numerische Mathematik, 31:377–403.
Stephen Della Pietra, Vincent Della Pietra, Robert L.
Mercer, and Salim Roukos. 1992. Adaptive language
modeling using minimum discriminant estimation. In
Proc. Speech and Natural Lang. DARPA Workshop.
Miroslav Dudik and Robert E. Schapire. 2006. Maxi-
mum entropy distribution estimation with generalized
regularization. In Proc. of COLT.
Bradley Efron. 1983. Estimating the error rate of a pre-
diction rule: Improvement on cross-validation. J. of
the American Statistical Assoc., 78(382):316–331.
Sally Floyd and Manfred Warmuth. 1995. Sample com-
pression, learnability, and the Vapnik-Chervonenkis
dimension. Machine Learning, 21(3):269–304.
Seymour Geisser. 1975. The predictive sample reuse
method with applications. J. of the American Statisti-
cal Assoc., 70:320–328.
I.J. Good. 1953. The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3 and 4):237–264.
Joshua T. Goodman. 2001. A bit of progress in language
modeling. MSR-TR-2001-72, Microsoft Research.
Joshua Goodman. 2004. Exponential priors for maxi-
mum entropy models. In Proc. ofNAACL.
Isabelle Guyon, Amir Saffari, Gideon Dror, and Joachim
Buhmann. 2006. Performance prediction challenge.
In Proc. of Intl. Conference on Neural Networks
(IJCNN06), IEEE World Congress on Computational
Intelligence (WCCI06), pp. 2958–2965, July.
Clifford M. Hurvich and Chih-Ling Tsai. 1989. Regres-
sion and time series model selection in small samples.
Biometrika, 76(2):297–307, June.
Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. ofEMNLP, pp. 137–144.
Dietrich Klakow and Jochen Peters. 2002. Testing the
correlation of word error rate and perplexity. Speech
Communications, 38(1):19–28.
John Langford. 2005. Tutorial on practical prediction
theory for classification. J. of Machine Learning Re-
search, 6:273–306.
Raymond Lau. 1994. Adaptive statistical language mod-
elling. Master’s thesis, Department of Electrical En-
gineering and Computer Science, Massachusetts Insti-
tute of Technology, Cambridge, MA.
Guy Lebanon and John Lafferty. 2001. Boosting and
maximum likelihood for exponential models. Tech.
Report CMU-CS-01-144, Carnegie Mellon Univ.
Jean-Dominique Lebreton, Kenneth P. Burnham, Jean
Clobert, and David R. Anderson. 1992. Modeling sur-
vival and testing biological hypotheses using marked
animals: a unified approach with case studies. Eco-
logical Monographs, 62:67–118.
Nick Littlestone and Manfred K. Warmuth. 1986. Re-
lating data compression and learnability. Tech. report,
Univ. of California, Santa Cruz.
David A. McAllester. 1999. PAC-Bayesian model aver-
aging. In Proc. of COLT, pp. 164–170.
Jorma Rissanen. 1978. Modeling by the shortest data
description. Automatica, 14:465–471.
Gideon Schwarz. 1978. Estimating the dimension of a
model. Annals of Statistics, 6(2):461–464.
M. Stone. 1974. Cross-validatory choice and assessment
of statistical predictions. J. of the Royal Statistical So-
ciety B, 36:111–147.
Robert Tibshirani. 1994. Regression shrinkage and se-
lection via the lasso. Tech. report, Univ. of Toronto.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley, New York.
</reference>
<page confidence="0.997744">
458
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925760">
<title confidence="0.999949">Performance Prediction for Exponential Language Models</title>
<author confidence="0.996275">F Stanley</author>
<affiliation confidence="0.970887">IBM T.J. Watson Research</affiliation>
<address confidence="0.95674">P.O. Box 218, Yorktown Heights, NY</address>
<email confidence="0.999658">stanchen@watson.ibm.com</email>
<abstract confidence="0.9993479">We investigate the task of performance prediction for language models belonging to the exponential family. First, we attempt to empirically discover a formula for predicting test cross-entropy for language models. We build models over varying domains, set sizes, and orders, and perform linear regression to see whether we can model test set performance as a simple function of training set performance and various model statistics. Remarkably, we find a simple relationship that predicts test set performance with a correlation of 0.9997. We analyze why this relationship holds and show that it holds for other exponential language models as well, including class-based models and minimum discrimination information models. Finally, we discuss how this relationship can be applied to improve language model performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>ond Intl</author>
</authors>
<booktitle>Symp. on Information Theory,</booktitle>
<pages>267--281</pages>
<marker>Intl, </marker>
<rawString>ond Intl. Symp. on Information Theory, pp. 267–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Allen</author>
</authors>
<title>The relationship between variable selection and data augmentation and a method for prediction.</title>
<date>1974</date>
<journal>Technometrics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="27885" citStr="Allen, 1974" startWordPosition="4845" endWordPosition="4846">bility (Vapnik, 1998); this technique has been used to compute error bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the size of its weights rather than the number of weights; this finding is similar in spirit to eq. (4). 456 In practice, the most accurate methods for performance prediction in many contexts are data-splitting methods (Guyon et al., 2006). These techniques include the hold-out method; leave-one-out and k-fold cross-validation; and bootstrapping (Allen, 1974; Stone, 1974; Geisser, 1975; Craven and Wahba, 1979; Efron, 1983). However, unlike non-datasplitting methods, these methods do not lend themselves well to providing insight into model design as discussed in Section 6. 6 Discussion We show that for several types of exponential language models, it is possible to accurately predict the cross-entropy of test data using the simple relationship given in eq. (4). When using E1 + E2 2 regularization with (α = 0.5, σ2 = 6), the value γ = 0.938 works well across varying model types, domains, vocabulary sizes, training set sizes, and n-gram orders, yiel</context>
</contexts>
<marker>Allen, 1974</marker>
<rawString>David M. Allen. 1974. The relationship between variable selection and data augmentation and a method for prediction. Technometrics, 16(1):125–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter L Bartlett</author>
</authors>
<title>The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network.</title>
<date>1998</date>
<journal>IEEE Trans. on Information Theory,</journal>
<volume>44</volume>
<issue>2</issue>
<contexts>
<context position="27409" citStr="Bartlett (1998)" startWordPosition="4769" endWordPosition="4770">ical guarantees that the true error rate of a classifier will be below (or above) some value with a certain probability. Langford (2005) evaluates these techniques over many data sets; while the bounds can sometimes be fairly tight, in many data sets the bounds are quite loose. When learning an element from a set of target classifiers, the Vapnik-Chervonenkis (VC) dimension of the set can be used to bound the true error rate relative to the training error rate with some probability (Vapnik, 1998); this technique has been used to compute error bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the size of its weights rather than the number of weights; this finding is similar in spirit to eq. (4). 456 In practice, the most accurate methods for performance prediction in many contexts are data-splitting methods (Guyon et al., 2006). These techniques include the hold-out method; leave-one-out and k-fold cross-validation; and bootstrapping (Allen, 1974; Stone, 1974; Geisser, 1975; Craven and Wahba, 1979; Efron, 1983). However, unlike non-datasplitting methods, these methods</context>
</contexts>
<marker>Bartlett, 1998</marker>
<rawString>Peter L. Bartlett. 1998. The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network. IEEE Trans. on Information Theory, 44(2):525–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alselm Blumer</author>
<author>Andrzej Ehrenfeucht</author>
<author>David Haussler</author>
<author>Manfred K Warmuth</author>
</authors>
<date>1987</date>
<booktitle>Occam’s razor. Information Processing Letters,</booktitle>
<pages>24--6</pages>
<contexts>
<context position="26618" citStr="Blumer et al., 1987" startWordPosition="4639" endWordPosition="4642"> using surrogate test set performance to estimate true performance. The most popular non-data-splitting methods for predicting test set cross-entropy (or likelihood) are AIC and variants such as AICc, quasi-AIC (QAIC), and QAICc (Akaike, 1973; Hurvich and Tsai, 1989; Lebreton et al., 1992). In Section 3, we considered performance prediction formulae with the same form as AIC and AICc (except using regularized parameter estimates), and neither performed as well as eq. (4); e.g., see Table 2. There are many techniques for bounding test set classification error including the Occam’s Razor bound (Blumer et al., 1987; McAllester, 1999), PAC-Bayes bound (McAllester, 1999), and the sample compression bound (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995). These methods derive theoretical guarantees that the true error rate of a classifier will be below (or above) some value with a certain probability. Langford (2005) evaluates these techniques over many data sets; while the bounds can sometimes be fairly tight, in many data sets the bounds are quite loose. When learning an element from a set of target classifiers, the Vapnik-Chervonenkis (VC) dimension of the set can be used to bound the true error </context>
</contexts>
<marker>Blumer, Ehrenfeucht, Haussler, Warmuth, 1987</marker>
<rawString>Alselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. 1987. Occam’s razor. Information Processing Letters, 24(6):377–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="22268" citStr="Brown et al., 1992" startWordPosition="3902" endWordPosition="3905">e features for each suffix of each ωy occurring in the training set. We use the notation png(y|ω1, ω2) to denote a model containing all features in the models png(y|ω1) and png(y|ω2). We consider three class models, models S, M, and L, defined as pS(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1) pS(wj|c1···cj,w1···wj−1)=png(wj|cj) pM(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1,wj−2wj−1) pM(wj|c1···cj,w1···wj−1)=png(wj|wj−2wj−1cj) pL(cj|c1···cj−1,w1···wj−1)=png(cj|wj−2cj−2wj−1cj−1) pL(wj|c1···cj,w1···wj−1)=png(wj|wj−2cj−2wj−1cj−1cj) Model S is an exponential version of the class-based n-gram model from (Brown et al., 1992); model M is a novel model introduced in (Chen, 2009); and model L is an exponential version of the model indexpredict from (Goodman, 2001). To evaluate whether eq. (4) can accurately predict test performance for these class-based models, we use the WSJ data and vocabulary from domain E and consider training set sizes of 1k, 10k, 100k, and 900k sentences. We create three different word classings containing 50, 150, and 500 classes using the algorithm of Brown et al. (1992) on the largest training set. For each training set and number of classes, we build both 3-gram and 4-gram versions of each</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Tech. Report TR-10-98,</tech>
<institution>Harvard Univ.</institution>
<contexts>
<context position="18072" citStr="Chen and Goodman, 1998" startWordPosition="3182" endWordPosition="3185">i] − Ep.[fi] values would let us satisfy eq. (4). Consider the relationship (EP[fi] − Ep*[fi]) × D ≈ γ sgn ˜λi (12) If we substitute this into eq. (11) and ignore the last term on the right again, this gives us exactly eq. (4). We refer to the value (E˜p[fi] − Ep.[fi]) × D as the discount of a feature. It can be thought of as representing how many times less the feature occurs in the test data as opposed to the training data, if the test data were normalized to be the same size as the training data. Discounts for n-grams have been studied extensively, e.g., (Good, 1953; Church and Gale, 1991; Chen and Goodman, 1998), and tend not to vary much across training set sizes. We can check how well eq. (12) holds for actual regularized n-gram models. We construct a total of ten n-gram models on domains A and E. We build four letter 5-gram models on domain A on training sets ranging in size from 100 words to 30k words, and six models (either trigram or 5-gram) on domain E on training sets ranging from 100 sentences to 30k sentences. We create large development test sets (45k words for domain A and 70k sentences for domain E) to better estimate Ep.[fi]. Because graphs of discounts as a function of ˜λi are very noi</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Tech. Report TR-10-98, Harvard Univ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A survey of smoothing techniques for maximum entropy models.</title>
<date>2000</date>
<journal>IEEE Trans. Speech and Aud. Proc.,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="6258" citStr="Chen and Rosenfeld, 2000" startWordPosition="1043" endWordPosition="1046">k and Schapire (2006), the most widely-used and effective methods for regularizing exponential models are `1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. source type of n sents. size A RH letter 2–7 100–75k 27 B WSJ POS 2–7 100–30k 45 C WSJ word 2–5 100–100k 300 D WSJ word 2–5 100–100k 3k E WSJ word 2–5 100–100k 21k F BN word 2–5 100–100k 84k G SWB word 2–5 100–100k 19k Table 1: Statistics of data sets. RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. Goodman, 2004) and `2 2 regularization (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001). While not as popular, another regularization scheme that has been shown to be effective is 2-norm inequality regularization (Kazama and Tsujii, 2003) which is an instance of `1+`22 regularization as noted by Dudik and Schapire (2006). Under `1 + `2 2 regularization, the regularized parameter estimates Λ˜ are chosen to optimize the objective function λ2i (5) Note that `1 regularization can be considered a special case of this (by taking σ = oc) as can `2 2 regularization (by taking α = 0). Here, we evaluate `1, `22, and `1 + `2 2 regularization for exponential n-g</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 2000. A survey of smoothing techniques for maximum entropy models. IEEE Trans. Speech and Aud. Proc., 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Performance prediction for exponential language models.</title>
<date>2008</date>
<tech>Tech. Report RC 24671,</tech>
<institution>IBM Research Division,</institution>
<contexts>
<context position="1845" citStr="Chen, 2008" startWordPosition="300" endWordPosition="301">ly predict the test set performance of a model estimated from the training data? This problem is known as performance prediction and is relevant for model selection, the task of selecting the best model from a set of candidate models given data.1 Let us first define some notation. Events have the form (x, y), where we attempt to predict the current word y given previous words x. We denote the training data as D = (x1, y1), ... , (xD, yD) and define ˜p(x, y) = countD(x, y)/D to be the empirical distribution of the training data. Similarly, we have 1A long version of this paper can be found at (Chen, 2008). a test set D* and an associated empirical distribution p*(x, y). We take the performance of a conditional language model p(y|x) to be the cross-entropy H(p*, p) between the empirical test distribution p* and the model p(y|x): H(p*,p) = − � p*(x, y) log p(y|x) (1) x,y This is equivalent to the negative mean loglikelihood per event, as well as to log perplexity. We only consider models in the exponential family. An exponential model pΛ(y|x) is a model with a set offeatures {f1(x, y), ... , fF(x, y)} and equal number of parameters A = {λ1, ... , λF} where exp(�F i�1 λifi(x, y)) pA(y|x) = (2) ZA</context>
<context position="20157" citStr="Chen, 2008" startWordPosition="3573" endWordPosition="3574">me insight as to when eq. (4) 454 test cross-entropy - training cross-entropy (nats) 5 4.5 4 3.5 3 2.5 2 1.5 1 0.5 0 0 1 2 3 4 5 E |� |/ c Figure 3: Graph of optimism on evaluation data vs. D �j=1 |˜λi |for various models. The ‘+’ marks correspond to models S, M, and L over different training set sizes, n-gram orders, and numbers of classes. The ‘×’ marks correspond to MDI models over different n-gram orders and in-domain training set sizes. The line and small points are taken from Figure 1. Due to space considerations, we only summarize our other findings; a longer discussion is provided in (Chen, 2008). We find that the absolute error in cross-entropy tends to be quite small across models for several reasons. For non-sparse models, there is significant variation in average discounts, but because D �F 1 |˜λi |is low, the overall error is low. In contrast, sparse models are dominated by singlecount n-grams with features whose average discount is quite close to γ = 0.938. Finally, the last term on the right in eq. (11) also plays a small but significant role in keeping the prediction error low. 4 Other Exponential Language Models In (Chen, 2009), we show how eq. (4) can be used to motivate a n</context>
</contexts>
<marker>Chen, 2008</marker>
<rawString>Stanley F. Chen. 2008. Performance prediction for exponential language models. Tech. Report RC 24671, IBM Research Division, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Shrinking exponential language models.</title>
<date>2009</date>
<booktitle>In Proc. ofHLT-NAACL.</booktitle>
<contexts>
<context position="20708" citStr="Chen, 2009" startWordPosition="3668" endWordPosition="3669">er findings; a longer discussion is provided in (Chen, 2008). We find that the absolute error in cross-entropy tends to be quite small across models for several reasons. For non-sparse models, there is significant variation in average discounts, but because D �F 1 |˜λi |is low, the overall error is low. In contrast, sparse models are dominated by singlecount n-grams with features whose average discount is quite close to γ = 0.938. Finally, the last term on the right in eq. (11) also plays a small but significant role in keeping the prediction error low. 4 Other Exponential Language Models In (Chen, 2009), we show how eq. (4) can be used to motivate a novel class-based language model and a regularized version of minimum discrimination information (MDI) models (Della Pietra et al., 1992). In this section, we analyze whether in addition to word n-gram models, eq. (4) holds for these other exponential language models as well. won’t hold. For example, if a feature function fi is doubled, its expectations and discount will also double. Thus, eq. (4) won’t hold in general for models with continuous feature values, as average discounts may vary widely. 4.1 Class-Based Language Models We assume a word</context>
<context position="22321" citStr="Chen, 2009" startWordPosition="3914" endWordPosition="3915">ng set. We use the notation png(y|ω1, ω2) to denote a model containing all features in the models png(y|ω1) and png(y|ω2). We consider three class models, models S, M, and L, defined as pS(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1) pS(wj|c1···cj,w1···wj−1)=png(wj|cj) pM(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1,wj−2wj−1) pM(wj|c1···cj,w1···wj−1)=png(wj|wj−2wj−1cj) pL(cj|c1···cj−1,w1···wj−1)=png(cj|wj−2cj−2wj−1cj−1) pL(wj|c1···cj,w1···wj−1)=png(wj|wj−2cj−2wj−1cj−1cj) Model S is an exponential version of the class-based n-gram model from (Brown et al., 1992); model M is a novel model introduced in (Chen, 2009); and model L is an exponential version of the model indexpredict from (Goodman, 2001). To evaluate whether eq. (4) can accurately predict test performance for these class-based models, we use the WSJ data and vocabulary from domain E and consider training set sizes of 1k, 10k, 100k, and 900k sentences. We create three different word classings containing 50, 150, and 500 classes using the algorithm of Brown et al. (1992) on the largest training set. For each training set and number of classes, we build both 3-gram and 4-gram versions of each of our three class models. In Figure 3, we plot opti</context>
<context position="31178" citStr="Chen, 2009" startWordPosition="5380" endWordPosition="5381">rameter values) is an important aspect of the complexity of a model. Accordingly, there have been many methods for model selection that measure the size of a model in terms of the number of features or parameters in the model, e.g., (Akaike, 1973; Rissanen, 1978; Schwarz, 1978). Surprisingly, for exponential language models, the number of model parameters seems to matter not at all; all that matters are the magnitudes of the parameter values. Consequently, one can improve such models by adding features (or a prior model) that reduce parameter values while maintaining training performance. In (Chen, 2009), we show how these ideas can be used to motivate heuristics for improving the performance of existing language models, and use these heuristics to develop a novel class-based model and a regularized version of MDI models that outperform comparable methods in both perplexity and speech recognition word-error rate on WSJ data. In addition, we show how the tradeoff between training set performance and model size impacts aspects of language modeling as diverse as backoff n-gram features, class-based models, and domain adaptation. In sum, eq. (4) provides a new and valuable framework for character</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Stanley F. Chen. 2009. Shrinking exponential language models. In Proc. ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="18047" citStr="Church and Gale, 1991" startWordPosition="3178" endWordPosition="3181">, we can ask what E˜p[fi] − Ep.[fi] values would let us satisfy eq. (4). Consider the relationship (EP[fi] − Ep*[fi]) × D ≈ γ sgn ˜λi (12) If we substitute this into eq. (11) and ignore the last term on the right again, this gives us exactly eq. (4). We refer to the value (E˜p[fi] − Ep.[fi]) × D as the discount of a feature. It can be thought of as representing how many times less the feature occurs in the test data as opposed to the training data, if the test data were normalized to be the same size as the training data. Discounts for n-grams have been studied extensively, e.g., (Good, 1953; Church and Gale, 1991; Chen and Goodman, 1998), and tend not to vary much across training set sizes. We can check how well eq. (12) holds for actual regularized n-gram models. We construct a total of ten n-gram models on domains A and E. We build four letter 5-gram models on domain A on training sets ranging in size from 100 words to 30k words, and six models (either trigram or 5-gram) on domain E on training sets ranging from 100 sentences to 30k sentences. We create large development test sets (45k words for domain A and 70k sentences for domain E) to better estimate Ep.[fi]. Because graphs of discounts as a fun</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth W. Church and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Craven</author>
<author>Grace Wahba</author>
</authors>
<title>Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the method of generalized crossvalidation.</title>
<date>1979</date>
<journal>Numerische Mathematik,</journal>
<pages>31--377</pages>
<contexts>
<context position="27937" citStr="Craven and Wahba, 1979" startWordPosition="4851" endWordPosition="4854"> been used to compute error bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the size of its weights rather than the number of weights; this finding is similar in spirit to eq. (4). 456 In practice, the most accurate methods for performance prediction in many contexts are data-splitting methods (Guyon et al., 2006). These techniques include the hold-out method; leave-one-out and k-fold cross-validation; and bootstrapping (Allen, 1974; Stone, 1974; Geisser, 1975; Craven and Wahba, 1979; Efron, 1983). However, unlike non-datasplitting methods, these methods do not lend themselves well to providing insight into model design as discussed in Section 6. 6 Discussion We show that for several types of exponential language models, it is possible to accurately predict the cross-entropy of test data using the simple relationship given in eq. (4). When using E1 + E2 2 regularization with (α = 0.5, σ2 = 6), the value γ = 0.938 works well across varying model types, domains, vocabulary sizes, training set sizes, and n-gram orders, yielding a mean absolute error of about 0.03 nats (3% in</context>
</contexts>
<marker>Craven, Wahba, 1979</marker>
<rawString>Peter Craven and Grace Wahba. 1979. Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the method of generalized crossvalidation. Numerische Mathematik, 31:377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Adaptive language modeling using minimum discriminant estimation.</title>
<date>1992</date>
<booktitle>In Proc. Speech and Natural Lang. DARPA Workshop.</booktitle>
<contexts>
<context position="20893" citStr="Pietra et al., 1992" startWordPosition="3697" endWordPosition="3700">n-sparse models, there is significant variation in average discounts, but because D �F 1 |˜λi |is low, the overall error is low. In contrast, sparse models are dominated by singlecount n-grams with features whose average discount is quite close to γ = 0.938. Finally, the last term on the right in eq. (11) also plays a small but significant role in keeping the prediction error low. 4 Other Exponential Language Models In (Chen, 2009), we show how eq. (4) can be used to motivate a novel class-based language model and a regularized version of minimum discrimination information (MDI) models (Della Pietra et al., 1992). In this section, we analyze whether in addition to word n-gram models, eq. (4) holds for these other exponential language models as well. won’t hold. For example, if a feature function fi is doubled, its expectations and discount will also double. Thus, eq. (4) won’t hold in general for models with continuous feature values, as average discounts may vary widely. 4.1 Class-Based Language Models We assume a word w is always mapped to the same class c(w). For a sentence w1 · · · wl, we have p(w1 ··· wl) 7= 11�+1 p(cj |c1 ··· cj−1, w1 ··· wj−1) × 7 ijl=1 p( wj|c1 ···cj w1 ··· wj−1) (13) where cj</context>
<context position="24186" citStr="Pietra et al., 1992" startWordPosition="4228" endWordPosition="4231">tion and one for class prediction. However, taking the log of eq. (13), we note that the cross-entropy of text can be expressed as the sum of the cross-entropy of its word tokens and the cross-entropy of its class tokens. It would not be surprising if eq. (4) holds separately for the class prediction model predicting class data and the word prediction model predicting word data, since all of these component models are basically n-gram models. Summing, this explains why eq. (4) holds for the whole class model. 4.2 Models with Prior Distributions Minimum discrimination information models (Della Pietra et al., 1992) are exponential models with a prior distribution q(y|x): pA(y|x) = q(y|x)exp(EFi=1 aifi(x, y)) (14) ZA(x) The central issue in performance prediction for MDI models is whether q(y|x) needs to be accounted for. That is, if we assume q is an exponential model, should its parameters aqi be included in the sum in eq. (4)? From eq. (11), we note that if E˜p[fi] − Ep.[fi] = 0 for a feature fi, then the feature does not affect the difference between test and training cross-entropy (ignoring its impact on the last term). By assumption, the training and test set for p come from the same distribution w</context>
</contexts>
<marker>Pietra, Pietra, Mercer, Roukos, 1992</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, Robert L. Mercer, and Salim Roukos. 1992. Adaptive language modeling using minimum discriminant estimation. In Proc. Speech and Natural Lang. DARPA Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miroslav Dudik</author>
<author>Robert E Schapire</author>
</authors>
<title>Maximum entropy distribution estimation with generalized regularization.</title>
<date>2006</date>
<booktitle>In Proc. of COLT.</booktitle>
<contexts>
<context position="5655" citStr="Dudik and Schapire (2006)" startWordPosition="935" endWordPosition="938"> the method and hyperparameter values that give the best overall performance. In Section 3, we discuss experiments to find a formula for predicting n-gram model performance, and provide an explanation for why eq. (4) works so well. In Section 4, we evaluate how well eq. (4) holds for several class-based language models and minimum discrimination information models. Finally, in Sections 5 and 6 we discuss related work and conclusions. 2 Selecting Regularization Settings In this section, we address the issue of how to perform regularization in our later experiments. Following the terminology of Dudik and Schapire (2006), the most widely-used and effective methods for regularizing exponential models are `1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. source type of n sents. size A RH letter 2–7 100–75k 27 B WSJ POS 2–7 100–30k 45 C WSJ word 2–5 100–100k 300 D WSJ word 2–5 100–100k 3k E WSJ word 2–5 100–100k 21k F BN word 2–5 100–100k 84k G SWB word 2–5 100–100k 19k Table 1: Statistics of data sets. RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. Goodman, 2004) and `2 2 regularization (Lau, 1994; Chen and Rosenfeld, 2</context>
</contexts>
<marker>Dudik, Schapire, 2006</marker>
<rawString>Miroslav Dudik and Robert E. Schapire. 2006. Maximum entropy distribution estimation with generalized regularization. In Proc. of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
</authors>
<title>Estimating the error rate of a prediction rule: Improvement on cross-validation.</title>
<date>1983</date>
<journal>J. of the American Statistical Assoc.,</journal>
<volume>78</volume>
<issue>382</issue>
<contexts>
<context position="27951" citStr="Efron, 1983" startWordPosition="4855" endWordPosition="4856">ror bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the size of its weights rather than the number of weights; this finding is similar in spirit to eq. (4). 456 In practice, the most accurate methods for performance prediction in many contexts are data-splitting methods (Guyon et al., 2006). These techniques include the hold-out method; leave-one-out and k-fold cross-validation; and bootstrapping (Allen, 1974; Stone, 1974; Geisser, 1975; Craven and Wahba, 1979; Efron, 1983). However, unlike non-datasplitting methods, these methods do not lend themselves well to providing insight into model design as discussed in Section 6. 6 Discussion We show that for several types of exponential language models, it is possible to accurately predict the cross-entropy of test data using the simple relationship given in eq. (4). When using E1 + E2 2 regularization with (α = 0.5, σ2 = 6), the value γ = 0.938 works well across varying model types, domains, vocabulary sizes, training set sizes, and n-gram orders, yielding a mean absolute error of about 0.03 nats (3% in perplexity). </context>
</contexts>
<marker>Efron, 1983</marker>
<rawString>Bradley Efron. 1983. Estimating the error rate of a prediction rule: Improvement on cross-validation. J. of the American Statistical Assoc., 78(382):316–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Floyd</author>
<author>Manfred Warmuth</author>
</authors>
<title>Sample compression, learnability, and the Vapnik-Chervonenkis dimension.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="26764" citStr="Floyd and Warmuth, 1995" startWordPosition="4659" endWordPosition="4662">-entropy (or likelihood) are AIC and variants such as AICc, quasi-AIC (QAIC), and QAICc (Akaike, 1973; Hurvich and Tsai, 1989; Lebreton et al., 1992). In Section 3, we considered performance prediction formulae with the same form as AIC and AICc (except using regularized parameter estimates), and neither performed as well as eq. (4); e.g., see Table 2. There are many techniques for bounding test set classification error including the Occam’s Razor bound (Blumer et al., 1987; McAllester, 1999), PAC-Bayes bound (McAllester, 1999), and the sample compression bound (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995). These methods derive theoretical guarantees that the true error rate of a classifier will be below (or above) some value with a certain probability. Langford (2005) evaluates these techniques over many data sets; while the bounds can sometimes be fairly tight, in many data sets the bounds are quite loose. When learning an element from a set of target classifiers, the Vapnik-Chervonenkis (VC) dimension of the set can be used to bound the true error rate relative to the training error rate with some probability (Vapnik, 1998); this technique has been used to compute error bounds for many types</context>
</contexts>
<marker>Floyd, Warmuth, 1995</marker>
<rawString>Sally Floyd and Manfred Warmuth. 1995. Sample compression, learnability, and the Vapnik-Chervonenkis dimension. Machine Learning, 21(3):269–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seymour Geisser</author>
</authors>
<title>The predictive sample reuse method with applications.</title>
<date>1975</date>
<journal>J. of the American Statistical Assoc.,</journal>
<pages>70--320</pages>
<contexts>
<context position="27913" citStr="Geisser, 1975" startWordPosition="4849" endWordPosition="4850">s technique has been used to compute error bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the size of its weights rather than the number of weights; this finding is similar in spirit to eq. (4). 456 In practice, the most accurate methods for performance prediction in many contexts are data-splitting methods (Guyon et al., 2006). These techniques include the hold-out method; leave-one-out and k-fold cross-validation; and bootstrapping (Allen, 1974; Stone, 1974; Geisser, 1975; Craven and Wahba, 1979; Efron, 1983). However, unlike non-datasplitting methods, these methods do not lend themselves well to providing insight into model design as discussed in Section 6. 6 Discussion We show that for several types of exponential language models, it is possible to accurately predict the cross-entropy of test data using the simple relationship given in eq. (4). When using E1 + E2 2 regularization with (α = 0.5, σ2 = 6), the value γ = 0.938 works well across varying model types, domains, vocabulary sizes, training set sizes, and n-gram orders, yielding a mean absolute error o</context>
</contexts>
<marker>Geisser, 1975</marker>
<rawString>Seymour Geisser. 1975. The predictive sample reuse method with applications. J. of the American Statistical Assoc., 70:320–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<volume>40</volume>
<issue>3</issue>
<pages>4--237</pages>
<contexts>
<context position="18024" citStr="Good, 1953" startWordPosition="3176" endWordPosition="3177">p.[fi]. Then, we can ask what E˜p[fi] − Ep.[fi] values would let us satisfy eq. (4). Consider the relationship (EP[fi] − Ep*[fi]) × D ≈ γ sgn ˜λi (12) If we substitute this into eq. (11) and ignore the last term on the right again, this gives us exactly eq. (4). We refer to the value (E˜p[fi] − Ep.[fi]) × D as the discount of a feature. It can be thought of as representing how many times less the feature occurs in the test data as opposed to the training data, if the test data were normalized to be the same size as the training data. Discounts for n-grams have been studied extensively, e.g., (Good, 1953; Church and Gale, 1991; Chen and Goodman, 1998), and tend not to vary much across training set sizes. We can check how well eq. (12) holds for actual regularized n-gram models. We construct a total of ten n-gram models on domains A and E. We build four letter 5-gram models on domain A on training sets ranging in size from 100 words to 30k words, and six models (either trigram or 5-gram) on domain E on training sets ranging from 100 sentences to 30k sentences. We create large development test sets (45k words for domain A and 70k sentences for domain E) to better estimate Ep.[fi]. Because graph</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>I.J. Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3 and 4):237–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<tech>MSR-TR-2001-72,</tech>
<institution>Microsoft Research.</institution>
<contexts>
<context position="22407" citStr="Goodman, 2001" startWordPosition="3929" endWordPosition="3930"> in the models png(y|ω1) and png(y|ω2). We consider three class models, models S, M, and L, defined as pS(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1) pS(wj|c1···cj,w1···wj−1)=png(wj|cj) pM(cj|c1···cj−1,w1···wj−1)=png(cj|cj−2cj−1,wj−2wj−1) pM(wj|c1···cj,w1···wj−1)=png(wj|wj−2wj−1cj) pL(cj|c1···cj−1,w1···wj−1)=png(cj|wj−2cj−2wj−1cj−1) pL(wj|c1···cj,w1···wj−1)=png(wj|wj−2cj−2wj−1cj−1cj) Model S is an exponential version of the class-based n-gram model from (Brown et al., 1992); model M is a novel model introduced in (Chen, 2009); and model L is an exponential version of the model indexpredict from (Goodman, 2001). To evaluate whether eq. (4) can accurately predict test performance for these class-based models, we use the WSJ data and vocabulary from domain E and consider training set sizes of 1k, 10k, 100k, and 900k sentences. We create three different word classings containing 50, 150, and 500 classes using the algorithm of Brown et al. (1992) on the largest training set. For each training set and number of classes, we build both 3-gram and 4-gram versions of each of our three class models. In Figure 3, we plot optimism (i.e., test minus �F training cross-entropy) versus 1 i=1 |˜λi |for these D model</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling. MSR-TR-2001-72, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential priors for maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. ofNAACL.</booktitle>
<contexts>
<context position="6197" citStr="Goodman, 2004" startWordPosition="1035" endWordPosition="1036">ater experiments. Following the terminology of Dudik and Schapire (2006), the most widely-used and effective methods for regularizing exponential models are `1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. source type of n sents. size A RH letter 2–7 100–75k 27 B WSJ POS 2–7 100–30k 45 C WSJ word 2–5 100–100k 300 D WSJ word 2–5 100–100k 3k E WSJ word 2–5 100–100k 21k F BN word 2–5 100–100k 84k G SWB word 2–5 100–100k 19k Table 1: Statistics of data sets. RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. Goodman, 2004) and `2 2 regularization (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001). While not as popular, another regularization scheme that has been shown to be effective is 2-norm inequality regularization (Kazama and Tsujii, 2003) which is an instance of `1+`22 regularization as noted by Dudik and Schapire (2006). Under `1 + `2 2 regularization, the regularized parameter estimates Λ˜ are chosen to optimize the objective function λ2i (5) Note that `1 regularization can be considered a special case of this (by taking σ = oc) as can `2 2 regularization (by taking α = 0). Here, we evalu</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential priors for maximum entropy models. In Proc. ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Amir Saffari</author>
<author>Gideon Dror</author>
<author>Joachim Buhmann</author>
</authors>
<title>Performance prediction challenge.</title>
<date>2006</date>
<booktitle>In Proc. of Intl. Conference on Neural Networks (IJCNN06), IEEE World Congress on Computational Intelligence (WCCI06),</booktitle>
<pages>2958--2965</pages>
<contexts>
<context position="27764" citStr="Guyon et al., 2006" startWordPosition="4828" endWordPosition="4831">ervonenkis (VC) dimension of the set can be used to bound the true error rate relative to the training error rate with some probability (Vapnik, 1998); this technique has been used to compute error bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the size of its weights rather than the number of weights; this finding is similar in spirit to eq. (4). 456 In practice, the most accurate methods for performance prediction in many contexts are data-splitting methods (Guyon et al., 2006). These techniques include the hold-out method; leave-one-out and k-fold cross-validation; and bootstrapping (Allen, 1974; Stone, 1974; Geisser, 1975; Craven and Wahba, 1979; Efron, 1983). However, unlike non-datasplitting methods, these methods do not lend themselves well to providing insight into model design as discussed in Section 6. 6 Discussion We show that for several types of exponential language models, it is possible to accurately predict the cross-entropy of test data using the simple relationship given in eq. (4). When using E1 + E2 2 regularization with (α = 0.5, σ2 = 6), the valu</context>
</contexts>
<marker>Guyon, Saffari, Dror, Buhmann, 2006</marker>
<rawString>Isabelle Guyon, Amir Saffari, Gideon Dror, and Joachim Buhmann. 2006. Performance prediction challenge. In Proc. of Intl. Conference on Neural Networks (IJCNN06), IEEE World Congress on Computational Intelligence (WCCI06), pp. 2958–2965, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clifford M Hurvich</author>
<author>Chih-Ling Tsai</author>
</authors>
<title>Regression and time series model selection in small samples.</title>
<date>1989</date>
<journal>Biometrika,</journal>
<volume>76</volume>
<issue>2</issue>
<contexts>
<context position="12579" citStr="Hurvich and Tsai, 1989" startWordPosition="2197" endWordPosition="2200">various n-gram models under `1 + `2 D 2 regularization, α = 0.5 and σ2 = 6. The line represents the predicted optimism according to eq. (9) with γ = 0.938. set cross-entropy as a linear function of these statistics. We assume that training and test data come from the same distribution; otherwise, it would be difficult to predict test performance. We use the same 218 n-gram models as in Section 2. For each model, we compute training set cross-entropy H(˜p, p˜Λ) as well as all of the statistics listed on the left in Table 2. The statistics FD, D−F −1, and F log D F D are motivated by AIC, AICc (Hurvich and Tsai, 1989), and the Bayesian Information Criterion (Schwarz, 1978), respectively. As features fi with ˜λi = 0 have no effect, instead of F we also consider using Fl0, the number of feaPF tures fi with ˜λi =6 0. The statistics 1 i=1 |˜λi |and D PF 1 i=1 ˜λ2 i are motivated by eq. (5). The statistics D with fractional exponents are suggested by Figure 2. The value 1 is present to handle constant offsets. After some initial investigation, it became clear that training set cross-entropy is a very good (partial) predictor of test set cross-entropy with coefficient 1. As there is ample theoretical support for</context>
<context position="26265" citStr="Hurvich and Tsai, 1989" startWordPosition="4580" endWordPosition="4583">formance prediction methods into two categories: non-data-splitting methods and data-splitting methods. In non-data-splitting methods, test performance is directly estimated from training set performance and/or other statistics of a model. Data-splitting methods involve partitioning training data into a truncated training set and a surrogate test set and using surrogate test set performance to estimate true performance. The most popular non-data-splitting methods for predicting test set cross-entropy (or likelihood) are AIC and variants such as AICc, quasi-AIC (QAIC), and QAICc (Akaike, 1973; Hurvich and Tsai, 1989; Lebreton et al., 1992). In Section 3, we considered performance prediction formulae with the same form as AIC and AICc (except using regularized parameter estimates), and neither performed as well as eq. (4); e.g., see Table 2. There are many techniques for bounding test set classification error including the Occam’s Razor bound (Blumer et al., 1987; McAllester, 1999), PAC-Bayes bound (McAllester, 1999), and the sample compression bound (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995). These methods derive theoretical guarantees that the true error rate of a classifier will be below </context>
</contexts>
<marker>Hurvich, Tsai, 1989</marker>
<rawString>Clifford M. Hurvich and Chih-Ling Tsai. 1989. Regression and time series model selection in small samples. Biometrika, 76(2):297–307, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Evaluation and extension of maximum entropy models with inequality constraints.</title>
<date>2003</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="5800" citStr="Kazama and Tsujii, 2003" startWordPosition="955" endWordPosition="958"> n-gram model performance, and provide an explanation for why eq. (4) works so well. In Section 4, we evaluate how well eq. (4) holds for several class-based language models and minimum discrimination information models. Finally, in Sections 5 and 6 we discuss related work and conclusions. 2 Selecting Regularization Settings In this section, we address the issue of how to perform regularization in our later experiments. Following the terminology of Dudik and Schapire (2006), the most widely-used and effective methods for regularizing exponential models are `1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. source type of n sents. size A RH letter 2–7 100–75k 27 B WSJ POS 2–7 100–30k 45 C WSJ word 2–5 100–100k 300 D WSJ word 2–5 100–100k 3k E WSJ word 2–5 100–100k 21k F BN word 2–5 100–100k 84k G SWB word 2–5 100–100k 19k Table 1: Statistics of data sets. RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. Goodman, 2004) and `2 2 regularization (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001). While not as popular, another regularization scheme that has been shown to be effective is 2-norm inequality re</context>
</contexts>
<marker>Kazama, Tsujii, 2003</marker>
<rawString>Jun’ichi Kazama and Jun’ichi Tsujii. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In Proc. ofEMNLP, pp. 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietrich Klakow</author>
<author>Jochen Peters</author>
</authors>
<title>Testing the correlation of word error rate and perplexity.</title>
<date>2002</date>
<journal>Speech Communications,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="29979" citStr="Klakow and Peters, 2002" startWordPosition="5182" endWordPosition="5185"> error can be several percent in perplexity, which means we cannot reliably rank models that are close in quality. In addition, in speech recognition and many other applications, an external test set is typically provided, which means we can measure test set performance directly. Thus, in practice, eq. (4) is not terribly useful for the task 5Here, we refer to predicting test set performance from training set performance and other model statistics. However, there has been a good deal of work on predicting speech recognition word-error rate from test set perplexity and other statistics, e.g., (Klakow and Peters, 2002). of model selection; instead, what eq. (4) gives us is insight into model design. That is, instead of selecting between candidate models once they have been built as in model selection, it is desirable to be able to select between models at the model design stage. Being able to intelligently compare models (without implementation) requires that we know which aspects of a model impact test performance, and this is exactly what eq. (4) tells us. Intuitively, simpler models should perform better on test data given equivalent training performance, and model structure (as opposed to parameter valu</context>
</contexts>
<marker>Klakow, Peters, 2002</marker>
<rawString>Dietrich Klakow and Jochen Peters. 2002. Testing the correlation of word error rate and perplexity. Speech Communications, 38(1):19–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Langford</author>
</authors>
<title>Tutorial on practical prediction theory for classification.</title>
<date>2005</date>
<journal>J. of Machine Learning Research,</journal>
<pages>6--273</pages>
<contexts>
<context position="26930" citStr="Langford (2005)" startWordPosition="4687" endWordPosition="4688">red performance prediction formulae with the same form as AIC and AICc (except using regularized parameter estimates), and neither performed as well as eq. (4); e.g., see Table 2. There are many techniques for bounding test set classification error including the Occam’s Razor bound (Blumer et al., 1987; McAllester, 1999), PAC-Bayes bound (McAllester, 1999), and the sample compression bound (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995). These methods derive theoretical guarantees that the true error rate of a classifier will be below (or above) some value with a certain probability. Langford (2005) evaluates these techniques over many data sets; while the bounds can sometimes be fairly tight, in many data sets the bounds are quite loose. When learning an element from a set of target classifiers, the Vapnik-Chervonenkis (VC) dimension of the set can be used to bound the true error rate relative to the training error rate with some probability (Vapnik, 1998); this technique has been used to compute error bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the s</context>
</contexts>
<marker>Langford, 2005</marker>
<rawString>John Langford. 2005. Tutorial on practical prediction theory for classification. J. of Machine Learning Research, 6:273–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond Lau</author>
</authors>
<title>Adaptive statistical language modelling.</title>
<date>1994</date>
<tech>Master’s thesis,</tech>
<institution>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="6232" citStr="Lau, 1994" startWordPosition="1041" endWordPosition="1042">ogy of Dudik and Schapire (2006), the most widely-used and effective methods for regularizing exponential models are `1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. source type of n sents. size A RH letter 2–7 100–75k 27 B WSJ POS 2–7 100–30k 45 C WSJ word 2–5 100–100k 300 D WSJ word 2–5 100–100k 3k E WSJ word 2–5 100–100k 21k F BN word 2–5 100–100k 84k G SWB word 2–5 100–100k 19k Table 1: Statistics of data sets. RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. Goodman, 2004) and `2 2 regularization (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001). While not as popular, another regularization scheme that has been shown to be effective is 2-norm inequality regularization (Kazama and Tsujii, 2003) which is an instance of `1+`22 regularization as noted by Dudik and Schapire (2006). Under `1 + `2 2 regularization, the regularized parameter estimates Λ˜ are chosen to optimize the objective function λ2i (5) Note that `1 regularization can be considered a special case of this (by taking σ = oc) as can `2 2 regularization (by taking α = 0). Here, we evaluate `1, `22, and `1 + `2 2 regulari</context>
</contexts>
<marker>Lau, 1994</marker>
<rawString>Raymond Lau. 1994. Adaptive statistical language modelling. Master’s thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Lebanon</author>
<author>John Lafferty</author>
</authors>
<title>Boosting and maximum likelihood for exponential models.</title>
<date>2001</date>
<tech>Tech. Report CMU-CS-01-144,</tech>
<institution>Carnegie Mellon Univ.</institution>
<contexts>
<context position="6287" citStr="Lebanon and Lafferty, 2001" startWordPosition="1047" endWordPosition="1050"> most widely-used and effective methods for regularizing exponential models are `1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. source type of n sents. size A RH letter 2–7 100–75k 27 B WSJ POS 2–7 100–30k 45 C WSJ word 2–5 100–100k 300 D WSJ word 2–5 100–100k 3k E WSJ word 2–5 100–100k 21k F BN word 2–5 100–100k 84k G SWB word 2–5 100–100k 19k Table 1: Statistics of data sets. RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. Goodman, 2004) and `2 2 regularization (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001). While not as popular, another regularization scheme that has been shown to be effective is 2-norm inequality regularization (Kazama and Tsujii, 2003) which is an instance of `1+`22 regularization as noted by Dudik and Schapire (2006). Under `1 + `2 2 regularization, the regularized parameter estimates Λ˜ are chosen to optimize the objective function λ2i (5) Note that `1 regularization can be considered a special case of this (by taking σ = oc) as can `2 2 regularization (by taking α = 0). Here, we evaluate `1, `22, and `1 + `2 2 regularization for exponential n-gram models. An exponential n-</context>
</contexts>
<marker>Lebanon, Lafferty, 2001</marker>
<rawString>Guy Lebanon and John Lafferty. 2001. Boosting and maximum likelihood for exponential models. Tech. Report CMU-CS-01-144, Carnegie Mellon Univ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Dominique Lebreton</author>
<author>Kenneth P Burnham</author>
<author>Jean Clobert</author>
<author>David R Anderson</author>
</authors>
<title>Modeling survival and testing biological hypotheses using marked animals: a unified approach with case studies.</title>
<date>1992</date>
<journal>Ecological Monographs,</journal>
<pages>62--67</pages>
<contexts>
<context position="26289" citStr="Lebreton et al., 1992" startWordPosition="4584" endWordPosition="4587">ods into two categories: non-data-splitting methods and data-splitting methods. In non-data-splitting methods, test performance is directly estimated from training set performance and/or other statistics of a model. Data-splitting methods involve partitioning training data into a truncated training set and a surrogate test set and using surrogate test set performance to estimate true performance. The most popular non-data-splitting methods for predicting test set cross-entropy (or likelihood) are AIC and variants such as AICc, quasi-AIC (QAIC), and QAICc (Akaike, 1973; Hurvich and Tsai, 1989; Lebreton et al., 1992). In Section 3, we considered performance prediction formulae with the same form as AIC and AICc (except using regularized parameter estimates), and neither performed as well as eq. (4); e.g., see Table 2. There are many techniques for bounding test set classification error including the Occam’s Razor bound (Blumer et al., 1987; McAllester, 1999), PAC-Bayes bound (McAllester, 1999), and the sample compression bound (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995). These methods derive theoretical guarantees that the true error rate of a classifier will be below (or above) some value wi</context>
</contexts>
<marker>Lebreton, Burnham, Clobert, Anderson, 1992</marker>
<rawString>Jean-Dominique Lebreton, Kenneth P. Burnham, Jean Clobert, and David R. Anderson. 1992. Modeling survival and testing biological hypotheses using marked animals: a unified approach with case studies. Ecological Monographs, 62:67–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Littlestone</author>
<author>Manfred K Warmuth</author>
</authors>
<title>Relating data compression and learnability.</title>
<date>1986</date>
<tech>Tech. report,</tech>
<institution>Univ. of California,</institution>
<location>Santa Cruz.</location>
<contexts>
<context position="26738" citStr="Littlestone and Warmuth, 1986" startWordPosition="4655" endWordPosition="4658">s for predicting test set cross-entropy (or likelihood) are AIC and variants such as AICc, quasi-AIC (QAIC), and QAICc (Akaike, 1973; Hurvich and Tsai, 1989; Lebreton et al., 1992). In Section 3, we considered performance prediction formulae with the same form as AIC and AICc (except using regularized parameter estimates), and neither performed as well as eq. (4); e.g., see Table 2. There are many techniques for bounding test set classification error including the Occam’s Razor bound (Blumer et al., 1987; McAllester, 1999), PAC-Bayes bound (McAllester, 1999), and the sample compression bound (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995). These methods derive theoretical guarantees that the true error rate of a classifier will be below (or above) some value with a certain probability. Langford (2005) evaluates these techniques over many data sets; while the bounds can sometimes be fairly tight, in many data sets the bounds are quite loose. When learning an element from a set of target classifiers, the Vapnik-Chervonenkis (VC) dimension of the set can be used to bound the true error rate relative to the training error rate with some probability (Vapnik, 1998); this technique has been used to compute e</context>
</contexts>
<marker>Littlestone, Warmuth, 1986</marker>
<rawString>Nick Littlestone and Manfred K. Warmuth. 1986. Relating data compression and learnability. Tech. report, Univ. of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A McAllester</author>
</authors>
<title>PAC-Bayesian model averaging.</title>
<date>1999</date>
<booktitle>In Proc. of COLT,</booktitle>
<pages>164--170</pages>
<contexts>
<context position="26637" citStr="McAllester, 1999" startWordPosition="4643" endWordPosition="4644"> set performance to estimate true performance. The most popular non-data-splitting methods for predicting test set cross-entropy (or likelihood) are AIC and variants such as AICc, quasi-AIC (QAIC), and QAICc (Akaike, 1973; Hurvich and Tsai, 1989; Lebreton et al., 1992). In Section 3, we considered performance prediction formulae with the same form as AIC and AICc (except using regularized parameter estimates), and neither performed as well as eq. (4); e.g., see Table 2. There are many techniques for bounding test set classification error including the Occam’s Razor bound (Blumer et al., 1987; McAllester, 1999), PAC-Bayes bound (McAllester, 1999), and the sample compression bound (Littlestone and Warmuth, 1986; Floyd and Warmuth, 1995). These methods derive theoretical guarantees that the true error rate of a classifier will be below (or above) some value with a certain probability. Langford (2005) evaluates these techniques over many data sets; while the bounds can sometimes be fairly tight, in many data sets the bounds are quite loose. When learning an element from a set of target classifiers, the Vapnik-Chervonenkis (VC) dimension of the set can be used to bound the true error rate relative to th</context>
</contexts>
<marker>McAllester, 1999</marker>
<rawString>David A. McAllester. 1999. PAC-Bayesian model averaging. In Proc. of COLT, pp. 164–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by the shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<pages>14--465</pages>
<contexts>
<context position="30829" citStr="Rissanen, 1978" startWordPosition="5328" endWordPosition="5329">ls at the model design stage. Being able to intelligently compare models (without implementation) requires that we know which aspects of a model impact test performance, and this is exactly what eq. (4) tells us. Intuitively, simpler models should perform better on test data given equivalent training performance, and model structure (as opposed to parameter values) is an important aspect of the complexity of a model. Accordingly, there have been many methods for model selection that measure the size of a model in terms of the number of features or parameters in the model, e.g., (Akaike, 1973; Rissanen, 1978; Schwarz, 1978). Surprisingly, for exponential language models, the number of model parameters seems to matter not at all; all that matters are the magnitudes of the parameter values. Consequently, one can improve such models by adding features (or a prior model) that reduce parameter values while maintaining training performance. In (Chen, 2009), we show how these ideas can be used to motivate heuristics for improving the performance of existing language models, and use these heuristics to develop a novel class-based model and a regularized version of MDI models that outperform comparable me</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Jorma Rissanen. 1978. Modeling by the shortest data description. Automatica, 14:465–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Schwarz</author>
</authors>
<title>Estimating the dimension of a model.</title>
<date>1978</date>
<journal>Annals of Statistics,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="12635" citStr="Schwarz, 1978" startWordPosition="2207" endWordPosition="2208">nd σ2 = 6. The line represents the predicted optimism according to eq. (9) with γ = 0.938. set cross-entropy as a linear function of these statistics. We assume that training and test data come from the same distribution; otherwise, it would be difficult to predict test performance. We use the same 218 n-gram models as in Section 2. For each model, we compute training set cross-entropy H(˜p, p˜Λ) as well as all of the statistics listed on the left in Table 2. The statistics FD, D−F −1, and F log D F D are motivated by AIC, AICc (Hurvich and Tsai, 1989), and the Bayesian Information Criterion (Schwarz, 1978), respectively. As features fi with ˜λi = 0 have no effect, instead of F we also consider using Fl0, the number of feaPF tures fi with ˜λi =6 0. The statistics 1 i=1 |˜λi |and D PF 1 i=1 ˜λ2 i are motivated by eq. (5). The statistics D with fractional exponents are suggested by Figure 2. The value 1 is present to handle constant offsets. After some initial investigation, it became clear that training set cross-entropy is a very good (partial) predictor of test set cross-entropy with coefficient 1. As there is ample theoretical support for this, instead of fitting test set performance directly,</context>
<context position="30845" citStr="Schwarz, 1978" startWordPosition="5330" endWordPosition="5331">design stage. Being able to intelligently compare models (without implementation) requires that we know which aspects of a model impact test performance, and this is exactly what eq. (4) tells us. Intuitively, simpler models should perform better on test data given equivalent training performance, and model structure (as opposed to parameter values) is an important aspect of the complexity of a model. Accordingly, there have been many methods for model selection that measure the size of a model in terms of the number of features or parameters in the model, e.g., (Akaike, 1973; Rissanen, 1978; Schwarz, 1978). Surprisingly, for exponential language models, the number of model parameters seems to matter not at all; all that matters are the magnitudes of the parameter values. Consequently, one can improve such models by adding features (or a prior model) that reduce parameter values while maintaining training performance. In (Chen, 2009), we show how these ideas can be used to motivate heuristics for improving the performance of existing language models, and use these heuristics to develop a novel class-based model and a regularized version of MDI models that outperform comparable methods in both pe</context>
</contexts>
<marker>Schwarz, 1978</marker>
<rawString>Gideon Schwarz. 1978. Estimating the dimension of a model. Annals of Statistics, 6(2):461–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
</authors>
<title>Cross-validatory choice and assessment of statistical predictions.</title>
<date>1974</date>
<journal>J. of the Royal Statistical Society B,</journal>
<pages>36--111</pages>
<contexts>
<context position="27898" citStr="Stone, 1974" startWordPosition="4847" endWordPosition="4848">k, 1998); this technique has been used to compute error bounds for many types of classifiers. For example, Bartlett (1998) shows that for a neural network with small weights and small training set squared error, the true error depends on the size of its weights rather than the number of weights; this finding is similar in spirit to eq. (4). 456 In practice, the most accurate methods for performance prediction in many contexts are data-splitting methods (Guyon et al., 2006). These techniques include the hold-out method; leave-one-out and k-fold cross-validation; and bootstrapping (Allen, 1974; Stone, 1974; Geisser, 1975; Craven and Wahba, 1979; Efron, 1983). However, unlike non-datasplitting methods, these methods do not lend themselves well to providing insight into model design as discussed in Section 6. 6 Discussion We show that for several types of exponential language models, it is possible to accurately predict the cross-entropy of test data using the simple relationship given in eq. (4). When using E1 + E2 2 regularization with (α = 0.5, σ2 = 6), the value γ = 0.938 works well across varying model types, domains, vocabulary sizes, training set sizes, and n-gram orders, yielding a mean a</context>
</contexts>
<marker>Stone, 1974</marker>
<rawString>M. Stone. 1974. Cross-validatory choice and assessment of statistical predictions. J. of the Royal Statistical Society B, 36:111–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1994</date>
<tech>Tech. report,</tech>
<institution>Univ. of Toronto. Vladimir</institution>
<contexts>
<context position="5775" citStr="Tibshirani, 1994" startWordPosition="953" endWordPosition="954">ula for predicting n-gram model performance, and provide an explanation for why eq. (4) works so well. In Section 4, we evaluate how well eq. (4) holds for several class-based language models and minimum discrimination information models. Finally, in Sections 5 and 6 we discuss related work and conclusions. 2 Selecting Regularization Settings In this section, we address the issue of how to perform regularization in our later experiments. Following the terminology of Dudik and Schapire (2006), the most widely-used and effective methods for regularizing exponential models are `1 regularization (Tibshirani, 1994; Kazama and Tsujii, 2003; data token range training voc. source type of n sents. size A RH letter 2–7 100–75k 27 B WSJ POS 2–7 100–30k 45 C WSJ word 2–5 100–100k 300 D WSJ word 2–5 100–100k 3k E WSJ word 2–5 100–100k 21k F BN word 2–5 100–100k 84k G SWB word 2–5 100–100k 19k Table 1: Statistics of data sets. RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. Goodman, 2004) and `2 2 regularization (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001). While not as popular, another regularization scheme that has been shown to be effectiv</context>
</contexts>
<marker>Tibshirani, 1994</marker>
<rawString>Robert Tibshirani. 1994. Regression shrinkage and selection via the lasso. Tech. report, Univ. of Toronto. Vladimir N. Vapnik. 1998. Statistical Learning Theory.</rawString>
</citation>
<citation valid="false">
<authors>
<author>John Wiley</author>
</authors>
<location>New York.</location>
<marker>Wiley, </marker>
<rawString>John Wiley, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>