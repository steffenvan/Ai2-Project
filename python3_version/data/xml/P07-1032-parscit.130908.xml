<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004635">
<title confidence="0.992285">
Formalism-Independent Parser Evaluation with CCG and DepBank
</title>
<author confidence="0.997869">
Stephen Clark
</author>
<affiliation confidence="0.997994">
Oxford University Computing Laboratory
</affiliation>
<address confidence="0.991285">
Wolfson Building, Parks Road
Oxford, OX1 3QD, UK
</address>
<email confidence="0.99918">
stephen.clark@comlab.ox.ac.uk
</email>
<author confidence="0.993132">
James R. Curran
</author>
<affiliation confidence="0.9979875">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.476106">
NSW 2006, Australia
</address>
<email confidence="0.997786">
james@it.usyd.edu.au
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999722">
A key question facing the parsing commu-
nity is how to compare parsers which use
different grammar formalisms and produce
different output. Evaluating a parser on the
same resource used to create it can lead
to non-comparable accuracy scores and an
over-optimistic view of parser performance.
In this paper we evaluate a CCG parser on
DepBank, and demonstrate the difficulties
in converting the parser output into Dep-
Bank grammatical relations. In addition we
present a method for measuring the effec-
tiveness of the conversion, which provides
an upper bound on parsing accuracy. The
CCG parser obtains an F-score of 81.9%
on labelled dependencies, against an upper
bound of 84.8%. We compare the CCG
parser against the RASP parser, outperform-
ing RASP by over 5% overall and on the ma-
jority of dependency types.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994025">
Parsers have been developed for a variety of gram-
mar formalisms, for example HPSG (Toutanova et
al., 2002; Malouf and van Noord, 2004), LFG (Ka-
plan et al., 2004; Cahill et al., 2004), TAG (Sarkar
and Joshi, 2003), CCG (Hockenmaier and Steed-
man, 2002; Clark and Curran, 2004b), and variants
of phrase-structure grammar (Briscoe et al., 2006),
including the phrase-structure grammar implicit in
the Penn Treebank (Collins, 2003; Charniak, 2000).
Different parsers produce different output, for ex-
ample phrase structure trees (Collins, 2003), depen-
dency trees (Nivre and Scholz, 2004), grammati-
cal relations (Briscoe et al., 2006), and formalism-
specific dependencies (Clark and Curran, 2004b).
This variety of formalisms and output creates a chal-
lenge for parser evaluation.
The majority of parser evaluations have used test
sets drawn from the same resource used to develop
the parser. This allows the many parsers based on
the Penn Treebank, for example, to be meaningfully
compared. However, there are two drawbacks to this
approach. First, parser evaluations using different
resources cannot be compared; for example, the Par-
seval scores obtained by Penn Treebank parsers can-
not be compared with the dependency F-scores ob-
tained by evaluating on the Parc Dependency Bank.
Second, using the same resource for development
and testing can lead to an over-optimistic view of
parser performance.
In this paper we evaluate a CCG parser (Clark
and Curran, 2004b) on the Briscoe and Carroll ver-
sion of DepBank (Briscoe and Carroll, 2006). The
CCG parser produces head-dependency relations, so
evaluating the parser should simply be a matter of
converting the CCG dependencies into those in Dep-
Bank. Such conversions have been performed for
other parsers, including parsers producing phrase
structure output (Kaplan et al., 2004; Preiss, 2003).
However, we found that performing such a conver-
sion is a time-consuming and non-trivial task.
The contributions of this paper are as follows.
First, we demonstrate the considerable difficulties
associated with formalism-independent parser eval-
uation, highlighting the problems in converting the
</bodyText>
<page confidence="0.962219">
248
</page>
<note confidence="0.925322">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999483166666667">
output of a parser from one representation to an-
other. Second, we develop a method for measur-
ing how effective the conversion process is, which
also provides an upper bound for the performance of
the parser, given the conversion process being used;
this method can be adapted by other researchers
to strengthen their own parser comparisons. And
third, we provide the first evaluation of a wide-
coverage CCG parser outside of CCGbank, obtaining
impressive results on DepBank and outperforming
the RASP parser (Briscoe et al., 2006) by over 5%
overall and on the majority of dependency types.
</bodyText>
<sectionHeader confidence="0.995783" genericHeader="method">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999984725000001">
The most common form of parser evaluation is to ap-
ply the Parseval metrics to phrase-structure parsers
based on the Penn Treebank, and the highest re-
ported scores are now over 90% (Bod, 2003; Char-
niak and Johnson, 2005). However, it is unclear
whether these high scores accurately reflect the per-
formance of parsers in applications. It has been ar-
gued that the Parseval metrics are too forgiving and
that phrase structure is not the ideal representation
for a gold standard (Carroll et al., 1998). Also, us-
ing the same resource for training and testing may
result in the parser learning systematic errors which
are present in both the training and testing mate-
rial. An example of this is from CCGbank (Hock-
enmaier, 2003), where all modifiers in noun-noun
compound constructions modify the final noun (be-
cause the Penn Treebank, from which CCGbank is
derived, does not contain the necessary information
to obtain the correct bracketing). Thus there are non-
negligible, systematic errors in both the training and
testing material, and the CCG parsers are being re-
warded for following particular mistakes.
There are parser evaluation suites which have
been designed to be formalism-independent and
which have been carefully and manually corrected.
Carroll et al. (1998) describe such a suite, consisting
of sentences taken from the Susanne corpus, anno-
tated with Grammatical Relations (GRs) which spec-
ify the syntactic relation between a head and depen-
dent. Thus all that is required to use such a scheme,
in theory, is that the parser being evaluated is able
to identify heads. A similar resource — the Parc
Dependency Bank (DepBank) (King et al., 2003)
— has been created using sentences from the Penn
Treebank. Briscoe and Carroll (2006) reannotated
this resource using their GRs scheme, and used it to
evaluate the RASP parser.
Kaplan et al. (2004) compare the Collins (2003)
parser with the Parc LFG parser by mapping LFG F-
structures and Penn Treebank parses into DepBank
dependencies, claiming that the LFG parser is con-
siderably more accurate with only a slight reduc-
tion in speed. Preiss (2003) compares the parsers of
Collins (2003) and Charniak (2000), the GR finder
of Buchholz et al. (1999), and the RASP parser, us-
ing the Carroll et al. (1998) gold-standard. The Penn
Treebank trees of the Collins and Charniak parsers,
and the GRs of the Buchholz parser, are mapped into
the required GRs, with the result that the GR finder
of Buchholz is the most accurate.
The major weakness of these evaluations is that
there is no measure of the difficultly of the conver-
sion process for each of the parsers. Kaplan et al.
(2004) clearly invested considerable time and ex-
pertise in mapping the output of the Collins parser
into the DepBank dependencies, but they also note
that “This conversion was relatively straightforward
for LFG structures ... However, a certain amount of
skill and intuition was required to provide a fair con-
version of the Collins trees”. Without some measure
of the difficulty — and effectiveness — of the con-
version, there remains a suspicion that the Collins
parser is being unfairly penalised.
One way of providing such a measure is to con-
vert the original gold standard on which the parser
is based and evaluate that against the new gold stan-
dard (assuming the two resources are based on the
same corpus). In the case of Kaplan et al. (2004), the
testing procedure would include running their con-
version process on Section 23 of the Penn Treebank
and evaluating the output against DepBank. As well
as providing some measure of the effectiveness of
the conversion, this method would also provide an
upper bound for the Collins parser, giving the score
that a perfect Penn Treebank parser would obtain on
DepBank (given the conversion process).
We perform such an evaluation for the CCG parser,
with the surprising result that the upper bound on
DepBank is only 84.8%, despite the considerable ef-
fort invested in developing the conversion process.
</bodyText>
<page confidence="0.944866">
249
</page>
<bodyText confidence="0.987523361702128">
3 The CCG Parser declarative sentence is persuade; and the head of the
Clark and Curran (2004b) describes the CCG parser infinitival complement’s subject is identified with
used for the evaluation. The grammar used by the the head of the object, using the variable X, as in
parser is extracted from CCGbank, a CCG version of standard unification-based accounts of control.
the Penn Treebank (Hockenmaier, 2003). The gram- Previous evaluations of CCG parsers have used the
mar consists of 425 lexical categories — expressing predicate-argument dependencies from CCGbank as
subcategorisation information — plus a small num- a test set (Hockenmaier and Steedman, 2002; Clark
ber of combinatory rules which combine the cate- and Curran, 2004b), with impressive results of over
gories (Steedman, 2000). A supertagger first assigns 84% F-score on labelled dependencies. In this paper
lexical categories to the words in a sentence, which we reinforce the earlier results with the first evalua-
are then combined by the parser using the combi- tion of a CCG parser outside of CCGbank.
natory rules and the CKY algorithm. A log-linear 4 Dependency Conversion to DepBank
model scores the alternative parses. We use the For the gold standard we chose the version of Dep-
normal-form model, which assigns probabilities to Bank reannotated by Briscoe and Carroll (2006),
single derivations based on the normal-form deriva- consisting of 700 sentences from Section 23 of the
tions in CCGbank. The features in the model are Penn Treebank. The B&amp;C scheme is similar to the
defined over local parts of the derivation and include original DepBank scheme (King et al., 2003), but
word-word dependencies. A packed chart represen- overall contains less grammatical detail; Briscoe and
tation allows efficient decoding, with the Viterbi al- Carroll (2006) describes the differences. We chose
gorithm finding the most probable derivation. this resource for the following reasons: it is pub-
The parser outputs predicate-argument dependen- licly available, allowing other researchers to com-
cies defined in terms of CCG lexical categories. pare against our results; the GRs making up the an-
More formally, a CCG predicate-argument depen- notation share some similarities with the predicate-
dency is a 5-tuple: (hf, f, s, ha, l), where hf is the argument dependencies output by the CCG parser;
lexical item of the lexical category expressing the and we can directly compare our parser against a
dependency relation; f is the lexical category; s is non-CCG parser, namely the RASP parser. We chose
the argument slot; ha is the head word of the ar- not to use the corpus based on the Susanne corpus
gument; and l encodes whether the dependency is (Carroll et al., 1998) because the GRs are less like
long-range. For example, the dependency encoding the CCG dependencies; the corpus is not based on
company as the object of bought (as in IBM bought the Penn Treebank, making comparison more diffi-
the company) is represented as follows: cult because of tokenisation differences, for exam-
(bought, (S\NP1)/NP2, 2, company, −) (1) ple; and the latest results for RASP are on DepBank.
The lexical category (S\NP1)/NP2 is the cate- The GRs are described in Briscoe and Carroll
gory of a transitive verb, with the first argument slot (2006) and Briscoe et al. (2006). Table 1 lists the
corresponding to the subject, and the second argu- GRs used in the evaluation. As an example, the sen-
ment slot corresponding to the direct object. The tence The parent sold Imperial produces three GRs:
final field indicates the nature of any long-range de- (det parent The),(ncsubj sold parent ) and
pendency; in (1) the dependency is local. (dobj sold Imperial). Note that some GRs — in
The predicate-argument dependencies — includ- this example ncsubj — have a subtype slot, giving
ing long-range dependencies — are encoded in the extra information. The subtype slot for ncsubj is
lexicon by adding head and dependency annota- used to indicate passive subjects, with the null value
tion to the lexical categories. For example, the “ ” for active subjects and obj for passive subjects.
expanded category for the control verb persuade Other subtype slots are discussed in Section 4.2.
is (((S[dcl]persuade\NP1)/(S[to]2\NPX))/NPX,3). Nu- The CCG dependencies were transformed into
merical subscripts on the argument categories rep- GRs in two stages. The first stage was to create
resent dependency relations; the head of the final a mapping between the CCG dependencies and the
250
</bodyText>
<figure confidence="0.9489293125">
GR description
conj coordinator
aux auxiliary
det determiner
ncmod non-clausal modifier
xmod unsaturated predicative modifier
cmod saturated clausal modifier
pmod PP modifier with a PP complement
ncsubj non-clausal subject
xsubj unsaturated predicative subject
csubj saturated clausal subject
dobj direct object
obj2 second object
iobj indirect object
pcomp PP which is a PP complement
xcomp unsaturated VP complement
</figure>
<tableCaption confidence="0.724889666666667">
ccomp saturated clausal complement
ta textual adjunct delimited by punctuation
Table 1: GRs in B&amp;C’s annotation of DepBank
</tableCaption>
<bodyText confidence="0.999962678571429">
GRs. This involved mapping each argument slot in
the 425 lexical categories in the CCG lexicon onto
a GR. In the second stage, the GRs created from the
parser output were post-processed to correct some of
the obvious remaining differences between the CCG
and GR representations.
In the process of performing the transformation
we encountered a methodological problem: with-
out looking at examples it was difficult to create
the mapping and impossible to know whether the
two representations were converging. Briscoe et al.
(2006) split the 700 sentences in DepBank into a test
and development set, but the latter only consists of
140 sentences which was not enough to reliably cre-
ate the transformation. There are some development
files in the RASP release which provide examples of
the GRs, which were used when possible, but these
only cover a subset of the CCG lexical categories.
Our solution to this problem was to convert the
gold standard dependencies from CCGbank into
GRs and use these to develop the transformation. So
we did inspect the annotation in DepBank, and com-
pared it to the transformed CCG dependencies, but
only the gold-standard CCG dependencies. Thus the
parser output was never used during this process.
We also ensured that the dependency mapping and
the post processing are general to the GRs scheme
and not specific to the test set or parser.
</bodyText>
<subsectionHeader confidence="0.926678">
4.1 Mapping the CCG dependencies to GRs
</subsectionHeader>
<tableCaption confidence="0.75999">
Table 2 gives some examples of the mapping; %l in-
dicates the word associated with the lexical category
</tableCaption>
<table confidence="0.961719846153846">
CCG lexical category slot GR
(S[dcl]\NP1)/NP2 1 (ncsubj %l %f )
(S[dcl]\NP1)/NP2 2 (dobj %l %f)
(S\NP)/(S\NP)1 1 (ncmod %f %l)
(NP\NP1)/NP2 1 (ncmod %f %l)
(NP\NP1)/NP2 2 (dobj %l %f)
NP[nb]/N1 1 (det %f %l)
(NP\NP1)/(S[pss]\NP)2 1 (xmod %f %l)
(NP\NP1)/(S[pss]\NP)2 2 (xcomp %l %f)
((S\NP)\(S\NP)1)/S[dcl]2 1 (cmod %f %l)
((S\NP)\(S\NP)1)/S[dcl]2 2 (ccomp %l %f)
((S[dcl]\NP1)/NP2)/NP3 2 (obj2 %l %f)
(S[dcl]\NP1)/(S[b]\NP)2 2 (aux %f %l)
</table>
<tableCaption confidence="0.999098">
Table 2: Examples of the dependency mapping
</tableCaption>
<bodyText confidence="0.999876527777778">
and %f is the head of the constituent filling the argu-
ment slot. Note that the order of %l and %f varies ac-
cording to whether the GR represents a complement
or modifier, in line with the Briscoe and Carroll an-
notation. For many of the CCG dependencies, the
mapping into GRs is straightforward. For example,
the first two rows of Table 2 show the mapping for
the transitive verb category (S[dcl]\NP1)/NP2: ar-
gument slot 1 is a non-clausal subject and argument
slot 2 is a direct object.
Creating the dependency transformation is more
difficult than these examples suggest. The first prob-
lem is that the mapping from CCG dependencies to
GRs is many-to-many. For example, the transitive
verb category (S[dcl]\NP)/NP applies to the cop-
ula in sentences like Imperial Corp. is the parent
ofImperial Savings &amp; Loan. With the default anno-
tation, the relation between is and parent would be
dobj, whereas in DepBank the argument of the cop-
ula is analysed as an xcomp. Table 3 gives some ex-
amples of how we attempt to deal with this problem.
The constraint in the first example means that, when-
ever the word associated with the transitive verb cat-
egory is a form of be, the second argument is xcomp,
otherwise the default case applies (in this case dobj).
There are a number of categories with similar con-
straints, checking whether the word associated with
the category is a form of be.
The second type of constraint, shown in the third
line of the table, checks the lexical category of the
word filling the argument slot. In this example, if the
lexical category of the preposition is PP/NP, then
the second argument of (S[dcl]\NP)/PP maps to
iobj; thus in The loss stems from several fac-
tors the relation between the verb and preposition
is (iobj stems from). If the lexical category of
</bodyText>
<page confidence="0.989175">
251
</page>
<table confidence="0.905903833333333">
CCG lexical category slot GR constraint example
(S[dcl]\NP1)/NP2 2 (xcomp %l %f) word=be The parent is Imperial
(dobj %l %f) The parent sold Imperial
(S[dcl]\NP1)/PP2 2 (iobj %l %f) cat=PP/NP The loss stems from several factors
(xcomp %l %f) cat=PP/(S[ng]\NP) The future depends on building ties
(S[dcl]\NP1)/(S[to]\NP)2 2 (xcomp %f %l %k) cat=(S[to]\NP)/(S[b]\NP) wants to wean itself away from
</table>
<tableCaption confidence="0.999143">
Table 3: Examples of the many-to-many nature of the CCG dependency to GRs mapping, and a ternary GR
</tableCaption>
<bodyText confidence="0.999888839285714">
the preposition is PP/(S[ng]\NP), then the GR
is xcomp; thus in The future depends on building
ties the relation between the verb and preposition
is (xcomp depends on). There are a number of
CCG dependencies with similar constraints, many of
them covering the iobj/xcomp distinction.
The second difficulty is that not all the GRs are bi-
nary relations, whereas the CCG dependencies are all
binary. The primary example of this is to-infinitival
constructions. For example, in the sentence The
company wants to wean itself away from expensive
gimmicks, the CCG parser produces two dependen-
cies relating wants, to and wean, whereas there is
only one GR: (xcomp to wants wean). The fi-
nal row of Table 3 gives an example. We im-
plement this constraint by introducing a %k vari-
able into the GR template which denotes the ar-
gument of the category in the constraint column
(which, as before, is the lexical category of the
word filling the argument slot). In the example, the
current category is (S[dcl]\NP1)/(S[to]\NP)2,
which is associated with wants; this combines with
(S[to]\NP)/(S[b]\NP), associated with to; and
the argument of (S[to]\NP)/(S[b]\NP) is wean.
The %k variable allows us to look beyond the argu-
ments of the current category when creating the GRs.
A further difficulty is that the head passing con-
ventions differ between DepBank and CCGbank. By
head passing we mean the mechanism which de-
termines the heads of constituents and the mecha-
nism by which words become arguments of long-
range dependencies. For example, in the sentence
The group said it would consider withholding roy-
alty payments, the DepBank and CCGbank annota-
tions create a dependency between said and the fol-
lowing clause. However, in DepBank the relation
is between said and consider, whereas in CCGbank
the relation is between said and would. We fixed this
problem by defining the head of would consider to
be consider rather than would, by changing the an-
notation of all the relevant lexical categories in the
CCG lexicon (mainly those creating aux relations).
There are more subject relations in CCGbank than
DepBank. In the previous example, CCGbank has a
subject relation between it and consider, and also it
and would, whereas DepBank only has the relation
between it and consider. In practice this means ig-
noring a number of the subject dependencies output
by the CCG parser.
Another example where the dependencies differ
is the treatment of relative pronouns. For example,
in Sen. Mitchell, who had proposed the streamlin-
ing, the subject of proposed is Mitchell in CCGbank
but who in DepBank. Again, we implemented this
change by fixing the head annotation in the lexical
categories which apply to relative pronouns.
</bodyText>
<subsectionHeader confidence="0.997353">
4.2 Post processing of the GR output
</subsectionHeader>
<bodyText confidence="0.999854791666667">
To obtain some idea of whether the schemes were
converging, we performed the following oracle ex-
periment. We took the CCG derivations from
CCGbank corresponding to the sentences in Dep-
Bank, and forced the parser to produce gold-
standard derivations, outputting the newly created
GRs. Treating the DepBank GRs as a gold-standard,
and comparing these with the CCGbank GRs, gave
precision and recall scores of only 76.23% and
79.56% respectively (using the RASP evaluation
tool). Thus given the current mapping, the perfect
CCGbank parser would achieve an F-score of only
77.86% when evaluated against DepBank.
On inspecting the output, it was clear that a
number of general rules could be applied to bring
the schemes closer together, which was imple-
mented as a post-processing script. The first set
of changes deals with coordination. One sig-
nificant difference between DepBank and CCG-
bank is the treatment of coordinations as argu-
ments. Consider the example The president and
chief executive officer said the loss stems from sev-
eral factors. For both DepBank and the trans-
formed CCGbank there are two conj GRs arising
</bodyText>
<page confidence="0.992955">
252
</page>
<bodyText confidence="0.999985861111111">
from the coordination: (conj and president) and
(conj and officer). The difference arises in the
subject of said: in DepBank the subject is and:
(ncsubj said and ), whereas in CCGbank there
are two subjects: (ncsubj said president ) and
(ncsubj said officer ). We deal with this dif-
ference by replacing any pairs of GRs which differ
only in their arguments, and where the arguments
are coordinated items, with a single GR containing
the coordination term as the argument.
Ampersands are a frequently occurring problem
in WSJ text. For example, the CCGbank analysis
of Standard &amp; Poor’s index assigns the lexical cat-
egory N/N to both Standard and &amp;, treating them
as modifiers of Poor, whereas DepBank treats &amp; as
a coordinating term. We fixed this by creating conj
GRs between any &amp; and the two words either side;
removing the modifier GR between the two words;
and replacing any GRs in which the words either side
of the &amp; are arguments with a single GR in which &amp;
is the argument.
The ta relation, which identifies text adjuncts de-
limited by punctuation, is difficult to assign cor-
rectly to the parser output. The simple punctuation
rules used by the parser do not contain enough in-
formation to distinguish between the various cases
of ta. Thus the only rule we have implemented,
which is somewhat specific to the newspaper genre,
is to replace GRs of the form (cmod say arg)
with (ta quote arg say), where say can be any
of say, said or says. This rule applies to only a small
subset of the ta cases but has high enough precision
to be worthy of inclusion.
A common source of error is the distinction be-
tween iobj and ncmod, which is not surprising given
the difficulty that human annotators have in distin-
guishing arguments and adjuncts. There are many
cases where an argument in DepBank is an adjunct
in CCGbank, and vice versa. The only change we
have made is to turn all ncmod GRs with of as the
modifier into iobj GRs (unless the ncmod is a par-
titive predeterminer). This was found to have high
precision and applies to a large number of cases.
There are some dependencies in CCGbank which
do not appear in DepBank. Examples include any
dependencies in which a punctuation mark is one of
the arguments; these were removed from the output.
We attempt to fill the subtype slot for some GRs.
The subtype slot specifies additional information
about the GR; examples include the value obj in a
passive ncsubj, indicating that the subject is an un-
derlying object; the value num in ncmod, indicating a
numerical quantity; and prt in ncmod to indicate a
verb particle. The passive case is identified as fol-
lows: any lexical category which starts S[pss]\NP
indicates a passive verb, and we also mark any verbs
POS tagged VBN and assigned the lexical category
N/N as passive. Both these rules have high preci-
sion, but still leave many of the cases in DepBank
unidentified. The numerical case is identified using
two rules: the num subtype is added if any argument
in a GR is assigned the lexical category N/N [num],
and if any of the arguments in an ncmod is POS
tagged CD. prt is added to an ncmod if the modi-
fiee has any of the verb POS tags and if the modifier
has POS tag RP.
The final columns of Table 4 show the accuracy
of the transformed gold-standard CCGbank depen-
dencies when compared with DepBank; the sim-
ple post-processing rules have increased the F-score
from 77.86% to 84.76%. This F-score is an upper
bound on the performance of the CCG parser.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999935095238095">
The results in Table 4 were obtained by parsing the
sentences from CCGbank corresponding to those
in the 560-sentence test set used by Briscoe et al.
(2006). We used the CCGbank sentences because
these differ in some ways to the original Penn Tree-
bank sentences (there are no quotation marks in
CCGbank, for example) and the parser has been
trained on CCGbank. Even here we experienced
some unexpected difficulties, since some of the to-
kenisation is different between DepBank and CCG-
bank and there are some sentences in DepBank
which have been significantly shortened compared
to the original Penn Treebank sentences. We mod-
ified the CCGbank sentences — and the CCGbank
analyses since these were used for the oracle ex-
periments — to be as close to the DepBank sen-
tences as possible. All the results were obtained us-
ing the RASP evaluation scripts, with the results for
the RASP parser taken from Briscoe et al. (2006).
The results for CCGbank were obtained using the
oracle method described above.
</bodyText>
<page confidence="0.996445">
253
</page>
<table confidence="0.999964095238095">
Relation Prec RASP F Prec CCG parser F CCGbank F # GRs
Rec Rec Prec Rec
aux 93.33 91.00 92.15 94.20 89.25 91.66 96.47 90.33 93.30 400
conj 72.39 72.27 72.33 79.73 77.98 78.84 83.07 80.27 81.65 595
ta 42.61 51.37 46.58 52.31 11.64 19.05 62.07 12.59 20.93 292
det 87.73 90.48 89.09 95.25 95.42 95.34 97.27 94.09 95.66 1114
ncmod 75.72 69.94 72.72 75.75 79.27 77.47 78.88 80.64 79.75 3 550
xmod 53.21 46.63 49.70 43.46 52.25 47.45 56.54 60.67 58.54 178
cmod 45.95 30.36 36.56 51.50 61.31 55.98 64.77 69.09 66.86 168
pmod 30.77 33.33 32.00 0.00 0.00 0.00 0.00 0.00 0.00 12
ncsubj 79.16 67.06 72.61 83.92 75.92 79.72 88.86 78.51 83.37 1354
xsubj 33.33 28.57 30.77 0.00 0.00 0.00 50.00 28.57 36.36 7
csubj 12.50 50.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 2
dobj 83.63 79.08 81.29 87.03 89.40 88.20 92.11 90.32 91.21 1764
obj2 23.08 30.00 26.09 65.00 65.00 65.00 66.67 60.00 63.16 20
iobj 70.77 76.10 73.34 77.60 70.04 73.62 83.59 69.81 76.08 544
xcomp 76.88 77.69 77.28 76.68 77.69 77.18 80.00 78.49 79.24 381
ccomp 46.44 69.42 55.55 79.55 72.16 75.68 80.81 76.31 78.49 291
pcomp 72.73 66.67 69.57 0.00 0.00 0.00 0.00 0.00 0.00 24
macroaverage 62.12 63.77 62.94 65.61 63.28 64.43 71.73 65.85 68.67
microaverage 77.66 74.98 76.29 82.44 81.28 81.86 86.86 82.75 84.76
</table>
<tableCaption confidence="0.9274995">
Table 4: Accuracy on DepBank. F-score is the balanced harmonic mean of precision (P) and recall (R):
2PR/(P + R). # GRs is the number of GRs in DepBank.
</tableCaption>
<bodyText confidence="0.99984244">
The CCG parser results are based on automati-
cally assigned POS tags, using the Curran and Clark
(2003) tagger. The coverage of the parser on Dep-
Bank is 100%. For a GR in the parser output to be
correct, it has to match the gold-standard GR exactly,
including any subtype slots; however, it is possible
for a GR to be incorrect at one level but correct at
a subsuming level.1 For example, if an ncmod GR is
incorrectly labelled with xmod, but is otherwise cor-
rect, it will be correct for all levels which subsume
both ncmod and xmod, for example mod. The micro-
averaged scores are calculated by aggregating the
counts for all the relations in the hierarchy, including
the subsuming relations; the macro-averaged scores
are the mean of the individual scores for each rela-
tion (Briscoe et al., 2006).
The results show that the performance of the CCG
parser is higher than RASP overall, and also higher
on the majority of GR types (especially the more
frequent types). RASP uses an unlexicalised pars-
ing model and has not been tuned to newspaper text.
On the other hand it has had many years of develop-
ment; thus it provides a strong baseline for this test
set. The overall F-score for the CCG parser, 81.86%,
is only 3 points below that for CCGbank, which pro-
</bodyText>
<footnote confidence="0.955309333333333">
1The GRs are arranged in a hierarchy, with those in Table 1 at
the leaves; a small number of more general GRs subsume these
(Briscoe and Carroll, 2006).
</footnote>
<bodyText confidence="0.33981">
vides an upper bound for the CCG parser (given the
conversion process being used).
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974565217391">
A contribution of this paper has been to high-
light the difficulties associated with cross-formalism
parser comparison. Note that the difficulties are not
unique to CCG, and many would apply to any cross-
formalism comparison, especially with parsers using
automatically extracted grammars. Parser evalua-
tion has improved on the original Parseval measures
(Carroll et al., 1998), but the challenge remains to
develop a representation and evaluation suite which
can be easily applied to a wide variety of parsers
and formalisms. Despite the difficulties, we have
given the first evaluation of a CCG parser outside of
CCGbank, outperforming the RASP parser by over
5% overall and on the majority of dependency types.
Can the CCG parser be compared with parsers
other than RASP? Briscoe and Carroll (2006) give a
rough comparison of RASP with the Parc LFG parser
on the different versions of DepBank, obtaining sim-
ilar results overall, but they acknowledge that the re-
sults are not strictly comparable because of the dif-
ferent annotation schemes used. Comparison with
Penn Treebank parsers would be difficult because,
for many constructions, the Penn Treebank trees and
</bodyText>
<page confidence="0.993146">
254
</page>
<bodyText confidence="0.999871076923077">
CCG derivations are different shapes, and reversing
the mapping Hockenmaier used to create CCGbank
would be very difficult. Hence we challenge other
parser developers to map their own parse output into
the version of DepBank used here.
One aspect of parser evaluation not covered in this
paper is efficiency. The CCG parser took only 22.6
seconds to parse the 560 sentences in DepBank, with
the accuracy given earlier. Using a cluster of 18 ma-
chines we have also parsed the entire Gigaword cor-
pus in less than five days. Hence, we conclude that
accurate, large-scale, linguistically-motivated NLP is
now practical with CCG.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999909">
We would like to thanks the anonymous review-
ers for their helpful comments. James Curran was
funded under ARC Discovery grants DP0453131
and DP0665973.
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901158536585">
Rens Bod. 2003. An efficient implementation of a new DOP
model. In Proceedings of the 10th Meeting of the EACL,
pages 19–26, Budapest, Hungary.
Ted Briscoe and John Carroll. 2006. Evaluating the accuracy
of an unlexicalized statistical parser on the PARC DepBank.
In Proceedings of the Poster Session of COLING/ACL-06,
Sydney, Australia.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In Proceedings of
the Interactive Demo Session of COLING/ACL-06, Sydney,
Australia.
Sabine Buchholz, Jorn Veenstra, and Walter Daelemans. 1999.
Cascaded grammatical relation assignment. In Proceedings
of EMNLP/VLC-99, pages 239–246, University of Mary-
land, June 21-22.
A. Cahill, M. Burke, R. O’Donovan, J. van Genabith, and
A. Way. 2004. Long-distance dependency resolution in au-
tomatically acquired wide-coverage PCFG-based LFG ap-
proximations. In Proceedings of the 42nd Meeting of the
ACL, pages 320–327, Barcelona, Spain.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: a survey and a new proposal. In Proceed-
ings of the 1st LREC Conference, pages 447–454, Granada,
Spain.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the ACL, University
of Michigan, Ann Arbor.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the NAACL, pages 132–
139, Seattle, WA.
Stephen Clark and James R. Curran. 2004a. The importance of
supertagging for wide-coverage CCG parsing. In Proceed-
ings of COLING-04, pages 282–288, Geneva, Switzerland.
Stephen Clark and James R. Curran. 2004b. Parsing the WSJ
using CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104–111, Barcelona, Spain.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):589–637.
James R. Curran and Stephen Clark. 2003. Investigating GIS
and smoothing for maximum entropy taggers. In Proceed-
ings of the 10th Meeting of the EACL, pages 91–98, Bu-
dapest, Hungary.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with Combinatory Categorial
Grammar. In Proceedings of the 40th Meeting of the ACL,
pages 335–342, Philadelphia, PA.
Julia Hockenmaier. 2003. Data and Models for Statistical
Parsing with Combinatory Categorial Grammar. Ph.D. the-
sis, University of Edinburgh.
Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell
III, Alexander Vasserman, and Richard Crouch. 2004.
Speed and accuracy in shallow and deep stochastic parsing.
In Proceedings of the HLT Conference and the 4th NAACL
Meeting (HLT-NAACL’04), Boston, MA.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrym-
ple, and Ronald M. Kaplan. 2003. The PARC 700 Depen-
dency Bank. In Proceedings of the LINC-03 Workshop, Bu-
dapest, Hungary.
Robert Malouf and Gertjan van Noord. 2004. Wide coverage
parsing with stochastic attribute value grammars. In Pro-
ceedings of the IJCNLP-04 Workshop: Beyond shallow anal-
yses - Formalisms and statistical modelingfor deep analyses,
Hainan Island, China.
Joakim Nivre and Mario Scholz. 2004. Deterministic depen-
dency parsing of English text. In Proceedings of COLING-
2004, pages 64–70, Geneva, Switzerland.
Judita Preiss. 2003. Using grammatical relations to compare
parsers. In Proceedings of the 10th Meeting of the EACL,
pages 291–298, Budapest, Hungary.
Anoop Sarkar and Aravind Joshi. 2003. Tree-adjoining gram-
mars and its application to statistical parsing. In Rens Bod,
Remko Scha, and Khalil Sima’an, editors, Data-oriented
parsing. CSLI.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Kristina Toutanova, Christopher Manning, Stuart Shieber, Dan
Flickinger, and Stephan Oepen. 2002. Parse disambiguation
for a rich HPSG grammar. In Proceedings of the First Work-
shop on Treebanks and Linguistic Theories, pages 253–263,
Sozopol, Bulgaria.
</reference>
<page confidence="0.998407">
255
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941475">
<title confidence="0.999734">Formalism-Independent Parser Evaluation with CCG and DepBank</title>
<author confidence="0.996448">Stephen Clark</author>
<affiliation confidence="0.999999">Oxford University Computing Laboratory</affiliation>
<address confidence="0.9828495">Wolfson Building, Parks Road Oxford, OX1 3QD, UK</address>
<email confidence="0.994224">stephen.clark@comlab.ox.ac.uk</email>
<author confidence="0.999978">James R Curran</author>
<affiliation confidence="0.999733">School of Information Technologies University of Sydney</affiliation>
<address confidence="0.997689">NSW 2006, Australia</address>
<email confidence="0.998909">james@it.usyd.edu.au</email>
<abstract confidence="0.999398714285714">A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%. We compare the against the outperformover 5% overall and on the majority of dependency types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An efficient implementation of a new DOP model.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Meeting of the EACL,</booktitle>
<pages>pages</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="4259" citStr="Bod, 2003" startWordPosition="656" endWordPosition="657">mance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons. And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types. 2 Previous Work The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005). However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998). Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material. An example of this is from CCGbank (Hockenmaier, 2003), where all modifiers in noun-noun compound constructions modify the f</context>
</contexts>
<marker>Bod, 2003</marker>
<rawString>Rens Bod. 2003. An efficient implementation of a new DOP model. In Proceedings of the 10th Meeting of the EACL, pages 19–26, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank.</title>
<date>2006</date>
<booktitle>In Proceedings of the Poster Session of COLING/ACL-06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2670" citStr="Briscoe and Carroll, 2006" startWordPosition="409" endWordPosition="412"> parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be compared with the dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting th</context>
<context position="5805" citStr="Briscoe and Carroll (2006)" startWordPosition="906" endWordPosition="909">e parser evaluation suites which have been designed to be formalism-independent and which have been carefully and manually corrected. Carroll et al. (1998) describe such a suite, consisting of sentences taken from the Susanne corpus, annotated with Grammatical Relations (GRs) which specify the syntactic relation between a head and dependent. Thus all that is required to use such a scheme, in theory, is that the parser being evaluated is able to identify heads. A similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (1998) gold-standard. The Penn Treebank trees of the Collins and Charniak parsers, and the</context>
<context position="9397" citStr="Briscoe and Carroll (2006)" startWordPosition="1500" endWordPosition="1503">urran, 2004b), with impressive results of over gories (Steedman, 2000). A supertagger first assigns 84% F-score on labelled dependencies. In this paper lexical categories to the words in a sentence, which we reinforce the earlier results with the first evaluaare then combined by the parser using the combi- tion of a CCG parser outside of CCGbank. natory rules and the CKY algorithm. A log-linear 4 Dependency Conversion to DepBank model scores the alternative parses. We use the For the gold standard we chose the version of Depnormal-form model, which assigns probabilities to Bank reannotated by Briscoe and Carroll (2006), single derivations based on the normal-form deriva- consisting of 700 sentences from Section 23 of the tions in CCGbank. The features in the model are Penn Treebank. The B&amp;C scheme is similar to the defined over local parts of the derivation and include original DepBank scheme (King et al., 2003), but word-word dependencies. A packed chart represen- overall contains less grammatical detail; Briscoe and tation allows efficient decoding, with the Viterbi al- Carroll (2006) describes the differences. We chose gorithm finding the most probable derivation. this resource for the following reasons:</context>
<context position="28530" citStr="Briscoe and Carroll, 2006" startWordPosition="4733" endWordPosition="4736">e et al., 2006). The results show that the performance of the CCG parser is higher than RASP overall, and also higher on the majority of GR types (especially the more frequent types). RASP uses an unlexicalised parsing model and has not been tuned to newspaper text. On the other hand it has had many years of development; thus it provides a strong baseline for this test set. The overall F-score for the CCG parser, 81.86%, is only 3 points below that for CCGbank, which pro1The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). vides an upper bound for the CCG parser (given the conversion process being used). 6 Conclusion A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison. Note that the difficulties are not unique to CCG, and many would apply to any crossformalism comparison, especially with parsers using automatically extracted grammars. Parser evaluation has improved on the original Parseval measures (Carroll et al., 1998), but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of p</context>
</contexts>
<marker>Briscoe, Carroll, 2006</marker>
<rawString>Ted Briscoe and John Carroll. 2006. Evaluating the accuracy of an unlexicalized statistical parser on the PARC DepBank. In Proceedings of the Poster Session of COLING/ACL-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Interactive Demo Session of COLING/ACL-06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="1480" citStr="Briscoe et al., 2006" startWordPosition="225" endWordPosition="228"> upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank,</context>
<context position="3992" citStr="Briscoe et al., 2006" startWordPosition="606" endWordPosition="609">48–255, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics output of a parser from one representation to another. Second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons. And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types. 2 Previous Work The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005). However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998). Also, using the same reso</context>
<context position="11367" citStr="Briscoe et al. (2006)" startWordPosition="1822" endWordPosition="1825">ndency is (Carroll et al., 1998) because the GRs are less like long-range. For example, the dependency encoding the CCG dependencies; the corpus is not based on company as the object of bought (as in IBM bought the Penn Treebank, making comparison more diffithe company) is represented as follows: cult because of tokenisation differences, for exam(bought, (S\NP1)/NP2, 2, company, −) (1) ple; and the latest results for RASP are on DepBank. The lexical category (S\NP1)/NP2 is the cate- The GRs are described in Briscoe and Carroll gory of a transitive verb, with the first argument slot (2006) and Briscoe et al. (2006). Table 1 lists the corresponding to the subject, and the second argu- GRs used in the evaluation. As an example, the senment slot corresponding to the direct object. The tence The parent sold Imperial produces three GRs: final field indicates the nature of any long-range de- (det parent The),(ncsubj sold parent ) and pendency; in (1) the dependency is local. (dobj sold Imperial). Note that some GRs — in The predicate-argument dependencies — includ- this example ncsubj — have a subtype slot, giving ing long-range dependencies — are encoded in the extra information. The subtype slot for ncsubj </context>
<context position="13638" citStr="Briscoe et al. (2006)" startWordPosition="2173" endWordPosition="2176">ment ta textual adjunct delimited by punctuation Table 1: GRs in B&amp;C’s annotation of DepBank GRs. This involved mapping each argument slot in the 425 lexical categories in the CCG lexicon onto a GR. In the second stage, the GRs created from the parser output were post-processed to correct some of the obvious remaining differences between the CCG and GR representations. In the process of performing the transformation we encountered a methodological problem: without looking at examples it was difficult to create the mapping and impossible to know whether the two representations were converging. Briscoe et al. (2006) split the 700 sentences in DepBank into a test and development set, but the latter only consists of 140 sentences which was not enough to reliably create the transformation. There are some development files in the RASP release which provide examples of the GRs, which were used when possible, but these only cover a subset of the CCG lexical categories. Our solution to this problem was to convert the gold standard dependencies from CCGbank into GRs and use these to develop the transformation. So we did inspect the annotation in DepBank, and compared it to the transformed CCG dependencies, but o</context>
<context position="24867" citStr="Briscoe et al. (2006)" startWordPosition="4084" endWordPosition="4087">/N [num], and if any of the arguments in an ncmod is POS tagged CD. prt is added to an ncmod if the modifiee has any of the verb POS tags and if the modifier has POS tag RP. The final columns of Table 4 show the accuracy of the transformed gold-standard CCGbank dependencies when compared with DepBank; the simple post-processing rules have increased the F-score from 77.86% to 84.76%. This F-score is an upper bound on the performance of the CCG parser. 5 Results The results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe et al. (2006). We used the CCGbank sentences because these differ in some ways to the original Penn Treebank sentences (there are no quotation marks in CCGbank, for example) and the parser has been trained on CCGbank. Even here we experienced some unexpected difficulties, since some of the tokenisation is different between DepBank and CCGbank and there are some sentences in DepBank which have been significantly shortened compared to the original Penn Treebank sentences. We modified the CCGbank sentences — and the CCGbank analyses since these were used for the oracle experiments — to be as close to the DepB</context>
<context position="27919" citStr="Briscoe et al., 2006" startWordPosition="4621" endWordPosition="4624">parser output to be correct, it has to match the gold-standard GR exactly, including any subtype slots; however, it is possible for a GR to be incorrect at one level but correct at a subsuming level.1 For example, if an ncmod GR is incorrectly labelled with xmod, but is otherwise correct, it will be correct for all levels which subsume both ncmod and xmod, for example mod. The microaveraged scores are calculated by aggregating the counts for all the relations in the hierarchy, including the subsuming relations; the macro-averaged scores are the mean of the individual scores for each relation (Briscoe et al., 2006). The results show that the performance of the CCG parser is higher than RASP overall, and also higher on the majority of GR types (especially the more frequent types). RASP uses an unlexicalised parsing model and has not been tuned to newspaper text. On the other hand it has had many years of development; thus it provides a strong baseline for this test set. The overall F-score for the CCG parser, 81.86%, is only 3 points below that for CCGbank, which pro1The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Car</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the Interactive Demo Session of COLING/ACL-06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
<author>Walter Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC-99,</booktitle>
<pages>239--246</pages>
<institution>University of Maryland,</institution>
<contexts>
<context position="6267" citStr="Buchholz et al. (1999)" startWordPosition="984" endWordPosition="987"> similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (1998) gold-standard. The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required GRs, with the result that the GR finder of Buchholz is the most accurate. The major weakness of these evaluations is that there is no measure of the difficultly of the conversion process for each of the parsers. Kaplan et al. (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that “This conversion was re</context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>Sabine Buchholz, Jorn Veenstra, and Walter Daelemans. 1999. Cascaded grammatical relation assignment. In Proceedings of EMNLP/VLC-99, pages 239–246, University of Maryland, June 21-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Burke</author>
<author>R O’Donovan</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>320--327</pages>
<location>Barcelona,</location>
<marker>Cahill, Burke, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>A. Cahill, M. Burke, R. O’Donovan, J. van Genabith, and A. Way. 2004. Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations. In Proceedings of the 42nd Meeting of the ACL, pages 320–327, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st LREC Conference,</booktitle>
<pages>447--454</pages>
<location>Granada,</location>
<contexts>
<context position="4565" citStr="Carroll et al., 1998" startWordPosition="705" endWordPosition="708">erforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types. 2 Previous Work The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005). However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998). Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material. An example of this is from CCGbank (Hockenmaier, 2003), where all modifiers in noun-noun compound constructions modify the final noun (because the Penn Treebank, from which CCGbank is derived, does not contain the necessary information to obtain the correct bracketing). Thus there are nonnegligible, systematic errors in both the training and testing material, and the CCG parsers are being rewarded for following particular mist</context>
<context position="6321" citStr="Carroll et al. (1998)" startWordPosition="995" endWordPosition="998">(King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (1998) gold-standard. The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required GRs, with the result that the GR finder of Buchholz is the most accurate. The major weakness of these evaluations is that there is no measure of the difficultly of the conversion process for each of the parsers. Kaplan et al. (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that “This conversion was relatively straightforward for LFG structures ... Howeve</context>
<context position="10778" citStr="Carroll et al., 1998" startWordPosition="1724" endWordPosition="1727">e against our results; the GRs making up the anMore formally, a CCG predicate-argument depen- notation share some similarities with the predicatedency is a 5-tuple: (hf, f, s, ha, l), where hf is the argument dependencies output by the CCG parser; lexical item of the lexical category expressing the and we can directly compare our parser against a dependency relation; f is the lexical category; s is non-CCG parser, namely the RASP parser. We chose the argument slot; ha is the head word of the ar- not to use the corpus based on the Susanne corpus gument; and l encodes whether the dependency is (Carroll et al., 1998) because the GRs are less like long-range. For example, the dependency encoding the CCG dependencies; the corpus is not based on company as the object of bought (as in IBM bought the Penn Treebank, making comparison more diffithe company) is represented as follows: cult because of tokenisation differences, for exam(bought, (S\NP1)/NP2, 2, company, −) (1) ple; and the latest results for RASP are on DepBank. The lexical category (S\NP1)/NP2 is the cate- The GRs are described in Briscoe and Carroll gory of a transitive verb, with the first argument slot (2006) and Briscoe et al. (2006). Table 1 l</context>
<context position="29003" citStr="Carroll et al., 1998" startWordPosition="4805" endWordPosition="4808">e GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). vides an upper bound for the CCG parser (given the conversion process being used). 6 Conclusion A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison. Note that the difficulties are not unique to CCG, and many would apply to any crossformalism comparison, especially with parsers using automatically extracted grammars. Parser evaluation has improved on the original Parseval measures (Carroll et al., 1998), but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalisms. Despite the difficulties, we have given the first evaluation of a CCG parser outside of CCGbank, outperforming the RASP parser by over 5% overall and on the majority of dependency types. Can the CCG parser be compared with parsers other than RASP? Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser on the different versions of DepBank, obtaining similar results overall, but they acknowledge that the results are not</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the 1st LREC Conference, pages 447–454, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<institution>University of Michigan,</institution>
<location>Ann Arbor.</location>
<contexts>
<context position="4288" citStr="Charniak and Johnson, 2005" startWordPosition="658" endWordPosition="662">e parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons. And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types. 2 Previous Work The most common form of parser evaluation is to apply the Parseval metrics to phrase-structure parsers based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005). However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998). Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material. An example of this is from CCGbank (Hockenmaier, 2003), where all modifiers in noun-noun compound constructions modify the final noun (because the Penn T</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine nbest parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the ACL, University of Michigan, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the NAACL,</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1582" citStr="Charniak, 2000" startWordPosition="240" endWordPosition="241">st an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, p</context>
<context position="6226" citStr="Charniak (2000)" startWordPosition="978" endWordPosition="979">uated is able to identify heads. A similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (1998) gold-standard. The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required GRs, with the result that the GR finder of Buchholz is the most accurate. The major weakness of these evaluations is that there is no measure of the difficultly of the conversion process for each of the parsers. Kaplan et al. (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but th</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the NAACL, pages 132– 139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>282--288</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1413" citStr="Clark and Curran, 2004" startWordPosition="216" endWordPosition="219">for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop t</context>
<context position="8138" citStr="Clark and Curran (2004" startWordPosition="1303" endWordPosition="1306">n 23 of the Penn Treebank and evaluating the output against DepBank. As well as providing some measure of the effectiveness of the conversion, this method would also provide an upper bound for the Collins parser, giving the score that a perfect Penn Treebank parser would obtain on DepBank (given the conversion process). We perform such an evaluation for the CCG parser, with the surprising result that the upper bound on DepBank is only 84.8%, despite the considerable effort invested in developing the conversion process. 249 3 The CCG Parser declarative sentence is persuade; and the head of the Clark and Curran (2004b) describes the CCG parser infinitival complement’s subject is identified with used for the evaluation. The grammar used by the the head of the object, using the variable X, as in parser is extracted from CCGbank, a CCG version of standard unification-based accounts of control. the Penn Treebank (Hockenmaier, 2003). The gram- Previous evaluations of CCG parsers have used the mar consists of 425 lexical categories — expressing predicate-argument dependencies from CCGbank as subcategorisation information — plus a small num- a test set (Hockenmaier and Steedman, 2002; Clark ber of combinatory ru</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing. In Proceedings of COLING-04, pages 282–288, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the ACL,</booktitle>
<pages>104--111</pages>
<location>Barcelona,</location>
<contexts>
<context position="1413" citStr="Clark and Curran, 2004" startWordPosition="216" endWordPosition="219">for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop t</context>
<context position="8138" citStr="Clark and Curran (2004" startWordPosition="1303" endWordPosition="1306">n 23 of the Penn Treebank and evaluating the output against DepBank. As well as providing some measure of the effectiveness of the conversion, this method would also provide an upper bound for the Collins parser, giving the score that a perfect Penn Treebank parser would obtain on DepBank (given the conversion process). We perform such an evaluation for the CCG parser, with the surprising result that the upper bound on DepBank is only 84.8%, despite the considerable effort invested in developing the conversion process. 249 3 The CCG Parser declarative sentence is persuade; and the head of the Clark and Curran (2004b) describes the CCG parser infinitival complement’s subject is identified with used for the evaluation. The grammar used by the the head of the object, using the variable X, as in parser is extracted from CCGbank, a CCG version of standard unification-based accounts of control. the Penn Treebank (Hockenmaier, 2003). The gram- Previous evaluations of CCG parsers have used the mar consists of 425 lexical categories — expressing predicate-argument dependencies from CCGbank as subcategorisation information — plus a small num- a test set (Hockenmaier and Steedman, 2002; Clark ber of combinatory ru</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Meeting of the ACL, pages 104–111, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1565" citStr="Collins, 2003" startWordPosition="238" endWordPosition="239">ndencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this a</context>
<context position="5944" citStr="Collins (2003)" startWordPosition="931" endWordPosition="932">998) describe such a suite, consisting of sentences taken from the Susanne corpus, annotated with Grammatical Relations (GRs) which specify the syntactic relation between a head and dependent. Thus all that is required to use such a scheme, in theory, is that the parser being evaluated is able to identify heads. A similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (1998) gold-standard. The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required GRs, with the result that the GR finder of Buchholz is the most accurate. The maj</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Meeting of the EACL,</booktitle>
<pages>91--98</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="27226" citStr="Curran and Clark (2003)" startWordPosition="4498" endWordPosition="4501">10 73.34 77.60 70.04 73.62 83.59 69.81 76.08 544 xcomp 76.88 77.69 77.28 76.68 77.69 77.18 80.00 78.49 79.24 381 ccomp 46.44 69.42 55.55 79.55 72.16 75.68 80.81 76.31 78.49 291 pcomp 72.73 66.67 69.57 0.00 0.00 0.00 0.00 0.00 0.00 24 macroaverage 62.12 63.77 62.94 65.61 63.28 64.43 71.73 65.85 68.67 microaverage 77.66 74.98 76.29 82.44 81.28 81.86 86.86 82.75 84.76 Table 4: Accuracy on DepBank. F-score is the balanced harmonic mean of precision (P) and recall (R): 2PR/(P + R). # GRs is the number of GRs in DepBank. The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger. The coverage of the parser on DepBank is 100%. For a GR in the parser output to be correct, it has to match the gold-standard GR exactly, including any subtype slots; however, it is possible for a GR to be incorrect at one level but correct at a subsuming level.1 For example, if an ncmod GR is incorrectly labelled with xmod, but is otherwise correct, it will be correct for all levels which subsume both ncmod and xmod, for example mod. The microaveraged scores are calculated by aggregating the counts for all the relations in the hierarchy, including the subsuming relations; the macro-a</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and smoothing for maximum entropy taggers. In Proceedings of the 10th Meeting of the EACL, pages 91–98, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1389" citStr="Hockenmaier and Steedman, 2002" startWordPosition="211" endWordPosition="215">In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same re</context>
<context position="8709" citStr="Hockenmaier and Steedman, 2002" startWordPosition="1389" endWordPosition="1392">nce is persuade; and the head of the Clark and Curran (2004b) describes the CCG parser infinitival complement’s subject is identified with used for the evaluation. The grammar used by the the head of the object, using the variable X, as in parser is extracted from CCGbank, a CCG version of standard unification-based accounts of control. the Penn Treebank (Hockenmaier, 2003). The gram- Previous evaluations of CCG parsers have used the mar consists of 425 lexical categories — expressing predicate-argument dependencies from CCGbank as subcategorisation information — plus a small num- a test set (Hockenmaier and Steedman, 2002; Clark ber of combinatory rules which combine the cate- and Curran, 2004b), with impressive results of over gories (Steedman, 2000). A supertagger first assigns 84% F-score on labelled dependencies. In this paper lexical categories to the words in a sentence, which we reinforce the earlier results with the first evaluaare then combined by the parser using the combi- tion of a CCG parser outside of CCGbank. natory rules and the CKY algorithm. A log-linear 4 Dependency Conversion to DepBank model scores the alternative parses. We use the For the gold standard we chose the version of Depnormal-f</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Meeting of the ACL, pages 335–342, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Data and Models for Statistical Parsing with Combinatory Categorial Grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4789" citStr="Hockenmaier, 2003" startWordPosition="745" endWordPosition="747"> based on the Penn Treebank, and the highest reported scores are now over 90% (Bod, 2003; Charniak and Johnson, 2005). However, it is unclear whether these high scores accurately reflect the performance of parsers in applications. It has been argued that the Parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard (Carroll et al., 1998). Also, using the same resource for training and testing may result in the parser learning systematic errors which are present in both the training and testing material. An example of this is from CCGbank (Hockenmaier, 2003), where all modifiers in noun-noun compound constructions modify the final noun (because the Penn Treebank, from which CCGbank is derived, does not contain the necessary information to obtain the correct bracketing). Thus there are nonnegligible, systematic errors in both the training and testing material, and the CCG parsers are being rewarded for following particular mistakes. There are parser evaluation suites which have been designed to be formalism-independent and which have been carefully and manually corrected. Carroll et al. (1998) describe such a suite, consisting of sentences taken f</context>
<context position="8455" citStr="Hockenmaier, 2003" startWordPosition="1354" endWordPosition="1355"> We perform such an evaluation for the CCG parser, with the surprising result that the upper bound on DepBank is only 84.8%, despite the considerable effort invested in developing the conversion process. 249 3 The CCG Parser declarative sentence is persuade; and the head of the Clark and Curran (2004b) describes the CCG parser infinitival complement’s subject is identified with used for the evaluation. The grammar used by the the head of the object, using the variable X, as in parser is extracted from CCGbank, a CCG version of standard unification-based accounts of control. the Penn Treebank (Hockenmaier, 2003). The gram- Previous evaluations of CCG parsers have used the mar consists of 425 lexical categories — expressing predicate-argument dependencies from CCGbank as subcategorisation information — plus a small num- a test set (Hockenmaier and Steedman, 2002; Clark ber of combinatory rules which combine the cate- and Curran, 2004b), with impressive results of over gories (Steedman, 2000). A supertagger first assigns 84% F-score on labelled dependencies. In this paper lexical categories to the words in a sentence, which we reinforce the earlier results with the first evaluaare then combined by the </context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Julia Hockenmaier. 2003. Data and Models for Statistical Parsing with Combinatory Categorial Grammar. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell Alexander Vasserman</author>
<author>Richard Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT Conference and the 4th NAACL Meeting (HLT-NAACL’04),</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="1300" citStr="Kaplan et al., 2004" startWordPosition="196" endWordPosition="200">ficulties in converting the parser output into DepBank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser </context>
<context position="2960" citStr="Kaplan et al., 2004" startWordPosition="452" endWordPosition="455">dependency F-scores obtained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the 248 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics output of a parser from one representation to another. Second, we develop a method for measuring </context>
<context position="5917" citStr="Kaplan et al. (2004)" startWordPosition="925" endWordPosition="928">ally corrected. Carroll et al. (1998) describe such a suite, consisting of sentences taken from the Susanne corpus, annotated with Grammatical Relations (GRs) which specify the syntactic relation between a head and dependent. Thus all that is required to use such a scheme, in theory, is that the parser being evaluated is able to identify heads. A similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (1998) gold-standard. The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required GRs, with the result that the GR finder of Buchholz is</context>
<context position="7436" citStr="Kaplan et al. (2004)" startWordPosition="1188" endWordPosition="1191">es, but they also note that “This conversion was relatively straightforward for LFG structures ... However, a certain amount of skill and intuition was required to provide a fair conversion of the Collins trees”. Without some measure of the difficulty — and effectiveness — of the conversion, there remains a suspicion that the Collins parser is being unfairly penalised. One way of providing such a measure is to convert the original gold standard on which the parser is based and evaluate that against the new gold standard (assuming the two resources are based on the same corpus). In the case of Kaplan et al. (2004), the testing procedure would include running their conversion process on Section 23 of the Penn Treebank and evaluating the output against DepBank. As well as providing some measure of the effectiveness of the conversion, this method would also provide an upper bound for the Collins parser, giving the score that a perfect Penn Treebank parser would obtain on DepBank (given the conversion process). We perform such an evaluation for the CCG parser, with the surprising result that the upper bound on DepBank is only 84.8%, despite the considerable effort invested in developing the conversion proc</context>
</contexts>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>Ron Kaplan, Stefan Riezler, Tracy H. King, John T. Maxwell III, Alexander Vasserman, and Richard Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proceedings of the HLT Conference and the 4th NAACL Meeting (HLT-NAACL’04), Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Stefan Riezler</author>
<author>Mary Dalrymple</author>
<author>Ronald M Kaplan</author>
</authors>
<date>2003</date>
<booktitle>The PARC 700 Dependency Bank. In Proceedings of the LINC-03 Workshop,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="5719" citStr="King et al., 2003" startWordPosition="892" endWordPosition="895">the CCG parsers are being rewarded for following particular mistakes. There are parser evaluation suites which have been designed to be formalism-independent and which have been carefully and manually corrected. Carroll et al. (1998) describe such a suite, consisting of sentences taken from the Susanne corpus, annotated with Grammatical Relations (GRs) which specify the syntactic relation between a head and dependent. Thus all that is required to use such a scheme, in theory, is that the parser being evaluated is able to identify heads. A similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (199</context>
<context position="9696" citStr="King et al., 2003" startWordPosition="1550" endWordPosition="1553"> tion of a CCG parser outside of CCGbank. natory rules and the CKY algorithm. A log-linear 4 Dependency Conversion to DepBank model scores the alternative parses. We use the For the gold standard we chose the version of Depnormal-form model, which assigns probabilities to Bank reannotated by Briscoe and Carroll (2006), single derivations based on the normal-form deriva- consisting of 700 sentences from Section 23 of the tions in CCGbank. The features in the model are Penn Treebank. The B&amp;C scheme is similar to the defined over local parts of the derivation and include original DepBank scheme (King et al., 2003), but word-word dependencies. A packed chart represen- overall contains less grammatical detail; Briscoe and tation allows efficient decoding, with the Viterbi al- Carroll (2006) describes the differences. We chose gorithm finding the most probable derivation. this resource for the following reasons: it is pubThe parser outputs predicate-argument dependen- licly available, allowing other researchers to comcies defined in terms of CCG lexical categories. pare against our results; the GRs making up the anMore formally, a CCG predicate-argument depen- notation share some similarities with the pre</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan. 2003. The PARC 700 Dependency Bank. In Proceedings of the LINC-03 Workshop, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop: Beyond</booktitle>
<location>Hainan Island, China.</location>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proceedings of the IJCNLP-04 Workshop: Beyond shallow analyses - Formalisms and statistical modelingfor deep analyses, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING2004,</booktitle>
<pages>64--70</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1721" citStr="Nivre and Scholz, 2004" startWordPosition="258" endWordPosition="261">ority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser. This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared. However, there are two drawbacks to this approach. First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot b</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING2004, pages 64–70, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
</authors>
<title>Using grammatical relations to compare parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Meeting of the EACL,</booktitle>
<pages>291--298</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2975" citStr="Preiss, 2003" startWordPosition="456" endWordPosition="457">btained by evaluating on the Parc Dependency Bank. Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance. In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006). The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank. Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003). However, we found that performing such a conversion is a time-consuming and non-trivial task. The contributions of this paper are as follows. First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the 248 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics output of a parser from one representation to another. Second, we develop a method for measuring how effective t</context>
<context position="6167" citStr="Preiss (2003)" startWordPosition="969" endWordPosition="970">e such a scheme, in theory, is that the parser being evaluated is able to identify heads. A similar resource — the Parc Dependency Bank (DepBank) (King et al., 2003) — has been created using sentences from the Penn Treebank. Briscoe and Carroll (2006) reannotated this resource using their GRs scheme, and used it to evaluate the RASP parser. Kaplan et al. (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG Fstructures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al. (1999), and the RASP parser, using the Carroll et al. (1998) gold-standard. The Penn Treebank trees of the Collins and Charniak parsers, and the GRs of the Buchholz parser, are mapped into the required GRs, with the result that the GR finder of Buchholz is the most accurate. The major weakness of these evaluations is that there is no measure of the difficultly of the conversion process for each of the parsers. Kaplan et al. (2004) clearly invested considerable time and expertise in mapping the output </context>
</contexts>
<marker>Preiss, 2003</marker>
<rawString>Judita Preiss. 2003. Using grammatical relations to compare parsers. In Proceedings of the 10th Meeting of the EACL, pages 291–298, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
<author>Aravind Joshi</author>
</authors>
<title>Tree-adjoining grammars and its application to statistical parsing.</title>
<date>2003</date>
<editor>In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-oriented parsing.</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="1352" citStr="Sarkar and Joshi, 2003" startWordPosition="206" endWordPosition="209">epBank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety of formalisms and output creates a challenge for parser evaluation. The majority of parser evaluations have </context>
</contexts>
<marker>Sarkar, Joshi, 2003</marker>
<rawString>Anoop Sarkar and Aravind Joshi. 2003. Tree-adjoining grammars and its application to statistical parsing. In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-oriented parsing. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8841" citStr="Steedman, 2000" startWordPosition="1411" endWordPosition="1412"> the evaluation. The grammar used by the the head of the object, using the variable X, as in parser is extracted from CCGbank, a CCG version of standard unification-based accounts of control. the Penn Treebank (Hockenmaier, 2003). The gram- Previous evaluations of CCG parsers have used the mar consists of 425 lexical categories — expressing predicate-argument dependencies from CCGbank as subcategorisation information — plus a small num- a test set (Hockenmaier and Steedman, 2002; Clark ber of combinatory rules which combine the cate- and Curran, 2004b), with impressive results of over gories (Steedman, 2000). A supertagger first assigns 84% F-score on labelled dependencies. In this paper lexical categories to the words in a sentence, which we reinforce the earlier results with the first evaluaare then combined by the parser using the combi- tion of a CCG parser outside of CCGbank. natory rules and the CKY algorithm. A log-linear 4 Dependency Conversion to DepBank model scores the alternative parses. We use the For the gold standard we chose the version of Depnormal-form model, which assigns probabilities to Bank reannotated by Briscoe and Carroll (2006), single derivations based on the normal-for</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher Manning</author>
<author>Stuart Shieber</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Parse disambiguation for a rich HPSG grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the First Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>253--263</pages>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="1245" citStr="Toutanova et al., 2002" startWordPosition="186" endWordPosition="189"> evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into DepBank grammatical relations. In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperforming RASP by over 5% overall and on the majority of dependency types. 1 Introduction Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000). Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b). This variety o</context>
</contexts>
<marker>Toutanova, Manning, Shieber, Flickinger, Oepen, 2002</marker>
<rawString>Kristina Toutanova, Christopher Manning, Stuart Shieber, Dan Flickinger, and Stephan Oepen. 2002. Parse disambiguation for a rich HPSG grammar. In Proceedings of the First Workshop on Treebanks and Linguistic Theories, pages 253–263, Sozopol, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>