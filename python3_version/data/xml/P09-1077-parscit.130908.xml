<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000274">
<title confidence="0.993927">
Automatic sense prediction for implicit discourse relations in text
</title>
<author confidence="0.999443">
Emily Pitler, Annie Louis, Ani Nenkova
</author>
<affiliation confidence="0.882457">
Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.998938">
epitler,lannie,nenkova@seas.upenn.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999926">
We present a series of experiments on au-
tomatically identifying the sense of im-
plicit discourse relations, i.e. relations
that are not marked with a discourse con-
nective such as “but” or “because”. We
work with a corpus of implicit relations
present in newspaper text and report re-
sults on a test set that is representative
of the naturally occurring distribution of
senses. We use several linguistically in-
formed features, including polarity tags,
Levin verb classes, length of verb phrases,
modality, context, and lexical features. In
addition, we revisit past approaches using
lexical pairs from unannotated text as fea-
tures, explain some of their shortcomings
and propose modifications. Our best com-
bination of features outperforms the base-
line from data intensive approaches by 4%
for comparison and 16% for contingency.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9589465">
Implicit discourse relations abound in text and
readers easily recover the sense of such relations
during semantic interpretation. But automatic
sense prediction for implicit relations is an out-
standing challenge in discourse processing.
Discourse relations, such as causal and contrast
relations, are often marked by explicit discourse
connectives (also called cue words) such as “be-
cause” or “but”. It is not uncommon, though, for a
discourse relation to hold between two text spans
without an explicit discourse connective, as the ex-
ample below demonstrates:
(1) The 101-year-old magazine has never had to woo ad-
vertisers with quite so much fervor before.
[because] It largely rested on its hard-to-fault demo-
graphics.
In this paper we address the problem of au-
tomatic sense prediction for discourse relations
in newspaper text. For our experiments, we use
the Penn Discourse Treebank, the largest exist-
ing corpus of discourse annotations for both im-
plicit and explicit relations. Our work is also
informed by the long tradition of data intensive
methods that rely on huge amounts of unanno-
tated text rather than on manually tagged corpora
(Marcu and Echihabi, 2001; Blair-Goldensohn et
al., 2007).
In our analysis, we focus only on implicit dis-
course relations and clearly separate these from
explicits. Explicit relations are easy to iden-
tify. The most general senses (comparison, con-
tingency, temporal and expansion) can be disam-
biguated in explicit relations with 93% accuracy
based solely on the discourse connective used to
signal the relation (Pitler et al., 2008). So report-
ing results on explicit and implicit relations sepa-
rately will allow for clearer tracking of progress.
In this paper we investigate the effectiveness of
various features designed to capture lexical and
semantic regularities for identifying the sense of
implicit relations. Given two text spans, previous
work has used the cross-product of the words in
the spans as features. We examine the most infor-
mative word pair features and find that they are not
the semantically-related pairs that researchers had
hoped. We then introduce several other methods
capturing the semantics of the spans (polarity fea-
tures, semantic classes, tense, etc.) and evaluate
their effectiveness. This is the first study which
reports results on classifying naturally occurring
implicit relations in text and uses the natural dis-
tribution of the various senses.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9935466">
Experiments on implicit and explicit relations
Previous work has dealt with the prediction of dis-
course relation sense, but often for explicits and at
the sentence level.
Soricut and Marcu (2003) address the task of
</bodyText>
<page confidence="0.988349">
683
</page>
<note confidence="0.999612">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683–691,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999918551020408">
parsing discourse structures within the same sen-
tence. They use the RST corpus (Carlson et al.,
2001), which contains 385 Wall Street Journal ar-
ticles annotated following the Rhetorical Structure
Theory (Mann and Thompson, 1988). Many of
the useful features, syntax in particular, exploit
the fact that both arguments of the connective are
found in the same sentence. Such features would
not be applicable to the analysis of implicit rela-
tions that occur intersententially.
Wellner et al. (2006) used the GraphBank (Wolf
and Gibson, 2005), which contains 105 Associated
Press and 30 Wall Street Journal articles annotated
with discourse relations. They achieve 81% accu-
racy in sense disambiguation on this corpus. How-
ever, GraphBank annotations do not differentiate
between implicits and explicits, so it is difficult to
verify success for implicit relations.
Experiments on artificial implicits Marcu and
Echihabi (2001) proposed a method for cheap ac-
quisition of training data for discourse relation
sense prediction. Their idea is to use unambiguous
patterns such as [Arg1, but Arg2.] to create syn-
thetic examples of implicit relations. They delete
the connective and use [Arg1, Arg2] as an example
of an implicit relation.
The approach is tested using binary classifica-
tion between relations on balanced data, a setting
very different from that of any realistic applica-
tion. For example, a question-answering appli-
cation that needs to identify causal relations (i.e.
as in Girju (2003)), must not only differentiate
causal relations from comparison relations, but
also from expansions, temporal relations, and pos-
sibly no relation at all. In addition, using equal
numbers of examples of each type can be mislead-
ing because the distribution of relations is known
to be skewed, with expansions occurring most fre-
quently. Causal and comparison relations, which
are most useful for applications, are less frequent.
Because of this, the recall of the classification
should be the primary metric of success, while
the Marcu and Echihabi (2001) experiments report
only accuracy.
Later work (Blair-Goldensohn et al., 2007;
Sporleder and Lascarides, 2008) has discovered
that the models learned do not perform as well on
implicit relations as one might expect from the test
accuracies on synthetic data.
</bodyText>
<sectionHeader confidence="0.974083" genericHeader="method">
3 Penn Discourse Treebank
</sectionHeader>
<bodyText confidence="0.999961674418605">
For our experiments, we use the Penn Discourse
Treebank (PDTB; Prasad et al., 2008), the largest
available annotated corpora of discourse relations.
The PDTB contains discourse annotations over the
same 2,312 Wall Street Journal (WSJ) articles as
the Penn Treebank.
For each explicit discourse connective (such as
“but” or “so”), annotators identified the two text
spans between which the relation holds and the
sense of the relation.
The PDTB also provides information about lo-
cal implicit relations. For each pair of adjacent
sentences within the same paragraph, annotators
selected the explicit discourse connective which
best expressed the relation between the sentences
and then assigned a sense to the relation. In Exam-
ple (1) above, the annotators identified “because”
as the most appropriate connective between the
sentences, and then labeled the implicit discourse
relation Contingency.
In the PDTB, explicit and implicit relations are
clearly distinguished, allowing us to concentrate
solely on the implicit relations.
As mentioned above, each implicit and explicit
relation is annotated with a sense. The senses
are arranged in a hierarchy, allowing for annota-
tions as specific as Contingency.Cause.reason. In
our experiments, we use only the top level of the
sense annotations: Comparison, Contingency, Ex-
pansion, and Temporal. Using just these four rela-
tions allows us to be theory-neutral; while differ-
ent frameworks (Hobbs, 1979; McKeown, 1985;
Mann and Thompson, 1988; Knott and Sanders,
1998; Asher and Lascarides, 2003) include differ-
ent relations of varying specificities, all of them
include these four core relations, sometimes under
different names.
Each relation in the PDTB takes two arguments.
Example (1) can be seen as the predicate Con-
tingency which takes the two sentences as argu-
ments. For implicits, the span in the first sentence
is called Arg1 and the span in the following sen-
tence is called Arg2.
</bodyText>
<sectionHeader confidence="0.886008" genericHeader="method">
4 Word pair features in prior work
</sectionHeader>
<bodyText confidence="0.9892916">
Cross product of words Discourse connectives
are the most reliable predictors of the semantic
sense of the relation (Marcu, 2000; Pitler et al.,
2008). However, in the absence of explicit mark-
ers, the most easily accessible features are the
</bodyText>
<page confidence="0.99826">
684
</page>
<bodyText confidence="0.999035361111111">
words in the two text spans of the relation. In-
tuitively, one would expect that there is some rela-
tionship that holds between the words in the two
arguments. Consider for example the following
sentences:
The recent explosion of country funds mirrors the ”closed-
end fund mania” of the 1920s, Mr. Foot says, when narrowly
focused funds grew wildly popular. They fell into oblivion
after the 1929 crash.
The words “popular” and “oblivion” are almost
antonyms, and one might hypothesize that their
occurrence in the two text spans is what triggers
the contrast relation between the sentences. Sim-
ilarly, a pair of words such as (rain, rot) might be
indicative of a causal relation. If this hypothesis is
correct, pairs of words (w1, w2) such that w1 ap-
pears in the first sentence and w2 appears in the
second sentence would be good features for iden-
tifying contrast relations.
Indeed, word pairs form the basic feature
of most previous work on classifying implicit
relations (Marcu and Echihabi, 2001; Blair-
Goldensohn et al., 2007; Sporleder and Las-
carides, 2008) or the simpler task of predicting
which connective should be used to express a rela-
tion (Lapata and Lascarides, 2004).
Semantic relations vs. function word pairs If
the hypothesis for word pair triggers of discourse
relations were true, the analysis of unambiguous
relations can be used to discover pairs of words
with causal or contrastive relations holding be-
tween them. Yet, feature analysis has not been per-
formed in prior studies to establish or refute this
possibility.
At the same time, feature selection is always
necessary for word pairs, which are numerous and
lead to data sparsity problems. Here, we present a
meta analysis of the feature selection work in three
prior studies.
One approach for reducing the number of fea-
tures follows the hypothesis of semantic rela-
tions between words. Marcu and Echihabi (2001)
considered only nouns, verbs and and other cue
phrases in word pairs. They found that even
with millions of training examples, prediction re-
sults using all words were superior to those based
on only pairs of non-function words. However,
since the learning curve is steeper when function
words were removed, they hypothesize that using
only non-function words will outperform using all
words once enough training data is available.
In a similar vein, Lapata and Lascarides (2004)
used pairings of only verbs, nouns and adjectives
for predicting which temporal connective is most
suitable to express the relation between two given
text spans. Verb pairs turned out to be one of the
best features, but no useful information was ob-
tained using nouns and adjectives.
Blair-Goldensohn et al. (2007) proposed sev-
eral refinements of the word pair model. They
show that (i) stemming, (ii) using a small fixed
vocabulary size consisting of only the most fre-
quent stems (which would tend to be dominated
by function words) and (iii) a cutoff on the mini-
mum frequency of a feature, all result in improved
performance. They also report that filtering stop-
words has a negative impact on the results.
Given these findings, we expect that pairs of
function words are informative features helpful in
predicting discourse relation sense. In our work
that we describe next, we use feature selection to
investigate the word pairs in detail.
</bodyText>
<subsectionHeader confidence="0.47682">
5 Analysis of word pair features
</subsectionHeader>
<bodyText confidence="0.975734464285714">
For the analysis of word pair features, we use
a large collection of automatically extracted ex-
plicit examples from the experiments in Blair-
Goldensohn et al. (2007). The data, from now on
referred to as TextRels, has explicit contrast and
causal relations which were extracted from the En-
glish Gigaword Corpus (Graff, 2003) which con-
tains over four million newswire articles.
The explicit cue phrase is removed from each
example and the spans are treated as belonging to
an implicit relation. Besides cause and contrast,
the TextRels data include a no-relation category
which consists of sentences from the same text that
are separated by at least three other sentences.
To identify features useful for classifying com-
parison vs other relations, we chose a random sam-
ple of 5000 examples for Contrast and 5000 Other
relations (2500 each of Cause and No-relation).
For the complete set of 10,000 examples, word
pair features were computed. After removing
word pairs that appear less than 5 times, the re-
maining features were ranked by information gain
using the MALLET toolkit1.
Table 1 lists the word pairs with highest infor-
mation gain for the Contrast vs. Other and Cause
vs. Other classification tasks. All contain very fre-
quent stop words, and interestingly for the Con-
1mallet.cs.umass.edu
</bodyText>
<page confidence="0.99594">
685
</page>
<bodyText confidence="0.999542848484849">
trast vs. Other task, most of the word pairs contain
discourse connectives.
This is certainly unexpected, given that word
pairs were formed by deleting the discourse con-
nectives from the sentences expressing Contrast.
Word pairs containing “but” as one of their ele-
ments in fact signal the presence of a relation that
is not Contrast.
Consider the example shown below:
The government says it has reached most isolated townships
by now, but because roads are blocked, getting anything but
basic food supplies to people remains difficult.
Following Marcu and Echihabi (2001), the pair
[The government says it has reached most isolated
townships by now, but] and [roads are blocked,
getting anything but basic food supplies to peo-
ple remains difficult.] is created as an example of
the Cause relation. Because of examples like this,
“but-but” is a very useful word pair feature indi-
cating Cause, as the but would have been removed
for the artifical Contrast examples. In fact, the top
17 features for classifying Contrast versus Other
all contain the word “but”, and are indications that
the relation is Other.
These findings indicate an unexpected anoma-
lous effect in the use of synthetic data. Since re-
lations are created by removing connectives, if an
unambiguous connective remains, its presence is a
reliable indicator that the example should be clas-
sified as Other. Such features might work well and
lead to high accuracy results in identifying syn-
thetic implicit relations, but are unlikely to be use-
ful in a realistic setting of actual implicits.
</bodyText>
<subsectionHeader confidence="0.804764">
Comparison vs. Other Contingency vs. Other
</subsectionHeader>
<bodyText confidence="0.802104285714286">
the-but s-but the-in
of-but for-but but-but
in-but was-but it-but
to-but that-but the-it*
and-and the-the in-in
to-the of-and a-of
said-but they-but of-in in-and in-of s-and
</bodyText>
<tableCaption confidence="0.994876">
Table 1: Word pairs with highest information gain.
</tableCaption>
<bodyText confidence="0.9418368">
Also note that the only two features predic-
tive of the comparison class (indicated by * in
Table 1): the-it and to-it, contain only func-
tion words rather than semantically related non-
function words. This ranking explains the obser-
vations reported in Blair-Goldensohn et al. (2007)
where removing stopwords degraded classifier
performance and why using only nouns, verbs or
adjectives (Marcu and Echihabi, 2001; Lapata and
Lascarides, 2004) is not the best option2.
</bodyText>
<sectionHeader confidence="0.954603" genericHeader="method">
6 Features for sense prediction of
implicit discourse relations
</sectionHeader>
<bodyText confidence="0.99965375">
The contrast between the “popular”/“oblivion” ex-
ample we started with above can be analyzed in
terms of lexical relations (near antonyms), but also
could be explained by different polarities of the
two words: “popular” is generally a positive word,
while “oblivion” has negative connotations.
While we agree that the actual words in the ar-
guments are quite useful, we also define several
higher-level features corresponding to various se-
mantic properties of the words. The words in the
two text spans of a relation are taken from the
gold-standard annotations in the PDTB.
Polarity Tags: We define features that represent
the sentiment of the words in the two spans. Each
word’s polarity was assigned according to its en-
try in the Multi-perspective Question Answering
Opinion Corpus (Wilson et al., 2005). In this re-
source, each sentiment word is annotated as posi-
tive, negative, both, or neutral. We use the number
of negated and non-negated positive, negative, and
neutral sentiment words in the two text spans as
features. If a writer refers to something as “nice”
in Arg1, that counts towards the positive sentiment
count (Arg1Positive); “not nice” would count to-
wards Arg1NegatePositive. A sentiment word is
negated if a word with a General Inquirer (Stone
et al., 1966) Negate tag precedes it. We also have
features for the cross products of these polarities
between Arg1 and Arg2.
We expected that these features could help
Comparison examples especially. Consider the
following example:
</bodyText>
<subsubsectionHeader confidence="0.4352235">
Executives at Time Inc. Magazine Co., a subsidiary of
Time Warner, have said the joint venture with Mr. Lang
</subsubsectionHeader>
<bodyText confidence="0.9540081">
wasn’t a good one. The venture, formed in 1986, was sup-
posed to be Time’s low-cost, safe entry into women’s maga-
zines.
The word good is annotated with positive po-
larity, however it is negated. Safe is tagged as
having positive polarity, so this opposition could
indicate the Comparison relation between the two
sentences.
Inquirer Tags: To get at the meanings of the
spans, we look up what semantic categories each
</bodyText>
<footnote confidence="0.5919925">
2In addition, an informal inspection of 100 word pairs
with high information gain for Contrast vs. Other (the longest
word pairs were chosen, as those are more likely to be content
words) found only six semantically opposed pairs.
</footnote>
<figure confidence="0.955513333333333">
and-but but-the to-it*
a-but he-but said-in
the-and in-the the-of
said-said to-of the-a
a-and a-the of-the
to-and to-to the-in
</figure>
<page confidence="0.992343">
686
</page>
<bodyText confidence="0.999962980392157">
word falls into according to the General Inquirer
lexicon (Stone et al., 1966). The General In-
quirer has classes for positive and negative polar-
ity, as well as more fine-grained categories such as
words related to virtue or vice. The Inquirer even
contains a category called “Comp” that includes
words that tend to indicate Comparison, such as
“optimal”, “other”, “supreme”, or “ultimate”.
Several of the categories are complementary:
Understatement versus Overstatement, Rise ver-
sus Fall, or Pleasure versus Pain. Pairs where one
argument contains words that indicate Rise and the
other argument indicates Fall might be good evi-
dence for a Comparison relation.
The benefit of using these tags instead of just
the word pairs is that we see more observations for
each semantic class than for any particular word,
reducing the data sparsity problem. For example,
the pair rose:fell often indicates a Comparison re-
lation when speaking about stocks. However, oc-
casionally authors refer to stock prices as “jump-
ing” rather than “rising”. Since both jump and rise
are members of the Rise class, new jump examples
can be classified using past rise examples.
Development testing showed that including fea-
tures for all words’ tags was not useful, so we in-
clude the Inquirer tags of only the verbs in the two
arguments and their cross-product. Just as for the
polarity features, we include features for both each
tag and its negation.
Money/Percent/Num: If two adjacent sen-
tences both contain numbers, dollar amounts, or
percentages, it is likely that a comparison rela-
tion might hold between the sentences. We in-
cluded a feature for the count of numbers, percent-
ages, and dollar amounts in Arg1 and Arg2. We
also included the number of times each combina-
tion of number/percent/dollar occurs in Arg1 and
Arg2. For example, if Arg1 mentions a percent-
age and Arg2 has two dollar amounts, the feature
Arg1Percent-Arg2Money would have a count of 2.
This feature is probably genre-dependent. Num-
bers and percentages often appear in financial texts
but would be less frequent in other genres.
WSJ-LM: This feature represents the extent to
which the words in the text spans are typical of
each relation. For each sense, we created uni-
gram and bigram language models over the im-
plicit examples in the training set. We compute
each example’s probability according to each of
these language models. The features are the ranks
of the spans’ likelihoods according to the vari-
ous language models. For example, if of the un-
igram models, the most likely relation to generate
this example was Contingency, then the example
would include the feature ContingencyUnigram1.
If the third most likely relation according to the
bigram models was Expansion, then it would in-
clude the feature ExpansionBigram3.
Expl-LM: This feature ranks the text spans ac-
cording to language models derived from the ex-
plicit examples in the TextRels corpus. However,
the corpus contains only Cause, Contrast and No-
relation, hence we expect the WSJ language mod-
els to be more helpful.
Verbs: These features include the number of
pairs of verbs in Arg1 and Arg2 from the same
verb class. Two verbs are from the same verb class
if each of their highest Levin verb class (Levin,
1993) levels (in the LCS Database (Dorr, 2001))
are the same. The intuition behind this feature is
that the more related the verbs, the more likely the
relation is an Expansion.
The verb features also include the average
length of verb phrases in each argument, as well
as the cross product of this feature for the two ar-
guments. We hypothesized that verb chunks that
contain more words, such as “They [are allowed to
proceed]” often contain rationales afterwards (sig-
nifying Contingency relations), while short verb
phrases like “They proceed” might occur more of-
ten in Expansion or Temporal relations.
Our final verb features were the part of speech
tags (gold-standard from the Penn Treebank) of
the main verb. One would expect that Expansion
would link sentences with the same tense, whereas
Contingency and Temporal relations would con-
tain verbs with different tenses.
First-Last, First3: The first and last words of
a relation’s arguments have been found to be par-
ticularly useful for predicting its sense (Wellner et
al., 2006). Wellner et al. (2006) suggest that these
words are such predictive features because they
are often explicit discourse connectives. In our
experiments on implicits, the first and last words
are not connectives. However, some implicits have
been found to be related by connective-like ex-
pressions which often appear in the beginning of
the second argument. In the PDTB, these are an-
notated as alternatively lexicalized relations (Al-
tLexes). To capture such effects, we included the
first and last words of Arg1 as features, the first
</bodyText>
<page confidence="0.996686">
687
</page>
<bodyText confidence="0.999928622222222">
and last words of Arg2, the pair of the first words
of Arg1 and Arg2, and the pair of the last words.
We also add two additional features which indicate
the first three words of each argument.
Modality: Modal words, such as “can”,
“should”, and “may”, are often used to express
conditional statements (i.e. “If I were a wealthy
man, I wouldn’t have to work hard.”) thus signal-
ing a Contingency relation. We include a feature
for the presence or absence of modals in Arg1 and
Arg2, features for specific modal words, and their
cross-products.
Context: Some implicit relations appear imme-
diately before or immediately after certain explicit
relations far more often than one would expect due
to chance (Pitler et al., 2008). We define a feature
indicating if the immediately preceding (or follow-
ing) relation was an explicit. If it was, we include
the connective trigger of the relation and its sense
as features. We use oracle annotations of the con-
nective sense, however, most of the connectives
are unambiguous.
One might expect a different distribution of re-
lation types in the beginning versus further in the
middle of a paragraph. We capture paragraph-
position information using a feature which indi-
cates if Arg1 begins a paragraph.
Word pairs Four variants of word pair mod-
els were used in our experiments. All the models
were eventually tested on implicit examples from
the PDTB, but the training set-up was varied.
Wordpairs-TextRels In this setting, we trained
a model on word pairs derived from unannotated
text (TextRels corpus).
Wordpairs-PDTBImpl Word pairs for training
were formed from the cross product of words in
the textual spans (Arg1 x Arg2) of the PDTB im-
plicit relations.
Wordpairs-selected Here, only word pairs from
Wordpairs-PDTBImpl with non-zero information
gain on the TextRels corpus were retained.
Wordpairs-PDTBExpl In this case, the model
was formed by using the word pairs from the ex-
plicit relations in the sections of the PDTB used
for training.
</bodyText>
<sectionHeader confidence="0.985541" genericHeader="method">
7 Classification Results
</sectionHeader>
<bodyText confidence="0.999846130434783">
For all experiments, we used sections 2-20 of the
PDTB for training and sections 21-22 for testing.
Sections 0-1 were used as a development set for
feature design.
We ran four binary classification tasks to iden-
tify each of the main relations from the rest. As
each of the relations besides Expansion are infre-
quent, we train using equal numbers of positive
and negative examples of the target relation. The
negative examples were chosen at random. We
used all of sections 21 and 22 for testing, so the
test set is representative of the natural distribution.
The training sets contained: Comparison (1927
positive, 1927 negative), Contingency (3500
each), Expansion3 (6356 each), and Temporal
(730 each).
The test set contained: 151 examples of Com-
parison, 291 examples of Contingency, 986 exam-
ples of Expansion, 82 examples of Temporal, and
13 examples of No-relation.
We used Naive Bayes, Maximum Entropy
(MaxEnt), and AdaBoost (Freund and Schapire,
1996) classifiers implemented in MALLET.
</bodyText>
<subsectionHeader confidence="0.939405">
7.1 Non-Wordpair Features
</subsectionHeader>
<bodyText confidence="0.999672296296296">
The performance using only our semantically in-
formed features is shown in Table 7. Only the
Naive Bayes classification results are given, as
space is limited and MaxEnt and AdaBoost gave
slightly lower accuracies overall.
The table lists the f-score for each of the target
relations, with overall accuracy shown in brack-
ets. Given that the experiments are run on natural
distribution of the data, which are skewed towards
Expansion relations, the f-score is the more impor-
tant measure to track.
Our random baseline is the f-score one would
achieve by randomly assigning classes in propor-
tion to its true distribution in the test set. The best
results for all four tasks are considerably higher
than random prediction, but still low overall. Our
features provide 6% to 18% absolute improve-
ments in f-score over the baseline for each of the
four tasks. The largest gain was in the Contin-
gency versus Other prediction task. The least im-
provement was for distinguishing Expansion ver-
sus Other. However, since Expansion forms the
largest class of relations, its f-score is still the
highest overall. We discuss the results per relation
class next.
Comparison We expected that polarity features
would be especially helpful for identifying Com-
</bodyText>
<footnote confidence="0.9637695">
3The PDTB also contains annotations of entity relations,
which most frameworks consider a subset of Expansion.
Thus, we include relations annotated as EntRel as positive
examples of Expansion.
</footnote>
<page confidence="0.98789">
688
</page>
<table confidence="0.998923454545454">
Features Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Four-way
Money/Percent/Num 19.04 (43.60) 18.78 (56.27) 22.01 (41.37) 10.40 (23.05) (63.38)
Polarity Tags 16.63 (55.22) 19.82 (76.63) 71.29 (59.23) 11.12 (18.12) (65.19)
WSJ-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Expl-LM 18.04 (9.91) 0.00 (80.89) 0.00 (35.26) 10.22 (5.38) (65.26)
Verbs 18.55 (26.19) 36.59 (62.44) 59.36 (52.53) 12.61 (41.63) (65.33)
First-Last, First3 21.01 (52.59) 36.75 (59.09) 63.22 (56.99) 15.93 (61.20) (65.40)
Inquirer tags 17.37 (43.8) 15.76 (77.54) 70.21 (58.04) 11.56 (37.69) (62.21)
Modality 17.70 (17.6) 21.83 (76.95) 15.38 (37.89) 11.17 (27.91) (65.33)
Context 19.32 (56.66) 29.55 (67.42) 67.77 (57.85) 12.34 (55.22) (64.01)
Random 9.91 19.11 64.74 5.38
</table>
<tableCaption confidence="0.999723">
Table 2: f-score (accuracy) using different features; Naive Bayes.
</tableCaption>
<bodyText confidence="0.999895721311476">
parison relations. Surprisingly, polarity was actu-
ally one of the worst classes of features for Com-
parison, achieving an f-score of 16.33 (in contrast
to using the first, last and first three words of the
sentences as features, which leads to an f-score of
21.01). We examined the prevalence of positive-
negative or negative-positive polarity pairs in our
training set. 30% of the Comparison examples
contain one of these opposite polarity pairs, while
31% of the Other examples contain an opposite
polarity pair. To our knowledge, this is the first
study to examine the prevalence of polarity words
in the arguments of discourse relations in their
natural distributions. Contrary to popular belief,
Comparisons do not tend to have more opposite
polarity pairs.
The two most useful classes of features for rec-
ognizing Comparison relations were the first, last
and first three words in the sentence and the con-
text features that indicate the presence of a para-
graph boundary or of an explicit relation just be-
fore or just after the location of the hypothesized
implicit relation (19.32 f-score).
Contingency The two best features for the Con-
tingency vs. Other distinction were verb informa-
tion (36.59 f-score) and first, last and first three
words in the sentence (36.75 f-score). Context
again was one of the features that led to improve-
ment. This makes sense, as Pitler et al. (2008)
found that implicit contingencies are often found
immediately following explicit comparisons.
We were surprised that the polarity features
were helpful for Contingency but not Comparison.
Again we looked at the prevalence of opposite po-
larity pairs. While for Comparison versus Other
there was not a significant difference, for Contin-
gency there are quite a few more opposite polarity
pairs (52%) than for not Contingency (41%).
The language model features were completely
useless for distinguishing contingencies from
other relations.
Expansion As Expansion is the majority class
in the natural distribution, recall is less of a prob-
lem than precision. The features that help achieve
the best f-score are all features that were found to
be useful in identifying other relations.
Polarity tags, Inquirer tags and context were
the best features for identifying expansions with
f-scores around 70%.
Temporal Implicit temporal relations are rela-
tively rare, making up only about 5% of our test
set. Most temporal relations are explicitly marked
with a connective like “when” or “after”.
Yet again, the first and last words of the sen-
tence turned out to be useful indicators for tem-
poral relations (15.93 f-score). The importance of
the first and last words for this distinction is clear.
It derives from the fact that temporal implicits of-
ten contain words like “yesterday” or “Monday” at
the end of the sentence. Context is the next most
helpful feature for temporal relations.
</bodyText>
<subsectionHeader confidence="0.971276">
7.2 Which word pairs help?
</subsectionHeader>
<bodyText confidence="0.999977352941177">
For Comparison and Contingency, we analyze the
behavior of word pair features under several differ-
ent settings. Specifically we want to address two
important related questions raised in recent work
by others: (i) is unannotated data from explicits
useful for training models that disambiguate im-
plicit discourse relations and (ii) are explicit and
implicit relations intrinsically different from each
other.
Wordpairs-TextRels is the worst approach. The
best use of word pair features is Wordpairs-
selected. This model gives 4% better absolute f-
score for Comparison and 14% for Contingency
over Wordpairs-TextRels. In this setting the Tex-
tRels data was used to choose the word pair fea-
tures, but the probabilities for each feature were
estimated using the training portion of the PDTB
</bodyText>
<page confidence="0.996614">
689
</page>
<table confidence="0.999810458333333">
Comp. vs. Other 17.13 (46.62)
Wordpairs-TextRels 19.39 (51.41)
Wordpairs-PDTBExpl 20.96 (42.55)
Wordpairs-PDTBImpl 21.01 (52.59)
First-last, first3 (best-non-wp) 21.88 (56.40)
Best-non-wp + Wordpairs-selected 21.96 (56.59)
Wordpairs-selected
Cont. vs. Other
Wordpairs-TextRels 31.10 (41.83)
Wordpairs-PDTBExpl 37.77 (56.73)
Wordpairs-PDTBImpl 43.79 (61.92)
Polarity, verbs, first-last, first3, 42.14 (66.64)
modality, context (best-non-wp)
Wordpairs-selected 45.60 (67.10)
Best-non-wp + Wordpairs-selected 47.13 (67.30)
Expn. vs. Other
Best-non-wp + wordpairs 62.39 (59.55)
Wordpairs-PDTBImpl 63.84 (60.28)
Polarity, inquirer tags, context (best- 76.42 (63.62)
non-wp)
Temp. vs. Other
First-last, first3 (best-non-wp) 15.93 (61.20)
Wordpairs-PDTBImpl 16.21 (61.98)
Best-non-wp + Wordpairs-PDTBImpl 16.76 (63.49)
</table>
<tableCaption confidence="0.9628665">
Table 3: f-score (accuracy) of various feature sets;
Naive Bayes.
</tableCaption>
<bodyText confidence="0.981040875">
implicit examples.
We also confirm that even within the PDTB,
information from annotated explicit relations
(Wordpairs-PDTBExpl) is not as helpful as
information from annotated implicit relations
(Wordpairs-PDTBImpl). The absolute difference
in f-score between the two models is close to 2%
for Comparison, and 6% for Contingency.
</bodyText>
<subsectionHeader confidence="0.997851">
7.3 Best results
</subsectionHeader>
<bodyText confidence="0.9999638">
Adding other features to word pairs leads to im-
proved performance for Contingency, Expansion
and Temporal relations, but not for Comparison.
For contingency detection, the best combina-
tion of our features included polarity, verb in-
formation, first and last words, modality, context
with Wordpairs-selected. This combination led
to a definite improvement, reaching an f-score of
47.13 (16% absolute improvement in f-score over
Wordpairs-TextRels).
For detecting expansions, the best combination
of our features (polarity+Inquirer tags+context)
outperformed Wordpairs-PDTBImpl by a wide
margin, close to 13% absolute improvement (f-
scores of 76.42 and 63.84 respectively).
</bodyText>
<subsectionHeader confidence="0.913017">
7.4 Sequence Model of Discourse Relations
</subsectionHeader>
<bodyText confidence="0.999972642857143">
Our results from the previous section show that
classification of implicits benefits from informa-
tion about nearby relations, and so we expected
improvements using a sequence model, rather than
classifying each relation independently.
We trained a CRF classifier (Lafferty et al.,
2001) over the sequence of implicit examples from
all documents in sections 02 to 20. The test set
is the same as used for the 2-way classifiers. We
compare against a 6-way4 Naive Bayes classifier.
Only word pairs were used as features for both.
Overall 6-way prediction accuracy is 43.27% for
the Naive Bayes model and 44.58% for the CRF
model.
</bodyText>
<sectionHeader confidence="0.997471" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999996310344827">
We have presented the first study that predicts im-
plicit discourse relations in a realistic setting (dis-
tinguishing a relation of interest from all others,
where the relations occur in their natural distri-
butions). Also unlike prior work, we separate the
task from the easier task of explicit discourse pre-
diction. Our experiments demonstrate that fea-
tures developed to capture word polarity, verb
classes and orientation, as well as some lexical
features are strong indicators of the type of dis-
course relation.
We analyze word pair features used in prior
work that were intended to capture such semantic
oppositions. We show that the features in fact do
not capture semantic relation but rather give infor-
mation about function word co-occurrences. How-
ever, they are still a useful source of information
for discourse relation prediction. The most bene-
ficial application of such features is when they are
selected from a large unannotated corpus of ex-
plicit relations, but then trained on manually an-
notated implicit relations.
Context, in terms of paragraph boundaries and
nearby explicit relations, also proves to be useful
for the prediction of implicit discourse relations.
It is helpful when added as a feature in a standard,
instance-by-instance learning model. A sequence
model also leads to over 1% absolute improvement
for the task.
</bodyText>
<sectionHeader confidence="0.998616" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999515833333333">
This work was partially supported by NSF grants
IIS-0803159, IIS-0705671 and IGERT 0504487.
We would like to thank Sasha Blair-Goldensohn
for providing us with the TextRels data and for
the insightful discussion in the early stages of our
work.
</bodyText>
<footnote confidence="0.942235">
4the four main relations, EntRel, NoRel
</footnote>
<page confidence="0.996816">
690
</page>
<sectionHeader confidence="0.99582" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999864451219512">
N. Asher and A. Lascarides. 2003. Logics of conver-
sation. Cambridge University Press.
S. Blair-Goldensohn, K.R. McKeown, and O.C. Ram-
bow. 2007. Building and Refining Rhetorical-
Semantic Relation Models. In Proceedings of
NAACL HLT, pages 428–435.
L. Carlson, D. Marcu, and M.E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. In Proceedings
of the Second SIGdial Workshop on Discourse and
Dialogue, pages 1–10.
B.J. Dorr. 2001. LCS Verb Database. Technical Re-
port Online Software Database, University of Mary-
land, College Park, MD.
Y. Freund and R.E. Schapire. 1996. Experiments with
a New Boosting Algorithm. In Machine Learning:
Proceedings of the Thirteenth International Confer-
ence, pages 148–156.
R. Girju. 2003. Automatic detection of causal relations
for Question Answering. In Proceedings of the ACL
2003 workshop on Multilingual summarization and
question answering-Volume 12, pages 76–83.
D. Graff. 2003. English gigaword corpus. Corpus
number LDC2003T05, Linguistic Data Consortium,
Philadelphia.
J. Hobbs. 1979. Coherence and coreference. Cogni-
tive Science, 3:67–90.
A. Knott and T. Sanders. 1998. The classification of
coherence relations and their linguistic markers: An
exploration of two languages. Journal of Pragmat-
ics, 30(2):135–175.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Interna-
tional Conference on Machine Learning 2001, pages
282–289.
M. Lapata and A. Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT-
NAACL 2004: Main Proceedings.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. Chicago, IL.
W.C. Mann and S.A. Thompson. 1988. Rhetorical
structure theory: Towards a functional theory of text
organization. Text, 8.
D. Marcu and A. Echihabi. 2001. An unsupervised
approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 368–375.
D. Marcu. 2000. The Theory and Practice of Dis-
course and Summarization. The MIT Press.
K. McKeown. 1985. Text Generation: Using Dis-
course strategies and Focus Constraints to Gener-
ate Natural Language Text. Cambridge University
Press, Cambridge, England.
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova,
A. Lee, and A. Joshi. 2008. Easily identifiable dis-
course relations. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING08), short paper.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In HLT-NAACL.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: An assessment. Natural Language Engineer-
ing, 14:369–416.
P.J. Stone, J. Kirsh, and Cambridge Computer Asso-
ciates. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky,
and R. Sauri. 2006. Classification of discourse co-
herence relations: An exploratory study using mul-
tiple knowledge sources. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347–354.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational
Linguistics, 31(2):249–288.
</reference>
<page confidence="0.998256">
691
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.556008">
<title confidence="0.99845">Automatic sense prediction for implicit discourse relations in text</title>
<author confidence="0.998691">Emily Pitler</author>
<author confidence="0.998691">Annie Louis</author>
<author confidence="0.998691">Ani Nenkova</author>
<affiliation confidence="0.9990205">Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.999312">Philadelphia, PA 19104, USA</address>
<email confidence="0.999893">epitler,lannie,nenkova@seas.upenn.edu</email>
<abstract confidence="0.966277571428572">We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”. We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses. We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications. Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Asher</author>
<author>A Lascarides</author>
</authors>
<title>Logics of conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7759" citStr="Asher and Lascarides, 2003" startWordPosition="1191" endWordPosition="1194">xplicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations. As mentioned above, each implicit and explicit relation is annotated with a sense. The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason. In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal. Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the ab</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>N. Asher and A. Lascarides. 2003. Logics of conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
<author>K R McKeown</author>
<author>O C Rambow</author>
</authors>
<title>Building and Refining RhetoricalSemantic Relation Models.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<pages>428--435</pages>
<contexts>
<context position="2283" citStr="Blair-Goldensohn et al., 2007" startWordPosition="344" endWordPosition="347">101-year-old magazine has never had to woo advertisers with quite so much fervor before. [because] It largely rested on its hard-to-fault demographics. In this paper we address the problem of automatic sense prediction for discourse relations in newspaper text. For our experiments, we use the Penn Discourse Treebank, the largest existing corpus of discourse annotations for both implicit and explicit relations. Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007). In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits. Explicit relations are easy to identify. The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008). So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress. In this paper we investigate the effectiveness of various features designed to capture lexical and semantic regu</context>
<context position="6014" citStr="Blair-Goldensohn et al., 2007" startWordPosition="927" endWordPosition="930">)), must not only differentiate causal relations from comparison relations, but also from expansions, temporal relations, and possibly no relation at all. In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 3 Penn Discourse Treebank For our experiments, we use the Penn Discourse Treebank (PDTB; Prasad et al., 2008), the largest available annotated corpora of discourse relations. The PDTB contains discourse annotations over the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. For each explicit discourse connective (such as “but” or “so”), annotators identified the two text spans between which the rel</context>
<context position="11111" citStr="Blair-Goldensohn et al. (2007)" startWordPosition="1745" endWordPosition="1748">were superior to those based on only pairs of non-function words. However, since the learning curve is steeper when function words were removed, they hypothesize that using only non-function words will outperform using all words once enough training data is available. In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans. Verb pairs turned out to be one of the best features, but no useful information was obtained using nouns and adjectives. Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model. They show that (i) stemming, (ii) using a small fixed vocabulary size consisting of only the most frequent stems (which would tend to be dominated by function words) and (iii) a cutoff on the minimum frequency of a feature, all result in improved performance. They also report that filtering stopwords has a negative impact on the results. Given these findings, we expect that pairs of function words are informative features helpful in predicting discourse relation sense. In our work that we describe next, we use feature selection to investiga</context>
<context position="15171" citStr="Blair-Goldensohn et al. (2007)" startWordPosition="2407" endWordPosition="2410">ations, but are unlikely to be useful in a realistic setting of actual implicits. Comparison vs. Other Contingency vs. Other the-but s-but the-in of-but for-but but-but in-but was-but it-but to-but that-but the-it* and-and the-the in-in to-the of-and a-of said-but they-but of-in in-and in-of s-and Table 1: Word pairs with highest information gain. Also note that the only two features predictive of the comparison class (indicated by * in Table 1): the-it and to-it, contain only function words rather than semantically related nonfunction words. This ranking explains the observations reported in Blair-Goldensohn et al. (2007) where removing stopwords degraded classifier performance and why using only nouns, verbs or adjectives (Marcu and Echihabi, 2001; Lapata and Lascarides, 2004) is not the best option2. 6 Features for sense prediction of implicit discourse relations The contrast between the “popular”/“oblivion” example we started with above can be analyzed in terms of lexical relations (near antonyms), but also could be explained by different polarities of the two words: “popular” is generally a positive word, while “oblivion” has negative connotations. While we agree that the actual words in the arguments are </context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Rambow, 2007</marker>
<rawString>S. Blair-Goldensohn, K.R. McKeown, and O.C. Rambow. 2007. Building and Refining RhetoricalSemantic Relation Models. In Proceedings of NAACL HLT, pages 428–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="3998" citStr="Carlson et al., 2001" startWordPosition="617" endWordPosition="620">ts results on classifying naturally occurring implicit relations in text and uses the natural distribution of the various senses. 2 Related Work Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense, but often for explicits and at the sentence level. Soricut and Marcu (2003) address the task of 683 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683–691, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence. Such features would not be applicable to the analysis of implicit relations that occur intersententially. Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations. They achieve 81% accuracy in sense disambiguation on </context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu, and M.E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Dorr</author>
</authors>
<title>LCS Verb Database.</title>
<date>2001</date>
<tech>Technical Report</tech>
<institution>Online Software Database, University of Maryland, College Park, MD.</institution>
<contexts>
<context position="21071" citStr="Dorr, 2001" startWordPosition="3374" endWordPosition="3375">st likely relation according to the bigram models was Expansion, then it would include the feature ExpansionBigram3. Expl-LM: This feature ranks the text spans according to language models derived from the explicit examples in the TextRels corpus. However, the corpus contains only Cause, Contrast and Norelation, hence we expect the WSJ language models to be more helpful. Verbs: These features include the number of pairs of verbs in Arg1 and Arg2 from the same verb class. Two verbs are from the same verb class if each of their highest Levin verb class (Levin, 1993) levels (in the LCS Database (Dorr, 2001)) are the same. The intuition behind this feature is that the more related the verbs, the more likely the relation is an Expansion. The verb features also include the average length of verb phrases in each argument, as well as the cross product of this feature for the two arguments. We hypothesized that verb chunks that contain more words, such as “They [are allowed to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations. Our final verb features were the part of s</context>
</contexts>
<marker>Dorr, 2001</marker>
<rawString>B.J. Dorr. 2001. LCS Verb Database. Technical Report Online Software Database, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a New Boosting Algorithm.</title>
<date>1996</date>
<booktitle>In Machine Learning: Proceedings of the Thirteenth International Conference,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="25559" citStr="Freund and Schapire, 1996" startWordPosition="4105" endWordPosition="4108"> equal numbers of positive and negative examples of the target relation. The negative examples were chosen at random. We used all of sections 21 and 22 for testing, so the test set is representative of the natural distribution. The training sets contained: Comparison (1927 positive, 1927 negative), Contingency (3500 each), Expansion3 (6356 each), and Temporal (730 each). The test set contained: 151 examples of Comparison, 291 examples of Contingency, 986 examples of Expansion, 82 examples of Temporal, and 13 examples of No-relation. We used Naive Bayes, Maximum Entropy (MaxEnt), and AdaBoost (Freund and Schapire, 1996) classifiers implemented in MALLET. 7.1 Non-Wordpair Features The performance using only our semantically informed features is shown in Table 7. Only the Naive Bayes classification results are given, as space is limited and MaxEnt and AdaBoost gave slightly lower accuracies overall. The table lists the f-score for each of the target relations, with overall accuracy shown in brackets. Given that the experiments are run on natural distribution of the data, which are skewed towards Expansion relations, the f-score is the more important measure to track. Our random baseline is the f-score one woul</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Y. Freund and R.E. Schapire. 1996. Experiments with a New Boosting Algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic detection of causal relations for Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering-Volume 12,</booktitle>
<pages>76--83</pages>
<contexts>
<context position="5386" citStr="Girju (2003)" startWordPosition="833" endWordPosition="834">tificial implicits Marcu and Echihabi (2001) proposed a method for cheap acquisition of training data for discourse relation sense prediction. Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to create synthetic examples of implicit relations. They delete the connective and use [Arg1, Arg2] as an example of an implicit relation. The approach is tested using binary classification between relations on balanced data, a setting very different from that of any realistic application. For example, a question-answering application that needs to identify causal relations (i.e. as in Girju (2003)), must not only differentiate causal relations from comparison relations, but also from expansions, temporal relations, and possibly no relation at all. In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. Later work (B</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic detection of causal relations for Question Answering. In Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering-Volume 12, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
</authors>
<title>English gigaword corpus.</title>
<date>2003</date>
<booktitle>Corpus number LDC2003T05, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="12096" citStr="Graff, 2003" startWordPosition="1912" endWordPosition="1913">pact on the results. Given these findings, we expect that pairs of function words are informative features helpful in predicting discourse relation sense. In our work that we describe next, we use feature selection to investigate the word pairs in detail. 5 Analysis of word pair features For the analysis of word pair features, we use a large collection of automatically extracted explicit examples from the experiments in BlairGoldensohn et al. (2007). The data, from now on referred to as TextRels, has explicit contrast and causal relations which were extracted from the English Gigaword Corpus (Graff, 2003) which contains over four million newswire articles. The explicit cue phrase is removed from each example and the spans are treated as belonging to an implicit relation. Besides cause and contrast, the TextRels data include a no-relation category which consists of sentences from the same text that are separated by at least three other sentences. To identify features useful for classifying comparison vs other relations, we chose a random sample of 5000 examples for Contrast and 5000 Other relations (2500 each of Cause and No-relation). For the complete set of 10,000 examples, word pair features</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>D. Graff. 2003. English gigaword corpus. Corpus number LDC2003T05, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Coherence and coreference.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--67</pages>
<contexts>
<context position="7665" citStr="Hobbs, 1979" startWordPosition="1179" endWordPosition="1180">, and then labeled the implicit discourse relation Contingency. In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations. As mentioned above, each implicit and explicit relation is annotated with a sense. The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason. In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal. Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictor</context>
</contexts>
<marker>Hobbs, 1979</marker>
<rawString>J. Hobbs. 1979. Coherence and coreference. Cognitive Science, 3:67–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Knott</author>
<author>T Sanders</author>
</authors>
<title>The classification of coherence relations and their linguistic markers: An exploration of two languages.</title>
<date>1998</date>
<journal>Journal of Pragmatics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="7730" citStr="Knott and Sanders, 1998" startWordPosition="1187" endWordPosition="1190">ntingency. In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations. As mentioned above, each implicit and explicit relation is annotated with a sense. The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason. In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal. Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et a</context>
</contexts>
<marker>Knott, Sanders, 1998</marker>
<rawString>A. Knott and T. Sanders. 1998. The classification of coherence relations and their linguistic markers: An exploration of two languages. Journal of Pragmatics, 30(2):135–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In International Conference on Machine Learning</booktitle>
<pages>282--289</pages>
<contexts>
<context position="33797" citStr="Lafferty et al., 2001" startWordPosition="5343" endWordPosition="5346">f 47.13 (16% absolute improvement in f-score over Wordpairs-TextRels). For detecting expansions, the best combination of our features (polarity+Inquirer tags+context) outperformed Wordpairs-PDTBImpl by a wide margin, close to 13% absolute improvement (fscores of 76.42 and 63.84 respectively). 7.4 Sequence Model of Discourse Relations Our results from the previous section show that classification of implicits benefits from information about nearby relations, and so we expected improvements using a sequence model, rather than classifying each relation independently. We trained a CRF classifier (Lafferty et al., 2001) over the sequence of implicit examples from all documents in sections 02 to 20. The test set is the same as used for the 2-way classifiers. We compare against a 6-way4 Naive Bayes classifier. Only word pairs were used as features for both. Overall 6-way prediction accuracy is 43.27% for the Naive Bayes model and 44.58% for the CRF model. 8 Conclusion We have presented the first study that predicts implicit discourse relations in a realistic setting (distinguishing a relation of interest from all others, where the relations occur in their natural distributions). Also unlike prior work, we sepa</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In International Conference on Machine Learning 2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>A Lascarides</author>
</authors>
<title>Inferring sentence-internal temporal relations.</title>
<date>2004</date>
<booktitle>In HLTNAACL 2004: Main Proceedings.</booktitle>
<contexts>
<context position="9611" citStr="Lapata and Lascarides, 2004" startWordPosition="1503" endWordPosition="1506">ontrast relation between the sentences. Similarly, a pair of words such as (rain, rot) might be indicative of a causal relation. If this hypothesis is correct, pairs of words (w1, w2) such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations. Indeed, word pairs form the basic feature of most previous work on classifying implicit relations (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007; Sporleder and Lascarides, 2008) or the simpler task of predicting which connective should be used to express a relation (Lapata and Lascarides, 2004). Semantic relations vs. function word pairs If the hypothesis for word pair triggers of discourse relations were true, the analysis of unambiguous relations can be used to discover pairs of words with causal or contrastive relations holding between them. Yet, feature analysis has not been performed in prior studies to establish or refute this possibility. At the same time, feature selection is always necessary for word pairs, which are numerous and lead to data sparsity problems. Here, we present a meta analysis of the feature selection work in three prior studies. One approach for reducing t</context>
<context position="15330" citStr="Lapata and Lascarides, 2004" startWordPosition="2429" endWordPosition="2432">but-but in-but was-but it-but to-but that-but the-it* and-and the-the in-in to-the of-and a-of said-but they-but of-in in-and in-of s-and Table 1: Word pairs with highest information gain. Also note that the only two features predictive of the comparison class (indicated by * in Table 1): the-it and to-it, contain only function words rather than semantically related nonfunction words. This ranking explains the observations reported in Blair-Goldensohn et al. (2007) where removing stopwords degraded classifier performance and why using only nouns, verbs or adjectives (Marcu and Echihabi, 2001; Lapata and Lascarides, 2004) is not the best option2. 6 Features for sense prediction of implicit discourse relations The contrast between the “popular”/“oblivion” example we started with above can be analyzed in terms of lexical relations (near antonyms), but also could be explained by different polarities of the two words: “popular” is generally a positive word, while “oblivion” has negative connotations. While we agree that the actual words in the arguments are quite useful, we also define several higher-level features corresponding to various semantic properties of the words. The words in the two text spans of a rela</context>
</contexts>
<marker>Lapata, Lascarides, 2004</marker>
<rawString>M. Lapata and A. Lascarides. 2004. Inferring sentence-internal temporal relations. In HLTNAACL 2004: Main Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<location>Chicago, IL.</location>
<contexts>
<context position="21030" citStr="Levin, 1993" startWordPosition="3367" endWordPosition="3368">ature ContingencyUnigram1. If the third most likely relation according to the bigram models was Expansion, then it would include the feature ExpansionBigram3. Expl-LM: This feature ranks the text spans according to language models derived from the explicit examples in the TextRels corpus. However, the corpus contains only Cause, Contrast and Norelation, hence we expect the WSJ language models to be more helpful. Verbs: These features include the number of pairs of verbs in Arg1 and Arg2 from the same verb class. Two verbs are from the same verb class if each of their highest Levin verb class (Levin, 1993) levels (in the LCS Database (Dorr, 2001)) are the same. The intuition behind this feature is that the more related the verbs, the more likely the relation is an Expansion. The verb features also include the average length of verb phrases in each argument, as well as the cross product of this feature for the two arguments. We hypothesized that verb chunks that contain more words, such as “They [are allowed to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations. O</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical structure theory: Towards a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8.</tech>
<contexts>
<context position="4125" citStr="Mann and Thompson, 1988" startWordPosition="635" endWordPosition="638">nses. 2 Related Work Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense, but often for explicits and at the sentence level. Soricut and Marcu (2003) address the task of 683 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683–691, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence. Such features would not be applicable to the analysis of implicit relations that occur intersententially. Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations. They achieve 81% accuracy in sense disambiguation on this corpus. However, GraphBank annotations do not differentiate between implicits and explicits, so it is difficult to verify </context>
<context position="7705" citStr="Mann and Thompson, 1988" startWordPosition="1183" endWordPosition="1186">cit discourse relation Contingency. In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations. As mentioned above, each implicit and explicit relation is annotated with a sense. The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason. In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal. Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation </context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W.C. Mann and S.A. Thompson. 1988. Rhetorical structure theory: Towards a functional theory of text organization. Text, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>368--375</pages>
<contexts>
<context position="2251" citStr="Marcu and Echihabi, 2001" startWordPosition="340" endWordPosition="343">low demonstrates: (1) The 101-year-old magazine has never had to woo advertisers with quite so much fervor before. [because] It largely rested on its hard-to-fault demographics. In this paper we address the problem of automatic sense prediction for discourse relations in newspaper text. For our experiments, we use the Penn Discourse Treebank, the largest existing corpus of discourse annotations for both implicit and explicit relations. Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007). In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits. Explicit relations are easy to identify. The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008). So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress. In this paper we investigate the effectiveness of various features designed to c</context>
<context position="4818" citStr="Marcu and Echihabi (2001)" startWordPosition="740" endWordPosition="743">t that both arguments of the connective are found in the same sentence. Such features would not be applicable to the analysis of implicit relations that occur intersententially. Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations. They achieve 81% accuracy in sense disambiguation on this corpus. However, GraphBank annotations do not differentiate between implicits and explicits, so it is difficult to verify success for implicit relations. Experiments on artificial implicits Marcu and Echihabi (2001) proposed a method for cheap acquisition of training data for discourse relation sense prediction. Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to create synthetic examples of implicit relations. They delete the connective and use [Arg1, Arg2] as an example of an implicit relation. The approach is tested using binary classification between relations on balanced data, a setting very different from that of any realistic application. For example, a question-answering application that needs to identify causal relations (i.e. as in Girju (2003)), must not only differentiate c</context>
<context position="9430" citStr="Marcu and Echihabi, 2001" startWordPosition="1473" endWordPosition="1476">livion after the 1929 crash. The words “popular” and “oblivion” are almost antonyms, and one might hypothesize that their occurrence in the two text spans is what triggers the contrast relation between the sentences. Similarly, a pair of words such as (rain, rot) might be indicative of a causal relation. If this hypothesis is correct, pairs of words (w1, w2) such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations. Indeed, word pairs form the basic feature of most previous work on classifying implicit relations (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007; Sporleder and Lascarides, 2008) or the simpler task of predicting which connective should be used to express a relation (Lapata and Lascarides, 2004). Semantic relations vs. function word pairs If the hypothesis for word pair triggers of discourse relations were true, the analysis of unambiguous relations can be used to discover pairs of words with causal or contrastive relations holding between them. Yet, feature analysis has not been performed in prior studies to establish or refute this possibility. At the same time, feature selection is always necessary for </context>
<context position="13643" citStr="Marcu and Echihabi (2001)" startWordPosition="2160" endWordPosition="2163">nd interestingly for the Con1mallet.cs.umass.edu 685 trast vs. Other task, most of the word pairs contain discourse connectives. This is certainly unexpected, given that word pairs were formed by deleting the discourse connectives from the sentences expressing Contrast. Word pairs containing “but” as one of their elements in fact signal the presence of a relation that is not Contrast. Consider the example shown below: The government says it has reached most isolated townships by now, but because roads are blocked, getting anything but basic food supplies to people remains difficult. Following Marcu and Echihabi (2001), the pair [The government says it has reached most isolated townships by now, but] and [roads are blocked, getting anything but basic food supplies to people remains difficult.] is created as an example of the Cause relation. Because of examples like this, “but-but” is a very useful word pair feature indicating Cause, as the but would have been removed for the artifical Contrast examples. In fact, the top 17 features for classifying Contrast versus Other all contain the word “but”, and are indications that the relation is Other. These findings indicate an unexpected anomalous effect in the us</context>
<context position="15300" citStr="Marcu and Echihabi, 2001" startWordPosition="2425" endWordPosition="2428">but the-in of-but for-but but-but in-but was-but it-but to-but that-but the-it* and-and the-the in-in to-the of-and a-of said-but they-but of-in in-and in-of s-and Table 1: Word pairs with highest information gain. Also note that the only two features predictive of the comparison class (indicated by * in Table 1): the-it and to-it, contain only function words rather than semantically related nonfunction words. This ranking explains the observations reported in Blair-Goldensohn et al. (2007) where removing stopwords degraded classifier performance and why using only nouns, verbs or adjectives (Marcu and Echihabi, 2001; Lapata and Lascarides, 2004) is not the best option2. 6 Features for sense prediction of implicit discourse relations The contrast between the “popular”/“oblivion” example we started with above can be analyzed in terms of lexical relations (near antonyms), but also could be explained by different polarities of the two words: “popular” is generally a positive word, while “oblivion” has negative connotations. While we agree that the actual words in the arguments are quite useful, we also define several higher-level features corresponding to various semantic properties of the words. The words i</context>
</contexts>
<marker>Marcu, Echihabi, 2001</marker>
<rawString>D. Marcu and A. Echihabi. 2001. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 368–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Theory and Practice of Discourse and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8317" citStr="Marcu, 2000" startWordPosition="1287" endWordPosition="1288"> Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the 684 words in the two text spans of the relation. Intuitively, one would expect that there is some relationship that holds between the words in the two arguments. Consider for example the following sentences: The recent explosion of country funds mirrors the ”closedend fund mania” of the 1920s, Mr. Foot says, when narrowly focused funds grew wildly popular. They fell into oblivion after the 1929 crash. The words “popular” and “oblivion” are almost antonyms, and one might hypothesize </context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>D. Marcu. 2000. The Theory and Practice of Discourse and Summarization. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Text Generation: Using Discourse strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="7680" citStr="McKeown, 1985" startWordPosition="1181" endWordPosition="1182">beled the implicit discourse relation Contingency. In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations. As mentioned above, each implicit and explicit relation is annotated with a sense. The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason. In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal. Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semant</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>K. McKeown. 1985. Text Generation: Using Discourse strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>M Raghupathy</author>
<author>H Mehta</author>
<author>A Nenkova</author>
<author>A Lee</author>
<author>A Joshi</author>
</authors>
<title>Easily identifiable discourse relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING08),</booktitle>
<pages>short paper.</pages>
<contexts>
<context position="2657" citStr="Pitler et al., 2008" startWordPosition="403" endWordPosition="406">licit and explicit relations. Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007). In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits. Explicit relations are easy to identify. The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008). So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress. In this paper we investigate the effectiveness of various features designed to capture lexical and semantic regularities for identifying the sense of implicit relations. Given two text spans, previous work has used the cross-product of the words in the spans as features. We examine the most informative word pair features and find that they are not the semantically-related pairs that researchers had hoped. We then introduce several other methods capturing the semantics of the spans </context>
<context position="8339" citStr="Pitler et al., 2008" startWordPosition="1289" endWordPosition="1292">nders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names. Each relation in the PDTB takes two arguments. Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments. For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2. 4 Word pair features in prior work Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008). However, in the absence of explicit markers, the most easily accessible features are the 684 words in the two text spans of the relation. Intuitively, one would expect that there is some relationship that holds between the words in the two arguments. Consider for example the following sentences: The recent explosion of country funds mirrors the ”closedend fund mania” of the 1920s, Mr. Foot says, when narrowly focused funds grew wildly popular. They fell into oblivion after the 1929 crash. The words “popular” and “oblivion” are almost antonyms, and one might hypothesize that their occurrence </context>
<context position="23318" citStr="Pitler et al., 2008" startWordPosition="3743" endWordPosition="3746"> We also add two additional features which indicate the first three words of each argument. Modality: Modal words, such as “can”, “should”, and “may”, are often used to express conditional statements (i.e. “If I were a wealthy man, I wouldn’t have to work hard.”) thus signaling a Contingency relation. We include a feature for the presence or absence of modals in Arg1 and Arg2, features for specific modal words, and their cross-products. Context: Some implicit relations appear immediately before or immediately after certain explicit relations far more often than one would expect due to chance (Pitler et al., 2008). We define a feature indicating if the immediately preceding (or following) relation was an explicit. If it was, we include the connective trigger of the relation and its sense as features. We use oracle annotations of the connective sense, however, most of the connectives are unambiguous. One might expect a different distribution of relation types in the beginning versus further in the middle of a paragraph. We capture paragraphposition information using a feature which indicates if Arg1 begins a paragraph. Word pairs Four variants of word pair models were used in our experiments. All the mo</context>
<context position="29281" citStr="Pitler et al. (2008)" startWordPosition="4693" endWordPosition="4696">e two most useful classes of features for recognizing Comparison relations were the first, last and first three words in the sentence and the context features that indicate the presence of a paragraph boundary or of an explicit relation just before or just after the location of the hypothesized implicit relation (19.32 f-score). Contingency The two best features for the Contingency vs. Other distinction were verb information (36.59 f-score) and first, last and first three words in the sentence (36.75 f-score). Context again was one of the features that led to improvement. This makes sense, as Pitler et al. (2008) found that implicit contingencies are often found immediately following explicit comparisons. We were surprised that the polarity features were helpful for Contingency but not Comparison. Again we looked at the prevalence of opposite polarity pairs. While for Comparison versus Other there was not a significant difference, for Contingency there are quite a few more opposite polarity pairs (52%) than for not Contingency (41%). The language model features were completely useless for distinguishing contingencies from other relations. Expansion As Expansion is the majority class in the natural dis</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, and A. Joshi. 2008. Easily identifiable discourse relations. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING08), short paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="3717" citStr="Soricut and Marcu (2003)" startWordPosition="568" endWordPosition="571"> and find that they are not the semantically-related pairs that researchers had hoped. We then introduce several other methods capturing the semantics of the spans (polarity features, semantic classes, tense, etc.) and evaluate their effectiveness. This is the first study which reports results on classifying naturally occurring implicit relations in text and uses the natural distribution of the various senses. 2 Related Work Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense, but often for explicits and at the sentence level. Soricut and Marcu (2003) address the task of 683 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683–691, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence. Such features would not be applicable to the analysi</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering,</journal>
<pages>14--369</pages>
<contexts>
<context position="6047" citStr="Sporleder and Lascarides, 2008" startWordPosition="931" endWordPosition="934"> causal relations from comparison relations, but also from expansions, temporal relations, and possibly no relation at all. In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently. Causal and comparison relations, which are most useful for applications, are less frequent. Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy. Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. 3 Penn Discourse Treebank For our experiments, we use the Penn Discourse Treebank (PDTB; Prasad et al., 2008), the largest available annotated corpora of discourse relations. The PDTB contains discourse annotations over the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. For each explicit discourse connective (such as “but” or “so”), annotators identified the two text spans between which the relation holds and the sense of the </context>
<context position="9493" citStr="Sporleder and Lascarides, 2008" startWordPosition="1482" endWordPosition="1486">livion” are almost antonyms, and one might hypothesize that their occurrence in the two text spans is what triggers the contrast relation between the sentences. Similarly, a pair of words such as (rain, rot) might be indicative of a causal relation. If this hypothesis is correct, pairs of words (w1, w2) such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations. Indeed, word pairs form the basic feature of most previous work on classifying implicit relations (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007; Sporleder and Lascarides, 2008) or the simpler task of predicting which connective should be used to express a relation (Lapata and Lascarides, 2004). Semantic relations vs. function word pairs If the hypothesis for word pair triggers of discourse relations were true, the analysis of unambiguous relations can be used to discover pairs of words with causal or contrastive relations holding between them. Yet, feature analysis has not been performed in prior studies to establish or refute this possibility. At the same time, feature selection is always necessary for word pairs, which are numerous and lead to data sparsity proble</context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>C. Sporleder and A. Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Natural Language Engineering, 14:369–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
<author>J Kirsh</author>
</authors>
<title>and Cambridge Computer Associates.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<marker>Stone, Kirsh, 1966</marker>
<rawString>P.J. Stone, J. Kirsh, and Cambridge Computer Associates. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>J Pustejovsky</author>
<author>C Havasi</author>
<author>A Rumshisky</author>
<author>R Sauri</author>
</authors>
<title>Classification of discourse coherence relations: An exploratory study using multiple knowledge sources.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="4392" citStr="Wellner et al. (2006)" startWordPosition="678" endWordPosition="681">l Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 683–691, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence. Such features would not be applicable to the analysis of implicit relations that occur intersententially. Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations. They achieve 81% accuracy in sense disambiguation on this corpus. However, GraphBank annotations do not differentiate between implicits and explicits, so it is difficult to verify success for implicit relations. Experiments on artificial implicits Marcu and Echihabi (2001) proposed a method for cheap acquisition of training data for discourse relation sense prediction. Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to crea</context>
<context position="22059" citStr="Wellner et al., 2006" startWordPosition="3534" endWordPosition="3537">d to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations. Our final verb features were the part of speech tags (gold-standard from the Penn Treebank) of the main verb. One would expect that Expansion would link sentences with the same tense, whereas Contingency and Temporal relations would contain verbs with different tenses. First-Last, First3: The first and last words of a relation’s arguments have been found to be particularly useful for predicting its sense (Wellner et al., 2006). Wellner et al. (2006) suggest that these words are such predictive features because they are often explicit discourse connectives. In our experiments on implicits, the first and last words are not connectives. However, some implicits have been found to be related by connective-like expressions which often appear in the beginning of the second argument. In the PDTB, these are annotated as alternatively lexicalized relations (AltLexes). To capture such effects, we included the first and last words of Arg1 as features, the first 687 and last words of Arg2, the pair of the first words of Arg1 an</context>
</contexts>
<marker>Wellner, Pustejovsky, Havasi, Rumshisky, Sauri, 2006</marker>
<rawString>B. Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, and R. Sauri. 2006. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. In Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="16224" citStr="Wilson et al., 2005" startWordPosition="2572" endWordPosition="2575">rities of the two words: “popular” is generally a positive word, while “oblivion” has negative connotations. While we agree that the actual words in the arguments are quite useful, we also define several higher-level features corresponding to various semantic properties of the words. The words in the two text spans of a relation are taken from the gold-standard annotations in the PDTB. Polarity Tags: We define features that represent the sentiment of the words in the two spans. Each word’s polarity was assigned according to its entry in the Multi-perspective Question Answering Opinion Corpus (Wilson et al., 2005). In this resource, each sentiment word is annotated as positive, negative, both, or neutral. We use the number of negated and non-negated positive, negative, and neutral sentiment words in the two text spans as features. If a writer refers to something as “nice” in Arg1, that counts towards the positive sentiment count (Arg1Positive); “not nice” would count towards Arg1NegatePositive. A sentiment word is negated if a word with a General Inquirer (Stone et al., 1966) Negate tag precedes it. We also have features for the cross products of these polarities between Arg1 and Arg2. We expected that</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
</authors>
<title>Representing discourse coherence: A corpus-based study.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="4435" citStr="Wolf and Gibson, 2005" startWordPosition="685" endWordPosition="688"> the AFNLP, pages 683–691, Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP parsing discourse structures within the same sentence. They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence. Such features would not be applicable to the analysis of implicit relations that occur intersententially. Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations. They achieve 81% accuracy in sense disambiguation on this corpus. However, GraphBank annotations do not differentiate between implicits and explicits, so it is difficult to verify success for implicit relations. Experiments on artificial implicits Marcu and Echihabi (2001) proposed a method for cheap acquisition of training data for discourse relation sense prediction. Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to create synthetic examples of implicit relations</context>
</contexts>
<marker>Wolf, Gibson, 2005</marker>
<rawString>F. Wolf and E. Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational Linguistics, 31(2):249–288.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>