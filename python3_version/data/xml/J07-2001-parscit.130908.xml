<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.971173">
Maximal Consistent Subsets
</title>
<author confidence="0.996588">
Robert Malouf∗
</author>
<affiliation confidence="0.934795">
San Diego State University
</affiliation>
<bodyText confidence="0.9385485">
Default unification operations combine strict information with information from one or more
defeasible feature structures. Many such operations require finding the maximal subsets of a set
of atomic constraints that are consistent with each other and with the strict feature structure,
where a subset is maximally consistent with respect to the subsumption ordering if no constraint
can be added to it without creating an inconsistency. Although this problem is NP-complete,
there are a number of heuristic optimizations that can be used to substantially reduce the size of
the search space. In this article, we propose a novel optimization, leaf pruning, which in some
cases yields an improvement in running time of several orders of magnitude over previously
described algorithms. This makes default unification efficient enough to be practical for a wide
range of problems and applications.
</bodyText>
<sectionHeader confidence="0.995078" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.978457333333334">
In unification-based grammatical frameworks, it often desirable to combine information
from possibly inconsistent sources. Over the years, a number of default unification
operations have been proposed1, which combine a strict feature structure with one or
more defeasible feature structures. These operations preserve all information in the
strict feature structure, while bringing in as much information as possible from the
defeasible structures. Default unification has been used to address a wide variety of
linguistic knowledge representation problems, including lexical inheritance hierarchies
(Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake
1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora
resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse
processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among
many others.
Although the various default unification operators differ in their particulars, most
involve something like Carpenter (1992)’s credulous default unification as one step.
The result of credulously adding the default information in G to the strict information
in F is:
cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and
G&apos; is maximal such that unify(F, G&apos;) is defined}
</bodyText>
<footnote confidence="0.9714382">
∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500
Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu.
1 For example, Bouma (1992), Carpenter (1992), Pr¨ust (1992), Calder (1993), Lascarides and Copestake
(1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii (2002). See Bouma (2006) for a
recent overview.
</footnote>
<note confidence="0.866538">
© 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 2
</note>
<bodyText confidence="0.9994432">
In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent
subset(s) of the atomic constraints in G. A subset of constraints is maximally consistent
with respect to the subsumption ordering if no constraint can be added to it without cre-
ating an inconsistency. In general, there may be more than one subset of the constraints
in G that is consistent with F and maximal, so the result of credulous default unification
is a set of feature structures.
One example of the use of credulous default unification for the resolution of dis-
course anaphora comes from Grover et al. (1994). Consider the mini-discourse: Jessy likes
her brother. So does Hannah. To resolve the anaphoric predicate in the second sentence, we
can set up meaning representations for the source and the target:
</bodyText>
<table confidence="0.6816505">
Source:  REL like  Target: [ AGENT hannah �
   AGENT 1 jessy 
� REL brother~
PATIENT THEME 1
</table>
<bodyText confidence="0.907488">
and credulously unify the source with the strict information in the target.
To proceed, we first decompose the source feature structure into the following five
atomic ground constraints:
</bodyText>
<equation confidence="0.991666">
{I REL like ] , [ AGENT jessy ] , [ PATIENT|REL brother ] ,
PATIENT  |THEME jessy , AGENT 1
1PATIENT|THEME 1 ] }
</equation>
<bodyText confidence="0.994334">
Then, we find the maximal subsets of the remaining constraints which are mutually
consistent with the target. This yields two solutions:
</bodyText>
<figure confidence="0.913449882352941">

  


REL like
AGENT 1 hannah
� REL brother~
PATIENT THEME 1
 
 
 
 
REL like
AGENT hannah
~ REL brother �
PATIENT
THEME jessy
</figure>
<bodyText confidence="0.9117875">
corresponding to the sloppy identity and the strict identity readings of the anaphoric
expression.
</bodyText>
<sectionHeader confidence="0.981347" genericHeader="keywords">
2. Algorithm
</sectionHeader>
<bodyText confidence="0.852052555555556">
A key step for applying most default unification operators is finding the maximal con-
sistent subsets of a set of constraints C. Unfortunately, finding these maximal consistent
subsets is an expensive operation, and, in fact, is NP-complete. Let T = {T1, ... , Tm} be
the set of conflicts in C, where a conflict is a minimal set of constraints that are mutually
inconsistent with the target. For this example, T consists of the two conflicts:
�� PATIENT  |THEME jessy , AGENT 1
1PATIENT|THEME 1 ] }, J [AGENT jessy } }
Removing any one of the constraints from a conflict Ti would break that conflict, so if we
could remove from C at least one member of each Ti, the remaining constraints would be
</bodyText>
<page confidence="0.998921">
154
</page>
<subsectionHeader confidence="0.365194">
Malouf Maximal Consistent Subsets
</subsectionHeader>
<bodyText confidence="0.999951375">
mutually consistent with the target information. Finding the maximal consistent subsets
of C then is equivalent to finding the minimal subset C&apos; ⊂ C such that each Ti contains
at least one member of C&apos;. This is the hitting subset problem, a classic NP-complete
problem (Karp 1972).
An algorithm to find the maximal consistent subsets of C must check each subset of
C for consistency. One way to proceed is to construct a spanning tree of the boolean
lattice of subsets of C. This takes the form of a binomial tree, as in Figure 1 (Bird
and Hinze 2003). At each node, we keep track of k, the index of the constraint that
was removed from the parent set to create that subset. For example, subset {c1, c3}
was formed by removing c2 from {c1, c2, c3}, so k = 2. The descendants of a node are
constructed by successively dropping each of the constraints ci where k &lt; i ≤ |C|. This
ensures that we will visit every subset of C exactly once.
The algorithm described by Grover et al. (1994) performs a breadth-first search
of the subset lattice, with one important optimization. Because the cardinality of the
subsets at each level is one greater than those on the level below it, a breadth-first
search of this tree will consider all larger sets before considering any smaller ones.
Furthermore, because each subset is produced by removing constraints from its parent
set, every node in a subtree is a subset of its root. This means that once a consistent set
is found, no descendants of that set can be maximal, and that subtree can be pruned
from the search space. However, consistent subsets that are maximal in their branch of
the tree may turn out not to be globally maximal. For example, in Figure 1, if {c1, c2}
is consistent and {c1, c3} is not, a breadth-first search would identify both {c1, c2} and
{c1} as consistent and (locally) maximal. A final post-check for set inclusion can remove
pseudo-maximal results like {c1}.
In addition to pruning branches rooted by a consistent subset (call this root prun-
ing), the organization of the search space into a binomial tree allows another valuable
optimization. The deepest leaf node in any subtree is the set formed from the root by
removing all constraints ck&lt;i≤|C|, and every set in the subset is a superset of that deepest
leaf. Because no superset of an inconsistent set of constraints can be consistent, if the
foot of a subtree is inconsistent then clearly no node in the tree can be consistent, and
the entire tree can be skipped (call this leaf pruning). Taking both root pruning and
leaf pruning together, the only subtrees that need to be explored are those whose root is
</bodyText>
<figureCaption confidence="0.825491">
Figure 1
</figureCaption>
<bodyText confidence="0.845119">
Boolean lattice and binomial spanning tree for |C |= 3.
</bodyText>
<page confidence="0.983321">
155
</page>
<note confidence="0.274447">
Computational Linguistics Volume 33, Number 2
</note>
<bodyText confidence="0.991549818181818">
inconsistent but whose deepest leaf is consistent. No other subtrees can possibly contain
a solution.
Figure 2 gives a breadth-first search algorithm that implements these optimiza-
tions. Like Grover et al. (1994), this algorithm requires a post-check to remove pseudo-
maximal subsets from results. A queue is used to keep track of subsets S that are yet to be
checked for consistency, along with the index k of the constraint that was last dropped,
and a flag leftmost that indicates whether that subset is the leftmost child. Because the
deepest leaf node of the leftmost child is the same as the deepest leaf node of the parent,
we are guaranteed that the deepest leaf of a leftmost child is consistent. Keeping track
of leftmost children allows us to avoid a substantial number of redundant consistency
checks.
</bodyText>
<sectionHeader confidence="0.993858" genericHeader="introduction">
3. Evaluation
</sectionHeader>
<bodyText confidence="0.999923461538462">
The graphs in Figure 3 and Figure 4 show an empirical comparison between a breadth-
first search with root pruning (BFS-R) and a breadth-first search with root and leaf
pruning (BFS-RL) on randomly generated problems. The graphs show the number of
subsets that were checked for consistency, as it relates to |C|, the number of constraints,
and p, the probability that two members of C are consistent. Larger values for p generally
lead to fewer but larger maximal consistent subsets. All counts are averaged across 100
randomly generated sets of ground constraints. In generating random problems, we
make the simplifying assumptions that all constraints are consistent with any inde-
feasible information, and that a subset of constraints that are pairwise consistent is a
consistent subset.
The first thing to note in these graphs is that root pruning by itself provides very
little benefit. For most values of p, the number of subsets checked by BFS-R is very
close to the worst case maximum 2|C|. A possible reason for this is that root pruning
</bodyText>
<figureCaption confidence="0.806402">
Figure 2
Find the maximal consistent subsets of C. Performs a breadth-first search of the subset tree, with
root and leaf pruning.
</figureCaption>
<page confidence="0.865232">
156
</page>
<figure confidence="0.723856">
Malouf Maximal Consistent Subsets
</figure>
<figureCaption confidence="0.954208">
Figure 3
</figureCaption>
<bodyText confidence="0.999587666666667">
Comparison of breadth-first search using root pruning alone (BFS-R) and in combination with
leaf pruning (BFS-RL). |C |is the number of ground constraints, p is the fraction of the ground
constraints which are pairwise consistent, and “Subsets visited” is the number of subsets of C
which were checked for consistency (on a logarithmic scale). All counts are based on the average
of 100 randomly generated problems.
will have the greatest effect when consistent subsets are found in the interior nodes of
the binomial search tree. However, the configuration of the search space is such that
most nodes are either leaves or very close to leaves, and only a few nodes have a large
number of descendants. Therefore, root pruning mostly removes very small subtrees
and has only a small effect on the overall cost of the algorithm.
The next observation to make is that for small values of |C |(in these experiments,
less than 7), BFS-RL is very slightly more expensive than BFS-R. In these cases, the
advantages of leaf pruning do not outweigh the cost of the extra consistency checks
required to implement it. As |C |increases, though, leaf pruning can offer substantial
improvements. For |C |= 19 and p = 0.1, leaf pruning eliminates more than 99.5% of
the search space, leading to a 185-fold improvement in running time. As p increases,
the benefits of leaf pruning do become more modest. Larger values of p mean fewer
inconsistent leaf nodes, so fewer subtrees are able to be eliminated. Even so, the savings
</bodyText>
<page confidence="0.966678">
157
</page>
<figure confidence="0.829801">
Computational Linguistics Volume 33, Number 2
</figure>
<figureCaption confidence="0.993133">
Figure 4
</figureCaption>
<bodyText confidence="0.994089703703704">
Comparison of subsets visited by BFS-R and BFS-RL as a function of p for |C |= 15. All counts
are based on the average of 100 randomly generated problems.
from leaf pruning can still be dramatic: at |C |= 19 and p = 0.9, leaf pruning yields a
nearly five-fold improvement in speed.
Values of |C |and p that can be realistically expected will vary widely from ap-
plication to application. An anonymous reviewer reports that in one application, the
resolution of non-monotonic lexical inheritance for constraint-based grammars, p is
generally greater than 0.7. This may be due in part to the fact that most constraint-
based grammar development platforms do not support defaults (Copestake’s [2002]
LKB is a notable exception), and so grammar engineers tend to avoid the use of default
overriding. Ginzburg and Sag (2001) propose a more comprehensive use of defaults, and
grammars written following these principles would likely have a much lower value of
p. To my knowledge, however, these ideas have not yet made their way into any large-
scale grammar implementations.
Ninomiya, Miyao, and Tsujii (2002) describe experiments using default unification
for robust parsing and automatic grammar augmentation via a kind of explanation-
based learning. For this application, all features of a rule in the base XHPSG grammar
(Tateisi et al. 1998) are considered defaults that can be overridden if necessary to get a
successful parse of a sentence. In this case, |C |is likely very large and grows quickly
with the length of the sentences being parsed. The value of p will depend on the
coverage of the base grammar, but can be expected to be fairly close to 1.0 for most
domains. In situations such as this, where p is expected to fall close to the worst case
for leaf pruning, one could consider inverting the search direction of the algorithm in
Figure 2. Rather than beginning with C and removing constraints until a consistent
subset is found, we could instead begin with the empty subset and add constraints
until an inconsistency is found. In either case, the frontier in the search space between
consistent and inconsistent subsets is where maximally consistent subsets will be found,
</bodyText>
<page confidence="0.994397">
158
</page>
<note confidence="0.263315">
Malouf Maximal Consistent Subsets
</note>
<bodyText confidence="0.9752615">
and leaf pruning can be used to eliminate regions of the search space that contain only
consistent or inconsistent subsets.
</bodyText>
<sectionHeader confidence="0.990476" genericHeader="conclusions">
4. Conclusions
</sectionHeader>
<bodyText confidence="0.99987175">
Finding the maximal consistent subsets of a set of ground constraints is an impor-
tant sub-problem for many natural language processing and knowledge representa-
tion tasks. Unfortunately, the problem is NP-complete, and in the worst case requires
checking all 2|C |subsets of C for consistency. Previously proposed algorithms have
produced approximate solutions (Boppana and Halld´orsson 1992), or have weakened
the requirements to make finding a solution easier (Ninomiya, Miyao, and Tsujii 2002).
By using deepest leaf pruning, the algorithm described in the previous sections
improves on the method of Grover et al. (1994) and is able to achieve substantial gains
over the worst case running time for a large class of problems. An efficient method for
finding maximal consistent subsets can make default unification practical for problems
such as large-scale lexical representation, on-line discourse processing, or ontology
construction.
</bodyText>
<sectionHeader confidence="0.89412" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.874080333333333">
Alexandersson, Jan and Tilman Becker. 2001.
Overlay as the basic operation for
discourse processing in a multimodal
</bodyText>
<reference confidence="0.963367867647059">
dialogue system. In Proceedings of the IJCAI
Workshop “Knowledge and Reasoning in
Practical Dialogue Systems,” pages 8–14,
Seattle, WA.
Alexandersson, Jan, Tilman Becker, and
Norbert Pfleger. 2004. Scoring for overlay
based on informational distance. In
Proceedings of KONVENS 2004, pages 1–4,
Vienna, Austria.
Bird, Richard and Ralf Hinze. 2003.
Functional pearl: Trouble shared is trouble
halved. In Proceedings of the 2003 ACM
SIGPLAN Workshop on Haskell, pages 1–6,
Uppsala, Sweden.
Boppana, Ravi and Magn´us M. Halld´orsson.
1992. Approximating maximum
independent sets by excluding subgraphs.
BIT Numerical Mathematics, 32(2):180–196.
Bouma, Gosse. 1992. Feature structures and
nonmonotonicity. Computational
Linguistics, 18(2):183–204.
Bouma, Gosse. 2006. Unification: Classical
and default. In Keith Brown, editor,
Encyclopedia of Language and Linguistics.
Elsevier, New York.
Briscoe, E. J. 1999. The acquisition of
grammar in an evolving population of
language agents. Electronic Transactions in
Artificial Intelligence. Special Issue: Machine
Intelligence, 3(035):47–77.
Calder, Jonathan. 1993. Feature-value logics:
Some limits on the role of defaults. In C. J.
Rupp, Mike Rosner, and Rod Johnson,
editors, Constraints, Language and
Computation. Academic Press, London,
pages 20–32.
Carpenter, Bob. 1992. Skeptical and
creduluous default unification with
applications to templates and inheritance.
In Ted Briscoe, Anne Copestake, and
Valerie de Paiva, editors, Default
Inheritance within Unification-Based
Approaches to the Lexicon. Cambridge
University Press, Cambridge, UK,
pages 13–37.
Copestake, Ann. 1993. Defaults in lexical
representation. In E. J. Briscoe,
A. Copestake, and V. de Paiva, editors,
Inheritance, Defaults and the Lexicon.
Cambridge University Press, Cambridge,
UK, pages 223–245.
Copestake, Ann. 2002. Implementing Typed
Feature Structure Grammars. CSLI
Publications, Stanford, CA.
Ginzburg, Jonathan and Ivan A. Sag. 2001.
Interrogative Investigations. CSLI
Publications, Stanford, CA.
Grover, Claire, Chris Brew, Marc Moens,
and Suresh Manandhar. 1994. Priority
union and generalisation in discourse
grammar. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics, pages 17–24, Las Cruces,
New Mexico.
Gurevych, Iryna, Robert Porzel, Elena Slinko,
Norbert Pfleger, Jan Alexandersson,
and Stefan Merten. 2003. Less is more:
Using a single knowledge representation
</reference>
<page confidence="0.995423">
159
</page>
<note confidence="0.708468666666667">
Computational Linguistics Volume 33, Number 2
in dialog systems. In Proceedings of the
HLT-NAACL 2003 Workshop on Text
</note>
<reference confidence="0.992267095238095">
Meaning, pages 14–21, Edmonton, Alberta.
Karp, Richard. 1972. Reducibility among
combinatorial problems. In R. E. Miller
and J. W. Thatcher, editors, Complexity of
Computer Computations. Plenum Press,
New York, pages 85–103.
Lascarides, Alex and Ann Copestake. 1998.
Pragmatics and word meaning. Journal of
Linguistics, 34:387–414.
Lascarides, Alex and Ann Copestake. 1999.
Default representation in constraint-based
frameworks. Computational Linguistics,
25:55–105.
Ninomiya, Takashi, Yusuke Miyao, and
Jun’ichi Tsujii. 2002. Lenient default
unification for robust processing within
unification based grammar formalisms.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING), pages 1–7, Taipei, Taiwan.
Petersen, Wiebke. 2004. A set-theoretic
approach for the induction of inheritance
hierarchies. Electronic Notes in Theoretical
Computer Science, 53:296–308.
Pr¨ust, Hub. 1992. On Discourse Structuring,
VP Anaphora and Gapping. Ph.D. thesis,
University of Amsterdam.
Pr¨ust, Hub, Remko Scha, and Martin
van den Berg. 1994. Discourse grammar
and verb phrase anaphora. Linguistics and
Philosophy, 17(3):261–327.
Tateisi, Yuka, Kentaro Torisawa, Yusuke
Miyao, and Jun’ichi Tsujii.1998.
Translating the XTAG English grammar
to HPSG. In Proceedings of the Fourth
International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+4),
pages 172–175, Philadelphia, PA.
Villavicencio, Aline. 2002. The Acquisition
of a Unification-Based Generalised Categorial
Grammar. Ph.D. thesis, Cambridge
University.
</reference>
<page confidence="0.99744">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.648379">
<title confidence="0.985874">Maximal Consistent Subsets</title>
<affiliation confidence="0.771689">San Diego State University</affiliation>
<abstract confidence="0.9763306">Default unification operations combine strict information with information from one or more defeasible feature structures. Many such operations require finding the maximal subsets of a set of atomic constraints that are consistent with each other and with the strict feature structure, where a subset is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency. Although this problem is NP-complete, there are a number of heuristic optimizations that can be used to substantially reduce the size of search space. In this article, we propose a novel optimization, pruning, in some cases yields an improvement in running time of several orders of magnitude over previously described algorithms. This makes default unification efficient enough to be practical for a wide range of problems and applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>dialogue system.</title>
<booktitle>In Proceedings of the IJCAI Workshop “Knowledge and Reasoning in Practical Dialogue Systems,”</booktitle>
<pages>8--14</pages>
<location>Seattle, WA.</location>
<marker></marker>
<rawString>dialogue system. In Proceedings of the IJCAI Workshop “Knowledge and Reasoning in Practical Dialogue Systems,” pages 8–14, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Alexandersson</author>
<author>Tilman Becker</author>
<author>Norbert Pfleger</author>
</authors>
<title>Scoring for overlay based on informational distance.</title>
<date>2004</date>
<booktitle>In Proceedings of KONVENS 2004,</booktitle>
<pages>1--4</pages>
<location>Vienna, Austria.</location>
<marker>Alexandersson, Becker, Pfleger, 2004</marker>
<rawString>Alexandersson, Jan, Tilman Becker, and Norbert Pfleger. 2004. Scoring for overlay based on informational distance. In Proceedings of KONVENS 2004, pages 1–4, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Bird</author>
<author>Ralf Hinze</author>
</authors>
<title>Functional pearl: Trouble shared is trouble halved.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 ACM SIGPLAN Workshop on Haskell,</booktitle>
<pages>1--6</pages>
<location>Uppsala,</location>
<contexts>
<context position="5678" citStr="Bird and Hinze 2003" startWordPosition="908" endWordPosition="911">remaining constraints would be 154 Malouf Maximal Consistent Subsets mutually consistent with the target information. Finding the maximal consistent subsets of C then is equivalent to finding the minimal subset C&apos; ⊂ C such that each Ti contains at least one member of C&apos;. This is the hitting subset problem, a classic NP-complete problem (Karp 1972). An algorithm to find the maximal consistent subsets of C must check each subset of C for consistency. One way to proceed is to construct a spanning tree of the boolean lattice of subsets of C. This takes the form of a binomial tree, as in Figure 1 (Bird and Hinze 2003). At each node, we keep track of k, the index of the constraint that was removed from the parent set to create that subset. For example, subset {c1, c3} was formed by removing c2 from {c1, c2, c3}, so k = 2. The descendants of a node are constructed by successively dropping each of the constraints ci where k &lt; i ≤ |C|. This ensures that we will visit every subset of C exactly once. The algorithm described by Grover et al. (1994) performs a breadth-first search of the subset lattice, with one important optimization. Because the cardinality of the subsets at each level is one greater than those </context>
</contexts>
<marker>Bird, Hinze, 2003</marker>
<rawString>Bird, Richard and Ralf Hinze. 2003. Functional pearl: Trouble shared is trouble halved. In Proceedings of the 2003 ACM SIGPLAN Workshop on Haskell, pages 1–6, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Boppana</author>
<author>Magn´us M Halld´orsson</author>
</authors>
<title>Approximating maximum independent sets by excluding subgraphs.</title>
<date>1992</date>
<journal>BIT Numerical Mathematics,</journal>
<volume>32</volume>
<issue>2</issue>
<marker>Boppana, Halld´orsson, 1992</marker>
<rawString>Boppana, Ravi and Magn´us M. Halld´orsson. 1992. Approximating maximum independent sets by excluding subgraphs. BIT Numerical Mathematics, 32(2):180–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
</authors>
<title>Feature structures and nonmonotonicity.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="2504" citStr="Bouma (1992)" startWordPosition="358" endWordPosition="359">d Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500 Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu. 1 For example, Bouma (1992), Carpenter (1992), Pr¨ust (1992), Calder (1993), Lascarides and Copestake (1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii (2002). See Bouma (2006) for a recent overview. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 2 In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent subset(s) of the atomic constraints in G. A subset of constraints is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency. In general, there </context>
</contexts>
<marker>Bouma, 1992</marker>
<rawString>Bouma, Gosse. 1992. Feature structures and nonmonotonicity. Computational Linguistics, 18(2):183–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gosse Bouma</author>
</authors>
<title>Unification: Classical and default.</title>
<date>2006</date>
<booktitle>Encyclopedia of Language and Linguistics.</booktitle>
<editor>In Keith Brown, editor,</editor>
<publisher>Elsevier,</publisher>
<location>New York.</location>
<contexts>
<context position="2676" citStr="Bouma (2006)" startWordPosition="381" endWordPosition="382"> default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500 Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu. 1 For example, Bouma (1992), Carpenter (1992), Pr¨ust (1992), Calder (1993), Lascarides and Copestake (1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii (2002). See Bouma (2006) for a recent overview. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 2 In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent subset(s) of the atomic constraints in G. A subset of constraints is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency. In general, there may be more than one subset of the constraints in G that is consistent with F and maximal, so the result of credulous default unification is a set of feature structures. On</context>
</contexts>
<marker>Bouma, 2006</marker>
<rawString>Bouma, Gosse. 2006. Unification: Classical and default. In Keith Brown, editor, Encyclopedia of Language and Linguistics. Elsevier, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
</authors>
<title>The acquisition of grammar in an evolving population of language agents.</title>
<date>1999</date>
<journal>Electronic Transactions in Artificial Intelligence. Special Issue: Machine Intelligence,</journal>
<volume>3</volume>
<issue>035</issue>
<contexts>
<context position="1702" citStr="Briscoe 1999" startWordPosition="241" endWordPosition="242">nt sources. Over the years, a number of default unification operations have been proposed1, which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} </context>
</contexts>
<marker>Briscoe, 1999</marker>
<rawString>Briscoe, E. J. 1999. The acquisition of grammar in an evolving population of language agents. Electronic Transactions in Artificial Intelligence. Special Issue: Machine Intelligence, 3(035):47–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Calder</author>
</authors>
<title>Feature-value logics: Some limits on the role of defaults. In</title>
<date>1993</date>
<journal>C. J.</journal>
<contexts>
<context position="2552" citStr="Calder (1993)" startWordPosition="364" endWordPosition="365">e various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500 Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu. 1 For example, Bouma (1992), Carpenter (1992), Pr¨ust (1992), Calder (1993), Lascarides and Copestake (1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii (2002). See Bouma (2006) for a recent overview. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 2 In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent subset(s) of the atomic constraints in G. A subset of constraints is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency. In general, there may be more than one subset of the constraints i</context>
</contexts>
<marker>Calder, 1993</marker>
<rawString>Calder, Jonathan. 1993. Feature-value logics: Some limits on the role of defaults. In C. J.</rawString>
</citation>
<citation valid="false">
<booktitle>Constraints, Language and Computation.</booktitle>
<pages>20--32</pages>
<editor>Rupp, Mike Rosner, and Rod Johnson, editors,</editor>
<publisher>Academic Press,</publisher>
<location>London,</location>
<marker></marker>
<rawString>Rupp, Mike Rosner, and Rod Johnson, editors, Constraints, Language and Computation. Academic Press, London, pages 20–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Skeptical and creduluous default unification with applications to templates and inheritance.</title>
<date>1992</date>
<booktitle>Default Inheritance within Unification-Based Approaches to the Lexicon.</booktitle>
<pages>13--37</pages>
<editor>In Ted Briscoe, Anne Copestake, and Valerie de Paiva, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<contexts>
<context position="2052" citStr="Carpenter (1992)" startWordPosition="289" endWordPosition="290">unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500 Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu. 1 For example, Bouma (1992), Carpenter (1992), Pr¨ust (1992), Calder (1993), Lascarides and Copestake (1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii </context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Carpenter, Bob. 1992. Skeptical and creduluous default unification with applications to templates and inheritance. In Ted Briscoe, Anne Copestake, and Valerie de Paiva, editors, Default Inheritance within Unification-Based Approaches to the Lexicon. Cambridge University Press, Cambridge, UK, pages 13–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Defaults in lexical representation.</title>
<date>1993</date>
<journal>In E. J. Briscoe,</journal>
<contexts>
<context position="1594" citStr="Copestake 1993" startWordPosition="227" endWordPosition="228">n unification-based grammatical frameworks, it often desirable to combine information from possibly inconsistent sources. Over the years, a number of default unification operations have been proposed1, which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is:</context>
</contexts>
<marker>Copestake, 1993</marker>
<rawString>Copestake, Ann. 1993. Defaults in lexical representation. In E. J. Briscoe,</rawString>
</citation>
<citation valid="false">
<booktitle>Inheritance, Defaults and the Lexicon.</booktitle>
<pages>223--245</pages>
<editor>A. Copestake, and V. de Paiva, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<marker></marker>
<rawString>A. Copestake, and V. de Paiva, editors, Inheritance, Defaults and the Lexicon. Cambridge University Press, Cambridge, UK, pages 223–245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<marker>Copestake, 2002</marker>
<rawString>Copestake, Ann. 2002. Implementing Typed Feature Structure Grammars. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Ginzburg</author>
<author>Ivan A Sag</author>
</authors>
<title>Interrogative Investigations.</title>
<date>2001</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="1618" citStr="Ginzburg and Sag 2001" startWordPosition="229" endWordPosition="232">sed grammatical frameworks, it often desirable to combine information from possibly inconsistent sources. Over the years, a number of default unification operations have been proposed1, which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {uni</context>
<context position="12307" citStr="Ginzburg and Sag (2001)" startWordPosition="2033" endWordPosition="2036">c: at |C |= 19 and p = 0.9, leaf pruning yields a nearly five-fold improvement in speed. Values of |C |and p that can be realistically expected will vary widely from application to application. An anonymous reviewer reports that in one application, the resolution of non-monotonic lexical inheritance for constraint-based grammars, p is generally greater than 0.7. This may be due in part to the fact that most constraintbased grammar development platforms do not support defaults (Copestake’s [2002] LKB is a notable exception), and so grammar engineers tend to avoid the use of default overriding. Ginzburg and Sag (2001) propose a more comprehensive use of defaults, and grammars written following these principles would likely have a much lower value of p. To my knowledge, however, these ideas have not yet made their way into any largescale grammar implementations. Ninomiya, Miyao, and Tsujii (2002) describe experiments using default unification for robust parsing and automatic grammar augmentation via a kind of explanationbased learning. For this application, all features of a rule in the base XHPSG grammar (Tateisi et al. 1998) are considered defaults that can be overridden if necessary to get a successful p</context>
</contexts>
<marker>Ginzburg, Sag, 2001</marker>
<rawString>Ginzburg, Jonathan and Ivan A. Sag. 2001. Interrogative Investigations. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Chris Brew</author>
<author>Marc Moens</author>
<author>Suresh Manandhar</author>
</authors>
<title>Priority union and generalisation in discourse grammar.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>17--24</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="1779" citStr="Grover et al. 1994" startWordPosition="249" endWordPosition="252">have been proposed1, which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego Sta</context>
<context position="3402" citStr="Grover et al. (1994)" startWordPosition="499" endWordPosition="502">33, Number 2 In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent subset(s) of the atomic constraints in G. A subset of constraints is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency. In general, there may be more than one subset of the constraints in G that is consistent with F and maximal, so the result of credulous default unification is a set of feature structures. One example of the use of credulous default unification for the resolution of discourse anaphora comes from Grover et al. (1994). Consider the mini-discourse: Jessy likes her brother. So does Hannah. To resolve the anaphoric predicate in the second sentence, we can set up meaning representations for the source and the target: Source:  REL like  Target: [ AGENT hannah �    AGENT 1 jessy  � REL brother~ PATIENT THEME 1 and credulously unify the source with the strict information in the target. To proceed, we first decompose the source feature structure into the following five atomic ground constraints: {I REL like ] , [ AGENT jessy ] , [ PATIENT|REL brother ] , PATIENT |THEME jessy , AGENT 1 1PATIENT|THEME 1 ] } </context>
<context position="6110" citStr="Grover et al. (1994)" startWordPosition="991" endWordPosition="994">f C for consistency. One way to proceed is to construct a spanning tree of the boolean lattice of subsets of C. This takes the form of a binomial tree, as in Figure 1 (Bird and Hinze 2003). At each node, we keep track of k, the index of the constraint that was removed from the parent set to create that subset. For example, subset {c1, c3} was formed by removing c2 from {c1, c2, c3}, so k = 2. The descendants of a node are constructed by successively dropping each of the constraints ci where k &lt; i ≤ |C|. This ensures that we will visit every subset of C exactly once. The algorithm described by Grover et al. (1994) performs a breadth-first search of the subset lattice, with one important optimization. Because the cardinality of the subsets at each level is one greater than those on the level below it, a breadth-first search of this tree will consider all larger sets before considering any smaller ones. Furthermore, because each subset is produced by removing constraints from its parent set, every node in a subtree is a subset of its root. This means that once a consistent set is found, no descendants of that set can be maximal, and that subtree can be pruned from the search space. However, consistent su</context>
<context position="8087" citStr="Grover et al. (1994)" startWordPosition="1325" endWordPosition="1328"> can be consistent, if the foot of a subtree is inconsistent then clearly no node in the tree can be consistent, and the entire tree can be skipped (call this leaf pruning). Taking both root pruning and leaf pruning together, the only subtrees that need to be explored are those whose root is Figure 1 Boolean lattice and binomial spanning tree for |C |= 3. 155 Computational Linguistics Volume 33, Number 2 inconsistent but whose deepest leaf is consistent. No other subtrees can possibly contain a solution. Figure 2 gives a breadth-first search algorithm that implements these optimizations. Like Grover et al. (1994), this algorithm requires a post-check to remove pseudomaximal subsets from results. A queue is used to keep track of subsets S that are yet to be checked for consistency, along with the index k of the constraint that was last dropped, and a flag leftmost that indicates whether that subset is the leftmost child. Because the deepest leaf node of the leftmost child is the same as the deepest leaf node of the parent, we are guaranteed that the deepest leaf of a leftmost child is consistent. Keeping track of leftmost children allows us to avoid a substantial number of redundant consistency checks.</context>
</contexts>
<marker>Grover, Brew, Moens, Manandhar, 1994</marker>
<rawString>Grover, Claire, Chris Brew, Marc Moens, and Suresh Manandhar. 1994. Priority union and generalisation in discourse grammar. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 17–24, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Robert Porzel</author>
<author>Elena Slinko</author>
<author>Norbert Pfleger</author>
<author>Jan Alexandersson</author>
<author>Stefan Merten</author>
</authors>
<title>Less is more: Using a single knowledge representation Meaning,</title>
<date>2003</date>
<pages>14--21</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="1865" citStr="Gurevych et al. 2003" startWordPosition="263" endWordPosition="266">sible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500 Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mai</context>
</contexts>
<marker>Gurevych, Porzel, Slinko, Pfleger, Alexandersson, Merten, 2003</marker>
<rawString>Gurevych, Iryna, Robert Porzel, Elena Slinko, Norbert Pfleger, Jan Alexandersson, and Stefan Merten. 2003. Less is more: Using a single knowledge representation Meaning, pages 14–21, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Karp</author>
</authors>
<title>Reducibility among combinatorial problems.</title>
<date>1972</date>
<journal>Complexity of Computer Computations.</journal>
<pages>85--103</pages>
<editor>In R. E. Miller and J. W. Thatcher, editors,</editor>
<publisher>Plenum Press,</publisher>
<location>New York,</location>
<contexts>
<context position="5407" citStr="Karp 1972" startWordPosition="858" endWordPosition="859">mple, T consists of the two conflicts: �� PATIENT |THEME jessy , AGENT 1 1PATIENT|THEME 1 ] }, J [AGENT jessy } } Removing any one of the constraints from a conflict Ti would break that conflict, so if we could remove from C at least one member of each Ti, the remaining constraints would be 154 Malouf Maximal Consistent Subsets mutually consistent with the target information. Finding the maximal consistent subsets of C then is equivalent to finding the minimal subset C&apos; ⊂ C such that each Ti contains at least one member of C&apos;. This is the hitting subset problem, a classic NP-complete problem (Karp 1972). An algorithm to find the maximal consistent subsets of C must check each subset of C for consistency. One way to proceed is to construct a spanning tree of the boolean lattice of subsets of C. This takes the form of a binomial tree, as in Figure 1 (Bird and Hinze 2003). At each node, we keep track of k, the index of the constraint that was removed from the parent set to create that subset. For example, subset {c1, c3} was formed by removing c2 from {c1, c2, c3}, so k = 2. The descendants of a node are constructed by successively dropping each of the constraints ci where k &lt; i ≤ |C|. This ens</context>
</contexts>
<marker>Karp, 1972</marker>
<rawString>Karp, Richard. 1972. Reducibility among combinatorial problems. In R. E. Miller and J. W. Thatcher, editors, Complexity of Computer Computations. Plenum Press, New York, pages 85–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Ann Copestake</author>
</authors>
<title>Pragmatics and word meaning.</title>
<date>1998</date>
<journal>Journal of Linguistics,</journal>
<pages>34--387</pages>
<contexts>
<context position="1669" citStr="Lascarides and Copestake 1998" startWordPosition="235" endWordPosition="238">ble to combine information from possibly inconsistent sources. Over the years, a number of default unification operations have been proposed1, which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal su</context>
</contexts>
<marker>Lascarides, Copestake, 1998</marker>
<rawString>Lascarides, Alex and Ann Copestake. 1998. Pragmatics and word meaning. Journal of Linguistics, 34:387–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lascarides</author>
<author>Ann Copestake</author>
</authors>
<title>Default representation in constraint-based frameworks.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<pages>25--55</pages>
<contexts>
<context position="2585" citStr="Lascarides and Copestake (1999)" startWordPosition="366" endWordPosition="369">lt unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asian/Middle Eastern Languages, San Diego State University, 5500 Campanile Drive, San Diego, CA 92182-7727 USA, E-mail: rmalouf@mail.sdsu.edu. 1 For example, Bouma (1992), Carpenter (1992), Pr¨ust (1992), Calder (1993), Lascarides and Copestake (1999), Alexandersson and Becker (2001), and Ninomiya, Miyao, and Tsujii (2002). See Bouma (2006) for a recent overview. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 2 In other words, cred-unify(F, G) is the result of unifying F with the maximal consistent subset(s) of the atomic constraints in G. A subset of constraints is maximally consistent with respect to the subsumption ordering if no constraint can be added to it without creating an inconsistency. In general, there may be more than one subset of the constraints in G that is consistent with F and</context>
</contexts>
<marker>Lascarides, Copestake, 1999</marker>
<rawString>Lascarides, Alex and Ann Copestake. 1999. Default representation in constraint-based frameworks. Computational Linguistics, 25:55–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Lenient default unification for robust processing within unification based grammar formalisms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan.</location>
<marker>Ninomiya, Miyao, Tsujii, 2002</marker>
<rawString>Ninomiya, Takashi, Yusuke Miyao, and Jun’ichi Tsujii. 2002. Lenient default unification for robust processing within unification based grammar formalisms. In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 1–7, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wiebke Petersen</author>
</authors>
<title>A set-theoretic approach for the induction of inheritance hierarchies.</title>
<date>2004</date>
<booktitle>Electronic Notes in Theoretical Computer Science,</booktitle>
<pages>53--296</pages>
<contexts>
<context position="1738" citStr="Petersen 2004" startWordPosition="245" endWordPosition="246">er of default unification operations have been proposed1, which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Linguistics and Asia</context>
</contexts>
<marker>Petersen, 2004</marker>
<rawString>Petersen, Wiebke. 2004. A set-theoretic approach for the induction of inheritance hierarchies. Electronic Notes in Theoretical Computer Science, 53:296–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hub Pr¨ust</author>
</authors>
<date>1992</date>
<booktitle>On Discourse Structuring, VP Anaphora and Gapping. Ph.D. thesis,</booktitle>
<institution>University of Amsterdam.</institution>
<marker>Pr¨ust, 1992</marker>
<rawString>Pr¨ust, Hub. 1992. On Discourse Structuring, VP Anaphora and Gapping. Ph.D. thesis, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hub Pr¨ust</author>
<author>Remko Scha</author>
<author>Martin van den Berg</author>
</authors>
<title>Discourse grammar and verb phrase anaphora.</title>
<date>1994</date>
<journal>Linguistics and Philosophy,</journal>
<volume>17</volume>
<issue>3</issue>
<marker>Pr¨ust, Scha, van den Berg, 1994</marker>
<rawString>Pr¨ust, Hub, Remko Scha, and Martin van den Berg. 1994. Discourse grammar and verb phrase anaphora. Linguistics and Philosophy, 17(3):261–327.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yuka Tateisi</author>
</authors>
<title>Kentaro Torisawa, Yusuke Miyao, and Jun’ichi Tsujii.1998. Translating the XTAG English grammar to HPSG.</title>
<booktitle>In Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+4),</booktitle>
<pages>172--175</pages>
<location>Philadelphia, PA.</location>
<marker>Tateisi, </marker>
<rawString>Tateisi, Yuka, Kentaro Torisawa, Yusuke Miyao, and Jun’ichi Tsujii.1998. Translating the XTAG English grammar to HPSG. In Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks (TAG+4), pages 172–175, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>The Acquisition of a Unification-Based Generalised Categorial Grammar.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Cambridge University.</institution>
<contexts>
<context position="1722" citStr="Villavicencio 2002" startWordPosition="243" endWordPosition="244">er the years, a number of default unification operations have been proposed1, which combine a strict feature structure with one or more defeasible feature structures. These operations preserve all information in the strict feature structure, while bringing in as much information as possible from the defeasible structures. Default unification has been used to address a wide variety of linguistic knowledge representation problems, including lexical inheritance hierarchies (Copestake 1993; Ginzburg and Sag 2001), lexical semantics (Lascarides and Copestake 1998), grammar induction (Briscoe 1999; Villavicencio 2002; Petersen 2004), anaphora resolution (Grover et al. 1994; Pr¨ust, Scha, and van den Berg 1994), and discourse processing (Gurevych et al. 2003; Alexandersson, Becker, and Pfleger 2004), among many others. Although the various default unification operators differ in their particulars, most involve something like Carpenter (1992)’s credulous default unification as one step. The result of credulously adding the default information in G to the strict information in F is: cred-unify(F, G) ≡ {unify(F, G&apos;), where G&apos; subsumes G and G&apos; is maximal such that unify(F, G&apos;) is defined} ∗ Department of Ling</context>
</contexts>
<marker>Villavicencio, 2002</marker>
<rawString>Villavicencio, Aline. 2002. The Acquisition of a Unification-Based Generalised Categorial Grammar. Ph.D. thesis, Cambridge University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>