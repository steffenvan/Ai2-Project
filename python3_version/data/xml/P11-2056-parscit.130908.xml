<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002483">
<title confidence="0.981724">
Models and Training for Unsupervised Preposition Sense Disambiguation
</title>
<author confidence="0.76607">
Dirk Hovy and Ashish Vaswani and Stephen Tratz and
David Chiang and Eduard Hovy
</author>
<affiliation confidence="0.938245333333333">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
</affiliation>
<email confidence="0.99775">
{dirkh,avaswani,stratz,chiang,hovy}@isi.edu
</email>
<sectionHeader confidence="0.984863" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997076">
We present a preliminary study on unsu-
pervised preposition sense disambiguation
(PSD), comparing different models and train-
ing techniques (EM, MAP-EM with Lo norm,
Bayesian inference using Gibbs sampling). To
our knowledge, this is the first attempt at un-
supervised preposition sense disambiguation.
Our best accuracy reaches 56%, a significant
improvement (at p &lt;.001) of 16% over the
most-frequent-sense baseline.
</bodyText>
<sectionHeader confidence="0.992094" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999588333333334">
Reliable disambiguation of words plays an impor-
tant role in many NLP applications. Prepositions
are ubiquitous—they account for more than 10% of
the 1.16m words in the Brown corpus—and highly
ambiguous. The Preposition Project (Litkowski and
Hargraves, 2005) lists an average of 9.76 senses
for each of the 34 most frequent English preposi-
tions, while nouns usually have around two (Word-
Net nouns average about 1.2 senses, 2.7 if monose-
mous nouns are excluded (Fellbaum, 1998)). Dis-
ambiguating prepositions is thus a challenging and
interesting task in itself (as exemplified by the Sem-
Eval 2007 task, (Litkowski and Hargraves, 2007)),
and holds promise for NLP applications such as
Information Extraction or Machine Translation.1
Given a sentence such as the following:
In the morning, he shopped in Rome
we ultimately want to be able to annotate it as
</bodyText>
<note confidence="0.90621025">
1See (Chan et al., 2007) for how using WSD can help MT.
in/TEMPORAL the morning/TIME he/PERSON
shopped/SOCIAL in/LOCATIVE
Rome/LOCATION
</note>
<bodyText confidence="0.998734035714286">
Here, the preposition in has two distinct meanings,
namely a temporal and a locative one. These mean-
ings are context-dependent. Ultimately, we want
to disambiguate prepositions not by and for them-
selves, but in the context of sequential semantic la-
beling. This should also improve disambiguation of
the words linked by the prepositions (here, morn-
ing, shopped, and Rome). We propose using un-
supervised methods in order to leverage unlabeled
data, since, to our knowledge, there are no annotated
data sets that include both preposition and argument
senses. In this paper, we present our unsupervised
framework and show results for preposition disam-
biguation. We hope to present results for the joint
disambiguation of preposition and arguments in a
future paper.
The results from this work can be incorporated
into a number of NLP problems, such as seman-
tic tagging, which tries to assign not only syntac-
tic, but also semantic categories to unlabeled text.
Knowledge about semantic constraints of preposi-
tional constructions would not only provide better
label accuracy, but also aid in resolving preposi-
tional attachment problems. Learning by Reading
approaches (Mulkar-Mehta et al., 2010) also cru-
cially depend on unsupervised techniques as the
ones described here for textual enrichment.
Our contributions are:
</bodyText>
<listItem confidence="0.5956325">
• we present the first unsupervised preposition
sense disambiguation (PSD) system
</listItem>
<page confidence="0.708176">
323
</page>
<note confidence="0.813297">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 323–328,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<listItem confidence="0.837315">
• we compare the effectiveness of various models
and unsupervised training methods
• we present ways to extend this work to prepo-
sitional arguments
</listItem>
<sectionHeader confidence="0.858932" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.9949905">
A preposition p acts as a link between two words, h
and o. The head word h (a noun, adjective, or verb)
governs the preposition. In our example above, the
head word is shopped. The object of the preposi-
tional phrase (usually a noun) is denoted o, in our
example morning and Rome. We will refer to h and
o collectively as the prepositional arguments. The
triple h, p, o forms a syntactically and semantically
constrained structure. This structure is reflected in
dependency parses as a common construction. In
our example sentence above, the respective struc-
tures would be shopped in morning and shopped in
Rome. The senses of each element are denoted by a
barred letter, i.e., p� denotes the preposition sense, h
denotes the sense of the head word, and o� the sense
of the object.
</bodyText>
<sectionHeader confidence="0.993386" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.99998452">
We use the data set for the SemEval 2007 PSD
task, which consists of a training (16k) and a test
set (8k) of sentences with sense-annotated preposi-
tions following the sense inventory of The Preposi-
tion Project, TPP (Litkowski and Hargraves, 2005).
It defines senses for each of the 34 most frequent
prepositions. There are on average 9.76 senses per
preposition. This corpus was chosen as a starting
point for our study since it allows a comparison with
the original SemEval task. We plan to use larger
amounts of additional training data.
We used an in-house dependency parser to extract
the prepositional constructions from the data (e.g.,
“shop/VB in/IN Rome/NNP”). Pronouns and num-
bers are collapsed into ”PRO” and ”NUM”, respec-
tively.
In order to constrain the argument senses, we con-
struct a dictionary that lists for each word all the
possible lexicographer senses according to Word-
Net. The set of lexicographer senses (45) is a higher
level abstraction which is sufficiently coarse to allow
for a good generalization. Unknown words are as-
sumed to have all possible senses applicable to their
respective word class (i.e. all noun senses for words
labeled as nouns, etc).
</bodyText>
<sectionHeader confidence="0.971726" genericHeader="method">
4 Graphical Model
</sectionHeader>
<bodyText confidence="0.936202722222222">
Figure 1: Graphical Models. a) 1st order HMM. b)
variant used in experiments (one model/preposition,
thus no conditioning on p). c) incorporates further
constraints on variables
As shown by Hovy et al. (2010), preposition
senses can be accurately disambiguated using only
the head word and object of the PP. We exploit this
property of prepositional constructions to represent
the constraints between h, p, and o in a graphical
model. We define a good model as one that reason-
ably constrains the choices, but is still tractable in
terms of the number of parameters being estimated.
As a starting point, we choose the standard first-
order Hidden Markov Model as depicted in Figure
1a. Since we train a separate model for each preposi-
tion, we can omit all arcs to p. This results in model
1b. The joint distribution over the network can thus
be written as
</bodyText>
<equation confidence="0.9950485">
Pp(h, o, h, P, o) = P( h) · P(h |h) · (1)
P(p |h) · P(�o|p) · P(o|�o)
</equation>
<bodyText confidence="0.9980785">
We want to incorporate as much information as
possible into the model to constrain the choices. In
Figure 1c, we condition p� on both h and 6, to reflect
the fact that prepositions act as links and determine
</bodyText>
<figure confidence="0.981081">
a)
�
�
�
h
h p o
h
h o
h
h o
�
�
�
p
p
p
�
�
�
o
o
o
324
</figure>
<bodyText confidence="0.9990694">
their sense mainly through context. In order to con-
strain the object sense o, we condition on h, similar
to a second-order HMM. The actual object o is con-
ditioned on both p� and o. The joint distribution is
equal to
</bodyText>
<equation confidence="0.993064">
Pp(h, o, h, p, o) = P( h) · P(h |h) · (2)
P(6|�h) · P(P|�h, o) · P(o|�o, p)
</equation>
<bodyText confidence="0.9999048">
Though we would like to also condition the prepo-
sition sense p� on the head word h (i.e., an arc be-
tween them in 1c) in order to capture idioms and
fixed phrases, this would increase the number of pa-
rameters prohibitively.
</bodyText>
<sectionHeader confidence="0.996482" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.9999728">
The training method largely determines how well the
resulting model explains the data. Ideally, the sense
distribution found by the model matches the real
one. Since most linguistic distributions are Zipfian,
we want a training method that encourages sparsity
in the model.
We briefly introduce different unsupervised train-
ing methods and discuss their respective advantages
and disadvantages. Unless specified otherwise, we
initialized all models uniformly, and trained until the
perplexity rate stopped increasing or a predefined
number of iterations was reached. Note that MAP-
EM and Bayesian Inference require tuning of some
hyper-parameters on held-out data, and are thus not
fully unsupervised.
</bodyText>
<subsectionHeader confidence="0.897599">
5.1 EM
</subsectionHeader>
<bodyText confidence="0.999972888888889">
We use the EM algorithm (Dempster et al., 1977) as
a baseline. It is relatively easy to implement with ex-
isting toolkits like Carmel (Graehl, 1997). However,
EM has a tendency to assume equal importance for
each parameter. It thus prefers “general” solutions,
assigning part of the probability mass to unlikely
states (Johnson, 2007). We ran EM on each model
for 100 iterations, or until the perplexity stopped de-
creasing below a threshold of 10−6.
</bodyText>
<subsectionHeader confidence="0.996564">
5.2 EM with Smoothing and Restarts
</subsectionHeader>
<bodyText confidence="0.999907428571429">
In addition to the baseline, we ran 100 restarts with
random initialization and smoothed the fractional
counts by adding 0.1 before normalizing (Eisner,
2002). Smoothing helps to prevent overfitting. Re-
peated random restarts help escape unfavorable ini-
tializations that lead to local maxima. Carmel pro-
vides options for both smoothing and restarts.
</bodyText>
<subsectionHeader confidence="0.835876">
5.3 MAP-EM with L0 Norm
</subsectionHeader>
<bodyText confidence="0.999996421052632">
Since we want to encourage sparsity in our mod-
els, we use the MDL-inspired technique intro-
duced by Vaswani et al. (2010). Here, the goal
is to increase the data likelihood while keeping
the number of parameters small. The authors use
a smoothed L0 prior, which encourages probabil-
ities to go down to 0. The prior involves hyper-
parameters α, which rewards sparsity, and β, which
controls how close the approximation is to the true
L0 norm.2 We perform a grid search to tune the
hyper-parameters of the smoothed L0 prior for ac-
curacy on the preposition against, since it has a
medium number of senses and instances. For HMM,
we set αtrans =100.0, βtrans =0.005, αemit =1.0,
βemit =0.75. The subscripts trans and emit de-
note the transition and emission parameters. For
our model, we set αtrans =70.0, βtrans =0.05,
αemit =110.0, βemit =0.0025. The latter resulted
in the best accuracy we achieved.
</bodyText>
<subsectionHeader confidence="0.990517">
5.4 Bayesian Inference
</subsectionHeader>
<bodyText confidence="0.999968142857143">
Instead of EM, we can use Bayesian inference with
Gibbs sampling and Dirichlet priors (also known as
the Chinese Restaurant Process, CRP). We follow
the approach of Chiang et al. (2010), running Gibbs
sampling for 10,000 iterations, with a burn-in pe-
riod of 5,000, and carry out automatic run selec-
tion over 10 random restarts.3 Again, we tuned the
hyper-parameters of our Dirichlet priors for accu-
racy via a grid search over the model for the prepo-
sition against. For both models, we set the concen-
tration parameter αtrans to 0.001, and αemit to 0.1.
This encourages sparsity in the model and allows for
a more nuanced explanation of the data by shifting
probability mass to the few prominent classes.
</bodyText>
<footnote confidence="0.89082825">
2For more details, the reader is referred to Vaswani et al.
(2010).
3Due to time and space constraints, we did not run the 1000
restarts used in Chiang et al. (2010).
</footnote>
<page confidence="0.556809">
325
</page>
<table confidence="0.9979526">
baseline Vanilla EM EM, smoothed, MAP-EM + CRP, 10 random
100 random smoothed L0 restarts
restarts norm
HMM 0.40 (0.40) 0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53)
our model 0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49)
</table>
<tableCaption confidence="0.961519">
Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAP-
EM+smoothed Lo norm on our model. Italics denote significant improvement over baseline at p &lt;.001.
Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters)
</tableCaption>
<sectionHeader confidence="0.999321" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999694020833333">
Given a sequence h, p, o, we want to find the se-
quence of senses h, p, o� that maximizes the joint
probability. Since unsupervised methods use the
provided labels indiscriminately, we have to map the
resulting predictions to the gold labels. The pre-
dicted label sequence h, p, o� generated by the model
via Viterbi decoding can then be compared to the
true key. We use many-to-1 mapping as described
by Johnson (2007) and used in other unsupervised
tasks (Berg-Kirkpatrick et al., 2010), where each
predicted sense is mapped to the gold label it most
frequently occurs with in the test data. Success is
measured by the percentage of accurate predictions.
Here, we only evaluate P.
The results presented in Table 1 were obtained
on the SemEval test set. We report results both
with and without against, since we tuned the hyper-
parameters of two training methods on this preposi-
tion. To test for significance, we use a two-tailed
t-test, comparing the number of correctly labeled
prepositions. As a baseline, we simply label all word
types with the same sense, i.e., each preposition to-
ken is labeled with its respective name. When using
many-to-1 accuracy, this technique is equivalent to a
most-frequent-sense baseline.
Vanilla EM does not improve significantly over
the baseline with either model, all other methods
do. Adding smoothing and random restarts increases
the gain considerably, illustrating how important
these techniques are for unsupervised training. We
note that EM performs better with the less complex
HMM.
CRP is somewhat surprisingly roughly equivalent
to EM with smoothing and random restarts. Accu-
racy might improve with more restarts.
MAP-EM with Lo normalization produces the
best result (56%), significantly outperforming the
baseline at p &lt; .001. With more parameters (9.7k
vs. 3.7k), which allow for a better modeling of
the data, Lo normalization helps by zeroing out in-
frequent ones. However, the difference between
our complex model and the best HMM (EM with
smoothing and random restarts, 55%) is not signifi-
cant.
The best (supervised) system in the SemEval task
(Ye and Baldwin, 2007) reached 69% accuracy. The
best current supervised system we are aware of
(Hovy et al., 2010) reaches 84.8%.
</bodyText>
<sectionHeader confidence="0.999095" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999978947368421">
The semantics of prepositions were topic of a special
issue of Computational Linguistics (Baldwin et al.,
2009). Preposition sense disambiguation was one of
the SemEval 2007 tasks (Litkowski and Hargraves,
2007), and was subsequently explored in a number
of papers using supervised approaches: O’Hara and
Wiebe (2009) present a supervised preposition sense
disambiguation approach which explores different
settings; Tratz and Hovy (2009), Hovy et al. (2010)
make explicit use of the arguments for preposition
sense disambiguation, using various features. We
differ from these approaches by using unsupervised
methods and including argument labeling.
The constraints of prepositional constructions
have been explored by Rudzicz and Mokhov (2003)
and O’Hara and Wiebe (2003) to annotate the se-
mantic role of complete PPs with FrameNet and
Penn Treebank categories. Ye and Baldwin (2006)
explore the constraints of prepositional phrases for
</bodyText>
<page confidence="0.856784">
326
</page>
<bodyText confidence="0.9729845">
semantic role labeling. We plan to use the con-
straints for argument disambiguation.
</bodyText>
<sectionHeader confidence="0.810766" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999985">
We evaluate the influence of two different models (to
represent constraints) and three unsupervised train-
ing methods (to achieve sparse sense distributions)
on PSD. Using MAP-EM with Lo norm on our
model, we achieve an accuracy of 56%. This is a
significant improvement (at p &lt;.001) over the base-
line and vanilla EM. We hope to shorten the gap to
supervised systems with more unlabeled data. We
also plan on training our models with EM with fea-
tures (Berg-Kirkpatrick et al., 2010).
The advantage of our approach is that the models
can be used to infer the senses of the prepositional
arguments as well as the preposition. We are cur-
rently annotating the data to produce a test set with
Amazon’s Mechanical Turk, in order to measure la-
bel accuracy for the preposition arguments.
</bodyText>
<sectionHeader confidence="0.990993" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999669666666667">
We would like to thank Steve DeNeefe, Jonathan
Graehl, Victoria Fossum, and Kevin Knight, as well
as the anonymous reviewers for helpful comments
on how to improve the paper. We would also like
to thank Morgan from Curious Palate for letting us
write there. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program and by DARPA under con-
tract DOI-NBC N10AP20031.
</bodyText>
<sectionHeader confidence="0.983215" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999760823529412">
Tim Baldwin, Valia Kordoni, and Aline Villavicencio.
2009. Prepositions in applications: A survey and in-
troduction to the special issue. Computational Lin-
guistics, 35(2):119–149.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless Unsu-
pervised Learning with Features. In North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Annual Meeting – Association
For Computational Linguistics, volume 45, pages 33–
40.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for Finite-State transducers. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 447–455. Association for
Computational Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1–38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10–
18. Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Jonathan Graehl. 1997. Carmel Finite-state Toolkit.
ISI/USC.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What’s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454–462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Mark Johnson. 2007. Why doesn’t EM find good HMM
POS-taggers. In Proceedings of the 2007Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296–305.
Ken Litkowski and Orin Hargraves. 2005. The prepo-
sition project. ACL-SIGSEM Workshop on “The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions”, pages 171–179.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Tom O’Hara and Janyce Wiebe. 2003. Preposi-
tion semantic classification via Penn Treebank and
FrameNet. In Proceedings of CoNLL, pages 79–86.
Tom O’Hara and Janyce Wiebe. 2009. Exploiting se-
mantic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151–184.
</reference>
<page confidence="0.53964">
327
</page>
<reference confidence="0.999531103448276">
Frank Rudzicz and Serguei A. Mokhov. 2003. Towards
a heuristic categorization of prepositional phrases in
english with wordnet. Technical report, Cornell
University, arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96–100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209–214. Association for Computational
Linguistics.
Patrick Ye and Tim Baldwin. 2006. Semantic role la-
beling of prepositional phrases. ACM Transactions
on Asian Language Information Processing (TALIP),
5(3):228–244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
</reference>
<page confidence="0.9351">
328
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.516224">
<title confidence="0.999992">Models and Training for Unsupervised Preposition Sense Disambiguation</title>
<author confidence="0.740793">Hovy Vaswani Tratz Chiang</author>
<affiliation confidence="0.9980825">Information Sciences University of Southern</affiliation>
<address confidence="0.997917">4676 Admiralty Way, Marina del Rey, CA</address>
<abstract confidence="0.998657363636363">We present a preliminary study on unsupervised preposition sense disambiguation (PSD), comparing different models and traintechniques (EM, MAP-EM with Bayesian inference using Gibbs sampling). To our knowledge, this is the first attempt at unsupervised preposition sense disambiguation. Our best accuracy reaches 56%, a significant (at of 16% over the most-frequent-sense baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tim Baldwin</author>
<author>Valia Kordoni</author>
<author>Aline Villavicencio</author>
</authors>
<title>Prepositions in applications: A survey and introduction to the special issue.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>2</issue>
<contexts>
<context position="13428" citStr="Baldwin et al., 2009" startWordPosition="2207" endWordPosition="2210">ly outperforming the baseline at p &lt; .001. With more parameters (9.7k vs. 3.7k), which allow for a better modeling of the data, Lo normalization helps by zeroing out infrequent ones. However, the difference between our complex model and the best HMM (EM with smoothing and random restarts, 55%) is not significant. The best (supervised) system in the SemEval task (Ye and Baldwin, 2007) reached 69% accuracy. The best current supervised system we are aware of (Hovy et al., 2010) reaches 84.8%. 7 Related Work The semantics of prepositions were topic of a special issue of Computational Linguistics (Baldwin et al., 2009). Preposition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from these approaches by using unsupervised methods and including argument labeling. The constraints of prepositional constructions have been expl</context>
</contexts>
<marker>Baldwin, Kordoni, Villavicencio, 2009</marker>
<rawString>Tim Baldwin, Valia Kordoni, and Aline Villavicencio. 2009. Prepositions in applications: A survey and introduction to the special issue. Computational Linguistics, 35(2):119–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless Unsupervised Learning with Features.</title>
<date>2010</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics.</booktitle>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless Unsupervised Learning with Features. In North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting – Association For Computational Linguistics,</booktitle>
<volume>45</volume>
<pages>33--40</pages>
<contexts>
<context position="1622" citStr="Chan et al., 2007" startWordPosition="239" endWordPosition="242">ists an average of 9.76 senses for each of the 34 most frequent English prepositions, while nouns usually have around two (WordNet nouns average about 1.2 senses, 2.7 if monosemous nouns are excluded (Fellbaum, 1998)). Disambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the SemEval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation.1 Given a sentence such as the following: In the morning, he shopped in Rome we ultimately want to be able to annotate it as 1See (Chan et al., 2007) for how using WSD can help MT. in/TEMPORAL the morning/TIME he/PERSON shopped/SOCIAL in/LOCATIVE Rome/LOCATION Here, the preposition in has two distinct meanings, namely a temporal and a locative one. These meanings are context-dependent. Ultimately, we want to disambiguate prepositions not by and for themselves, but in the context of sequential semantic labeling. This should also improve disambiguation of the words linked by the prepositions (here, morning, shopped, and Rome). We propose using unsupervised methods in order to leverage unlabeled data, since, to our knowledge, there are no ann</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Annual Meeting – Association For Computational Linguistics, volume 45, pages 33– 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Adam Pauls</author>
<author>Sujith Ravi</author>
</authors>
<title>Bayesian inference for Finite-State transducers. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>447--455</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9863" citStr="Chiang et al. (2010)" startWordPosition="1622" endWordPosition="1625">parameters of the smoothed L0 prior for accuracy on the preposition against, since it has a medium number of senses and instances. For HMM, we set αtrans =100.0, βtrans =0.005, αemit =1.0, βemit =0.75. The subscripts trans and emit denote the transition and emission parameters. For our model, we set αtrans =70.0, βtrans =0.05, αemit =110.0, βemit =0.0025. The latter resulted in the best accuracy we achieved. 5.4 Bayesian Inference Instead of EM, we can use Bayesian inference with Gibbs sampling and Dirichlet priors (also known as the Chinese Restaurant Process, CRP). We follow the approach of Chiang et al. (2010), running Gibbs sampling for 10,000 iterations, with a burn-in period of 5,000, and carry out automatic run selection over 10 random restarts.3 Again, we tuned the hyper-parameters of our Dirichlet priors for accuracy via a grid search over the model for the preposition against. For both models, we set the concentration parameter αtrans to 0.001, and αemit to 0.1. This encourages sparsity in the model and allows for a more nuanced explanation of the data by shifting probability mass to the few prominent classes. 2For more details, the reader is referred to Vaswani et al. (2010). 3Due to time a</context>
</contexts>
<marker>Chiang, Graehl, Knight, Pauls, Ravi, 2010</marker>
<rawString>David Chiang, Jonathan Graehl, Kevin Knight, Adam Pauls, and Sujith Ravi. 2010. Bayesian inference for Finite-State transducers. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 447–455. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="7950" citStr="Dempster et al., 1977" startWordPosition="1305" endWordPosition="1308">model matches the real one. Since most linguistic distributions are Zipfian, we want a training method that encourages sparsity in the model. We briefly introduce different unsupervised training methods and discuss their respective advantages and disadvantages. Unless specified otherwise, we initialized all models uniformly, and trained until the perplexity rate stopped increasing or a predefined number of iterations was reached. Note that MAPEM and Bayesian Inference require tuning of some hyper-parameters on held-out data, and are thus not fully unsupervised. 5.1 EM We use the EM algorithm (Dempster et al., 1977) as a baseline. It is relatively easy to implement with existing toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers “general” solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6. 5.2 EM with Smoothing and Restarts In addition to the baseline, we ran 100 restarts with random initialization and smoothed the fractional counts by adding 0.1 before normalizing (Eisner, 2002). Smo</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>An interactive spreadsheet for teaching the forward-backward algorithm.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Effective tools</booktitle>
<contexts>
<context position="8545" citStr="Eisner, 2002" startWordPosition="1403" endWordPosition="1404">ster et al., 1977) as a baseline. It is relatively easy to implement with existing toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers “general” solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6. 5.2 EM with Smoothing and Restarts In addition to the baseline, we ran 100 restarts with random initialization and smoothed the fractional counts by adding 0.1 before normalizing (Eisner, 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initializations that lead to local maxima. Carmel provides options for both smoothing and restarts. 5.3 MAP-EM with L0 Norm Since we want to encourage sparsity in our models, we use the MDL-inspired technique introduced by Vaswani et al. (2010). Here, the goal is to increase the data likelihood while keeping the number of parameters small. The authors use a smoothed L0 prior, which encourages probabilities to go down to 0. The prior involves hyperparameters α, which rewards sparsity, and β, which control</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. An interactive spreadsheet for teaching the forward-backward algorithm. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics-Volume 1, pages 10– 18. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press USA.</publisher>
<contexts>
<context position="1220" citStr="Fellbaum, 1998" startWordPosition="175" endWordPosition="176">uation. Our best accuracy reaches 56%, a significant improvement (at p &lt;.001) of 16% over the most-frequent-sense baseline. 1 Introduction Reliable disambiguation of words plays an important role in many NLP applications. Prepositions are ubiquitous—they account for more than 10% of the 1.16m words in the Brown corpus—and highly ambiguous. The Preposition Project (Litkowski and Hargraves, 2005) lists an average of 9.76 senses for each of the 34 most frequent English prepositions, while nouns usually have around two (WordNet nouns average about 1.2 senses, 2.7 if monosemous nouns are excluded (Fellbaum, 1998)). Disambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the SemEval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation.1 Given a sentence such as the following: In the morning, he shopped in Rome we ultimately want to be able to annotate it as 1See (Chan et al., 2007) for how using WSD can help MT. in/TEMPORAL the morning/TIME he/PERSON shopped/SOCIAL in/LOCATIVE Rome/LOCATION Here, the preposition in has two distinct meanings, namely a temporal and a locative o</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
</authors>
<title>Carmel Finite-state Toolkit.</title>
<date>1997</date>
<publisher>ISI/USC.</publisher>
<contexts>
<context position="8050" citStr="Graehl, 1997" startWordPosition="1324" endWordPosition="1325">ncourages sparsity in the model. We briefly introduce different unsupervised training methods and discuss their respective advantages and disadvantages. Unless specified otherwise, we initialized all models uniformly, and trained until the perplexity rate stopped increasing or a predefined number of iterations was reached. Note that MAPEM and Bayesian Inference require tuning of some hyper-parameters on held-out data, and are thus not fully unsupervised. 5.1 EM We use the EM algorithm (Dempster et al., 1977) as a baseline. It is relatively easy to implement with existing toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers “general” solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6. 5.2 EM with Smoothing and Restarts In addition to the baseline, we ran 100 restarts with random initialization and smoothed the fractional counts by adding 0.1 before normalizing (Eisner, 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initialization</context>
</contexts>
<marker>Graehl, 1997</marker>
<rawString>Jonathan Graehl. 1997. Carmel Finite-state Toolkit. ISI/USC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>What’s in a Preposition? Dimensions of Sense Disambiguation for an Interesting Word Class.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>454--462</pages>
<location>Beijing, China,</location>
<contexts>
<context position="5690" citStr="Hovy et al. (2010)" startWordPosition="890" endWordPosition="893">construct a dictionary that lists for each word all the possible lexicographer senses according to WordNet. The set of lexicographer senses (45) is a higher level abstraction which is sufficiently coarse to allow for a good generalization. Unknown words are assumed to have all possible senses applicable to their respective word class (i.e. all noun senses for words labeled as nouns, etc). 4 Graphical Model Figure 1: Graphical Models. a) 1st order HMM. b) variant used in experiments (one model/preposition, thus no conditioning on p). c) incorporates further constraints on variables As shown by Hovy et al. (2010), preposition senses can be accurately disambiguated using only the head word and object of the PP. We exploit this property of prepositional constructions to represent the constraints between h, p, and o in a graphical model. We define a good model as one that reasonably constrains the choices, but is still tractable in terms of the number of parameters being estimated. As a starting point, we choose the standard firstorder Hidden Markov Model as depicted in Figure 1a. Since we train a separate model for each preposition, we can omit all arcs to p. This results in model 1b. The joint distribu</context>
<context position="13286" citStr="Hovy et al., 2010" startWordPosition="2185" endWordPosition="2188">ng and random restarts. Accuracy might improve with more restarts. MAP-EM with Lo normalization produces the best result (56%), significantly outperforming the baseline at p &lt; .001. With more parameters (9.7k vs. 3.7k), which allow for a better modeling of the data, Lo normalization helps by zeroing out infrequent ones. However, the difference between our complex model and the best HMM (EM with smoothing and random restarts, 55%) is not significant. The best (supervised) system in the SemEval task (Ye and Baldwin, 2007) reached 69% accuracy. The best current supervised system we are aware of (Hovy et al., 2010) reaches 84.8%. 7 Related Work The semantics of prepositions were topic of a special issue of Computational Linguistics (Baldwin et al., 2009). Preposition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from</context>
</contexts>
<marker>Hovy, Tratz, Hovy, 2010</marker>
<rawString>Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010. What’s in a Preposition? Dimensions of Sense Disambiguation for an Interesting Word Class. In Coling 2010: Posters, pages 454–462, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Why doesn’t EM find good HMM POS-taggers.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>296--305</pages>
<contexts>
<context position="8236" citStr="Johnson, 2007" startWordPosition="1352" endWordPosition="1353">nitialized all models uniformly, and trained until the perplexity rate stopped increasing or a predefined number of iterations was reached. Note that MAPEM and Bayesian Inference require tuning of some hyper-parameters on held-out data, and are thus not fully unsupervised. 5.1 EM We use the EM algorithm (Dempster et al., 1977) as a baseline. It is relatively easy to implement with existing toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers “general” solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6. 5.2 EM with Smoothing and Restarts In addition to the baseline, we ran 100 restarts with random initialization and smoothed the fractional counts by adding 0.1 before normalizing (Eisner, 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initializations that lead to local maxima. Carmel provides options for both smoothing and restarts. 5.3 MAP-EM with L0 Norm Since we want to encourage sparsity in our models, we use the MDL-inspired t</context>
<context position="11494" citStr="Johnson (2007)" startWordPosition="1901" endWordPosition="1902">d Lo norm on our model. Italics denote significant improvement over baseline at p &lt;.001. Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters) 6 Results Given a sequence h, p, o, we want to find the sequence of senses h, p, o� that maximizes the joint probability. Since unsupervised methods use the provided labels indiscriminately, we have to map the resulting predictions to the gold labels. The predicted label sequence h, p, o� generated by the model via Viterbi decoding can then be compared to the true key. We use many-to-1 mapping as described by Johnson (2007) and used in other unsupervised tasks (Berg-Kirkpatrick et al., 2010), where each predicted sense is mapped to the gold label it most frequently occurs with in the test data. Success is measured by the percentage of accurate predictions. Here, we only evaluate P. The results presented in Table 1 were obtained on the SemEval test set. We report results both with and without against, since we tuned the hyperparameters of two training methods on this preposition. To test for significance, we use a two-tailed t-test, comparing the number of correctly labeled prepositions. As a baseline, we simply </context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Why doesn’t EM find good HMM POS-taggers. In Proceedings of the 2007Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>The preposition project.</title>
<date>2005</date>
<booktitle>ACL-SIGSEM Workshop on “The Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications”,</booktitle>
<pages>171--179</pages>
<contexts>
<context position="1002" citStr="Litkowski and Hargraves, 2005" startWordPosition="135" endWordPosition="138">sense disambiguation (PSD), comparing different models and training techniques (EM, MAP-EM with Lo norm, Bayesian inference using Gibbs sampling). To our knowledge, this is the first attempt at unsupervised preposition sense disambiguation. Our best accuracy reaches 56%, a significant improvement (at p &lt;.001) of 16% over the most-frequent-sense baseline. 1 Introduction Reliable disambiguation of words plays an important role in many NLP applications. Prepositions are ubiquitous—they account for more than 10% of the 1.16m words in the Brown corpus—and highly ambiguous. The Preposition Project (Litkowski and Hargraves, 2005) lists an average of 9.76 senses for each of the 34 most frequent English prepositions, while nouns usually have around two (WordNet nouns average about 1.2 senses, 2.7 if monosemous nouns are excluded (Fellbaum, 1998)). Disambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the SemEval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation.1 Given a sentence such as the following: In the morning, he shopped in Rome we ultimately want to be able to annotate it as 1See</context>
<context position="4532" citStr="Litkowski and Hargraves, 2005" startWordPosition="701" endWordPosition="704">lly constrained structure. This structure is reflected in dependency parses as a common construction. In our example sentence above, the respective structures would be shopped in morning and shopped in Rome. The senses of each element are denoted by a barred letter, i.e., p� denotes the preposition sense, h denotes the sense of the head word, and o� the sense of the object. 3 Data We use the data set for the SemEval 2007 PSD task, which consists of a training (16k) and a test set (8k) of sentences with sense-annotated prepositions following the sense inventory of The Preposition Project, TPP (Litkowski and Hargraves, 2005). It defines senses for each of the 34 most frequent prepositions. There are on average 9.76 senses per preposition. This corpus was chosen as a starting point for our study since it allows a comparison with the original SemEval task. We plan to use larger amounts of additional training data. We used an in-house dependency parser to extract the prepositional constructions from the data (e.g., “shop/VB in/IN Rome/NNP”). Pronouns and numbers are collapsed into ”PRO” and ”NUM”, respectively. In order to constrain the argument senses, we construct a dictionary that lists for each word all the poss</context>
</contexts>
<marker>Litkowski, Hargraves, 2005</marker>
<rawString>Ken Litkowski and Orin Hargraves. 2005. The preposition project. ACL-SIGSEM Workshop on “The Linguistic Dimensions of Prepositions and Their Use in Computational Linguistic Formalisms and Applications”, pages 171–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval2007 Task 06: Word-Sense Disambiguation of Prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1377" citStr="Litkowski and Hargraves, 2007" startWordPosition="197" endWordPosition="200">iable disambiguation of words plays an important role in many NLP applications. Prepositions are ubiquitous—they account for more than 10% of the 1.16m words in the Brown corpus—and highly ambiguous. The Preposition Project (Litkowski and Hargraves, 2005) lists an average of 9.76 senses for each of the 34 most frequent English prepositions, while nouns usually have around two (WordNet nouns average about 1.2 senses, 2.7 if monosemous nouns are excluded (Fellbaum, 1998)). Disambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the SemEval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation.1 Given a sentence such as the following: In the morning, he shopped in Rome we ultimately want to be able to annotate it as 1See (Chan et al., 2007) for how using WSD can help MT. in/TEMPORAL the morning/TIME he/PERSON shopped/SOCIAL in/LOCATIVE Rome/LOCATION Here, the preposition in has two distinct meanings, namely a temporal and a locative one. These meanings are context-dependent. Ultimately, we want to disambiguate prepositions not by and for themselves, but in the context of sequential semant</context>
<context position="13528" citStr="Litkowski and Hargraves, 2007" startWordPosition="2221" endWordPosition="2224">w for a better modeling of the data, Lo normalization helps by zeroing out infrequent ones. However, the difference between our complex model and the best HMM (EM with smoothing and random restarts, 55%) is not significant. The best (supervised) system in the SemEval task (Ye and Baldwin, 2007) reached 69% accuracy. The best current supervised system we are aware of (Hovy et al., 2010) reaches 84.8%. 7 Related Work The semantics of prepositions were topic of a special issue of Computational Linguistics (Baldwin et al., 2009). Preposition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from these approaches by using unsupervised methods and including argument labeling. The constraints of prepositional constructions have been explored by Rudzicz and Mokhov (2003) and O’Hara and Wiebe (2003) to annotate the semantic role of compl</context>
</contexts>
<marker>Litkowski, Hargraves, 2007</marker>
<rawString>Ken Litkowski and Orin Hargraves. 2007. SemEval2007 Task 06: Word-Sense Disambiguation of Prepositions. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<date>2010</date>
<booktitle>Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading. Association for Computational Linguistics,</booktitle>
<editor>Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard Hovy, Bernardo Magnini, and Christopher Manning, editors.</editor>
<location>Los Angeles, California,</location>
<contexts>
<context position="5690" citStr="(2010)" startWordPosition="893" endWordPosition="893">dictionary that lists for each word all the possible lexicographer senses according to WordNet. The set of lexicographer senses (45) is a higher level abstraction which is sufficiently coarse to allow for a good generalization. Unknown words are assumed to have all possible senses applicable to their respective word class (i.e. all noun senses for words labeled as nouns, etc). 4 Graphical Model Figure 1: Graphical Models. a) 1st order HMM. b) variant used in experiments (one model/preposition, thus no conditioning on p). c) incorporates further constraints on variables As shown by Hovy et al. (2010), preposition senses can be accurately disambiguated using only the head word and object of the PP. We exploit this property of prepositional constructions to represent the constraints between h, p, and o in a graphical model. We define a good model as one that reasonably constrains the choices, but is still tractable in terms of the number of parameters being estimated. As a starting point, we choose the standard firstorder Hidden Markov Model as depicted in Figure 1a. Since we train a separate model for each preposition, we can omit all arcs to p. This results in model 1b. The joint distribu</context>
<context position="8880" citStr="(2010)" startWordPosition="1459" endWordPosition="1459">ations, or until the perplexity stopped decreasing below a threshold of 10−6. 5.2 EM with Smoothing and Restarts In addition to the baseline, we ran 100 restarts with random initialization and smoothed the fractional counts by adding 0.1 before normalizing (Eisner, 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initializations that lead to local maxima. Carmel provides options for both smoothing and restarts. 5.3 MAP-EM with L0 Norm Since we want to encourage sparsity in our models, we use the MDL-inspired technique introduced by Vaswani et al. (2010). Here, the goal is to increase the data likelihood while keeping the number of parameters small. The authors use a smoothed L0 prior, which encourages probabilities to go down to 0. The prior involves hyperparameters α, which rewards sparsity, and β, which controls how close the approximation is to the true L0 norm.2 We perform a grid search to tune the hyper-parameters of the smoothed L0 prior for accuracy on the preposition against, since it has a medium number of senses and instances. For HMM, we set αtrans =100.0, βtrans =0.005, αemit =1.0, βemit =0.75. The subscripts trans and emit denot</context>
<context position="10447" citStr="(2010)" startWordPosition="1726" endWordPosition="1726">f Chiang et al. (2010), running Gibbs sampling for 10,000 iterations, with a burn-in period of 5,000, and carry out automatic run selection over 10 random restarts.3 Again, we tuned the hyper-parameters of our Dirichlet priors for accuracy via a grid search over the model for the preposition against. For both models, we set the concentration parameter αtrans to 0.001, and αemit to 0.1. This encourages sparsity in the model and allows for a more nuanced explanation of the data by shifting probability mass to the few prominent classes. 2For more details, the reader is referred to Vaswani et al. (2010). 3Due to time and space constraints, we did not run the 1000 restarts used in Chiang et al. (2010). 325 baseline Vanilla EM EM, smoothed, MAP-EM + CRP, 10 random 100 random smoothed L0 restarts restarts norm HMM 0.40 (0.40) 0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53) our model 0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49) Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAPEM+smoothed Lo norm on our model. Italics denote significant improvement over baseline at p &lt;.001. Numbers in brackets include against (used to tune MAP-EM and Bayesian Inferenc</context>
<context position="13774" citStr="(2010)" startWordPosition="2259" endWordPosition="2259">sk (Ye and Baldwin, 2007) reached 69% accuracy. The best current supervised system we are aware of (Hovy et al., 2010) reaches 84.8%. 7 Related Work The semantics of prepositions were topic of a special issue of Computational Linguistics (Baldwin et al., 2009). Preposition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from these approaches by using unsupervised methods and including argument labeling. The constraints of prepositional constructions have been explored by Rudzicz and Mokhov (2003) and O’Hara and Wiebe (2003) to annotate the semantic role of complete PPs with FrameNet and Penn Treebank categories. Ye and Baldwin (2006) explore the constraints of prepositional phrases for 326 semantic role labeling. We plan to use the constraints for argument disambiguation. 8 Conclusion and Future Work We</context>
</contexts>
<marker>2010</marker>
<rawString>Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard Hovy, Bernardo Magnini, and Christopher Manning, editors. 2010. Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading. Association for Computational Linguistics, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Janyce Wiebe</author>
</authors>
<title>Preposition semantic classification via Penn Treebank and FrameNet.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>79--86</pages>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>Tom O’Hara and Janyce Wiebe. 2003. Preposition semantic classification via Penn Treebank and FrameNet. In Proceedings of CoNLL, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom O’Hara</author>
<author>Janyce Wiebe</author>
</authors>
<title>Exploiting semantic role resources for preposition disambiguation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>2</issue>
<marker>O’Hara, Wiebe, 2009</marker>
<rawString>Tom O’Hara and Janyce Wiebe. 2009. Exploiting semantic role resources for preposition disambiguation. Computational Linguistics, 35(2):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rudzicz</author>
<author>Serguei A Mokhov</author>
</authors>
<title>Towards a heuristic categorization of prepositional phrases in english with wordnet.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Cornell University,</institution>
<contexts>
<context position="14061" citStr="Rudzicz and Mokhov (2003)" startWordPosition="2295" endWordPosition="2298">ition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from these approaches by using unsupervised methods and including argument labeling. The constraints of prepositional constructions have been explored by Rudzicz and Mokhov (2003) and O’Hara and Wiebe (2003) to annotate the semantic role of complete PPs with FrameNet and Penn Treebank categories. Ye and Baldwin (2006) explore the constraints of prepositional phrases for 326 semantic role labeling. We plan to use the constraints for argument disambiguation. 8 Conclusion and Future Work We evaluate the influence of two different models (to represent constraints) and three unsupervised training methods (to achieve sparse sense distributions) on PSD. Using MAP-EM with Lo norm on our model, we achieve an accuracy of 56%. This is a significant improvement (at p &lt;.001) over t</context>
</contexts>
<marker>Rudzicz, Mokhov, 2003</marker>
<rawString>Frank Rudzicz and Serguei A. Mokhov. 2003. Towards a heuristic categorization of prepositional phrases in english with wordnet. Technical report, Cornell University, arxiv1.library.cornell.edu/abs/1002.1095-?context=cs.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephen Tratz</author>
<author>Dirk Hovy</author>
</authors>
<title>Disambiguation of Preposition Sense Using Linguistically Motivated Features.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium,</booktitle>
<pages>96--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="13754" citStr="Tratz and Hovy (2009)" startWordPosition="2252" endWordPosition="2255">upervised) system in the SemEval task (Ye and Baldwin, 2007) reached 69% accuracy. The best current supervised system we are aware of (Hovy et al., 2010) reaches 84.8%. 7 Related Work The semantics of prepositions were topic of a special issue of Computational Linguistics (Baldwin et al., 2009). Preposition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from these approaches by using unsupervised methods and including argument labeling. The constraints of prepositional constructions have been explored by Rudzicz and Mokhov (2003) and O’Hara and Wiebe (2003) to annotate the semantic role of complete PPs with FrameNet and Penn Treebank categories. Ye and Baldwin (2006) explore the constraints of prepositional phrases for 326 semantic role labeling. We plan to use the constraints for argument disambiguation. 8 Conclusio</context>
</contexts>
<marker>Tratz, Hovy, 2009</marker>
<rawString>Stephen Tratz and Dirk Hovy. 2009. Disambiguation of Preposition Sense Using Linguistically Motivated Features. In Proceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium, pages 96–100, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Adam Pauls</author>
<author>David Chiang</author>
</authors>
<title>Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>209--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8880" citStr="Vaswani et al. (2010)" startWordPosition="1456" endWordPosition="1459">el for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6. 5.2 EM with Smoothing and Restarts In addition to the baseline, we ran 100 restarts with random initialization and smoothed the fractional counts by adding 0.1 before normalizing (Eisner, 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initializations that lead to local maxima. Carmel provides options for both smoothing and restarts. 5.3 MAP-EM with L0 Norm Since we want to encourage sparsity in our models, we use the MDL-inspired technique introduced by Vaswani et al. (2010). Here, the goal is to increase the data likelihood while keeping the number of parameters small. The authors use a smoothed L0 prior, which encourages probabilities to go down to 0. The prior involves hyperparameters α, which rewards sparsity, and β, which controls how close the approximation is to the true L0 norm.2 We perform a grid search to tune the hyper-parameters of the smoothed L0 prior for accuracy on the preposition against, since it has a medium number of senses and instances. For HMM, we set αtrans =100.0, βtrans =0.005, αemit =1.0, βemit =0.75. The subscripts trans and emit denot</context>
<context position="10447" citStr="Vaswani et al. (2010)" startWordPosition="1723" endWordPosition="1726"> the approach of Chiang et al. (2010), running Gibbs sampling for 10,000 iterations, with a burn-in period of 5,000, and carry out automatic run selection over 10 random restarts.3 Again, we tuned the hyper-parameters of our Dirichlet priors for accuracy via a grid search over the model for the preposition against. For both models, we set the concentration parameter αtrans to 0.001, and αemit to 0.1. This encourages sparsity in the model and allows for a more nuanced explanation of the data by shifting probability mass to the few prominent classes. 2For more details, the reader is referred to Vaswani et al. (2010). 3Due to time and space constraints, we did not run the 1000 restarts used in Chiang et al. (2010). 325 baseline Vanilla EM EM, smoothed, MAP-EM + CRP, 10 random 100 random smoothed L0 restarts restarts norm HMM 0.40 (0.40) 0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53) our model 0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49) Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAPEM+smoothed Lo norm on our model. Italics denote significant improvement over baseline at p &lt;.001. Numbers in brackets include against (used to tune MAP-EM and Bayesian Inferenc</context>
</contexts>
<marker>Vaswani, Pauls, Chiang, 2010</marker>
<rawString>Ashish Vaswani, Adam Pauls, and David Chiang. 2010. Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging. In Proceedings of the ACL 2010 Conference Short Papers, pages 209–214. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Tim Baldwin</author>
</authors>
<title>Semantic role labeling of prepositional phrases.</title>
<date>2006</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="14201" citStr="Ye and Baldwin (2006)" startWordPosition="2319" endWordPosition="2322">s using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use of the arguments for preposition sense disambiguation, using various features. We differ from these approaches by using unsupervised methods and including argument labeling. The constraints of prepositional constructions have been explored by Rudzicz and Mokhov (2003) and O’Hara and Wiebe (2003) to annotate the semantic role of complete PPs with FrameNet and Penn Treebank categories. Ye and Baldwin (2006) explore the constraints of prepositional phrases for 326 semantic role labeling. We plan to use the constraints for argument disambiguation. 8 Conclusion and Future Work We evaluate the influence of two different models (to represent constraints) and three unsupervised training methods (to achieve sparse sense distributions) on PSD. Using MAP-EM with Lo norm on our model, we achieve an accuracy of 56%. This is a significant improvement (at p &lt;.001) over the baseline and vanilla EM. We hope to shorten the gap to supervised systems with more unlabeled data. We also plan on training our models w</context>
</contexts>
<marker>Ye, Baldwin, 2006</marker>
<rawString>Patrick Ye and Tim Baldwin. 2006. Semantic role labeling of prepositional phrases. ACM Transactions on Asian Language Information Processing (TALIP), 5(3):228–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="13193" citStr="Ye and Baldwin, 2007" startWordPosition="2169" endWordPosition="2172">er with the less complex HMM. CRP is somewhat surprisingly roughly equivalent to EM with smoothing and random restarts. Accuracy might improve with more restarts. MAP-EM with Lo normalization produces the best result (56%), significantly outperforming the baseline at p &lt; .001. With more parameters (9.7k vs. 3.7k), which allow for a better modeling of the data, Lo normalization helps by zeroing out infrequent ones. However, the difference between our complex model and the best HMM (EM with smoothing and random restarts, 55%) is not significant. The best (supervised) system in the SemEval task (Ye and Baldwin, 2007) reached 69% accuracy. The best current supervised system we are aware of (Hovy et al., 2010) reaches 84.8%. 7 Related Work The semantics of prepositions were topic of a special issue of Computational Linguistics (Baldwin et al., 2009). Preposition sense disambiguation was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O’Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al. (2010) make explicit use </context>
</contexts>
<marker>Ye, Baldwin, 2007</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2007. MELB-YB: Preposition Sense Disambiguation Using Rich Semantic Features. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>