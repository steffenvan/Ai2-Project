<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000106">
<title confidence="0.985646">
Acquiring Applicable Common Sense Knowledge from the Web
</title>
<author confidence="0.94793">
Hansen A. Schwartz and Fernando Gomez
</author>
<affiliation confidence="0.9978475">
School of Electrical Engineering and Computer Science
University of Central Florida
</affiliation>
<address confidence="0.593636">
Orlando, FL 32816, USA
</address>
<email confidence="0.99833">
{hschwartz, gomez}@cs.ucf.edu
</email>
<sectionHeader confidence="0.993846" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995614">
In this paper, a framework for acquiring com-
mon sense knowledge from the Web is pre-
sented. Common sense knowledge includes
information about the world that humans use
in their everyday lives. To acquire this knowl-
edge, relationships between nouns are re-
trieved by using search phrases with automat-
ically filled constituents. Through empirical
analysis of the acquired nouns over Word-
Net, probabilities are produced for relation-
ships between a concept and a word rather
than between two words. A specific goal of
our acquisition method is to acquire knowl-
edge that can be successfully applied to NLP
problems. We test the validity of the acquired
knowledge by means of an application to the
problem of word sense disambiguation. Re-
sults show that the knowledge can be used to
improve the accuracy of a state of the art un-
supervised disambiguation system.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999679357142857">
Common sense knowledge (CSK) is the knowledge
we use in everyday life without necessarily being
aware of it. Panton et al. (2006) of the Cyc project,
define common sense as “the knowledge that every
person assumes his neighbors also possess”. Al-
though the term common sense may be understood
as a process such as reasoning, we are referring only
to knowledge. It is CSK that tells us keys are kept in
one’s pocket and keys are used to open a door, but
CSK does not hold that keys are kept in a kitchen
sink or that keys are used to turn on a microwave,
although all are possible.
To show the need for this information more
clearly we provide a couple sentences:
</bodyText>
<page confidence="0.591442">
1
</page>
<bodyText confidence="0.999833176470588">
She put the batter in the refrigerator. (1)
He ate the apple in the refrigerator. (2)
In (1), we are dealing with lexical ambiguity. There
is little doubt for us to determine just what the “bat-
ter” is (food/substance used in baking). However, a
computer must determine that it is not someone who
swings a bat in baseball that is being put into a re-
frigerator, although it is entirely possible to do (de-
pending on the size of the refrigerator). This demon-
strates how CSK can be useful in solving word sense
disambiguation. We know it is common for food to
be found in a refrigerator and so we easily resolve
batter as a food/substance rather than a person.
CSK can also help to solve syntactic ambiguity.
The problem of prepositional phrase attachment oc-
curs in sentences similar to (2). In this case, it is
difficult for a computer to determine if “he” is in the
refrigerator eating an apple or if the “apple” which
he ate was in the refrigerator. Like the previous ex-
ample, the knowledge that food is commonly found
in a refrigerator and people are not, leads us to un-
derstand that “in the refrigerator” should be attached
to the noun phrase “the apple” and not as a modifier
of the verb phrase “ate”.
Unfortunately, there are not many sources of CSK
readily available for use in computer algorithms.
Those sets of knowledge that are available, such
as the CYC project (Lenat, 1995) or ConceptNet
(Liu and Singh, 2004) rely on manually provided
or crafted data. Our aim is to develop an auto-
matic approach to acquire CSK1 by turning to the
vast amount of unannotated text that is available on
the Web. In turn, we present a method to automat-
ically retrieve and analyze phrases from the Web.
</bodyText>
<footnote confidence="0.975324">
1data available at: http://eecs.ucf.edu/˜hschwartz/CSK/
</footnote>
<note confidence="0.9945915">
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 1–9,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999939571428571">
We employ the use of a syntactic parser to accu-
rately match syntactic patterns of phrases acquired
from the Web. The data is analyzed over WordNet
(Miller et al., 1993) in order to induce knowledge
about word senses or concepts rather than words. Fi-
nally, we evaluate whether the knowledge by apply-
ing it to the problem of word sense disambiguation.
</bodyText>
<sectionHeader confidence="0.985164" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9962855">
The particular type of CSK that we experiment with
in this paper is described formally as follows:
A relationship, e1Re2, exists between entities
e1 and e2 if one finds “e1 is R e2.”
Some examples include: “a cup is on a table” and
“food is in a refrigerator”, which would result in re-
lationships: cupontable and foodinrefrigerator. The
next section attempts to make the relationship more
clear, as we provide a brief linguistic background of
prepositions and relationships.
</bodyText>
<subsectionHeader confidence="0.983523">
2.1 Prepositions and Relationships
</subsectionHeader>
<bodyText confidence="0.929019129032258">
Prepositions state a relationship between two enti-
ties (Quirk et al., 1985). One of the entities is typ-
ically a constituent of the sentence while the other
is the complement to the preposition. For exam-
ple, consider the relationship between ‘furniture’
and ‘house’ in the following sentences:
The furniture is...
...at the house.
...on the house.
...in the house.
‘The furniture’ is the subject of the sentence, while
‘the house’ is a prepositional complement. Notice
that the meaning is different for each sentence de-
pending on the actual preposition (‘at’, ‘on’, or ‘in’),
and thus furniture relates to house in three different
ways. Although each relationship between furniture
and house is possible, only one would be considered
CSK to most people: furnitureinhouse.
We focus on prepositions which indicate a posi-
tive spacial relationship given by Quirk et al. (1985).
There are three types of such relationships: “at a
point”, “on a line or surface”, and “in an area or vol-
ume”. In particular, we concentrate on the 1 to 3
dimensional relationships given in Table 1, denoted
on and in throughout the paper. At, the 0 dimen-
sional relationship, occurred far less frequently. The
dims description prepositions
1 or 2 on surface or line on, onto, atop, upon,
on top of, down on
2 or 3 in area or volume in, into, inside,
within, inside of
</bodyText>
<tableCaption confidence="0.9884425">
Table 1: Spatial dimensions (dims) and corresponding
prepositions.
</tableCaption>
<bodyText confidence="0.900775333333333">
sentences below exemplify each of the 1 to 3 dimen-
sional relationships:
on surface The keyboard is on the table.
on line The beach is on US 1.
in area The bank is in New York.
in volume The vegetables are in the bowl.
</bodyText>
<subsectionHeader confidence="0.552781">
2.2 Related Work
</subsectionHeader>
<bodyText confidence="0.999979935483871">
As a prevalent source of lexical knowledge, dictio-
nary definitions may be regarded as common sense.
However, some definitions may be considered expert
knowledge rather than CSK. The scope of definitions
certainly do not provide all necessary information
(such as keys are commonly kept in one’s pocket).
We examine WordNet in particular because the hy-
pernym relation has been developed extensively for
nouns. The noun ontology is used in our work to
help induce relationships involving concepts (senses
of nouns) rather than just among words. This notion
of inducing CSK among concepts, rather than words,
is a key difference between our work and similar re-
search.
The work on VerbOcean is similar to our research
in the use of the Web for acquiring relationships
(Chklovski and Pantel, 2004). They used patterns
of phrases in order to search the Web for semantic
relations among verbs. The knowledge they acquire
falls into the category of CSK, but the specific re-
lationships are different than ours in that they are
among verb word forms and senses are not resolved.
ConceptNet was created based on the OpenMind
Commonsense project (Liu and Singh, 2004). The
project acquired knowledge through an interface on
the Web by having users play games and answer
questions about words. A contribution of Concept-
Net is that it has a wide range of relations. While
WordNet provides connections between concepts
(senses of words), ConceptNet only provides rela-
tionships between word forms.
</bodyText>
<page confidence="0.997016">
2
</page>
<figure confidence="0.999950923076923">
a chosen
nounB
search
phrases
for CSK
web search
parse and match
Noun Acquisition
create
web queries
Concept Analysis
WordNet
Ontology
Word
probabilities:
nounA
[injon]
nounB
determine
concept
probabilities
concept
probabilities:
conceptA
[injon]
nounB
</figure>
<figureCaption confidence="0.9973715">
Figure 1: The overall common sense knowledge acquisition framework under the assumption that one is acquiring
concepts (WordNet synsets) in a relationship with a given nounB (word).
</figureCaption>
<bodyText confidence="0.999984551020408">
A project in progress for over twenty years, CYC
has been acquiring common sense knowledge about
everyday objects and actions stored in 106 axioms
(Lenat, 1995). The axioms, handcrafted by workers
at CYCcorp, represent knowledge rooted in propo-
sitions. There are three layers of information: the
first two, access and physical, contain meta data,
while the third, logical layer, stores high level im-
plicit meanings. Only a portion of CYC is available
to the public.
Our method for acquiring knowledge is somewhat
similar to that of (Hearst, 1992). Patterns are built
manually. However, we do not use our manually
constructed patterns (referred to as search phrases)
to query the Web. Instead the search phrases are ab-
stract patterns that are used to automatically gener-
ate more specific web queries by filling constituents
based on lists of words.
The SemEval-2007 Task 4 presents a good
overview of work in noun-noun relationships (Girju
et al., 2007). Our work is related in that the rela-
tionships we acquire are between nominals, and in
order to build their corpus Girju et al. queried the
web with patterns like that of Hearst’s work (Hearst,
1992). The SemEval task was to choose or clas-
sify relationships, rather than acquire and apply rela-
tionships. Additionally, the relationship classes they
use are not necessarily within the scope of common
sense knowledge.
Similar to our research, in (Agirre et al., 2001)
knowledge is acquired about WordNet concepts.
They find topics signatures, sets of related words,
based on data from the Web and use them for word
sense disambiguation. However, the type of rela-
tionship between words of a topic signature and the
WordNet concept is not made explicit, and the au-
thors find the topic signatures are not very effective
for word sense disambiguation.
Finally, we note one approach to using the Web
for NLP applications is to acquire knowledge on the
fly. Previous work has approached solutions to word
sense disambiguation by acquiring words or phrases
directly based on the sentences or words being dis-
ambiguated (Martinez et al., 2006; Schwartz and
Gomez, 2008). These methods dynamically acquire
the data at runtime, rather than automatically create
a common sense database of relations that is readily
available. Additionally, in our current approach, we
are able to acquire explicit CSK relationships.
</bodyText>
<sectionHeader confidence="0.956951" genericHeader="method">
3 Common Sense Acquisition
</sectionHeader>
<bodyText confidence="0.975775333333333">
The two major phases of our framework, “Noun Ac-
quisition” and “Concept Analysis”, are outlined in
Figure 1 and described within this section.
</bodyText>
<subsectionHeader confidence="0.998909">
3.1 Noun Acquisition
</subsectionHeader>
<bodyText confidence="0.9999614">
The first step of our method is to acquire nouns
(as words) from the Web which are in a relation-
ship with other nouns. A Web search is performed
in order to retrieve samples of text matching a web
query created from a search phrase for the relation-
ship. Each sample is syntactically parsed to verify
a match with the corresponding web query, and the
noun(s) filling a missing constituent of the parse are
recorded.
The framework itself is very flexible, and it can
handle the acquisition of words from other parts of
speech. However, to be clear, we focus the explana-
tion on the use of the framework to acquire specific
types of relationships between nouns. Below we de-
scribe the procedures in more detail.
</bodyText>
<page confidence="0.987175">
3
</page>
<subsectionHeader confidence="0.975295">
3.1.1 Creating Web Queries
</subsectionHeader>
<bodyText confidence="0.987673107142857">
Web queries are created semi-automatically by
defining these parameters of a search phrase:
nounA the first noun phrase
nounB the second noun phrase
prep preposition, if any, used in the phrase
verb verb, if any, used in the phrase.
Table 2 lists all of the search phrases we use, one of
which we use as an example throughout this section:
place nounA prep nounB
The verb, “place” in this case, is statically defined as
part of the search phrase.
Prepositions were chosen to describe the type of
relationship we were seeking to acquire as described
in the background section. We limited ourselves to
the “on” and “in” relationships since these were the
most common.
on = (on, onto, atop, upon, on top of, down on)
in = (in, into, inside, within, inside of)
When noun parameters are provided, determiners
or possessive pronouns selected from the list below
are included. This provides greater accuracy in our
search results.
det = (the, a/an, this, that, my, your, his, her)
Finally, the undefined parameters are replaced
with a ‘*’. Below is a web query created from our
search phrase where nounB is ‘refrigerator’, prep is
‘in’, det is ‘the’, and nounA is undefined:
place * in the refrigerator
</bodyText>
<subsectionHeader confidence="0.991751">
3.1.2 Searching the Web
</subsectionHeader>
<bodyText confidence="0.913573333333333">
Given a nounB, The search algorithm can be sum-
marized through the pseudocode below.
for each search phrase
</bodyText>
<equation confidence="0.9766272">
for each prep
for each det
query= create query(search phrase,
prep, det, nounB));
samples= websearch(query);
</equation>
<bodyText confidence="0.999404333333333">
The searches were carried out through the Google
Search API2, or the Yahoo! Search Web Services3.
Each search phrase, listed in Table 2, was run until
a maximum of 2000 results were returned. Dupli-
cate samples were removed to reduce the effects of
websites replicating the text of one another.
</bodyText>
<footnote confidence="0.9991235">
2no longer supported by Google
3http://developer.yahoo.com/search/
</footnote>
<table confidence="0.991846785714286">
relation search phrase voice
on, in nounA is located prep nounB passive
nounA is found prep nounB
nounA is situated prep nounB
on, in nounA is prep nounB active
put nounA prep nounB
place nounA prep nounB
lay nounA prep nounB
set nounA prep nounB
locate nounA prep nounB
position nounA prep nounB
on hang nounA prep nounB active
mount nounA prep nounB
attach nounA prep nounB
</table>
<tableCaption confidence="0.963157">
Table 2: Search phrases and relationships used for acqui-
sition of CSK.
</tableCaption>
<subsectionHeader confidence="0.843272">
3.1.3 Parse and Match
</subsectionHeader>
<bodyText confidence="0.9879202">
The results we want to achieve in this step should
describe a relationship:
nounA is [in I on] nounB
We use Charniak’s parser (Charniak, 2000) on both
the web query and the results returned from the web
in order to ensure accuracy. To demonstrate this pro-
cess, we extend our example, “place * in the refrig-
erator”.
First, we get a parse with * (nounA) represented
as ‘something’.
</bodyText>
<equation confidence="0.751507">
(VP (VB place)
(NP (NN something))
(PP (IN in) (NP (DT the) (NN refrigerator))))
</equation>
<bodyText confidence="0.999365666666667">
We now know the constituent(s) which replace ‘(NN
something)’ will be our nounA. For example, in the
following parse ‘batter’ is resolved as nounA.
</bodyText>
<equation confidence="0.7538154">
(S1 (S (NP (PRP He))
(VP (AUX was) (VP (VBN told) (S (VP (TO to)
(VP (VB place)
(NP (DT the) (JJ mixed) (NN batter))
(PP (IN in) (NP (DT the) (NN refrigerator))))]
</equation>
<bodyText confidence="0.9998635">
The head noun of the matching phrase is determined,
which is ‘batter’ in the phrase ‘(DT the) (JJ mixed)
(NN batter)’. Words are only recorded if they are
present as a noun in WordNet. If the noun phrase
contains a compound noun found in WordNet, then
the compound noun is recorded instead.
The parse also helps to eliminate bad results. For
the following sentence, the verb phrase does not
</bodyText>
<page confidence="0.987945">
4
</page>
<bodyText confidence="0.999421333333333">
match the parse of the web query due to an extra PP,
and therefore we do not pull out “for several hours”
as nounA.
</bodyText>
<table confidence="0.970611875">
(S1 (S (VP (VP (VB Mix)
(NP (DT the) (NN sugar))
(PRT (RP in))
(PP (TO to) (NP (DT the) (NN dough))))
(CC and)
(VP (VB place)
(PP (IN for) (NP (JJ several) (NNS hours)))
(PP (IN in) (NP (DT the) (NN refrigerator)))))))
</table>
<bodyText confidence="0.991410555555556">
One may note that this malformed sentence is com-
municating that ‘dough’ is placed in the refrigerator,
but the method does not handle this.
At the end of the noun acquisition phase, we are
left with frequency counts of nouns being retrieved
from a context matching the syntactic structure of
a web query. This can easily be represented as the
probability of a noun, nA, being returned to a query
for the relationship, R, with noun nB.
</bodyText>
<equation confidence="0.537154">
pw(nA, R, nB)
</equation>
<bodyText confidence="0.9999575">
This value along with the other steps we have gone
over are stored in a MySQL relational database4.
One could trace a relationship probability between
nouns back to the web results which were matched
to a web query, and even determine the abstract
search phrase which produced the web query.
</bodyText>
<subsectionHeader confidence="0.999245">
3.2 Concept Analysis
</subsectionHeader>
<bodyText confidence="0.948381133333333">
A focus of this work is on going beyond relation-
ships between words. We would like to acquire
knowledge about specific concepts in WordNet. In
particular, we are trying to induce:
conceptA is [in I on] nounB.
where conceptA is a concept in WordNet (such as a
sense of nounA), and nounB remains simply a word.
For the analysis, we rely on the vast amount of
nouns we are able to acquire in order to create proba-
bilities for relationships of conceptARnounB. To get
a grasp of the idea in general, consider ‘table’ as a
nounB of interest. By examining all possible hyper-
nyms of all senses of each nounA one will find it
is common for abstract entities to be “in a table”
(i.e. data in a table), artifacts to be “on a table” (i.e.
</bodyText>
<footnote confidence="0.933022">
4http://www.mysql.com
</footnote>
<bodyText confidence="0.999696294117647">
cup on a table), and physical things (including living
things) to be “at a table” (i.e. the employees at the
table). The same idea could be applied in reverse if
one acquires knowledge for a set of nounAs. How-
ever, this paper only focuses on acquiring knowl-
edge for the nounB constituent in a search phrase.
To begin with, one should note that concepts in
WordNet are represented as synsets. A synset is
a group of word-senses that have the same mean-
ing. For example, (batter-1, hitter-1, slugger-1,
batsman-1) is a synset with the meaning “(baseball)
a ballplayer who is batting”. We use WordNet ver-
sion 3.0 in order to take advantage of the latest up-
dates and corrections to the noun ontology. Since a
word has multiple senses, we represent the probabil-
ity that a word-sense, nAs, resulted from a query for
a relationship, R with nounB as:
</bodyText>
<equation confidence="0.996484">
pns(nAs, R, nB) = pw(lemma(nAs), R, nB)
senses(lemma(nAs))
</equation>
<bodyText confidence="0.999211">
where senses returns the number of senses of the
word (lemma) within the word-sense nAs. We
can then extend the probability to apply to a synset,
syns, as:
</bodyText>
<equation confidence="0.8897485">
�psyn(syns, R, nB) = pns(nAs, R, nB)
nAs∈syns
</equation>
<bodyText confidence="0.964654">
Finally, we define a recursive function based on
the idea that a concept subsumes all concepts below
it (hyponyms) in the WordNet ontology:
</bodyText>
<equation confidence="0.999347333333333">
Pc(cA, R, nB) = psyn(syns(cA), R, nB)
+ � Pc(h, R, nB)
h∈hypos(cA)
</equation>
<bodyText confidence="0.999782692307692">
where cA is a concept/node in WordNet, syns re-
turns the synset which represents the concept, and
hypos returns the set of all direct hyponyms within
the WordNet ontology. For example, (money-3) is
a (currency-1), so Pc(currency-1, R, nB) receives
psyn((money-3), R, nB) among others. This type
of calculation over WordNet follows much like
that of Resnik’s (1999) information-content calcu-
lation. Note that the function no longer recurs
when reaching a concept with no hyponyms and that
Pc(entity-1, R, nB) is always 1 (entity-1 is the root
node). Pc now represents a probability for the rela-
tionship: conceptARnounB.
</bodyText>
<page confidence="0.992834">
5
</page>
<table confidence="0.9995585625">
nounB #nounAs nounB #nounAs
basket 3300 boat 2787
bookcase 260 bottle 4742
bowl 5252 cabin 720
cabinet 1474 canoe 163
car 5534 ceiling 1187
city 1432 desk 4770
drawer 1638 dresser 698
floor 2850 house 4627
jar 4462 kitchen 2948
pocket 4771 refrigerator 2897
road 5493 room 5023
shelf 2581 ship 1469
sink 296 sofa 509
table 5312 truck 528
van 301 wall 2285
</table>
<tableCaption confidence="0.971217333333333">
Table 3: List of nouns which fill the nounB constituent
in a search phrase, and the corresponding occurrences of
nounAs acquired for each.
</tableCaption>
<sectionHeader confidence="0.996636" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999909571428572">
Our evaluation focuses on the applicability of the
acquired CSK. We acquired relationships for the 30
nouns listed in Table 3. These nouns represent all
possible words to fill the nounB constituent of a
search phrase. The corresponding #nounAs indi-
cates the number of nounAs that were acquired from
the Web for each nounB. For example, 4771 nounAs
were acquired for ‘pocket’. This means 4771 results
from the web matched the parse of a web query for
‘pocket’ and contained a nounA in WordNet (keep-
ing in mind duplicates Web text were removed).
Delving deeper into our example, below are
the top 20 nounAs found for the relationship
nounAinpocket.
</bodyText>
<construct confidence="0.6578555">
money, hand, cash, firework, something, dol-
lar, ball, hands, key, coin, pedometer, card,
battery, item, phone, penny, music, buck, im-
plant, wallet
</construct>
<bodyText confidence="0.999958861111111">
As described in the concept analysis section, occur-
rences of each nounA for a given nounB lead to p,,,
values, which in turn are used to produce P, values
for concepts in WordNet. The application of CSK
utilizes these probabilities rather than simply lists of
words or even lists of concepts. However, challenges
were encountered during the noun acquisition step
before the probabilities were produced.
Many challenges of the noun acquisition step
were overcome through the use of a parser. For ex-
ample, phrases such as “Palestine is on the road to
becoming...” could be eliminated since the parser
marks the prepositional phrase “to becoming” as be-
ing attached to “the road”. Thus, the parse of the
web sample does not match the parse of the web
query used to acquire it. Other times, noun-noun re-
lationships were common simply because many web
pages seem to copy the text of others. This prob-
lem was handled through the elimination of dupli-
cate text samples from the Web. In the end, only
about one in four results from the Web were actually
used. Numbers in Table 3 reflect the result of these
eliminations.
Some issues of the acquisition step were not di-
rectly addressed in this paper. A domain may tend
to be more prevalent on the Internet and skew the
CSK, such as fireworkinpocket. Another example,
babyinbasket was very common due to biblical ref-
erences. Fictional works and metaphors also pro-
vided uncommon relationships dispersed within the
results. Additionally, the parser makes mistakes. It
was the hope that the concept analysis step would
help to mitigate some noise from these problems.
A final issue was the bottleneck of limited queries
per day by the search engines, which restricted us to
testing on only the 30 nouns listed.
</bodyText>
<subsectionHeader confidence="0.999128">
4.1 Disambiguation System
</subsectionHeader>
<bodyText confidence="0.9999289375">
The CSK is not intended to be used by itself for dis-
ambiguation. It would be far from accurate to as-
sume the sense of a noun can be disambiguated sim-
ply by observing its relationship with one other noun
in the sentence. For example, one of the test sen-
tences incorporated the relationship noteinpocket.
Multiple senses of note are likely to be found in a
pocket (i.e. the senses referring to “a brief written
record”, “a short personal letter”, or “a piece of pa-
per money”). In other cases, a relationship may not
be found for any sense of a target word. Therefore,
our knowledge is intended to be used as a reference,
consulted by a disambiguation system.
We integrate our knowledge into a state of the art
“all-words” word sense disambiguation algorithm.
These algorithms are considered unsupervised or
</bodyText>
<page confidence="0.99856">
6
</page>
<bodyText confidence="0.99996641025641">
minimally supervised, because they do not require
specific training data that is designed for instances
of words in the testing data. In other words, these
systems are designed to handle any word they come
across. Our knowledge can supplement such a sys-
tem, because the data can be acquired automatically
for an unlimited number of nouns, assuming limit-
less web query restrictions.
The basis of our disambiguation system is the
publicly available GWSD system (Sinha and Mihal-
cea, 2007). Sinha and Mihalcea report higher re-
sults on the Senseval-2 and Senseval-3 datasets than
any of the participating unsupervised system. Ad-
ditionally, GWSD is compatible with WordNet 3.0
and its output made it easy to integrate our knowl-
edge. Sense predictions from four different graph
metrics are produced, and we are able to incorporate
our knowledge as another prediction within a voting
scheme.
Considering the role of our knowledge as a refer-
ence, in some cases we would like the CSK to sug-
gest multiple senses and in others none. For each
target noun instance in the corpus, we lookup the
P,(c, R, nB) value, where c is the WordNet concept
that corresponds to a sense of the target noun. We
choose nB by matching the phrase “in|on det nB”
within the sentence. The system suggests all senses
with a P, value greater than 0.75 of the maximum P,
value over all senses. If no senses have a P, value
then no senses are suggested.
During voting, tallies of predictions and sugges-
tions are taken for each sense of a noun. Ties are
broken by choosing the lowest sense number among
all those involved in the tie. Note that this is differ-
ent than choosing the most frequent sense (i.e. the
lowest sense number from all senses), in that only
the top predicted senses are considered. This same
type of voting is used with and without the CSK sug-
gestions.
</bodyText>
<subsectionHeader confidence="0.967152">
4.2 Experimental Corpus
</subsectionHeader>
<bodyText confidence="0.979448225">
A goal of our work was to acquire data which could
be applied to NLP problems. We focus particularly
on the difficult problem of word sense disambigua-
tion. Due to the lack of sense tagged data, we were
unable to find an annotated corpus with instances
of all the nouns in Table 3 as prepositional com-
plements. This was not surprising considering one
of the reasons that minimally supervised approaches
have become more popular is that they do not require
hand-tagged training data (Mihalcea, 2002; Diab,
2004; McCarthy et al., 2004).
We created a corpus from sentences in Wikipedia
which contained the phrase “in|on det lemma”,
where det is a determiner or possessive pronoun,
lemma is a noun from Table 3, and in|on is a prepo-
sition for either relationship described earlier. Be-
low we have provided an example from our corpus
where the knowledge from ‘pocket’ can be applied
to disambiguate ‘key’.
Now Tony’s key to the fiat is in the pocket of his
raincoat, so on returning to his fiat some time
later he realizes that he cannot get inside.
The corpus5 contained a total of 342 sen-
tences, with one target noun annotated per sen-
tence. The target nouns were selected to poten-
tially fill the nounA constituent in the relationship
nounARnounB, and they were assigned all appro-
priate WordNet 3.0 senses. Considering the fine-
grained nature of WordNet (Ide and Wilks, 2006),
26.3% of the instances were annotated with multi-
ple senses. We also restricted the corpus to only
include polysemous nouns, or nouns which had an
additional sense beyond the senses assigned to it.
Inter-annotator agreement was used to validate
the corpus. Because the corpus was built by an
author of the work, we asked a non-author to re-
annotate the corpus without knowledge of the orig-
inal annotations. This second annotator was told to
choose all appropriate senses just as did the original
annotator. Agreement was calculated as:
</bodyText>
<equation confidence="0.645759">
agree = (1:
i∈C
</equation>
<bodyText confidence="0.999888">
where 51 and 52 are the two sets of sense annota-
tions, and i is an instance of the corpus, C.
The agreement and other data concerning corpus
annotation can be found in Table 4. As a point of
comparison, the Senseval 3 all-words task had a 75%
agreement on nouns (Snyder and Palmer, 2004). A
second evaluation of agreement was also done. The
non-author annotations were treated as if they came
</bodyText>
<footnote confidence="0.991572">
5available at: http://eecs.ucf.edu/˜hschwartz/CSK/
</footnote>
<table confidence="0.599117666666667">
� ÷ 342
|51i n 52i|
|51i U 52i|
</table>
<page confidence="0.903842">
7
</page>
<table confidence="0.9980535">
insts agree F1h F1rnd F1MFS
on 131 79.9 84.7 28.2 71.0
in 211 80.8 91.9 27.2 67.8
both 342 80.5 89.2 27.6 69.0
</table>
<tableCaption confidence="0.9924082">
Table 4: Experimental corpus data for each relation-
ship (on, in). insts: number of annotated instances;
agree: inter-annotator agreement %; F1 values (precision
= recall): h: human annotation, rnd: random baseline,
MFS: most frequent sense baseline.
</tableCaption>
<table confidence="0.999887166666667">
without CSK with CSK
F1all F1indeg F1all F1indeg
on 62.6 63.4 64.9 67.2
in 68.7 69.7 71.6 72.5
both 66.4 67.3 69.0 70.5
ties 37 0 66 72
</table>
<tableCaption confidence="0.878074">
Table 5: F1 values (precision = recall) on our experimen-
tal corpus with and without CSK. F1all: using all 4 graph
metrics; F1indeg: using only the indegree metric; ties:
number of instances where tie votes occurred.
</tableCaption>
<bodyText confidence="0.996305666666667">
from a disambiguation system in order to get a hu-
man upper-bound of performance. Just as the auto-
matic system handled tie votes, when one word had
multiple sense annotations, the annotation with the
lowest sense number was used. This performance
upper-bound is shown as F1h in Table 4.
</bodyText>
<subsectionHeader confidence="0.711301">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999992424242424">
Our disambiguation results are presented in Table
5. We found that, in all cases, including CSK im-
proved results. It turned out that 54.7% of the noun
instances received at least one suggestion from the
CSK, and 24.5% of the instances received multiple
suggestions. It is not clear why the on results were
slightly below that for in. We suspect the on por-
tion of the corpus was slightly more difficult be-
cause the human annotation (F1h) found a similar
phenomenon.
One observation we made when setting up the
test was that the indegree metric alone performed
slightly better than using the votes of all four met-
rics. This was not surprising considering Sinha and
Mihalcea found the indegree metric by itself to per-
form only slightly below a combination of metrics
on the senseval data (Sinha and Mihalcea, 2007).
Therefore, Table 5 also reports the use of the inde-
gree metric by itself or with CSK, F1indeg. In these
cases we saw the greatest improvements of using
CSK, producing an an error reduction of about 4.5%
and outperforming the F1MFS value.
Several additional experiments were performed.
Note that even during ties, the chosen sense was
taken from the predictions and suggestions. When
we instead incorporated an MFS backoff strategy for
ties, our top results for F1indeg with CSK dropped to
70.2. We also ran a precision test with no predictions
made for tie votes, and found a precision of 71.9%
on the 270 instances that did not have a tie for top
votes (this also used the indegree metric with CSK).
All results supported our goal of acquiring CSK that
was applicable to word sense disambiguation.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999997125">
We found our acquired CSK to be useful when incor-
porated into a word sense disambiguation system,
finding an error reduction of around 4.5% for top re-
sults. Relationships between nouns were acquired
from the Web through a unique search method of
filling constituents in a search phrase. Samples re-
turned from the Web were restricted by a require-
ment to match the syntactic parse of a web query.
The resulting data was analyzed over WordNet to
produce probabilities of relationships in the form of
conceptARnounB, where conceptA is a concept in
WordNet rather than an ambiguous noun.
In our effort to validate the knowledge through ap-
plication, many steps along the way were left open
for future investigations. First, there is a need to ex-
haustively search for CSK of all nouns and to acquire
other forms of CSK. With this improvement CSK
could be tested on a standard corpus, rather than
a corpus focused on select nouns and prepositional
phrases. Looking into acquisition improvements, a
study of the effectiveness of the parse would be ben-
eficial. Finally, the applicability of the knowledge
may be increased through a more complex concept
analysis or utilizing a more advanced voting scheme.
</bodyText>
<sectionHeader confidence="0.99934" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.993537333333333">
This research was supported by the NASA Engi-
neering and Safety Center under Grant/Cooperative
Agreement NNX08AJ98A.
</bodyText>
<page confidence="0.996989">
8
</page>
<sectionHeader confidence="0.995838" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828851851852">
Eneko Agirre, Olatz Ansa, and David Martinez. 2001.
Enriching wordnet concepts with topic signatures. In
In Proceedings of the NAACL workshop on WordNet
and Other Lexical Resources: Applications, Exten-
sions and Customizations, Pittsburg, USA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132–139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of Conference on Empirical
Methods in Natural Language Processing (EMNLP-
04), Barcelona, Spain.
Mona Diab. 2004. Relieving the data acquisition bottle-
neck in word sense disambiguation. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL’04), pages 303–310.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic rela-
tions between nominals. In Proceedings of SemEval-
2007, pages 13–18, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In In Proceedings of
the 14th International Conference on Computational
Linguistics (COLING-92), pages 539–545.
Nancy Ide and Yorick Wilks, 2006. Word Sense Dis-
ambiguation: Algorithms And Applications, chapter 3:
Making Sense About Sense. Springer.
Douglas B. Lenat. 1995. CYC: a large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33–38.
H. Liu and P Singh. 2004. Conceptnet: A practical com-
monsense reasoning toolkit. BT Technology Journal,
22:211–226.
David Martinez, Eneko Agirre, and Xinglong Wang.
2006. Word relatives in context for word sense dis-
ambiguation. In Proceedings of the 2006 Australasian
Language Technology Workshop, pages 42–50.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics,
pages 279–286, Barcelona, Spain, July. Association
for Computational Linguistics.
Rada Mihalcea. 2002. Bootstrapping large sense tagged
corpora. In Proceedings of the 3rd International
Conference on Languages Resources and Evaluations
LREC 2002, Las Palmas, Spain, May.
George Miller, R. Beckwith, Christiane Fellbaum,
D. Gross, and K. Miller. 1993. Five papers on word-
net. Technical report, Princeton University.
Kathy Panton, Cynthia Matuszek, Douglas Lenat, Dave
Schneider, Michael Witbrock, Nick Siegel, and Blake
Shepard. 2006. Common sense reasoning : From cyc
to intelligent assistant. In Y. Cai and J. Abascal, edi-
tors, Ambient Intelligence in Everyday Life, pages 1–
31.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammaer
of the English Language. Longman.
Philip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95–130.
Hansen A. Schwartz and Fernando Gomez. 2008. Ac-
quiring knowledge from the web to be used as selec-
tors for noun sense disambiguation. In CoNLL 2008:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 105–112,
Manchester, England, August.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. Irvine, CA, Septem-
ber.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In ACL Senseval-3 Workshop,
Barcelona, Spain, July.
</reference>
<page confidence="0.997114">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.955295">
<title confidence="0.99964">Acquiring Applicable Common Sense Knowledge from the Web</title>
<author confidence="0.997687">Hansen A Schwartz</author>
<author confidence="0.997687">Fernando</author>
<affiliation confidence="0.9998595">School of Electrical Engineering and Computer University of Central</affiliation>
<address confidence="0.982946">Orlando, FL 32816,</address>
<abstract confidence="0.998782523809523">In this paper, a framework for acquiring common sense knowledge from the Web is presented. Common sense knowledge includes information about the world that humans use in their everyday lives. To acquire this knowledge, relationships between nouns are retrieved by using search phrases with automatically filled constituents. Through empirical analysis of the acquired nouns over Word- Net, probabilities are produced for relationships between a concept and a word rather than between two words. A specific goal of our acquisition method is to acquire knowledge that can be successfully applied to NLP problems. We test the validity of the acquired knowledge by means of an application to the problem of word sense disambiguation. Results show that the knowledge can be used to improve the accuracy of a state of the art unsupervised disambiguation system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>David Martinez</author>
</authors>
<title>Enriching wordnet concepts with topic signatures. In</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations,</booktitle>
<location>Pittsburg, USA.</location>
<contexts>
<context position="9574" citStr="Agirre et al., 2001" startWordPosition="1589" endWordPosition="1592"> by filling constituents based on lists of words. The SemEval-2007 Task 4 presents a good overview of work in noun-noun relationships (Girju et al., 2007). Our work is related in that the relationships we acquire are between nominals, and in order to build their corpus Girju et al. queried the web with patterns like that of Hearst’s work (Hearst, 1992). The SemEval task was to choose or classify relationships, rather than acquire and apply relationships. Additionally, the relationship classes they use are not necessarily within the scope of common sense knowledge. Similar to our research, in (Agirre et al., 2001) knowledge is acquired about WordNet concepts. They find topics signatures, sets of related words, based on data from the Web and use them for word sense disambiguation. However, the type of relationship between words of a topic signature and the WordNet concept is not made explicit, and the authors find the topic signatures are not very effective for word sense disambiguation. Finally, we note one approach to using the Web for NLP applications is to acquire knowledge on the fly. Previous work has approached solutions to word sense disambiguation by acquiring words or phrases directly based on</context>
</contexts>
<marker>Agirre, Ansa, Martinez, 2001</marker>
<rawString>Eneko Agirre, Olatz Ansa, and David Martinez. 2001. Enriching wordnet concepts with topic signatures. In In Proceedings of the NAACL workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, Pittsburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="13845" citStr="Charniak, 2000" startWordPosition="2307" endWordPosition="2308">ch/ relation search phrase voice on, in nounA is located prep nounB passive nounA is found prep nounB nounA is situated prep nounB on, in nounA is prep nounB active put nounA prep nounB place nounA prep nounB lay nounA prep nounB set nounA prep nounB locate nounA prep nounB position nounA prep nounB on hang nounA prep nounB active mount nounA prep nounB attach nounA prep nounB Table 2: Search phrases and relationships used for acquisition of CSK. 3.1.3 Parse and Match The results we want to achieve in this step should describe a relationship: nounA is [in I on] nounB We use Charniak’s parser (Charniak, 2000) on both the web query and the results returned from the web in order to ensure accuracy. To demonstrate this process, we extend our example, “place * in the refrigerator”. First, we get a parse with * (nounA) represented as ‘something’. (VP (VB place) (NP (NN something)) (PP (IN in) (NP (DT the) (NN refrigerator)))) We now know the constituent(s) which replace ‘(NN something)’ will be our nounA. For example, in the following parse ‘batter’ is resolved as nounA. (S1 (S (NP (PRP He)) (VP (AUX was) (VP (VBN told) (S (VP (TO to) (VP (VB place) (NP (DT the) (JJ mixed) (NN batter)) (PP (IN in) (NP </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 132–139, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP04),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="7029" citStr="Chklovski and Pantel, 2004" startWordPosition="1183" endWordPosition="1186">er than CSK. The scope of definitions certainly do not provide all necessary information (such as keys are commonly kept in one’s pocket). We examine WordNet in particular because the hypernym relation has been developed extensively for nouns. The noun ontology is used in our work to help induce relationships involving concepts (senses of nouns) rather than just among words. This notion of inducing CSK among concepts, rather than words, is a key difference between our work and similar research. The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). They used patterns of phrases in order to search the Web for semantic relations among verbs. The knowledge they acquire falls into the category of CSK, but the specific relationships are different than ours in that they are among verb word forms and senses are not resolved. ConceptNet was created based on the OpenMind Commonsense project (Liu and Singh, 2004). The project acquired knowledge through an interface on the Web by having users play games and answer questions about words. A contribution of ConceptNet is that it has a wide range of relations. While WordNet provides connections betwe</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP04), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>Relieving the data acquisition bottleneck in word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04),</booktitle>
<pages>303--310</pages>
<contexts>
<context position="24892" citStr="Diab, 2004" startWordPosition="4226" endWordPosition="4227">dered. This same type of voting is used with and without the CSK suggestions. 4.2 Experimental Corpus A goal of our work was to acquire data which could be applied to NLP problems. We focus particularly on the difficult problem of word sense disambiguation. Due to the lack of sense tagged data, we were unable to find an annotated corpus with instances of all the nouns in Table 3 as prepositional complements. This was not surprising considering one of the reasons that minimally supervised approaches have become more popular is that they do not require hand-tagged training data (Mihalcea, 2002; Diab, 2004; McCarthy et al., 2004). We created a corpus from sentences in Wikipedia which contained the phrase “in|on det lemma”, where det is a determiner or possessive pronoun, lemma is a noun from Table 3, and in|on is a preposition for either relationship described earlier. Below we have provided an example from our corpus where the knowledge from ‘pocket’ can be applied to disambiguate ‘key’. Now Tony’s key to the fiat is in the pocket of his raincoat, so on returning to his fiat some time later he realizes that he cannot get inside. The corpus5 contained a total of 342 sentences, with one target n</context>
</contexts>
<marker>Diab, 2004</marker>
<rawString>Mona Diab. 2004. Relieving the data acquisition bottleneck in word sense disambiguation. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04), pages 303–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>Semeval-2007 task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval2007,</booktitle>
<pages>13--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9108" citStr="Girju et al., 2007" startWordPosition="1511" endWordPosition="1514">in meta data, while the third, logical layer, stores high level implicit meanings. Only a portion of CYC is available to the public. Our method for acquiring knowledge is somewhat similar to that of (Hearst, 1992). Patterns are built manually. However, we do not use our manually constructed patterns (referred to as search phrases) to query the Web. Instead the search phrases are abstract patterns that are used to automatically generate more specific web queries by filling constituents based on lists of words. The SemEval-2007 Task 4 presents a good overview of work in noun-noun relationships (Girju et al., 2007). Our work is related in that the relationships we acquire are between nominals, and in order to build their corpus Girju et al. queried the web with patterns like that of Hearst’s work (Hearst, 1992). The SemEval task was to choose or classify relationships, rather than acquire and apply relationships. Additionally, the relationship classes they use are not necessarily within the scope of common sense knowledge. Similar to our research, in (Agirre et al., 2001) knowledge is acquired about WordNet concepts. They find topics signatures, sets of related words, based on data from the Web and use </context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-2007 task 04: Classification of semantic relations between nominals. In Proceedings of SemEval2007, pages 13–18, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora. In</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92),</booktitle>
<pages>539--545</pages>
<contexts>
<context position="8702" citStr="Hearst, 1992" startWordPosition="1447" endWordPosition="1448">pts (WordNet synsets) in a relationship with a given nounB (word). A project in progress for over twenty years, CYC has been acquiring common sense knowledge about everyday objects and actions stored in 106 axioms (Lenat, 1995). The axioms, handcrafted by workers at CYCcorp, represent knowledge rooted in propositions. There are three layers of information: the first two, access and physical, contain meta data, while the third, logical layer, stores high level implicit meanings. Only a portion of CYC is available to the public. Our method for acquiring knowledge is somewhat similar to that of (Hearst, 1992). Patterns are built manually. However, we do not use our manually constructed patterns (referred to as search phrases) to query the Web. Instead the search phrases are abstract patterns that are used to automatically generate more specific web queries by filling constituents based on lists of words. The SemEval-2007 Task 4 presents a good overview of work in noun-noun relationships (Girju et al., 2007). Our work is related in that the relationships we acquire are between nominals, and in order to build their corpus Girju et al. queried the web with patterns like that of Hearst’s work (Hearst,</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Yorick Wilks</author>
</authors>
<title>Word Sense Disambiguation: Algorithms And Applications, chapter 3: Making Sense About Sense.</title>
<date>2006</date>
<publisher>Springer.</publisher>
<contexts>
<context position="25752" citStr="Ide and Wilks, 2006" startWordPosition="4373" endWordPosition="4376">ationship described earlier. Below we have provided an example from our corpus where the knowledge from ‘pocket’ can be applied to disambiguate ‘key’. Now Tony’s key to the fiat is in the pocket of his raincoat, so on returning to his fiat some time later he realizes that he cannot get inside. The corpus5 contained a total of 342 sentences, with one target noun annotated per sentence. The target nouns were selected to potentially fill the nounA constituent in the relationship nounARnounB, and they were assigned all appropriate WordNet 3.0 senses. Considering the finegrained nature of WordNet (Ide and Wilks, 2006), 26.3% of the instances were annotated with multiple senses. We also restricted the corpus to only include polysemous nouns, or nouns which had an additional sense beyond the senses assigned to it. Inter-annotator agreement was used to validate the corpus. Because the corpus was built by an author of the work, we asked a non-author to reannotate the corpus without knowledge of the original annotations. This second annotator was told to choose all appropriate senses just as did the original annotator. Agreement was calculated as: agree = (1: i∈C where 51 and 52 are the two sets of sense annota</context>
</contexts>
<marker>Ide, Wilks, 2006</marker>
<rawString>Nancy Ide and Yorick Wilks, 2006. Word Sense Disambiguation: Algorithms And Applications, chapter 3: Making Sense About Sense. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
</authors>
<title>CYC: a large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="3157" citStr="Lenat, 1995" startWordPosition="545" endWordPosition="546">similar to (2). In this case, it is difficult for a computer to determine if “he” is in the refrigerator eating an apple or if the “apple” which he ate was in the refrigerator. Like the previous example, the knowledge that food is commonly found in a refrigerator and people are not, leads us to understand that “in the refrigerator” should be attached to the noun phrase “the apple” and not as a modifier of the verb phrase “ate”. Unfortunately, there are not many sources of CSK readily available for use in computer algorithms. Those sets of knowledge that are available, such as the CYC project (Lenat, 1995) or ConceptNet (Liu and Singh, 2004) rely on manually provided or crafted data. Our aim is to develop an automatic approach to acquire CSK1 by turning to the vast amount of unannotated text that is available on the Web. In turn, we present a method to automatically retrieve and analyze phrases from the Web. 1data available at: http://eecs.ucf.edu/˜hschwartz/CSK/ Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 1–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We employ the use of a syntactic pa</context>
<context position="8316" citStr="Lenat, 1995" startWordPosition="1385" endWordPosition="1386">en word forms. 2 a chosen nounB search phrases for CSK web search parse and match Noun Acquisition create web queries Concept Analysis WordNet Ontology Word probabilities: nounA [injon] nounB determine concept probabilities concept probabilities: conceptA [injon] nounB Figure 1: The overall common sense knowledge acquisition framework under the assumption that one is acquiring concepts (WordNet synsets) in a relationship with a given nounB (word). A project in progress for over twenty years, CYC has been acquiring common sense knowledge about everyday objects and actions stored in 106 axioms (Lenat, 1995). The axioms, handcrafted by workers at CYCcorp, represent knowledge rooted in propositions. There are three layers of information: the first two, access and physical, contain meta data, while the third, logical layer, stores high level implicit meanings. Only a portion of CYC is available to the public. Our method for acquiring knowledge is somewhat similar to that of (Hearst, 1992). Patterns are built manually. However, we do not use our manually constructed patterns (referred to as search phrases) to query the Web. Instead the search phrases are abstract patterns that are used to automatica</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Douglas B. Lenat. 1995. CYC: a large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>P Singh</author>
</authors>
<title>Conceptnet: A practical commonsense reasoning toolkit.</title>
<date>2004</date>
<journal>BT Technology Journal,</journal>
<pages>22--211</pages>
<contexts>
<context position="3193" citStr="Liu and Singh, 2004" startWordPosition="549" endWordPosition="552">, it is difficult for a computer to determine if “he” is in the refrigerator eating an apple or if the “apple” which he ate was in the refrigerator. Like the previous example, the knowledge that food is commonly found in a refrigerator and people are not, leads us to understand that “in the refrigerator” should be attached to the noun phrase “the apple” and not as a modifier of the verb phrase “ate”. Unfortunately, there are not many sources of CSK readily available for use in computer algorithms. Those sets of knowledge that are available, such as the CYC project (Lenat, 1995) or ConceptNet (Liu and Singh, 2004) rely on manually provided or crafted data. Our aim is to develop an automatic approach to acquire CSK1 by turning to the vast amount of unannotated text that is available on the Web. In turn, we present a method to automatically retrieve and analyze phrases from the Web. 1data available at: http://eecs.ucf.edu/˜hschwartz/CSK/ Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 1–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We employ the use of a syntactic parser to accurately match syntactic p</context>
<context position="7392" citStr="Liu and Singh, 2004" startWordPosition="1244" endWordPosition="1247"> words. This notion of inducing CSK among concepts, rather than words, is a key difference between our work and similar research. The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). They used patterns of phrases in order to search the Web for semantic relations among verbs. The knowledge they acquire falls into the category of CSK, but the specific relationships are different than ours in that they are among verb word forms and senses are not resolved. ConceptNet was created based on the OpenMind Commonsense project (Liu and Singh, 2004). The project acquired knowledge through an interface on the Web by having users play games and answer questions about words. A contribution of ConceptNet is that it has a wide range of relations. While WordNet provides connections between concepts (senses of words), ConceptNet only provides relationships between word forms. 2 a chosen nounB search phrases for CSK web search parse and match Noun Acquisition create web queries Concept Analysis WordNet Ontology Word probabilities: nounA [injon] nounB determine concept probabilities concept probabilities: conceptA [injon] nounB Figure 1: The over</context>
</contexts>
<marker>Liu, Singh, 2004</marker>
<rawString>H. Liu and P Singh. 2004. Conceptnet: A practical commonsense reasoning toolkit. BT Technology Journal, 22:211–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Eneko Agirre</author>
<author>Xinglong Wang</author>
</authors>
<title>Word relatives in context for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Australasian Language Technology Workshop,</booktitle>
<pages>42--50</pages>
<contexts>
<context position="10240" citStr="Martinez et al., 2006" startWordPosition="1699" endWordPosition="1702">. They find topics signatures, sets of related words, based on data from the Web and use them for word sense disambiguation. However, the type of relationship between words of a topic signature and the WordNet concept is not made explicit, and the authors find the topic signatures are not very effective for word sense disambiguation. Finally, we note one approach to using the Web for NLP applications is to acquire knowledge on the fly. Previous work has approached solutions to word sense disambiguation by acquiring words or phrases directly based on the sentences or words being disambiguated (Martinez et al., 2006; Schwartz and Gomez, 2008). These methods dynamically acquire the data at runtime, rather than automatically create a common sense database of relations that is readily available. Additionally, in our current approach, we are able to acquire explicit CSK relationships. 3 Common Sense Acquisition The two major phases of our framework, “Noun Acquisition” and “Concept Analysis”, are outlined in Figure 1 and described within this section. 3.1 Noun Acquisition The first step of our method is to acquire nouns (as words) from the Web which are in a relationship with other nouns. A Web search is perf</context>
</contexts>
<marker>Martinez, Agirre, Wang, 2006</marker>
<rawString>David Martinez, Eneko Agirre, and Xinglong Wang. 2006. Word relatives in context for word sense disambiguation. In Proceedings of the 2006 Australasian Language Technology Workshop, pages 42–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>279--286</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona, Spain,</location>
<contexts>
<context position="24916" citStr="McCarthy et al., 2004" startWordPosition="4228" endWordPosition="4231">same type of voting is used with and without the CSK suggestions. 4.2 Experimental Corpus A goal of our work was to acquire data which could be applied to NLP problems. We focus particularly on the difficult problem of word sense disambiguation. Due to the lack of sense tagged data, we were unable to find an annotated corpus with instances of all the nouns in Table 3 as prepositional complements. This was not surprising considering one of the reasons that minimally supervised approaches have become more popular is that they do not require hand-tagged training data (Mihalcea, 2002; Diab, 2004; McCarthy et al., 2004). We created a corpus from sentences in Wikipedia which contained the phrase “in|on det lemma”, where det is a determiner or possessive pronoun, lemma is a noun from Table 3, and in|on is a preposition for either relationship described earlier. Below we have provided an example from our corpus where the knowledge from ‘pocket’ can be applied to disambiguate ‘key’. Now Tony’s key to the fiat is in the pocket of his raincoat, so on returning to his fiat some time later he realizes that he cannot get inside. The corpus5 contained a total of 342 sentences, with one target noun annotated per senten</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, pages 279–286, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Bootstrapping large sense tagged corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Languages Resources and Evaluations LREC 2002,</booktitle>
<location>Las Palmas, Spain,</location>
<contexts>
<context position="24880" citStr="Mihalcea, 2002" startWordPosition="4224" endWordPosition="4225">senses are considered. This same type of voting is used with and without the CSK suggestions. 4.2 Experimental Corpus A goal of our work was to acquire data which could be applied to NLP problems. We focus particularly on the difficult problem of word sense disambiguation. Due to the lack of sense tagged data, we were unable to find an annotated corpus with instances of all the nouns in Table 3 as prepositional complements. This was not surprising considering one of the reasons that minimally supervised approaches have become more popular is that they do not require hand-tagged training data (Mihalcea, 2002; Diab, 2004; McCarthy et al., 2004). We created a corpus from sentences in Wikipedia which contained the phrase “in|on det lemma”, where det is a determiner or possessive pronoun, lemma is a noun from Table 3, and in|on is a preposition for either relationship described earlier. Below we have provided an example from our corpus where the knowledge from ‘pocket’ can be applied to disambiguate ‘key’. Now Tony’s key to the fiat is in the pocket of his raincoat, so on returning to his fiat some time later he realizes that he cannot get inside. The corpus5 contained a total of 342 sentences, with </context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Rada Mihalcea. 2002. Bootstrapping large sense tagged corpora. In Proceedings of the 3rd International Conference on Languages Resources and Evaluations LREC 2002, Las Palmas, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>R Beckwith</author>
<author>Christiane Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Princeton University.</institution>
<contexts>
<context position="3890" citStr="Miller et al., 1993" startWordPosition="659" endWordPosition="662">c approach to acquire CSK1 by turning to the vast amount of unannotated text that is available on the Web. In turn, we present a method to automatically retrieve and analyze phrases from the Web. 1data available at: http://eecs.ucf.edu/˜hschwartz/CSK/ Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 1–9, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics We employ the use of a syntactic parser to accurately match syntactic patterns of phrases acquired from the Web. The data is analyzed over WordNet (Miller et al., 1993) in order to induce knowledge about word senses or concepts rather than words. Finally, we evaluate whether the knowledge by applying it to the problem of word sense disambiguation. 2 Background The particular type of CSK that we experiment with in this paper is described formally as follows: A relationship, e1Re2, exists between entities e1 and e2 if one finds “e1 is R e2.” Some examples include: “a cup is on a table” and “food is in a refrigerator”, which would result in relationships: cupontable and foodinrefrigerator. The next section attempts to make the relationship more clear, as we pro</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George Miller, R. Beckwith, Christiane Fellbaum, D. Gross, and K. Miller. 1993. Five papers on wordnet. Technical report, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathy Panton</author>
<author>Cynthia Matuszek</author>
<author>Douglas Lenat</author>
<author>Dave Schneider</author>
<author>Michael Witbrock</author>
<author>Nick Siegel</author>
<author>Blake Shepard</author>
</authors>
<title>Common sense reasoning : From cyc to intelligent assistant.</title>
<date>2006</date>
<booktitle>Ambient Intelligence in Everyday Life,</booktitle>
<pages>1--31</pages>
<editor>In Y. Cai and J. Abascal, editors,</editor>
<contexts>
<context position="1239" citStr="Panton et al. (2006)" startWordPosition="194" endWordPosition="197"> WordNet, probabilities are produced for relationships between a concept and a word rather than between two words. A specific goal of our acquisition method is to acquire knowledge that can be successfully applied to NLP problems. We test the validity of the acquired knowledge by means of an application to the problem of word sense disambiguation. Results show that the knowledge can be used to improve the accuracy of a state of the art unsupervised disambiguation system. 1 Introduction Common sense knowledge (CSK) is the knowledge we use in everyday life without necessarily being aware of it. Panton et al. (2006) of the Cyc project, define common sense as “the knowledge that every person assumes his neighbors also possess”. Although the term common sense may be understood as a process such as reasoning, we are referring only to knowledge. It is CSK that tells us keys are kept in one’s pocket and keys are used to open a door, but CSK does not hold that keys are kept in a kitchen sink or that keys are used to turn on a microwave, although all are possible. To show the need for this information more clearly we provide a couple sentences: 1 She put the batter in the refrigerator. (1) He ate the apple in t</context>
</contexts>
<marker>Panton, Matuszek, Lenat, Schneider, Witbrock, Siegel, Shepard, 2006</marker>
<rawString>Kathy Panton, Cynthia Matuszek, Douglas Lenat, Dave Schneider, Michael Witbrock, Nick Siegel, and Blake Shepard. 2006. Common sense reasoning : From cyc to intelligent assistant. In Y. Cai and J. Abascal, editors, Ambient Intelligence in Everyday Life, pages 1– 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammaer of the English Language. Longman.</journal>
<contexts>
<context position="4670" citStr="Quirk et al., 1985" startWordPosition="786" endWordPosition="789">ense disambiguation. 2 Background The particular type of CSK that we experiment with in this paper is described formally as follows: A relationship, e1Re2, exists between entities e1 and e2 if one finds “e1 is R e2.” Some examples include: “a cup is on a table” and “food is in a refrigerator”, which would result in relationships: cupontable and foodinrefrigerator. The next section attempts to make the relationship more clear, as we provide a brief linguistic background of prepositions and relationships. 2.1 Prepositions and Relationships Prepositions state a relationship between two entities (Quirk et al., 1985). One of the entities is typically a constituent of the sentence while the other is the complement to the preposition. For example, consider the relationship between ‘furniture’ and ‘house’ in the following sentences: The furniture is... ...at the house. ...on the house. ...in the house. ‘The furniture’ is the subject of the sentence, while ‘the house’ is a prepositional complement. Notice that the meaning is different for each sentence depending on the actual preposition (‘at’, ‘on’, or ‘in’), and thus furniture relates to house in three different ways. Although each relationship between furn</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammaer of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--95</pages>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hansen A Schwartz</author>
<author>Fernando Gomez</author>
</authors>
<title>Acquiring knowledge from the web to be used as selectors for noun sense disambiguation.</title>
<date>2008</date>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>105--112</pages>
<location>Manchester, England,</location>
<contexts>
<context position="10267" citStr="Schwartz and Gomez, 2008" startWordPosition="1703" endWordPosition="1706">atures, sets of related words, based on data from the Web and use them for word sense disambiguation. However, the type of relationship between words of a topic signature and the WordNet concept is not made explicit, and the authors find the topic signatures are not very effective for word sense disambiguation. Finally, we note one approach to using the Web for NLP applications is to acquire knowledge on the fly. Previous work has approached solutions to word sense disambiguation by acquiring words or phrases directly based on the sentences or words being disambiguated (Martinez et al., 2006; Schwartz and Gomez, 2008). These methods dynamically acquire the data at runtime, rather than automatically create a common sense database of relations that is readily available. Additionally, in our current approach, we are able to acquire explicit CSK relationships. 3 Common Sense Acquisition The two major phases of our framework, “Noun Acquisition” and “Concept Analysis”, are outlined in Figure 1 and described within this section. 3.1 Noun Acquisition The first step of our method is to acquire nouns (as words) from the Web which are in a relationship with other nouns. A Web search is performed in order to retrieve </context>
</contexts>
<marker>Schwartz, Gomez, 2008</marker>
<rawString>Hansen A. Schwartz and Fernando Gomez. 2008. Acquiring knowledge from the web to be used as selectors for noun sense disambiguation. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 105–112, Manchester, England, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised graph-based word sense disambiguation using measures of word semantic similarity.</title>
<date>2007</date>
<location>Irvine, CA,</location>
<contexts>
<context position="23011" citStr="Sinha and Mihalcea, 2007" startWordPosition="3893" endWordPosition="3897">te our knowledge into a state of the art “all-words” word sense disambiguation algorithm. These algorithms are considered unsupervised or 6 minimally supervised, because they do not require specific training data that is designed for instances of words in the testing data. In other words, these systems are designed to handle any word they come across. Our knowledge can supplement such a system, because the data can be acquired automatically for an unlimited number of nouns, assuming limitless web query restrictions. The basis of our disambiguation system is the publicly available GWSD system (Sinha and Mihalcea, 2007). Sinha and Mihalcea report higher results on the Senseval-2 and Senseval-3 datasets than any of the participating unsupervised system. Additionally, GWSD is compatible with WordNet 3.0 and its output made it easy to integrate our knowledge. Sense predictions from four different graph metrics are produced, and we are able to incorporate our knowledge as another prediction within a voting scheme. Considering the role of our knowledge as a reference, in some cases we would like the CSK to suggest multiple senses and in others none. For each target noun instance in the corpus, we lookup the P,(c,</context>
<context position="28604" citStr="Sinha and Mihalcea, 2007" startWordPosition="4864" endWordPosition="4867">gestion from the CSK, and 24.5% of the instances received multiple suggestions. It is not clear why the on results were slightly below that for in. We suspect the on portion of the corpus was slightly more difficult because the human annotation (F1h) found a similar phenomenon. One observation we made when setting up the test was that the indegree metric alone performed slightly better than using the votes of all four metrics. This was not surprising considering Sinha and Mihalcea found the indegree metric by itself to perform only slightly below a combination of metrics on the senseval data (Sinha and Mihalcea, 2007). Therefore, Table 5 also reports the use of the indegree metric by itself or with CSK, F1indeg. In these cases we saw the greatest improvements of using CSK, producing an an error reduction of about 4.5% and outperforming the F1MFS value. Several additional experiments were performed. Note that even during ties, the chosen sense was taken from the predictions and suggestions. When we instead incorporated an MFS backoff strategy for ties, our top results for F1indeg with CSK dropped to 70.2. We also ran a precision test with no predictions made for tie votes, and found a precision of 71.9% on </context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2007. Unsupervised graph-based word sense disambiguation using measures of word semantic similarity. Irvine, CA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task.</title>
<date>2004</date>
<booktitle>In ACL Senseval-3 Workshop,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="26591" citStr="Snyder and Palmer, 2004" startWordPosition="4520" endWordPosition="4523">agreement was used to validate the corpus. Because the corpus was built by an author of the work, we asked a non-author to reannotate the corpus without knowledge of the original annotations. This second annotator was told to choose all appropriate senses just as did the original annotator. Agreement was calculated as: agree = (1: i∈C where 51 and 52 are the two sets of sense annotations, and i is an instance of the corpus, C. The agreement and other data concerning corpus annotation can be found in Table 4. As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). A second evaluation of agreement was also done. The non-author annotations were treated as if they came 5available at: http://eecs.ucf.edu/˜hschwartz/CSK/ � ÷ 342 |51i n 52i| |51i U 52i| 7 insts agree F1h F1rnd F1MFS on 131 79.9 84.7 28.2 71.0 in 211 80.8 91.9 27.2 67.8 both 342 80.5 89.2 27.6 69.0 Table 4: Experimental corpus data for each relationship (on, in). insts: number of annotated instances; agree: inter-annotator agreement %; F1 values (precision = recall): h: human annotation, rnd: random baseline, MFS: most frequent sense baseline. without CSK with CSK F1all F1indeg F1all F1indeg</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English all-words task. In ACL Senseval-3 Workshop, Barcelona, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>