<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.479668">
Cross-lingual Opinion Analysis via Negative Transfer Detection
</title>
<author confidence="0.990264">
Lin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang1
</author>
<affiliation confidence="0.992568666666667">
1Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School,
Harbin Institute of Technology, Shenzhen 518055
2Department Of Computing, the Hong Kong Polytechnic University
</affiliation>
<email confidence="0.6967115">
guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn,
csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn
</email>
<sectionHeader confidence="0.971698" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919764705882">
Transfer learning has been used in opin-
ion analysis to make use of available lan-
guage resources for other resource scarce
languages. However, the cumulative
class noise in transfer learning adversely
affects performance when more training
data is used. In this paper, we propose a
novel method in transductive transfer
learning to identify noises through the
detection of negative transfers. Evalua-
tion on NLP&amp;CC 2013 cross-lingual
opinion analysis dataset shows that our
approach outperforms the state-of-the-art
systems. More significantly, our system
shows a monotonic increase trend in per-
formance improvement when more train-
ing data are used.
</bodyText>
<sectionHeader confidence="0.997461" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894510638298">
Mining opinions from text by identifying their
positive and negative polarities is an important
task and supervised learning methods have been
quite successful. However, supervised methods
require labeled samples for modeling and the
lack of sufficient training data is the performance
bottle-neck in opinion analysis especially for re-
source scarce languages. To solve this problem,
the transfer leaning method (Arnold et al., 2007)
have been used to make use of samples from a
resource rich source language to a resource
scarce target language, also known as cross lan-
guage opinion analysis (CLOA).
In transductive transfer learning (TTL) where
the source language has labeled data and the tar-
get language has only unlabeled data, an algo-
rithm needs to select samples from the unlabeled
target language as the training data and assign
them with class labels using some estimated con-
fidence. These labeled samples in the target lan-
guage, referred to as the transferred samples, also
have a probability of being misclassified. During
training iterations, the misclassification introduc-
es class noise which accumulates, resulting in a
so called negative transfer that affects the classi-
fication performance.
In this paper, we propose a novel method
aimed at reducing class noise for TTL in CLOA.
The basic idea is to utilize transferred samples
with high quality to identify those negative trans-
fers and remove them as class noise to reduce
noise accumulation in future training iterations.
Evaluations on NLP&amp;CC 2013 CLOA evalua-
tion data set show that our algorithm achieves the
best result, outperforming the current state-of-
the-art systems. More significantly, our system
shows a monotonic increasing trend in perfor-
mance when more training data are used beating
the performance degradation curse of most trans-
fer learning methods when training data reaches
certain size.
The rest of the paper is organized as follows.
Section 2 introduces related works in transfer
learning, cross lingual opinion analysis, and class
noise detection technology. Section 3 presents
our algorithm. Section 4 gives performance eval-
uation. Section 5 concludes this paper.
</bodyText>
<sectionHeader confidence="0.995367" genericHeader="introduction">
2 Related works
</sectionHeader>
<bodyText confidence="0.971037111111111">
TTL has been widely used before the formal
concept and definition of TTL was given in (Ar-
nold, 2007). Wan introduced the co-training
method into cross-lingual opinion analysis (Wan,
2009; Zhou et al., 2011), and Aue et al. intro-
duced transfer learning into cross domain analy-
sis (Aue, 2005) which solves similar problems.
In this paper, we will use the terms source lan-
guage and target language to refer to all cross
lingual/domain analysis.
Traditionally, transfer learning methods focus
on how to estimate the confidence score of trans-
ferred samples in the target language or domain
(Blitzer et al, 2006, Huang et al., 2007; Sugiya-
ma et al., 2008, Chen et al, 2011, Lu et al., 2011).
In some tasks, researchers utilize NLP tools such
as alignment to reduce the bias towards that of
860
</bodyText>
<subsectionHeader confidence="0.320794">
*Corresponding author
</subsectionHeader>
<bodyText confidence="0.970700212121212">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
the source language in transfer learning (Meng et
al., 2012). However, detecting misclassification
in transferred samples (referred to as class noise)
and reducing negative transfers are still an unre-
solved problem.
There are two basic methods for class noise
detection in machine learning. The first is the
classification based method (Brodley and Friedl,
1999; Zhu et al, 2003; Zhu 2004; Sluban et al.,
2010) and the second is the graph based method
(Zighed et al, 2002; Muhlenbach et al, 2004;
Jiang and Zhou, 2004). Class noise detection can
also be applied to semi-supervised learning be-
cause noise can accumulate in iterations too. Li
employed Zighed’s cut edge weight statistic
method in self-training (Li and Zhou, 2005) and
co-training (Li and Zhou, 2011). Chao used Li’s
method in tri-training (Chao et al, 2008). (Fuku-
moto et al, 2013) used the support vectors to de-
tect class noise in semi-supervised learning.
In TTL, however, training and testing samples
cannot be assumed to have the same distributions.
Thus, noise detection methods used in semi-
supervised learning are not directly suited in
TTL. Y. Cheng has tried to use semi-supervised
method (Jiang and Zhou, 2004) in transfer learn-
ing (Cheng and Li, 2009). His experiment
showed that their approach would work when the
source domain and the target domain share simi-
lar distributions. How to reduce negative trans-
fers is still a problem in transfer learning.
</bodyText>
<sectionHeader confidence="0.971927" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999989533333333">
In order to reduce negative transfers, we pro-
pose to incorporate class noise detection into
TTL. The basic idea is to first select high quality
labeled samples after certain iterations as indica-
tor to detect class noise in transferred samples.
We then remove noisy samples that cause nega-
tive transfers from the current accumulated train-
ing set to retain an improved set of training data
for the remainder of the training phase. This neg-
ative sample reduction process can be repeated
several times during transfer learning. Two ques-
tions must be answered in this approach: (1) how
to measure the quality of transferred samples,
and (2) how to utilize high quality labeled sam-
ples to detect class noise in training data.
</bodyText>
<subsectionHeader confidence="0.99998">
3.1 Estimating Testing Error
</subsectionHeader>
<bodyText confidence="0.999991272727273">
To determine the quality of the transferred
samples that are added iteratively in the learning
process, we cannot use training error to estimate
true error because the training data and the test-
ing data have different distributions. In this work,
we employ the Probably Approximately Correct
(PAC) learning theory to estimate the error
boundary. According to the PAC learning theory,
the least error boundary ε is determined by the
size of the training set m and the class noise rate
il, bound by the following relation:
</bodyText>
<equation confidence="0.576805">
E a �1/m( 1 —71) Z ( 1)
</equation>
<bodyText confidence="0.999935357142857">
In TTL, m increases linearly, yet il is multi-
plied in each iteration. This means the signifi-
cance of m to performance is higher at the begin-
ning of transfer learning and gradually slows
down in later iterations. On the contrary, the in-
fluence of class noise increases. That is why per-
formance improves initially and gradually falls to
negative transfer when noise accumulation out-
performs the learned information as shown in
Fig.1. In TTL, transferred samples in both the
training data and test data have the same distribu-
tion. This implies that we can apply the PAC
theory to analyze the error boundary of the ma-
chine learning model using transferred data.
</bodyText>
<figureCaption confidence="0.731692">
Figure 1 Negative transfer in the learning process
</figureCaption>
<bodyText confidence="0.999306333333333">
According to PAC theorem with an assumed
fixed probability 6 (Angluin and Laird, 1988),
the least error boundary ε is given by:
</bodyText>
<equation confidence="0.993426">
E _ �2 ln(2N/6) /(m( 1 — 71) 2) (2)
</equation>
<bodyText confidence="0.9999135">
where I is a constant decided by the hypothesis
space. In any iteration during TTL, the hypothe-
sis space is the same and the probability 6 is
fixed. Thus the least error boundary is deter-
mined by the size of the transferred sample m
and the class noise of transferred samples il. Ac-
cording to (2), we apply a manifold assumption
based method to estimate il. Let T be the number
of iterations to serve as one period. We then es-
timate the least error boundary before and after
each T to measure the quality of transferred sam-
ples during each T. If the least error boundary is
reduced, it means that transferred samples used
in this period are of high quality and can improve
the performance. Otherwise, the transfer learning
algorithm should stop.
</bodyText>
<page confidence="0.572559">
861
</page>
<bodyText confidence="0.999325083333333">
According to the manifold assumption, the
conditional probability P (yi I xi) can be approxi-
mated by the frequency of P(yi = yj) which is
equal to P (Iij = 0). In opinion annotations, the
agreement of two annotators is often no larger
than 0.8. This means that for the best cases
P (Ii j = 1) =0.2. Hence Ii j follows a Bernoulli
distribution with p=0.2 for the best cases in
manual annotations.
Let Cij = {(xj, yj) } be the vertices that are
connected to the ith vertex, the statistical magni-
tude of the ith vertex can be defined as:
</bodyText>
<equation confidence="0.9899105">
J = z Wij-jIij ( 5)
i
</equation>
<bodyText confidence="0.9997925">
where j refers to the j th vertex that is connected
to the ith vertex.
From the theory of cut edge statics, we know
that the expectation of Ji is:
</bodyText>
<equation confidence="0.985436">
l&apos; = P(Iij=1) *z jWij ( 6)
i
</equation>
<bodyText confidence="0.722478">
And the variance of Ji is:
</bodyText>
<equation confidence="0.906113857142857">
Q 2 = P(Iij=0) P(Iij=1) *z Wij2j( 7)
i
By the Center Limit Theorem (CLT), Ji fol-
lows the normal distribution:
( i—tti) — 1V( 0,1)
(t—µi)
1 IrN/27cvi
</equation>
<bodyText confidence="0.999877166666667">
Note that experiments (Li and Zhou, 2011;
Cheng and Li, 2009; Brodley and Friedl, 1999)
have shown that pi is related to the error rate of
the example (xi, yi), but it does not reflect the
ground-truth probability in statistics. Hence we
assume the class noise rate of example (xi, yi) is:
</bodyText>
<equation confidence="0.86918">
Il = 1 p ( 10)
i i
</equation>
<bodyText confidence="0.969888235294118">
We take the general significant level of 0.05
to reject the null hypothesis. It means that if Ili of
(xi, yi) is larger than 0.95, the sample will be
considered as a class noisy sample. Furthermore,
Il can be used to estimate the average class noise
i
rate of a transferred samples in (2).
In our proposed approach, we establish the
quality estimate period T to conduct class noise
detection to estimate the class noise rate of trans-
ferred samples. Based on the average class noise
we can get the least error boundary so as to tell if
an added sample is of high quality. If the newly
added samples are of high quality, they can be
used to detect class noise in transferred training
data. Otherwise, transfer learning should stop.
The flow chart for negative transfer is in Fig.2.
</bodyText>
<figureCaption confidence="0.530815">
Figure 2 Flow charts of negative transfer detection
</figureCaption>
<bodyText confidence="0.9999553">
In the above flow chart, SLS and TLS refer to
the source and target language samples, respec-
tively. TS refers to the transferred samples. Let T
denote quality estimate period T in terms of itera-
tion numbers. The transfer process select k sam-
ples in each iteration. When one period of trans-
fer process finishes, the negative transfer detec-
tion will estimate the quality by comparing and
either select the new transferred samples or re-
move class noise accumulated up to this iteration.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998546">
4.1 Experiment Setting
</subsectionHeader>
<bodyText confidence="0.977413">
The proposed approach is evaluated on the
NLP&amp;CC 2013 cross-lingual opinion analysis (in
</bodyText>
<figure confidence="0.993091466666667">
Output SLS and TS
(period 1 to n-1)
Input
Input
SLS(labeled)
TLS
(unlabeled)
Classifier
TS
period 1
T iterations per period
TS
period 2
TS
period n
Top k
Transfer
process
Negative
transfer
detection
Yes
KNN
graph
tn &lt; tn-1?
No
Delete TS
qi≥ 0.95
period 1 to n-1
Estimate qi and tn
</figure>
<subsectionHeader confidence="0.769982">
3.2 Estimating Class Noise
</subsectionHeader>
<bodyText confidence="0.99917625">
For formula (2) to work, we need to know the
class noise rate η to calculate the error boundary.
Obviously, we cannot use conditional probabili-
ties from the training data in the source language
to estimate the noise rate η of the transferred
samples because the distribution of source lan-
guage is different from that of target language.
Consider a KNN graph on the transferred
samples using any similarity metric, for example,
cosine similarity, for any two connected vertex
(xi,yi)and (xj, yj) in the graph from samples to
classes, the edge weight is given by:
</bodyText>
<equation confidence="0.8769037">
W = s m(xi,xj) ( 3)
ij
Furthermore, a sign function for the two vertices
(xi, yi)and (xj, yj), is defined as:
10, if yi = yj
(4)
1, if
I =
i j
Yi�Yj
</equation>
<bodyText confidence="0.929248666666667">
vi
To detect the noise rate of a sample (xi, yi) ,
we can use (8) as the null hypothesis to test the
significant level. Let pi denotes probability of
the correct classification for a transferred sample.
p should follow a normal distribution,
</bodyText>
<equation confidence="0.447331">
i
862
</equation>
<bodyText confidence="0.9993691875">
short, NLP&amp;CC) dataset1. In the training set,
there are 12,000 labeled English Amazon.com
products reviews, denoted by Train_ENG, and
120 labeled Chinese product reviews, denoted as
Train_CHN, from three categories, DVD, BOOK,
MUSIC. 94,651 unlabeled Chinese products re-
views from corresponding categories are used as
the development set, denoted as Dev_CHN. In
the testing set, there are 12,000 Chinese product
reviews (shown in Table.1). This dataset is de-
signed to evaluate the CLOA algorithm which
uses Train_CHN, Train_ENG and Dev_CHN to
train a classifier for Test_CHN. The performance
is evaluated by the correct classification accuracy
for each category in Test_CHN2:
where c is either DVD, BOOK or MUSIC.
</bodyText>
<table confidence="0.9217742">
Team DVD Book Music
Train_CHN 40 40 40
Train_ENG 4000 4000 4000
Dev_CHN 17814 47071 29677
Test CHN 4000 4000 4000
</table>
<tableCaption confidence="0.523944">
Table.1 The NLP&amp;CC 2013 CLOA dataset
</tableCaption>
<bodyText confidence="0.999915285714286">
In the experiment, the basic transfer learning
algorithm is co-training. The Chinese word seg-
mentation tool is ICTCLAS (Zhang et al, 2003)
and Google Translator3 is the MT for the source
language. The monolingual opinion classifier is
SVMlight4, word unigram/bigram features are em-
ployed.
</bodyText>
<subsectionHeader confidence="0.935445">
4.2 CLOA Experiment Results
</subsectionHeader>
<bodyText confidence="0.9993154">
Firstly, we evaluate the baseline systems
which use the same monolingual opinion classi-
fier with three training dataset including
Train_CHN, translated Train_ENG and their un-
ion, respectively.
</bodyText>
<table confidence="0.9984276">
DVD Book Music Accuracy
Train_CHN 0.552 0.513 0.500 0.522
Train_ENG 0.729 0.733 0.722 0.728
Train_CHN 0.737 0.722 0.742 0.734
+Train_ENG
</table>
<tableCaption confidence="0.510972">
Table.2 Baseline performances
</tableCaption>
<bodyText confidence="0.9942958">
It can be seen that using the same method, the
classifier trained by Train_CHN are on avergage
20% worse than the English counter parts.The
combined use of Train_CHN and translated
Train_ENG, however, obtained similar
</bodyText>
<footnote confidence="0.999723">
1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip
2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf
3https://translate.google.com
4http://svmlight.joachims.org/
</footnote>
<bodyText confidence="0.999698263157895">
performance to the English counter parts. This
means the predominant training comes from the
English training data.
In the second set of experiment, we compare
our proposed approach to the official results in
NLP&amp;CC 2013 CLOA evaluation and the result
is given in Table 3. Note that in Table 3, the top
performer of NLP&amp;CC 2013 CLOA evaluation
is the HLT-HITSZ system(underscored in the
table), which used the co-training method in
transfer learning (Gui et al, 2013), proving that
co-training is quite effective for cross-lingual
analysis. With the additional negative transfer
detection, our proposed approach achieves the
best performance on this dataset outperformed
the top system (by HLT-HITSZ) by a 2.97%
which translate to 13.1% error reduction im-
provement to this state-of-the-art system as
shown in the last row of Table 3.
</bodyText>
<table confidence="0.986650666666667">
Team DVD Book Music Accuracy
BUAA 0.481 0.498 0.503 0.494
BISTU 0.647 0.598 0.661 0.635
HLT-HITSZ 0.777 0.785 0.751 0.771
THUIR 0.739 0.742 0.733 0.738
SJTU 0.772 0.724 0.745 0.747
WHU 0.783 0.770 0.760 0.771
Our approach 0.816 0.801 0.786 0.801
Error
Reduction 0.152 0.072 0.110 0.131
Table.3 Performance compares with NLP&amp;CC
2013 CLOA evaluation results
</table>
<bodyText confidence="0.999306">
To further investigate the effectiveness of our
method, the third set of experiments evaluate the
negative transfer detection (NTD) compared to
co-training (CO) without negative transfer
detection as shown in Table.4 and Fig.3 Here, we
use the union of Train_CHN and Train_ENG as
labeled data and Dev_CHN as unlabeled data to
be transferred in the learning algorithms.
</bodyText>
<table confidence="0.998469428571429">
DVD Book Music Mean
Best case 0.816 0.801 0.786 0.801
NTD Best period 0.809 0.798 0.782 0.796
Mean 0.805 0.795 0.781 0.794
Best case 0.804 0.796 0.783 0.794
CO Best period 0.803 0.794 0.781 0.792
Mean 0.797 0.790 0.775 0.787
</table>
<tableCaption confidence="0.652157">
Table.4 CLOA performances
</tableCaption>
<bodyText confidence="0.999420714285714">
Taking all categories of data, our proposed
method improves the overall average precision
(the best cases) from 79.4% to 80.1% when
compared to the state of the art system which
translates to error reduction of 3.40% (p-
value≤0.01 in Wilcoxon signed rank test). Alt-
hough the improvement does not seem large, our
</bodyText>
<page confidence="0.815102">
863
</page>
<note confidence="0.825072">
DVD Book Music
</note>
<figureCaption confidence="0.964997">
Figure 3 Performance of negative transfer detection vs. co-training
</figureCaption>
<bodyText confidence="0.997555142857143">
algorithm shows a different behavior in that it
can continue to make use of available training
data to improve the system performance. In other
words, we do not need to identify the tipping
point where the performance degradation can
occur when more training samples are used. Our
approach has also shown the advantage of stable
improvement.
In the most practical tasks, co-training based
approach has the difficulty to determine when to
stop the training process because of the negative
transfer. And thus, there is no sure way to obtain
the above best average precision. On the contrary,
the performance of our proposed approach keeps
stable improvement with more iterations, i.e. our
approach has a much better chance to ensure the
best performance. Another experiment is con-
ducted to compare the performance of our pro-
posed transfer learning based approach with su-
pervised learning. Here, the achieved perfor-
mance of 3-folder cross validation are given in
</bodyText>
<tableCaption confidence="0.757127">
Table 5.
</tableCaption>
<table confidence="0.985568333333333">
DVD Book Music Average
Supervised 0.833 0.800 0.801 0.811
Our approach 0.816 0.801 0.786 0.801
</table>
<tableCaption confidence="0.552873">
Table.5 Comparison with supervised learning
</tableCaption>
<bodyText confidence="0.99994655">
The accuracy of our approach is only 1.0%
lower than the supervised learning using 2/3 of
Test_CHN. In the BOOK subset, our approach
achieves match result. Note that the performance
gap in different subsets shows positive correla-
tion to the size of Dev_CHN. The more samples
are given in Dev_CHN, a higher precision is
achieved even though these samples are unla-
beled. According to the theorem of PAC, we
know that the accuracy of a classifier training
from a large enough training set with confined
class noise rate will approximate the accuracy of
classifier training from a non-class noise training
set. This experiment shows that our proposed
negative transfer detection controls the class
noise rate in a very limited boundary. Theoreti-
cally speaking, it can catch up with the perfor-
mance of supervised learning if enough unla-
beled samples are available. In fact, such an ad-
vantage is the essence of our proposed approach.
</bodyText>
<sectionHeader confidence="0.99774" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999964045454545">
In this paper, we propose a negative transfer
detection approach for transfer learning method
in order to handle cumulative class noise and
reduce negative transfer in the process of transfer
learning. The basic idea is to utilize high quality
samples after transfer learning to detect class
noise in transferred samples. We take cross lin-
gual opinion analysis as the data set to evaluate
our method. Experiments show that our proposed
approach obtains a more stable performance im-
provement by reducing negative transfers. Our
approach reduced 13.1% errors than the top sys-
tem on the NLP&amp;CC 2013 CLOA evaluation
dataset. In BOOK category it even achieves bet-
ter result than the supervised learning. Experi-
mental results also show that our approach can
obtain better performance when the transferred
samples are added incrementally, which in pre-
vious works would decrease the system perfor-
mance. In future work, we plan to extend this
method into other language/domain resources to
identify more transferred samples.
</bodyText>
<sectionHeader confidence="0.993813" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<reference confidence="0.976387583333333">
This research is supported by NSFC 61203378,
61300112, 61370165, Natural Science Founda-
tion of GuangDong S2013010014475, MOE
Specialized Research Fund for the Doctoral Pro-
gram of Higher Education 20122302120070,
Open Projects Program of National Laboratory
of Pattern Recognition,Shenzhen Foundational
Research Funding JCYJ20120613152557576,
JC201005260118A, Shenzhen International Co-
operation Research Funding
GJHZ20120613110641217 and Hong Kong Pol-
ytechnic University Project code Z0EP.
</reference>
<page confidence="0.938869">
864
</page>
<sectionHeader confidence="0.937296" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999948034090909">
Angluin, D., Laird, P. 1988. Learning from Noisy
Examples. Machine Learning, 2(4): 343-370.
Arnold, A., Nallapati, R., Cohen, W. W. 2007. A
Comparative Study of Methods for Transductive
Transfer Learning. In Proc. 7th IEEE ICDM Work-
shops, pages 77-82.
Aue, A., Gamon, M. 2005. Customizing Sentiment
Classifiers to New Domains: a Case Study, In Proc.
of t RANLP.
Blitzer, J., McDonald, R., Pereira, F. 2006. Domain
Adaptation with Structural Correspondence Learn-
ing. In Proc. EMNLP, 120-128.
Brodley, C. E., Friedl, M. A. 1999. Identifying and
Eliminating Mislabeled Training Instances. Journal
of Artificial Intelligence Research, 11:131-167.
Chao, D., Guo, M. Z., Liu, Y., Li, H. F. 2008. Partic-
ipatory Learning based Semi-supervised Classifica-
tion. In Proc. of 4th ICNC, pages 207-216.
Cheng, Y., Li, Q. Y. 2009. Transfer Learning with
Data Edit. LNAI, pages 427–434.
Chen, M., Weinberger, K. Q., Blitzer, J. C. 2011.
Co-Training for Domain Adaptation. In Proc. of
23th NIPS.
Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text
Classification from Positive and Unlabeled Data
using Misclassified Data Correction. In Proc. of
51st ACL, pages 474-478.
Gui, L., Xu, R., Xu, J., et al. 2013. A Mixed Model
for Cross Lingual Opinion Analysis. In CCIS, 400,
pages 93-104.
Huang, J., Smola, A., Gretton, A., Borgwardt, K.M.,
Scholkopf, B. 2007. Correcting Sample Selection
Bias by Unlabeled Data. In Proc. of 19th NIPS,
pages 601-608.
Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for
kNN Classifiers with Neural Network Ensemble. In
LNCS, 3173, pages 356-361.
Li, M., Zhou, Z. H. 2005. SETRED: Self-Training
with Editing. In Proc. of PAKDD, pages 611-621.
Li, M., Zhou, Z. H. 2011. COTRADE: Confident Co-
Training With Data Editing. IEEE Transactions on
Systems, Man, and Cybernetics—Part B: Cyber-
netics, 41(6):1612-1627.
Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011.
Joint Bilingual Sentiment Classification with Un-
labeled Parallel Corpora. In Proc. of 49th ACL,
pages 320-330.
Meng, X. F., Wei, F. R., Liu, X. H., et al. 2012.
Cross-Lingual Mixture Model for Sentiment Clas-
sification. In Proc. of 50th ACL, pages 572-581.
Muhlenbach, F., Lallich, S., Zighed, D. A. 2004.
Identifying and Handling Mislabeled Instances.
Journal of Intelligent Information System, 22(1):
89-109.
Pan, S. J., Yang, Q. 2010. A Survey on Transfer
Learning, IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345-1360.
Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for
Multi-view Learning and Manifold Co-
Regularization. In Proc. of 25th ICML, pages 976–
983.
Sluban, B., Gamberger, D., Lavra, N. 2010. Advanc-
es in Class Noise Detection. In Proc.19th ECAI,
pages 1105-1106.
Sugiyama, M., Nakajima, S., Kashima, H., Buenau,
P.V., Kawanabe, M. 2008. Direct Importance Es-
timation with Model Selection and its Application
to Covariate Shift Adaptation. In Proc. 20th NIPS.
Wan, X. 2009. Co-Training for Cross-Lingual Senti-
ment Classification, In Proc. of the 47th Annual
Meeting of the ACL and the 4th IJCNLP of the
AFNLP, 235–243.
Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q.
2003. HHMM-based Chinese Lexical Analyzer
ICTCLAS. In 2nd SIGHAN workshop affiliated
with 41th ACL, pages 184-187.
Zhou, X., Wan X., Xiao, J. 2011. Cross-Language
Opinion Target Extraction in Review Texts. In
Proc. of IEEE 12th ICDM, pages 1200-1205.
Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003. Eliminating
Class Noise in Large Datasets. In Proc. of 12th
ICML, pages 920-927.
Zhu, X. Q. 2004. Cost-guided Class Noise Handling
for Effective Cost-sensitive Learning In Proc. of 4th
IEEE ICDM, pages 297-304.
Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.
Separability Index in Supervised Learning. In Proc.
of PKDD, pages 475-487.
</reference>
<page confidence="0.949605">
865
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.325861">
<title confidence="0.999395">Cross-lingual Opinion Analysis via Negative Transfer Detection</title>
<author confidence="0.970824">Ruifeng Qin Jun Jian Bin Xiaolong</author>
<affiliation confidence="0.990429">Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate Harbin Institute of Technology, Shenzhen</affiliation>
<address confidence="0.504702">Of Computing, the Hong Kong Polytechnic</address>
<email confidence="0.7164675">guilin.nlp@gmail.com,xuruifeng@hitsz.edu.cn,csluqin@comp.polyu.edu.hk,csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn</email>
<abstract confidence="0.999463055555555">Transfer learning has been used in opinion analysis to make use of available language resources for other resource scarce languages. However, the cumulative class noise in transfer learning adversely affects performance when more training data is used. In this paper, we propose a novel method in transductive transfer learning to identify noises through the detection of negative transfers. Evaluation on NLP&amp;CC 2013 cross-lingual opinion analysis dataset shows that our approach outperforms the state-of-the-art systems. More significantly, our system shows a monotonic increase trend in performance improvement when more training data are used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This research is supported by NSFC 61203378, 61300112, 61370165, Natural Science Foundation of GuangDong S2013010014475, MOE Specialized Research Fund for the Doctoral Program of</title>
<booktitle>Higher Education 20122302120070, Open Projects Program of National Laboratory of Pattern Recognition,Shenzhen Foundational Research Funding JCYJ20120613152557576, JC201005260118A, Shenzhen International Cooperation Research Funding GJHZ20120613110641217 and Hong Kong Polytechnic University Project code Z0EP.</booktitle>
<marker></marker>
<rawString>This research is supported by NSFC 61203378, 61300112, 61370165, Natural Science Foundation of GuangDong S2013010014475, MOE Specialized Research Fund for the Doctoral Program of Higher Education 20122302120070, Open Projects Program of National Laboratory of Pattern Recognition,Shenzhen Foundational Research Funding JCYJ20120613152557576, JC201005260118A, Shenzhen International Cooperation Research Funding GJHZ20120613110641217 and Hong Kong Polytechnic University Project code Z0EP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Angluin</author>
<author>P Laird</author>
</authors>
<title>Learning from Noisy Examples.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<issue>4</issue>
<pages>343--370</pages>
<contexts>
<context position="7883" citStr="Angluin and Laird, 1988" startWordPosition="1235" endWordPosition="1238">radually slows down in later iterations. On the contrary, the influence of class noise increases. That is why performance improves initially and gradually falls to negative transfer when noise accumulation outperforms the learned information as shown in Fig.1. In TTL, transferred samples in both the training data and test data have the same distribution. This implies that we can apply the PAC theory to analyze the error boundary of the machine learning model using transferred data. Figure 1 Negative transfer in the learning process According to PAC theorem with an assumed fixed probability 6 (Angluin and Laird, 1988), the least error boundary ε is given by: E _ �2 ln(2N/6) /(m( 1 — 71) 2) (2) where I is a constant decided by the hypothesis space. In any iteration during TTL, the hypothesis space is the same and the probability 6 is fixed. Thus the least error boundary is determined by the size of the transferred sample m and the class noise of transferred samples il. According to (2), we apply a manifold assumption based method to estimate il. Let T be the number of iterations to serve as one period. We then estimate the least error boundary before and after each T to measure the quality of transferred sa</context>
</contexts>
<marker>Angluin, Laird, 1988</marker>
<rawString>Angluin, D., Laird, P. 1988. Learning from Noisy Examples. Machine Learning, 2(4): 343-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Arnold</author>
<author>R Nallapati</author>
<author>W W Cohen</author>
</authors>
<title>A Comparative Study of Methods for Transductive Transfer Learning.</title>
<date>2007</date>
<booktitle>In Proc. 7th IEEE ICDM Workshops,</booktitle>
<pages>77--82</pages>
<contexts>
<context position="1592" citStr="Arnold et al., 2007" startWordPosition="209" endWordPosition="212">roach outperforms the state-of-the-art systems. More significantly, our system shows a monotonic increase trend in performance improvement when more training data are used. 1 Introduction Mining opinions from text by identifying their positive and negative polarities is an important task and supervised learning methods have been quite successful. However, supervised methods require labeled samples for modeling and the lack of sufficient training data is the performance bottle-neck in opinion analysis especially for resource scarce languages. To solve this problem, the transfer leaning method (Arnold et al., 2007) have been used to make use of samples from a resource rich source language to a resource scarce target language, also known as cross language opinion analysis (CLOA). In transductive transfer learning (TTL) where the source language has labeled data and the target language has only unlabeled data, an algorithm needs to select samples from the unlabeled target language as the training data and assign them with class labels using some estimated confidence. These labeled samples in the target language, referred to as the transferred samples, also have a probability of being misclassified. During</context>
</contexts>
<marker>Arnold, Nallapati, Cohen, 2007</marker>
<rawString>Arnold, A., Nallapati, R., Cohen, W. W. 2007. A Comparative Study of Methods for Transductive Transfer Learning. In Proc. 7th IEEE ICDM Workshops, pages 77-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Aue</author>
<author>M Gamon</author>
</authors>
<title>Customizing Sentiment Classifiers to New Domains: a Case Study,</title>
<date>2005</date>
<booktitle>In Proc. of t RANLP.</booktitle>
<marker>Aue, Gamon, 2005</marker>
<rawString>Aue, A., Gamon, M. 2005. Customizing Sentiment Classifiers to New Domains: a Case Study, In Proc. of t RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Domain Adaptation with Structural Correspondence Learning.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="3934" citStr="Blitzer et al, 2006" startWordPosition="582" endWordPosition="585">Related works TTL has been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers a</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>Blitzer, J., McDonald, R., Pereira, F. 2006. Domain Adaptation with Structural Correspondence Learning. In Proc. EMNLP, 120-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Brodley</author>
<author>M A Friedl</author>
</authors>
<title>Identifying and Eliminating Mislabeled Training Instances.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--131</pages>
<contexts>
<context position="4711" citStr="Brodley and Friedl, 1999" startWordPosition="700" endWordPosition="703"> the bias towards that of 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing </context>
<context position="9707" citStr="Brodley and Friedl, 1999" startWordPosition="1585" endWordPosition="1588">s in manual annotations. Let Cij = {(xj, yj) } be the vertices that are connected to the ith vertex, the statistical magnitude of the ith vertex can be defined as: J = z Wij-jIij ( 5) i where j refers to the j th vertex that is connected to the ith vertex. From the theory of cut edge statics, we know that the expectation of Ji is: l&apos; = P(Iij=1) *z jWij ( 6) i And the variance of Ji is: Q 2 = P(Iij=0) P(Iij=1) *z Wij2j( 7) i By the Center Limit Theorem (CLT), Ji follows the normal distribution: ( i—tti) — 1V( 0,1) (t—µi) 1 IrN/27cvi Note that experiments (Li and Zhou, 2011; Cheng and Li, 2009; Brodley and Friedl, 1999) have shown that pi is related to the error rate of the example (xi, yi), but it does not reflect the ground-truth probability in statistics. Hence we assume the class noise rate of example (xi, yi) is: Il = 1 p ( 10) i i We take the general significant level of 0.05 to reject the null hypothesis. It means that if Ili of (xi, yi) is larger than 0.95, the sample will be considered as a class noisy sample. Furthermore, Il can be used to estimate the average class noise i rate of a transferred samples in (2). In our proposed approach, we establish the quality estimate period T to conduct class no</context>
</contexts>
<marker>Brodley, Friedl, 1999</marker>
<rawString>Brodley, C. E., Friedl, M. A. 1999. Identifying and Eliminating Mislabeled Training Instances. Journal of Artificial Intelligence Research, 11:131-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chao</author>
<author>M Z Guo</author>
<author>Y Liu</author>
<author>H F Li</author>
</authors>
<title>Participatory Learning based Semi-supervised Classification.</title>
<date>2008</date>
<booktitle>In Proc. of 4th ICNC,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="5172" citStr="Chao et al, 2008" startWordPosition="778" endWordPosition="781">d problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method (Jiang and Zhou, 2004) in transfer learning (Cheng and Li, 2009). His experiment showed that their approach would work when the source domain and the target domain share similar distributions. How to reduce negative transfers is still a problem in transfer lea</context>
</contexts>
<marker>Chao, Guo, Liu, Li, 2008</marker>
<rawString>Chao, D., Guo, M. Z., Liu, Y., Li, H. F. 2008. Participatory Learning based Semi-supervised Classification. In Proc. of 4th ICNC, pages 207-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Cheng</author>
<author>Q Y Li</author>
</authors>
<date>2009</date>
<booktitle>Transfer Learning with Data Edit. LNAI,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="5576" citStr="Cheng and Li, 2009" startWordPosition="845" endWordPosition="848"> accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method (Jiang and Zhou, 2004) in transfer learning (Cheng and Li, 2009). His experiment showed that their approach would work when the source domain and the target domain share similar distributions. How to reduce negative transfers is still a problem in transfer learning. 3 Our Approach In order to reduce negative transfers, we propose to incorporate class noise detection into TTL. The basic idea is to first select high quality labeled samples after certain iterations as indicator to detect class noise in transferred samples. We then remove noisy samples that cause negative transfers from the current accumulated training set to retain an improved set of training</context>
<context position="9680" citStr="Cheng and Li, 2009" startWordPosition="1581" endWordPosition="1584">.2 for the best cases in manual annotations. Let Cij = {(xj, yj) } be the vertices that are connected to the ith vertex, the statistical magnitude of the ith vertex can be defined as: J = z Wij-jIij ( 5) i where j refers to the j th vertex that is connected to the ith vertex. From the theory of cut edge statics, we know that the expectation of Ji is: l&apos; = P(Iij=1) *z jWij ( 6) i And the variance of Ji is: Q 2 = P(Iij=0) P(Iij=1) *z Wij2j( 7) i By the Center Limit Theorem (CLT), Ji follows the normal distribution: ( i—tti) — 1V( 0,1) (t—µi) 1 IrN/27cvi Note that experiments (Li and Zhou, 2011; Cheng and Li, 2009; Brodley and Friedl, 1999) have shown that pi is related to the error rate of the example (xi, yi), but it does not reflect the ground-truth probability in statistics. Hence we assume the class noise rate of example (xi, yi) is: Il = 1 p ( 10) i i We take the general significant level of 0.05 to reject the null hypothesis. It means that if Ili of (xi, yi) is larger than 0.95, the sample will be considered as a class noisy sample. Furthermore, Il can be used to estimate the average class noise i rate of a transferred samples in (2). In our proposed approach, we establish the quality estimate p</context>
</contexts>
<marker>Cheng, Li, 2009</marker>
<rawString>Cheng, Y., Li, Q. Y. 2009. Transfer Learning with Data Edit. LNAI, pages 427–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chen</author>
<author>K Q Weinberger</author>
<author>J C Blitzer</author>
</authors>
<title>Co-Training for Domain Adaptation.</title>
<date>2011</date>
<booktitle>In Proc. of 23th NIPS.</booktitle>
<contexts>
<context position="3995" citStr="Chen et al, 2011" startWordPosition="595" endWordPosition="598"> and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods f</context>
</contexts>
<marker>Chen, Weinberger, Blitzer, 2011</marker>
<rawString>Chen, M., Weinberger, K. Q., Blitzer, J. C. 2011. Co-Training for Domain Adaptation. In Proc. of 23th NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fukumoto</author>
<author>Y Suzuki</author>
<author>S Matsuyoshi</author>
</authors>
<title>Text Classification from Positive and Unlabeled Data using Misclassified Data Correction.</title>
<date>2013</date>
<booktitle>In Proc. of 51st ACL,</booktitle>
<pages>474--478</pages>
<contexts>
<context position="5196" citStr="Fukumoto et al, 2013" startWordPosition="782" endWordPosition="786"> two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method (Jiang and Zhou, 2004) in transfer learning (Cheng and Li, 2009). His experiment showed that their approach would work when the source domain and the target domain share similar distributions. How to reduce negative transfers is still a problem in transfer learning. 3 Our Approach In</context>
</contexts>
<marker>Fukumoto, Suzuki, Matsuyoshi, 2013</marker>
<rawString>Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text Classification from Positive and Unlabeled Data using Misclassified Data Correction. In Proc. of 51st ACL, pages 474-478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gui</author>
<author>R Xu</author>
<author>J Xu</author>
</authors>
<title>A Mixed Model for Cross Lingual Opinion Analysis.</title>
<date>2013</date>
<booktitle>In CCIS, 400,</booktitle>
<pages>93--104</pages>
<contexts>
<context position="15042" citStr="Gui et al, 2013" startWordPosition="2459" endWordPosition="2462">rence/2013/dldoc/evdata03.zip 2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 3https://translate.google.com 4http://svmlight.joachims.org/ performance to the English counter parts. This means the predominant training comes from the English training data. In the second set of experiment, we compare our proposed approach to the official results in NLP&amp;CC 2013 CLOA evaluation and the result is given in Table 3. Note that in Table 3, the top performer of NLP&amp;CC 2013 CLOA evaluation is the HLT-HITSZ system(underscored in the table), which used the co-training method in transfer learning (Gui et al, 2013), proving that co-training is quite effective for cross-lingual analysis. With the additional negative transfer detection, our proposed approach achieves the best performance on this dataset outperformed the top system (by HLT-HITSZ) by a 2.97% which translate to 13.1% error reduction improvement to this state-of-the-art system as shown in the last row of Table 3. Team DVD Book Music Accuracy BUAA 0.481 0.498 0.503 0.494 BISTU 0.647 0.598 0.661 0.635 HLT-HITSZ 0.777 0.785 0.751 0.771 THUIR 0.739 0.742 0.733 0.738 SJTU 0.772 0.724 0.745 0.747 WHU 0.783 0.770 0.760 0.771 Our approach 0.816 0.801</context>
</contexts>
<marker>Gui, Xu, Xu, 2013</marker>
<rawString>Gui, L., Xu, R., Xu, J., et al. 2013. A Mixed Model for Cross Lingual Opinion Analysis. In CCIS, 400, pages 93-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>A Smola</author>
<author>A Gretton</author>
<author>K M Borgwardt</author>
<author>B Scholkopf</author>
</authors>
<title>Correcting Sample Selection Bias by Unlabeled Data.</title>
<date>2007</date>
<booktitle>In Proc. of 19th NIPS,</booktitle>
<pages>601--608</pages>
<contexts>
<context position="3954" citStr="Huang et al., 2007" startWordPosition="586" endWordPosition="589"> been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolv</context>
</contexts>
<marker>Huang, Smola, Gretton, Borgwardt, Scholkopf, 2007</marker>
<rawString>Huang, J., Smola, A., Gretton, A., Borgwardt, K.M., Scholkopf, B. 2007. Correcting Sample Selection Bias by Unlabeled Data. In Proc. of 19th NIPS, pages 601-608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Jiang</author>
<author>Z H Zhou</author>
</authors>
<title>Editing Training Data for kNN Classifiers with Neural Network Ensemble.</title>
<date>2004</date>
<booktitle>In LNCS,</booktitle>
<pages>3173--356</pages>
<contexts>
<context position="4868" citStr="Jiang and Zhou, 2004" startWordPosition="730" endWordPosition="733">s 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Ch</context>
</contexts>
<marker>Jiang, Zhou, 2004</marker>
<rawString>Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for kNN Classifiers with Neural Network Ensemble. In LNCS, 3173, pages 356-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>Z H Zhou</author>
</authors>
<title>SETRED: Self-Training with Editing.</title>
<date>2005</date>
<booktitle>In Proc. of PAKDD,</booktitle>
<pages>611--621</pages>
<contexts>
<context position="5078" citStr="Li and Zhou, 2005" startWordPosition="762" endWordPosition="765">red samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method (Jiang and Zhou, 2004) in transfer learning (Cheng and Li, 2009). His experiment showed that their approach would work when the source domain and the target domain sh</context>
</contexts>
<marker>Li, Zhou, 2005</marker>
<rawString>Li, M., Zhou, Z. H. 2005. SETRED: Self-Training with Editing. In Proc. of PAKDD, pages 611-621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Li</author>
<author>Z H Zhou</author>
</authors>
<title>COTRADE: Confident CoTraining With Data Editing.</title>
<date>2011</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics—Part B: Cybernetics,</journal>
<pages>41--6</pages>
<contexts>
<context position="5114" citStr="Li and Zhou, 2011" startWordPosition="768" endWordPosition="771">ise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not directly suited in TTL. Y. Cheng has tried to use semi-supervised method (Jiang and Zhou, 2004) in transfer learning (Cheng and Li, 2009). His experiment showed that their approach would work when the source domain and the target domain share similar distributions. How to re</context>
<context position="9660" citStr="Li and Zhou, 2011" startWordPosition="1577" endWordPosition="1580">stribution with p=0.2 for the best cases in manual annotations. Let Cij = {(xj, yj) } be the vertices that are connected to the ith vertex, the statistical magnitude of the ith vertex can be defined as: J = z Wij-jIij ( 5) i where j refers to the j th vertex that is connected to the ith vertex. From the theory of cut edge statics, we know that the expectation of Ji is: l&apos; = P(Iij=1) *z jWij ( 6) i And the variance of Ji is: Q 2 = P(Iij=0) P(Iij=1) *z Wij2j( 7) i By the Center Limit Theorem (CLT), Ji follows the normal distribution: ( i—tti) — 1V( 0,1) (t—µi) 1 IrN/27cvi Note that experiments (Li and Zhou, 2011; Cheng and Li, 2009; Brodley and Friedl, 1999) have shown that pi is related to the error rate of the example (xi, yi), but it does not reflect the ground-truth probability in statistics. Hence we assume the class noise rate of example (xi, yi) is: Il = 1 p ( 10) i i We take the general significant level of 0.05 to reject the null hypothesis. It means that if Ili of (xi, yi) is larger than 0.95, the sample will be considered as a class noisy sample. Furthermore, Il can be used to estimate the average class noise i rate of a transferred samples in (2). In our proposed approach, we establish th</context>
</contexts>
<marker>Li, Zhou, 2011</marker>
<rawString>Li, M., Zhou, Z. H. 2011. COTRADE: Confident CoTraining With Data Editing. IEEE Transactions on Systems, Man, and Cybernetics—Part B: Cybernetics, 41(6):1612-1627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lu</author>
<author>C H Tang</author>
<author>C Cardie</author>
<author>B K Tsou</author>
</authors>
<title>Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora.</title>
<date>2011</date>
<booktitle>In Proc. of 49th ACL,</booktitle>
<pages>320--330</pages>
<contexts>
<context position="4013" citStr="Lu et al., 2011" startWordPosition="599" endWordPosition="602"> TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise det</context>
</contexts>
<marker>Lu, Tang, Cardie, Tsou, 2011</marker>
<rawString>Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011. Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora. In Proc. of 49th ACL, pages 320-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X F Meng</author>
<author>F R Wei</author>
<author>X H Liu</author>
</authors>
<title>Cross-Lingual Mixture Model for Sentiment Classification.</title>
<date>2012</date>
<booktitle>In Proc. of 50th ACL,</booktitle>
<pages>572--581</pages>
<contexts>
<context position="4410" citStr="Meng et al., 2012" startWordPosition="656" endWordPosition="659">er learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s c</context>
</contexts>
<marker>Meng, Wei, Liu, 2012</marker>
<rawString>Meng, X. F., Wei, F. R., Liu, X. H., et al. 2012. Cross-Lingual Mixture Model for Sentiment Classification. In Proc. of 50th ACL, pages 572-581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Muhlenbach</author>
<author>S Lallich</author>
<author>D A Zighed</author>
</authors>
<title>Identifying and Handling Mislabeled Instances.</title>
<date>2004</date>
<journal>Journal of Intelligent Information System,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>89--109</pages>
<contexts>
<context position="4845" citStr="Muhlenbach et al, 2004" startWordPosition="726" endWordPosition="729">ics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised learning are not direct</context>
</contexts>
<marker>Muhlenbach, Lallich, Zighed, 2004</marker>
<rawString>Muhlenbach, F., Lallich, S., Zighed, D. A. 2004. Identifying and Handling Mislabeled Instances. Journal of Intelligent Information System, 22(1): 89-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Pan</author>
<author>Q Yang</author>
</authors>
<title>A Survey on Transfer Learning,</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<pages>22--10</pages>
<marker>Pan, Yang, 2010</marker>
<rawString>Pan, S. J., Yang, Q. 2010. A Survey on Transfer Learning, IEEE Transactions on Knowledge and Data Engineering, 22(10):1345-1360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sindhwani</author>
<author>D S Rosenberg</author>
</authors>
<title>An RKHS for Multi-view Learning and Manifold CoRegularization.</title>
<date>2008</date>
<booktitle>In Proc. of 25th ICML,</booktitle>
<pages>976--983</pages>
<marker>Sindhwani, Rosenberg, 2008</marker>
<rawString>Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for Multi-view Learning and Manifold CoRegularization. In Proc. of 25th ICML, pages 976– 983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sluban</author>
<author>D Gamberger</author>
<author>N Lavra</author>
</authors>
<date>2010</date>
<booktitle>Advances in Class Noise Detection. In Proc.19th ECAI,</booktitle>
<pages>1105--1106</pages>
<contexts>
<context position="4760" citStr="Sluban et al., 2010" startWordPosition="710" endWordPosition="713">roceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distri</context>
</contexts>
<marker>Sluban, Gamberger, Lavra, 2010</marker>
<rawString>Sluban, B., Gamberger, D., Lavra, N. 2010. Advances in Class Noise Detection. In Proc.19th ECAI, pages 1105-1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sugiyama</author>
<author>S Nakajima</author>
<author>H Kashima</author>
<author>P V Buenau</author>
<author>M Kawanabe</author>
</authors>
<title>Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation.</title>
<date>2008</date>
<booktitle>In Proc. 20th NIPS.</booktitle>
<contexts>
<context position="3977" citStr="Sugiyama et al., 2008" startWordPosition="590" endWordPosition="594">fore the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are t</context>
</contexts>
<marker>Sugiyama, Nakajima, Kashima, Buenau, Kawanabe, 2008</marker>
<rawString>Sugiyama, M., Nakajima, S., Kashima, H., Buenau, P.V., Kawanabe, M. 2008. Direct Importance Estimation with Model Selection and its Application to Covariate Shift Adaptation. In Proc. 20th NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
</authors>
<title>Co-Training for Cross-Lingual Sentiment Classification,</title>
<date>2009</date>
<booktitle>In Proc. of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>235--243</pages>
<contexts>
<context position="3515" citStr="Wan, 2009" startWordPosition="514" endWordPosition="515"> are used beating the performance degradation curse of most transfer learning methods when training data reaches certain size. The rest of the paper is organized as follows. Section 2 introduces related works in transfer learning, cross lingual opinion analysis, and class noise detection technology. Section 3 presents our algorithm. Section 4 gives performance evaluation. Section 5 concludes this paper. 2 Related works TTL has been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 86</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Wan, X. 2009. Co-Training for Cross-Lingual Sentiment Classification, In Proc. of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, 235–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Zhang</author>
<author>H K Yu</author>
<author>D Y Xiong</author>
<author>Q Liu</author>
</authors>
<title>HHMM-based Chinese Lexical Analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In 2nd SIGHAN workshop affiliated with 41th ACL,</booktitle>
<pages>184--187</pages>
<contexts>
<context position="13640" citStr="Zhang et al, 2003" startWordPosition="2268" endWordPosition="2271">are 12,000 Chinese product reviews (shown in Table.1). This dataset is designed to evaluate the CLOA algorithm which uses Train_CHN, Train_ENG and Dev_CHN to train a classifier for Test_CHN. The performance is evaluated by the correct classification accuracy for each category in Test_CHN2: where c is either DVD, BOOK or MUSIC. Team DVD Book Music Train_CHN 40 40 40 Train_ENG 4000 4000 4000 Dev_CHN 17814 47071 29677 Test CHN 4000 4000 4000 Table.1 The NLP&amp;CC 2013 CLOA dataset In the experiment, the basic transfer learning algorithm is co-training. The Chinese word segmentation tool is ICTCLAS (Zhang et al, 2003) and Google Translator3 is the MT for the source language. The monolingual opinion classifier is SVMlight4, word unigram/bigram features are employed. 4.2 CLOA Experiment Results Firstly, we evaluate the baseline systems which use the same monolingual opinion classifier with three training dataset including Train_CHN, translated Train_ENG and their union, respectively. DVD Book Music Accuracy Train_CHN 0.552 0.513 0.500 0.522 Train_ENG 0.729 0.733 0.722 0.728 Train_CHN 0.737 0.722 0.742 0.734 +Train_ENG Table.2 Baseline performances It can be seen that using the same method, the classifier tra</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q. 2003. HHMM-based Chinese Lexical Analyzer ICTCLAS. In 2nd SIGHAN workshop affiliated with 41th ACL, pages 184-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhou</author>
<author>X Wan</author>
<author>J Xiao</author>
</authors>
<title>Cross-Language Opinion Target Extraction in Review Texts.</title>
<date>2011</date>
<booktitle>In Proc. of IEEE 12th ICDM,</booktitle>
<pages>1200--1205</pages>
<contexts>
<context position="3535" citStr="Zhou et al., 2011" startWordPosition="516" endWordPosition="519">eating the performance degradation curse of most transfer learning methods when training data reaches certain size. The rest of the paper is organized as follows. Section 2 introduces related works in transfer learning, cross lingual opinion analysis, and class noise detection technology. Section 3 presents our algorithm. Section 4 gives performance evaluation. Section 5 concludes this paper. 2 Related works TTL has been widely used before the formal concept and definition of TTL was given in (Arnold, 2007). Wan introduced the co-training method into cross-lingual opinion analysis (Wan, 2009; Zhou et al., 2011), and Aue et al. introduced transfer learning into cross domain analysis (Aue, 2005) which solves similar problems. In this paper, we will use the terms source language and target language to refer to all cross lingual/domain analysis. Traditionally, transfer learning methods focus on how to estimate the confidence score of transferred samples in the target language or domain (Blitzer et al, 2006, Huang et al., 2007; Sugiyama et al., 2008, Chen et al, 2011, Lu et al., 2011). In some tasks, researchers utilize NLP tools such as alignment to reduce the bias towards that of 860 *Corresponding aut</context>
</contexts>
<marker>Zhou, Wan, Xiao, 2011</marker>
<rawString>Zhou, X., Wan X., Xiao, J. 2011. Cross-Language Opinion Target Extraction in Review Texts. In Proc. of IEEE 12th ICDM, pages 1200-1205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Q Zhu</author>
<author>X D Wu</author>
<author>Q J Chen</author>
</authors>
<title>Eliminating Class Noise in Large Datasets.</title>
<date>2003</date>
<booktitle>In Proc. of 12th ICML,</booktitle>
<pages>920--927</pages>
<contexts>
<context position="4728" citStr="Zhu et al, 2003" startWordPosition="704" endWordPosition="707">860 *Corresponding author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be</context>
</contexts>
<marker>Zhu, Wu, Chen, 2003</marker>
<rawString>Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003. Eliminating Class Noise in Large Datasets. In Proc. of 12th ICML, pages 920-927.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Q Zhu</author>
</authors>
<title>Cost-guided Class Noise Handling for Effective Cost-sensitive Learning In</title>
<date>2004</date>
<booktitle>Proc. of 4th IEEE ICDM,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="4738" citStr="Zhu 2004" startWordPosition="708" endWordPosition="709">g author Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed t</context>
</contexts>
<marker>Zhu, 2004</marker>
<rawString>Zhu, X. Q. 2004. Cost-guided Class Noise Handling for Effective Cost-sensitive Learning In Proc. of 4th IEEE ICDM, pages 297-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Zighed</author>
<author>S Lallich</author>
<author>F Muhlenbach</author>
</authors>
<title>Separability Index in Supervised Learning.</title>
<date>2002</date>
<booktitle>In Proc. of PKDD,</booktitle>
<pages>475--487</pages>
<contexts>
<context position="4821" citStr="Zighed et al, 2002" startWordPosition="722" endWordPosition="725">mputational Linguistics (Short Papers), pages 860–865, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics the source language in transfer learning (Meng et al., 2012). However, detecting misclassification in transferred samples (referred to as class noise) and reducing negative transfers are still an unresolved problem. There are two basic methods for class noise detection in machine learning. The first is the classification based method (Brodley and Friedl, 1999; Zhu et al, 2003; Zhu 2004; Sluban et al., 2010) and the second is the graph based method (Zighed et al, 2002; Muhlenbach et al, 2004; Jiang and Zhou, 2004). Class noise detection can also be applied to semi-supervised learning because noise can accumulate in iterations too. Li employed Zighed’s cut edge weight statistic method in self-training (Li and Zhou, 2005) and co-training (Li and Zhou, 2011). Chao used Li’s method in tri-training (Chao et al, 2008). (Fukumoto et al, 2013) used the support vectors to detect class noise in semi-supervised learning. In TTL, however, training and testing samples cannot be assumed to have the same distributions. Thus, noise detection methods used in semisupervised</context>
</contexts>
<marker>Zighed, Lallich, Muhlenbach, 2002</marker>
<rawString>Zighed, D. A., Lallich, S., Muhlenbach, F. 2002. Separability Index in Supervised Learning. In Proc. of PKDD, pages 475-487.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>