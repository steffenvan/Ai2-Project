<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.084566">
<title confidence="0.976676">
Summarization of Historical Articles Using Temporal Event Clustering
</title>
<author confidence="0.995567">
James Gung
</author>
<affiliation confidence="0.9983255">
Department of Computer Science
Miami University
</affiliation>
<address confidence="0.817878">
Oxford, Ohio 45056
</address>
<email confidence="0.997484">
gungjm@muohio.edu
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991822">
In this paper, we investigate the use of tempo-
ral information for improving extractive sum-
marization of historical articles. Our method
clusters sentences based on their timestamps
and temporal similarity. Each resulting clus-
ter is assigned an importance score which
can then be used as a weight in traditional
sentence ranking techniques. Temporal im-
portance weighting offers consistent improve-
ments over baseline systems.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998704269230769">
Extensive research has gone into determining
which features of text documents are useful for cal-
culating the importance of sentences for extractive
summarization, as well as how to use these features
(Gupta and Lehal, 2010). Little work, however, has
considered the importance of temporal information
towards single document summarization. This is
likely because many text documents have very few
explicit time features and do not necessarily describe
topics in chronological order.
Historical articles, such as Wikipedia articles de-
scribing wars, battles, or other major events, tend to
contain many explicit time features. Historical arti-
cles also tend to describe events in chronological or-
der. In addition, historical articles tend to focus on a
single central event. The importance of other events
can then be judged by their temporal distance from
this central event. Finally, important events in an ar-
ticle will be described in greater detail, employing
more sentences than less important events.
This paper investigates the value of a temporal-
based score towards automatic summarization,
specifically focusing on historical articles. We in-
vestigate whether or not such a score can be used as
a weight in traditional sentence ranking techniques
to improve summarization quality.
</bodyText>
<author confidence="0.457922">
Jugal Kalita
</author>
<affiliation confidence="0.9969115">
Department of Computer Science
University of Colorado
</affiliation>
<address confidence="0.817141">
Colorado Springs CO 80920
</address>
<email confidence="0.993536">
jkalita@uccs.edu
</email>
<sectionHeader confidence="0.99995" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999396945945946">
Event-based summarization is a recent approach
to summary generation. (Filatova and Hatzivas-
siloglou, 2004) introduced atomic events, which are
named entities connected by a relation such as a verb
or action noun. Events are selected for summary by
applying a maximum coverage algorithm to mini-
mize redundancy while maintaining coverage of the
major concepts of the document. (Vanderwende et
al., 2004) identify events as triples consisting of two
nodes and a relation. PageRank is then used to de-
termine the relative importance of these triples rep-
resented in a graph. Sentence generation techniques
are applied towards summarization.
Limited work has explored the use of temporal
information for summarization. (Lim et al., 2005)
use the explicit time information in the context of
multi-document summarization for sentence extrac-
tion and detection of redundant sentences, ordering
input documents by time. They observe that impor-
tant sentences tend to occur in in time slots contain-
ing more documents and time slots occurring at the
end and beginning of the documents set. They se-
lect topic sentences for each time slot, giving higher
weights based on the above observation.
(Wu et al., 2007) extract event elements, the ar-
guments in an event, and event terms, the actions.
Each event is placed on a timeline divided into in-
tervals consistent with the timespan of the article.
Each element and event term receives a weight cor-
responding to the total number of elements and event
terms located in each time interval the event element
or term occupies. Each sentence is scored by the to-
tal weight of event elements and terms it contains.
Clustering of events based on time has also re-
ceived little attention. (Foote and Cooper, 2003) in-
vestigate clustering towards organizing timestamped
digital photographs. They present a method that first
</bodyText>
<page confidence="0.905977">
631
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 631–635,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999848428571429">
calculates the temporal similarity between all pairs
of photographs at multiple time scales. These values
are stored in a chronologically ordered matrix. Clus-
ter boundaries are determined by calculating novelty
scores for each set of similarity matrices. These are
used to form the final clusters. We adopt this clus-
tering method for clustering timestamped sentences.
</bodyText>
<sectionHeader confidence="0.994584" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999962818181818">
The goal of our method is to give each sentence
in an article a temporal importance score that can
be used as a weight in traditional sentence ranking
techniques. To do this, we need to gain an idea
of the temporal structure of events in an article. A
score must then be assigned to each group corre-
sponding to the importance of the group’s timespan
to the article as a whole. Each sentence in a partic-
ular group will be assigned the same temporal im-
portance score, necessitating the use of a sentence
ranking technique to find a complete summary.
</bodyText>
<subsectionHeader confidence="0.994627">
3.1 Temporal Information Extraction
</subsectionHeader>
<bodyText confidence="0.999992846153846">
We use Heideltime, a rule-based system that
uses sets of regular expressions, to extract explicit
time expressions in the article and normalize them
(Str¨otgen and Gertz, 2010). Events that occur be-
tween each Heideltime-extracted timestamp are as-
signed timestamps consisting of when the prior
timestamp ends and the subsequent timestamp be-
gins. The approach is naive and is described in
(Chasin et al., 2011). This method of temporal ex-
traction is not reliable, but serves the purposes of
testing as a reasonable baseline for temporal extrac-
tion systems. As the precision increases, the perfor-
mance of our system should also improve.
</bodyText>
<subsectionHeader confidence="0.99957">
3.2 Temporal Clustering
</subsectionHeader>
<bodyText confidence="0.9988794">
To cluster sentences into temporally-related
groups, we adopt a clustering method proposed by
Foote et al. to group digital photograph collections.
Inter-sentence similarity is calculated between ev-
ery pair of sentences using Equation (1).
</bodyText>
<equation confidence="0.9798875">
SK (i, C Itz K − t, |l
�) =exp — J (1)
</equation>
<bodyText confidence="0.999942">
The similarity measure is based inversely on the
distance between the central time of the sentences.
Similarity scores are calculated at varying granular-
ities. If the article focuses on a central event that
</bodyText>
<figureCaption confidence="0.978012">
Figure 1: Similarity matrices at varying k displayed as
heat maps, darker representing more similar entries
</figureCaption>
<bodyText confidence="0.999922675">
occurs over only a few hours, such as the assassi-
nation of John F. Kennedy, the best clustering will
generally be found from similarities calculated us-
ing a smaller time granularity. Conversely, articles
with central events spanning several years, such as
the American Civil War, will be clustered using sim-
ilarities calculated at larger time granularities.
The similarities are placed in a matrix and orga-
nized chronologically in order of event occurrence
time. In this matrix, entries close to the diagonal
are among the most similar and the actual diagonal
entries are maximally similar (diagonal entries cor-
respond to similarities between the same sentences).
To identify temporal event boundaries, (Foote and
Cooper, 2003) calculate novelty scores. A checker-
board kernel in which diagonal regions contain all
positive weights and off-diagonal regions contain all
negative weights is correlated along the diagonal of
the similarity matrix. The weights of each entry in
the kernel are calculated from a Gaussian function
such that the most central entries have the highest (or
lowest in the off-diagonal regions) values. The result
is maximized when the kernel is located on tempo-
ral event boundaries. In relatively uniform regions,
the positive and negative weights cancel each other
out, resulting in small novelty scores. Where there
is a gap in similarity, presumably at an event bound-
ary, off diagonal squares are dissimilar, increasing
the novelty score. In calculating novelty scores with
each set of similarity scores, we obtain a hierarchi-
cal set of boundaries. With each time granularity, we
have a potential clustering option.
In order to choose the best clustering, we calcu-
late a confidence score C for each boundary set,
then choose the clustering with the highest score, as
suggested in (Foote and Cooper, 2003). This score
is the sum of intercluster similarities (IntraS) be-
tween adjacent clusters subtracted from the sum of
intracluster (InterS) similarities as seen in Equa-
tion (4). A high confidence score suggests low inter-
</bodyText>
<page confidence="0.990866">
632
</page>
<bodyText confidence="0.953531">
cluster similarity and high intracluster similarity.
</bodyText>
<equation confidence="0.989254">
SK(i, j) (2)
(bl+1 − bl)2
SK(i, j)
(bl+1 − bl)(bl+2 − bl+1)
(3)
CS(BK) = IntraS(BK)S − InterSBK)S (4)
</equation>
<subsectionHeader confidence="0.998614">
3.3 Estimating Clustering Paramaters
</subsectionHeader>
<bodyText confidence="0.999960714285714">
Historical articles describing wars generally have
much larger timespans than articles describing bat-
tles. Looking at battles at a broad time granularity
applicable to wars may not produce a meaningful
clustering. Thus, we should estimate the temporal
structure of each article before clustering. The time
granularity for each clustering is controlled by the
k parameter in the similarity function between sen-
tences. To find multiple clusterings, we start at a
base k, then increment k by a multiplier for each new
clustering. We calculate the base k using the stan-
dard deviation for event times in the article. Mea-
suring the spread of events in the article gives us an
estimate of what time scale we should use.
</bodyText>
<subsectionHeader confidence="0.998353">
3.4 Calculating Temporal Importance
</subsectionHeader>
<bodyText confidence="0.9999912">
We use three novel metrics to calculate the impor-
tance of a cluster towards a summary. The first met-
ric is based on the size of the cluster (Eqn 5). This
is motivated by the assumption that more important
events will be described in greater detail, thus pro-
ducing larger clusters. The second metric (Eqn 6) is
based on the distance from the cluster’s centroid to
the centroid of the largest cluster, corresponding to
the central event of the article. This metric is moti-
vated by the assumption that historical articles have
a central event which is described in the greatest de-
tail. The third metric is based on the spread of the
cluster (Eqn 7). Clusters with large spreads are un-
likely to pertain to the same event, and should there-
fore be penalized.
</bodyText>
<equation confidence="0.9996635">
Size(Ci) = |x|Ci ||(5)
CmaSim(Ci) = exp −  |tCiCentroid − tMaxClusterCentroid  |)
</equation>
<bodyText confidence="0.999977">
The parameters m and n serve to weight the impor-
tance of these measures and are assigned based on
the spread of events in an article. For n, we used the
standard deviation of event times in the article. For
m, we used the cluster similarity score from Equa-
tion (4). The three measures work in tandem to en-
sure that the importance measure will be valid even
if the largest cluster does not correspond to the cen-
tral event of the article.
</bodyText>
<subsectionHeader confidence="0.938607">
3.5 Final Sentence Ranking
</subsectionHeader>
<bodyText confidence="0.9999238">
Each sentence is assigned a temporal importance
weight equal to the importance score of the clus-
ter to which it belongs. To find a complete ranking
of the sentences, we apply a sentence ranking tech-
nique. Any automatic summarization technique that
ranks its sentences with numerical scores can poten-
tially be augmented with our temporal importance
weight. We multiply the base scores from the rank-
ing by the associated temporal importance weights
for each sentence to find the final ranking.
</bodyText>
<equation confidence="0.999022">
WS(Vi) = (1 − d) (8)
P
VjEIn(Vi) v�EOut(Vj) wj,k
</equation>
<bodyText confidence="0.9959565">
Like several graph-based methods for sentence rank-
ing for summarization (e.g., (Erkan and Radev,
2004)), we use Google’s PageRank algorithm
(Equation 8) with a damping factor d of 0.85.
</bodyText>
<equation confidence="0.9798155">
Similarity(Si, Sj) = |{wk|wk ∈Si&amp;wk ∈ Sj}|
log(|Si|) + log(|Sj|)
</equation>
<bodyText confidence="0.961472888888889">
(9)
We use TextRank (Mihalcea and Tarau, 2004) in
our experiments. Our similarity measure is calcu-
lated using the number of shared named entities and
nouns between sentences as seen in equation 9. For
identification of named entities, we use Stanford
NER (Finkel et al., 2005). It is straightforward to
weight the resulting TextRank scores for each sen-
tence using their cluster’s temporal importance.
</bodyText>
<sectionHeader confidence="0.994502" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999746">
We test on a set of 13 Wikipedia articles describ-
ing historical battles. The average article length is
189 sentences and 4,367 words. The longest ar-
ticle is 545 sentences and contains 11,563 words.
The shortest article is 51 sentences and contains
</bodyText>
<equation confidence="0.942423473684211">
bl+1
E
i,j=bl
IntraS(BK)S =
|Bk|−1
E
l=1
InterS(BK)S = |Bk|−2 bl+1 bl+2
E E E
l=1 i=bl j=bl+1
m
(6)
� �
σCi
Spread(Ci) = exp − (7)
n ∗ (tmax − tmin)
X
+d ∗
wj,i W S(Vj)
</equation>
<page confidence="0.995478">
633
</page>
<bodyText confidence="0.9989208">
1,476 words. Each article has at least two human-
annotated gold standard summaries. Volunteers
were asked to choose the most important sentences
from each article. We evaluate using ROUGE-2 bi-
gram matching (Lin, 2004).
</bodyText>
<subsectionHeader confidence="0.998379">
4.1 Clustering
</subsectionHeader>
<bodyText confidence="0.999969066666667">
Each Wikipedia article contains a topic sentence
stating the timespan of the main event in the article.
This provides an easy way to determine whether a
clustering is successful. If the largest cluster con-
tains the timespan of the main event described by
the topic sentence, we consider the clustering to be
successful. The articles vary greatly in length. Also,
the ratio of sentences with time features to sentences
without is considerably varied. In 92% of the arti-
cles, there were successful clusterings. An exam-
ple of an article that didn’t cluster is Nickel Grass,
where the main event was divided into two clusters.
It is of interest to note that this article had one of
lowest time feature to sentence ratios, which possi-
bly explains the poor clustering.
</bodyText>
<subsectionHeader confidence="0.945543">
4.2 Temporal Importance Weighting
</subsectionHeader>
<bodyText confidence="0.99852148">
We test our TextRank implementation with and
without temporal importance weighting.
We observe improvements in general using the
TextRank system with temporal importance weight-
ing. The ROUGE-2 score increased by 15.72%
across all the articles. The lowest increase was
0% and the highest was 128.86%. The average
ROUGE-2 scores were 0.2575 weighted and 0.2362
unweighted, a statistically significant increase with
a 95% confidence interval of 0.0066 to 0.0360.
In particular, we see significant improvements
in articles that contain sentences TextRank ranked
highly but have events occurring at significantly dif-
ferent times than the central event of the article. Al-
though the content of these sentences is highly re-
lated to the rest of the article, they should not be
included in the summary since their events happen
nowhere near the main event temporally.
Our random ranking system, which randomly
assigns base importance scores to each sentence,
observed only small improvements, of 4.27% on
average, when augmented with temporal impor-
tance weighting. It is likely that additional human-
annotated summaries are necessary for conclusive
results.
</bodyText>
<sectionHeader confidence="0.997678" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99999505882353">
The novelty-based clustering method worked ex-
tremely well for our purposes. These results can
likely be improved upon using more advanced tem-
poral extraction and interpolation methods, since we
used a naive method for interpolating between time
features prone to error. The temporal importance
weighting worked very well with TextRank and rea-
sonably well with random ranking.
It may also be fairly easy to predict the success of
using this temporal weight a priori to summarization
of an article. A small ratio of explicit time features to
sentences (less than 0.15) indicates that the temporal
interpolation process may not be very accurate. The
linearity of time features is also a good indication
of the success of temporal extraction. Finally, the
spread of time features in an article is a clue to the
success of our weighting method.
</bodyText>
<sectionHeader confidence="0.998196" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999105">
Research reported here has been funded partially
by NSF grants CNS-0958576 and CNS-0851783.
</bodyText>
<sectionHeader confidence="0.999455" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999845333333333">
R. Chasin, D. Woodward, and J. Kalita, 2011. Machine
Intelligence: Recent Advances, chapter Extracting and
Displaying Temporal Entities from Historical Articles.
Narosa Publishing, Delhi.
G. Erkan and D.R. Radev. 2004. Lexrank: Graph-based
lexical centrality as salience in text summarization. J.
Artif. Intell. Res., 22:457–479.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In ACL Workshop on Sum-
marization.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL, pages
363–370.
J. Foote and M. Cooper. 2003. Media segmentation us-
ing self-similarity decomposition. In SPIE, volume
5021, pages 167–175.
V. Gupta and G.S. Lehal. 2010. A survey of text sum-
marization extractive techniques. Journal of Emerging
Technologies in Web Intelligence, 2(3):258–268.
J.M. Lim, I.S. Kang, J.H. Bae, and J.H. Lee. 2005. Sen-
tence extraction using time features in multi-document
summarization. Information Retrieval Technology,
pages 82–93.
</reference>
<page confidence="0.985779">
634
</page>
<reference confidence="0.999828375">
C.Y. Lin. 2004. Rouge: A package for automatic evalu-
ation of summaries. In Workshop on text summariza-
tion, pages 25–26.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In EMNLP, pages 404–411.
J. Str¨otgen and M. Gertz. 2010. Heideltime: High qual-
ity rule-based extraction and normalization of tempo-
ral expressions. In 5th International Workshop on Se-
mantic Evaluation, pages 321–324.
L. Vanderwende, M. Banko, and A. Menezes. 2004.
Event-centric summary generation. Working notes of
DUC.
M. Wu, W. Li, Q. Lu, and K.F. Wong. 2007. Event-
based summarization using time features. Compu-
tational Linguistics and Intelligent Text Processing,
pages 563–574.
</reference>
<page confidence="0.998785">
635
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.480553">
<title confidence="0.99955">Summarization of Historical Articles Using Temporal Event Clustering</title>
<author confidence="0.994292">James</author>
<affiliation confidence="0.8217065">Department of Computer Miami</affiliation>
<address confidence="0.833979">Oxford, Ohio</address>
<email confidence="0.999905">gungjm@muohio.edu</email>
<abstract confidence="0.997440090909091">In this paper, we investigate the use of temporal information for improving extractive summarization of historical articles. Our method clusters sentences based on their timestamps and temporal similarity. Each resulting cluster is assigned an importance score which can then be used as a weight in traditional sentence ranking techniques. Temporal importance weighting offers consistent improvements over baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Chasin</author>
<author>D Woodward</author>
<author>J Kalita</author>
</authors>
<title>Machine Intelligence: Recent Advances, chapter Extracting and Displaying Temporal Entities from Historical Articles.</title>
<date>2011</date>
<publisher>Narosa Publishing,</publisher>
<location>Delhi.</location>
<contexts>
<context position="5466" citStr="Chasin et al., 2011" startWordPosition="844" endWordPosition="847">ole. Each sentence in a particular group will be assigned the same temporal importance score, necessitating the use of a sentence ranking technique to find a complete summary. 3.1 Temporal Information Extraction We use Heideltime, a rule-based system that uses sets of regular expressions, to extract explicit time expressions in the article and normalize them (Str¨otgen and Gertz, 2010). Events that occur between each Heideltime-extracted timestamp are assigned timestamps consisting of when the prior timestamp ends and the subsequent timestamp begins. The approach is naive and is described in (Chasin et al., 2011). This method of temporal extraction is not reliable, but serves the purposes of testing as a reasonable baseline for temporal extraction systems. As the precision increases, the performance of our system should also improve. 3.2 Temporal Clustering To cluster sentences into temporally-related groups, we adopt a clustering method proposed by Foote et al. to group digital photograph collections. Inter-sentence similarity is calculated between every pair of sentences using Equation (1). SK (i, C Itz K − t, |l �) =exp — J (1) The similarity measure is based inversely on the distance between the c</context>
</contexts>
<marker>Chasin, Woodward, Kalita, 2011</marker>
<rawString>R. Chasin, D. Woodward, and J. Kalita, 2011. Machine Intelligence: Recent Advances, chapter Extracting and Displaying Temporal Entities from Historical Articles. Narosa Publishing, Delhi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res.,</journal>
<pages>22--457</pages>
<contexts>
<context position="11241" citStr="Erkan and Radev, 2004" startWordPosition="1795" endWordPosition="1798">gned a temporal importance weight equal to the importance score of the cluster to which it belongs. To find a complete ranking of the sentences, we apply a sentence ranking technique. Any automatic summarization technique that ranks its sentences with numerical scores can potentially be augmented with our temporal importance weight. We multiply the base scores from the ranking by the associated temporal importance weights for each sentence to find the final ranking. WS(Vi) = (1 − d) (8) P VjEIn(Vi) v�EOut(Vj) wj,k Like several graph-based methods for sentence ranking for summarization (e.g., (Erkan and Radev, 2004)), we use Google’s PageRank algorithm (Equation 8) with a damping factor d of 0.85. Similarity(Si, Sj) = |{wk|wk ∈Si&amp;wk ∈ Sj}| log(|Si|) + log(|Sj|) (9) We use TextRank (Mihalcea and Tarau, 2004) in our experiments. Our similarity measure is calculated using the number of shared named entities and nouns between sentences as seen in equation 9. For identification of named entities, we use Stanford NER (Finkel et al., 2005). It is straightforward to weight the resulting TextRank scores for each sentence using their cluster’s temporal importance. 4 Experimental Results We test on a set of 13 Wiki</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D.R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res., 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In ACL Workshop on Summarization.</booktitle>
<contexts>
<context position="2131" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="307" endWordPosition="311"> important events in an article will be described in greater detail, employing more sentences than less important events. This paper investigates the value of a temporalbased score towards automatic summarization, specifically focusing on historical articles. We investigate whether or not such a score can be used as a weight in traditional sentence ranking techniques to improve summarization quality. Jugal Kalita Department of Computer Science University of Colorado Colorado Springs CO 80920 jkalita@uccs.edu 2 Related Work Event-based summarization is a recent approach to summary generation. (Filatova and Hatzivassiloglou, 2004) introduced atomic events, which are named entities connected by a relation such as a verb or action noun. Events are selected for summary by applying a maximum coverage algorithm to minimize redundancy while maintaining coverage of the major concepts of the document. (Vanderwende et al., 2004) identify events as triples consisting of two nodes and a relation. PageRank is then used to determine the relative importance of these triples represented in a graph. Sentence generation techniques are applied towards summarization. Limited work has explored the use of temporal information for summariza</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>E. Filatova and V. Hatzivassiloglou. 2004. Event-based extractive summarization. In ACL Workshop on Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="11666" citStr="Finkel et al., 2005" startWordPosition="1865" endWordPosition="1868">or each sentence to find the final ranking. WS(Vi) = (1 − d) (8) P VjEIn(Vi) v�EOut(Vj) wj,k Like several graph-based methods for sentence ranking for summarization (e.g., (Erkan and Radev, 2004)), we use Google’s PageRank algorithm (Equation 8) with a damping factor d of 0.85. Similarity(Si, Sj) = |{wk|wk ∈Si&amp;wk ∈ Sj}| log(|Si|) + log(|Sj|) (9) We use TextRank (Mihalcea and Tarau, 2004) in our experiments. Our similarity measure is calculated using the number of shared named entities and nouns between sentences as seen in equation 9. For identification of named entities, we use Stanford NER (Finkel et al., 2005). It is straightforward to weight the resulting TextRank scores for each sentence using their cluster’s temporal importance. 4 Experimental Results We test on a set of 13 Wikipedia articles describing historical battles. The average article length is 189 sentences and 4,367 words. The longest article is 545 sentences and contains 11,563 words. The shortest article is 51 sentences and contains bl+1 E i,j=bl IntraS(BK)S = |Bk|−1 E l=1 InterS(BK)S = |Bk|−2 bl+1 bl+2 E E E l=1 i=bl j=bl+1 m (6) � � σCi Spread(Ci) = exp − (7) n ∗ (tmax − tmin) X +d ∗ wj,i W S(Vj) 633 1,476 words. Each article has a</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J.R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Foote</author>
<author>M Cooper</author>
</authors>
<title>Media segmentation using self-similarity decomposition.</title>
<date>2003</date>
<booktitle>In SPIE,</booktitle>
<volume>5021</volume>
<pages>167--175</pages>
<contexts>
<context position="3758" citStr="Foote and Cooper, 2003" startWordPosition="576" endWordPosition="579">h time slot, giving higher weights based on the above observation. (Wu et al., 2007) extract event elements, the arguments in an event, and event terms, the actions. Each event is placed on a timeline divided into intervals consistent with the timespan of the article. Each element and event term receives a weight corresponding to the total number of elements and event terms located in each time interval the event element or term occupies. Each sentence is scored by the total weight of event elements and terms it contains. Clustering of events based on time has also received little attention. (Foote and Cooper, 2003) investigate clustering towards organizing timestamped digital photographs. They present a method that first 631 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 631–635, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics calculates the temporal similarity between all pairs of photographs at multiple time scales. These values are stored in a chronologically ordered matrix. Cluster boundaries are determined by calculating novelty scores for each set of similarity matrices. These ar</context>
<context position="7037" citStr="Foote and Cooper, 2003" startWordPosition="1093" endWordPosition="1096">erally be found from similarities calculated using a smaller time granularity. Conversely, articles with central events spanning several years, such as the American Civil War, will be clustered using similarities calculated at larger time granularities. The similarities are placed in a matrix and organized chronologically in order of event occurrence time. In this matrix, entries close to the diagonal are among the most similar and the actual diagonal entries are maximally similar (diagonal entries correspond to similarities between the same sentences). To identify temporal event boundaries, (Foote and Cooper, 2003) calculate novelty scores. A checkerboard kernel in which diagonal regions contain all positive weights and off-diagonal regions contain all negative weights is correlated along the diagonal of the similarity matrix. The weights of each entry in the kernel are calculated from a Gaussian function such that the most central entries have the highest (or lowest in the off-diagonal regions) values. The result is maximized when the kernel is located on temporal event boundaries. In relatively uniform regions, the positive and negative weights cancel each other out, resulting in small novelty scores.</context>
</contexts>
<marker>Foote, Cooper, 2003</marker>
<rawString>J. Foote and M. Cooper. 2003. Media segmentation using self-similarity decomposition. In SPIE, volume 5021, pages 167–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Gupta</author>
<author>G S Lehal</author>
</authors>
<title>A survey of text summarization extractive techniques.</title>
<date>2010</date>
<journal>Journal of Emerging Technologies in Web Intelligence,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="836" citStr="Gupta and Lehal, 2010" startWordPosition="117" endWordPosition="120"> temporal information for improving extractive summarization of historical articles. Our method clusters sentences based on their timestamps and temporal similarity. Each resulting cluster is assigned an importance score which can then be used as a weight in traditional sentence ranking techniques. Temporal importance weighting offers consistent improvements over baseline systems. 1 Introduction Extensive research has gone into determining which features of text documents are useful for calculating the importance of sentences for extractive summarization, as well as how to use these features (Gupta and Lehal, 2010). Little work, however, has considered the importance of temporal information towards single document summarization. This is likely because many text documents have very few explicit time features and do not necessarily describe topics in chronological order. Historical articles, such as Wikipedia articles describing wars, battles, or other major events, tend to contain many explicit time features. Historical articles also tend to describe events in chronological order. In addition, historical articles tend to focus on a single central event. The importance of other events can then be judged b</context>
</contexts>
<marker>Gupta, Lehal, 2010</marker>
<rawString>V. Gupta and G.S. Lehal. 2010. A survey of text summarization extractive techniques. Journal of Emerging Technologies in Web Intelligence, 2(3):258–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Lim</author>
<author>I S Kang</author>
<author>J H Bae</author>
<author>J H Lee</author>
</authors>
<title>Sentence extraction using time features in multi-document summarization. Information Retrieval Technology,</title>
<date>2005</date>
<pages>82--93</pages>
<contexts>
<context position="2755" citStr="Lim et al., 2005" startWordPosition="406" endWordPosition="409">uced atomic events, which are named entities connected by a relation such as a verb or action noun. Events are selected for summary by applying a maximum coverage algorithm to minimize redundancy while maintaining coverage of the major concepts of the document. (Vanderwende et al., 2004) identify events as triples consisting of two nodes and a relation. PageRank is then used to determine the relative importance of these triples represented in a graph. Sentence generation techniques are applied towards summarization. Limited work has explored the use of temporal information for summarization. (Lim et al., 2005) use the explicit time information in the context of multi-document summarization for sentence extraction and detection of redundant sentences, ordering input documents by time. They observe that important sentences tend to occur in in time slots containing more documents and time slots occurring at the end and beginning of the documents set. They select topic sentences for each time slot, giving higher weights based on the above observation. (Wu et al., 2007) extract event elements, the arguments in an event, and event terms, the actions. Each event is placed on a timeline divided into interv</context>
</contexts>
<marker>Lim, Kang, Bae, Lee, 2005</marker>
<rawString>J.M. Lim, I.S. Kang, J.H. Bae, and J.H. Lee. 2005. Sentence extraction using time features in multi-document summarization. Information Retrieval Technology, pages 82–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Workshop on text summarization,</booktitle>
<pages>25--26</pages>
<contexts>
<context position="12451" citStr="Lin, 2004" startWordPosition="2005" endWordPosition="2006">pedia articles describing historical battles. The average article length is 189 sentences and 4,367 words. The longest article is 545 sentences and contains 11,563 words. The shortest article is 51 sentences and contains bl+1 E i,j=bl IntraS(BK)S = |Bk|−1 E l=1 InterS(BK)S = |Bk|−2 bl+1 bl+2 E E E l=1 i=bl j=bl+1 m (6) � � σCi Spread(Ci) = exp − (7) n ∗ (tmax − tmin) X +d ∗ wj,i W S(Vj) 633 1,476 words. Each article has at least two humanannotated gold standard summaries. Volunteers were asked to choose the most important sentences from each article. We evaluate using ROUGE-2 bigram matching (Lin, 2004). 4.1 Clustering Each Wikipedia article contains a topic sentence stating the timespan of the main event in the article. This provides an easy way to determine whether a clustering is successful. If the largest cluster contains the timespan of the main event described by the topic sentence, we consider the clustering to be successful. The articles vary greatly in length. Also, the ratio of sentences with time features to sentences without is considerably varied. In 92% of the articles, there were successful clusterings. An example of an article that didn’t cluster is Nickel Grass, where the ma</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.Y. Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Workshop on text summarization, pages 25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="11436" citStr="Mihalcea and Tarau, 2004" startWordPosition="1827" endWordPosition="1830">matic summarization technique that ranks its sentences with numerical scores can potentially be augmented with our temporal importance weight. We multiply the base scores from the ranking by the associated temporal importance weights for each sentence to find the final ranking. WS(Vi) = (1 − d) (8) P VjEIn(Vi) v�EOut(Vj) wj,k Like several graph-based methods for sentence ranking for summarization (e.g., (Erkan and Radev, 2004)), we use Google’s PageRank algorithm (Equation 8) with a damping factor d of 0.85. Similarity(Si, Sj) = |{wk|wk ∈Si&amp;wk ∈ Sj}| log(|Si|) + log(|Sj|) (9) We use TextRank (Mihalcea and Tarau, 2004) in our experiments. Our similarity measure is calculated using the number of shared named entities and nouns between sentences as seen in equation 9. For identification of named entities, we use Stanford NER (Finkel et al., 2005). It is straightforward to weight the resulting TextRank scores for each sentence using their cluster’s temporal importance. 4 Experimental Results We test on a set of 13 Wikipedia articles describing historical battles. The average article length is 189 sentences and 4,367 words. The longest article is 545 sentences and contains 11,563 words. The shortest article is </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. Textrank: Bringing order into texts. In EMNLP, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Str¨otgen</author>
<author>M Gertz</author>
</authors>
<title>Heideltime: High quality rule-based extraction and normalization of temporal expressions.</title>
<date>2010</date>
<booktitle>In 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>321--324</pages>
<marker>Str¨otgen, Gertz, 2010</marker>
<rawString>J. Str¨otgen and M. Gertz. 2010. Heideltime: High quality rule-based extraction and normalization of temporal expressions. In 5th International Workshop on Semantic Evaluation, pages 321–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>M Banko</author>
<author>A Menezes</author>
</authors>
<title>Event-centric summary generation. Working notes of DUC.</title>
<date>2004</date>
<contexts>
<context position="2426" citStr="Vanderwende et al., 2004" startWordPosition="355" endWordPosition="358"> be used as a weight in traditional sentence ranking techniques to improve summarization quality. Jugal Kalita Department of Computer Science University of Colorado Colorado Springs CO 80920 jkalita@uccs.edu 2 Related Work Event-based summarization is a recent approach to summary generation. (Filatova and Hatzivassiloglou, 2004) introduced atomic events, which are named entities connected by a relation such as a verb or action noun. Events are selected for summary by applying a maximum coverage algorithm to minimize redundancy while maintaining coverage of the major concepts of the document. (Vanderwende et al., 2004) identify events as triples consisting of two nodes and a relation. PageRank is then used to determine the relative importance of these triples represented in a graph. Sentence generation techniques are applied towards summarization. Limited work has explored the use of temporal information for summarization. (Lim et al., 2005) use the explicit time information in the context of multi-document summarization for sentence extraction and detection of redundant sentences, ordering input documents by time. They observe that important sentences tend to occur in in time slots containing more document</context>
</contexts>
<marker>Vanderwende, Banko, Menezes, 2004</marker>
<rawString>L. Vanderwende, M. Banko, and A. Menezes. 2004. Event-centric summary generation. Working notes of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wu</author>
<author>W Li</author>
<author>Q Lu</author>
<author>K F Wong</author>
</authors>
<title>Eventbased summarization using time features.</title>
<date>2007</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>563--574</pages>
<contexts>
<context position="3219" citStr="Wu et al., 2007" startWordPosition="482" endWordPosition="485">generation techniques are applied towards summarization. Limited work has explored the use of temporal information for summarization. (Lim et al., 2005) use the explicit time information in the context of multi-document summarization for sentence extraction and detection of redundant sentences, ordering input documents by time. They observe that important sentences tend to occur in in time slots containing more documents and time slots occurring at the end and beginning of the documents set. They select topic sentences for each time slot, giving higher weights based on the above observation. (Wu et al., 2007) extract event elements, the arguments in an event, and event terms, the actions. Each event is placed on a timeline divided into intervals consistent with the timespan of the article. Each element and event term receives a weight corresponding to the total number of elements and event terms located in each time interval the event element or term occupies. Each sentence is scored by the total weight of event elements and terms it contains. Clustering of events based on time has also received little attention. (Foote and Cooper, 2003) investigate clustering towards organizing timestamped digita</context>
</contexts>
<marker>Wu, Li, Lu, Wong, 2007</marker>
<rawString>M. Wu, W. Li, Q. Lu, and K.F. Wong. 2007. Eventbased summarization using time features. Computational Linguistics and Intelligent Text Processing, pages 563–574.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>