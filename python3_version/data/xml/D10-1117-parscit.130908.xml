<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.97579">
Unsupervised Induction of Tree Substitution Grammars
for Dependency Parsing
</title>
<author confidence="0.997367">
Phil Blunsom Trevor Cohn
</author>
<affiliation confidence="0.9995475">
Computing Laboratory Department of Computer Science
University of Oxford University of Sheffield
</affiliation>
<email confidence="0.991958">
Phil.Blunsom@comlab.ox.ac.uk T.Cohn@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.994573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999395294117647">
Inducing a grammar directly from text is
one of the oldest and most challenging tasks
in Computational Linguistics. Significant
progress has been made for inducing depen-
dency grammars, however the models em-
ployed are overly simplistic, particularly in
comparison to supervised parsing models. In
this paper we present an approach to depen-
dency grammar induction using tree substi-
tution grammar which is capable of learn-
ing large dependency fragments and thereby
better modelling the text. We define a hi-
erarchical non-parametric Pitman-Yor Process
prior which biases towards a small grammar
with simple productions. This approach sig-
nificantly improves the state-of-the-art, when
measured by head attachment accuracy.
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999881692307693">
Grammar induction is a central problem in Compu-
tational Linguistics, the aim of which is to induce
linguistic structures from an unannotated text cor-
pus. Despite considerable research effort this un-
supervised problem remains largely unsolved, par-
ticularly for traditional phrase-structure parsing ap-
proaches (Clark, 2001; Klein and Manning, 2002).
Phrase-structure parser induction is made difficult
due to two types of ambiguity: the constituent struc-
ture and the constituent labels. In particular the con-
stituent labels are highly ambiguous, firstly we don’t
know a priori how many there are, and secondly la-
bels that appear high in a tree (e.g., an S category
for a clause) rely on the correct inference of all the
latent labels below them. However recent work on
the induction of dependency grammars has proved
more fruitful (Klein and Manning, 2004). Depen-
dency grammars (Mel&apos;ˇcuk, 1988) should be easier to
induce from text compared to phrase-structure gram-
mars because the set of labels (heads) are directly
observed as the words in the sentence.
Approaches to unsupervised grammar induction,
both for phrase-structure and dependency grammars,
have typically used very simplistic models (Clark,
2001; Klein and Manning, 2004), especially in com-
parison to supervised parsing models (Collins, 2003;
Clark and Curran, 2004; McDonald, 2006). Sim-
ple models are attractive for grammar induction be-
cause they have a limited capacity to overfit, how-
ever they are incapable of modelling many known
linguistic phenomena. We posit that more complex
grammars could be used to better model the unsuper-
vised task, provided that active measures are taken
to prevent overfitting. In this paper we present an
approach to dependency grammar induction using
a tree-substitution grammar (TSG) with a Bayesian
non-parametric prior. This allows the model to learn
large dependency fragments to best describe the text,
with the prior biasing the model towards fewer and
smaller grammar productions.
We adopt the split-head construction (Eisner,
2000; Johnson, 2007) to map dependency parses to
context free grammar (CFG) derivations, over which
we apply a model of TSG induction (Cohn et al.,
2009). The model uses a hierarchical Pitman-Yor
process to encode a backoff path from TSG to CFG
rules, and from lexicalised to unlexicalised rules.
Our best lexicalised model achieves a head attach-
ment accuracy of of 55.7% on Section 23 of the WSJ
data set, which significantly improves over state-of-
the-art and far exceeds an EM baseline (Klein and
Manning, 2004) which obtains 35.9%.
</bodyText>
<page confidence="0.959618">
1204
</page>
<note confidence="0.765747333333333">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204–1213,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
CFG Rule DMV Distribution Description
</note>
<equation confidence="0.839009933333333">
S → LH HR p(root = H) The head of the sentence is H.
LH → Hl p(STOP|dir = L, head = H, val = 0) H has no left children.
LH → L1 p(CONT|dir = L, head = H, val = 0) H has at least one left child.
H
L∗H → Hl p(STOP|dir = L, head = H, val = 1) H has no more left children.
L∗H → L1 p(CONT|dir = L, head = H, val = 1) H has another left child.
H
HR → Hr p(STOP|dir = R, head = H, val = 0) H has no right children.
HR → HR1 p(CONT|dir = R, head = H, val = 0) H has at least one right child.
HR∗ → Hr p(STOP|dir = R, head = H, val = 1) H has no more right children.
HR∗ → HR1 p(CONT|dir = R, head = H, val = 1) H has another right child.
L1H → LC CMH* p(C|dir = L, head = H) C is a left child of H.
HR1 → H*MC CR p(C|dir = R, head = H) C is a right child of H.
CMH* → CR L∗H p = 1 Unambiguous
H*MC → HR∗ LC p = 1 Unambiguous
</equation>
<tableCaption confidence="0.879416">
Table 1: The CFG-DMV grammar schema. Note that the actual CFG is created by instantiating these templates with
part-of-speech tags observed in the data for the variables H and C. Valency (val) can take the value 0 (no attachment
</tableCaption>
<bodyText confidence="0.647289666666667">
in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent;
superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more
children and X∗ that it has already attached a child.
</bodyText>
<sectionHeader confidence="0.984857" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999971795918367">
The most successful framework for unsupervised
dependency induction is the Dependency Model
with Valence (DMV) (Klein and Manning, 2004).
This model has been adapted and extended by a
number of authors and currently represents the state-
of-the-art for dependency induction (Cohen and
Smith, 2009; Headden III et al., 2009). Eisner
(2000) introduced the split-head algorithm which
permits efficient O(|w|3) parsing complexity by
replicating (splitting) each terminal and processing
left and right dependents separately. We employ
the related fold-unfold representation of Johnson
(2007) that defines a CFG equivalent of the split-
head parsing algorithm, allowing us to easily adapt
CFG-based grammar models to dependency gram-
mar. Table 1 shows the equivalent CFG grammar for
the DMV model (CFG-DMV) using the unfold-fold
transformation. The key insight to understanding the
non-terminals in this grammar is that the subscripts
encode the terminals at the boundaries of the span
of that non-terminal. For example the non-terminal
LH encodes that the right most terminal spanned
by this constituent is H (and the reverse for HR),
while AMB encodes that A and B are the left-most
and right-most terminals of the span. The * and 1
superscripts are used to encode the valency of the
head, both indicate that the head has at least one
attached dependent in the specified direction. This
grammar allows O(|w|3) parsing complexity which
follows from the terminals of the dependency tree
being observed, such that each span of the parse
chart uniquely specifies its possible heads (either the
leftmost, rightmost or both) and therefore the num-
ber of possible non-terminals for each span is con-
stant. The transform is illustrated in figures 1a and
1c which show the CFG tree for an example sentence
and the equivalent dependency tree.
Normally DMV based models have been trained
on part-of-speech tags of the words in a sentence,
rather than the words themselves. Headden III et al.
(2009) showed that performance could be improved
by including high frequency words as well as tags
in their model. In this paper we refer to such mod-
els as lexicalised; words which occur more than one
hundred times in the training corpus are represented
by a word/tag pair, while those less frequent are rep-
resented simply by their tags. We are also able to
show that this basic approach to lexicalisation im-
proves the performance of our models.
</bodyText>
<page confidence="0.97849">
1205
</page>
<figure confidence="0.955326454545455">
S
Lhates[V] hates[V]R
Lh1ates[V] hates[V]R1
(a) A TSG-DMV derivation for the sentence George hates broc-
coli. George and broccoli occur less than the lexicalisation cutoff
and are thus represented by the part-of-speech N, while hates is
common and therefore is represented by a word/tag pair. Bold
nodes indicate frontier nodes of elementary trees.
NMhates[V]*
L�
hates[V]
hates[V]l
hates[V]*MN
hates[V]R*
hates[V]r
LN
Nl
NR
Nr
NR
Nr
LN
Nl
S
hates[V]R
hates[V]R1
hates[V]*MN NR
Lhates[V]
L1
hates[V ]
LN NMhates[V]*
(b) A TSG-DMV elementary rule from Figure 1a. This rule en-
codes a dependency between the subject and object of hates that
</figure>
<figureCaption confidence="0.828863">
is not present in the CFG-DMV. Note that this rule doesn’t re-
strict hates, or its arguments, to having a single left and right
child. More dependents can be inserted using additional rules
below the M/L/R frontier non-terminals.
</figureCaption>
<figure confidence="0.570264666666667">
George hates broccoli ROOT
(c) A traditional dependency tree representation of the parse tree
in Figure 1a before applying the lexicalisation cutoff.
</figure>
<figureCaption confidence="0.998625">
Figure 1: TSG-DMV representation of dependency trees.
</figureCaption>
<sectionHeader confidence="0.99683" genericHeader="method">
3 Lexicalised TSG-DMV
</sectionHeader>
<bodyText confidence="0.999995214285714">
The models we investigate in this paper build upon
the CFG-DMV by defining a Tree Substitution
Grammar (TSG) over the space of CFG rules. A
TSG is a 4-tuple, G = (T, N, S, R), where T is a set
of terminal symbols, N is a set of non-terminal sym-
bols, S E N is the distinguished root non-terminal
and R is a set of productions (rules). The produc-
tions take the form of elementary trees – tree frag-
ments of height &gt; 1, where each internal node is
labelled with a non-terminal and each leaf is la-
belled with either a terminal or a non-terminal. Non-
terminal leaves are called frontier non-terminals and
form the substitution sites in the generative process
of creating trees with the grammar.
A derivation creates a tree by starting with the
root symbol and rewriting (substituting) it with an
elementary tree, then continuing to rewrite fron-
tier non-terminals with elementary trees until there
are no remaining frontier non-terminals. We can
represent derivations as sequences of elementary
trees, e, by specifying that during the generation of
the tree each elementary tree is substituted for the
left-most frontier non-terminal. Figure 1a shows a
TSG derivation for the dependency tree in Figure 1c
where bold nonterminal labels denote substitution
sites (root/frontier nodes in the elementary trees).
The probability of a derivation, e, is the product
of the probabilities of its component rules,
</bodyText>
<equation confidence="0.899253">
P(e) = � P(e|c) . (1)
c→eEe
</equation>
<bodyText confidence="0.99658075">
where each rewrite is assumed conditionally inde-
pendent of all others given its root nonterminal, c =
root(e). The probability of a tree, t, and string of
words, w, are
</bodyText>
<equation confidence="0.9589645">
P(t) = � P(e) and P(w) = � P(t) ,
e:tree(e)=t t:yield(t)=w
</equation>
<bodyText confidence="0.9983782">
respectively, where tree(e) returns the tree for the
derivation e and yield(t) returns the string of termi-
nal symbols at the leaves of t.
A Probabilistic Tree Substitution Grammar
(PTSG), like a PCFG, assigns a probability to each
rule in the grammar, denoted P(e|c). The probabil-
ity of a derivation, e, is the product of the proba-
bilities of its component rules. Estimating a PTSG
requires learning the sufficient statistics for P(e|c)
in (1) based on a training sample. Parsing involves
</bodyText>
<page confidence="0.981674">
1206
</page>
<bodyText confidence="0.9999255">
finding the most probable tree for a given string
(arg maxt P(t|w)). This is typically approximated
by finding the most probable derivation which can
be done efficiently using the CYK algorithm.
</bodyText>
<subsectionHeader confidence="0.999401">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.97001109375">
In this work we propose the Tree Substitution Gram-
mar Dependency Model with Valence (TSG-DMV).
We define a hierarchical non-parametric TSG model
on the space of parse trees licensed by the CFG
grammar in Table 1. Our model is a generalisa-
tion of that of Cohn et al. (2009) and Cohn et al.
(2011). We extend those works by moving from a
single level Dirichlet Process (DP) distribution over
rules to a multi-level Pitman-Yor Process (PYP), and
including lexicalisation. The PYP has been shown
to generate distributions particularly well suited to
modelling language (Teh, 2006; Goldwater et al.,
2006). Teh (2006) used a hierarchical PYP to model
backoff in language models, we leverage this same
capability to model backoff in TSG rules. This ef-
fectively allows smoothing from lexicalised to un-
lexicalised grammars, and from TSG to CFG rules.
Here we describe our deepest model which has
a four level hierarchy, depicted graphically in Table
2. In Section 5 we evaluate different subsets of this
hierarchy. The topmost level of our model describes
lexicalised elementary elementary fragments (e) as
produced by a PYP,
e|c ∼ Ge
Ge|ae, be, Plcfg ∼ PYP(ae, be, Plcfg(·|c)) ,
where ae and be control the strength of the backoff
distribution Plcfg. The space of lexicalised TSG rules
will inevitably be very sparse, so the base distribu-
tion Plcfg backs-off to calculating the probability of
a TSG rules as the product of the CFG rules it con-
tains, multiplied by a geometric distribution over the
size of the rule.
</bodyText>
<equation confidence="0.681550333333333">
�Plcfg(e|c) = �sf� (1 − sig)
fEF(e) iEI(e)
× A(lex-cfg-rules(e|c))
α|c ∼ Ae
Ae|alcfg
e , blcfg
e , Pcfg ∼ PYP(alcfg
e , blcfg
e , Pcfg(·|c)),
</equation>
<bodyText confidence="0.9999752">
where I(e) are the set of internal nodes in e exclud-
ing the root, F(e) are the set of frontier non-terminal
nodes, and ci is the non-terminal symbol for node
i and se is the probability of stopping expanding a
node labelled c. The function lex-cfg-rules(e|c) re-
turns the CFG rules internal to e, each of the form
c&apos; → α; each CFG rule is drawn from the back-
off distribution, Ae0. We treat se as a parameter
which is estimated during training, as described in
Section 4.2.
The next level of backoff (Pcfg) removes the lexi-
calisation from the CFG rules, describing the gener-
ation of a lexicalised rule by first generating an un-
lexicalised rule from a PYP, then generating the lex-
icalisaton from a uniform distribution over words:1
</bodyText>
<equation confidence="0.9961655">
Pcfg(α|c) = B(unlex(α)|unlex(c))
1
× |w|I-I
α&apos;|c&apos; ∼ Be0
Be0  |ac g, bcfg, Psh ∼ PYP(acfg, bcfg, Psh(·
e0 e e e
</equation>
<bodyText confidence="0.999735363636364">
where unlex(·) removes the lexicalisation from non-
terminals leaving only the tags.
The final base distribution over CFG-DMV rules
(Psh) is inspired by the skip-head smoothing model
of Headden III et al. (2009). This model showed that
smoothing the DMV by removing the heads from the
CFG rules significantly improved performance. We
replicate this behavior through a final level in our hi-
erarchy which generates the CFG rules without their
heads, then generates the heads from a uniform dis-
tribution:
</bodyText>
<equation confidence="0.87687">
1
Psh(α|c) = C(drop-head(c → α)) ×
α|c ∼ Ce
Ce|ashe , bshe ∼ PYP(ashe , bshe , Uniform(·|c)),
</equation>
<bodyText confidence="0.999909875">
where drop-head(·) removes the symbols that mark
the head on the CFG rules, and P is the set of part-
of-speech tags. Each stage of backoff is illustrated in
Table 2, showing the rules generated from the TSG
elementary tree in Figure 1b.
Note that while the supervised model of Cohn et
al. (2009) used a fixed back-off PCFG distribution,
this model implicitly infers this distribution within
</bodyText>
<footnote confidence="0.9207145">
1All unlexicalised words are actually given the generic UNK
symbol as their lexicalisation.
</footnote>
<figure confidence="0.891608923076923">
|P|
1207
Plcfg Pcfg Psh
S
Lhates[V] hates[V]R
Lhates[V]
1
Lhates[V ]
S
L· ·R
L·
L1
·
S
LV V R
LV
L1
V
1
Lhates[V]
LN NMhates[V]*
hates[V]R
hates[V ]R1
L1
V
LN NMV*
V R
V R1
L1
·
LN NM·*
·R1
·R
V R1
V*MN NR
·R1
·*MN NR
hates[V ]R1
hates[V]*MN NR
</figure>
<tableCaption confidence="0.971496">
Table 2: Backoff trees for the elementary tree in Figure 1b.
</tableCaption>
<bodyText confidence="0.999921">
its hierarchy, essentially learning the DMV model
embedded in the TSG.
In this application to dependency grammar our
model is capable of learning tree fragments which
group CFG parameters. As such the model can learn
to condition dependency links on the valence, e.g. by
combining LH —* L1H and L1H —* LC CMH* rules
into a single fragment the model can learn a pa-
rameter that the leftmost child of H is C. By link-
ing together multiple L1H or HR1 non-terminals the
model can learn groups of dependencies that occur
together, e.g. tree fragments representing the com-
plete preferred argument frame of a verb.
</bodyText>
<sectionHeader confidence="0.999824" genericHeader="method">
4 Inference
</sectionHeader>
<subsectionHeader confidence="0.942031">
4.1 Training
</subsectionHeader>
<bodyText confidence="0.999972454545454">
To train our model we use Markov Chain Monte
Carlo sampling (Geman and Geman, 1984). Where
previous supervised TSG models (Cohn et al., 2009)
permit an efficient local sampler, the lack of an ob-
served parse tree in our unsupervised model makes
this sampler not applicable. Instead we use a re-
cently proposed blocked Metroplis-Hastings (MH)
sampler (Cohn and Blunsom, 2010) which exploits a
factorisation of the derivation probabilities such that
whole trees can be sampled efficiently. See Cohn
and Blunsom (2010) for details. That algorithm is
applied using a dynamic program over an observed
tree, the generalisation to our situation of an inside
pass over the space of all trees is straightforward.
A final consideration is the initialisation of the
sampler. Klein and Manning (2004) emphasised the
importance of the initialiser for achieving good per-
formance with their model. We employ the same
harmonic initialiser as described in that work. The
initial derivations for our sampler are the Viterbi
derivations under the CFG parameterised according
to this initialiser.
</bodyText>
<subsectionHeader confidence="0.998008">
4.2 Sampling hyperparameters
</subsectionHeader>
<bodyText confidence="0.996740666666667">
We treat the hyper-parameters {(a&apos;, b�c , sc) , c E N}
as random variables in our model and infer their val-
ues during training. We choose quite vague priors
for each hyper-parameter, encoding our lack of in-
formation about their values.
We place prior distributions on the PYP discount
ac and concentration bc hyperparamters and sam-
ple their values using a slice sampler. We use the
range doubling slice sampling technique of (Neal,
2003) to draw a new sample of a0c from its condi-
tional distribution.2 For the discount parameters ac
we employ a uniform Beta distribution, as we have
no strong prior knowledge of what its value should
be (ac — Beta(1,1)). Similarly, we treat the concen-
tration parameters, bc, as being generated by a vague
gamma prior, bc — Gamma(1, 1), and sample a new
value b0 c using the same slice-sampling approach as
for ac:
</bodyText>
<equation confidence="0.845893">
P(bc|z) a P(z|bc) x Gamma(bc|1, 1).
</equation>
<footnote confidence="0.998533666666667">
2We made use of the slice sampler included in
Mark Johnson’s Adaptor Grammar implementation
http://www.cog.brown.edu/˜mj/Software.htm.
</footnote>
<page confidence="0.813299">
1208
</page>
<table confidence="0.9993528">
Corpus Words Sentences
Sections 2-21 (|x |&lt; 10) 42505 6007
Section 22 (|x |&lt; 10) 1805 258
Section 23 (|x |&lt; 10) 2649 398
Section 23 (|x |&lt; oc) 49368 2416
</table>
<tableCaption confidence="0.9299565">
Table 3: Corpus statistics for the training and testing data
for the TSG-DMV model. All models are trained on the
gold standard part-of-speech tags after removing punctu-
ation.
</tableCaption>
<bodyText confidence="0.99846175">
We use a vague Beta prior for the stopping probabil-
ities in Plcfg, s, — Beta(1,1).
All the hyper-parameters are resampled after ev-
ery 10th sample of the corpus derivations.
</bodyText>
<subsectionHeader confidence="0.998258">
4.3 Parsing
</subsectionHeader>
<bodyText confidence="0.9999385625">
Unfortunately finding the maximising parse tree for
a string under our TSG-DMV model is intractable
due to the inter-rule dependencies created by the
PYP formulation. Previous work has used Monte
Carlo techniques to sample for one of the maxi-
mum probability parse (MPP), maximum probabil-
ity derivation (MPD) or maximum marginal parse
(MMP) (Cohn et al., 2009; Bod, 2006). We take a
simpler approach and use the Viterbi algorithm to
calculate the MPD under an approximating TSG de-
fined by the last set of derivations sampled for the
corpus during training. Our results indicate that this
is a reasonable approximation, though the experi-
ence of other researchers suggests that calculating
the MMP under the approximating TSG may also
be beneficial for DMV (Cohen et al., 2008).
</bodyText>
<sectionHeader confidence="0.999539" genericHeader="conclusions">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999992840909091">
We follow the standard evaluation regime for DMV
style models by performing experiments on the text
of the WSJ section of the Penn. Treebank (Marcus et
al., 1993) and reporting head attachment accuracy.
Like previous work we pre-process the training and
test data to remove punctuation, training our unlex-
icalised models on the gold-standard part-of-speech
tags, and including words occurring more than 100
times in our lexicalised models (Headden III et al.,
2009). It is very difficult for an unsupervised model
to learn from long training sentences as they contain
a great deal of ambiguity, therefore the majority of
DMV based models have been trained on sentences
restricted in length to &lt; 10 tokens.3 This has the
added benefit of decreasing the runtime for exper-
iments. We present experiments with this training
scenario. The training data comes from sections 2-
21, while section 23 is used for evaluation. An ad-
vantage of our sampling based approach over pre-
vious work is that we infer all the hyperparameters,
as such we don’t require the use of section 22 for
tuning the model.
The models are evaluated in terms of head attach-
ment accuracy (the percentage of correctly predicted
head indexes for each token in the test data), on two
subsets of the testing data. Although we can argue
that unsupervised models are better learnt from short
sentences, it is much harder to argue that we don’t
then need to be able to parse long sentences with a
trained model. The most commonly employed test
set mirrors the training data by only including sen-
tences &lt; 10. In this work we focus on the accuracy
of our models on the whole of section 23, without
any pruning for length. The training and testing cor-
pora statistics are presented in Table 3. Subsequent
to the evaluation reported in Table 4 we use section
22 to report the correlation between heldout accu-
racy and the model log-likelihood (LLH) for ana-
lytic purposes.
As we are using a sampler during training, the re-
sult of any single run is non-deterministic and will
exhibit a degree of variance. All our reported results
are the mean and standard deviation (Q) from forty
sampling runs.
</bodyText>
<subsectionHeader confidence="0.938106">
5.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999984909090909">
Table 4 shows the head attachment accuracy results
for our TSG-DMV, plus many other significant pre-
viously proposed models. The subset of hierarchical
priors used by each model is noted in brackets.
The performance of our models is extremely en-
couraging, particularly the fact that it achieves the
highest reported accuracy on the full test set by a
considerable margin. On the |w |&lt; 10 test set all the
TSG-DMVs are second only to the L-EVG model
of Headden III et al. (2009). The L-EVG model
extends DMV by adding additional lexicalisation,
</bodyText>
<footnote confidence="0.527088">
3See Spitkovsky et al. (2010a) for an exception to this rule.
</footnote>
<page confidence="0.967298">
1209
</page>
<table confidence="0.999114227272727">
Directed Attachment
Accuracy on WSJ23
Model |w |&lt; 10 |w |&lt; 00
Attach-Right 38.4 31.7
EM (Klein and Manning, 2004) 46.1 35.9
Dirichlet (Cohen et al., 2008) 46.1 36.9
LN (Cohen et al., 2008) 59.4 40.5
SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4
DMV (Headden III et al., 2009) 55.7σ=8.0 -
DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 -
EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 -
L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 -
Less is More (Spitkovsky et al., 2010a) 56.2 44.1
Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0
Viterbi EM (Spitkovsky et al., 2010b) 65.3 47.9
Hypertext Markup (Spitkovsky et al., 2010c) 69.3 50.4
Adaptor Grammar (Cohen et al., 2010) 50.2 -
TSG-DMV (Pcfg) 65.9σ=2.4 53.1σ=2.4
TSG-DMV (Pcfg, Psh) 65.1σ=2.2 51.5σ=2.0
LexTSG-DMV (Plcfg, Pcfg) 67.2σ=1.4 55.2σ=2.2
LexTSG-DMV (Plcfg, Pcfg, Psh) 67.7σ=1.5 55.7σ=2.0
Supervised MLE (Cohen and Smith, 2009) 84.5 68.8
</table>
<tableCaption confidence="0.812905">
Table 4: Mean and variance for the head attachment accu-
racy of our TSG-DMV models (highlighted) with varying
backoff paths, and many other high performing models.
Citations indicate where the model and result were re-
ported. Our models labelled TSG used an unlexicalised
top level G, PYP, while those labelled LexTSG used the
full lexicalised G,.
</tableCaption>
<bodyText confidence="0.999906523076923">
valency conditioning, interpolated back-off smooth-
ing and a random initialiser. In particular Head-
den III et al. (2009) shows that the random initialiser
is crucial for good performance, however this ini-
tialiser requires training 1000 models to select a sin-
gle best model for evaluation and results in consider-
able variance in test set performance. Note also that
our model exhibits considerably less variance than
those induced using this random initialiser, suggest-
ing that the combination of the harmonic initialiser
and blocked-MH sampling may be a more practica-
ble training regime.
The recently proposed Adaptor Grammar DMV
model of Cohen et al. (2010) is similar in many
way to our TSG model, incorporating a Pitman Yor
prior over units larger than CFG rules. As such it
is surprising that our model is performing signif-
icantly better than this model. We can identify a
number of differences that may impact these results:
the Adaptor Grammar model is trained using vari-
ational inference with the space of tree fragments
truncated, while we employ a sampler which can
nominally explore the full space of tree fragments;
and the adapted tree fragments must be complete
subtrees (i.e. they don’t contain variables), whereas
our model can make use of arbitrary tree fragments.
An interesting avenue for further research would be
to extend the variational algorithm of Cohen et al.
(2010) to our TSG model, possibly speeding infer-
ence and allowing easier parallelisation.
In Figure 2a we graph the model LLH on the train-
ing data versus the head attachment accuracy on the
heldout set. The graph was generated by running
160 models for varying numbers of samples and
evaluating their accuracy. This graph indicates that
the improvements in the posterior probability of the
model are correlated with the evaluation, though the
correlation is not as high as we might require in or-
der to use LLH as a model selection criteria similar
to Headden III et al. (2009). Further refinements to
the model could improve this correlation.
The scaling perfomance of the model as the num-
ber of samples is increased is shown in Figure 2b.
Performance improves as the training data is sam-
pled for longer, and continues to trend upwards be-
yond 1000 samples (the point for which we’ve re-
ported results in Table 4). This suggests that longer
sampling runs – and better inference techniques –
could yield further improvements.
For further analysis Table 5 shows the accuracy
of the model at predicting the head for frequent
types, while Table 6 shows the performance on de-
pendencies of various lengths. We emphasise that
these results are for the single best performing sam-
pler run on the heldout corpus and there is consid-
erable variation in the analyses produced by each
sampler. Unsurprisingly, the model appears to be
more accurate when predicting short dependencies,
a result that is also reflected in the per type accura-
cies. The model is relatively good at identifying the
root verb in each sentence, especially those headed
by past tense verbs (VBD, was), and to a lesser de-
gree VBPs (are). Conjunctions such as and pose
a particular difficulty when evaluating dependency
models as the correct modelling of these remains a
</bodyText>
<page confidence="0.951771">
1210
</page>
<figure confidence="0.999488381294964">
Perplexity vs. Accuracy Correlation
−5.4 −5.3 −5.2 −5.1 −5.0 −4.9 −4.8
PYP.LLH
(a) Correlation (R2 = 0.2) between the training LLH of the
PYP Model and heldout directed head attachment accuracy
(WSJ Section 22, |w |≤ 10) for LexTSG-DMV (Plcfg, Pcfg, Psh).
Directed.Attachment.Accuracy
56 58 60 62 64 66 68
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
● ●
●
● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ● ● ●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
● ●
●
●
●
●
●
●
●
●
●
●●●●
●
● ●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
Number of Samples vs. Accuracy
●
●
●
●
●
0 500 1000 1500 2000
Samples
(b) Mean heldout directed head attachment accuracy (WSJ Sec-
tion 22, |w |≤ 10) versus the number of samples used during
training for LexTSG-DMV (Plcfg, Pcfg, Psh).
Directed Attachment Accuracy
59 60 61 62 63 64 65 66
</figure>
<figureCaption confidence="0.99722">
Figure 2
</figureCaption>
<bodyText confidence="0.99960916">
contentious linguistic issue and it’s not clear what
the ‘correct’ analysis should be. Our model gets a
respectable 75% accuracy for and conjunctions, but
for conjunctions (CC) as a whole, the model per-
forms poorly (39%).
Table 7 list the most frequent TSG rules lexi-
calised with has. The most frequent rule is sim-
ply the single level equivalent of the DMV termi-
nal rule for has. Almost as frequent is rule 3, here
the grammar incorporates the terminal into a larger
elementary fragment, encoding that it is the head
of the past participle occuring immediately to it’s
right. This shows the model’s ability to learn the
verb’s argument position conditioned on both the
head and child type, something lacking in DMV.
Rule 7 further refines this preferred analysis for has
been by lexicalising both the head and child. Rules
(4,5,8,10) employ similar conditioning for proper
and ordinary nouns heading noun phrases to the
left of has. We believe that it is the ability of the
TSG to encode stronger constraints on argument po-
sitions that leads to the model’s higher accuracy
on longer sentences, while other models do well
on shorter sentences but relatively poorly on longer
ones (Spitkovsky et al., 2010c).
</bodyText>
<page confidence="0.974276">
1211
</page>
<figure confidence="0.4600425">
6 Conclusion
●
● ●●
● ● ●●●
</figure>
<bodyText confidence="0.999332565217391">
In this paper we have made two significant contri-
butions to probabilistic modelling and grammar in-
duction. We have shown that it is possible to suc-
cessfully learn hierarchical Pitman-Yor models that
encode deep and complex backoff paths over highly
structured latent spaces. By applying these models
to the induction of dependency grammars we have
also been able to advance the state-of-the-art, in-
creasing the head attachment accuracy on section 23
of the Wall Street Journal Corpus by more than 5%.
Further gains in performance may come from an
exploration of the backoff paths employed within the
model. In particular more extensive experimentation
with alternate priors and larger training data may al-
low the removal of the lexicalisation cutoff which is
currently in place to counter sparsity.
We envisage that in future many grammar for-
malisms that have been shown to be effective in su-
pervised parsing, such as categorial, unification and
tree adjoining grammars, will prove amenable to
unsupervised induction using the hierarchical non-
parametric modelling approaches we have demon-
strated in this paper.
</bodyText>
<table confidence="0.998757090909091">
Count
1 94 L∗has−V BZ →
2 74 L1has−V BZ →
3 71 has−V BZ*MV BN →
4 54 NNMhas−V BZ* →
5 36 NNMhas−V BZ* →
6 36 has−V BZR∗ →
7 30 has−V BZ*Mbeen−V BN →
8 27 NNPMhas−V BZ* →
9 25 has−V BZR →
10 18 L1has−V BZ →
LexTSG-DMV Rules
(L∗has−V BZ has-VBZl)
(L1has−V BZ (LNN L1NN) NNMhas−V BZ*)
(has−V BZ*MV BN (has−V BZR∗ has-VBZ,) LVBN)
(NNMhas−V BZ* NNR (L∗has−V BZ has-VBZl))
(NNMhas−V BZ* NNR L∗has−V BZ)
(has−V BZR∗ (has−V BZR1 has−V BZ*MV BN (VBNR VBN,)))
(has−V BZ*Mbeen−V BN (has−V BZR∗ has-VBZ,) Lbeen−V BN)
(NNPMhas−V BZ* NNPR (L∗has−V BZ has-VBZl))
(has−V BZR (has−V BZR1 has−V BZ*MNNS (NNSR NNSR1)))
(L1has−V BZ LNNP NNPMhas−V BZ*)
</table>
<tableCaption confidence="0.999912">
Table 7: The ten most frequent LexTSG-DMV rules in a final training sample that contain has.
</tableCaption>
<sectionHeader confidence="0.994242" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999591689189189">
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proc. of the 44th Annual Meeting of
the ACL and 21st International Conference on Compu-
tational Linguistics (COLING/ACL-2006), pages 865–
872, Sydney, Australia, July.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proc. of the
42nd Annual Meeting of the ACL (ACL-2004), pages
103–110, Barcelona, Spain.
Alexander Clark. 2001. Unsupervised induction of
stochastic context-free grammars using distributional
clustering. In ConLL ’01: Proceedings of the 2001
workshop on Computational Natural Language Learn-
ing, pages 1–8. Association for Computational Lin-
guistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In NAACL ’09: Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
74–82, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Daphne Koller, Dale
Schuurmans, Yoshua Bengio, and Lon Bottou, editors,
NIPS, pages 321–328. MIT Press.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked inference
in Bayesian tree substitution grammars. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, page To Appear, Uppsala,
Sweden.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In NAACL ’09: Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics on ZZZ, pages 548–556,
Morristown, NJ, USA. Association for Computational
Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2011. Inducing tree-substitution grammars. Journal
of Machine Learning Research. To Appear.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589–637.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29–62. Kluwer Academic
Publishers, October.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721–741.
Sharon Goldwater, Tom Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Y. Weiss,
B. Sch¨olkopf, and J. Platt, editors, Advances in Neural
Information Processing Systems 18, pages 459–466.
MIT Press, Cambridge, MA.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
101–109, Boulder, Colorado, June.
</reference>
<page confidence="0.97888">
1212
</page>
<table confidence="0.91481285">
Child Tag Predicted Accuracy
Head Correct (%)
NN 181 0.64
NNP 130 0.71
DT 127 0.87
NNS 108 0.72
VBD 108 0.81
JJ 106 0.80
IN 81 0.55
RB 65 0.61
PRP 64 0.97
VBZ 47 0.80
VBN 36 0.86
VBP 30 0.77
CD 26 0.23
VB 25 0.68
the 42 0.88
was 29 0.97
The 25 0.83
of 18 0.78
</table>
<figure confidence="0.826434181818182">
a 18 0.90
to 17 0.50
in 16 0.89
is 15 0.79
n’t 15 0.83
were 12 0.86
are 11 0.92
It 11 1.00
for 9 0.64
and 9 0.75
’s 9 1.00
</figure>
<tableCaption confidence="0.522693333333333">
Table 5: Per tag type predicted count and accuracy,
for the most frequent 15 un/lexicalised tokens on the
WSJ Section 22 |w |≤ 10 heldout set (LexTSG-DMV
</tableCaption>
<reference confidence="0.9958109375">
(Plcfg, Pcfg&apos; Psh)).
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 168–175, Prague, Czech Republic, June.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
128–135, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In ACL ’04: Proceedings
</reference>
<table confidence="0.882948090909091">
Distance Precision Recall F1
1 0.70 0.75 0.72
2 0.70 0.62 0.65
3 0.66 0.62 0.64
4 0.56 0.56 0.56
5 0.53 0.49 0.51
6 0.59 0.66 0.62
7 0.50 0.44 0.47
8 0.57 0.33 0.42
9 0.67 0.40 0.50
10 1.00 0.17 0.29
</table>
<tableCaption confidence="0.924661">
Table 6: Link distance precision, recall and f-score, on
the WSJ Section 22 |w |≤ 10 heldout set.
</tableCaption>
<reference confidence="0.999197657142857">
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, page 478.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313–330.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Igor&apos; A. Mel&apos;ˇcuk. 1988. Dependency Syntax: theory and
practice. State University of New York Press, Albany.
Radford Neal. 2003. Slice sampling. Annals of Statis-
tics, 31:705–767.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From Baby Steps to Leapfrog: How
“Less is More” in unsupervised dependency parsing.
In Human Language Technologies: The 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,
and Christopher D. Manning. 2010b. Viterbi training
improves unsupervised dependency parsing. In Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning (CoNLL-2010).
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010c. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2010).
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 985–
992.
</reference>
<page confidence="0.973442">
1213
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.739215">
<title confidence="0.99978">Unsupervised Induction of Tree Substitution for Dependency Parsing</title>
<author confidence="0.999779">Phil Blunsom Trevor Cohn</author>
<affiliation confidence="0.999818">Computing Laboratory Department of Computer Science University of Oxford University of Sheffield</affiliation>
<email confidence="0.783945">Phil.Blunsom@comlab.ox.ac.ukT.Cohn@dcs.shef.ac.uk</email>
<abstract confidence="0.996728888888889">Inducing a grammar directly from text is one of the oldest and most challenging tasks in Computational Linguistics. Significant progress has been made for inducing dependency grammars, however the models employed are overly simplistic, particularly in comparison to supervised parsing models. In this paper we present an approach to dependency grammar induction using tree substitution grammar which is capable of learning large dependency fragments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>An all-subtrees approach to unsupervised parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006),</booktitle>
<pages>865--872</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="18528" citStr="Bod, 2006" startWordPosition="3109" endWordPosition="3110">andard part-of-speech tags after removing punctuation. We use a vague Beta prior for the stopping probabilities in Plcfg, s, — Beta(1,1). All the hyper-parameters are resampled after every 10th sample of the corpus derivations. 4.3 Parsing Unfortunately finding the maximising parse tree for a string under our TSG-DMV model is intractable due to the inter-rule dependencies created by the PYP formulation. Previous work has used Monte Carlo techniques to sample for one of the maximum probability parse (MPP), maximum probability derivation (MPD) or maximum marginal parse (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and reporting hea</context>
</contexts>
<marker>Bod, 2006</marker>
<rawString>Rens Bod. 2006. An all-subtrees approach to unsupervised parsing. In Proc. of the 44th Annual Meeting of the ACL and 21st International Conference on Computational Linguistics (COLING/ACL-2006), pages 865– 872, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the ACL (ACL-2004),</booktitle>
<pages>103--110</pages>
<location>Barcelona,</location>
<contexts>
<context position="2317" citStr="Clark and Curran, 2004" startWordPosition="336" endWordPosition="339">ce of all the latent labels below them. However recent work on the induction of dependency grammars has proved more fruitful (Klein and Manning, 2004). Dependency grammars (Mel&apos;ˇcuk, 1988) should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence. Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models (Clark, 2001; Klein and Manning, 2004), especially in comparison to supervised parsing models (Collins, 2003; Clark and Curran, 2004; McDonald, 2006). Simple models are attractive for grammar induction because they have a limited capacity to overfit, however they are incapable of modelling many known linguistic phenomena. We posit that more complex grammars could be used to better model the unsupervised task, provided that active measures are taken to prevent overfitting. In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior. This allows the model to learn large dependency fragments to best describe the text, with the prior biasing </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In Proc. of the 42nd Annual Meeting of the ACL (ACL-2004), pages 103–110, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised induction of stochastic context-free grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>In ConLL ’01: Proceedings of the 2001 workshop on Computational Natural Language Learning,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1311" citStr="Clark, 2001" startWordPosition="180" endWordPosition="181">agments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy. 1 Introduction Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus. Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches (Clark, 2001; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved more fruitful (Klein and Manning, 2004). Dependency grammars (Mel&apos;ˇcuk, 1988) should be easier to induce </context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In ConLL ’01: Proceedings of the 2001 workshop on Computational Natural Language Learning, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>74--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5442" citStr="Cohen and Smith, 2009" startWordPosition="907" endWordPosition="910">alue 0 (no attachment in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent; superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more children and X∗ that it has already attached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this g</context>
<context position="21953" citStr="Cohen and Smith, 2009" startWordPosition="3692" endWordPosition="3695">odels is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |&lt; 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3See Spitkovsky et al. (2010a) for an exception to this rule. 1209 Directed Attachment Accuracy on WSJ23 Model |w |&lt; 10 |w |&lt; 00 Attach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al., 2008) 46.1 36.9 LN (Cohen et al., 2008) 59.4 40.5 SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4 DMV (Headden III et al., 2009) 55.7σ=8.0 - DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 - EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 - L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 - Less is More (Spitkovsky et al., 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0 Viterbi EM (Spitkovsky et al., 2010b) 65.3 47.9 Hypertext Markup (Spitkovsky et al., 2010c) 69.3 50.4 Adaptor Grammar (Cohen et al., 2010) 50.2 - TSG-DMV (Pcfg) 65.9σ=2.4 53.1σ=2.4 TSG-DMV (Pcfg, Psh) 65.1σ=2.2 51.5σ=2.0 LexTSG-DMV (Plcfg, Pcfg) 67.2σ=1.4 55.2σ=2.2 LexTSG-DMV (Plcfg, Pcfg, </context>
</contexts>
<marker>Cohen, Smith, 2009</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2009. Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 74–82, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Logistic normal priors for unsupervised probabilistic grammar induction.</title>
<date>2008</date>
<pages>321--328</pages>
<editor>In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and Lon Bottou, editors, NIPS,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="18932" citStr="Cohen et al., 2008" startWordPosition="3174" endWordPosition="3177">ormulation. Previous work has used Monte Carlo techniques to sample for one of the maximum probability parse (MPP), maximum probability derivation (MPD) or maximum marginal parse (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and reporting head attachment accuracy. Like previous work we pre-process the training and test data to remove punctuation, training our unlexicalised models on the gold-standard part-of-speech tags, and including words occurring more than 100 times in our lexicalised models (Headden III et al., 2009). It is very difficult for an unsupervised model to learn from long training sentences as they contain a great deal of </context>
<context position="21872" citStr="Cohen et al., 2008" startWordPosition="3676" endWordPosition="3679">hical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |&lt; 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3See Spitkovsky et al. (2010a) for an exception to this rule. 1209 Directed Attachment Accuracy on WSJ23 Model |w |&lt; 10 |w |&lt; 00 Attach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al., 2008) 46.1 36.9 LN (Cohen et al., 2008) 59.4 40.5 SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4 DMV (Headden III et al., 2009) 55.7σ=8.0 - DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 - EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 - L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 - Less is More (Spitkovsky et al., 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0 Viterbi EM (Spitkovsky et al., 2010b) 65.3 47.9 Hypertext Markup (Spitkovsky et al., 2010c) 69.3 50.4 Adaptor Grammar (Cohen et al., 2010) 50.2 - TSG-DMV (Pcfg) 65.9σ=2.4 53.1σ=2.4 TSG-DMV (Pcfg, Psh) 65.1σ=2.2</context>
</contexts>
<marker>Cohen, Gimpel, Smith, 2008</marker>
<rawString>Shay B. Cohen, Kevin Gimpel, and Noah A. Smith. 2008. Logistic normal priors for unsupervised probabilistic grammar induction. In Daphne Koller, Dale Schuurmans, Yoshua Bengio, and Lon Bottou, editors, NIPS, pages 321–328. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>David M Blei</author>
<author>Noah A Smith</author>
</authors>
<title>Variational inference for adaptor grammars.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22400" citStr="Cohen et al., 2010" startWordPosition="3768" endWordPosition="3771">ttach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al., 2008) 46.1 36.9 LN (Cohen et al., 2008) 59.4 40.5 SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4 DMV (Headden III et al., 2009) 55.7σ=8.0 - DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 - EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 - L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 - Less is More (Spitkovsky et al., 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0 Viterbi EM (Spitkovsky et al., 2010b) 65.3 47.9 Hypertext Markup (Spitkovsky et al., 2010c) 69.3 50.4 Adaptor Grammar (Cohen et al., 2010) 50.2 - TSG-DMV (Pcfg) 65.9σ=2.4 53.1σ=2.4 TSG-DMV (Pcfg, Psh) 65.1σ=2.2 51.5σ=2.0 LexTSG-DMV (Plcfg, Pcfg) 67.2σ=1.4 55.2σ=2.2 LexTSG-DMV (Plcfg, Pcfg, Psh) 67.7σ=1.5 55.7σ=2.0 Supervised MLE (Cohen and Smith, 2009) 84.5 68.8 Table 4: Mean and variance for the head attachment accuracy of our TSG-DMV models (highlighted) with varying backoff paths, and many other high performing models. Citations indicate where the model and result were reported. Our models labelled TSG used an unlexicalised top level G, PYP, while those labelled LexTSG used the full lexicalised G,. valency conditioning, inter</context>
<context position="23630" citStr="Cohen et al. (2010)" startWordPosition="3957" endWordPosition="3960">k-off smoothing and a random initialiser. In particular Headden III et al. (2009) shows that the random initialiser is crucial for good performance, however this initialiser requires training 1000 models to select a single best model for evaluation and results in considerable variance in test set performance. Note also that our model exhibits considerably less variance than those induced using this random initialiser, suggesting that the combination of the harmonic initialiser and blocked-MH sampling may be a more practicable training regime. The recently proposed Adaptor Grammar DMV model of Cohen et al. (2010) is similar in many way to our TSG model, incorporating a Pitman Yor prior over units larger than CFG rules. As such it is surprising that our model is performing significantly better than this model. We can identify a number of differences that may impact these results: the Adaptor Grammar model is trained using variational inference with the space of tree fragments truncated, while we employ a sampler which can nominally explore the full space of tree fragments; and the adapted tree fragments must be complete subtrees (i.e. they don’t contain variables), whereas our model can make use of arb</context>
</contexts>
<marker>Cohen, Blei, Smith, 2010</marker>
<rawString>Shay B. Cohen, David M. Blei, and Noah A. Smith. 2010. Variational inference for adaptor grammars. In Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked inference in Bayesian tree substitution grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>page To Appear, Uppsala,</location>
<contexts>
<context position="15887" citStr="Cohn and Blunsom, 2010" startWordPosition="2675" endWordPosition="2678">leftmost child of H is C. By linking together multiple L1H or HR1 non-terminals the model can learn groups of dependencies that occur together, e.g. tree fragments representing the complete preferred argument frame of a verb. 4 Inference 4.1 Training To train our model we use Markov Chain Monte Carlo sampling (Geman and Geman, 1984). Where previous supervised TSG models (Cohn et al., 2009) permit an efficient local sampler, the lack of an observed parse tree in our unsupervised model makes this sampler not applicable. Instead we use a recently proposed blocked Metroplis-Hastings (MH) sampler (Cohn and Blunsom, 2010) which exploits a factorisation of the derivation probabilities such that whole trees can be sampled efficiently. See Cohn and Blunsom (2010) for details. That algorithm is applied using a dynamic program over an observed tree, the generalisation to our situation of an inside pass over the space of all trees is straightforward. A final consideration is the initialisation of the sampler. Klein and Manning (2004) emphasised the importance of the initialiser for achieving good performance with their model. We employ the same harmonic initialiser as described in that work. The initial derivations </context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, page To Appear, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate tree-substitution grammars.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on ZZZ,</booktitle>
<pages>548--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3173" citStr="Cohn et al., 2009" startWordPosition="470" endWordPosition="473">sed to better model the unsupervised task, provided that active measures are taken to prevent overfitting. In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior. This allows the model to learn large dependency fragments to best describe the text, with the prior biasing the model towards fewer and smaller grammar productions. We adopt the split-head construction (Eisner, 2000; Johnson, 2007) to map dependency parses to context free grammar (CFG) derivations, over which we apply a model of TSG induction (Cohn et al., 2009). The model uses a hierarchical Pitman-Yor process to encode a backoff path from TSG to CFG rules, and from lexicalised to unlexicalised rules. Our best lexicalised model achieves a head attachment accuracy of of 55.7% on Section 23 of the WSJ data set, which significantly improves over state-ofthe-art and far exceeds an EM baseline (Klein and Manning, 2004) which obtains 35.9%. 1204 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204–1213, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics CFG Rule DMV Di</context>
<context position="11284" citStr="Cohn et al. (2009)" startWordPosition="1873" endWordPosition="1876">ent rules. Estimating a PTSG requires learning the sufficient statistics for P(e|c) in (1) based on a training sample. Parsing involves 1206 finding the most probable tree for a given string (arg maxt P(t|w)). This is typically approximated by finding the most probable derivation which can be done efficiently using the CYK algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest </context>
<context position="14390" citStr="Cohn et al. (2009)" startWordPosition="2411" endWordPosition="2414">ads from the CFG rules significantly improved performance. We replicate this behavior through a final level in our hierarchy which generates the CFG rules without their heads, then generates the heads from a uniform distribution: 1 Psh(α|c) = C(drop-head(c → α)) × α|c ∼ Ce Ce|ashe , bshe ∼ PYP(ashe , bshe , Uniform(·|c)), where drop-head(·) removes the symbols that mark the head on the CFG rules, and P is the set of partof-speech tags. Each stage of backoff is illustrated in Table 2, showing the rules generated from the TSG elementary tree in Figure 1b. Note that while the supervised model of Cohn et al. (2009) used a fixed back-off PCFG distribution, this model implicitly infers this distribution within 1All unlexicalised words are actually given the generic UNK symbol as their lexicalisation. |P| 1207 Plcfg Pcfg Psh S Lhates[V] hates[V]R Lhates[V] 1 Lhates[V ] S L· ·R L· L1 · S LV V R LV L1 V 1 Lhates[V] LN NMhates[V]* hates[V]R hates[V ]R1 L1 V LN NMV* V R V R1 L1 · LN NM·* ·R1 ·R V R1 V*MN NR ·R1 ·*MN NR hates[V ]R1 hates[V]*MN NR Table 2: Backoff trees for the elementary tree in Figure 1b. its hierarchy, essentially learning the DMV model embedded in the TSG. In this application to dependency g</context>
<context position="15656" citStr="Cohn et al., 2009" startWordPosition="2638" endWordPosition="2641">gments which group CFG parameters. As such the model can learn to condition dependency links on the valence, e.g. by combining LH —* L1H and L1H —* LC CMH* rules into a single fragment the model can learn a parameter that the leftmost child of H is C. By linking together multiple L1H or HR1 non-terminals the model can learn groups of dependencies that occur together, e.g. tree fragments representing the complete preferred argument frame of a verb. 4 Inference 4.1 Training To train our model we use Markov Chain Monte Carlo sampling (Geman and Geman, 1984). Where previous supervised TSG models (Cohn et al., 2009) permit an efficient local sampler, the lack of an observed parse tree in our unsupervised model makes this sampler not applicable. Instead we use a recently proposed blocked Metroplis-Hastings (MH) sampler (Cohn and Blunsom, 2010) which exploits a factorisation of the derivation probabilities such that whole trees can be sampled efficiently. See Cohn and Blunsom (2010) for details. That algorithm is applied using a dynamic program over an observed tree, the generalisation to our situation of an inside pass over the space of all trees is straightforward. A final consideration is the initialisa</context>
<context position="18516" citStr="Cohn et al., 2009" startWordPosition="3105" endWordPosition="3108">ined on the gold standard part-of-speech tags after removing punctuation. We use a vague Beta prior for the stopping probabilities in Plcfg, s, — Beta(1,1). All the hyper-parameters are resampled after every 10th sample of the corpus derivations. 4.3 Parsing Unfortunately finding the maximising parse tree for a string under our TSG-DMV model is intractable due to the inter-rule dependencies created by the PYP formulation. Previous work has used Monte Carlo techniques to sample for one of the maximum probability parse (MPP), maximum probability derivation (MPD) or maximum marginal parse (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and r</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on ZZZ, pages 548–556, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research.</journal>
<note>To Appear.</note>
<contexts>
<context position="11307" citStr="Cohn et al. (2011)" startWordPosition="1878" endWordPosition="1881"> PTSG requires learning the sufficient statistics for P(e|c) in (1) based on a training sample. Parsing involves 1206 finding the most probable tree for a given string (arg maxt P(t|w)). This is typically approximated by finding the most probable derivation which can be done efficiently using the CYK algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest model which has a four </context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2011</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2011. Inducing tree-substitution grammars. Journal of Machine Learning Research. To Appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2293" citStr="Collins, 2003" startWordPosition="334" endWordPosition="335">correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved more fruitful (Klein and Manning, 2004). Dependency grammars (Mel&apos;ˇcuk, 1988) should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence. Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models (Clark, 2001; Klein and Manning, 2004), especially in comparison to supervised parsing models (Collins, 2003; Clark and Curran, 2004; McDonald, 2006). Simple models are attractive for grammar induction because they have a limited capacity to overfit, however they are incapable of modelling many known linguistic phenomena. We posit that more complex grammars could be used to better model the unsupervised task, provided that active measures are taken to prevent overfitting. In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior. This allows the model to learn large dependency fragments to best describe the text,</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<editor>In Harry Bunt and Anton Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="3024" citStr="Eisner, 2000" startWordPosition="447" endWordPosition="448">ed capacity to overfit, however they are incapable of modelling many known linguistic phenomena. We posit that more complex grammars could be used to better model the unsupervised task, provided that active measures are taken to prevent overfitting. In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior. This allows the model to learn large dependency fragments to best describe the text, with the prior biasing the model towards fewer and smaller grammar productions. We adopt the split-head construction (Eisner, 2000; Johnson, 2007) to map dependency parses to context free grammar (CFG) derivations, over which we apply a model of TSG induction (Cohn et al., 2009). The model uses a hierarchical Pitman-Yor process to encode a backoff path from TSG to CFG rules, and from lexicalised to unlexicalised rules. Our best lexicalised model achieves a head attachment accuracy of of 55.7% on Section 23 of the WSJ data set, which significantly improves over state-ofthe-art and far exceeds an EM baseline (Klein and Manning, 2004) which obtains 35.9%. 1204 Proceedings of the 2010 Conference on Empirical Methods in Natur</context>
<context position="5484" citStr="Eisner (2000)" startWordPosition="916" endWordPosition="917">d 1 (one or more attachment). L and R indicates child dependents left or right of the parent; superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more children and X∗ that it has already attached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this grammar is that the subscripts encode the t</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 29–62. Kluwer Academic Publishers, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Donald Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="15598" citStr="Geman and Geman, 1984" startWordPosition="2629" endWordPosition="2632">o dependency grammar our model is capable of learning tree fragments which group CFG parameters. As such the model can learn to condition dependency links on the valence, e.g. by combining LH —* L1H and L1H —* LC CMH* rules into a single fragment the model can learn a parameter that the leftmost child of H is C. By linking together multiple L1H or HR1 non-terminals the model can learn groups of dependencies that occur together, e.g. tree fragments representing the complete preferred argument frame of a verb. 4 Inference 4.1 Training To train our model we use Markov Chain Monte Carlo sampling (Geman and Geman, 1984). Where previous supervised TSG models (Cohn et al., 2009) permit an efficient local sampler, the lack of an observed parse tree in our unsupervised model makes this sampler not applicable. Instead we use a recently proposed blocked Metroplis-Hastings (MH) sampler (Cohn and Blunsom, 2010) which exploits a factorisation of the derivation probabilities such that whole trees can be sampled efficiently. See Cohn and Blunsom (2010) for details. That algorithm is applied using a dynamic program over an observed tree, the generalisation to our situation of an inside pass over the space of all trees i</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>Stuart Geman and Donald Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>459--466</pages>
<editor>Y. Weiss, B. Sch¨olkopf, and J. Platt, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="11612" citStr="Goldwater et al., 2006" startWordPosition="1923" endWordPosition="1926">K algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest model which has a four level hierarchy, depicted graphically in Table 2. In Section 5 we evaluate different subsets of this hierarchy. The topmost level of our model describes lexicalised elementary elementary fragments (e) as produced by a PYP, e|c ∼ Ge Ge|ae, be, Plcfg ∼ PYP(ae, be, Plcfg(·|c)) , where ae and be control the </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Tom Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages 459–466. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William P Headden Mark Johnson</author>
<author>David McClosky</author>
</authors>
<title>Improving unsupervised dependency parsing with richer contexts and smoothing.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<tech>(Plcfg, Pcfg&apos; Psh)).</tech>
<pages>101--109</pages>
<location>Boulder, Colorado,</location>
<marker>Johnson, McClosky, 2009</marker>
<rawString>William P. Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 101–109, Boulder, Colorado, June. (Plcfg, Pcfg&apos; Psh)).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>168--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3040" citStr="Johnson, 2007" startWordPosition="449" endWordPosition="450"> overfit, however they are incapable of modelling many known linguistic phenomena. We posit that more complex grammars could be used to better model the unsupervised task, provided that active measures are taken to prevent overfitting. In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior. This allows the model to learn large dependency fragments to best describe the text, with the prior biasing the model towards fewer and smaller grammar productions. We adopt the split-head construction (Eisner, 2000; Johnson, 2007) to map dependency parses to context free grammar (CFG) derivations, over which we apply a model of TSG induction (Cohn et al., 2009). The model uses a hierarchical Pitman-Yor process to encode a backoff path from TSG to CFG rules, and from lexicalised to unlexicalised rules. Our best lexicalised model achieves a head attachment accuracy of of 55.7% on Section 23 of the WSJ data set, which significantly improves over state-ofthe-art and far exceeds an EM baseline (Klein and Manning, 2004) which obtains 35.9%. 1204 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Proc</context>
<context position="5732" citStr="Johnson (2007)" startWordPosition="947" endWordPosition="948">ttached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this grammar is that the subscripts encode the terminals at the boundaries of the span of that non-terminal. For example the non-terminal LH encodes that the right most terminal spanned by this constituent is H (and the reverse for HR), while AMB encodes that A and B are the left-most and right-</context>
</contexts>
<marker>Johnson, 2007</marker>
<rawString>Mark Johnson. 2007. Transforming projective bilexical dependency grammars into efficiently-parsable CFGs with unfold-fold. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 168–175, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative constituent-context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="1337" citStr="Klein and Manning, 2002" startWordPosition="182" endWordPosition="185">hereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy. 1 Introduction Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus. Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches (Clark, 2001; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved more fruitful (Klein and Manning, 2004). Dependency grammars (Mel&apos;ˇcuk, 1988) should be easier to induce from text compared to phra</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. A generative constituent-context model for improved grammar induction. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 128–135, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>478</pages>
<contexts>
<context position="1845" citStr="Klein and Manning, 2004" startWordPosition="267" endWordPosition="270">ly unsolved, particularly for traditional phrase-structure parsing approaches (Clark, 2001; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved more fruitful (Klein and Manning, 2004). Dependency grammars (Mel&apos;ˇcuk, 1988) should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence. Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models (Clark, 2001; Klein and Manning, 2004), especially in comparison to supervised parsing models (Collins, 2003; Clark and Curran, 2004; McDonald, 2006). Simple models are attractive for grammar induction because they have a limited capacity to overfit, however t</context>
<context position="3533" citStr="Klein and Manning, 2004" startWordPosition="530" endWordPosition="533">biasing the model towards fewer and smaller grammar productions. We adopt the split-head construction (Eisner, 2000; Johnson, 2007) to map dependency parses to context free grammar (CFG) derivations, over which we apply a model of TSG induction (Cohn et al., 2009). The model uses a hierarchical Pitman-Yor process to encode a backoff path from TSG to CFG rules, and from lexicalised to unlexicalised rules. Our best lexicalised model achieves a head attachment accuracy of of 55.7% on Section 23 of the WSJ data set, which significantly improves over state-ofthe-art and far exceeds an EM baseline (Klein and Manning, 2004) which obtains 35.9%. 1204 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204–1213, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics CFG Rule DMV Distribution Description S → LH HR p(root = H) The head of the sentence is H. LH → Hl p(STOP|dir = L, head = H, val = 0) H has no left children. LH → L1 p(CONT|dir = L, head = H, val = 0) H has at least one left child. H L∗H → Hl p(STOP|dir = L, head = H, val = 1) H has no more left children. L∗H → L1 p(CONT|dir = L, head = H, val = 1) H has another left child</context>
<context position="5284" citStr="Klein and Manning, 2004" startWordPosition="882" endWordPosition="885"> the actual CFG is created by instantiating these templates with part-of-speech tags observed in the data for the variables H and C. Valency (val) can take the value 0 (no attachment in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent; superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more children and X∗ that it has already attached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 s</context>
<context position="16301" citStr="Klein and Manning (2004)" startWordPosition="2740" endWordPosition="2743">local sampler, the lack of an observed parse tree in our unsupervised model makes this sampler not applicable. Instead we use a recently proposed blocked Metroplis-Hastings (MH) sampler (Cohn and Blunsom, 2010) which exploits a factorisation of the derivation probabilities such that whole trees can be sampled efficiently. See Cohn and Blunsom (2010) for details. That algorithm is applied using a dynamic program over an observed tree, the generalisation to our situation of an inside pass over the space of all trees is straightforward. A final consideration is the initialisation of the sampler. Klein and Manning (2004) emphasised the importance of the initialiser for achieving good performance with their model. We employ the same harmonic initialiser as described in that work. The initial derivations for our sampler are the Viterbi derivations under the CFG parameterised according to this initialiser. 4.2 Sampling hyperparameters We treat the hyper-parameters {(a&apos;, b�c , sc) , c E N} as random variables in our model and infer their values during training. We choose quite vague priors for each hyper-parameter, encoding our lack of information about their values. We place prior distributions on the PYP discou</context>
<context position="21831" citStr="Klein and Manning, 2004" startWordPosition="3669" endWordPosition="3672">viously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |&lt; 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3See Spitkovsky et al. (2010a) for an exception to this rule. 1209 Directed Attachment Accuracy on WSJ23 Model |w |&lt; 10 |w |&lt; 00 Attach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al., 2008) 46.1 36.9 LN (Cohen et al., 2008) 59.4 40.5 SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4 DMV (Headden III et al., 2009) 55.7σ=8.0 - DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 - EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 - L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 - Less is More (Spitkovsky et al., 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0 Viterbi EM (Spitkovsky et al., 2010b) 65.3 47.9 Hypertext Markup (Spitkovsky et al., 2010c) 69.3 50.4 Adaptor Grammar (Cohen et al., 2010) 50.2 - TSG-DMV (Pcfg) 65.9σ=2.</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="19110" citStr="Marcus et al., 1993" startWordPosition="3204" endWordPosition="3207">e (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and reporting head attachment accuracy. Like previous work we pre-process the training and test data to remove punctuation, training our unlexicalised models on the gold-standard part-of-speech tags, and including words occurring more than 100 times in our lexicalised models (Headden III et al., 2009). It is very difficult for an unsupervised model to learn from long training sentences as they contain a great deal of ambiguity, therefore the majority of DMV based models have been trained on sentences restricted in length to &lt; 10 tokens.3 This has the added benefit of decreasing the runtime fo</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Training and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2334" citStr="McDonald, 2006" startWordPosition="340" endWordPosition="341">els below them. However recent work on the induction of dependency grammars has proved more fruitful (Klein and Manning, 2004). Dependency grammars (Mel&apos;ˇcuk, 1988) should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence. Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models (Clark, 2001; Klein and Manning, 2004), especially in comparison to supervised parsing models (Collins, 2003; Clark and Curran, 2004; McDonald, 2006). Simple models are attractive for grammar induction because they have a limited capacity to overfit, however they are incapable of modelling many known linguistic phenomena. We posit that more complex grammars could be used to better model the unsupervised task, provided that active measures are taken to prevent overfitting. In this paper we present an approach to dependency grammar induction using a tree-substitution grammar (TSG) with a Bayesian non-parametric prior. This allows the model to learn large dependency fragments to best describe the text, with the prior biasing the model towards</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>Ryan McDonald. 2006. Discriminative Training and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor&apos; A Mel&apos;ˇcuk</author>
</authors>
<title>Dependency Syntax: theory and practice.</title>
<date>1988</date>
<publisher>State University of New York Press,</publisher>
<location>Albany.</location>
<marker>Mel&apos;ˇcuk, 1988</marker>
<rawString>Igor&apos; A. Mel&apos;ˇcuk. 1988. Dependency Syntax: theory and practice. State University of New York Press, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="17056" citStr="Neal, 2003" startWordPosition="2863" endWordPosition="2864">bed in that work. The initial derivations for our sampler are the Viterbi derivations under the CFG parameterised according to this initialiser. 4.2 Sampling hyperparameters We treat the hyper-parameters {(a&apos;, b�c , sc) , c E N} as random variables in our model and infer their values during training. We choose quite vague priors for each hyper-parameter, encoding our lack of information about their values. We place prior distributions on the PYP discount ac and concentration bc hyperparamters and sample their values using a slice sampler. We use the range doubling slice sampling technique of (Neal, 2003) to draw a new sample of a0c from its conditional distribution.2 For the discount parameters ac we employ a uniform Beta distribution, as we have no strong prior knowledge of what its value should be (ac — Beta(1,1)). Similarly, we treat the concentration parameters, bc, as being generated by a vague gamma prior, bc — Gamma(1, 1), and sample a new value b0 c using the same slice-sampling approach as for ac: P(bc|z) a P(z|bc) x Gamma(bc|1, 1). 2We made use of the slice sampler included in Mark Johnson’s Adaptor Grammar implementation http://www.cog.brown.edu/˜mj/Software.htm. 1208 Corpus Words </context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21679" citStr="Spitkovsky et al. (2010" startWordPosition="3642" endWordPosition="3645">iation (Q) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |&lt; 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3See Spitkovsky et al. (2010a) for an exception to this rule. 1209 Directed Attachment Accuracy on WSJ23 Model |w |&lt; 10 |w |&lt; 00 Attach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al., 2008) 46.1 36.9 LN (Cohen et al., 2008) 59.4 40.5 SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4 DMV (Headden III et al., 2009) 55.7σ=8.0 - DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 - EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 - L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 - Less is More (Spitkovsky et al., 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0 Viterbi EM (Spitk</context>
<context position="28269" citStr="Spitkovsky et al., 2010" startWordPosition="4830" endWordPosition="4833"> shows the model’s ability to learn the verb’s argument position conditioned on both the head and child type, something lacking in DMV. Rule 7 further refines this preferred analysis for has been by lexicalising both the head and child. Rules (4,5,8,10) employ similar conditioning for proper and ordinary nouns heading noun phrases to the left of has. We believe that it is the ability of the TSG to encode stronger constraints on argument positions that leads to the model’s higher accuracy on longer sentences, while other models do well on shorter sentences but relatively poorly on longer ones (Spitkovsky et al., 2010c). 1211 6 Conclusion ● ● ●● ● ● ●●● In this paper we have made two significant contributions to probabilistic modelling and grammar induction. We have shown that it is possible to successfully learn hierarchical Pitman-Yor models that encode deep and complex backoff paths over highly structured latent spaces. By applying these models to the induction of dependency grammars we have also been able to advance the state-of-the-art, increasing the head attachment accuracy on section 23 of the Wall Street Journal Corpus by more than 5%. Further gains in performance may come from an exploration of t</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2010a. From Baby Steps to Leapfrog: How “Less is More” in unsupervised dependency parsing. In Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010).</booktitle>
<contexts>
<context position="21679" citStr="Spitkovsky et al. (2010" startWordPosition="3642" endWordPosition="3645">iation (Q) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |&lt; 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3See Spitkovsky et al. (2010a) for an exception to this rule. 1209 Directed Attachment Accuracy on WSJ23 Model |w |&lt; 10 |w |&lt; 00 Attach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al., 2008) 46.1 36.9 LN (Cohen et al., 2008) 59.4 40.5 SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4 DMV (Headden III et al., 2009) 55.7σ=8.0 - DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 - EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 - L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 - Less is More (Spitkovsky et al., 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0 Viterbi EM (Spitk</context>
<context position="28269" citStr="Spitkovsky et al., 2010" startWordPosition="4830" endWordPosition="4833"> shows the model’s ability to learn the verb’s argument position conditioned on both the head and child type, something lacking in DMV. Rule 7 further refines this preferred analysis for has been by lexicalising both the head and child. Rules (4,5,8,10) employ similar conditioning for proper and ordinary nouns heading noun phrases to the left of has. We believe that it is the ability of the TSG to encode stronger constraints on argument positions that leads to the model’s higher accuracy on longer sentences, while other models do well on shorter sentences but relatively poorly on longer ones (Spitkovsky et al., 2010c). 1211 6 Conclusion ● ● ●● ● ● ●●● In this paper we have made two significant contributions to probabilistic modelling and grammar induction. We have shown that it is possible to successfully learn hierarchical Pitman-Yor models that encode deep and complex backoff paths over highly structured latent spaces. By applying these models to the induction of dependency grammars we have also been able to advance the state-of-the-art, increasing the head attachment accuracy on section 23 of the Wall Street Journal Corpus by more than 5%. Further gains in performance may come from an exploration of t</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D. Manning. 2010b. Viterbi training improves unsupervised dependency parsing. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Daniel Jurafsky</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Profiting from mark-up: Hyper-text annotations for guided parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="21679" citStr="Spitkovsky et al. (2010" startWordPosition="3642" endWordPosition="3645">iation (Q) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |&lt; 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3See Spitkovsky et al. (2010a) for an exception to this rule. 1209 Directed Attachment Accuracy on WSJ23 Model |w |&lt; 10 |w |&lt; 00 Attach-Right 38.4 31.7 EM (Klein and Manning, 2004) 46.1 35.9 Dirichlet (Cohen et al., 2008) 46.1 36.9 LN (Cohen et al., 2008) 59.4 40.5 SLN, TIE V&amp;N (Cohen and Smith, 2009) 61.3 41.4 DMV (Headden III et al., 2009) 55.7σ=8.0 - DMV smoothed (Headden III et al., 2009) 61.2σ=1.2 - EVG smoothed (Headden III et al., 2009) 65.0σ=5.7 - L-EVG smoothed (Headden III et al., 2009) 68.8σ=4.5 - Less is More (Spitkovsky et al., 2010a) 56.2 44.1 Leap Frog (Spitkovsky et al., 2010a) 57.1 45.0 Viterbi EM (Spitk</context>
<context position="28269" citStr="Spitkovsky et al., 2010" startWordPosition="4830" endWordPosition="4833"> shows the model’s ability to learn the verb’s argument position conditioned on both the head and child type, something lacking in DMV. Rule 7 further refines this preferred analysis for has been by lexicalising both the head and child. Rules (4,5,8,10) employ similar conditioning for proper and ordinary nouns heading noun phrases to the left of has. We believe that it is the ability of the TSG to encode stronger constraints on argument positions that leads to the model’s higher accuracy on longer sentences, while other models do well on shorter sentences but relatively poorly on longer ones (Spitkovsky et al., 2010c). 1211 6 Conclusion ● ● ●● ● ● ●●● In this paper we have made two significant contributions to probabilistic modelling and grammar induction. We have shown that it is possible to successfully learn hierarchical Pitman-Yor models that encode deep and complex backoff paths over highly structured latent spaces. By applying these models to the induction of dependency grammars we have also been able to advance the state-of-the-art, increasing the head attachment accuracy on section 23 of the Wall Street Journal Corpus by more than 5%. Further gains in performance may come from an exploration of t</context>
</contexts>
<marker>Spitkovsky, Jurafsky, Alshawi, 2010</marker>
<rawString>Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Alshawi. 2010c. Profiting from mark-up: Hyper-text annotations for guided parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<contexts>
<context position="11587" citStr="Teh, 2006" startWordPosition="1921" endWordPosition="1922">sing the CYK algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest model which has a four level hierarchy, depicted graphically in Table 2. In Section 5 we evaluate different subsets of this hierarchy. The topmost level of our model describes lexicalised elementary elementary fragments (e) as produced by a PYP, e|c ∼ Ge Ge|ae, be, Plcfg ∼ PYP(ae, be, Plcfg(·|c)) , whe</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y. W. Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 985– 992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>