<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029825">
<title confidence="0.990811">
Finding Opinion Manipulation Trolls in News Community Forums
</title>
<author confidence="0.978063">
Todor Mihaylov Georgi D. Georgiev Preslav Nakov
</author>
<affiliation confidence="0.957378">
FMI Ontotext AD Qatar Computing Research Institute
Sofia University Sofia, Bulgaria HBKU, Qatar
</affiliation>
<email confidence="0.980092">
tbmihailov@gmail.com georgiev@ontotext.com pnakov@qf.org.qa
</email>
<sectionHeader confidence="0.993563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973625">
The emergence of user forums in elec-
tronic news media has given rise to
the proliferation of opinion manipulation
trolls. Finding such trolls automatically
is a hard task, as there is no easy way to
recognize or even to define what they are;
this also makes it hard to get training and
testing data. We solve this issue pragmati-
cally: we assume that a user who is called
a troll by several people is likely to be one.
We experiment with different variations of
this definition, and in each case we show
that we can train a classifier to distinguish
a likely troll from a non-troll with very
high accuracy, 82–95%, thanks to our rich
feature set.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980448275862">
With the rise of social media, it became normal
for people to read and follow other users’ opinion.
This created the opportunity for corporations, gov-
ernments and others to distribute rumors, misin-
formation, speculation and to use other dishonest
practices to manipulate user opinion (Derczynski
and Bontcheva, 2014a). They could consistently
use trolls (Cambria et al., 2010), write fake posts
and comments in public forums, thus making ve-
racity one of the challenges in digital social net-
working (Derczynski and Bontcheva, 2014b).
The practice of using opinion manipulation
trolls has been reality since the rise of Internet and
community forums. It has been shown that user
opinions about products, companies and politics
can be influenced by posts by other users in online
forums and social networks (Dellarocas, 2006).
This makes it easy for companies and political par-
ties to gain popularity by paying for “reputation
management” to people or companies that write in
discussion forums and social networks fake opin-
ions from fake profiles.
In Europe, the problem has emerged in the con-
text of the crisis in Ukraine.12 There have been a
number of publications in news media describing
the behavior of organized trolls that try to manipu-
late other users’ opinion.345 Still, it is hard for fo-
rum administrators to block them as trolls try not
to violate the forum rules.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961583333333">
Troll detection and offensive language use are un-
derstudied problems (Xu and Zhu, 2010). They
have been addressed using analysis of the seman-
tics and sentiment in posts to filter out trolls (Cam-
bria et al., 2010); there have been also studies of
general troll behavior (Herring et al., 2002; Buck-
els et al., 2014). Another approach has been to
use lexico-syntactic features about user’s writing
style, structure and specific cyber-bullying con-
tent (Chen et al., 2012); cyber-bullying was de-
tected using sentiment analysis (Xu et al., 2012);
graph-based approaches over signed social net-
works have been used as well (Ortega et al., 2012;
Kumar et al., 2014). A related problem is that of
trustworthiness of statements on the Web (Rowe
and Butters, 2009). Yet another related problem
is Web spam detection, which has been addressed
as a text classification problem (Sebastiani, 2002),
e.g., using spam keyword spotting (Dave et al.,
2003), lexical affinity of arbitrary words to spam
content (Hu and Liu, 2004), frequency of punc-
tuation and word co-occurrence (Li et al., 2006).
See (Castillo and Davison, 2011) for an overview
on adversarial web search.
</bodyText>
<footnote confidence="0.994162363636364">
1http://www.forbes.com/sites/peterhimler/2014/05/06/
russias-media-trolls/
2http://www.theguardian.com/commentisfree/2014/may/04/
pro-russia-trolls-ukraine-guardian-online
3http://www.washingtonpost.com/
news/the-intersect/wp/2014/06/04/
hunting-for-paid-russian-trolls-in-the-washington-post-comments-section/
4http://www.theguardian.com/world/2015/apr/02/
putin-kremlin-inside-russian-troll-house
5http://www.theguardian.com/commentisfree/2014/may/04/
pro-russia-trolls-ukraine-guardian-online
</footnote>
<page confidence="0.937017">
310
</page>
<note confidence="0.854963">
Proceedings of the 19th Conference on Computational Language Learning, pages 310–314,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.990060714285714">
Object Count
Publications 34,514
Comments 1,930,818
-of which replies 897,806
User profiles 14,598
Topics 232
Tags 13,575
</table>
<tableCaption confidence="0.99992">
Table 1: Statistics about our dataset.
</tableCaption>
<sectionHeader confidence="0.980047" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.996514">
We crawled the largest Internet community forum
of a Bulgarian media, that of Dnevnik.bg,6 a daily
newspaper that requires users to be signed in in
order to comment, which makes it easy to track
them. The platform allows users to comment on
news, to reply to other users’ comments and to
vote on them with thumbs up or thumbs down. In
the forum, the official language is Bulgarian and
all comments are written in Bulgarian.
Each publication has a category, a subcategory,
and a list of manually selected tags (keywords).
We crawled all publications in the Bulgaria, Eu-
rope, and World categories, which turned out to be
mostly about politics, for the period 01-Jan-2013
to 01-Apr-2015, together with the comments and
the corresponding user profiles as seen in Table 1.
We considered as trolls users who were called
such by at least n distinct users, and non-trolls if
they have never been called so. Requiring that a
user should have at least 100 comments in order to
be interesting for our experiments left us with 317
trolls and 964 non-trolls. Here are two examples
(translated):
“To comment from ”Historama”: Murzi7, you
know that you cannot manipulate public opinion,
right?”
“To comment from ”Rozalina”: You, trolls, are
so funny :) I saw the same signature under other
comments:)”
</bodyText>
<footnote confidence="0.688882">
6http://dnevnik.bg
7Murzi is the short for murzilka. According to series of re-
</footnote>
<bodyText confidence="0.949181">
cent publications in Bulgarian media, Russian Internet users
reportedly use the term murzilka to refer to Internet trolls. As
a result, this term was adopted by some pro-Western Bulgar-
ian forum users as a way to refer to users that they perceive
as pro-Russian opinion manipulation trolls. Despite the term
being now in circulation in Bulgaria, it is not really in use in
Russia. In fact, the vast majority of Russian Internet users
have never heard that murzilka, the name of a cute monkey-
like children’s toy and of a popular Soviet-time children’s
journal, could possibly be used to refer to Internet trolls.
</bodyText>
<sectionHeader confidence="0.976876" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.99228784">
Our features are motivated by several publications
about troll behavior mentioned above.
For each user, we extract statistics such as num-
ber of comments posted, number of days in the
forum, number of days with at least one comment,
and number of publications commented on. All
(other) features are scaled with respect to these
statistics, which makes it possible to handle users
that registered only recently. Our features can be
divided in the following groups:
Vote-based features. We calculate the num-
ber of comments with positive and negative votes
for each user. This is useful as we assume that
non-trolls are likely to disagree with trolls, and to
give them negative votes. We use the sum from
all comments as a feature. We also count sepa-
rately the comments with high, low and medium
positive to negative ratio. Here are some ex-
ample features: the number of comments where
(positive/negative) &lt; 0.25, and the number of
comments where (positive/negative) &lt; 0.50.
Comment-to-publication similarity. These
features measure the similarity between comments
and publications. We use cosine and TF.IDF-
weighted vectors for the comment and for the pub-
lication. The idea is that trolls might try to change
or blurr the topic of the publication if it differs
from his/her views or agenda.
Comment order-based features. We count
how many user comments the user has among the
first k. The idea is that trolls might try to be among
the first to comment to achieve higher impact.
Top loved/hated comments. We calculate the
number of times the user’s comments were among
the top 1, 3, 5, 10 most loved/hated comments
in some thread. The idea is that in the comment
thread below many publications there are some
trolls that oppose all other users, and usually their
comments are among the most hated.
Comment replies-based features. These are
features that count how many user comments are
replies to other comments, how many are replies
to replies, and so on. The assumption here is that
trolls try to post the most comments and want to
dominate the conversation, especially when de-
fending a specific cause. We further generate com-
plex features that mix comment reply features and
vote counts-based features, thus generating even
more features that model the relationship between
replies and user agreement/disagreement.
</bodyText>
<page confidence="0.995139">
311
</page>
<bodyText confidence="0.999902485714286">
Time-based features. We generate features
from the number of comments posted during dif-
ferent time periods on a daily or on a weekly ba-
sis. We assume that users that write comments on
purpose could be paid, or could be activists of po-
litical parties, and they probably have some usual
times to post, e.g., maybe they do it as a full-time
job. On the other hand, most non-trolls work from
9:00 to 18:00, and thus we could expect that they
should probably post less comments during this
part of the day. We have time-based features that
count the number of comments from 9:00 to 9:59,
from 12:00 to 12:59, during working hours 9:00-
18:00, etc.
All the above features are scaled, i.e., divided
by the number of comments, the number of days
in the forum, the number of days with more than
one comment. Overall, we have a total of 338 such
scaled features. In addition, we define a new set of
features, which are non-scaled.
Non-scaled features. The non-scaled features
are features based on the same statistics as above,
but just not divided by the number of comments /
number of days in the forum / number of days with
more that one comment, etc. For example, one
non-scaled feature is the number of times a com-
ment by the target user was voted negatively, i.e.,
as thumbs down, by other users. As a non-scaled
feature, we would use this number directly, while
above we would scale it by dividing it by the total
number of user’s comments, by the total number of
publications the user has commented on, etc. Ob-
viously, there is a danger in using non-scaled fea-
tures: older users are likely to have higher values
for them compared to recently-registered users.
</bodyText>
<sectionHeader confidence="0.996875" genericHeader="method">
5 Experiments and Evaluation
</sectionHeader>
<bodyText confidence="0.999968785714286">
As we mentioned above, in our experiments,
we focus on users with at least 100 comments.
This includes 317 trolls and 964 non-trolls. For
each user, we extract the above-described features,
scaled and non-scaled, and we normalize them in
the -1 to 1 interval. We then use a support vector
machine (SVM) classifier (Chang and Lin, 2011)
with an RBF kernel with C=32 and g=0.0078125
as this was the best-performing configuration. In
order to avoid overfitting, we used 5-fold cross-
validation. The results are shown in Tables 2 and
3, where the Accuracy column shows the cross-
validation accuracy and the Diff column shows the
improvement over the majority class baseline.
</bodyText>
<table confidence="0.999378">
Features Accuracy Diff
AS + Non-scaled 94.37(+3.74) 19.13
AS − total comments 91.17(+0.54) 15.93
AS − comment order 91.10(+0.46) 15.85
AS − similarity 91.02(+0.39) 15.77
AS − time day of week 90.78(+0.15) 15.53
AS − trigg rep range 90.78(+0.15) 15.53
AS − time all 90.71(+0.07) 15.46
All scaled (AS) 90.63 15.38
AS − top loved/hated 90.55(-0.07) 15.30
AS − time hours 90.47(-0.15) 15.22
AS − vote u/down rep 90.47(-0.15) 15.22
AS − similarity top 90.32(-0.31) 15.07
AS − triggered cmnts 90.32(-0.31) 15.07
AS − is rep to has rep 90.08(-0.54) 14.83
AS − vote up/down all 89.69(-0.93) 14.44
AS − is reply 89.61(-1.01) 14.36
AS − up/down votes 88.29(-2.34) 13.04
</table>
<tableCaption confidence="0.970521">
Table 2: Results for classifying 317 mentioned
</tableCaption>
<bodyText confidence="0.979365966666667">
trolls vs. 964 non-trolls for All Scaled (AS) ‘–’
(minus) some scaled feature group. The Accuracy
column shows the cross-validation accuracy, and
the Diff column shows the improvement over the
majority class baseline.
Table 2 presents the results when using all fea-
tures, as well as when using all features but ex-
cluding/adding one feature group. Here All scaled
(AS) refers to the features from all groups ex-
cept for those in the non-scaled features group de-
scribed last in the previous section.
We can see that the best feature set is the
one that includes all features, including the Non-
scaled features group: adding this group con-
tributes +3.74 to accuracy. We further see that ex-
cluding features based on time, e.g., AS − time day
of week and AS − time all, improves the accuracy,
which means that time of posting is not so impor-
tant as a feature. Similarly, we see that it hurts
accuracy to use as features the total number or the
order of comments. Finally, the most important
features turn out to be those based on replies and
on thumbs up/down votes.
Next, Table 3 shows results of experiments
when using different feature groups in isolation.
As expected, the features that hurt most when ex-
cluded from the All scaled feature set, perform
best when used alone. Here, the similarity features
perform worst, which suggests that trolls tend not
to change the topic.
</bodyText>
<page confidence="0.996566">
312
</page>
<table confidence="0.999834647058823">
Features Accuracy Diff
All Non-scaled 93.21 17.95
Only vote up/down 87.67 12.41
Only vote up/down totals 87.20 11.94
Only reply up/down voted 86.10 10.85
Only time hours 84.93 9.68
Only time all 84.31 9.06
Only is reply with rep 82.83 7.57
Only triggered rep range 82.83 7.57
Only day of week 82.28 7.03
Only total comments 82.28 7.03
Only reply status 80.72 5.46
Only triggered replies 80.33 5.07
Only comment order 80.09 4.84
Only top loved/hated 79.39 4.14
Only pub similarity top 75.25 0.00
Only pub similarity 75.25 0.00
</table>
<tableCaption confidence="0.99253">
Table 3: Results for classifying 317 mentioned
</tableCaption>
<bodyText confidence="0.9379552">
trolls vs. 964 non-trolls for individual feature
groups (all scaled, except for line 1). The Accu-
racy column shows the cross-validation accuracy,
and the Diff column shows the improvement over
the majority class baseline.
</bodyText>
<sectionHeader confidence="0.99852" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999623">
We considered as trolls people who try to manipu-
late other users’ opinion. Our positive and neg-
ative examples are based on trolls having been
called such by at least n other users (we used
n = 5).
However, this is much of a witch hunt and
despite our good overall results, the data needs
some manual checking in future work. We are
also aware that some trolls can actually accuse
non-trolls of being trolls, and we cannot be sure
whether this is true or not unless we have some-
one to check it manually. In fact, we do have a list
of trolls that are known to have been paid (as ex-
posed in Bulgarian media), but there are only 15
of them, and we could not build a good classifier
using only them due to the severe class imbalance.
As the choice of a minimum number of accu-
sations for a user of being a troll that we used to
define a troll, namely n = 5, might be seen as
arbitrary, we also experimented with n = 3, 4, 5,
6, while keeping the required minimum number of
comments per user to be 100 as before. The re-
sults are shown in Table 4. We can see that as the
number of troll mentions/accusations increases, so
does the cross-validation accuracy.
</bodyText>
<table confidence="0.9939482">
min mentions 3 4 5 6
trolls 545 419 317 260
non-troll 964 964 964 964
Accuracy 85.49 87.85 90.87 92.32
Diff +21.60 +18.15 +15.61 +13.56
</table>
<tableCaption confidence="0.95175">
Table 4: Results for classifying mentioned trolls
</tableCaption>
<bodyText confidence="0.98364624137931">
vs. non-trolls, using different numbers of mini-
mum troll accusations to define a troll (users with
100 comments or more only). The Accuracy col-
umn shows the cross-validation accuracy, and the
Diff column shows the improvement over the ma-
jority class baseline.
However, this is partly due to the increased class
imbalance of trolls vs. non-trolls, which can be
seen by the decrease in the improvement of our
classifier compared to the majority class baseline.
We also ran experiments with a fixed number of
minimum mentions for the trolls (namely 5 as be-
fore), but with varying minimum number of com-
ments per user: 10, 25, 50, 100. The results are
shown in Figure 1. We can see that as the min-
imum number of comments increases, the cross-
validation accuracy for both the baseline and for
our classifier decreases (as the troll-vs-non-troll
ratio becomes more balanced); yet, the improve-
ment of our classifier over the baseline increases,
which means that the more we know about a user,
the better we can predict whether s/he will be seen
as a troll by other users.
Figure 1: Results for classifying mentioned trolls
vs. non-trolls for users with a different minimal
number of comments (trolls were accused of be-
ing such by 5 or more different users). Shown are
results for our classifier and for the majority class
baseline.
</bodyText>
<page confidence="0.999095">
313
</page>
<sectionHeader confidence="0.97907" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999977533333333">
We have presented experiments in trying to dis-
tinguish trolls vs. non-trolls in news community
forums. We have experimented with a large num-
ber of features, both scaled and non-scaled, and
we have achieved very strong overall results using
statistics such as number of comments, of posi-
tive and negative votes, of posting replies, activity
over time, etc. The nature of our features means
that our troll detection works best for “elder trolls”
with at least 100 comments in the forum. In future
work, we plan to add content features such as key-
words, topics, named entities, part of speech, and
named entities, which should help detect “fresh”
trolls. Our ultimate objective is to be able to find
and expose paid opinion manipulation trolls.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999931">
We would like to thank the anonymous review-
ers for their constructive comments, which have
helped us to improve the paper.
</bodyText>
<sectionHeader confidence="0.998167" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999082809523809">
Erin E Buckels, Paul D Trapnell, and Delroy L Paulhus.
2014. Trolls just want to have fun. Personality and
individual Differences, 67:97–102.
Erik Cambria, Praphul Chandra, Avinash Sharma, and
Amir Hussain. 2010. Do not feel the trolls. In Pro-
ceedings of the 3rd International Workshop on So-
cial Data on the Web, SDoW ’10, Shanghai, China.
Carlos Castillo and Brian D. Davison. 2011. Adversar-
ial web search. Found. Trends Inf. Retr., 4(5):377–
486, May.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu.
2012. Detecting offensive language in social me-
dia to protect adolescent online safety. In Proceed-
ings of the 2012 International Conference on Pri-
vacy, Security, Risk and Trust and of the 2012 In-
ternational Conference on Social Computing, PAS-
SAT/SocialCom ’12, pages 71–80, Amsterdam,
Netherlands.
Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extrac-
tion and semantic classification of product reviews.
In Proceedings of the 12th international World Wide
Web conference, WWW ’03, pages 519–528, Bu-
dapest, Hungary.
Chrysanthos Dellarocas. 2006. Strategic manip-
ulation of internet opinion forums: Implications
for consumers and firms. Management Science,
52(10):1577–1593.
Leon Derczynski and Kalina Bontcheva. 2014a.
Pheme: Veracity in digital social networks. In Pro-
ceedings of the UMAP Project Synergy workshop.
Leon Derczynski and Kalina Bontcheva. 2014b.
Spatio-temporal grounding of claims made on the
web, in pheme. In Proceedings of the 10th Joint
ISO-ACL SIGSEM Workshop on Interoperable Se-
mantic Annotation, ISA ’14, page 65, Reykjavik,
Iceland.
Susan Herring, Kirk Job-Sluder, Rebecca Scheckler,
and Sasha Barab. 2002. Searching for safety on-
line: Managing “trolling” in a feminist forum. The
Information Society, 18(5):371–384.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’04, pages
168–177, Seattle, WA, USA.
Srijan Kumar, Francesca Spezzano, and VS Subrah-
manian. 2014. Accurately detecting trolls in
slashdot zoo via decluttering. In Proceedings of
the 2014 IEEE/ACM International Conference on
Advances in Social Network Analysis and Mining,
ASONAM ’14, pages 188–195, Beijing, China.
Wenbin Li, Ning Zhong, and Chunnian Liu. 2006.
Combining multiple email filters based on multivari-
ate statistical analysis. In Foundations of Intelligent
Systems, pages 729–738. Springer.
F. Javier Ortega, Jos´e A. Troyano, Ferm´ın L. Cruz,
Carlos G. Vallejo, and Fernando Enr´ıquez. 2012.
Propagation of trust and distrust for the detection
of trolls in a social network. Computer Networks,
56(12):2884 – 2895.
Matthew Rowe and Jonathan Butters. 2009. Assess-
ing Trust: Contextual Accountability. In Proceed-
ings of the First Workshop on Trust and Privacy on
the Social and Semantic Web, SPOT ’09, Heraklion,
Greece.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1–47.
Zhi Xu and Sencun Zhu. 2010. Filtering offensive lan-
guage in online communities using grammatical re-
lations. In Proceedings of the Seventh Annual Col-
laboration, Electronic Messaging, Anti-Abuse and
Spam Conference.
Jun-Ming Xu, Xiaojin Zhu, and Amy Bellmore. 2012.
Fast learning for sentiment analysis on bullying. In
Proceedings of the First International Workshop on
Issues of Sentiment Discovery and Opinion Mining,
WISDOM ’12, pages 10:1–10:6, New York, NY,
USA.
</reference>
<page confidence="0.999126">
314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.591997">
<title confidence="0.999993">Finding Opinion Manipulation Trolls in News Community Forums</title>
<author confidence="0.978576">Todor Mihaylov Georgi D Georgiev Preslav Nakov</author>
<affiliation confidence="0.8213675">FMI Ontotext AD Qatar Computing Research Institute Sofia University Sofia, Bulgaria HBKU, Qatar</affiliation>
<email confidence="0.966931">tbmihailov@gmail.comgeorgiev@ontotext.compnakov@qf.org.qa</email>
<abstract confidence="0.998596411764706">The emergence of user forums in electronic news media has given rise to proliferation of manipulation Finding such trolls automatically is a hard task, as there is no easy way to recognize or even to define what they are; this also makes it hard to get training and testing data. We solve this issue pragmatically: we assume that a user who is called a troll by several people is likely to be one. We experiment with different variations of this definition, and in each case we show that we can train a classifier to distinguish a likely troll from a non-troll with very high accuracy, 82–95%, thanks to our rich feature set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Erin E Buckels</author>
<author>Paul D Trapnell</author>
<author>Delroy L Paulhus</author>
</authors>
<title>Trolls just want to have fun. Personality and individual Differences,</title>
<date>2014</date>
<pages>67--97</pages>
<contexts>
<context position="2636" citStr="Buckels et al., 2014" startWordPosition="427" endWordPosition="431">in the context of the crisis in Ukraine.12 There have been a number of publications in news media describing the behavior of organized trolls that try to manipulate other users’ opinion.345 Still, it is hard for forum administrators to block them as trolls try not to violate the forum rules. 2 Related Work Troll detection and offensive language use are understudied problems (Xu and Zhu, 2010). They have been addressed using analysis of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spott</context>
</contexts>
<marker>Buckels, Trapnell, Paulhus, 2014</marker>
<rawString>Erin E Buckels, Paul D Trapnell, and Delroy L Paulhus. 2014. Trolls just want to have fun. Personality and individual Differences, 67:97–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Cambria</author>
<author>Praphul Chandra</author>
<author>Avinash Sharma</author>
<author>Amir Hussain</author>
</authors>
<title>Do not feel the trolls.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd International Workshop on Social Data on the Web, SDoW ’10,</booktitle>
<location>Shanghai, China.</location>
<contexts>
<context position="1311" citStr="Cambria et al., 2010" startWordPosition="204" endWordPosition="207">. We experiment with different variations of this definition, and in each case we show that we can train a classifier to distinguish a likely troll from a non-troll with very high accuracy, 82–95%, thanks to our rich feature set. 1 Introduction With the rise of social media, it became normal for people to read and follow other users’ opinion. This created the opportunity for corporations, governments and others to distribute rumors, misinformation, speculation and to use other dishonest practices to manipulate user opinion (Derczynski and Bontcheva, 2014a). They could consistently use trolls (Cambria et al., 2010), write fake posts and comments in public forums, thus making veracity one of the challenges in digital social networking (Derczynski and Bontcheva, 2014b). The practice of using opinion manipulation trolls has been reality since the rise of Internet and community forums. It has been shown that user opinions about products, companies and politics can be influenced by posts by other users in online forums and social networks (Dellarocas, 2006). This makes it easy for companies and political parties to gain popularity by paying for “reputation management” to people or companies that write in dis</context>
<context position="2535" citStr="Cambria et al., 2010" startWordPosition="409" endWordPosition="413">sion forums and social networks fake opinions from fake profiles. In Europe, the problem has emerged in the context of the crisis in Ukraine.12 There have been a number of publications in news media describing the behavior of organized trolls that try to manipulate other users’ opinion.345 Still, it is hard for forum administrators to block them as trolls try not to violate the forum rules. 2 Related Work Troll detection and offensive language use are understudied problems (Xu and Zhu, 2010). They have been addressed using analysis of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which h</context>
</contexts>
<marker>Cambria, Chandra, Sharma, Hussain, 2010</marker>
<rawString>Erik Cambria, Praphul Chandra, Avinash Sharma, and Amir Hussain. 2010. Do not feel the trolls. In Proceedings of the 3rd International Workshop on Social Data on the Web, SDoW ’10, Shanghai, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Castillo</author>
<author>Brian D Davison</author>
</authors>
<title>Adversarial web search.</title>
<date>2011</date>
<journal>Found. Trends Inf. Retr.,</journal>
<volume>4</volume>
<issue>5</issue>
<pages>486</pages>
<contexts>
<context position="3432" citStr="Castillo and Davison, 2011" startWordPosition="553" endWordPosition="556">g was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. 1http://www.forbes.com/sites/peterhimler/2014/05/06/ russias-media-trolls/ 2http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 3http://www.washingtonpost.com/ news/the-intersect/wp/2014/06/04/ hunting-for-paid-russian-trolls-in-the-washington-post-comments-section/ 4http://www.theguardian.com/world/2015/apr/02/ putin-kremlin-inside-russian-troll-house 5http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 310 Proceedings of the 19th Conference on Computational Lang</context>
</contexts>
<marker>Castillo, Davison, 2011</marker>
<rawString>Carlos Castillo and Brian D. Davison. 2011. Adversarial web search. Found. Trends Inf. Retr., 4(5):377– 486, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="10637" citStr="Chang and Lin, 2011" startWordPosition="1691" endWordPosition="1694">he total number of user’s comments, by the total number of publications the user has commented on, etc. Obviously, there is a danger in using non-scaled features: older users are likely to have higher values for them compared to recently-registered users. 5 Experiments and Evaluation As we mentioned above, in our experiments, we focus on users with at least 100 comments. This includes 317 trolls and 964 non-trolls. For each user, we extract the above-described features, scaled and non-scaled, and we normalize them in the -1 to 1 interval. We then use a support vector machine (SVM) classifier (Chang and Lin, 2011) with an RBF kernel with C=32 and g=0.0078125 as this was the best-performing configuration. In order to avoid overfitting, we used 5-fold crossvalidation. The results are shown in Tables 2 and 3, where the Accuracy column shows the crossvalidation accuracy and the Diff column shows the improvement over the majority class baseline. Features Accuracy Diff AS + Non-scaled 94.37(+3.74) 19.13 AS − total comments 91.17(+0.54) 15.93 AS − comment order 91.10(+0.46) 15.85 AS − similarity 91.02(+0.39) 15.77 AS − time day of week 90.78(+0.15) 15.53 AS − trigg rep range 90.78(+0.15) 15.53 AS − time all 9</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Chen</author>
<author>Yilu Zhou</author>
<author>Sencun Zhu</author>
<author>Heng Xu</author>
</authors>
<title>Detecting offensive language in social media to protect adolescent online safety.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 International Conference on Privacy, Security, Risk and Trust and of the 2012 International Conference on Social Computing, PASSAT/SocialCom ’12,</booktitle>
<pages>71--80</pages>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="2790" citStr="Chen et al., 2012" startWordPosition="450" endWordPosition="453">ipulate other users’ opinion.345 Still, it is hard for forum administrators to block them as trolls try not to violate the forum rules. 2 Related Work Troll detection and offensive language use are understudied problems (Xu and Zhu, 2010). They have been addressed using analysis of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al</context>
</contexts>
<marker>Chen, Zhou, Zhu, Xu, 2012</marker>
<rawString>Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu. 2012. Detecting offensive language in social media to protect adolescent online safety. In Proceedings of the 2012 International Conference on Privacy, Security, Risk and Trust and of the 2012 International Conference on Social Computing, PASSAT/SocialCom ’12, pages 71–80, Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international World Wide Web conference, WWW ’03,</booktitle>
<pages>519--528</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="3259" citStr="Dave et al., 2003" startWordPosition="525" endWordPosition="528">ther approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. 1http://www.forbes.com/sites/peterhimler/2014/05/06/ russias-media-trolls/ 2http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 3http://www.washingtonpost.com/ news/the-intersect/wp/2014/06/04/ hunting-for-paid-russian-trolls-in-the-washington-post-comments-section/ 4http://www.theguardian.com/world/2015/apr/02/ putin-kremlin-inside-russ</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of the 12th international World Wide Web conference, WWW ’03, pages 519–528, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chrysanthos Dellarocas</author>
</authors>
<title>Strategic manipulation of internet opinion forums: Implications for consumers and firms.</title>
<date>2006</date>
<journal>Management Science,</journal>
<volume>52</volume>
<issue>10</issue>
<contexts>
<context position="1757" citStr="Dellarocas, 2006" startWordPosition="277" endWordPosition="278">n, speculation and to use other dishonest practices to manipulate user opinion (Derczynski and Bontcheva, 2014a). They could consistently use trolls (Cambria et al., 2010), write fake posts and comments in public forums, thus making veracity one of the challenges in digital social networking (Derczynski and Bontcheva, 2014b). The practice of using opinion manipulation trolls has been reality since the rise of Internet and community forums. It has been shown that user opinions about products, companies and politics can be influenced by posts by other users in online forums and social networks (Dellarocas, 2006). This makes it easy for companies and political parties to gain popularity by paying for “reputation management” to people or companies that write in discussion forums and social networks fake opinions from fake profiles. In Europe, the problem has emerged in the context of the crisis in Ukraine.12 There have been a number of publications in news media describing the behavior of organized trolls that try to manipulate other users’ opinion.345 Still, it is hard for forum administrators to block them as trolls try not to violate the forum rules. 2 Related Work Troll detection and offensive lang</context>
</contexts>
<marker>Dellarocas, 2006</marker>
<rawString>Chrysanthos Dellarocas. 2006. Strategic manipulation of internet opinion forums: Implications for consumers and firms. Management Science, 52(10):1577–1593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Pheme: Veracity in digital social networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the UMAP Project Synergy workshop.</booktitle>
<contexts>
<context position="1250" citStr="Derczynski and Bontcheva, 2014" startWordPosition="195" endWordPosition="198">hat a user who is called a troll by several people is likely to be one. We experiment with different variations of this definition, and in each case we show that we can train a classifier to distinguish a likely troll from a non-troll with very high accuracy, 82–95%, thanks to our rich feature set. 1 Introduction With the rise of social media, it became normal for people to read and follow other users’ opinion. This created the opportunity for corporations, governments and others to distribute rumors, misinformation, speculation and to use other dishonest practices to manipulate user opinion (Derczynski and Bontcheva, 2014a). They could consistently use trolls (Cambria et al., 2010), write fake posts and comments in public forums, thus making veracity one of the challenges in digital social networking (Derczynski and Bontcheva, 2014b). The practice of using opinion manipulation trolls has been reality since the rise of Internet and community forums. It has been shown that user opinions about products, companies and politics can be influenced by posts by other users in online forums and social networks (Dellarocas, 2006). This makes it easy for companies and political parties to gain popularity by paying for “re</context>
</contexts>
<marker>Derczynski, Bontcheva, 2014</marker>
<rawString>Leon Derczynski and Kalina Bontcheva. 2014a. Pheme: Veracity in digital social networks. In Proceedings of the UMAP Project Synergy workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Spatio-temporal grounding of claims made on the web, in pheme.</title>
<date>2014</date>
<booktitle>In Proceedings of the 10th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic Annotation, ISA ’14,</booktitle>
<pages>65</pages>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="1250" citStr="Derczynski and Bontcheva, 2014" startWordPosition="195" endWordPosition="198">hat a user who is called a troll by several people is likely to be one. We experiment with different variations of this definition, and in each case we show that we can train a classifier to distinguish a likely troll from a non-troll with very high accuracy, 82–95%, thanks to our rich feature set. 1 Introduction With the rise of social media, it became normal for people to read and follow other users’ opinion. This created the opportunity for corporations, governments and others to distribute rumors, misinformation, speculation and to use other dishonest practices to manipulate user opinion (Derczynski and Bontcheva, 2014a). They could consistently use trolls (Cambria et al., 2010), write fake posts and comments in public forums, thus making veracity one of the challenges in digital social networking (Derczynski and Bontcheva, 2014b). The practice of using opinion manipulation trolls has been reality since the rise of Internet and community forums. It has been shown that user opinions about products, companies and politics can be influenced by posts by other users in online forums and social networks (Dellarocas, 2006). This makes it easy for companies and political parties to gain popularity by paying for “re</context>
</contexts>
<marker>Derczynski, Bontcheva, 2014</marker>
<rawString>Leon Derczynski and Kalina Bontcheva. 2014b. Spatio-temporal grounding of claims made on the web, in pheme. In Proceedings of the 10th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic Annotation, ISA ’14, page 65, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Herring</author>
<author>Kirk Job-Sluder</author>
<author>Rebecca Scheckler</author>
<author>Sasha Barab</author>
</authors>
<title>Searching for safety online: Managing “trolling” in a feminist forum.</title>
<date>2002</date>
<journal>The Information Society,</journal>
<volume>18</volume>
<issue>5</issue>
<contexts>
<context position="2613" citStr="Herring et al., 2002" startWordPosition="423" endWordPosition="426">e problem has emerged in the context of the crisis in Ukraine.12 There have been a number of publications in news media describing the behavior of organized trolls that try to manipulate other users’ opinion.345 Still, it is hard for forum administrators to block them as trolls try not to violate the forum rules. 2 Related Work Troll detection and offensive language use are understudied problems (Xu and Zhu, 2010). They have been addressed using analysis of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., u</context>
</contexts>
<marker>Herring, Job-Sluder, Scheckler, Barab, 2002</marker>
<rawString>Susan Herring, Kirk Job-Sluder, Rebecca Scheckler, and Sasha Barab. 2002. Searching for safety online: Managing “trolling” in a feminist forum. The Information Society, 18(5):371–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="3331" citStr="Hu and Liu, 2004" startWordPosition="537" endWordPosition="540">ing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. 1http://www.forbes.com/sites/peterhimler/2014/05/06/ russias-media-trolls/ 2http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 3http://www.washingtonpost.com/ news/the-intersect/wp/2014/06/04/ hunting-for-paid-russian-trolls-in-the-washington-post-comments-section/ 4http://www.theguardian.com/world/2015/apr/02/ putin-kremlin-inside-russian-troll-house 5http://www.theguardian.com/commentisfree/2014/may/04/ p</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04, pages 168–177, Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srijan Kumar</author>
<author>Francesca Spezzano</author>
<author>VS Subrahmanian</author>
</authors>
<title>Accurately detecting trolls in slashdot zoo via decluttering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Network Analysis and Mining, ASONAM ’14,</booktitle>
<pages>188--195</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2979" citStr="Kumar et al., 2014" startWordPosition="481" endWordPosition="484">e use are understudied problems (Xu and Zhu, 2010). They have been addressed using analysis of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. 1http://www.forbes.com/sites/peterhimler/2014/05/06/ russias-media-trolls/ 2http://www.theguardian.com/</context>
</contexts>
<marker>Kumar, Spezzano, Subrahmanian, 2014</marker>
<rawString>Srijan Kumar, Francesca Spezzano, and VS Subrahmanian. 2014. Accurately detecting trolls in slashdot zoo via decluttering. In Proceedings of the 2014 IEEE/ACM International Conference on Advances in Social Network Analysis and Mining, ASONAM ’14, pages 188–195, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Li</author>
<author>Ning Zhong</author>
<author>Chunnian Liu</author>
</authors>
<title>Combining multiple email filters based on multivariate statistical analysis.</title>
<date>2006</date>
<booktitle>In Foundations of Intelligent Systems,</booktitle>
<pages>729--738</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3398" citStr="Li et al., 2006" startWordPosition="548" endWordPosition="551">., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. 1http://www.forbes.com/sites/peterhimler/2014/05/06/ russias-media-trolls/ 2http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 3http://www.washingtonpost.com/ news/the-intersect/wp/2014/06/04/ hunting-for-paid-russian-trolls-in-the-washington-post-comments-section/ 4http://www.theguardian.com/world/2015/apr/02/ putin-kremlin-inside-russian-troll-house 5http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 310 Proceedings of the 19t</context>
</contexts>
<marker>Li, Zhong, Liu, 2006</marker>
<rawString>Wenbin Li, Ning Zhong, and Chunnian Liu. 2006. Combining multiple email filters based on multivariate statistical analysis. In Foundations of Intelligent Systems, pages 729–738. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Javier Ortega</author>
<author>Jos´e A Troyano</author>
<author>Ferm´ın L Cruz</author>
<author>Carlos G Vallejo</author>
<author>Fernando Enr´ıquez</author>
</authors>
<title>Propagation of trust and distrust for the detection of trolls in a social network.</title>
<date>2012</date>
<journal>Computer Networks,</journal>
<volume>56</volume>
<issue>12</issue>
<pages>2895</pages>
<marker>Ortega, Troyano, Cruz, Vallejo, Enr´ıquez, 2012</marker>
<rawString>F. Javier Ortega, Jos´e A. Troyano, Ferm´ın L. Cruz, Carlos G. Vallejo, and Fernando Enr´ıquez. 2012. Propagation of trust and distrust for the detection of trolls in a social network. Computer Networks, 56(12):2884 – 2895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Rowe</author>
<author>Jonathan Butters</author>
</authors>
<title>Assessing Trust: Contextual Accountability.</title>
<date>2009</date>
<booktitle>In Proceedings of the First Workshop on Trust and Privacy on the Social and Semantic Web, SPOT ’09,</booktitle>
<location>Heraklion, Greece.</location>
<contexts>
<context position="3075" citStr="Rowe and Butters, 2009" startWordPosition="497" endWordPosition="500">of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. 1http://www.forbes.com/sites/peterhimler/2014/05/06/ russias-media-trolls/ 2http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 3http://www.washingtonpost.</context>
</contexts>
<marker>Rowe, Butters, 2009</marker>
<rawString>Matthew Rowe and Jonathan Butters. 2009. Assessing Trust: Contextual Accountability. In Proceedings of the First Workshop on Trust and Privacy on the Social and Semantic Web, SPOT ’09, Heraklion, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM computing surveys (CSUR),</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="3204" citStr="Sebastiani, 2002" startWordPosition="518" endWordPosition="519">vior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. 1http://www.forbes.com/sites/peterhimler/2014/05/06/ russias-media-trolls/ 2http://www.theguardian.com/commentisfree/2014/may/04/ pro-russia-trolls-ukraine-guardian-online 3http://www.washingtonpost.com/ news/the-intersect/wp/2014/06/04/ hunting-for-paid-russian-trolls-in-the-washington-post-comments-section/ 4http://www.thegu</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Xu</author>
<author>Sencun Zhu</author>
</authors>
<title>Filtering offensive language in online communities using grammatical relations.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh Annual Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference.</booktitle>
<contexts>
<context position="2410" citStr="Xu and Zhu, 2010" startWordPosition="387" endWordPosition="390">nd political parties to gain popularity by paying for “reputation management” to people or companies that write in discussion forums and social networks fake opinions from fake profiles. In Europe, the problem has emerged in the context of the crisis in Ukraine.12 There have been a number of publications in news media describing the behavior of organized trolls that try to manipulate other users’ opinion.345 Still, it is hard for forum administrators to block them as trolls try not to violate the forum rules. 2 Related Work Troll detection and offensive language use are understudied problems (Xu and Zhu, 2010). They have been addressed using analysis of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of </context>
</contexts>
<marker>Xu, Zhu, 2010</marker>
<rawString>Zhi Xu and Sencun Zhu. 2010. Filtering offensive language in online communities using grammatical relations. In Proceedings of the Seventh Annual Collaboration, Electronic Messaging, Anti-Abuse and Spam Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-Ming Xu</author>
<author>Xiaojin Zhu</author>
<author>Amy Bellmore</author>
</authors>
<title>Fast learning for sentiment analysis on bullying.</title>
<date>2012</date>
<booktitle>In Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM ’12,</booktitle>
<pages>10--1</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="2862" citStr="Xu et al., 2012" startWordPosition="461" endWordPosition="464">s to block them as trolls try not to violate the forum rules. 2 Related Work Troll detection and offensive language use are understudied problems (Xu and Zhu, 2010). They have been addressed using analysis of the semantics and sentiment in posts to filter out trolls (Cambria et al., 2010); there have been also studies of general troll behavior (Herring et al., 2002; Buckels et al., 2014). Another approach has been to use lexico-syntactic features about user’s writing style, structure and specific cyber-bullying content (Chen et al., 2012); cyber-bullying was detected using sentiment analysis (Xu et al., 2012); graph-based approaches over signed social networks have been used as well (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). Yet another related problem is Web spam detection, which has been addressed as a text classification problem (Sebastiani, 2002), e.g., using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversaria</context>
</contexts>
<marker>Xu, Zhu, Bellmore, 2012</marker>
<rawString>Jun-Ming Xu, Xiaojin Zhu, and Amy Bellmore. 2012. Fast learning for sentiment analysis on bullying. In Proceedings of the First International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM ’12, pages 10:1–10:6, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>