<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.995389">
Active Learning for Statistical Phrase-based Machine Translation∗
</title>
<author confidence="0.991968">
Gholamreza Haffari and Maxim Roy and Anoop Sarkar
</author>
<affiliation confidence="0.825224666666667">
School of Computing Science
Simon Fraser University
Burnaby, BC, Canada
</affiliation>
<email confidence="0.981943">
{ghaffar1,maximr,anoop}@cs.sfu.ca
</email>
<sectionHeader confidence="0.997197" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998404733333333">
Statistical machine translation (SMT) mod-
els need large bilingual corpora for train-
ing, which are unavailable for some language
pairs. This paper provides the first serious ex-
perimental study of active learning for SMT.
We use active learning to improve the qual-
ity of a phrase-based SMT system, and show
significant improvements in translation com-
pared to a random sentence selection baseline,
when test and training data are taken from the
same or different domains. Experimental re-
sults are shown in a simulated setting using
three language pairs, and in a realistic situa-
tion for Bangla-English, a language pair with
limited translation resources.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.972260461538462">
Statistical machine translation (SMT) systems have
made great strides in translation quality. However,
high quality translation output is dependent on the
availability of massive amounts of parallel text in
the source and target language. However, there are a
large number of languages that are considered “low-
density”, either because the population speaking the
language is not very large, or even if millions of peo-
ple speak the language, insufficient amounts of par-
allel text are available in that language.
A statistical translation system can be improved
or adapted by incorporating new training data in the
form of parallel text. In this paper, we propose sev-
eral novel active learning (AL) strategies for statis-
tical machine translation in order to attack this prob-
lem. Conventional techniques for AL of classifiers
are problematic in the SMT setting. Selective sam-
pling of sentences for AL may lead to a parallel cor-
pus where each sentence does not share any phrase
∗We would like to thank Chris Callison-Burch for fruitful
discussions. This research was partially supported by NSERC,
Canada (RGPIN: 264905) and by an IBM Faculty Award to the
third author.
pairs with the others. Thus, new sentences cannot
be translated since we lack evidence for how phrase
pairs combine to form novel translations. In this pa-
per, we take the approach of exploration vs. exploita-
tion: where in some cases we pick sentences that
are not entirely novel to improve translation statis-
tics, while also injecting novel translation pairs to
improve coverage.
There may be evidence to show that AL is use-
ful even when we have massive amounts of parallel
training data. (Turchi et al., 2008) presents a com-
prehensive learning curve analysis of a phrase-based
SMT system, and one of the conclusions they draw
is, “The first obvious approach is an effort to iden-
tify or produce data sets on demand (active learning,
where the learning system can request translations of
specific sentences, to satisfy its information needs).”
Despite the promise of active learning for SMT
there has been very little experimental work pub-
lished on this issue (see Sec. 5). In this paper, we
make several novel contributions to the area of ac-
tive learning for SMT:
• We use a novel framework for AL, which to our
knowledge has not been used in AL experiments be-
fore. We assume a small amount of parallel text and
a large amount of monolingual source language text.
Using these resources, we create a large noisy par-
allel text which we then iteratively improve using
small injections of human translations.
</bodyText>
<listItem confidence="0.909350636363636">
• We provide many useful and novel features use-
ful for AL in SMT. In translation, we can leverage a
whole new set of features that were out of reach for
classification systems: we devise features that look
at the source language, but also devise features that
make an estimate of the potential utility of transla-
tions from the source, e.g. phrase pairs that could be
extracted.
• We show that AL can be useful in domain adapta-
tion. We provide the first experimental evidence in
SMT that active learning can be used to inject care-
</listItem>
<page confidence="0.980855">
415
</page>
<note confidence="0.888355">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 415–423,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.939202">
fully selected translations in order to improve SMT
output in a new domain.
</bodyText>
<listItem confidence="0.861259333333333">
• We compare our proposed features to a random se-
lection baseline in a simulated setting for three lan-
guage pairs. We also use a realistic setting: using hu-
man expert annotations in our AL system we create
an improved SMT system to translate from Bangla
to English, a language pair with very few resources.
</listItem>
<sectionHeader confidence="0.818148" genericHeader="method">
2 An Active Learning Framework for SMT
</sectionHeader>
<bodyText confidence="0.999947263157895">
Starting from an SMT model trained initially on
bilingual data, the problem is to minimize the hu-
man effort in translating new sentences which will
be added to the training data to make the retrained
SMT model achieves a certain level of performance.
Thus, given a bitext L := {(fz, ez)} and a mono-
lingual source text U := {fj}, the goal is to select
a subset of highly informative sentences from U to
present to a human expert for translation. Highly in-
formative sentences are those which, together with
their translations, help the retrained SMT system
quickly reach a certain level of translation quality.
This learning scenario is known as active learning
with Selective Sampling (Cohn et al., 1994).
Algorithm 1 describes the experimental setup we
propose for active learning. We train our initial MT
system on the bilingual corpus L, and use it to trans-
late all monolingual sentences in U. We denote sen-
tences in U together with their translations as U+
(line 4 of Algorithm 1). Then we retrain the SMT
system on LUU+ and use the resulting model to de-
code the test set. Afterwards, we select and remove
a subset of highly informative sentences from U,
and add those sentences together with their human-
provided translations to L. This process is continued
iteratively until a certain level of translation quality,
which in our case is measured by the BLEU score, is
met. In the baseline, against which we compare our
sentence selection methods, the sentences are cho-
sen randomly.
When (re-)training the model, two phrase tables
are learned: one from L and the other one from
U+. The phrase table obtained from U+ is added
as a new feature function in the log-linear trans-
lation model. The alternative is to ignore U+ as
in a conventional AL setting, however, in our ex-
periments we have found that using more bilingual
data, even noisy data, results in better translations.
</bodyText>
<construct confidence="0.341734">
Algorithm 1 AL-SMT
</construct>
<listItem confidence="0.9792115">
1: Given bilingual corpus L, and monolingual cor-
pus U.
2: MF,E = train(L, ∅)
3: for t = 1, 2,... do
4: U+ = translate(U, MF,E)
5: Select k sentence pairs from U+, and ask a
human for their true translations.
6: Remove the k sentences from U, and add the
k sentence pairs (translated by human) to L
7: MF,E = train(L, U+)
8: Monitor the performance on the test set T
9: end for
</listItem>
<bodyText confidence="0.999374923076923">
Phrase tables from U+ will get a 0 score in mini-
mum error rate training if they are not useful, so our
method is more general. Also, this method has been
shown empirically to be more effective (Ueffing et
al., 2007b) than (1) using the weighted combination
of the two phrase tables from L and U+, or (2) com-
bining the two sets of data and training from the bi-
text L U U+.
The setup in Algorithm 1 helps us to investigate
how to maximally take advantage of human effort
(for sentence translation) when learning an SMT
model from the available data, that includes bilin-
gual and monolingual text.
</bodyText>
<sectionHeader confidence="0.969521" genericHeader="method">
3 Sentence Selection Strategies
</sectionHeader>
<bodyText confidence="0.999903333333333">
Our sentence selection strategies can be divided into
two categories: (1) those which are independent of
the target language and just look into the source lan-
guage, and (2) those which also take into account the
target language. From the description of the meth-
ods, it will be clear to which category they belong to.
We will see in Sec. 4 that the most promising sen-
tence selection strategies belong to the second cate-
gory.
</bodyText>
<subsectionHeader confidence="0.999308">
3.1 The Utility of Translation Units
</subsectionHeader>
<bodyText confidence="0.999488875">
Phrases are basic units of translation in phrase-based
SMT models. The phrases potentially extracted
from a sentence indicate its informativeness. The
more new phrases a sentence can offer, the more
informative it is. Additionally phrase translation
probabilities need to be estimated accurately, which
means sentences that contain rare phrases are also
informative. When selecting new sentences for hu-
</bodyText>
<page confidence="0.998486">
416
</page>
<bodyText confidence="0.999948363636364">
man translation, we need to pay attention to this
tradeoff between exploration and exploitation, i.e.
selecting sentences to discover new phrases vs es-
timating accurately the phrase translation probabil-
ities. A similar argument can be made that empha-
sizes the importance of words rather than phrases for
any SMT model. Also we should take into account
that smoothing is a means for accurate estimation of
translation probabilities when events are rare. In our
work, we focus on methods that effectively expand
the lexicon or set of phrases of the model.
</bodyText>
<subsectionHeader confidence="0.564817">
3.1.1 Phrases (Geom-Phrase, Arith-Phrase)1
</subsectionHeader>
<bodyText confidence="0.999905111111111">
The more frequent a phrase is in the unlabeled
data, the more important it is to know its translation;
since it is more likely to occur in the test data (es-
pecially when the test data is in-domain with respect
to unlabeled data). The more frequent a phrase is in
the labeled data, the more unimportant it is; since
probably we have observed most of its translations.
Based on the above observations, we measure the
importance score of a sentence as:
</bodyText>
<equation confidence="0.960356">
� H
Op g(s) :=
x∈Xps
</equation>
<bodyText confidence="0.7597885">
where Xps is the set of possible phrases that sentence
s can offer, and P(x|D) is the probability of observ-
</bodyText>
<equation confidence="0.5535265">
ing x in the data D: P(x|D) = Count(x)+ǫ
I: x∈Xp D Count(x)+ǫ.
</equation>
<bodyText confidence="0.99994">
The score (1) is the averaged probability ratio of
the set of candidate phrases, i.e. the probability of
the candidate phrases under a probabilistic phrase
model based on U divided by that based on L. In ad-
dition to the geometric average in (1), we may also
consider the arithmetic average score:
</bodyText>
<equation confidence="0.9993065">
P(x|U) (2)
P(x|L)
</equation>
<bodyText confidence="0.995977857142857">
Note that (1) can be re-written as
|X3 |Ex∈Xps log p(x||L) in the logarithm space,
which is similar to (2) with the difference of
additional log.
In parallel data L, phrases are the ones which are
extracted by the usual phrase extraction algorithm;
but what are the candidate phrases in the unlabeled
</bodyText>
<footnote confidence="0.891548">
1The names in the parentheses are short names used to iden-
tify the method in the experimental results.
</footnote>
<bodyText confidence="0.999897928571429">
data? Considering the k-best list of translations can
tell us the possible phrases the input sentence may
offer. For each translation, we have access to the
phrases used by the decoder to produce that output.
However, there may be islands of out-of-vocabulary
(OOV) words that were not in the phrase table and
not translated by the decoder as a phrase. We group
together such groups of OOV words to form an OOV
phrase. The set of possible phrases we extract from
the decoder output contain those coming from the
phrase table (from labeled data L) and those coming
from OOVs. OOV phrases are also used in our com-
putation, where P(x  |L) for an OOV phrase x is
the uniform probability over all OOV phrases.
</bodyText>
<subsubsectionHeader confidence="0.91782">
3.1.2 n-grams (Geom n-gram, Arith n-gram)
</subsubsectionHeader>
<bodyText confidence="0.999972">
As an alternative to phrases, we consider n-grams
as basic units of generalization. The resulting score
is the weighted combination of the n-gram based
scores:
where Xns denotes n-grams in the sentence s, and
P(x|D, n) is the probability of x in the set of n-
grams in D. The weights wn adjust the importance
of the scores of n-grams with different lengths. In
addition to taking geometric average, we also con-
sider the arithmetic average:
</bodyText>
<equation confidence="0.999294">
P(x|U, n) (4)
P(x  |L, n)
</equation>
<bodyText confidence="0.99992725">
As a special case when N = 1, the score motivates
selecting sentences which increase the number of
unique words with new words appearing with higher
frequency in U than L.
</bodyText>
<subsectionHeader confidence="0.717751">
3.2 Similarity to the Bilingual Training Data
(Similarity)
</subsectionHeader>
<bodyText confidence="0.9999605">
The simplest way to expand the lexicon set is to
choose sentences from U which are as dissimilar
as possible to L. We measure the similarity using
weighted n-gram coverage (Ueffing et al., 2007b).
</bodyText>
<subsectionHeader confidence="0.999421">
3.3 Confidence of Translations (Confidence)
</subsectionHeader>
<bodyText confidence="0.9999535">
The decoder produces an output translation e using
the probability p(e  |f). This probability can be
</bodyText>
<equation confidence="0.975784">
P(x  |U)1 1
P(x |L) J |Xps  |(1)
1 �
Opa(s) := |Xps |
</equation>
<figure confidence="0.695531285714286">
x∈Xps
ONg (s) := N wn � P(x|U, n)
n=1 |Xns  |log (3)
x∈Xns P(x  |L, n)
ONa (s) := N wn �
E |Xns |x∈Xns
n=1
</figure>
<page confidence="0.990787">
417
</page>
<bodyText confidence="0.9997895">
treated as a confidence score for the translation. To
make the confidence score for sentences with dif-
ferent lengths comparable, we normalize using the
sentence length (Ueffing et al., 2007b).
</bodyText>
<subsectionHeader confidence="0.963881">
3.4 Feature Combination (Combined)
</subsectionHeader>
<bodyText confidence="0.999956">
The idea is to take into account the information from
several simpler methods, e.g. those mentioned in
Sec. 3.1–3.3, when producing the final ranking of
sentences. We can either merge the output rankings
of those simpler models2, or use the scores gener-
ated by them as input features for a higher level
ranking model. We use a linear model:
</bodyText>
<equation confidence="0.9972">
F(s) = � αkφk(s) (5)
k
</equation>
<bodyText confidence="0.999950571428571">
where αk are the model parameters, and φk(.) are
the feature functions from Sections 3.1–3.3, e.g.
confidence score, similarity to L, and score for the
utility of translation units. Using 20K of Spanish
unlabeled text we compared the r2 correlation co-
efficient between each of these scores which, apart
from the arithmetic and geometric versions of the
same score, showed low correlation. And so the in-
formation they provide should be complementary to
each other.
We train the parameters in (5) using two bilingual
development sets dev1 and dev2, the sentences in
dev1 can be ranked with respect to the amount by
which each particular sentence improves the BLEU
score of the retrained3 SMT model on dev2. Having
this ranking, we look for the weight vector which
produces the same ordering of sentences. As an al-
ternative to this method (or its computationally de-
manding generalization in which instead of a single
sentence, several sets of sentences of size k are se-
lected and ranked) we use a hill climbing search on
the surface of dev2’s BLEU score. For a fixed value
of the weight vector, dev1 sentences are ranked and
then the top-k output is selected and the amount
of improvement the retrained SMT system gives on
dev2’s BLEU score is measured. Starting from a
random initial value for αk’s, we improve one di-
mension at a time and traverse the discrete grid
</bodyText>
<footnote confidence="0.960784">
2To see how different rankings can be combined, see (Re-
ichart et al., 2008) which proposes this for multi-task AL.
3Here the retrained SMT model is the one learned by adding
a particular sentence from dev1 into L.
</footnote>
<bodyText confidence="0.998183333333333">
placed on the values of the weight vector. Starting
with a coarse grid, we make it finer when we get
stuck in local optima during hill climbing.
</bodyText>
<subsectionHeader confidence="0.955747">
3.5 Hierarchical Adaptive Sampling (HAS)
</subsectionHeader>
<bodyText confidence="0.999961757575758">
(Dasgupta and Hsu, 2008) propose a technique for
sample selection that, under certain settings, is guar-
anteed to be no worse than random sampling. Their
method exploits the cluster structure (if there is any)
in the unlabeled data. Ideally, querying the label
of only one of the data points in a cluster would
be enough to determine the label of the other data
points in that cluster. Their method requires that the
data set is provided in the form of a tree represent-
ing a hierarchical clustering of the data. In AL for
SMT, such a unique clustering of the unlabeled data
would be inappropriate or ad-hoc. For this reason,
we present a new algorithm inspired by the ratio-
nale provided in (Dasgupta and Hsu, 2008) that can
be used in our setting, where we construct a tree-
based partition of the data dynamically4. This dy-
namic tree construction allows us to extend the HAS
algorithm from classifiers to the SMT task.
The algorithm adaptively samples sentences from
U while building a hierarchical clustering of the sen-
tences in U (see Fig. 1 and Algorithm 2). At any it-
eration, first we retrain the SMT model and translate
all monolingual sentences. At this point one mono-
lingual set of sentences represented by one of the
tree leaves is chosen for further partitioning: a leaf
H is chosen which has the lowest average decoder
confidence score for its sentence translations. We
then rank all sentences in H based on their similar-
ity to L and put the top α|H |sentences in H1 and
the rest in H2. To select K sentences, we randomly
sample QK sentences from H1 and (1 − Q)K sen-
tences from H2 and ask a human for their transla-
tions.
</bodyText>
<subsectionHeader confidence="0.498234">
3.6 Reverse Model (Reverse)
</subsectionHeader>
<bodyText confidence="0.999111">
While a translation system MF,E is built from lan-
guage F to language E, we also build a translation
system in the reverse direction ME,F. To mea-
sure how informative a monolingual sentence f is,
we translate it to English by MF,E and then project
</bodyText>
<footnote confidence="0.993910333333333">
4The dynamic nature of the hierarchy comes from two fac-
tors: (1) selecting a leaf node for splitting, and (2) splitting a
leaf node based on its similarity to the growing L.
</footnote>
<page confidence="0.934736">
418
</page>
<bodyText confidence="0.257303">
Algorithm 2 Hierarchical-Adaptive-Sampling
</bodyText>
<listItem confidence="0.9518462">
1: MF→E = train(L, ∅)
2: Initialize the tree T by setting its root to U
3: v := root(T)
4: for t = 1, 2, ... do
5: // rank and split sentence in v
</listItem>
<equation confidence="0.71363">
X1, X2 := Partition(L, v, α)
6: // randomly sample and remove sents from Xi
Y1, Y2 := Sampling(X1,X2,Q)
7: // make Xi children of node v in the tree T
T := UpdateTree(X1, X2, v, T)
8: // Y i+ has sents in Yi together with human trans
L:=LUY1+UY2+
</equation>
<listItem confidence="0.89801625">
9: MF→E = train(L, U)
10: for all leaves l E T do
11: Z[l] := Average normalized confidence scores
of sentence translations in l
</listItem>
<figure confidence="0.8188158">
12: end for
13: v := BestLeaf(T, Z)
14: Monitor the performance on the test set
15: end for
H22 H21
</figure>
<figureCaption confidence="0.988533">
Figure 1: Adaptively sampling the sentences while con-
structing a hierarchical clustering of U.
</figureCaption>
<bodyText confidence="0.999274">
the translation back to French using ME→F. Denote
this reconstructed version of the original French
sentence by ˜f. Comparing f with f˜ using BLEU (or
other measures) can tell us how much information
has been lost due to our direct and/or reverse transla-
tion systems. The sentences with higher information
loss are selected for translation by a human.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999889888888889">
The SMT system we applied in our experiments is
PORTAGE (Ueffing et al., 2007a). The models (or
features) which are employed by the decoder are:
(a) one or several phrase table(s), which model the
translation direction p(f  |e), (b) one or several
n-gram language model(s) trained with the SRILM
toolkit (Stolcke, 2002); in the experiments reported
here, we used 4-gram models on the NIST data,
and a trigram model on EuroParl, (c) a distortion
</bodyText>
<table confidence="0.9997504">
corpus language use sentences
EuroParl Fr,Ge,Sp in-dom L 5K
in-dom U 20K
in-dom dev 2K
in-dom test 2K
See Sec. 4.2 Bangla in-dom L 11K
in-dom U 20K
in-dom dev 450
in-dom test 1K
Hansards Fr out-dom L 5K
</table>
<tableCaption confidence="0.799305">
Table 1: Specification of different data sets we will use in
experiments. The target language is English in the bilin-
gual sets, and the source languages are either French (Fr),
German (Ge), Spanish (Sp), or Bangla.
</tableCaption>
<bodyText confidence="0.99996025">
model which assigns a penalty based on the number
of source words which are skipped when generating
a new target phrase, and (d) a word penalty. These
different models are combined log-linearly. Their
weights are optimized w.r.t. BLEU score using the
algorithm described in (Och, 2003). This is done on
a development corpus which we will call dev1 in this
paper.
The weight vectors in n-gram and similarity
methods are set to (.15, .2, .3, .35) to emphasize
longer n-grams. We set α = Q = .35 for HAS,
and use the 100-best list of translations when identi-
fying candidate phrases while setting the maximum
phrase length to 10. We set c = .5 to smooth proba-
bilities when computing scores based on translation
units.
</bodyText>
<subsectionHeader confidence="0.963219">
4.1 Simulated Low Density Language Pairs
</subsectionHeader>
<bodyText confidence="0.97775825">
We use three language pairs (French-English,
German-English, Spanish-English) to compare all of
the proposed sentence selection strategies in a simu-
lated AL setting. The training data comes from Eu-
roParl corpus as distributed for the shared task in
the NAACL 2006 workshop on statistical machine
translation (WSMT06). For each language pair, the
first 5K sentences from its bilingual corpus consti-
tute L, and the next 20K sentences serve as U where
the target side translation is ignored. The size of L
was taken to be 5K in order to be close to a real-
istic setting in SMT. We use the first 2K sentences
from the test sets provided for WSMT06, which are
in-domain, as our test sets. The corpus statistics are
summarized in Table 1. The results are shown in
Fig. 2. After building the initial MT systems, we se-
</bodyText>
<equation confidence="0.907627333333333">
H1
H2
H := U
</equation>
<page confidence="0.981184">
419
</page>
<note confidence="0.585128">
French to English German to English Spanish to English
</note>
<figure confidence="0.999317879120879">
0.225
0.215
0.205
0.195
0.22
0.21
0.19
0 5 10 15 20 25
0.2
0 5 10 15 20 25
0 5 10 15 20 25
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0.175
0.165
0.155
0.145
0.17
0.16
0.15
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0.225
0.215
0.205
0.23
0.22
0.21
0.2
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
Added Sentences (multiple of 200)
French to English
Added Sentences (multiple of 200)
German to English
Added Sentences (multiple of 200)
Spanish to English
0.225
0.175
0.225
0.22
0.165
0.21
0.215
0.16
0.205
0.21
0.155
0.2
0.205
0.15
0.195
0.2
0.19
0 5 10 15 20 25 0 5 10 15 20 25
0.22
0.215
Geom 4−gram
Geom 1−gram
Similarity
Combined
Random
0.17
Geom 4−gram
Geom 1−gram
Similarity
Combined
Random
0.23
0.145
Geom 4−gram
Geom 1−gram
Similarity
Combined
Random
0 5 10 15 20 25
Added Sentences (multiple of 200)
Added Sentences (multiple of 200)
Added Sentences (multiple of 200)
</figure>
<figureCaption confidence="0.8065056">
Figure 2: BLEU scores for different sentence selection strategies per iteration of the AL algorithm. Plots at the top
show the performance of sentence selection methods which depend on the target language in addition to the source
language (hierarchical adaptive sampling, reverse model, decoder confidence, average and geometric phrase-based
score), and plots at the bottom show methods which are independent of the target language (geometric 4-gram and
1-gram, similarity to L, and random sentence selection baseline).
</figureCaption>
<bodyText confidence="0.999701933333333">
lect and remove 200 sentence from U in each itera-
tion and add them together with translations to L for
25 iterations. Each experiment which involves ran-
domness, such as random sentence selection base-
line and HAS, is averaged over three independent
runs. Selecting sentences based on the phrase-based
utility score outperforms the strong random sentence
selection baseline and other methods (Table 2). De-
coder confidence performs poorly as a criterion for
sentence selection in this setting, and HAS which
is built on top of confidence and similarity scores
outperforms both of them. Although choosing sen-
tences based on their n-gram score ignores the re-
lationship between source and target languages, this
methods outperforms random sentence selection.
</bodyText>
<subsectionHeader confidence="0.980571">
4.2 Realistic Low Density Language Pair
</subsectionHeader>
<bodyText confidence="0.99982685">
We apply active learning to the Bangla-English ma-
chine translation task. Bangla is the official lan-
guage of Bangladesh and second most spoken lan-
guage in India. It has more than 200 million speak-
ers around the world. However, Bangla has few
available language resources, and lacks resources
for machine translation. In our experiments, we use
training data provided by the Linguistic Data Con-
sortium5 containing ∼11k sentences. It contains
newswire text from the BBC Asian Network and
some other South Asian news websites. A bilingual
Bangla-English dictionary collected from different
websites was also used as part of the training set
which contains around 85k words. Our monolingual
corpus6 is built by collecting text from the Prothom
Alo newspaper, and contains all the news available
for the year of 2005 – including magazines and pe-
riodicals. The corpus has 18,067,470 word tokens
and 386,639 word types. For our language model we
used data from the English section of EuroParl. The
</bodyText>
<footnote confidence="0.873142666666667">
5LDC Catalog No.: LDC2008E29.
6Provided by the Center for Research on Bangla Language
Processing, BRAC University, Bangladesh.
</footnote>
<bodyText confidence="0.9999449375">
development set used to optimize the model weights
in the decoder, and test set used for evaluation was
taken from the same LDC corpus mentioned above.
We applied our active learning framework to the
problem of creating a larger Bangla-English parallel
text resource. The second author is a native speaker
of Bangla and participated in the active learning
loop, translating 100 sentences in each iteration. We
compared a smaller number of alternative methods
to keep the annotation cost down. The results are
shown in Fig. 3. Unlike the simulated setting, in this
realistic setting for AL, adding more human transla-
tion does not always result in better translation per-
formance7. Geom 4-gram and Geom phrase are the
features that prove most useful in extracting useful
sentences for the human expert to translate.
</bodyText>
<subsectionHeader confidence="0.998072">
4.3 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999949071428572">
In this section, we investigate the behavior of the
proposed methods when unlabeled data U and test
data T are in-domain and parallel training text L is
out-of-domain.
We report experiments for French to English
translation task where T and development sets are
the same as those in section 4.1 but the bilingual
training data come from Hansards8 corpus. The do-
main is similar to EuroParl, but the vocabulary is
very different. The results are shown in Fig. 4, and
summarized in Table 3. As expected, unigram based
sentence selection performs well in this scenario
since it quickly expands the lexicon set of the bilin-
gual data in an effective manner (Fig 5). By ignor-
</bodyText>
<footnote confidence="0.9240345">
7This is likely due to the fact that the translator in the AL
loop was not the same as the original translator for the labeled
data.
8The transcription of official records of the Cana-
dian Parliament as distributed at http://www.isi.edu/natural-
language/download/hansard
</footnote>
<table confidence="0.9983878">
Lang. Geom Phrase Random (baseline)
Pair bleu% per% wer% bleu% per% wer%
Fr-En 22.49 27.99 38.45 21.97 28.31 38.80
Gr-En 17.54 31.51 44.28 17.25 31.63 44.41
Sp-En 23.03 28.86 39.17 23.00 28.97 39.21
</table>
<tableCaption confidence="0.9867135">
Table 2: Phrase-based utility selection is compared
with random sentence selection baseline with respect to
BLEU, wer (word error rate), and per (position indepen-
dent word error rate) across three language pairs.
</tableCaption>
<table confidence="0.9998345">
method bleu% per% wer%
Geom 1-gram 14.92 34.83 46.06
Confidence 14.74 35.02 46.11
Random (baseline) 14.11 35.28 46.47
</table>
<tableCaption confidence="0.898313666666667">
Table 3: Comparison of methods in domain adaptation
scenario. The bold numbers show statistically significant
improvement with respect to the baseline.
</tableCaption>
<bodyText confidence="0.996610333333333">
ing sentences for which the translations are already
known based on L, it does not waste resources. On
the other hand, it raises the importance of high fre-
quency words in U. Interestingly, decoder confi-
dence is also a good criterion for sentence selection
in this particular case.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99993525">
Despite the promise of active learning for SMT
for domain adaptation and low-density/low-resource
languages, there has been very little work published
on this issue. A Ph.D. proposal by Chris Callison-
Burch (Callison-burch, 2003) lays out the promise
of AL for SMT and proposes some algorithms.
However, the lack of experimental results means that
performance and feasibility of those methods can-
not be compared to ours. (Mohit and Hwa, 2007)
provide a technique to classify phrases as difficult
to translate (DTP), and incorporate human transla-
tions for these phrases. Their approach is differ-
ent from AL: they use human translations for DTPs
in order to improve translation output in the de-
coder. There is work on sampling sentence pairs for
SMT (Kauchak, 2006; Eck et al., 2005) but the goal
</bodyText>
<subsectionHeader confidence="0.280041">
Bangla to English
</subsectionHeader>
<bodyText confidence="0.45412">
Added Sentences (multiple of 100)
</bodyText>
<figureCaption confidence="0.9990855">
Figure 3: Improving Bangla to English translation perfor-
mance using active learning.
</figureCaption>
<figure confidence="0.921315052631579">
0.057
0.056
BLEU score
0.055
Geom Phrase
HAS
Geom 4−gram
Random
0.05
0 1 2 3 4 5
0.054
0.053
0.052
0.051
421
French to English
Added Sentences (multiple of 200)
French to English
Added Sentences (multiple of 200)
</figure>
<figureCaption confidence="0.980015">
Figure 4: Performance of different sentence selection
methods for domain adaptation scenario.
</figureCaption>
<bodyText confidence="0.999975181818182">
has been to limit the amount of training data in order
to reduce the memory footprint of the SMT decoder.
To compute this score, (Eck et al., 2005) use n-gram
features very different from the n-gram features pro-
posed in this paper. (Kato and Barnard, 2007) imple-
ment an AL system for SMT for language pairs with
limited resources (En-Xhosa, En-Zulu, En-Setswana
and En-Afrikaans), but the experiments are on a very
small simulated data set. The only feature used is
the confidence score of the SMT system, which we
showed in our experiments is not a reliable feature.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999937555555556">
We provided a novel active learning framework for
SMT which utilizes both labeled and unlabeled data.
Several sentence selection strategies were proposed
and comprehensively compared across three simu-
lated language pairs and a realistic setting of Bangla-
English translation with scarce resources. Based
on our experiments, we conclude that paying atten-
tion to units of translations, i.e. words and candi-
date phrases in particular, is essential to sentence se-
</bodyText>
<table confidence="0.999841">
Fr2En Ge2En Sp2En Ha2En
Avg # of trans 1.30 1.26 1.27 1.30
1.24 1.25 1.20 1.26
1.22 1.23 1.19 1.24
1.22 1.24 1.19 1.24
Avg phrase len 2.85 2.56 2.85 2.85
3.47 2.74 3.54 3.17
3.95 3.34 3.94 3.48
3.58 2.94 3.63 3.36
# of phrases 27,566 29,297 30,750 27,566
78,026 64,694 93,593 108,787
79,343 63,191 93,276 115,177
77,394 65,198 94,597 115,671
# unique events 31,824 33,141 34,937 31,824
103,124 84,512 125,094 117,214
86,210 69,357 100,176 127,314
84,787 72,280 101,636 128,912
</table>
<tableCaption confidence="0.779541">
Table 4: Average number of english phrases per source
language phrase, average length of the source language
phrases, number of source language phrases, and number
</tableCaption>
<bodyText confidence="0.898418736842105">
of phrase pairs which has been seen once in the phrase ta-
bles across three language pairs (French text taken from
Hansard is abbreviated by ’Ha’). From top to bottom
in each row, the numbers belong to: before starting AL,
and after finishing AL based on ’Geom Phrase’, ’Confi-
dence’, and ’Random’.
lection in AL-SMT. Increasing the coverage of the
bilingual training data is important but is not the
only factor (see Table 4 and Fig. 5). For exam-
ple, decoder confidence for sentence selection has
low coverage (in terms of new words), but performs
well in the domain adaptation scenario and performs
poorly otherwise. In future work, we plan to ex-
plore selection methods based on potential phrases,
adaptive sampling using features other than decoder
confidence and the use of features from confidence
estimation in MT (Ueffing and Ney, 2007).
French to English
Added Sentences (multiple of 200)
</bodyText>
<figureCaption confidence="0.992566">
Figure 5: Number of words in domain adaptation sce-
nario.
</figureCaption>
<figure confidence="0.99975429787234">
0.15
HAS
Reverse
Confidence
Arith Phrase
Geom Phrase
Random
0.13
0.125
0 5 10 15
20 25
0.145
BLEU score
0.14
0.135
0.13
0.125
0 5 10 15 20
Geom 4−gram
Geom 1−gram
Combined
Random
25
BLEU score
0.14
0.135
0.15
0.145
0 5 10 15 20 25
Number of New Words
18000
16000
14000
12000
10000
8000
6000
4000
2000
Geom 4−gram
HAS
Reverse
Confidence
Similarity
Random
Geom 1−gram
Geom Phrase
</figure>
<page confidence="0.99527">
422
</page>
<sectionHeader confidence="0.998105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735188679245">
Chris Callison-burch. 2003. Active learning for statisti-
cal machine translation. In PhD Proposal, Edinburgh
University.
David Cohn, Les Atlas, and Richard Ladner. 1994. Im-
proving generalization with active learning. In Ma-
chine Learning Journal.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
sampling for active learning. In proceedings of Inter-
national Conference on Machine Learning.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low cost portability for statistical machine translation
based in n-gram frequency and tf-idf. In proceedings
ofInternational Workshop on Spoken Language Trans-
lation (IWSLT).
R.S.M. Kato and E. Barnard. 2007. Statistical transla-
tion with scarce resources: a south african case study.
SAIEE Africa Research Journal, 98(4):136–140, De-
cember.
David Kauchak. 2006. Contribution to research on ma-
chine translation. In PhD Thesis, University of Cali-
fornia at San Diego.
Behrang Mohit and Rebecca Hwa. 2007. Localization
of difficult-to-translate phrases. In proceedings of the
2nd ACL Workshop on Statistical Machine Transla-
tions.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In proceedings of
Annual Meeting of the Association for Computational
Linguistics (ACL).
Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rap-
poport. 2008. Multi-task active learning for linguistic
annotations. In proceedings ofAnnual Meeting of the
Association for Computational Linguistics (ACL).
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In proceedings of Interna-
tional Conference on Spoken Language Processing
(ICSLP).
Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008.
Learning performance of a machine translation sys-
tem: a statistical and computational analysis. In pro-
ceedings of the Third Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics (ACL).
Nicola Ueffing and Hermann Ney. 2007. Word-level
confidence estimation for machine translation. Com-
putational Linguistics, 33(1):9–40.
N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson.
2007a. NRC’s Portage system for WMT 2007. In
Proc. ACL Workshop on SMT.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007b. Transductive learning for statistical machine
translation. In proceedings of Annual Meeting of the
Association for Computational Linguistics (ACL).
</reference>
<page confidence="0.999423">
423
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985374">
<title confidence="0.999953">Learning for Statistical Phrase-based Machine</title>
<author confidence="0.999337">Haffari Roy</author>
<affiliation confidence="0.9966815">School of Computing Simon Fraser University</affiliation>
<address confidence="0.996994">Burnaby, BC, Canada</address>
<abstract confidence="0.9997058125">Statistical machine translation (SMT) models need large bilingual corpora for training, which are unavailable for some language pairs. This paper provides the first serious experimental study of active learning for SMT. We use active learning to improve the quality of a phrase-based SMT system, and show significant improvements in translation compared to a random sentence selection baseline, when test and training data are taken from the same or different domains. Experimental results are shown in a simulated setting using three language pairs, and in a realistic situation for Bangla-English, a language pair with limited translation resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-burch</author>
</authors>
<title>Active learning for statistical machine translation. In</title>
<date>2003</date>
<tech>PhD Proposal,</tech>
<institution>Edinburgh University.</institution>
<contexts>
<context position="26663" citStr="Callison-burch, 2003" startWordPosition="4543" endWordPosition="4544">nario. The bold numbers show statistically significant improvement with respect to the baseline. ing sentences for which the translations are already known based on L, it does not waste resources. On the other hand, it raises the importance of high frequency words in U. Interestingly, decoder confidence is also a good criterion for sentence selection in this particular case. 5 Related Work Despite the promise of active learning for SMT for domain adaptation and low-density/low-resource languages, there has been very little work published on this issue. A Ph.D. proposal by Chris CallisonBurch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. (Mohit and Hwa, 2007) provide a technique to classify phrases as difficult to translate (DTP), and incorporate human translations for these phrases. Their approach is different from AL: they use human translations for DTPs in order to improve translation output in the decoder. There is work on sampling sentence pairs for SMT (Kauchak, 2006; Eck et al., 2005) but the goal Bangla to English Added Sentences </context>
</contexts>
<marker>Callison-burch, 2003</marker>
<rawString>Chris Callison-burch. 2003. Active learning for statistical machine translation. In PhD Proposal, Edinburgh University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Cohn</author>
<author>Les Atlas</author>
<author>Richard Ladner</author>
</authors>
<title>Improving generalization with active learning.</title>
<date>1994</date>
<booktitle>In Machine Learning Journal.</booktitle>
<contexts>
<context position="5318" citStr="Cohn et al., 1994" startWordPosition="873" endWordPosition="876">imize the human effort in translating new sentences which will be added to the training data to make the retrained SMT model achieves a certain level of performance. Thus, given a bitext L := {(fz, ez)} and a monolingual source text U := {fj}, the goal is to select a subset of highly informative sentences from U to present to a human expert for translation. Highly informative sentences are those which, together with their translations, help the retrained SMT system quickly reach a certain level of translation quality. This learning scenario is known as active learning with Selective Sampling (Cohn et al., 1994). Algorithm 1 describes the experimental setup we propose for active learning. We train our initial MT system on the bilingual corpus L, and use it to translate all monolingual sentences in U. We denote sentences in U together with their translations as U+ (line 4 of Algorithm 1). Then we retrain the SMT system on LUU+ and use the resulting model to decode the test set. Afterwards, we select and remove a subset of highly informative sentences from U, and add those sentences together with their humanprovided translations to L. This process is continued iteratively until a certain level of trans</context>
</contexts>
<marker>Cohn, Atlas, Ladner, 1994</marker>
<rawString>David Cohn, Les Atlas, and Richard Ladner. 1994. Improving generalization with active learning. In Machine Learning Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
<author>Daniel Hsu</author>
</authors>
<title>Hierarchical sampling for active learning.</title>
<date>2008</date>
<booktitle>In proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="14661" citStr="Dasgupta and Hsu, 2008" startWordPosition="2506" endWordPosition="2509">mount of improvement the retrained SMT system gives on dev2’s BLEU score is measured. Starting from a random initial value for αk’s, we improve one dimension at a time and traverse the discrete grid 2To see how different rankings can be combined, see (Reichart et al., 2008) which proposes this for multi-task AL. 3Here the retrained SMT model is the one learned by adding a particular sentence from dev1 into L. placed on the values of the weight vector. Starting with a coarse grid, we make it finer when we get stuck in local optima during hill climbing. 3.5 Hierarchical Adaptive Sampling (HAS) (Dasgupta and Hsu, 2008) propose a technique for sample selection that, under certain settings, is guaranteed to be no worse than random sampling. Their method exploits the cluster structure (if there is any) in the unlabeled data. Ideally, querying the label of only one of the data points in a cluster would be enough to determine the label of the other data points in that cluster. Their method requires that the data set is provided in the form of a tree representing a hierarchical clustering of the data. In AL for SMT, such a unique clustering of the unlabeled data would be inappropriate or ad-hoc. For this reason, </context>
</contexts>
<marker>Dasgupta, Hsu, 2008</marker>
<rawString>Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical sampling for active learning. In proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Low cost portability for statistical machine translation based in n-gram frequency and tf-idf.</title>
<date>2005</date>
<booktitle>In proceedings ofInternational Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="27215" citStr="Eck et al., 2005" startWordPosition="4634" endWordPosition="4637">e. A Ph.D. proposal by Chris CallisonBurch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. (Mohit and Hwa, 2007) provide a technique to classify phrases as difficult to translate (DTP), and incorporate human translations for these phrases. Their approach is different from AL: they use human translations for DTPs in order to improve translation output in the decoder. There is work on sampling sentence pairs for SMT (Kauchak, 2006; Eck et al., 2005) but the goal Bangla to English Added Sentences (multiple of 100) Figure 3: Improving Bangla to English translation performance using active learning. 0.057 0.056 BLEU score 0.055 Geom Phrase HAS Geom 4−gram Random 0.05 0 1 2 3 4 5 0.054 0.053 0.052 0.051 421 French to English Added Sentences (multiple of 200) French to English Added Sentences (multiple of 200) Figure 4: Performance of different sentence selection methods for domain adaptation scenario. has been to limit the amount of training data in order to reduce the memory footprint of the SMT decoder. To compute this score, (Eck et al., </context>
</contexts>
<marker>Eck, Vogel, Waibel, 2005</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Low cost portability for statistical machine translation based in n-gram frequency and tf-idf. In proceedings ofInternational Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S M Kato</author>
<author>E Barnard</author>
</authors>
<title>Statistical translation with scarce resources: a south african case study.</title>
<date>2007</date>
<journal>SAIEE Africa Research Journal,</journal>
<volume>98</volume>
<issue>4</issue>
<contexts>
<context position="27929" citStr="Kato and Barnard, 2007" startWordPosition="4754" endWordPosition="4757">a to English translation performance using active learning. 0.057 0.056 BLEU score 0.055 Geom Phrase HAS Geom 4−gram Random 0.05 0 1 2 3 4 5 0.054 0.053 0.052 0.051 421 French to English Added Sentences (multiple of 200) French to English Added Sentences (multiple of 200) Figure 4: Performance of different sentence selection methods for domain adaptation scenario. has been to limit the amount of training data in order to reduce the memory footprint of the SMT decoder. To compute this score, (Eck et al., 2005) use n-gram features very different from the n-gram features proposed in this paper. (Kato and Barnard, 2007) implement an AL system for SMT for language pairs with limited resources (En-Xhosa, En-Zulu, En-Setswana and En-Afrikaans), but the experiments are on a very small simulated data set. The only feature used is the confidence score of the SMT system, which we showed in our experiments is not a reliable feature. 6 Conclusions We provided a novel active learning framework for SMT which utilizes both labeled and unlabeled data. Several sentence selection strategies were proposed and comprehensively compared across three simulated language pairs and a realistic setting of BanglaEnglish translation </context>
</contexts>
<marker>Kato, Barnard, 2007</marker>
<rawString>R.S.M. Kato and E. Barnard. 2007. Statistical translation with scarce resources: a south african case study. SAIEE Africa Research Journal, 98(4):136–140, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
</authors>
<title>Contribution to research on machine translation. In</title>
<date>2006</date>
<tech>PhD Thesis,</tech>
<institution>University of California at San Diego.</institution>
<contexts>
<context position="27196" citStr="Kauchak, 2006" startWordPosition="4632" endWordPosition="4633">ed on this issue. A Ph.D. proposal by Chris CallisonBurch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. (Mohit and Hwa, 2007) provide a technique to classify phrases as difficult to translate (DTP), and incorporate human translations for these phrases. Their approach is different from AL: they use human translations for DTPs in order to improve translation output in the decoder. There is work on sampling sentence pairs for SMT (Kauchak, 2006; Eck et al., 2005) but the goal Bangla to English Added Sentences (multiple of 100) Figure 3: Improving Bangla to English translation performance using active learning. 0.057 0.056 BLEU score 0.055 Geom Phrase HAS Geom 4−gram Random 0.05 0 1 2 3 4 5 0.054 0.053 0.052 0.051 421 French to English Added Sentences (multiple of 200) French to English Added Sentences (multiple of 200) Figure 4: Performance of different sentence selection methods for domain adaptation scenario. has been to limit the amount of training data in order to reduce the memory footprint of the SMT decoder. To compute this s</context>
</contexts>
<marker>Kauchak, 2006</marker>
<rawString>David Kauchak. 2006. Contribution to research on machine translation. In PhD Thesis, University of California at San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Rebecca Hwa</author>
</authors>
<title>Localization of difficult-to-translate phrases.</title>
<date>2007</date>
<booktitle>In proceedings of the 2nd ACL Workshop on Statistical Machine Translations.</booktitle>
<contexts>
<context position="26876" citStr="Mohit and Hwa, 2007" startWordPosition="4577" endWordPosition="4580">, it raises the importance of high frequency words in U. Interestingly, decoder confidence is also a good criterion for sentence selection in this particular case. 5 Related Work Despite the promise of active learning for SMT for domain adaptation and low-density/low-resource languages, there has been very little work published on this issue. A Ph.D. proposal by Chris CallisonBurch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. (Mohit and Hwa, 2007) provide a technique to classify phrases as difficult to translate (DTP), and incorporate human translations for these phrases. Their approach is different from AL: they use human translations for DTPs in order to improve translation output in the decoder. There is work on sampling sentence pairs for SMT (Kauchak, 2006; Eck et al., 2005) but the goal Bangla to English Added Sentences (multiple of 100) Figure 3: Improving Bangla to English translation performance using active learning. 0.057 0.056 BLEU score 0.055 Geom Phrase HAS Geom 4−gram Random 0.05 0 1 2 3 4 5 0.054 0.053 0.052 0.051 421 F</context>
</contexts>
<marker>Mohit, Hwa, 2007</marker>
<rawString>Behrang Mohit and Rebecca Hwa. 2007. Localization of difficult-to-translate phrases. In proceedings of the 2nd ACL Workshop on Statistical Machine Translations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In proceedings of Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="18992" citStr="Och, 2003" startWordPosition="3283" endWordPosition="3284"> test 2K See Sec. 4.2 Bangla in-dom L 11K in-dom U 20K in-dom dev 450 in-dom test 1K Hansards Fr out-dom L 5K Table 1: Specification of different data sets we will use in experiments. The target language is English in the bilingual sets, and the source languages are either French (Fr), German (Ge), Spanish (Sp), or Bangla. model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, and (d) a word penalty. These different models are combined log-linearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper. The weight vectors in n-gram and similarity methods are set to (.15, .2, .3, .35) to emphasize longer n-grams. We set α = Q = .35 for HAS, and use the 100-best list of translations when identifying candidate phrases while setting the maximum phrase length to 10. We set c = .5 to smooth probabilities when computing scores based on translation units. 4.1 Simulated Low Density Language Pairs We use three language pairs (French-English, German-English, Spanish-English) to compare all of the proposed sentence selection st</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In proceedings of Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Katrin Tomanek</author>
<author>Udo Hahn</author>
<author>Ari Rappoport</author>
</authors>
<title>Multi-task active learning for linguistic annotations.</title>
<date>2008</date>
<booktitle>In proceedings ofAnnual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14312" citStr="Reichart et al., 2008" startWordPosition="2445" endWordPosition="2449">ive to this method (or its computationally demanding generalization in which instead of a single sentence, several sets of sentences of size k are selected and ranked) we use a hill climbing search on the surface of dev2’s BLEU score. For a fixed value of the weight vector, dev1 sentences are ranked and then the top-k output is selected and the amount of improvement the retrained SMT system gives on dev2’s BLEU score is measured. Starting from a random initial value for αk’s, we improve one dimension at a time and traverse the discrete grid 2To see how different rankings can be combined, see (Reichart et al., 2008) which proposes this for multi-task AL. 3Here the retrained SMT model is the one learned by adding a particular sentence from dev1 into L. placed on the values of the weight vector. Starting with a coarse grid, we make it finer when we get stuck in local optima during hill climbing. 3.5 Hierarchical Adaptive Sampling (HAS) (Dasgupta and Hsu, 2008) propose a technique for sample selection that, under certain settings, is guaranteed to be no worse than random sampling. Their method exploits the cluster structure (if there is any) in the unlabeled data. Ideally, querying the label of only one of </context>
</contexts>
<marker>Reichart, Tomanek, Hahn, Rappoport, 2008</marker>
<rawString>Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport. 2008. Multi-task active learning for linguistic annotations. In proceedings ofAnnual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In proceedings of International Conference on Spoken Language Processing (ICSLP).</booktitle>
<contexts>
<context position="18163" citStr="Stolcke, 2002" startWordPosition="3140" endWordPosition="3141">ersion of the original French sentence by ˜f. Comparing f with f˜ using BLEU (or other measures) can tell us how much information has been lost due to our direct and/or reverse translation systems. The sentences with higher information loss are selected for translation by a human. 4 Experiments The SMT system we applied in our experiments is PORTAGE (Ueffing et al., 2007a). The models (or features) which are employed by the decoder are: (a) one or several phrase table(s), which model the translation direction p(f |e), (b) one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the experiments reported here, we used 4-gram models on the NIST data, and a trigram model on EuroParl, (c) a distortion corpus language use sentences EuroParl Fr,Ge,Sp in-dom L 5K in-dom U 20K in-dom dev 2K in-dom test 2K See Sec. 4.2 Bangla in-dom L 11K in-dom U 20K in-dom dev 450 in-dom test 1K Hansards Fr out-dom L 5K Table 1: Specification of different data sets we will use in experiments. The target language is English in the bilingual sets, and the source languages are either French (Fr), German (Ge), Spanish (Sp), or Bangla. model which assigns a penalty based on the number of sou</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In proceedings of International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Tijl De Bie</author>
<author>Nello Cristianini</author>
</authors>
<title>Learning performance of a machine translation system: a statistical and computational analysis.</title>
<date>2008</date>
<booktitle>In proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics (ACL).</booktitle>
<marker>Turchi, De Bie, Cristianini, 2008</marker>
<rawString>Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learning performance of a machine translation system: a statistical and computational analysis. In proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Word-level confidence estimation for machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. Word-level confidence estimation for machine translation. Computational Linguistics, 33(1):9–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>M Simard</author>
<author>S Larkin</author>
<author>J H Johnson</author>
</authors>
<title>NRC’s Portage system for WMT</title>
<date>2007</date>
<booktitle>In Proc. ACL Workshop on SMT.</booktitle>
<contexts>
<context position="7103" citStr="Ueffing et al., 2007" startWordPosition="1198" endWordPosition="1201">slations. Algorithm 1 AL-SMT 1: Given bilingual corpus L, and monolingual corpus U. 2: MF,E = train(L, ∅) 3: for t = 1, 2,... do 4: U+ = translate(U, MF,E) 5: Select k sentence pairs from U+, and ask a human for their true translations. 6: Remove the k sentences from U, and add the k sentence pairs (translated by human) to L 7: MF,E = train(L, U+) 8: Monitor the performance on the test set T 9: end for Phrase tables from U+ will get a 0 score in minimum error rate training if they are not useful, so our method is more general. Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext L U U+. The setup in Algorithm 1 helps us to investigate how to maximally take advantage of human effort (for sentence translation) when learning an SMT model from the available data, that includes bilingual and monolingual text. 3 Sentence Selection Strategies Our sentence selection strategies can be divided into two categories: (1) those which are independent of the target language and just look into the source language, and (2) those which als</context>
<context position="11972" citStr="Ueffing et al., 2007" startWordPosition="2036" endWordPosition="2039">D. The weights wn adjust the importance of the scores of n-grams with different lengths. In addition to taking geometric average, we also consider the arithmetic average: P(x|U, n) (4) P(x |L, n) As a special case when N = 1, the score motivates selecting sentences which increase the number of unique words with new words appearing with higher frequency in U than L. 3.2 Similarity to the Bilingual Training Data (Similarity) The simplest way to expand the lexicon set is to choose sentences from U which are as dissimilar as possible to L. We measure the similarity using weighted n-gram coverage (Ueffing et al., 2007b). 3.3 Confidence of Translations (Confidence) The decoder produces an output translation e using the probability p(e |f). This probability can be P(x |U)1 1 P(x |L) J |Xps |(1) 1 � Opa(s) := |Xps | x∈Xps ONg (s) := N wn � P(x|U, n) n=1 |Xns |log (3) x∈Xns P(x |L, n) ONa (s) := N wn � E |Xns |x∈Xns n=1 417 treated as a confidence score for the translation. To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al., 2007b). 3.4 Feature Combination (Combined) The idea is to take into account the information from several s</context>
<context position="17922" citStr="Ueffing et al., 2007" startWordPosition="3100" endWordPosition="3103">:= BestLeaf(T, Z) 14: Monitor the performance on the test set 15: end for H22 H21 Figure 1: Adaptively sampling the sentences while constructing a hierarchical clustering of U. the translation back to French using ME→F. Denote this reconstructed version of the original French sentence by ˜f. Comparing f with f˜ using BLEU (or other measures) can tell us how much information has been lost due to our direct and/or reverse translation systems. The sentences with higher information loss are selected for translation by a human. 4 Experiments The SMT system we applied in our experiments is PORTAGE (Ueffing et al., 2007a). The models (or features) which are employed by the decoder are: (a) one or several phrase table(s), which model the translation direction p(f |e), (b) one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the experiments reported here, we used 4-gram models on the NIST data, and a trigram model on EuroParl, (c) a distortion corpus language use sentences EuroParl Fr,Ge,Sp in-dom L 5K in-dom U 20K in-dom dev 2K in-dom test 2K See Sec. 4.2 Bangla in-dom L 11K in-dom U 20K in-dom dev 450 in-dom test 1K Hansards Fr out-dom L 5K Table 1: Specification of diff</context>
</contexts>
<marker>Ueffing, Simard, Larkin, Johnson, 2007</marker>
<rawString>N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson. 2007a. NRC’s Portage system for WMT 2007. In Proc. ACL Workshop on SMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Transductive learning for statistical machine translation.</title>
<date>2007</date>
<booktitle>In proceedings of Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7103" citStr="Ueffing et al., 2007" startWordPosition="1198" endWordPosition="1201">slations. Algorithm 1 AL-SMT 1: Given bilingual corpus L, and monolingual corpus U. 2: MF,E = train(L, ∅) 3: for t = 1, 2,... do 4: U+ = translate(U, MF,E) 5: Select k sentence pairs from U+, and ask a human for their true translations. 6: Remove the k sentences from U, and add the k sentence pairs (translated by human) to L 7: MF,E = train(L, U+) 8: Monitor the performance on the test set T 9: end for Phrase tables from U+ will get a 0 score in minimum error rate training if they are not useful, so our method is more general. Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext L U U+. The setup in Algorithm 1 helps us to investigate how to maximally take advantage of human effort (for sentence translation) when learning an SMT model from the available data, that includes bilingual and monolingual text. 3 Sentence Selection Strategies Our sentence selection strategies can be divided into two categories: (1) those which are independent of the target language and just look into the source language, and (2) those which als</context>
<context position="11972" citStr="Ueffing et al., 2007" startWordPosition="2036" endWordPosition="2039">D. The weights wn adjust the importance of the scores of n-grams with different lengths. In addition to taking geometric average, we also consider the arithmetic average: P(x|U, n) (4) P(x |L, n) As a special case when N = 1, the score motivates selecting sentences which increase the number of unique words with new words appearing with higher frequency in U than L. 3.2 Similarity to the Bilingual Training Data (Similarity) The simplest way to expand the lexicon set is to choose sentences from U which are as dissimilar as possible to L. We measure the similarity using weighted n-gram coverage (Ueffing et al., 2007b). 3.3 Confidence of Translations (Confidence) The decoder produces an output translation e using the probability p(e |f). This probability can be P(x |U)1 1 P(x |L) J |Xps |(1) 1 � Opa(s) := |Xps | x∈Xps ONg (s) := N wn � P(x|U, n) n=1 |Xns |log (3) x∈Xns P(x |L, n) ONa (s) := N wn � E |Xns |x∈Xns n=1 417 treated as a confidence score for the translation. To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al., 2007b). 3.4 Feature Combination (Combined) The idea is to take into account the information from several s</context>
<context position="17922" citStr="Ueffing et al., 2007" startWordPosition="3100" endWordPosition="3103">:= BestLeaf(T, Z) 14: Monitor the performance on the test set 15: end for H22 H21 Figure 1: Adaptively sampling the sentences while constructing a hierarchical clustering of U. the translation back to French using ME→F. Denote this reconstructed version of the original French sentence by ˜f. Comparing f with f˜ using BLEU (or other measures) can tell us how much information has been lost due to our direct and/or reverse translation systems. The sentences with higher information loss are selected for translation by a human. 4 Experiments The SMT system we applied in our experiments is PORTAGE (Ueffing et al., 2007a). The models (or features) which are employed by the decoder are: (a) one or several phrase table(s), which model the translation direction p(f |e), (b) one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the experiments reported here, we used 4-gram models on the NIST data, and a trigram model on EuroParl, (c) a distortion corpus language use sentences EuroParl Fr,Ge,Sp in-dom L 5K in-dom U 20K in-dom dev 2K in-dom test 2K See Sec. 4.2 Bangla in-dom L 11K in-dom U 20K in-dom dev 450 in-dom test 1K Hansards Fr out-dom L 5K Table 1: Specification of diff</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007b. Transductive learning for statistical machine translation. In proceedings of Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>