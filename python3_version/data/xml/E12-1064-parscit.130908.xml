<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.986096">
Towards a model of formal and informal address in English
</title>
<author confidence="0.980432">
Manaal Faruqui
</author>
<affiliation confidence="0.944054333333333">
Computer Science and Engineering
Indian Institute of Technology
Kharagpur, India
</affiliation>
<email confidence="0.987919">
manaalfar@gmail.com
</email>
<author confidence="0.989448">
Sebastian Padó
</author>
<affiliation confidence="0.991897">
Institute of Computational Linguistics
Heidelberg University
</affiliation>
<address confidence="0.801736">
Heidelberg, Germany
</address>
<email confidence="0.997714">
pado@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999673833333333">
Informal and formal (“T/V”) address in dia-
logue is not distinguished overtly in mod-
ern English, e.g. by pronoun choice like
in many other languages such as French
(“tu”/“vous”). Our study investigates the
status of the T/V distinction in English liter-
ary texts. Our main findings are: (a) human
raters can label monolingual English utter-
ances as T or V fairly well, given sufficient
context; (b), a bilingual corpus can be ex-
ploited to induce a supervised classifier for
T/V without human annotation. It assigns
T/V at sentence level with up to 68% accu-
racy, relying mainly on lexical features; (c),
there is a marked asymmetry between lex-
ical features for formal speech (which are
conventionalized and therefore general) and
informal speech (which are text-specific).
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988624">
In many Indo-European languages, there are two
pronouns corresponding to the English you. This
distinction is generally referred to as the T/V di-
chotomy, from the Latin pronouns tu (informal, T)
and vos (formal, V) (Brown and Gilman, 1960).
The V form (such as Sie in German and Vous in
French) can express neutrality or polite distance
and is used to address social superiors. The T
form (German du, French tu) is employed towards
friends or addressees of lower social standing, and
implies solidarity or lack of formality.
English used to have a T/V distinction until the
18th century, using you as V pronoun and thou
for T. However, in contemporary English, you has
taken over both uses, and the T/V distinction is not
marked anymore. In NLP, this makes generation
in English and translation into English easy. Con-
versely, many NLP tasks suffer from the lack of
information about formality, e.g. the extraction of
social relationships or, notably, machine transla-
tion from English into languages with a T/V dis-
tinction which involves a pronoun choice.
In this paper, we investigate the possibility to
recover the T/V distinction for (monolingual) sen-
tences of 19th and 20th-century English such as:
</bodyText>
<listItem confidence="0.999353">
(1) Can I help you, Sir? (V)
(2) You are my best friend! (T)
</listItem>
<bodyText confidence="0.999945818181818">
After describing the creation of an English corpus
of T/V labels via annotation projection (Section 3),
we present an annotation study (Section 4) which
establishes that taggers can indeed assign T/V la-
bels to monolingual English utterances in context
fairly reliably. Section 5 investigates how T/V is
expressed in English texts by experimenting with
different types of features, including words, seman-
tic classes, and expressions based on Politeness
Theory. We find word features to be most reliable,
obtaining an accuracy of close to 70%.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999972125">
There is a large body of work on the T/V distinc-
tion in (socio-)linguistics and translation studies,
covering in particular the conditions governing
T/V usage in different languages (Kretzenbacher
et al., 2006; Schüpbach et al., 2006) and the diffi-
culties in translation (Ardila, 2003; Künzli, 2010).
However, many observations from this literature
are difficult to operationalize. Brown and Levin-
son (1987) propose a general theory of politeness
which makes many detailed predictions. They as-
sume that the pragmatic goal of being polite gives
rise to general communication strategies, such as
avoiding to lose face (cf. Section 5.2).
In computational linguistics, it is a common
observation that for almost every language pair,
there are distinctions that are expressed overtly
</bodyText>
<page confidence="0.988502">
623
</page>
<note confidence="0.998363">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.976397">
Figure 1: T/V label induction for English sentences in
a parallel corpus with annotation projection
</figureCaption>
<bodyText confidence="0.9999718">
in one language, but remain covert in the other.
Examples include morphology (Fraser, 2009) and
tense (Schiehlen, 1998). A technique that is often
applied in such cases is annotation projection, the
use of parallel corpora to copy information from a
language where it is overtly realized to one where
it is not (Yarowsky and Ngai, 2001; Hwa et al.,
2005; Bentivogli and Pianta, 2005).
The phenomenon of formal and informal ad-
dress has been considered in the contexts of transla-
tion into (Hobbs and Kameyama, 1990; Kanayama,
2003) and generation in Japanese (Bateman, 1988).
Li and Yarowsky (2008) learn pairs of formal and
informal constructions in Chinese with a para-
phrase mining strategy. Other relevant recent stud-
ies consider the extraction of social networks from
corpora (Elson et al., 2010). A related study is
(Bramsen et al., 2011) which considers another
sociolinguistic distinction, classifying utterances
as “upspeak” and “downspeak” based on the social
relationship between speaker and addressee.
This paper extends a previous pilot study
(Faruqui and Padó, 2011). It presents more an-
notation, investigates a larger and better motivated
feature set, and discusses the findings in detail.
</bodyText>
<sectionHeader confidence="0.995885" genericHeader="method">
3 A Parallel Corpus of Literary Texts
</sectionHeader>
<bodyText confidence="0.9998245">
This section discusses the construction of T/V gold
standard labels for English sentences. We obtain
these labels from a parallel English–German cor-
pus using the technique of annotation projection
(Yarowsky and Ngai, 2001) sketched in Figure 1:
We first identify the T/V status of German pro-
nouns, then copy this T/V information onto the
corresponding English sentence.
</bodyText>
<subsectionHeader confidence="0.999476">
3.1 Data Selection and Preparation
</subsectionHeader>
<bodyText confidence="0.999823114285715">
Annotation projection requires a parallel corpus.
We found commonly used parallel corpora like EU-
ROPARL (Koehn, 2005) or the JRC Acquis corpus
(Steinberger et al., 2006) to be unsuitable for our
study since they either contain almost no direct
address at all or, if they do, just formal address (V).
Fortunately, for many literary texts from the 19th
and early 20th century, copyright has expired, and
they are freely available in several languages.
We identified 110 stories and novels among the
texts provided by Project Gutenberg (English) and
Project Gutenberg-DE (German)1 that were avail-
able in both languages, with a total of 0.5M sen-
tences per language. Examples are Dickens’ David
Copperfield or Tolstoy’s Anna Karenina. We ex-
cluded plays and poems, as well as 19th-century
adventure novels by Sir Walter Scott and James F.
Cooper which use anachronistic English for stylis-
tic reasons, including words that previously (until
the 16th century) indicated T (“thee”, “didst”).
We cleaned the English and German novels man-
ually by deleting the tables of contents, prologues,
epilogues, as well as chapter numbers and titles
occurring at the beginning of each chapter to ob-
tain properly parallel texts. The files were then
formatted to contain one sentence per line using
the sentence splitter and tokenizer provided with
EUROPARL (Koehn, 2005). Blank lines were
inserted to preserve paragraph boundaries. All
novels were lemmatized and POS-tagged using
TreeTagger (Schmid, 1994).2 Finally, they were
sentence-aligned using Gargantuan (Braune and
Fraser, 2010), an aligner that supports one-to-many
alignments, and word-aligned in both directions
using Giza++ (Och and Ney, 2003).
</bodyText>
<subsectionHeader confidence="0.999114">
3.2 T/V Gold Labels for English Utterances
</subsectionHeader>
<bodyText confidence="0.976366571428571">
As Figure 1 shows, the automatic construction of
T/V labels for English involves two steps.
Step 1: Labeling German Pronouns as T/V.
German has three relevant personal pronouns for
the T/V distinction: du (T), sie (V), and ihr (T/V).
However, various ambiguities makes their interpre-
tation non-straightforward.
The pronoun ihr can both be used for plural T
address or for a somewhat archaic singular or plu-
ral V address. In principle, these usages should
be distinguished by capitalization (V pronouns
are generally capitalized in German), but many
T instances in our corpora informal use are nev-
ertheless capitalized. Additional, ihr can be the
</bodyText>
<footnote confidence="0.991661666666667">
1http://www.gutenberg.org, http://gutenberg.spiegel.de/
2It must be expected that the tagger degrades on this
dataset; however we did not quantify this effect.
</footnote>
<table confidence="0.364658166666667">
Step 1: German pronoun Step 2: copy T/V class
provides overt T/V label label to English sentence
V projection
Darf ich Sie etwas fragen?
Please permit me to ask
you a question.
</table>
<page confidence="0.997605">
624
</page>
<bodyText confidence="0.992951875">
dative form of the 3rd person feminine pronoun sie
(she/her). These instances are neutral with respect
to T/V but were misanalysed by TreeTagger as in-
stances of the T/V lemma ihr. Since TreeTagger
does not provide person information, and we did
not want to use a full parser, we decided to omit
ihr/Ihr from consideration.3
Of the two remaining pronouns (du and sie), du
expresses (singular) T. A minor problem is pre-
sented by novels set in France, where du is used as
an nobiliary particle. These instances can be recog-
nised reliably since the names before and after du
are generally unknown to the German tagger. Thus
we do not interpret du as T if the word preceding
or succeeding it has “unknown” as its lemma.
The V pronoun, sie, doubles as the pronoun for
third person (she/they) when not capitalized. We
therefore interpret only capitalized instances of Sie
as V. Furthermore, we ignore utterance-initial po-
sitions, where all words are capitalized. This is
defined as tokens directly after a sentence bound-
ary (POS $.) or after a bracket (POS $().
These rules concentrate on precision rather than
recall. They leave many instances of German sec-
ond person pronouns unlabeled; however, this is
not a problem since we do not currently aim at
obtaining complete coverage on the English side
of our parallel corpus. From the 0.5M German sen-
tences, about 14% of the sentences were labeled
as T or V (37K for V and 28K for T). In a random
sample of roughly 300 German sentences which
we analysed, we did not find any errors. This puts
the precision of our heuristics at above 99%.
Step 2: Annotation Projection. We now copy
the information over onto the English side. We
originally intended to transfer T/V labels between
German and English word-aligned pronouns. How-
ever, we pronouns are not necessarily translated
into pronouns; additionally, we found word align-
ment accuracy for pronouns to be far from perfect,
due to the variability in function word translation.
For these reason, we decided to look at T/V labels
at the level of complete sentences, ignoring word
alignment. This is generally unproblematic – ad-
dress is almost always consistent within sentences:
of the 65K German sentences with T or V labels,
only 269 (&lt; 0.5%) contain both T and V. Our pro-
jection on the English side results in 25K V and
</bodyText>
<footnote confidence="0.8062905">
3Instances of ihr as possessive pronoun occurred as well,
but could be filtered out on the basis of the POS tag.
</footnote>
<table confidence="0.990944">
Comparison No context In context
A1 vs. A2 75% (.49) 79% (.58)
A1 vs. GS 60% (.20) 70% (.40)
A2 vs. GS 65% (.30) 76% (.52)
(A1 n A2) vs. GS 67% (.34) 79% (.58)
</table>
<tableCaption confidence="0.97555175">
Table 1: Manual annotation for T/V on a 200-sentence
sample. Comparison among human annotators (A1 and
A2) and to projected gold standard (GS). All cells show
raw agreement and Cohen’s r. (in parentheses).
</tableCaption>
<bodyText confidence="0.991489222222222">
18K T sentences4, of which 255 (0.6%) are labeled
as both T and V. We exclude these sentences.
Note that this strategy relies on the direct cor-
respondence assumption (Hwa et al., 2005), that
is, it assumes that the T/V status of an utterance is
not changed in translation. We believe that this is
a reasonable assumption, given that T/V is deter-
mined by the social relation between interlocutors;
but see Section 4 for discussion.
</bodyText>
<subsectionHeader confidence="0.999489">
3.3 Data Splitting
</subsectionHeader>
<bodyText confidence="0.999956333333333">
Finally, we divided our English data into train-
ing, development and test sets with 74 novels
(26K sentences), 19 novels (9K sentences) and
13 novels (8K sentences), respectively. The cor-
pus is available for download at http://www.
nlpado.de/~sebastian/data.shtml.
</bodyText>
<sectionHeader confidence="0.972007" genericHeader="method">
4 Human Annotation of T/V for English
</sectionHeader>
<bodyText confidence="0.999950736842105">
This section investigates how well the T/V distinc-
tion can be made in English by human raters, and
on the basis of what information. Two annotators
with near native-speaker competence in English
were asked to label 200 random sentences from
the training set as T or V. Sentences were first pre-
sented in isolation (“no context”). Subsequently,
they were presented with three sentences pre- and
post-context each (“in context”).
Table 1 shows the results of the annotation
study. The first line compares the annotations
of the two annotators against each other (inter-
annotator agreement). The next two lines compare
the taggers’ annotations against the gold standard
labels projected from German (GS). The last line
compares the annotator-assigned labels to the GS
for the instances on which the annotators agree.
For all cases, we report raw accuracy and Co-
hen’s r. (1960), i.e. chance-corrected agreement.
</bodyText>
<footnote confidence="0.985755">
4Our sentence aligner supports one-to-many alignments
and often aligns single German to multiple English sentences.
</footnote>
<page confidence="0.997997">
625
</page>
<bodyText confidence="0.999907055555556">
We first observe that the T/V distinction is con-
siderably more difficult to make for individual
sentences (no context) than when the discourse is
available. In context, inter-annotator agreement in-
creases from 75% to 79%, and agreement with the
gold standard rises by 10%. It is notable that the
two annotators agree worse with one another than
with the gold standard (see below for discussion).
On those instances where they agree, Cohen’s r.
reaches 0.58 in context, which is interpreted as
approaching good agreement (Fleiss, 1981). Al-
though far from perfect, this inter-annotator agree-
ment is comparable to results for the annotation
of fine-grained word sense or sentiment (Navigli,
2009; Bermingham and Smeaton, 2009).
An analysis of disagreements showed that many
sentences can be uttered in both T and V contexts
and cannot be labeled without context:
</bodyText>
<listItem confidence="0.990246">
(3) “And perhaps sometime you may see her.”
</listItem>
<bodyText confidence="0.981362487804878">
This case (gold label: V) is disambiguated by the
previous sentence which indicates a hierarchical
social relation between speaker and addressee:
(4) “And she is a sort of relation of your lord-
ship’s,” said Dawson... .
Still, even a three-sentence window is often not
sufficient, since the surrounding sentences may be
just as uninformative. In these cases, more global
information about the situation is necessary. Even
with perfect information, however, judgments can
sometimes deviate, as there are considerable “grey
areas” in T/V usage (Kretzenbacher et al., 2006).
In addition, social rules like T/V usage vary
in time and between countries (SchUpbach et al.,
2006). This helps to explain why annotators agree
better with one another than with the gold standard:
21st century annotators tend to be unfamiliar with
19th century T/V usage. Consider this example
from a book written in second person perspective:
(5) Finally, you acquaint Caroline with the
fatal result: she begins by consoling you.
“One hundred thousand francs lost! We
shall have to practice the strictest econ-
omy”, you imprudently add.5
Here, the author and translator use V to refer to the
reader, while today’s usage would almost certainly
5H. de Balzac: Petty Troubles of Married Life
be T, as presumed by both annotators. Conver-
sations between lovers or family members form
another example, where T is modern usage, but
the novels tend to use V:
(6) [...] she covered her face with the other
to conceal her tears. “Corinne!”, said Os-
wald, “Dear Corinne! My absence has
then rendered you unhappy!”6
In sum, our annotation study establishes that the
T/V distinction, although not realized by different
pronouns in English, can be recovered manually
from text, provided that discourse context is avail-
able. A substantial part of the errors is due to social
changes in T/V usage.
</bodyText>
<sectionHeader confidence="0.996954" genericHeader="method">
5 Monolingual TN Modeling
</sectionHeader>
<bodyText confidence="0.999923846153846">
The second part of the paper explores the auto-
matic prediction of the T/V distinction for English
sentences. Given the ability to create an English
training corpus with T/V labels with the annotation
projection methods described in Section 3.2, we
can phrase T/V prediction for English as a standard
supervised learning task. Our experiments have
a twin motivation: (a), on the NLP side, we are
mainly interested in obtaining a robust classifier
to assign the labels T and V to English sentences;
(b), on the sociolinguistic side, we are interested in
investigating through which features the categories
T and V are expressed in English.
</bodyText>
<subsectionHeader confidence="0.980492">
5.1 Classification Framework
</subsectionHeader>
<bodyText confidence="0.999971625">
We phrase T/V labeling as a binary classification
task at the sentence level, performing the classifica-
tion with L2-regularized logistic regression using
the LibLINEAR library (Fan et al., 2008). Logis-
tic regression defines the probability that a binary
response variable y takes some value as a logit-
transformed linear combination of the features fi,
each of which is assigned a coefficient Oi.
</bodyText>
<equation confidence="0.9238605">
p(y = 1) = 1 + 1 e−z with z =�
i
</equation>
<bodyText confidence="0.9995804">
Regularization incorporates the size of the coef-
ficient vector O into the objective function, sub-
tracting it from the likelihood of the data given the
model. This allows the user to trade faithfulness
to the data against generalization.7
</bodyText>
<footnote confidence="0.807675">
6A.L.G. de Staël: Corinne
7We use LIBLINEAR’s default parameters and set the
cost (regularization) parameter to 0.01.
</footnote>
<equation confidence="0.555946333333333">
Oifi (7)
626
P(C|V )
P(CIT)
Words
Mister, sir, Monsieur, sirrah, ...
</equation>
<bodyText confidence="0.7657625">
indicative for V, ranked by the ratio of probabilities
for T and V, estimated on the training set.
</bodyText>
<table confidence="0.657464">
4.59
2.36
1.60
Mlle., Mr., M., Herr, Dr., ...
Gentlemen, patients, rascals, ...
</table>
<tableCaption confidence="0.923685">
Table 2: 3 of the 400 clustering-based semantic classes
(classes most indicative for V)
</tableCaption>
<subsectionHeader confidence="0.996226">
5.2 Feature Types
</subsectionHeader>
<bodyText confidence="0.999601560606061">
We experiment with three features types that are
candidates to express the T/V English distinction.
Word Features. The intuition to use word fea-
tures draws on the parallel between T/V and infor-
mation retrieval tasks like document classification:
some words are presumably correlated with formal
address (like titles), while others should indicate
informal address (like first names). In a prelimi-
nary experiment, we noticed that in the absence of
further constraints, many of the most indicative fea-
tures are names of persons from particular novels
which are systematically addressed formally (like
Phileas Fogg from J. Vernes’ Around the world in
eighty days) or informally (like Mowgli, Baloo,
and Bagheera from R. Kipling’s Jungle Book).
These features clearly do not generalize to new
books. We therefore added a constraint to remove
all features which did not occur in at least three
novels. To reduce the number of word features to a
reasonable order of magnitude, we also performed
a x2-based feature selection (Manning et al., 2008)
on the training set. Preliminary experiments es-
tablished that selecting the top 800 word features
yielded a model with good generalization.
Semantic Class Features. Our second feature
type is semantic class features. These can be seen
as another strategy to counteract the sparseness
at the level of word features. We cluster words
into 400 semantic classes on the basis of distribu-
tional and morphological similarity features which
are extracted from an unlabeled English collec-
tion of Gutenberg novels comprising more than
100M tokens, using the approach by Clark (2003).
These features measure how similar tokens are to
one another in terms of their occurrences in the
document and are useful in Named Entity Recog-
nition (Finkel and Manning, 2009). As features
in the T/V classification of a given sentence, we
simply count for each class the number of tokens
in this class present in the current sentence. For
illustration, Table 2 shows the three classes most
Politeness Theory Features. The third feature
type is based on the Politeness Theory (Brown
and Levinson, 1987). Brown and Levinson’s pre-
diction is that politeness levels will be detectable
in concrete utterances in a number of ways, e.g.
a higher use of conjunctive or hedges in polite
speech. Formal address (i.e., V as opposed to T) is
one such expression. Politeness Theory therefore
predicts that other politeness indicators should cor-
relate with the T/V classification. This holds in
particular for English, where pronoun choice is
unavailable to indicate politeness.
We constructed 16 features on the basis of Po-
liteness Theory predictions, that is, classes of ex-
pressions indicating either formality or informality.
From a computational perspective, the problem
with Politeness Theory predictions is that they are
only described qualitatively and by example, with-
out detailed lists. For each feature, we manually
identified around 10 words or multi-word relevant
expressions. Table 3 shows these 16 features with
their intended classes and some example expres-
sions. Similar to the semantic class features, the
value of each politeness feature is the sum of the
frequencies of its members in a sentence.
</bodyText>
<subsectionHeader confidence="0.997315">
5.3 Context: Size and Type
</subsectionHeader>
<bodyText confidence="0.999967">
As our annotation study in Section 4 found, con-
text is crucial for human annotators, and this pre-
sumably carries over to automatic methods human
annotators: if the features for a sentence are com-
puted just on that sentence, we will face extremely
sparse data. We experiment with symmetrical win-
dow contexts, varying the size between n = 0 (just
the target sentence) and n = 10 (target sentence
plus 10 preceding and 10 succeeding sentences).
This kind of simple “sentence context” makes an
important oversimplification, however. It lumps to-
gether material from different speech turns as well
as from “narrative” sentences, which may generate
misleading features. For example, narrative sen-
tences may refer to protagonists by their full names
including titles (strong features for V) even when
these protagonists are in T-style conversations:
</bodyText>
<listItem confidence="0.447234333333333">
(8) “You are the love of my life”, said Sir
Phileas Fogg.8 (T)
8J. Verne: Around the world in 80 days
</listItem>
<page confidence="0.926867">
627
</page>
<table confidence="0.999534555555556">
Class Example expressions Class Example expressions
Inclusion (T) let’s, shall we Exclamations (T) hey, yeah
Subjunctive I (T) can, will Subjunctive II (V) could, would
Proximity (T) this, here Distance (V) that, there
Negated question (V) didn’t I, hasn’t it Indirect question (V) would there, is there
Indefinites (V) someone, something Apologizing (V) bother, pardon
Polite adverbs (V) marvellous, superb Optimism (V) I hope, would you
Why + modal (V) why would(n’t) Impersonals (V) necessary, have to
Polite markers (V) please, sorry Hedges (V) in fact, I guess
</table>
<tableCaption confidence="0.99917">
Table 3: 16 Politeness theory-based features with intended classes and example expressions
</tableCaption>
<bodyText confidence="0.962470739130435">
Example (8) also demonstrates that narrative mate-
rial and direct speech may even be mixed within
individual sentences.
For these reasons, we introduce an alternative
concept of context, namely direct speech context,
whose purpose is to exclude narrative material. We
compute direct speech context in two steps: (a),
segmentation of sentences into chunks that are
either completely narrative or speech, and (b), la-
beling of chunks with a classifier that distinguishes
these two classes. The segmentation step (a) takes
place with a regular expression that subdivides sen-
tences on every occurrence of quotes (“ , ” , ’ , ‘,
etc.). As training data for the classification step
(b), we manually tagged 1000 chunks from our
training data as either B-DS (begin direct speech),
I-DS (inside direct speech) and O (outside direct
speech, i.e. narrative material).9 We used this
dataset to train the CRF-based sequence tagger
Mallet (McCallum, 2002) using all tokens, includ-
ing punctuation, as features.10 This tagger is used
to classify all chunks in our dataset, resulting in
output like the following example:
</bodyText>
<listItem confidence="0.832855">
(9) (B-DS) “I am going to see his Ghost!
(I-DS) It will be his Ghost not him!”
(O) Mr. Lorry quietly chafed the
hands that held his arm.11
</listItem>
<bodyText confidence="0.990808428571429">
Direct speech chunks belonging to the same sen-
tence are subsequently recombined.
We define the direct speech context of size n for
a given sentence as the n preceding and following
direct speech chunks that are labeled B-DS or I-DS
while skipping any chunks labeled O. Note that
this definition of direct speech context still lumps
</bodyText>
<footnote confidence="0.9741566">
9The labels are chosen after IOB notation conventions
(Ramshaw and Marcus, 1995).
10We also experimented with rule-based chunk labeling
based on quotes, but found the use of quotes too inconsistent.
11C. Dickens: A tale of two cities.
</footnote>
<figure confidence="0.476147">
Context size (n)
</figure>
<figureCaption confidence="0.997528666666667">
Figure 2: Accuracy vs. number of sentences in context
(empty circles: sentence context; solid circles: direct
speech context)
</figureCaption>
<bodyText confidence="0.99994075">
together utterances by different speakers and can
therefore yield misleading features in the case of
asymmetric conversational situations, in addition
to possible direct speech misclassifications.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="method">
6 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999109">
6.1 Evaluation on the Development Set
</subsectionHeader>
<bodyText confidence="0.921088777777778">
We first perform model selection on the develop-
ment set and then validate our results on the test
set (cf. Section 3.3).
Influence of Context. Figure 2 shows the influ-
ence of size and type of context, using only words
as features. Without context, we obtain a perfor-
mance of 61.1% (sentence context) and of 62.9%
(direct speech context). These numbers beat the
random baseline (50.0%) and the frequency base-
</bodyText>
<figureCaption confidence="0.80961225">
line (59.1%). The addition of more context further
improves performance substantially for both con-
text types. The ideal context size is fairly large,
namely 7 sentences and 8 direct speech chunks, re-
</figureCaption>
<figure confidence="0.999683166666667">
0 2 4 6 8 10
Accuracy (%)
61 62 63 64 65 66 67
●
●● ●
●
●
●
●
●
●
● ● ● ●
● ● ●
●
●
●
●
●
</figure>
<page confidence="0.984679">
628
</page>
<table confidence="0.999011636363636">
Model Accuracy
Random Baseline 50.0
Frequency Baseline 59.1
Words 67.0**
SemClass 57.5
PoliteClass 59.6
Words + SemClass 66.6**
Words + PoliteClass 66.4**
Words + PoliteClass + SemClass 66.2**
Raw human IAA (no context) 75.0
Raw human IAA (in context) 79.0
</table>
<tableCaption confidence="0.994267333333333">
Table 4: T/V classification accuracy on the develop-
ment set (direct speech context, size 8). **: Significant
difference to frequency baseline (p&lt;0.01)
</tableCaption>
<bodyText confidence="0.999509419354839">
spectively. This indicates that sparseness is indeed
a major challenge, and context can become large
before the effects mentioned in Section 5.3 counter-
act the positive effect of more data. Direct speech
context outperforms sentence context throughout,
with a maximum accuracy of 67.0% as compared
to 65.2%, even though it shows higher variation,
which we attribute to the less stable nature of the
direct speech chunks and their automatically cre-
ated labels. From now on, we adopt a direct speech
context of size 8 unless specified differently.
Influence of Features. Table 4 shows the results
for different feature types. The best model (word
features only) is highly significantly better than
the frequency baseline (which it beats by 8%) as
determined by a bootstrap resampling test (Noreen,
1989). It gains 17% over the random baseline,
but is still more than 10% below inter-annotator
agreement in context, which is often seen as an
upper bound for automatic models.
Disappointingly, the comparison of the feature
groups yields a null result: We are not able to
improve over the results for just word features with
either the semantic class or the politeness features.
Neither feature type outperforms the frequency
baseline significantly (p&gt;0.05). Combinations of
the different feature types also do worse than just
words. The differences between the best model
(just words) and the combination models are all
not significant (p&gt;0.05). These negative results
warrant further analysis. It follows in Section 6.3.
</bodyText>
<subsectionHeader confidence="0.983166">
6.2 Results on the Test Set
</subsectionHeader>
<bodyText confidence="0.998969">
Table 5 shows the results of evaluating models
with the best feature set and with different context
sizes on the test set, in order to verify that we did
</bodyText>
<figure confidence="0.9955135">
Model Accuracy A to dev set
Frequency baseline 59.3 + 0.2
62.5 - 0.4
67.3 + 1.0
67.5 + 0.5
66.8 + 1.0
</figure>
<tableCaption confidence="0.9604765">
Table 5: T/V classification accuracy on the test set and
differences to dev set results (direct speech context)
</tableCaption>
<bodyText confidence="0.999835333333333">
not overfit on the development set when picking
the best model. The tendencies correspond well
to the development set: the frequency baseline is
almost identical, as are the results for the different
models. The differences to the development set
are all equal to or smaller than 1% accuracy, and
the best result at 67.5% is 0.5% better than on the
development set. This is a reassuring result, as our
model appears to generalize well to unseen data.
</bodyText>
<subsectionHeader confidence="0.999615">
6.3 Analysis by Feature Types
</subsectionHeader>
<bodyText confidence="0.999973791666667">
The results from Section 6.1 motivate further anal-
ysis of the individual feature types.
Analysis of Word Features. Word features are
by far the most effective features. Table 6 lists
the top twenty words indicating T and V (ranked
by the ratio of probabilities for the two classes
on the training set). The list still includes some
proper names like Vrazumihin or Louis-Gaston
(even though all features have to occur in at least
three novels), but they are relatively infrequent.
The most prominent indicators for the formal class
V are titles (monsieur, (ma)’am) and instances of
formulaic language (Permit (me), Excuse (me)).
There are also some terms which are not straight-
forward indicators of formal address (angelic, stub-
bornness), but are associated with a high register.
There is a notable asymmetry between T and
V. The word features for T are considerably more
difficult to interpret. We find some forms of earlier
period English (thee, hast, thou, wilt) that result
from occasional archaic passages in the novels as
well first names (Louis-Gaston, Justine). Never-
theless, most features are not straightforward to
connect to specifically informal speech.
</bodyText>
<subsectionHeader confidence="0.794648">
Analysis of Semantic Class Features. We
</subsectionHeader>
<bodyText confidence="0.9996522">
ranked the semantic classes we obtained by distri-
butional clustering in a similar manner to the word
features. Table 2 shows the top three classes in-
dicative for V. Almost all others of the 400 clusters
do not have a strong formal/informal association
</bodyText>
<table confidence="0.966134392857143">
Words (no context)
Words (context size 6)
Words (context size 8)
Words (context size 10)
629
Top 20 words for V Top 20 words for T
Word w P(w|V ) Word w P(wIT)
P(wIT) P(wIV)
Excuse 36.5 thee 94.3
Permit 35.0 amenable 94.3
’ai 29.2 stuttering 94.3
’am 29.2 guardian 94.3
stubbornness 29.2 hast 92.0
flights 29.2 Louis-Gaston 92.0
monsieur 28.6 lease-making 92.0
Vrazumihin 28.6 melancholic 92.0
mademoiselle 26.5 ferry-boat 92.0
angelic 26.5 Justine 92.0
Allow 24.5 Thou 66.0
madame 21.2 responsibility 63.8
delicacies 21.2 thou 63.8
entrapped 21.2 Iddibal 63.8
lack-a-day 21.2 twenty-fifth 63.8
ma 21.0 Chic 63.8
duke 18.0 allegiance 63.8
policeman 18.0 Jouy 63.8
free-will 18.0 wilt 47.0
Canon 18.0 shall 47.0
</table>
<tableCaption confidence="0.998762">
Table 6: Most indicative word features for T or V
</tableCaption>
<bodyText confidence="0.999955980392157">
but mix formal, informal, and neutral vocabulary.
This tendency is already apparent in class 3: Gen-
tlemen is clearly formal, while rascals is informal.
patients can belong to either class. Even in class
1, we find Sirrah, a contemptuous term used in ad-
dressing a man or boy with a low formality score
(p(w|V )/p(w|T) = 0.22). From cluster 4 onward,
none of the clusters is strongly associated with ei-
ther V or T (p(c|V )/p(c|T) ≈ 1).
Our interpretation of these observations is that
in contrast to text categorization, there is no clear-
cut topical or domain difference between T and V:
both categories co-occur with words from almost
any domain. In consequence, semantic classes do
not, in general, represent strong unambiguous indi-
cators. Similar to the word features, the situation
is worse for T than for V: there still are reasonably
strong features for V, the “marked” case, but it is
more difficult to find indicators for T.
Analysis of politeness features. A major reason
for the ineffectiveness of the Politeness Theory-
based features seems to be their low frequency:
in the best model, with a direct speech context of
size 8, only an average of 7 politeness features
was active for any given sentence. However, fre-
quency was not the only problem – the politeness
features were generally unable to discriminate well
between T and V. For all features, the values of
p(f|V )/p(f|T) are between 0.9 and 1.3, that is,
the features were only weakly indicative of one of
the classes. Furthermore, not all features turned
out to be indicative of the class we designed them
for. The best indicator for V was the Indefinites
feature (somehow, someone cf. Table 3), as ex-
pected. In contrast, the best indicator for T was the
Negation question feature which was supposedly
an indicator for V (didn’t I, haven’t we).
A majority of politeness features (13 of the 16)
had p(f|V )/p(f|T) values above 1, that is, were
indicative for the class V. Thus for this feature type,
like for the others, it appears to be more difficult to
identify T than to identify V. This negative result
can be attributed at least in part to our method of
hand-crafting lists of expressions for these features.
The inadvertent inclusion of overly general terms
V might be responsible for the features’ inability
to discriminate well, while we have presumably
missed specific terms which has hurt coverage.
This situation may in the future be remedied with
the semi-automatic acquisition of instantiations of
politeness features.
</bodyText>
<subsectionHeader confidence="0.999917">
6.4 Analysis of Individual Novels
</subsectionHeader>
<bodyText confidence="0.999971230769231">
One possible hypothesis regarding the difficulty
of finding indicators for the class T is that indi-
cators for T tend to be more novel-specific than
indicators for V, since formal language is more
conventionalized (Brown and Levinson, 1987). If
this were the case, then our strategy of building
well-generalizing models by combining text from
different novels would naturally result in models
that have problems with picking up T features.
To investigate this hypothesis, we trained mod-
els with the best parameters as before (8-sentence
direct speech context, words as features). How-
ever, this time we trained novel-specific models,
splitting each novel into 50% training data and
50% testing data. We required novels to contain
more than 200 labeled sentences. This ruled out
most short stories, leaving us with 7 novels in the
test set. The results are shown in Table 7 and show
a clear improvement. The accuracy is 13% higher
than in our main experiment (67% vs. 80%), even
though the models were trained on considerably
less data. Six of the seven novels perform above
the 67.5% result from the main experiment.
The top-ranked features for T and V show a
much higher percentage of names for both T and
V than in the main experiment. This is to be ex-
</bodyText>
<page confidence="0.992128">
630
</page>
<table confidence="0.999566555555556">
Novel Accuracy
H. Beecher-Stove: Uncle Tom’s Cabin 90.0
J. Spyri: Cornelli 88.3
E. Zola: Lourdes 83.9
H. de Balzac: Cousin Pons 82.3
C. Dickens: The Pickwick Papers 77.7
C. Dickens: Nicholas Nickleby 74.8
F. Hodgson Burnett: Little Lord 61.6
All (micro average) 80.0
</table>
<tableCaption confidence="0.9684735">
Table 7: T/V prediction models for individual novels
(50% of each novel for training and 50% testing)
</tableCaption>
<bodyText confidence="0.99995">
pected, since this experiment does not restrict itself
to features that occurred in at least three novels.
The price we pay for this is worse generalization to
other novels. There is also still a T/V asymmetry:
more top features are shared among the V lists of
individual novels and with the main experiment
V list than on the T side. Like in the main exper-
iment (cf. Section 6.3), V features indicate titles
and other features of elevated speech, while T fea-
tures mostly refer to novel-specific protagonists
and events. In sum, these results provide evidence
for a difference in status of T and V.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="discussions">
7 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999928223684211">
In this paper, we have studied the distinction
between formal and information (T/V) address,
which is not expressed overtly through pronoun
choice or morphosyntactic marking in modern En-
glish. Our hypothesis was that the T/V distinction
can be recovered in English nevertheless. Our man-
ual annotation study has shown that annotators can
in fact tag monolingual English sentences as T or
V with reasonable accuracy, but only if they have
sufficient context. We exploited the overt informa-
tion from German pronouns to induce T/V labels
for English and used this labeled corpus to train a
monolingual T/V classifier for English. We exper-
imented with features based on words, semantic
classes, and Politeness Theory predictions.
With regard to our NLP goal of building a T/V
classifier, we conclude that T/V classification is
a phenomenon that can be modelled on the basis
of corpus features. A major factor in classifica-
tion performance is the inclusion of a wide context
to counteract sparse data, and more sophisticated
context definitions improve results. We currently
achieve top accuracies of 67%-68%, which still
leave room for improvement. We next plan to
couple our T/V classifier with a machine trans-
lation system for a task-based evaluation on the
translation of direct address into German and other
languages with different T/V pronouns.
Considering our sociolinguistic goal of deter-
mining the ways in which English realizes the T/V
distinction, we first obtained a negative result: only
word features perform well, while semantic classes
and politeness features do hardly better than a fre-
quency baseline. Notably, there are no clear “topi-
cal” divisions between T and V, like for example
in text categorization: almost all words are very
weakly correlated with either class, and seman-
tically similar words can co-occur with different
classes. Consequently, distributionally determined
semantic classes are not helpful for the distinction.
Politeness features are difficult to operationalize
with sufficiently high precision and recall.
An interesting result is the asymmetry between
the linguistic features for V and T at the lexical
level. V language appears to be more convention-
alized; the models therefore identified formulaic
expressions and titles as indicators for V. On the
other hand, very few such generic features exist for
the class T; consequently, the classifier has a hard
time learning good discriminating and yet generic
features. Those features that are indicative of T,
such as first names, are highly novel-specific and
were deliberately excluded from the main exper-
iment. When we switched to individual novels,
the models picked up such features, and accuracy
increased – at the cost of lower generalizability
between novels. A more technical solution to this
problem would be the training of a single-class
classifier for V, treating T as the “default” class
(Tax and Duin, 1999).
Finally, an error analysis showed that many er-
rors arise from sentences that are too short or un-
specific to determine T or V reliably. This points
to the fact that T/V should not be modelled as a
sentence-level classification task in the first place:
T/V is not a choice made for each sentence, but
one that is determined once for each pair of inter-
locutors and rarely changed. In future work, we
will attempt to learn social networks from novels
(Elson et al., 2010), which should provide con-
straints on all instances of communication between
a speaker and an addressee. However, the big – and
unsolved, as far as we know – challenge is to au-
tomatically assign turns to interlocutors, given the
varied and often inconsistent presentation of direct
speech turns in novels.
</bodyText>
<page confidence="0.998171">
631
</page>
<sectionHeader confidence="0.996274" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833125">
John Ardila. 2003. (Non-Deictic, Socio-Expressive)
T-/V-Pronoun Distinction in Spanish/English Formal
Locutionary Acts. Forum for Modern Language
Studies, 39(1):74–86.
John A. Bateman. 1988. Aspects of clause politeness in
Japanese: An extended inquiry semantics treatment.
In Proceedings of ACL, pages 147–154, Buffalo,
New York.
Luisa Bentivogli and Emanuele Pianta. 2005. Ex-
ploiting parallel texts in the creation of multilingual
semantically annotated resources: the MultiSemCor
Corpus. Journal of Natural Language Engineering,
11(3):247–261.
Adam Bermingham and Alan F. Smeaton. 2009. A
study of inter-annotator agreement for opinion re-
trieval. In Proceedings of ACM SIGIR, pages 784–
785.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of ACL/HLT, pages 773–782, Portland, OR.
Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for symmet-
rical and asymmetrical parallel corpora. In Coling
2010: Posters, pages 81–89, Beijing, China.
Roger Brown and Albert Gilman. 1960. The pronouns
of power and solidarity. In Thomas A. Sebeok, edi-
tor, Style in Language, pages 253–277. MIT Press,
Cambridge, MA.
Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some Universals in Language Usage. Num-
ber 4 in Studies in Interactional Sociolinguistics.
Cambridge University Press.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL, pages 59–66, Bu-
dapest, Hungary.
J. Cohen. 1960. A Coefficient of Agreement for Nomi-
nal Scales. Educational and Psychological Measure-
ment, 20(1):37–46.
David Elson, Nicholas Dames, and Kathleen McKe-
own. 2010. Extracting social networks from literary
fiction. In Proceedings ofACL, pages 138–147, Up-
psala, Sweden.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Manaal Faruqui and Sebastian Padó. 2011. “I Thou
Thee, Thou Traitor”: Predicting formal vs. infor-
mal address in English literature. In Proceedings of
ACL/HLT 2011, pages 467–472, Portland, OR.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Nested named entity recognition. In Proceedings of
EMNLP, pages 141–150, Singapore.
Joseph L. Fleiss. 1981. Statistical methods for rates
and proportions. John Wiley, New York, 2nd edi-
tion.
Alexander Fraser. 2009. Experiments in morphosyn-
tactic processing for translating to and from German.
In Proceedings of the EACL MT workshop, pages
115–119, Athens, Greece.
Jerry Hobbs and Megumi Kameyama. 1990. Trans-
lation by abduction. In Proceedings of COLING,
pages 155–161, Helsinki, Finland.
Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across parallel
texts. Journal of Natural Language Engineering,
11(3):311–325.
Hiroshi Kanayama. 2003. Paraphrasing rules for au-
tomatic evaluation of translation into Japanese. In
Proceedings of the Second International Workshop
on Paraphrasing, pages 88–93, Sapporo, Japan.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit, pages 79–86,
Phuket, Thailand.
Heinz L. Kretzenbacher, Michael Clyne, and Doris
Schüpbach. 2006. Pronominal Address in German:
Rules, Anarchy and Embarrassment Potential. Aus-
tralian Review of Applied Linguistics, 39(2):17.1–
17.18.
Alexander Künzli. 2010. Address pronouns as a prob-
lem in French-Swedish translation and translation
revision. Babel, 55(4):364–380.
Zhifei Li and David Yarowsky. 2008. Mining and
modeling relations between formal and informal Chi-
nese phrases from web corpora. In Proceedings of
EMNLP, pages 1031–1040, Honolulu, Hawaii.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
UK, 1st edition.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Roberto Navigli. 2009. Word Sense Disambiguation:
a survey. ACM Computing Surveys, 41(2):1–69.
Eric W. Noreen. 1989. Computer-intensive Methods
for Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In Proceed-
ing of the 3rd ACL Workshop on Very Large Corpora,
Cambridge, MA.
Michael Schiehlen. 1998. Learning tense transla-
tion from bilingual corpora. In Proceedings of
ACL/COLING, pages 1183–1187, Montreal, Canada.
</reference>
<page confidence="0.974135">
632
</page>
<reference confidence="0.999731782608696">
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44–49, Manchester, UK.
Doris Schüpbach, John Hajek, Jane Warren, Michael
Clyne, Heinz Kretzenbacher, and Catrin Norrby.
2006. A cross-linguistic comparison of address pro-
noun use in four European languages: Intralingual
and interlingual dimensions. In Proceedings of the
Annual Meeting of the Australian Linguistic Society,
Brisbane, Australia.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaž Erjavec, and Dan Tufis. 2006.
The JRC-Acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In Proceedings of LREC,
pages 2142–2147, Genoa, Italy.
David M. J. Tax and Robert P. W. Duin. 1999. Sup-
port vector domain description. Pattern Recognition
Letters, 20:1191–1199.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
NAACL, pages 200–207, Pittsburgh, PA.
</reference>
<page confidence="0.999145">
633
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560833">
<title confidence="0.994155">Towards a model of formal and informal address in English</title>
<author confidence="0.673883">Manaal</author>
<affiliation confidence="0.960242333333333">Computer Science and Indian Institute of Kharagpur,</affiliation>
<email confidence="0.999581">manaalfar@gmail.com</email>
<author confidence="0.991702">Sebastian Padó</author>
<affiliation confidence="0.999744">Institute of Computational Linguistics Heidelberg University</affiliation>
<address confidence="0.987029">Heidelberg, Germany</address>
<email confidence="0.999247">pado@cl.uni-heidelberg.de</email>
<abstract confidence="0.998084">Informal and formal (“T/V”) address in dialogue is not distinguished overtly in modern English, e.g. by pronoun choice like in many other languages such as French (“tu”/“vous”). Our study investigates the status of the T/V distinction in English literary texts. Our main findings are: (a) human raters can label monolingual English utterances as T or V fairly well, given sufficient context; (b), a bilingual corpus can be exploited to induce a supervised classifier for T/V without human annotation. It assigns T/V at sentence level with up to 68% accuracy, relying mainly on lexical features; (c), there is a marked asymmetry between lexical features for formal speech (which are conventionalized and therefore general) and informal speech (which are text-specific).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Ardila</author>
</authors>
<title>(Non-Deictic, Socio-Expressive) T-/V-Pronoun Distinction in Spanish/English Formal Locutionary Acts. Forum for Modern Language Studies,</title>
<date>2003</date>
<contexts>
<context position="3193" citStr="Ardila, 2003" startWordPosition="504" endWordPosition="505"> utterances in context fairly reliably. Section 5 investigates how T/V is expressed in English texts by experimenting with different types of features, including words, semantic classes, and expressions based on Politeness Theory. We find word features to be most reliable, obtaining an accuracy of close to 70%. 2 Related Work There is a large body of work on the T/V distinction in (socio-)linguistics and translation studies, covering in particular the conditions governing T/V usage in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and the difficulties in translation (Ardila, 2003; Künzli, 2010). However, many observations from this literature are difficult to operationalize. Brown and Levinson (1987) propose a general theory of politeness which makes many detailed predictions. They assume that the pragmatic goal of being polite gives rise to general communication strategies, such as avoiding to lose face (cf. Section 5.2). In computational linguistics, it is a common observation that for almost every language pair, there are distinctions that are expressed overtly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Lingu</context>
</contexts>
<marker>Ardila, 2003</marker>
<rawString>John Ardila. 2003. (Non-Deictic, Socio-Expressive) T-/V-Pronoun Distinction in Spanish/English Formal Locutionary Acts. Forum for Modern Language Studies, 39(1):74–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bateman</author>
</authors>
<title>Aspects of clause politeness in Japanese: An extended inquiry semantics treatment.</title>
<date>1988</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>147--154</pages>
<location>Buffalo, New York.</location>
<contexts>
<context position="4574" citStr="Bateman, 1988" startWordPosition="717" endWordPosition="718">arallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and disc</context>
</contexts>
<marker>Bateman, 1988</marker>
<rawString>John A. Bateman. 1988. Aspects of clause politeness in Japanese: An extended inquiry semantics treatment. In Proceedings of ACL, pages 147–154, Buffalo, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Emanuele Pianta</author>
</authors>
<title>Exploiting parallel texts in the creation of multilingual semantically annotated resources: the MultiSemCor Corpus.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="4385" citStr="Bentivogli and Pianta, 2005" startWordPosition="685" endWordPosition="688"> Association for Computational Linguistics, pages 623–633, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship bet</context>
</contexts>
<marker>Bentivogli, Pianta, 2005</marker>
<rawString>Luisa Bentivogli and Emanuele Pianta. 2005. Exploiting parallel texts in the creation of multilingual semantically annotated resources: the MultiSemCor Corpus. Journal of Natural Language Engineering, 11(3):247–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan F Smeaton</author>
</authors>
<title>A study of inter-annotator agreement for opinion retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<pages>784--785</pages>
<contexts>
<context position="13654" citStr="Bermingham and Smeaton, 2009" startWordPosition="2186" endWordPosition="2189">l sentences (no context) than when the discourse is available. In context, inter-annotator agreement increases from 75% to 79%, and agreement with the gold standard rises by 10%. It is notable that the two annotators agree worse with one another than with the gold standard (see below for discussion). On those instances where they agree, Cohen’s r. reaches 0.58 in context, which is interpreted as approaching good agreement (Fleiss, 1981). Although far from perfect, this inter-annotator agreement is comparable to results for the annotation of fine-grained word sense or sentiment (Navigli, 2009; Bermingham and Smeaton, 2009). An analysis of disagreements showed that many sentences can be uttered in both T and V contexts and cannot be labeled without context: (3) “And perhaps sometime you may see her.” This case (gold label: V) is disambiguated by the previous sentence which indicates a hierarchical social relation between speaker and addressee: (4) “And she is a sort of relation of your lordship’s,” said Dawson... . Still, even a three-sentence window is often not sufficient, since the surrounding sentences may be just as uninformative. In these cases, more global information about the situation is necessary. Eve</context>
</contexts>
<marker>Bermingham, Smeaton, 2009</marker>
<rawString>Adam Bermingham and Alan F. Smeaton. 2009. A study of inter-annotator agreement for opinion retrieval. In Proceedings of ACM SIGIR, pages 784– 785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Bramsen</author>
<author>Martha Escobar-Molano</author>
<author>Ami Patel</author>
<author>Rafael Alonso</author>
</authors>
<title>Extracting social power relationships from natural language.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL/HLT,</booktitle>
<pages>773--782</pages>
<location>Portland, OR.</location>
<contexts>
<context position="4843" citStr="Bramsen et al., 2011" startWordPosition="759" endWordPosition="762">ra to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yar</context>
</contexts>
<marker>Bramsen, Escobar-Molano, Patel, Alonso, 2011</marker>
<rawString>Philip Bramsen, Martha Escobar-Molano, Ami Patel, and Rafael Alonso. 2011. Extracting social power relationships from natural language. In Proceedings of ACL/HLT, pages 773–782, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Braune</author>
<author>Alexander Fraser</author>
</authors>
<title>Improved unsupervised sentence alignment for symmetrical and asymmetrical parallel corpora.</title>
<date>2010</date>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>81--89</pages>
<location>Beijing, China.</location>
<contexts>
<context position="7211" citStr="Braune and Fraser, 2010" startWordPosition="1121" endWordPosition="1124">th century) indicated T (“thee”, “didst”). We cleaned the English and German novels manually by deleting the tables of contents, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), and ihr (T/V). However, various ambiguities makes their interpretation non-straightforward. The pronoun ihr can both be used for plural T address or for a somewhat archaic singular or plural V address. In principle, t</context>
</contexts>
<marker>Braune, Fraser, 2010</marker>
<rawString>Fabienne Braune and Alexander Fraser. 2010. Improved unsupervised sentence alignment for symmetrical and asymmetrical parallel corpora. In Coling 2010: Posters, pages 81–89, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
<author>Albert Gilman</author>
</authors>
<title>The pronouns of power and solidarity.</title>
<date>1960</date>
<booktitle>Style in Language,</booktitle>
<pages>253--277</pages>
<editor>In Thomas A. Sebeok, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1328" citStr="Brown and Gilman, 1960" startWordPosition="197" endWordPosition="200">ilingual corpus can be exploited to induce a supervised classifier for T/V without human annotation. It assigns T/V at sentence level with up to 68% accuracy, relying mainly on lexical features; (c), there is a marked asymmetry between lexical features for formal speech (which are conventionalized and therefore general) and informal speech (which are text-specific). 1 Introduction In many Indo-European languages, there are two pronouns corresponding to the English you. This distinction is generally referred to as the T/V dichotomy, from the Latin pronouns tu (informal, T) and vos (formal, V) (Brown and Gilman, 1960). The V form (such as Sie in German and Vous in French) can express neutrality or polite distance and is used to address social superiors. The T form (German du, French tu) is employed towards friends or addressees of lower social standing, and implies solidarity or lack of formality. English used to have a T/V distinction until the 18th century, using you as V pronoun and thou for T. However, in contemporary English, you has taken over both uses, and the T/V distinction is not marked anymore. In NLP, this makes generation in English and translation into English easy. Conversely, many NLP task</context>
</contexts>
<marker>Brown, Gilman, 1960</marker>
<rawString>Roger Brown and Albert Gilman. 1960. The pronouns of power and solidarity. In Thomas A. Sebeok, editor, Style in Language, pages 253–277. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Penelope Brown</author>
<author>Stephen C Levinson</author>
</authors>
<title>Politeness: Some Universals in Language Usage.</title>
<date>1987</date>
<journal>Number</journal>
<booktitle>in Studies in Interactional Sociolinguistics.</booktitle>
<volume>4</volume>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3316" citStr="Brown and Levinson (1987)" startWordPosition="518" endWordPosition="522">menting with different types of features, including words, semantic classes, and expressions based on Politeness Theory. We find word features to be most reliable, obtaining an accuracy of close to 70%. 2 Related Work There is a large body of work on the T/V distinction in (socio-)linguistics and translation studies, covering in particular the conditions governing T/V usage in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and the difficulties in translation (Ardila, 2003; Künzli, 2010). However, many observations from this literature are difficult to operationalize. Brown and Levinson (1987) propose a general theory of politeness which makes many detailed predictions. They assume that the pragmatic goal of being polite gives rise to general communication strategies, such as avoiding to lose face (cf. Section 5.2). In computational linguistics, it is a common observation that for almost every language pair, there are distinctions that are expressed overtly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Figure 1: T/V </context>
<context position="19637" citStr="Brown and Levinson, 1987" startWordPosition="3155" endWordPosition="3158"> an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition (Finkel and Manning, 2009). As features in the T/V classification of a given sentence, we simply count for each class the number of tokens in this class present in the current sentence. For illustration, Table 2 shows the three classes most Politeness Theory Features. The third feature type is based on the Politeness Theory (Brown and Levinson, 1987). Brown and Levinson’s prediction is that politeness levels will be detectable in concrete utterances in a number of ways, e.g. a higher use of conjunctive or hedges in polite speech. Formal address (i.e., V as opposed to T) is one such expression. Politeness Theory therefore predicts that other politeness indicators should correlate with the T/V classification. This holds in particular for English, where pronoun choice is unavailable to indicate politeness. We constructed 16 features on the basis of Politeness Theory predictions, that is, classes of expressions indicating either formality or </context>
<context position="33076" citStr="Brown and Levinson, 1987" startWordPosition="5361" endWordPosition="5364">f expressions for these features. The inadvertent inclusion of overly general terms V might be responsible for the features’ inability to discriminate well, while we have presumably missed specific terms which has hurt coverage. This situation may in the future be remedied with the semi-automatic acquisition of instantiations of politeness features. 6.4 Analysis of Individual Novels One possible hypothesis regarding the difficulty of finding indicators for the class T is that indicators for T tend to be more novel-specific than indicators for V, since formal language is more conventionalized (Brown and Levinson, 1987). If this were the case, then our strategy of building well-generalizing models by combining text from different novels would naturally result in models that have problems with picking up T features. To investigate this hypothesis, we trained models with the best parameters as before (8-sentence direct speech context, words as features). However, this time we trained novel-specific models, splitting each novel into 50% training data and 50% testing data. We required novels to contain more than 200 labeled sentences. This ruled out most short stories, leaving us with 7 novels in the test set. T</context>
</contexts>
<marker>Brown, Levinson, 1987</marker>
<rawString>Penelope Brown and Stephen C. Levinson. 1987. Politeness: Some Universals in Language Usage. Number 4 in Studies in Interactional Sociolinguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>59--66</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="19133" citStr="Clark (2003)" startWordPosition="3073" endWordPosition="3074">eature selection (Manning et al., 2008) on the training set. Preliminary experiments established that selecting the top 800 word features yielded a model with good generalization. Semantic Class Features. Our second feature type is semantic class features. These can be seen as another strategy to counteract the sparseness at the level of word features. We cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition (Finkel and Manning, 2009). As features in the T/V classification of a given sentence, we simply count for each class the number of tokens in this class present in the current sentence. For illustration, Table 2 shows the three classes most Politeness Theory Features. The third feature type is based on the Politeness Theory (Brown and Levinson, 1987). Brown and Levinson’s prediction is that politeness levels will be detectable in concrete utter</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of EACL, pages 59–66, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement,</title>
<date>1960</date>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>138--147</pages>
<location>Uppsala,</location>
<contexts>
<context position="4800" citStr="Elson et al., 2010" startWordPosition="751" endWordPosition="754">ion projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using </context>
</contexts>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David Elson, Nicholas Dames, and Kathleen McKeown. 2010. Extracting social networks from literary fiction. In Proceedings ofACL, pages 138–147, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="16575" citStr="Fan et al., 2008" startWordPosition="2657" endWordPosition="2660"> we can phrase T/V prediction for English as a standard supervised learning task. Our experiments have a twin motivation: (a), on the NLP side, we are mainly interested in obtaining a robust classifier to assign the labels T and V to English sentences; (b), on the sociolinguistic side, we are interested in investigating through which features the categories T and V are expressed in English. 5.1 Classification Framework We phrase T/V labeling as a binary classification task at the sentence level, performing the classification with L2-regularized logistic regression using the LibLINEAR library (Fan et al., 2008). Logistic regression defines the probability that a binary response variable y takes some value as a logittransformed linear combination of the features fi, each of which is assigned a coefficient Oi. p(y = 1) = 1 + 1 e−z with z =� i Regularization incorporates the size of the coefficient vector O into the objective function, subtracting it from the likelihood of the data given the model. This allows the user to trade faithfulness to the data against generalization.7 6A.L.G. de Staël: Corinne 7We use LIBLINEAR’s default parameters and set the cost (regularization) parameter to 0.01. Oifi (7) </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Sebastian Padó</author>
</authors>
<title>I Thou Thee, Thou Traitor”: Predicting formal vs. informal address in English literature.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL/HLT 2011,</booktitle>
<pages>467--472</pages>
<location>Portland, OR.</location>
<contexts>
<context position="5079" citStr="Faruqui and Padó, 2011" startWordPosition="790" endWordPosition="793">the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yarowsky and Ngai, 2001) sketched in Figure 1: We first identify the T/V status of German pronouns, then copy this T/V information onto the corresponding English sentence. 3.1 Data Selection and Preparation Annotation projection requires a</context>
</contexts>
<marker>Faruqui, Padó, 2011</marker>
<rawString>Manaal Faruqui and Sebastian Padó. 2011. “I Thou Thee, Thou Traitor”: Predicting formal vs. informal address in English literature. In Proceedings of ACL/HLT 2011, pages 467–472, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Nested named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="19311" citStr="Finkel and Manning, 2009" startWordPosition="3101" endWordPosition="3104">eneralization. Semantic Class Features. Our second feature type is semantic class features. These can be seen as another strategy to counteract the sparseness at the level of word features. We cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure how similar tokens are to one another in terms of their occurrences in the document and are useful in Named Entity Recognition (Finkel and Manning, 2009). As features in the T/V classification of a given sentence, we simply count for each class the number of tokens in this class present in the current sentence. For illustration, Table 2 shows the three classes most Politeness Theory Features. The third feature type is based on the Politeness Theory (Brown and Levinson, 1987). Brown and Levinson’s prediction is that politeness levels will be detectable in concrete utterances in a number of ways, e.g. a higher use of conjunctive or hedges in polite speech. Formal address (i.e., V as opposed to T) is one such expression. Politeness Theory therefo</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Nested named entity recognition. In Proceedings of EMNLP, pages 141–150, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Statistical methods for rates and proportions.</title>
<date>1981</date>
<publisher>John Wiley,</publisher>
<location>New York,</location>
<note>2nd edition.</note>
<contexts>
<context position="13465" citStr="Fleiss, 1981" startWordPosition="2160" endWordPosition="2161">y alignments and often aligns single German to multiple English sentences. 625 We first observe that the T/V distinction is considerably more difficult to make for individual sentences (no context) than when the discourse is available. In context, inter-annotator agreement increases from 75% to 79%, and agreement with the gold standard rises by 10%. It is notable that the two annotators agree worse with one another than with the gold standard (see below for discussion). On those instances where they agree, Cohen’s r. reaches 0.58 in context, which is interpreted as approaching good agreement (Fleiss, 1981). Although far from perfect, this inter-annotator agreement is comparable to results for the annotation of fine-grained word sense or sentiment (Navigli, 2009; Bermingham and Smeaton, 2009). An analysis of disagreements showed that many sentences can be uttered in both T and V contexts and cannot be labeled without context: (3) “And perhaps sometime you may see her.” This case (gold label: V) is disambiguated by the previous sentence which indicates a hierarchical social relation between speaker and addressee: (4) “And she is a sort of relation of your lordship’s,” said Dawson... . Still, even</context>
</contexts>
<marker>Fleiss, 1981</marker>
<rawString>Joseph L. Fleiss. 1981. Statistical methods for rates and proportions. John Wiley, New York, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
</authors>
<title>Experiments in morphosyntactic processing for translating to and from German.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL MT workshop,</booktitle>
<pages>115--119</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="4093" citStr="Fraser, 2009" startWordPosition="637" endWordPosition="638">on strategies, such as avoiding to lose face (cf. Section 5.2). In computational linguistics, it is a common observation that for almost every language pair, there are distinctions that are expressed overtly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy.</context>
</contexts>
<marker>Fraser, 2009</marker>
<rawString>Alexander Fraser. 2009. Experiments in morphosyntactic processing for translating to and from German. In Proceedings of the EACL MT workshop, pages 115–119, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
<author>Megumi Kameyama</author>
</authors>
<title>Translation by abduction.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>155--161</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="4514" citStr="Hobbs and Kameyama, 1990" startWordPosition="707" endWordPosition="710">Linguistics Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, inve</context>
</contexts>
<marker>Hobbs, Kameyama, 1990</marker>
<rawString>Jerry Hobbs and Megumi Kameyama. 1990. Translation by abduction. In Proceedings of COLING, pages 155–161, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philipp Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="4355" citStr="Hwa et al., 2005" startWordPosition="681" endWordPosition="684">ean Chapter of the Association for Computational Linguistics, pages 623–633, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based </context>
<context position="11337" citStr="Hwa et al., 2005" startWordPosition="1822" endWordPosition="1825">ed as well, but could be filtered out on the basis of the POS tag. Comparison No context In context A1 vs. A2 75% (.49) 79% (.58) A1 vs. GS 60% (.20) 70% (.40) A2 vs. GS 65% (.30) 76% (.52) (A1 n A2) vs. GS 67% (.34) 79% (.58) Table 1: Manual annotation for T/V on a 200-sentence sample. Comparison among human annotators (A1 and A2) and to projected gold standard (GS). All cells show raw agreement and Cohen’s r. (in parentheses). 18K T sentences4, of which 255 (0.6%) are labeled as both T and V. We exclude these sentences. Note that this strategy relies on the direct correspondence assumption (Hwa et al., 2005), that is, it assumes that the T/V status of an utterance is not changed in translation. We believe that this is a reasonable assumption, given that T/V is determined by the social relation between interlocutors; but see Section 4 for discussion. 3.3 Data Splitting Finally, we divided our English data into training, development and test sets with 74 novels (26K sentences), 19 novels (9K sentences) and 13 novels (8K sentences), respectively. The corpus is available for download at http://www. nlpado.de/~sebastian/data.shtml. 4 Human Annotation of T/V for English This section investigates how we</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philipp Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Journal of Natural Language Engineering, 11(3):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
</authors>
<title>Paraphrasing rules for automatic evaluation of translation into Japanese.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Workshop on Paraphrasing,</booktitle>
<pages>88--93</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4531" citStr="Kanayama, 2003" startWordPosition="711" endWordPosition="712">label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger</context>
</contexts>
<marker>Kanayama, 2003</marker>
<rawString>Hiroshi Kanayama. 2003. Paraphrasing rules for automatic evaluation of translation into Japanese. In Proceedings of the Second International Workshop on Paraphrasing, pages 88–93, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="5764" citStr="Koehn, 2005" startWordPosition="897" endWordPosition="898"> feature set, and discusses the findings in detail. 3 A Parallel Corpus of Literary Texts This section discusses the construction of T/V gold standard labels for English sentences. We obtain these labels from a parallel English–German corpus using the technique of annotation projection (Yarowsky and Ngai, 2001) sketched in Figure 1: We first identify the T/V status of German pronouns, then copy this T/V information onto the corresponding English sentence. 3.1 Data Selection and Preparation Annotation projection requires a parallel corpus. We found commonly used parallel corpora like EUROPARL (Koehn, 2005) or the JRC Acquis corpus (Steinberger et al., 2006) to be unsuitable for our study since they either contain almost no direct address at all or, if they do, just formal address (V). Fortunately, for many literary texts from the 19th and early 20th century, copyright has expired, and they are freely available in several languages. We identified 110 stories and novels among the texts provided by Project Gutenberg (English) and Project Gutenberg-DE (German)1 that were available in both languages, with a total of 0.5M sentences per language. Examples are Dickens’ David Copperfield or Tolstoy’s An</context>
<context position="6995" citStr="Koehn, 2005" startWordPosition="1095" endWordPosition="1096">luded plays and poems, as well as 19th-century adventure novels by Sir Walter Scott and James F. Cooper which use anachronistic English for stylistic reasons, including words that previously (until the 16th century) indicated T (“thee”, “didst”). We cleaned the English and German novels manually by deleting the tables of contents, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), a</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the 10th Machine Translation Summit, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinz L Kretzenbacher</author>
<author>Michael Clyne</author>
<author>Doris Schüpbach</author>
</authors>
<title>Pronominal Address in German: Rules, Anarchy and Embarrassment Potential. Australian Review of Applied Linguistics,</title>
<date>2006</date>
<volume>39</volume>
<issue>2</issue>
<pages>17--18</pages>
<contexts>
<context position="3118" citStr="Kretzenbacher et al., 2006" startWordPosition="490" endWordPosition="493">ion 4) which establishes that taggers can indeed assign T/V labels to monolingual English utterances in context fairly reliably. Section 5 investigates how T/V is expressed in English texts by experimenting with different types of features, including words, semantic classes, and expressions based on Politeness Theory. We find word features to be most reliable, obtaining an accuracy of close to 70%. 2 Related Work There is a large body of work on the T/V distinction in (socio-)linguistics and translation studies, covering in particular the conditions governing T/V usage in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and the difficulties in translation (Ardila, 2003; Künzli, 2010). However, many observations from this literature are difficult to operationalize. Brown and Levinson (1987) propose a general theory of politeness which makes many detailed predictions. They assume that the pragmatic goal of being polite gives rise to general communication strategies, such as avoiding to lose face (cf. Section 5.2). In computational linguistics, it is a common observation that for almost every language pair, there are distinctions that are expressed overtly 623 Proceedings of the 13th Co</context>
<context position="14404" citStr="Kretzenbacher et al., 2006" startWordPosition="2304" endWordPosition="2307">hout context: (3) “And perhaps sometime you may see her.” This case (gold label: V) is disambiguated by the previous sentence which indicates a hierarchical social relation between speaker and addressee: (4) “And she is a sort of relation of your lordship’s,” said Dawson... . Still, even a three-sentence window is often not sufficient, since the surrounding sentences may be just as uninformative. In these cases, more global information about the situation is necessary. Even with perfect information, however, judgments can sometimes deviate, as there are considerable “grey areas” in T/V usage (Kretzenbacher et al., 2006). In addition, social rules like T/V usage vary in time and between countries (SchUpbach et al., 2006). This helps to explain why annotators agree better with one another than with the gold standard: 21st century annotators tend to be unfamiliar with 19th century T/V usage. Consider this example from a book written in second person perspective: (5) Finally, you acquaint Caroline with the fatal result: she begins by consoling you. “One hundred thousand francs lost! We shall have to practice the strictest economy”, you imprudently add.5 Here, the author and translator use V to refer to the reade</context>
</contexts>
<marker>Kretzenbacher, Clyne, Schüpbach, 2006</marker>
<rawString>Heinz L. Kretzenbacher, Michael Clyne, and Doris Schüpbach. 2006. Pronominal Address in German: Rules, Anarchy and Embarrassment Potential. Australian Review of Applied Linguistics, 39(2):17.1– 17.18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Künzli</author>
</authors>
<title>Address pronouns as a problem in French-Swedish translation and translation revision.</title>
<date>2010</date>
<journal>Babel,</journal>
<volume>55</volume>
<issue>4</issue>
<contexts>
<context position="3208" citStr="Künzli, 2010" startWordPosition="506" endWordPosition="507"> context fairly reliably. Section 5 investigates how T/V is expressed in English texts by experimenting with different types of features, including words, semantic classes, and expressions based on Politeness Theory. We find word features to be most reliable, obtaining an accuracy of close to 70%. 2 Related Work There is a large body of work on the T/V distinction in (socio-)linguistics and translation studies, covering in particular the conditions governing T/V usage in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and the difficulties in translation (Ardila, 2003; Künzli, 2010). However, many observations from this literature are difficult to operationalize. Brown and Levinson (1987) propose a general theory of politeness which makes many detailed predictions. They assume that the pragmatic goal of being polite gives rise to general communication strategies, such as avoiding to lose face (cf. Section 5.2). In computational linguistics, it is a common observation that for almost every language pair, there are distinctions that are expressed overtly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 6</context>
</contexts>
<marker>Künzli, 2010</marker>
<rawString>Alexander Künzli. 2010. Address pronouns as a problem in French-Swedish translation and translation revision. Babel, 55(4):364–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>David Yarowsky</author>
</authors>
<title>Mining and modeling relations between formal and informal Chinese phrases from web corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1031--1040</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="4598" citStr="Li and Yarowsky (2008)" startWordPosition="719" endWordPosition="722">ith annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and “downspeak” based on the social relationship between speaker and addressee. This paper extends a previous pilot study (Faruqui and Padó, 2011). It presents more annotation, investigates a larger and better motivated feature set, and discusses the findings in de</context>
</contexts>
<marker>Li, Yarowsky, 2008</marker>
<rawString>Zhifei Li and David Yarowsky. 2008. Mining and modeling relations between formal and informal Chinese phrases from web corpora. In Proceedings of EMNLP, pages 1031–1040, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schütze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK,</location>
<contexts>
<context position="18560" citStr="Manning et al., 2008" startWordPosition="2982" endWordPosition="2985">iced that in the absence of further constraints, many of the most indicative features are names of persons from particular novels which are systematically addressed formally (like Phileas Fogg from J. Vernes’ Around the world in eighty days) or informally (like Mowgli, Baloo, and Bagheera from R. Kipling’s Jungle Book). These features clearly do not generalize to new books. We therefore added a constraint to remove all features which did not occur in at least three novels. To reduce the number of word features to a reasonable order of magnitude, we also performed a x2-based feature selection (Manning et al., 2008) on the training set. Preliminary experiments established that selecting the top 800 word features yielded a model with good generalization. Semantic Class Features. Our second feature type is semantic class features. These can be seen as another strategy to counteract the sparseness at the level of word features. We cluster words into 400 semantic classes on the basis of distributional and morphological similarity features which are extracted from an unlabeled English collection of Gutenberg novels comprising more than 100M tokens, using the approach by Clark (2003). These features measure ho</context>
</contexts>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="23308" citStr="McCallum, 2002" startWordPosition="3738" endWordPosition="3739">tion of sentences into chunks that are either completely narrative or speech, and (b), labeling of chunks with a classifier that distinguishes these two classes. The segmentation step (a) takes place with a regular expression that subdivides sentences on every occurrence of quotes (“ , ” , ’ , ‘, etc.). As training data for the classification step (b), we manually tagged 1000 chunks from our training data as either B-DS (begin direct speech), I-DS (inside direct speech) and O (outside direct speech, i.e. narrative material).9 We used this dataset to train the CRF-based sequence tagger Mallet (McCallum, 2002) using all tokens, including punctuation, as features.10 This tagger is used to classify all chunks in our dataset, resulting in output like the following example: (9) (B-DS) “I am going to see his Ghost! (I-DS) It will be his Ghost not him!” (O) Mr. Lorry quietly chafed the hands that held his arm.11 Direct speech chunks belonging to the same sentence are subsequently recombined. We define the direct speech context of size n for a given sentence as the n preceding and following direct speech chunks that are labeled B-DS or I-DS while skipping any chunks labeled O. Note that this definition of</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: a survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="13623" citStr="Navigli, 2009" startWordPosition="2184" endWordPosition="2185">e for individual sentences (no context) than when the discourse is available. In context, inter-annotator agreement increases from 75% to 79%, and agreement with the gold standard rises by 10%. It is notable that the two annotators agree worse with one another than with the gold standard (see below for discussion). On those instances where they agree, Cohen’s r. reaches 0.58 in context, which is interpreted as approaching good agreement (Fleiss, 1981). Although far from perfect, this inter-annotator agreement is comparable to results for the annotation of fine-grained word sense or sentiment (Navigli, 2009; Bermingham and Smeaton, 2009). An analysis of disagreements showed that many sentences can be uttered in both T and V contexts and cannot be labeled without context: (3) “And perhaps sometime you may see her.” This case (gold label: V) is disambiguated by the previous sentence which indicates a hierarchical social relation between speaker and addressee: (4) “And she is a sort of relation of your lordship’s,” said Dawson... . Still, even a three-sentence window is often not sufficient, since the surrounding sentences may be just as uninformative. In these cases, more global information about </context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word Sense Disambiguation: a survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>John Wiley and Sons Inc.</publisher>
<contexts>
<context position="26491" citStr="Noreen, 1989" startWordPosition="4268" endWordPosition="4269">. Direct speech context outperforms sentence context throughout, with a maximum accuracy of 67.0% as compared to 65.2%, even though it shows higher variation, which we attribute to the less stable nature of the direct speech chunks and their automatically created labels. From now on, we adopt a direct speech context of size 8 unless specified differently. Influence of Features. Table 4 shows the results for different feature types. The best model (word features only) is highly significantly better than the frequency baseline (which it beats by 8%) as determined by a bootstrap resampling test (Noreen, 1989). It gains 17% over the random baseline, but is still more than 10% below inter-annotator agreement in context, which is often seen as an upper bound for automatic models. Disappointingly, the comparison of the feature groups yields a null result: We are not able to improve over the results for just word features with either the semantic class or the politeness features. Neither feature type outperforms the frequency baseline significantly (p&gt;0.05). Combinations of the different feature types also do worse than just words. The differences between the best model (just words) and the combination</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive Methods for Testing Hypotheses: An Introduction. John Wiley and Sons Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7330" citStr="Och and Ney, 2003" startWordPosition="1138" endWordPosition="1141">ts, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), and ihr (T/V). However, various ambiguities makes their interpretation non-straightforward. The pronoun ihr can both be used for plural T address or for a somewhat archaic singular or plural V address. In principle, these usages should be distinguished by capitalization (V pronouns are generally capitalized in German), but many T inst</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceeding of the 3rd ACL Workshop on Very Large Corpora,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="24023" citStr="Ramshaw and Marcus, 1995" startWordPosition="3858" endWordPosition="3861">ll chunks in our dataset, resulting in output like the following example: (9) (B-DS) “I am going to see his Ghost! (I-DS) It will be his Ghost not him!” (O) Mr. Lorry quietly chafed the hands that held his arm.11 Direct speech chunks belonging to the same sentence are subsequently recombined. We define the direct speech context of size n for a given sentence as the n preceding and following direct speech chunks that are labeled B-DS or I-DS while skipping any chunks labeled O. Note that this definition of direct speech context still lumps 9The labels are chosen after IOB notation conventions (Ramshaw and Marcus, 1995). 10We also experimented with rule-based chunk labeling based on quotes, but found the use of quotes too inconsistent. 11C. Dickens: A tale of two cities. Context size (n) Figure 2: Accuracy vs. number of sentences in context (empty circles: sentence context; solid circles: direct speech context) together utterances by different speakers and can therefore yield misleading features in the case of asymmetric conversational situations, in addition to possible direct speech misclassifications. 6 Experimental Evaluation 6.1 Evaluation on the Development Set We first perform model selection on the d</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In Proceeding of the 3rd ACL Workshop on Very Large Corpora, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schiehlen</author>
</authors>
<title>Learning tense translation from bilingual corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL/COLING,</booktitle>
<pages>1183--1187</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="4121" citStr="Schiehlen, 1998" startWordPosition="641" endWordPosition="642">oiding to lose face (cf. Section 5.2). In computational linguistics, it is a common observation that for almost every language pair, there are distinctions that are expressed overtly 623 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studi</context>
</contexts>
<marker>Schiehlen, 1998</marker>
<rawString>Michael Schiehlen. 1998. Learning tense translation from bilingual corpora. In Proceedings of ACL/COLING, pages 1183–1187, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="7130" citStr="Schmid, 1994" startWordPosition="1113" endWordPosition="1114">h for stylistic reasons, including words that previously (until the 16th century) indicated T (“thee”, “didst”). We cleaned the English and German novels manually by deleting the tables of contents, prologues, epilogues, as well as chapter numbers and titles occurring at the beginning of each chapter to obtain properly parallel texts. The files were then formatted to contain one sentence per line using the sentence splitter and tokenizer provided with EUROPARL (Koehn, 2005). Blank lines were inserted to preserve paragraph boundaries. All novels were lemmatized and POS-tagged using TreeTagger (Schmid, 1994).2 Finally, they were sentence-aligned using Gargantuan (Braune and Fraser, 2010), an aligner that supports one-to-many alignments, and word-aligned in both directions using Giza++ (Och and Ney, 2003). 3.2 T/V Gold Labels for English Utterances As Figure 1 shows, the automatic construction of T/V labels for English involves two steps. Step 1: Labeling German Pronouns as T/V. German has three relevant personal pronouns for the T/V distinction: du (T), sie (V), and ihr (T/V). However, various ambiguities makes their interpretation non-straightforward. The pronoun ihr can both be used for plural </context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doris Schüpbach</author>
<author>John Hajek</author>
<author>Jane Warren</author>
<author>Michael Clyne</author>
<author>Heinz Kretzenbacher</author>
<author>Catrin Norrby</author>
</authors>
<title>A cross-linguistic comparison of address pronoun use in four European languages: Intralingual and interlingual dimensions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the Australian Linguistic Society,</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="3143" citStr="Schüpbach et al., 2006" startWordPosition="494" endWordPosition="497">t taggers can indeed assign T/V labels to monolingual English utterances in context fairly reliably. Section 5 investigates how T/V is expressed in English texts by experimenting with different types of features, including words, semantic classes, and expressions based on Politeness Theory. We find word features to be most reliable, obtaining an accuracy of close to 70%. 2 Related Work There is a large body of work on the T/V distinction in (socio-)linguistics and translation studies, covering in particular the conditions governing T/V usage in different languages (Kretzenbacher et al., 2006; Schüpbach et al., 2006) and the difficulties in translation (Ardila, 2003; Künzli, 2010). However, many observations from this literature are difficult to operationalize. Brown and Levinson (1987) propose a general theory of politeness which makes many detailed predictions. They assume that the pragmatic goal of being polite gives rise to general communication strategies, such as avoiding to lose face (cf. Section 5.2). In computational linguistics, it is a common observation that for almost every language pair, there are distinctions that are expressed overtly 623 Proceedings of the 13th Conference of the European </context>
</contexts>
<marker>Schüpbach, Hajek, Warren, Clyne, Kretzenbacher, Norrby, 2006</marker>
<rawString>Doris Schüpbach, John Hajek, Jane Warren, Michael Clyne, Heinz Kretzenbacher, and Catrin Norrby. 2006. A cross-linguistic comparison of address pronoun use in four European languages: Intralingual and interlingual dimensions. In Proceedings of the Annual Meeting of the Australian Linguistic Society, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
</authors>
<title>Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, and Dan Tufis.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2142--2147</pages>
<location>Genoa, Italy.</location>
<marker>Steinberger, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, and Dan Tufis. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of LREC, pages 2142–2147, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M J Tax</author>
<author>Robert P W Duin</author>
</authors>
<title>Support vector domain description.</title>
<date>1999</date>
<journal>Pattern Recognition Letters,</journal>
<pages>20--1191</pages>
<contexts>
<context position="38000" citStr="Tax and Duin, 1999" startWordPosition="6161" endWordPosition="6164">other hand, very few such generic features exist for the class T; consequently, the classifier has a hard time learning good discriminating and yet generic features. Those features that are indicative of T, such as first names, are highly novel-specific and were deliberately excluded from the main experiment. When we switched to individual novels, the models picked up such features, and accuracy increased – at the cost of lower generalizability between novels. A more technical solution to this problem would be the training of a single-class classifier for V, treating T as the “default” class (Tax and Duin, 1999). Finally, an error analysis showed that many errors arise from sentences that are too short or unspecific to determine T or V reliably. This points to the fact that T/V should not be modelled as a sentence-level classification task in the first place: T/V is not a choice made for each sentence, but one that is determined once for each pair of interlocutors and rarely changed. In future work, we will attempt to learn social networks from novels (Elson et al., 2010), which should provide constraints on all instances of communication between a speaker and an addressee. However, the big – and uns</context>
</contexts>
<marker>Tax, Duin, 1999</marker>
<rawString>David M. J. Tax and Robert P. W. Duin. 1999. Support vector domain description. Pattern Recognition Letters, 20:1191–1199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>200--207</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4337" citStr="Yarowsky and Ngai, 2001" startWordPosition="677" endWordPosition="680">h Conference of the European Chapter of the Association for Computational Linguistics, pages 623–633, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics Figure 1: T/V label induction for English sentences in a parallel corpus with annotation projection in one language, but remain covert in the other. Examples include morphology (Fraser, 2009) and tense (Schiehlen, 1998). A technique that is often applied in such cases is annotation projection, the use of parallel corpora to copy information from a language where it is overtly realized to one where it is not (Yarowsky and Ngai, 2001; Hwa et al., 2005; Bentivogli and Pianta, 2005). The phenomenon of formal and informal address has been considered in the contexts of translation into (Hobbs and Kameyama, 1990; Kanayama, 2003) and generation in Japanese (Bateman, 1988). Li and Yarowsky (2008) learn pairs of formal and informal constructions in Chinese with a paraphrase mining strategy. Other relevant recent studies consider the extraction of social networks from corpora (Elson et al., 2010). A related study is (Bramsen et al., 2011) which considers another sociolinguistic distinction, classifying utterances as “upspeak” and </context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings of NAACL, pages 200–207, Pittsburgh, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>