<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.15574775">
Book Review
A Grammar of English on Mathematical
Principles
Zellig Harris
</figure>
<figureCaption confidence="0.2393345">
John Wiley and Sons, New York.
1982. Pp. xvi, 429. $43.50.
</figureCaption>
<bodyText confidence="0.999940571428571">
This is the most recent and most comprehensive develop-
ment of Zellig Harris&apos;s formal linguistics. As such, it
merits careful study by anyone seriously interested in the
scientific study of language, and in particular by anyone
working with computers to analyse or use natural
language. The intrinsic interest Harris holds for compu-
tational linguistics stems chiefly from:
</bodyText>
<listItem confidence="0.922210545454545">
• The simplicity and elegance of the mathematical model
he proposes for language. Particularly attractive is its
freedom from highly abstract hierarchies of grammat-
ical objects and operations subject to change in the
next gust of theoretical fashion.
• The comprehensiveness of his grammar with respect to
the semantic, syntactic, and morphological detail of
natural language, as exemplified by English.
• The use he makes of the observation that the metalan-
guage (the language in which the grammar is stated)
must of necessity be contained in the object language
</listItem>
<bodyText confidence="0.998673">
being described. This is a principle reason his
approach avoids building the hierarchies of grammat-
ical and semantic mechanisms — and computational
representations for them! — that many investigators
have come to accept as necessary and even desirable.
Of particular interest is his use of language itself to
account for the indeterminately numerous and inter-
minably complex issues of the context and use of
language (pragmatics and all that).
</bodyText>
<listItem confidence="0.962479230769231">
• His partitioning of semantics into &apos;objective
information&apos; versus communicative and expressive
nuance, relying only on formal linguistic criteria.
• His notion of sublanguage; in particular, the sublan-
guage generated by his base, which is free of para-
phrase yet informationally complete (albeit at the cost
of being for most utterances &apos;unspeakably&apos; cumber-
some in style).
• His linking of &apos;reductions&apos; (approximately, transforma-
tions) to points of informational redundancy in
discourse.
• His identification of affixes and most prepositions as
&apos;argument indicators&apos; and &apos;operator indicators&apos;, and his
</listItem>
<bodyText confidence="0.983148534482759">
exploitation of them as providing traces of derivation.
Together, these characteristics suggest an approach to
computational parsing and synthesis that could be both
highly efficient and semantically sensitive. Beyond that,
they indicate avenues for design of artificial languages
and language-like systems that have yet to be tried.
Somewhat less than the first half of the book (chapters
1-3, pages 1-185) presents H&apos;s model of language. The
remaining 228 pages (chapters 4-9 plus the Appendix)
restate the categories and phenomena of traditional
grammar in some detail as both a demonstration and a
test of that model.&apos; It is a densely written book, and
astonishingly comprehensive. Every page is filled with
information and insights enough to be expanded to a
journal article, reflecting a grasp of syntactic and seman-
tic data that is of extraordinary depth and breadth, both
synchronically and diachronically. The reader should not
assume that H neglects a particular problem simply
because he does not summarize his solutions under a
familiar label, such as Neg Raising.2 On the contrary,
even readers who are loathe seriously to entertain an
alternative paradigm in linguistics will profit from study
of this encyclopedic restatement of grammar.
There are a number of reasons, to be sure, why schol-
ars may prefer to ignore H&apos;s work. Many readers have
found H&apos;s prose tough sledding. As Jane Robinson once
observed (pc):
If I have an idea what he&apos;s talking about, I can under-
stand him. As someone said of Quine, once you&apos;ve
understood what he means, you realize he couldn&apos;t have
said it any other way. Harris is that way for me. It&apos;s just
that what he&apos;s trying to say is difficult.
Let those readers who are for this reason reluctant to
tackle another Harris opus be assured. The writing here,
in addition to being explicit and unambiguous, as always,
is also quite clear and straightforward. Even in his justi-
fications of the more complex derivations, where what he
is trying to say is indeed difficult, one&apos;s attention span
need be only somewhat longer than usual to hold the
1 Jespersen was reputedly H&apos;s principle guide for this second portion of
the book. In his preface, I-1 cites Jespersen&apos;s monumental Modern English
Grammar on Historical Principles (1909-1931), together with the OED,
as &amp;quot;the two indispensable aids on the English language&amp;quot;. The parallelism
of titles is surely no Secident. I believe that FI also culled the literature to
ensure that he was accounting for all the examples that other linguists
had found for one reason or another to be problematic.
2 Another pitfall for the unwary reader is, ironically, a consequence of
H&apos;s long-standing rhetorical practice of repeating key information in
succinct form so that each chapter. or section is relatively autonomous.
He does this in an effort to make his writings easier to read and use.
However, a superficial reader might first encounter a controversial topic
(for example, H&apos;s proposed bisentential metalinguistic operator co-state in
the source of and, or) in such a recapitulation, and reject it out of hand,
having missed the main section where it is properly developed and justi-
fied. Unfortunately, the editors have failed to indicate the most impor-
tant entry where the index has several, so that one must cross-check the
page numbers of the index against those of the table of contents to find
the main entry.
</bodyText>
<note confidence="0.51215">
Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 203
Book Review A Grammar of English on Mathematical Principles
</note>
<bodyText confidence="0.99626395652174">
strands of argumentation in mind. Indeed, complex argu-
mentation can be no stop to any student of generative
linguistics! No, remaining difficulties for the reader will
be due to the unfamiliarity of the approach, which H
himself cites (page vi) as his reason for discussing some
&apos;methodological considerations and grammatical
constructions&apos; redundantly in more than one section of
the book.
More serious is the question of familiar jargon and
shared terms for debate. As I suggested above, he does
not use the jargon of generative linguistics. In a field,
and indeed in a social climate, where there is great
competition to be &apos;more state of the art than thou&apos;, the
obvious assumption is that H has simply not kept up.
Not to beat around the bush, the central issue in any
evaluation of H&apos;s work in linguistics is that it has been
eclipsed by that of his most famous student, Noam
Chomsky. More especially, it has been obscured by the
spirit of polemic that has consumed the field of linguistics
in this country for the past twenty or twenty-five years.
H is both stubbornly unpolemical and stubbornly auton-
omous. These characteristics have unfortunately contrib-
uted to his contributions going largely unnoticed in the
United States (though not abroad). Perhaps it is time to
allow the curtain to fall on this Oedipal drama and evalu-
ate H&apos;s work on its merits.
After all, H&apos;s isolation from &apos;mainstream linguistics&apos;
has also had its advantages. Undistracted by academic
politics and the demands of cross-paradigmatic refuta-
tions, he has been painstakingly &amp;quot;formulating as a math-
ematical system all the properties and relations necessary
and sufficient for the whole of natural language [rather
than] investigating a mathematically definable system
[such as phrase-structure grammar] which has some
relation to language, as being a generalization or subset&amp;quot;
(Harris 1968:1). He now offers results that linguists and
computational linguists should pay close attention to, at a
time when many far less well-developed alternatives rush
to fill a perceived theoretical vacuum in the field.
The limitations of a review preclude a detailed
comparison of H&apos;s model with more familiar generative
models, but a sketch at least is in order. In this review, I
refer to the Harrisian model of language as &apos;constructive
grammar&apos; and to the Harrisian paradigm for linguistics as
&apos;constructive linguistics&apos;.3 A constructive grammar has at
least the following six characteristics:
</bodyText>
<listItem confidence="0.991478">
1. The semantic primes are words in the language, a
base vocabulary that is a proper subset of the vocab-
ulary of the language as a whole.4
2. Generation of sentences in the base is by word entry,
beginning with entry of (mostly concrete) base
nouns. The only condition for a word to enter is that
its argument requirement must be met by some previ-
ously entering word or words, generally the last entry
or entries, which must not already be in the argument
of some other word. The base vocabulary has thus a
few simple classes of words:
</listItem>
<bodyText confidence="0.993575454545455">
base nouns with null argument
On, Onn operators requiring base nouns as argu-
ments
00, 000 requiring operators as arguments
Ono, Oon requiring combinations of operators and
base nouns
In addition to these classes, almost all of the opera-
tors require morphophonemic insertion of &apos;argument
indicators&apos; such as -ing and that. (These were termed
the &apos;trace&apos; of &apos;incremental transformations&apos; in Harris
1965 and 1968.)
</bodyText>
<listItem confidence="0.999835090909091">
3. The base generates a sublanguage,5 which is infor-
mationally complete while containing no para-
phrases. This is at the expense of redundancy and
other stylistic awkwardness, so that utterances of any
complexity in the base sublanguage are unlikely to be
encountered in ordinary discourse. As in prior
reports of H&apos;s work, base sentences are all assertions,
other forms such as questions and imperatives being
derived from underlying performatives I ask, I
request, and the like.
4. A well-defined system of reductions yields the other
</listItem>
<bodyText confidence="0.904415571428571">
sentences of the language as paraphrases of base
sentences.° The reductions were called the
&apos;paraphrastic transformations&apos;, and &apos;extended
morphophonemics&apos; in earlier reports. They consist of
permutation of words (movement), zeroing,7 and
morphophonemic changes of phonological shape.
Each reduction leaves a &apos;trace&apos; so that the underlying
3 `Harrisian&apos; imputes to Harris some obscure responsibility for work that
others do in this paradigm. H&apos;s use of &apos;constructive grammar&apos; (e.g.
Harris 1968, pp. 17f, 20, 32, 89, 121) is apparently allied with intuition-
ist usage in mathematics. The term &apos;constructive&apos; provides a counterpoise
to the (equally irrelevant) rhetorical overtones that have grown up around
the term &apos;generative&apos; as a kind of trademark. A constructive grammar
such as the one under review is of course generative in the technical sense
of producing an explicit structural description for every sentence of the
language. Furthermore, at least since his 1965 report in Language H has
been explicitly concerned with generative rules for deriving sentences
from base structures. (Newmeyer 1980:37 misinterprets Corcoran
1972:279 on this point: he ignores Corcoran&apos;s further discussion in that
paper, as well as H&apos;s own later writings, in particular section 8.8 of
Harris 1969, Corcoran&apos;s principal reference). This notwithstanding, few
would claim that H is a generative grammarian, and a correlative term
other than &apos;Harrisian grammar&apos; seems in order.
4 The suggestion outlined in Harris (1969), and exemplified in Gross
(1973), that some of the base vocabulary may be suppletives factored
from partial homonyms and synonyms, is not taken up systematically in
this book. Echoes of this program may be found here and there, however,
as for example in H&apos;s observations on the derivation of -hood, -dom, which
were once free words in English (suppletive forms of something like state
and estate, respectively). Note here also the discussion (p. 73) of classifi-
ers in the sublanguage of a science, in particular the sublanguage of
grammar (which is the metalanguage of the language as whole).
5 A sublanguage is a subset of sentences in the language, mathematically
defined by closure under a subset of the operations that are defined for
the language as a whole (cf. Harris 1968, e.g. p. 152). The notion of
sublanguage, with its semantic interpretation of subject-matter special-
ization, runs somewhat counter to rationalist notions of innate ideas and
may be difficult to characterize in generativist terms. Nevertheless,
sublanguages are as important as social and regional dialects for an
understanding of language change, and are essential to an understanding
of semantics of natural language as opposed to formal languages.
Kittredge and Lehrburger (1982) provides a brief survey.
</bodyText>
<page confidence="0.961241">
204 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984
</page>
<subsectionHeader confidence="0.42548">
Book Review A Grammar of English on Mathematical Principles
</subsectionHeader>
<bodyText confidence="0.977448285714286">
redundancies of the base sublanguage are
recoverable.8 Linearization of the operator-argument
dependencies — in English either &apos;normal&apos; SVO or a
`topicalizing&apos; linear order — is accomplished by the
reduction system, not the base. The reduction
system includes much of what is in the lexicon in
generative grammar (cf. Gross 1979).
</bodyText>
<listItem confidence="0.967289">
5. Metalinguistic information required for many
reductions, such as coreferentiality and lexical identi-
ty, is expressed within the language by conjoined
metalanguage sentences, rather than by a separate
grammatical mechanism such as subscripts.9 Similar-
ly, &apos;shared knowledge&apos; contextual and pragmatic
information is expressed by conjoined sentences
(including ordinary dictionary definitions) that are
zeroable because of their redundancy.
6. The set of possible arguments for a given operator
(or vice-versa) is graded as to acceptability. These
gradings correspond with differences of meaning in
the base sublanguage, and thence in the whole
language. They diverge in detail from one sublan-
6 Weak paraphrases: see below.
</listItem>
<bodyText confidence="0.987974434343435">
7 Zeroing is similar to deletion with a condition of recoverability (but
contrast the Constructive treatment at pp. 50 ff. and fn. 10 with Chom-
sky (1964), which introduced the latter notion). The term &apos;deletion&apos; is
often applied to &apos;replacement&apos;, as in pronominalization. This and other
points of difference between the two terms seem to be an artifact of
generative derivations being defined on abstract phrase-structure trees
and Constructive derivations being defined on lexical dependencies. For
an example of problems with deletion as defined on nodes of phrase-struc-
ture trees, see for example Postal (1978), pp. 10-11, and ref. cit. For
Postal (p. 23), there is no way to formulate in a transformational frame-
work the fact that `to contraction&apos; — phonetic reduction of to under the
seven verb forms want, going, have, ought, used, got, and supposed — is
determined by &apos;subject sharing&apos;, though that is precisely how H&apos;s trans-
formational grammar did and his constructive grammar does account for
these phenomena.
8 These traces, and those noted under 3 above, were the basis for refor-
mulation of transformations as elementary sentence-differences, a crucial
step in development of the constructive model. They are of course unre-
lated to the trace marker proposed in Chomsky (1973), which, as Gross
(1979: 874) points out, is simply a computational device, a pointer to a
memory address, such as is found in programming languages.
9 See Harris (1968: 17 ff.) for reasons why the metalanguage must be
within the language. This property of language of course does not in
itself preclude use of other types of notation for the metalanguage.
Convenient notational reductions of other specialized sublanguages to
formal symbol systems come to mind, as for example the notational
systems of logic and mathematics, whose most complex formulae are
nevertheless always stateable somehow in sentences of the corresponding
sublanguages. The motivation for such a notational system (computa-
tional convenience) is offset here by El&apos;s demonstration that metalinguis-
tic information is normally zeroed under reductions defined for the
language as a whole. There is also the open question whether the pres-
ence of low-information and metalanguage conjuncts in the base sublan-
guage constrains the reduction system, and whether subscripts and other
computationally convenient notations might therefore give rise to some
thorny problems as an artifact (see note 21 below). Furthermore, since
sentences containing overt (unzeroed) assertions of metalinguistic infor-
mation do occur in normal discourse, and cannot be excluded as ungram-
matical, any grammar must account for them whether it uses them in this
way or not, so it might as well use them.
I° See Zadeh 1965.
I I See also the discussion of analogic extension, below.
guage or subject-matter domain to another. Equiv-
alently, the fuzzy set&amp;quot;) of &apos;normal&apos; co-occurrents for
a given word differs from one such domain to anoth-
er within the base sublanguage.
In informal, intuitive terms, a constructive grammar
generates sentences from the bottom up, beginning with
word entry, whereas a generative grammar generates
sentences from the top down, beginning with the abstract
symbol S. The grammatical apparatus of constructive
grammar (the rules together with their requirements and
exceptions) is very simple and parsimonious. H&apos;s under-
lying structures, the rules for producing derived struc-
tures, and the structures to be assigned to surface
sentences are all well defined. Consequently, H&apos;s argu-
mentation about alternative ways of dealing with prob-
lematic examples has a welcome concreteness and
specificity about it. In particular,11 one may directly
assess the semantic well-formedness of base
constructions and of each intermediate stage of deriva-
tion, as well as the sentences ultimately derived from
them, because they are all sentences. By contrast, in
generative argumentation, definitions of base structures
and derived structures are always subject to controversy
because the chief principle for controlling them is
linguists&apos; judgments of semantic relations (such as para-
phrase) among sentences derived from them. Even if
one could claim to assess the semantic well-formedness
of abstract underlying structures, these are typically so
ill-defined as to compel us to rely almost totally on
surface forms to choose among alternative adjustments to
the base or to the system of rules for derivation. And as
we all know, a seemingly minor tweak in the base or deri-
vation rules can and usually does have major and largely
unforeseen consequences for the surface forms generated
by the grammar.
H has always given primacy to semantics over syntax.
Even in his earlier structural linguistics, H&apos;s distribution-
alism was a study of semantics and not the empty taxon-
omy of the &apos;structuralist&apos; stereotype; H&apos;s two empirical
touchstones, contrast and differential acceptability, are
both semantic notions; and H&apos;s aim in reducing redun-
dancy in grammar (see below) is to get syntax out of the
way of semantics. For Chomsky, on the other hand,
syntax has always been central and semantics must be
effected by a separate interpretive mechanism. Munz
(1972) sketches the history of this divergence of the two
paradigms.
The mention of paraphrase under characteristic 4 may
trouble some readers. The use of judgments of para-
phrase as a criterion for grammatical relationship has
given rise to endless confusion and dissension in linguis-
tics. This is because paraphrase in any strong sense is an
exceedingly rare phenomenon in natural language. For
H, paraphrase (&apos;weak paraphrase&apos;) is rather an interpreta-
tion of the fact that the semantic dependencies of the
base are preserved by (are recoverable under) the
reductions. The reductions in turn are defined with
</bodyText>
<figure confidence="0.3097535">
Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 205
Book Review A Grammar of English on Mathematical Principles
</figure>
<bodyText confidence="0.92989381300813">
respect to the more primitive notion of graded accepta-
bility (characteristic 6).
On first exposure to H&apos;s system, readers are frequently
puzzled as to the motivation of characteristic 6, and are
most likely to object to stylistically peculiar sentences
postulated as sources (and as intermediate constructions
in certain derivations) under characteristics 3 and 5.
Characteristic 6 was of course previously H&apos;s criterion
for transformation,12 and gives him a metric for &apos;weak
paraphrase&apos;: preservation of the objective information
being transmitted, with expressive and communicative
nuance13 attributed to the reductions. This metric works
as follows: Supposing two sentence forms14 A and B to
be related by some derivational step in H&apos;s grammar, if
any two sentences among the satisfiers of sentence form
A differ on some scale of acceptability, the correspond-
ing sentences among the satisfiers of sentence-form B
always differ in the same way.15
The relation to information content follows from the
following observation: given a base sentence with a valid
variable (N or one of the subclasses of 0) in place of one
of its words, the base sentences resulting from various
valid substitutions of words for the variable are not all
equally sayable, and this is a direct reflection of the
meanings of the words. By this means, then, Harris&apos;s
grammar captures what the dust jacket refers to as &apos;weak
semantics&apos;16 without the cost of a separate semantic
component.
H&apos;s grammar is the product of long evolution, rather
than of sudden revolution. His method of improving his
grammar is to extend existing, established reductions to
new argument domains, rather than to throw out the
previous version of the grammatical apparatus for a
revolutionary new one. (See Sager (1981) for examples
of this within an earlier form of H&apos;s paradigm.) His meth-
od mirrors the way language itself changes over time, and
indeed incorporates the mechanism of language change
12 It was an alternative to well-defined selection or &apos;cooccurrence sets&apos; as
a criterion for two sentence-sets to be transforms in 1957, and the only
criterion after I-1 determined that simple well-defined co-occurrence did
not stand up, as he reported in his 1965 paper.
13 These more subtle aspects of meaning, which are more difficult to
characterize formally, are thus segregated for study together with other
essentially gestural systems of communication, as distinct from trans-
mission of objective information. In this way, I-1&apos;s metric provides a prin-
ciple by which to control the more baroque excesses of abstract syntax
without giving up the power of the base to generate underlying semantic
structures directly.
14 A sentence form is a sequence of variables and constants, where the
variables are word-classes and the constants are particular words or
affixes, such as those of the sequence be . . -en by of passive sentences.
Any n-tuple of words that may more or less acceptably be substituted for
the n-tuple of variables constitutes a satisfier of the sentence form.
15 A seeming exception is where a sentence form itself— that is, all of its
satisfiers — has reduced acceptability. (These sentences H marks -I- with a
dagger.) However, even where some acceptability differences are
collapsed and become difficult for linguists and their informants (or
experimental subjects) to access, the corresponding sentences are never
reversed in their positions on the given scale of acceptability, but at most
are only reduced to identity as &apos;equally marginal&apos;.
by analogic extension. H&apos;s formulation of analogic
extension in his reduction system accounts in a unified
way for synchronically productive analogic extension,
such as metaphor, as well as for diachronic change.
Then, because constructions that are at first difficult to
account for turn out typically to be at the diachronic or
analogic &apos;growing edge&apos; of the language, reachable by
refinement and extension of the domain of well-attested
operations and reductions, the same mechanism provides
an elegant solution to the sticky problem of the ill-de-
fined and shifting boundaries of language. As an added
dividend, his derivations of many constructions fit their
history.17
Analogic extension is possible, indeed quite natural,
when rules are defined in string-grammatical terms, and
when the base generates a subset of the language. H&apos;s
grammar works from the inside out, as it were, generat-
ing a subset and extending to include the rest of the
language. The generative paradigm works rather from
the outside in: an excessively powerful system of rules
generates a superset of objects containing the set of
sentences, and a major task is pruning and filtering;
degrees of grammaticality and degrees of acceptability
(the two are often not distinguished) are problematic
rather than being fundamental data of linguistics; and
dynamic processes of metaphor, analogy, and language
change are difficult to integrate with one another and
with synchronic descriptions of languages.
16 There is no mention of this &apos;weak semantics&apos; in the book proper,
though it does occur in Hiz (1979). This &apos;weak semantics&apos; must be
construed as a form of &apos; propositional meaning&apos; with the proviso that a
great deal of pragmatics, implied meaning, and the like, may appear as
conjoined &apos;common knowledge&apos; sentences that are zeroable because so
utterly redundant (characteristic 5). This makes sense only in context of
a knowledge base, a theme to which I will revert at the end of this review.
Examples of H&apos;s treatment of some familiar semantic problems follow
this comparison of the two paradigms.
17 See e.g. pp. 27-28 and 377-78 for discussion. However, H&apos;s derivation
of please (p. 351 and n 7) could be improved with more historical perspec-
tive. He suggests:
Please under I request_you is a special reduction, on expectabil-
ity grounds (3.55) of a secondary You_please me: from
go, thereby_you please me or the like.
Better would be:
I request [you] that you go,
if you would please me!
Go, if you would please me!
Go, if you please!
Go, please! -&gt;
Please go!
I am not certain what support there is in H&apos;s system for the permutation
at the end of this derivational sequence; the last, commaless sentence does
preserve the lowered intonation that please has in the preceding sentence.
Contrast the derivation of the corresponding sentence with a comma:
If you would please me,
I request [you] that you go!
If you would please me, go!
If you please, go!
Please, go!
Nuances of deference, irony, and the like associated sometimes with
please and more often with if you please arise historically from normally
construed context of a superior granting a boon and thereby pleasing an
inferior.
</bodyText>
<page confidence="0.837256">
206 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984
</page>
<figure confidence="0.299596">
Book Review A Grammar of English on Mathematical Principles
</figure>
<bodyText confidence="0.989535852173913">
Put another way, H&apos;s reports and books since 1955
have always presented a complete grammar with a
program for refinement and extension. His work cannot
be understood and properly evaluated using critical tools
that are appropriate for generative grammatical writings,
which with few exceptions present a fragment of gram-
mar with a program for either generalizing it or integrat-
ing it with other like proposals.
A crucial difference between constructive grammar
and generative grammar is the importance H has always
assigned to freeing his system as much as possible from
redundancy.
[The] strong connection between the grammatical form
and the meaning is achieved by two analytic methods:
first, by recognizing as many as possible of the regular
reductions (and morphophonemic variants) that produce
changed forms without changing their information;
second, by keeping the grammatical description as unre-
dundant as possible so that the essential redundancy of
language as an information-bearing system (which
consists in the redundancy of the dependence relation
and of selection) not be masked by further redundancy in
the description itself. . . [O]ne must recognize that
every new term or category or subclass that is not deriva-
ble from the primitives of the system, and also every rule,
including every limitation on the carrying-out of a rule,
and every ad hoc explanation[,] is a redundancy of the
description. [pp. 10-111
This has not been a major concern in the generative
paradigm;18 indeed, with a phrase-structure base, there is
no principled way to control the proliferation of nonter-
minal symbols or to control the structure of the apparatus
of the grammar in general, as for example in derived
constituent structure. It is therefore almost inevitable in
generative grammatical work that artifactual, extrinsic
structure intrudes upon and masks the intrinsic structure
of the language that the grammatical apparatus seeks to
explain. It follows that some putative language universals
may be artifacts of generative grammatical apparatus,
found in various languages just because the same appara-
tus is used, much as the padres of old wrote grammars of
Latin with exotic vocabularies.
Although universals are not H&apos;s explicit concern —
presumably, he would determine them empirically after
working out the transformational structure of other
languages in constructive terms rather than making them
a prerequisite for such work — one may draw some
conclusions about them from this book. Surely, the very
18 H&apos;s reduction of redundancy has in its intent some relation to the
notion of simplicity in generative grammar, but is both more sensitive and
intuitively more satisfying than gross measures of formalisms, such as
symbol-counting.
19 For Korean, see e.g. Harris (1968:109-113). For French, see Sager
(1981: 10.2); Sager, Claris, and Clifford (1970); and Salkoff (1973).
For German, see Langerhans (1981). For Takelma, see Kendall (1977).
20 Except insofar as H&apos;s indefinites may be thought of as classifiers with
the broadest selection. There is some parallelism between the notion of a
`designated representative&apos; of a category introduced in Chomsky (1964)
and H&apos;s use of indefinites where a zeroed argument is not specified by
context.
simple information-transmitting dependency structure of
H&apos;s base is universal, and having been found for English
(and in some measure for French, German, Korean,
Takelma, and other languages) may now be presumed as
an objective of work in other languages. And surely the
major types of reduction involving permutation and zero-
ing, such as his length permutation, include many
universals, though details of the &apos;morphophonemic&apos; types
of reduction are more likely to be language specific.19
Comparison with generativist findings of semantic
universals is more problematic. Differences of accepta-
bility or likelihood among the arguments of a given oper-
ator, or among the operators (or operators and
co-arguments) for a given word, are notoriously variable
and vague. For H, selection restrictions within the major
dependency classes of base nouns and operators form at
best fuzzy subsets that are inherently beyond the reach
of the binary semantic features of interpretive semantics.
To be sure, there are some hard yes/no selection
restrictions that can be successfully encoded by binary
semantic features, but these are produced by the
reduction system (cf. Harris 1976:251 for a succinct
statement), and therefore for H are not a problem of
lexicon at all.
H does little with the classifier words2° that are explic-
itly part of the vocabulary — the hierarchies of words that
make up the taxonomies, both folk and scientific, that
were the original motivation for componential analysis
and semantic features. Classifier words among the opera-
tors (act is an example) go unmentioned except for the
&apos;appropriate&apos; operator needed for the sources of certain
derived nouns (5.25, p. 224). H does show how novel
utterances may be related to familiar ones of known
acceptability by substituting classifier words, as in the
following example from page 6:
Some blue and mauve onion-skin shot through the
air at 759.06 miles per second.
Substituting classifier words:
Some colored solid object shot through the air at a
particular velocity; blue and mauve are colors;
onion-skin is a solid object; 759.06 miles per
second is a particular velocity.
Some blue and mauve colored solid object consist-
ing of onion-skin shot through the air at a velocity
of 759.06 miles per second.
It is not clear to the reviewer at least whether H can find
further well-established reductions to derive the first
sentence above from the last one (that appears not to be
his aim in citing this example). Note, however, that a
similar mechanism underlies Sager&apos;s computer formatting
of sublanguage discourse (Sager 1981, 1982; Hirschman
and Sager 1981). Some classifier words presumably
embody semantic universals (e.g. animal, human, object)
so that such use of classifiers could be a device for
machine translation. One may hope that H or another
</bodyText>
<figure confidence="0.327032">
Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 207
Book Review A Grammar of English on Mathematical Principles
</figure>
<bodyText confidence="0.998193158333334">
worker in the constructive paradigm will take up these
undeveloped themes in some future report on the lexicon
for a constructive grammar.
Presumably, the next important phase of H&apos;s work
includes a specification of the lexicon for his grammar
and a return to discourse analysis from the point of view
of operator dependencies. Characteristic 6 above
suggests that the form of the lexicon of a constructive
grammar differs in important ways from that of other
approaches. Obviously, much that has exercised students
of generative lexicography is captured by Harris&apos;s
reduction system. Even more obviously, there is no need
to map the underlying base sentences onto some seman-
tic representation, since the base sentences are them-
selves the semantic representation. Finally, much of the
formalism of the generative lexicon, motivated as it is by
characteristics of phrase-structure grammar, has no place
here.2I
Lacking H&apos;s specification, I suppose that the lexicon of
a constructive grammar would list a subset of words and
affixes (the base vocabulary of N and operators, and the
argument indicators and traces of reductions), together
with their form class, if any. (It might also list some
words that are products of particular reductions, together
with their derivations, depending upon how much one
wanted to include for pragmatic reasons relating to
computability.)
For the acceptabilities, one could start with an
&apos;acceptability model&apos;, a finite set of base sentences —
21 For computer work, it might seem convenient to introduce some
special notational conventions, such as subscripts for metalinguistic iden-
tity statements preserving a straightforward formal relationship to the
words used to express them within the language, as noted above under
characteristic 5. Such a program is questionable, however, insofar as the
presence of such zeroable sequences in sources may interact with the
normal restrictions defined for the reductions and thus impose some
significant constraints on derivations. (There is some evidence for this in
certain island phenomena.) In a system with metalanguage statements of
coreference and the like represented in a separate notation outside the
language, these constraints have to be restated in the form of special rules
or exceptions to rules, introducing extrinsic redundancy precisely of the
sort that we wish to avoid.
22 See for example Sager (1981), Gross (1975, 1979), Salkoff (1973),
and references cited in those works.
23 The first three examples were suggested by Wheeler 1984.
24 With other verbs, such as stand in He stood the box on end, The tree
stood in the forest, and f He stood the tree in the forest, H sets up a causa-
tive source (Zero Causative, 6.8). In both cases, the higher verb imposes
restrictions on the derived form, such as we see reflected in the limited
acceptability of the sentence just cited with a dagger t (which might
occur acceptably, forexample, in a myth or folk tale). These restrictions
betray the zeroed presence of the higher verb and indicate which form is
primitive and which derived.
Following H&apos;s practice, I am using a dagger to mark a sentence of
reduced acceptability that is still grammatical. It marks relatively lower
acceptability of marginal but still sayable sentences, rather than indicat-
ing their position on some absolute scale of acceptability. In some of the
examples that follow where one sentence in a derivation is stylistically
strange and another is even more difficult on semantic grounds, sayable
only in some enabling special context, the dagger marks only the one with
lowest acceptability to indicate the contrast. Note that the acceptability
of such sentences is reduced precisely because the less redundant and
more conventional reduction is available and obvious.
actual sequences of words — that are known to be accept-
able for a given subject-matter domain. These sentences
would be of three sorts. The primary ones for establish-
ing acceptabilities would use only classifier nouns, as in
the onion-skin example. There would be a set of
&apos;dictionary sentences&apos; for each classifier N giving accept-
abilities for its co-argument N under be. (For compact-
ness, many of these sentences could have classifiers in
both argument positions. These acceptabilities are well-
defined, or nearly so, only for the classifier words in
sublanguages of science.) The grammar would draw upon
these dictionary sentences to extrapolate the fuzzy set of
acceptable arguments for a given operator, or of opera-
tors (and, where required, co-argument N) for a given N
(see Harris 1976, p. 247 and fn 13). Metaphor and
analogic extension fall naturally out of the same process.
Finally, another type of dictionary sentence would be
needed to meet the word-sharing requirement under
conjunction that distinguishes coherent discourse from
incoherent strings of sentences (pp. 13, 163-4; also
Harris 1976, p. 241 and fn. 6; 1982, pp. 232-3). These
&apos;shared knowledge&apos; sentences are normally zeroed since
they add only information that the hearer is presumed
already to have. The three sets of acceptability-model
sentences would be unique for each definable sublan-
guage, presumably with greater overlap between related
subject-matter domains and with some shared &apos;common
usage&apos;, and could function as a knowledge base for
particular computer applications.
Much highly successful work has been done with
computer grammars using string-based parsers of an
earlier Harrisian type.22 The present work suggests that a
different sort of parser might use the predetermined
constructive dependency class of each word, the operator
indicator for each operator, and the traces of the
reductions, as &apos;handles&apos; to facilitate the parsing process.
Such an approach was not feasible before the precise
form and interrelationship of the base and the reduction
system were determined. It should make possible
computer grammars that are both more efficient and
more semantically sensitive than any now implemented.
As I observed above, this is a complete grammar, and
one whose internal integrity and parsimony is such that
we examine individual fragments of the grammar out of
context at our peril. With that caveat, here are some
examples drawn from the book in the interest of giving
more substance to the general remarks above.
Among the familiar semantic problems23 that H covers
is the middle voice (pp. 368-69) which we may exemplify
by hit vs. break, where only the latter may become seem-
ingly intransitive (The window broke, *The window hit).
H derives the intransitive form of verbs like break from a
zeroable higher operator that strongly selects a subject
identical to the object of the lower verb:24
The window underwent one&apos;s breaking of it.
The window underwent breaking. -*
The window broke.
</bodyText>
<page confidence="0.933896">
208 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984
</page>
<note confidence="0.34455">
Book Review A Grammar of English on Mathematical Principles
</note>
<bodyText confidence="0.999504583333333">
The window underwent one&apos;s hitting of it. -1.
t The window underwent hitting.
* The window hit.
We may exemplify a second classic problem of seman-
tic analysis by the relation between find and seek, where
Fred found Pegasus presupposes the existence of Pegasus,
and Fred sought Pegasus does not. Seek is in the depend-
ency class Ono, and its apparent status here as Onn is due
to zeroing of the lower operator find.25 The source of the
example sentence is thus Fred sought to find Pegasus,
where seek is asserted (under the I say operating on the
whole sentence), but find is not asserted, and hence its
presupposition about the existence of Pegasus is not
asserted.26
A third familiar semantic problem is the scope of
quantifiers. For H, quantifiers are not semantic primi-
tives, but are reductions from complex sources:
. . . The occurrences of quantifiers (including the
numbers) can be derived within the framework of the
present analysis by two assumptions about their source.
One is that they are the second argument of an operator,
such as mounts (up) to, which is shown in the scale
construction, . . . The other is that the plural and plural
quantifiers are cases of this mounts up to operating on
and.
The passive constructions Someone was opposed by every-
one and Everyone was opposed by someone illustrate how H
handles questions of quantifier scope. To understand
them, we will first have to look at H&apos;s treatment of the
passive.
The first transformation isolated in the late 1940s, the
passive has generally been assumed to be a single (para-
phrastic) transformation. Many anomalous examples of
passives have raised questions about the semantic proper-
ties of transformations in general. However, each of the
three physical components of passive sentences occurs
elsewhere in the grammar.
The first of these components is the apparent permu-
tation of object and subject. H argues that the object,
rather than being permuted, is the subject of a higher
verb of the semantic set that make their subject a recipi-
ent (including have, need, undergo). As elsewhere in the
grammar (such as the hit vs. break example, above),
when this higher verb operates on a sentence containing
an object N metalinguistically identified with its subject,
the redundant lower object may be zeroed:
He had a scolding of him by me. -•
He had a scolding by me.
</bodyText>
<footnote confidence="0.845629428571429">
25 In some contexts, perhaps in the argument of capture or some other
operator. Note that one may seek to do anything, but in most contexts
only finding is so predictable as the purpose of one&apos;s seeking that the verb
find is zeroable under seek.
26 In Harris 1976, argues that problems of intension vs. extension do
not arise in his inscriptional approach to semantics, as they do in the more
familiar interpretive and set-theoretic approaches.
</footnote>
<bodyText confidence="0.996525">
The second component of the passive is be . . . -en
before the original subject. The -en occurs with stative
meaning in the have . . . -en (perfect) construction, as
well as in the diachronically moribund construction seen
for example in He is risen. H takes these to be occur-
rences of the same suffix, a suppletive form of underlying
be in a state: He is in the state of his scolding by me.
The third component is by (sometimes another prepo-
sition, as in is interested in, is tired of, is tired from)
before the original subject. The by (or other preposition)
appears in nominalizations such as The chopping of the
trees by the settlers.
H shows that the domain and selection of the passive
is precisely the product of the domains and selections of
these components. As a bonus, he thereby accounts for
those semantic idiosyncrasies that have made the passive
problematic. The full discussion defies brief paraphrase,
but the following examples27 illustrate the point:
The house was in a state of the building of it by a
farmer.
The house was built by a farmer.
The train was in a state of the catching of it by John.
The train was caught by John.
John had the state of his catching the train.
John had caught the train.
One opposed somebody and one opposed somebody
up to everyone.
t One and one oppose somebody, up to everyone. -*
Everyone opposed somebody.
Somebody was in the state of the opposing of him by
one
Somebody was opposed by one.
t Somebody was opposed by one and one up to every-
one. -*
Somebody was opposed by everyone.
&amp;quot;Even though the source is a dubious kind of sentence&amp;quot;,
observes H,
. . . it closely fits the meaning and the selection in passive
sentences. Saying N is in the state of S requires that the
sentence S be not merely asserted about N but constitute
a state of N. The source via state explains &amp;quot;among other
things&amp;quot; why we can say Lago di Garda has been visited by
Goethe but not *Goethe has visited Lago di Garda because
27 Even here, I have omitted some details. The arguments for the
&apos;counting&apos; sources of plural quantifiers such as everyone are developed at
length in section 5.5 (pages 244-263). H there refers us to section 7.15
for the derivation of each, any, every from modifiers on the metalinguistic
I say that is the highest operator on a sentence, but I can find no account
of each there or anywhere else in the book. This is the more astonishing
because of the important role of for each in H&apos;s metalanguage. For this
and some other reasons, H&apos;s description of quantifiers seems somewhat
less thoroughly worked out and complete than other parts of the gram-
mar. One may well guess that this area might be more problematic than
most,
</bodyText>
<figure confidence="0.3322128">
Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 209
Book Review A Grammar of English on Mathematical Principles
Lago di Garda still exists and can be said to still have the
state of Goethe&apos;s visiting it, whereas one cannot now say
that Goethe has in the present the state of his visiting the
</figure>
<figureCaption confidence="0.237042">
lake. (p. 17)
</figureCaption>
<bodyText confidence="0.99985556">
A final example is the definite article. H analyses this
not as a modifier or adjunct of the following noun, but
vice-versa: the is itself a noun, a variant of the &apos;indefinite&apos;
noun that,28 to which the following noun with its modifi-
ers is in apposition: That which is a small book fell -*
The small book fe11.29 This is the form of the source for
apposition in general, so that many of its characteristics
are not peculiar to the definite article. The special
restrictions and semantic peculiarities of various uses of
the fall out of the contexts of the underlying that which is.
(Section 5.36 gives details.) For example in When H. W
retired the other day, he had sweet revenge on the alarm
clock which had awakened him at 6 AM daily for 47 years,
. . . where the revenge was on anything that had been
an alarm clock and had awakened him&apos;, (page 243), a
condition that is met by the underlying that which is an
alarm clock. Similarly, with the generic the in the family
doctor is fast disappearing, it is that (of that which is) and
not doctor that is the subject of disappear (loc. cit.).
From these examples, one may perhaps get a sense of
how unfamiliar H&apos;s derivations are — similar in form and
intent to those of Wierzbicka (e.g. her 1982) though
much more parsimonious — and perhaps even of how
tightly interwoven the argumentation for each derivation
is with that for other parts of the grammar.
Some final comments on the book as a product are in
order. It is fitting tribute to the publisher&apos;s craft,
published as it was in John Wiley &amp; Sons&apos; celebratory
175th year. It is well produced, with the attractive layout
and good-quality printing, paper, and binding that one
expects from Wiley. With due consideration for the costs
of publication today, this helps to justify the steep price.
The book is marred by some typographical errors, but
they are generally not confusing. Cross-references and
citations are inconveniently buried in the footnotes that
follow each chapter, so that one must resort to the index
to find them. Because of the complexity of the subject,
and the repetition of particular topics in various
locations, the index is an especially important help to the
reader. It is comprehensive and well arranged, although
one might quibble about certain lapses.3° The book is
well organized and laced with cross-references, showing
the pains that author and publisher have taken to make
this very important work accessible to readers willing to
put similar care into reading it.
Altogether, we have here an impressive analysis of the
grammar of English as a whole, embodying a model of
language and a paradigm for linguistics that is clear,
explicit, and verifiable. Linguists of all persuasions
would do well to study it carefully and on its own terms.
</bodyText>
<subsubsectionHeader confidence="0.235434">
Bruce E. Nevin
</subsubsectionHeader>
<bodyText confidence="0.830582576923077">
Bolt Beranek and Newman Inc.
28 An &apos;indefinite&apos; noun is one with exceptionally broad selection. This
derivation accounts for the being almost always the leftmost prenominal,
and explains precisely what may precede it and why (pp. 237, 263).
29 This in turn is derived from a base sentence containing the requisite
metalinguistic identification of the noun with some explicit or implicit
nearby repetition: A thing mentioned (nearby) — said thing is a small book
— fell. More explicitly: A thing [prior is same as mentioned (nearby)] — a
thing [prior is same as penult] is a small book —fell. The words in square
brackets are interrupting metalinguistic asides about the words of the
sentence themselves, for convenience abbreviated by said in the first
version. In this small and precisely defined metalanguage, prior refers to
the word before the current word (that is, the word before the word prior
itself), and penult refers to the word before the &apos;prior&apos; word. The two
occurrences of thing each refer to the other, yielding the &apos;peculiar self-re-
ferring effect of the restrictive that which&apos; (p. 95). All anaphoric and
epiphoric reference is handled by the same sort of zeroable metalinguistic
sameness statement. So-called crossing coreference, as in The man who
shows he deserves it will get the prize he desires, is not problematic for H&apos;s
reduction rules, since they are not defined in terms of constituent struc-
ture.
30 For example, the range 40-54 for operator indicator (rather than the
individual pages 40 and 54), be as operator on p. 69, everyone on p. 366,
nonrestrictive relative clause on p. 162, zeroing of which is on pp. 201, 238,
zero causative on pp. 317 f, and numerous instances where a well placed f
or If would help to indicate the major entry.
</bodyText>
<page confidence="0.960711">
210 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984
</page>
<figure confidence="0.539217">
Book Review A Grammar of English on Mathematical Principles
</figure>
<sectionHeader confidence="0.805217" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.993968596491228">
Chomsky, Noam. 1964 Current Issues in Linguistic Theory. Mouton,
The Hague.
Corcoran, John 1972 Harris on the Structures of Language. In Plotz
1972.
Gross, Maurice 1973 About the French verb TO KNOW. Studia
Linguistica 122-124.
Gross, Maurice 1975 Methodes en Syntaxe Hermann, Paris.
Gross, Maurice 1979 On the Failure of Generative Grammar. Lg.
55(4): 859-885.
Harris, Zellig S. 1965 Transformational Theory. Lg. 41: 363-401.
Harris, Zellig S. 1968 Mathematical Structures of Language. John Wiley
&amp; Sons, New York.
Harris, Zellig S. 1969 The Two Systems of Grammar: Report and
Paraphrase. TDAP, Philadelphia. Reprinted in Papers in Structural
and Transformational Linguistics (1970). Reidel, Dordrecht.
Harris, Zellig S. 1976 A Theory of Language Structure. Am. Phil.
Quart. 13(4): 237-55.
Harris, Zellig S. 1982 Discourse and Sublanguage. In Kittredge and
Lehrburger 1982, pp. 231-6.
Hirschman, L. and Sager, N. 1981 Automataic Information-Format-
ting of a Medical Sublanguage. In Kittredge and Lehrberger 1982
pp. 27-80.
Hiz, Henry 1979 On some general principles of semantics of a natural
language. Syntax and Semantics 10: 343-352.
Jespersen, Otto 1909-1931 A Modern English Grammar on Historical
Principles. George Allen and Unwin, Ltd., London.
Kendall, Daythat Lee 1977 A Syntactic Analysis of Takeluma Texts.
Ph.D. dissertation, Department of Linguistics, University of Penn-
sylvania, Philadelphia, Pennsylvania.
Kittredge, Richard and Lehrburger, J., Eds. 1982 Sublanguage:
Studies of Language in Restricted Semantic Domain. de Gruyter, New
York.
Langerhans, Ilse 1971 Unrestricted Predicates as Transformational
Sources in German. TDAP 86, University of Pennsylvania, Phila-
delphia.
Munz, Jim 1972 Reflections on the Development of Transformational
Theories. In Plotz 1972, pp. 251-74.
Newmeyer, Frederick J. 1980 Linguistic Theory in America. Academic
Press, New York.
PlOtz, Senta, Ed. 1972 Transformationelle Analyse. Athenaum, Frank-
furt.
Postal, Paul M. 1978 Traces and the Description of English Comple-
mentizer Contraction. L19(1): 1-30
Sager, Naomi 1981 Natural Language Information Processing. Addi-
son-Wesley, Reading, Massachusetts.
Sager, Naomi; Claris, P.; and Clifford, J. 1970 French String Gram-
mar, String Program Reports, No. 8. Linguistic String Project, New
York University, New York.
Salkoff, Morris 1973 Une Grammaire en Chaine du Francais. Dunod,
Paris.
Wheeler, Eric S. 1984 Review of Zellig S. Harris, A Grammar of
English on Mathematical Principles. Computers in the the Humanities
17(3): 88-92.
Wierzbicka, Anna 1982 Why Can You Have a Drink When You Can&apos;t
Have an Eat?. Lg. 58(4): 753-99
Zadeh, L.A. 1965 Fuzzy Sets. Information and Control 8: 338-353.
Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 211
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000140">
<title confidence="0.997528333333333">Book Review A Grammar of English on Mathematical Principles</title>
<author confidence="0.911972">Zellig Harris</author>
<affiliation confidence="0.691689">John Wiley and Sons, New York.</affiliation>
<abstract confidence="0.994475565965583">1982. Pp. xvi, 429. $43.50. This is the most recent and most comprehensive development of Zellig Harris&apos;s formal linguistics. As such, it merits careful study by anyone seriously interested in the scientific study of language, and in particular by anyone working with computers to analyse or use natural language. The intrinsic interest Harris holds for computational linguistics stems chiefly from: • The simplicity and elegance of the mathematical model he proposes for language. Particularly attractive is its freedom from highly abstract hierarchies of grammatical objects and operations subject to change in the next gust of theoretical fashion. • The comprehensiveness of his grammar with respect to the semantic, syntactic, and morphological detail of natural language, as exemplified by English. • The use he makes of the observation that the metalanguage (the language in which the grammar is stated) must of necessity be contained in the object language being described. This is a principle reason approach avoids building the hierarchies of grammatical and semantic mechanisms — and computational representations for them! — that many investigators have come to accept as necessary and even desirable. Of particular interest is his use of language itself to account for the indeterminately numerous and interminably complex issues of the context and use of language (pragmatics and all that). • His partitioning of semantics into &apos;objective information&apos; versus communicative and expressive nuance, relying only on formal linguistic criteria. • His notion of sublanguage; in particular, the sublanguage generated by his base, which is free of paraphrase yet informationally complete (albeit at the cost of being for most utterances &apos;unspeakably&apos; cumbersome in style). • His linking of &apos;reductions&apos; (approximately, transformations) to points of informational redundancy in discourse. • His identification of affixes and most prepositions as &apos;argument indicators&apos; and &apos;operator indicators&apos;, and his exploitation of them as providing traces of derivation. Together, these characteristics suggest an approach to computational parsing and synthesis that could be both highly efficient and semantically sensitive. Beyond that, they indicate avenues for design of artificial languages and language-like systems that have yet to be tried. Somewhat less than the first half of the book (chapters 1-3, pages 1-185) presents H&apos;s model of language. The remaining 228 pages (chapters 4-9 plus the Appendix) restate the categories and phenomena of traditional grammar in some detail as both a demonstration and a test of that model.&apos; It is a densely written book, and astonishingly comprehensive. Every page is filled with information and insights enough to be expanded to a journal article, reflecting a grasp of syntactic and semantic data that is of extraordinary depth and breadth, both synchronically and diachronically. The reader should not assume that H neglects a particular problem simply because he does not summarize his solutions under a label, such as Neg On the contrary, even readers who are loathe seriously to entertain an alternative paradigm in linguistics will profit from study of this encyclopedic restatement of grammar. There are a number of reasons, to be sure, why scholars may prefer to ignore H&apos;s work. Many readers have found H&apos;s prose tough sledding. As Jane Robinson once observed (pc): I an idea what he&apos;s talking about, I can understand him. As someone said of Quine, once you&apos;ve understood what he means, you realize he couldn&apos;t have said it any other way. Harris is that way for me. It&apos;s just that what he&apos;s trying to say is difficult. Let those readers who are for this reason reluctant to tackle another Harris opus be assured. The writing here, in addition to being explicit and unambiguous, as always, is also quite clear and straightforward. Even in his justifications of the more complex derivations, where what he is trying to say is indeed difficult, one&apos;s attention span need be only somewhat longer than usual to hold the 1Jespersen was reputedly H&apos;s principle guide for this second portion of book. In his preface, I-1 cites Jespersen&apos;s monumental English on Historical Principles together with the as &amp;quot;the two indispensable aids on the English language&amp;quot;. The parallelism titles is surely no Secident. I believe that culled the literature to ensure that he was accounting for all the examples that other linguists had found for one reason or another to be problematic. 2Another pitfall for the unwary reader is, ironically, a consequence of H&apos;s long-standing rhetorical practice of repeating key information in form so that each or section is relatively autonomous. He does this in an effort to make his writings easier to read and use. However, a superficial reader might first encounter a controversial topic example, H&apos;s proposed bisentential metalinguistic operator source of or) such a recapitulation, and reject it out of hand, having missed the main section where it is properly developed and justified. Unfortunately, the editors have failed to indicate the most important entry where the index has several, so that one must cross-check the page numbers of the index against those of the table of contents to find the main entry. Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles strands of argumentation in mind. Indeed, complex argumentation can be no stop to any student of generative linguistics! No, remaining difficulties for the reader will be due to the unfamiliarity of the approach, which H himself cites (page vi) as his reason for discussing some &apos;methodological considerations and grammatical constructions&apos; redundantly in more than one section of the book. More serious is the question of familiar jargon and shared terms for debate. As I suggested above, he does not use the jargon of generative linguistics. In a field, and indeed in a social climate, where there is great competition to be &apos;more state of the art than thou&apos;, the obvious assumption is that H has simply not kept up. Not to beat around the bush, the central issue in any evaluation of H&apos;s work in linguistics is that it has been eclipsed by that of his most famous student, Noam Chomsky. More especially, it has been obscured by the spirit of polemic that has consumed the field of linguistics in this country for the past twenty or twenty-five years. H is both stubbornly unpolemical and stubbornly autonomous. These characteristics have unfortunately contributed to his contributions going largely unnoticed in the United States (though not abroad). Perhaps it is time to allow the curtain to fall on this Oedipal drama and evaluate H&apos;s work on its merits. After all, H&apos;s isolation from &apos;mainstream linguistics&apos; has also had its advantages. Undistracted by academic politics and the demands of cross-paradigmatic refutations, he has been painstakingly &amp;quot;formulating as a mathematical system all the properties and relations necessary and sufficient for the whole of natural language [rather than] investigating a mathematically definable system [such as phrase-structure grammar] which has some relation to language, as being a generalization or subset&amp;quot; (Harris 1968:1). He now offers results that linguists and computational linguists should pay close attention to, at a time when many far less well-developed alternatives rush to fill a perceived theoretical vacuum in the field. The limitations of a review preclude a detailed comparison of H&apos;s model with more familiar generative models, but a sketch at least is in order. In this review, I refer to the Harrisian model of language as &apos;constructive grammar&apos; and to the Harrisian paradigm for linguistics as A grammar at least the following six characteristics: 1. The semantic primes are words in the language, a base vocabulary that is a proper subset of the vocabof the language as a 2. Generation of sentences in the base is by word entry, beginning with entry of (mostly concrete) base nouns. The only condition for a word to enter is that its argument requirement must be met by some previously entering word or words, generally the last entry or entries, which must not already be in the argument of some other word. The base vocabulary has thus a few simple classes of words: base nouns with null argument operators requiring base nouns as arguments requiring operators as arguments requiring combinations of operators and base nouns In addition to these classes, almost all of the operators require morphophonemic insertion of &apos;argument such as were termed the &apos;trace&apos; of &apos;incremental transformations&apos; in Harris 1965 and 1968.) The base generates a which is informationally complete while containing no paraphrases. This is at the expense of redundancy and other stylistic awkwardness, so that utterances of any complexity in the base sublanguage are unlikely to be encountered in ordinary discourse. As in prior reports of H&apos;s work, base sentences are all assertions, other forms such as questions and imperatives being from underlying performatives ask, I the like. 4. A well-defined system of reductions yields the other sentences of the language as paraphrases of base sentences.° The reductions were called the &apos;paraphrastic transformations&apos;, and &apos;extended morphophonemics&apos; in earlier reports. They consist of of words (movement), and morphophonemic changes of phonological shape. Each reduction leaves a &apos;trace&apos; so that the underlying 3`Harrisian&apos; imputes to Harris some obscure responsibility for work that others do in this paradigm. H&apos;s use of &apos;constructive grammar&apos; (e.g. Harris 1968, pp. 17f, 20, 32, 89, 121) is apparently allied with intuitionist usage in mathematics. The term &apos;constructive&apos; provides a counterpoise to the (equally irrelevant) rhetorical overtones that have grown up around the term &apos;generative&apos; as a kind of trademark. A constructive grammar such as the one under review is of course generative in the technical sense of producing an explicit structural description for every sentence of the Furthermore, at least since his 1965 report in has been explicitly concerned with generative rules for deriving sentences from base structures. (Newmeyer 1980:37 misinterprets Corcoran 1972:279 on this point: he ignores Corcoran&apos;s further discussion in that paper, as well as H&apos;s own later writings, in particular section 8.8 of Harris 1969, Corcoran&apos;s principal reference). This notwithstanding, few would claim that H is a generative grammarian, and a correlative term other than &apos;Harrisian grammar&apos; seems in order. 4The suggestion outlined in Harris (1969), and exemplified in Gross (1973), that some of the base vocabulary may be suppletives factored from partial homonyms and synonyms, is not taken up systematically in this book. Echoes of this program may be found here and there, however, for example in H&apos;s observations on the derivation of -dom, once free words in English (suppletive forms of something like Note here also the discussion (p. 73) of classifiers in the sublanguage of a science, in particular the sublanguage of grammar (which is the metalanguage of the language as whole). 5A sublanguage is a subset of sentences in the language, mathematically defined by closure under a subset of the operations that are defined for the language as a whole (cf. Harris 1968, e.g. p. 152). The notion of sublanguage, with its semantic interpretation of subject-matter specialization, runs somewhat counter to rationalist notions of innate ideas and may be difficult to characterize in generativist terms. Nevertheless, sublanguages are as important as social and regional dialects for an understanding of language change, and are essential to an understanding of semantics of natural language as opposed to formal languages. Kittredge and Lehrburger (1982) provides a brief survey. Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles redundancies of the base sublanguage are Linearization of the operator-argument dependencies — in English either &apos;normal&apos; SVO or a `topicalizing&apos; linear order — is accomplished by the reduction system, not the base. The reduction system includes much of what is in the lexicon in generative grammar (cf. Gross 1979). 5. Metalinguistic information required for many reductions, such as coreferentiality and lexical identiis expressed language by conjoined metalanguage sentences, rather than by a separate mechanism such as Similarly, &apos;shared knowledge&apos; contextual and pragmatic information is expressed by conjoined sentences (including ordinary dictionary definitions) that are zeroable because of their redundancy. 6. The set of possible arguments for a given operator (or vice-versa) is graded as to acceptability. These gradings correspond with differences of meaning in the base sublanguage, and thence in the whole They diverge in detail from one sublan- 6Weak paraphrases: see below. 7Zeroing is similar to deletion with a condition of recoverability (but contrast the Constructive treatment at pp. 50 ff. and fn. 10 with Chomsky (1964), which introduced the latter notion). The term &apos;deletion&apos; is often applied to &apos;replacement&apos;, as in pronominalization. This and other points of difference between the two terms seem to be an artifact of generative derivations being defined on abstract phrase-structure trees and Constructive derivations being defined on lexical dependencies. For an example of problems with deletion as defined on nodes of phrase-structure trees, see for example Postal (1978), pp. 10-11, and ref. cit. For Postal (p. 23), there is no way to formulate in a transformational framethe fact that `to contraction&apos; — phonetic reduction of the verb forms going, have, ought, used, got, — determined by &apos;subject sharing&apos;, though that is precisely how H&apos;s transformational grammar did and his constructive grammar does account for these phenomena. 8These traces, and those noted under 3 above, were the basis for reformulation of transformations as elementary sentence-differences, a crucial step in development of the constructive model. They are of course unrelated to the trace marker proposed in Chomsky (1973), which, as Gross (1979: 874) points out, is simply a computational device, a pointer to a memory address, such as is found in programming languages. 9See Harris (1968: 17 ff.) for reasons why the metalanguage must be within the language. This property of language of course does not in itself preclude use of other types of notation for the metalanguage. Convenient notational reductions of other specialized sublanguages to formal symbol systems come to mind, as for example the notational systems of logic and mathematics, whose most complex formulae are nevertheless always stateable somehow in sentences of the corresponding sublanguages. The motivation for such a notational system (computational convenience) is offset here by El&apos;s demonstration that metalinguistic information is normally zeroed under reductions defined for the language as a whole. There is also the open question whether the presence of low-information and metalanguage conjuncts in the base sublanguage constrains the reduction system, and whether subscripts and other computationally convenient notations might therefore give rise to some thorny problems as an artifact (see note 21 below). Furthermore, since sentences containing overt (unzeroed) assertions of metalinguistic information do occur in normal discourse, and cannot be excluded as ungrammatical, any grammar must account for them whether it uses them in this way or not, so it might as well use them. I°See Zadeh 1965. ISee also the discussion of analogic extension, below. guage or subject-matter domain to another. Equivthe fuzzy of &apos;normal&apos; co-occurrents for a given word differs from one such domain to another within the base sublanguage. In informal, intuitive terms, a constructive grammar generates sentences from the bottom up, beginning with word entry, whereas a generative grammar generates sentences from the top down, beginning with the abstract symbol S. The grammatical apparatus of constructive grammar (the rules together with their requirements and exceptions) is very simple and parsimonious. H&apos;s underlying structures, the rules for producing derived structures, and the structures to be assigned to surface sentences are all well defined. Consequently, H&apos;s argumentation about alternative ways of dealing with problematic examples has a welcome concreteness and about it. In one may directly assess the semantic well-formedness of base constructions and of each intermediate stage of derivation, as well as the sentences ultimately derived from them, because they are all sentences. By contrast, in generative argumentation, definitions of base structures and derived structures are always subject to controversy because the chief principle for controlling them is linguists&apos; judgments of semantic relations (such as paraphrase) among sentences derived from them. Even if one could claim to assess the semantic well-formedness of abstract underlying structures, these are typically so ill-defined as to compel us to rely almost totally on surface forms to choose among alternative adjustments to the base or to the system of rules for derivation. And as we all know, a seemingly minor tweak in the base or derivation rules can and usually does have major and largely unforeseen consequences for the surface forms generated by the grammar. H has always given primacy to semantics over syntax. Even in his earlier structural linguistics, H&apos;s distributionalism was a study of semantics and not the empty taxonomy of the &apos;structuralist&apos; stereotype; H&apos;s two empirical touchstones, contrast and differential acceptability, are both semantic notions; and H&apos;s aim in reducing redundancy in grammar (see below) is to get syntax out of the way of semantics. For Chomsky, on the other hand, syntax has always been central and semantics must be effected by a separate interpretive mechanism. Munz (1972) sketches the history of this divergence of the two paradigms. The mention of paraphrase under characteristic 4 may trouble some readers. The use of judgments of paraphrase as a criterion for grammatical relationship has given rise to endless confusion and dissension in linguistics. This is because paraphrase in any strong sense is an exceedingly rare phenomenon in natural language. For paraphrase (&apos;weak paraphrase&apos;) is rather an interpretathe fact that the semantic dependencies of the base are preserved by (are recoverable under) the reductions. The reductions in turn are defined with Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles respect to the more primitive notion of graded acceptability (characteristic 6). On first exposure to H&apos;s system, readers are frequently puzzled as to the motivation of characteristic 6, and are most likely to object to stylistically peculiar sentences postulated as sources (and as intermediate constructions in certain derivations) under characteristics 3 and 5. Characteristic 6 was of course previously H&apos;s criterion and gives him a metric for &apos;weak paraphrase&apos;: preservation of the objective information being transmitted, with expressive and communicative attributed to the reductions. This metric works follows: Supposing two sentence A and be related by some derivational step in H&apos;s grammar, if any two sentences among the satisfiers of sentence form A differ on some scale of acceptability, the correspondsentences among the satisfiers of sentence-form differ in the same The relation to information content follows from the following observation: given a base sentence with a valid variable (N or one of the subclasses of 0) in place of one of its words, the base sentences resulting from various valid substitutions of words for the variable are not all equally sayable, and this is a direct reflection of the meanings of the words. By this means, then, Harris&apos;s grammar captures what the dust jacket refers to as &apos;weak without the cost of a separate semantic component. H&apos;s grammar is the product of long evolution, rather than of sudden revolution. His method of improving his grammar is to extend existing, established reductions to new argument domains, rather than to throw out the previous version of the grammatical apparatus for a revolutionary new one. (See Sager (1981) for examples of this within an earlier form of H&apos;s paradigm.) His method mirrors the way language itself changes over time, and indeed incorporates the mechanism of language change 12It was an alternative to well-defined selection or &apos;cooccurrence sets&apos; as a criterion for two sentence-sets to be transforms in 1957, and the only criterion after I-1 determined that simple well-defined co-occurrence did not stand up, as he reported in his 1965 paper. 13These more subtle aspects of meaning, which are more difficult to characterize formally, are thus segregated for study together with other essentially gestural systems of communication, as distinct from transmission of objective information. In this way, I-1&apos;s metric provides a principle by which to control the more baroque excesses of abstract syntax without giving up the power of the base to generate underlying semantic structures directly. 14A sentence form is a sequence of variables and constants, where the variables are word-classes and the constants are particular words or such as those of the sequence . . -en by passive sentences. Any n-tuple of words that may more or less acceptably be substituted for n-tuple of variables constitutes a the sentence form. 15A seeming exception is where a sentence form itself— that is, all of its — has reduced acceptability. (These sentences H marks with a dagger.) However, even where some acceptability differences collapsed and become difficult for linguists and their informants (or experimental subjects) to access, the corresponding sentences are never their positions on the given scale of acceptability, but at most are only reduced to identity as &apos;equally marginal&apos;. by analogic extension. H&apos;s formulation of analogic extension in his reduction system accounts in a unified way for synchronically productive analogic extension, such as metaphor, as well as for diachronic change. Then, because constructions that are at first difficult to account for turn out typically to be at the diachronic or analogic &apos;growing edge&apos; of the language, reachable by refinement and extension of the domain of well-attested operations and reductions, the same mechanism provides an elegant solution to the sticky problem of the ill-defined and shifting boundaries of language. As an added dividend, his derivations of many constructions fit their Analogic extension is possible, indeed quite natural, when rules are defined in string-grammatical terms, and when the base generates a subset of the language. H&apos;s grammar works from the inside out, as it were, generating a subset and extending to include the rest of the language. The generative paradigm works rather from the outside in: an excessively powerful system of rules generates a superset of objects containing the set of sentences, and a major task is pruning and filtering; degrees of grammaticality and degrees of acceptability (the two are often not distinguished) are problematic rather than being fundamental data of linguistics; and dynamic processes of metaphor, analogy, and language change are difficult to integrate with one another and with synchronic descriptions of languages. 16There is no mention of this &apos;weak semantics&apos; in the book proper, though it does occur in Hiz (1979). This &apos;weak semantics&apos; must be construed as a form of &apos; propositional meaning&apos; with the proviso that a great deal of pragmatics, implied meaning, and the like, may appear as conjoined &apos;common knowledge&apos; sentences that are zeroable because so utterly redundant (characteristic 5). This makes sense only in context of knowledge base, a theme to which I will revert at of this review. Examples of H&apos;s treatment of some familiar semantic problems follow this comparison of the two paradigms. 17See e.g. pp. 27-28 and 377-78 for discussion. However, H&apos;s derivation 351 and n 7) could be improved with more historical perspective. He suggests: request_you a special reduction, on expectabilgrounds (3.55) of a secondary me: thereby_you please me the like. Better would be: I request [you] that you go, if you would please me! Go, if you would please me! Go, if you please! Go, please! -&gt; Please go! I am not certain what support there is in H&apos;s system for the permutation at the end of this derivational sequence; the last, commaless sentence does the lowered intonation that in the preceding sentence. Contrast the derivation of the corresponding sentence with a comma: If you would please me, I request [you] that you go! If you would please me, go! If you please, go! Please, go! Nuances of deference, irony, and the like associated sometimes with more often with you please historically from normally construed context of a superior granting a boon and thereby pleasing an inferior. Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles Put another way, H&apos;s reports and books since 1955 have always presented a complete grammar with a program for refinement and extension. His work cannot be understood and properly evaluated using critical tools that are appropriate for generative grammatical writings, which with few exceptions present a fragment of grammar with a program for either generalizing it or integrating it with other like proposals. A crucial difference between constructive grammar and generative grammar is the importance H has always assigned to freeing his system as much as possible from redundancy. [The] strong connection between the grammatical form and the meaning is achieved by two analytic methods: first, by recognizing as many as possible of the regular reductions (and morphophonemic variants) that produce changed forms without changing their information; second, by keeping the grammatical description as unredundant as possible so that the essential redundancy of language as an information-bearing system (which consists in the redundancy of the dependence relation and of selection) not be masked by further redundancy in the description itself. . . [O]ne must recognize every new term or category or subclass that is not derivable from the primitives of the system, and also every rule, including every limitation on the carrying-out of a rule, and every ad hoc explanation[,] is a redundancy of the description. [pp. 10-111 This has not been a major concern in the generative indeed, with a phrase-structure base, there is no principled way to control the proliferation of nonterminal symbols or to control the structure of the apparatus of the grammar in general, as for example in derived constituent structure. It is therefore almost inevitable in generative grammatical work that artifactual, extrinsic structure intrudes upon and masks the intrinsic structure of the language that the grammatical apparatus seeks to explain. It follows that some putative language universals may be artifacts of generative grammatical apparatus, found in various languages just because the same apparatus is used, much as the padres of old wrote grammars of Latin with exotic vocabularies. Although universals are not H&apos;s explicit concern — presumably, he would determine them empirically after working out the transformational structure of other languages in constructive terms rather than making them a prerequisite for such work — one may draw some conclusions about them from this book. Surely, the very 18H&apos;s reduction of redundancy has in its intent some relation to the notion of simplicity in generative grammar, but is both more sensitive and intuitively more satisfying than gross measures of formalisms, such as symbol-counting.</abstract>
<note confidence="0.752837">19For Korean, see e.g. Harris (1968:109-113). For French, see Sager (1981: 10.2); Sager, Claris, and Clifford (1970); and Salkoff (1973). For German, see Langerhans (1981). For Takelma, see Kendall (1977). 20Except insofar as H&apos;s indefinites may be thought of as classifiers with</note>
<abstract confidence="0.996603852601156">the broadest selection. There is some parallelism between the notion of a `designated representative&apos; of a category introduced in Chomsky (1964) and H&apos;s use of indefinites where a zeroed argument is not specified by context. simple information-transmitting dependency structure of H&apos;s base is universal, and having been found for English (and in some measure for French, German, Korean, Takelma, and other languages) may now be presumed as an objective of work in other languages. And surely the major types of reduction involving permutation and zeroing, such as his length permutation, include many universals, though details of the &apos;morphophonemic&apos; types reduction are more likely to be language Comparison with generativist findings of semantic universals is more problematic. Differences of acceptability or likelihood among the arguments of a given operator, or among the operators (or operators and co-arguments) for a given word, are notoriously variable and vague. For H, selection restrictions within the major dependency classes of base nouns and operators form at best fuzzy subsets that are inherently beyond the reach of the binary semantic features of interpretive semantics. To be sure, there are some hard yes/no selection restrictions that can be successfully encoded by binary semantic features, but these are produced by the reduction system (cf. Harris 1976:251 for a succinct statement), and therefore for H are not a problem of lexicon at all. does little with the classifier that are explicitly part of the vocabulary — the hierarchies of words that make up the taxonomies, both folk and scientific, that were the original motivation for componential analysis and semantic features. Classifier words among the operaan example) go unmentioned except for the &apos;appropriate&apos; operator needed for the sources of certain derived nouns (5.25, p. 224). H does show how novel utterances may be related to familiar ones of known acceptability by substituting classifier words, as in the following example from page 6: Some blue and mauve onion-skin shot through the air at 759.06 miles per second. Substituting classifier words: Some colored solid object shot through the air at a particular velocity; blue and mauve are colors; onion-skin is a solid object; 759.06 miles per second is a particular velocity. Some blue and mauve colored solid object consisting of onion-skin shot through the air at a velocity of 759.06 miles per second. It is not clear to the reviewer at least whether H can find further well-established reductions to derive the first sentence above from the last one (that appears not to be his aim in citing this example). Note, however, that a similar mechanism underlies Sager&apos;s computer formatting of sublanguage discourse (Sager 1981, 1982; Hirschman and Sager 1981). Some classifier words presumably semantic universals (e.g. human, object) so that such use of classifiers could be a device for machine translation. One may hope that H or another Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles worker in the constructive paradigm will take up these undeveloped themes in some future report on the lexicon for a constructive grammar. Presumably, the next important phase of H&apos;s work includes a specification of the lexicon for his grammar and a return to discourse analysis from the point of view of operator dependencies. Characteristic 6 suggests that the form of the lexicon of a constructive grammar differs in important ways from that of other approaches. Obviously, much that has exercised students of generative lexicography is captured by Harris&apos;s reduction system. Even more obviously, there is no need to map the underlying base sentences onto some semantic representation, since the base sentences are themselves the semantic representation. Finally, much of the formalism of the generative lexicon, motivated as it is by characteristics of phrase-structure grammar, has no place Lacking H&apos;s specification, I suppose that the lexicon of a constructive grammar would list a subset of words and affixes (the base vocabulary of N and operators, and the argument indicators and traces of reductions), together with their form class, if any. (It might also list some words that are products of particular reductions, together with their derivations, depending upon how much one wanted to include for pragmatic reasons relating to computability.) For the acceptabilities, one could start with an &apos;acceptability model&apos;, a finite set of base sentences — 21For computer work, it might seem convenient to introduce some special notational conventions, such as subscripts for metalinguistic identity statements preserving a straightforward formal relationship to the used to express within the language, as above under characteristic 5. Such a program is questionable, however, insofar as the presence of such zeroable sequences in sources may interact with the normal restrictions defined for the reductions and thus impose some significant constraints on derivations. (There is some evidence for this in certain island phenomena.) In a system with metalanguage statements of coreference and the like represented in a separate notation outside the language, these constraints have to be restated in the form of special rules or exceptions to rules, introducing extrinsic redundancy precisely of the sort that we wish to avoid. 22See for example Sager (1981), Gross (1975, 1979), Salkoff (1973), and references cited in those works. 23The first three examples were suggested by Wheeler 1984. 24With other verbs, such as stood the box on end, The tree in the forest, fHe stood the tree in the forest, up a causative source (Zero Causative, 6.8). In both cases, the higher verb imposes restrictions on the derived form, such as we see reflected in the limited acceptability of the sentence just cited with a dagger t (which might occur acceptably, forexample, in a myth or folk tale). These restrictions betray the zeroed presence of the higher verb and indicate which form is primitive and which derived. Following H&apos;s practice, I am using a dagger to mark a sentence of reduced acceptability that is still grammatical. It marks relatively lower acceptability of marginal but still sayable sentences, rather than indicating their position on some absolute scale of acceptability. In some of the examples that follow where one sentence in a derivation is stylistically strange and another is even more difficult on semantic grounds, sayable only in some enabling special context, the dagger marks only the one with lowest acceptability to indicate the contrast. Note that the acceptability of such sentences is reduced precisely because the less redundant and more conventional reduction is available and obvious. actual sequences of words — that are known to be acceptable for a given subject-matter domain. These sentences would be of three sorts. The primary ones for establishing acceptabilities would use only classifier nouns, as in the onion-skin example. There would be a set of &apos;dictionary sentences&apos; for each classifier N giving acceptfor its co-argument N under compactness, many of these sentences could have classifiers in both argument positions. These acceptabilities are welldefined, or nearly so, only for the classifier words in sublanguages of science.) The grammar would draw upon these dictionary sentences to extrapolate the fuzzy set of acceptable arguments for a given operator, or of operators (and, where required, co-argument N) for a given N (see Harris 1976, p. 247 and fn 13). Metaphor and analogic extension fall naturally out of the same process. Finally, another type of dictionary sentence would be needed to meet the word-sharing requirement under conjunction that distinguishes coherent discourse from incoherent strings of sentences (pp. 13, 163-4; also Harris 1976, p. 241 and fn. 6; 1982, pp. 232-3). These &apos;shared knowledge&apos; sentences are normally zeroed since they add only information that the hearer is presumed already to have. The three sets of acceptability-model sentences would be unique for each definable sublanguage, presumably with greater overlap between related subject-matter domains and with some shared &apos;common usage&apos;, and could function as a knowledge base for particular computer applications. Much highly successful work has been done with computer grammars using string-based parsers of an Harrisian The present work suggests that a different sort of parser might use the predetermined constructive dependency class of each word, the operator indicator for each operator, and the traces of the reductions, as &apos;handles&apos; to facilitate the parsing process. Such an approach was not feasible before the precise form and interrelationship of the base and the reduction system were determined. It should make possible computer grammars that are both more efficient and more semantically sensitive than any now implemented. As I observed above, this is a complete grammar, and one whose internal integrity and parsimony is such that we examine individual fragments of the grammar out of context at our peril. With that caveat, here are some examples drawn from the book in the interest of giving more substance to the general remarks above. the familiar semantic that H covers is the middle voice (pp. 368-69) which we may exemplify only the latter may become seemintransitive window broke, *The window hit). H derives the intransitive form of verbs like break from a zeroable higher operator that strongly selects a subject to the object of the lower The window underwent one&apos;s breaking of it. The window underwent breaking. -* The window broke. Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles The window underwent one&apos;s hitting of it. -1. t The window underwent hitting. * The window hit. We may exemplify a second classic problem of semananalysis by the relation between found Pegasus the existence of Pegasus, sought Pegasus not. in the dependclass and its apparent status here as is due zeroing of the lower operator The source of the sentence is thus sought to find Pegasus, asserted (under the say on the sentence), but not asserted, and hence its presupposition about the existence of Pegasus is not A third familiar semantic problem is the scope of quantifiers. For H, quantifiers are not semantic primitives, but are reductions from complex sources: . . occurrences of quantifiers (including the numbers) can be derived within the framework of the present analysis by two assumptions about their source. One is that they are the second argument of an operator, as (up) to, is shown in the scale construction, . . . The other is that the plural and plural are cases of this up to on and. passive constructions was opposed by everywas opposed by someone how H handles questions of quantifier scope. To understand them, we will first have to look at H&apos;s treatment of the passive. The first transformation isolated in the late 1940s, the passive has generally been assumed to be a single (paraphrastic) transformation. Many anomalous examples of passives have raised questions about the semantic properties of transformations in general. However, each of the three physical components of passive sentences occurs elsewhere in the grammar. The first of these components is the apparent permutation of object and subject. H argues that the object, rather than being permuted, is the subject of a higher verb of the semantic set that make their subject a recipi- (including need, undergo). elsewhere in the (such as the above), when this higher verb operates on a sentence containing an object N metalinguistically identified with its subject, the redundant lower object may be zeroed: He had a scolding of him by me. -• He had a scolding by me. 25In some contexts, perhaps in the argument of some other operator. Note that one may seek to do anything, but in most contexts only finding is so predictable as the purpose of one&apos;s seeking that the verb zeroable under 26In Harris 1976, argues that problems of intension vs. extension do not arise in his inscriptional approach to semantics, as they do in the more familiar interpretive and set-theoretic approaches. second component of the passive is . . . -en the original The with stative in the . . . -en construction, as well as in the diachronically moribund construction seen example in is risen. takes these to be occurrences of the same suffix, a suppletive form of underlying be in a state: He is in the state of his scolding by me. third component is another prepoas in interested in, is tired of, is tired from) the original subject. The other preposition) in nominalizations such as chopping of the trees by the settlers. H shows that the domain and selection of the passive is precisely the product of the domains and selections of these components. As a bonus, he thereby accounts for those semantic idiosyncrasies that have made the passive problematic. The full discussion defies brief paraphrase, the following illustrate the point: The house was in a state of the building of it by a farmer. The house was built by a farmer. The train was in a state of the catching of it by John. The train was caught by John. John had the state of his catching the train. John had caught the train. One opposed somebody and one opposed somebody up to everyone. t One and one oppose somebody, up to everyone. -* Everyone opposed somebody. Somebody was in the state of the opposing of him by one Somebody was opposed by one. t Somebody was opposed by one and one up to everyone. -* Somebody was opposed by everyone. &amp;quot;Even though the source is a dubious kind of sentence&amp;quot;, observes H, . . closely fits the meaning and the selection in passive Saying in the state of that the not merely asserted about constitute state of source via &amp;quot;among other why we can say di Garda has been visited by not has visited Lago di Garda 27Even here, I have omitted some details. The arguments for the sources of plural quantifiers such as developed at length in section 5.5 (pages 244-263). H there refers us to section 7.15 the derivation of any, every modifiers on the metalinguistic say is the highest operator on a sentence, but I can find no account or anywhere else in the book. This is the more astonishing of the important role of each H&apos;s metalanguage. For this and some other reasons, H&apos;s description of quantifiers seems somewhat less thoroughly worked out and complete than other parts of the grammar. One may well guess that this area might be more problematic than most, Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles Lago di Garda still exists and can be said to still have the state of Goethe&apos;s visiting it, whereas one cannot now say that Goethe has in the present the state of his visiting the lake. (p. 17) A final example is the definite article. H analyses this not as a modifier or adjunct of the following noun, but itself a noun, a variant of the &apos;indefinite&apos; to which the following noun with its modifiis in apposition: which is a small book fell -* small book This is the form of the source for apposition in general, so that many of its characteristics are not peculiar to the definite article. The special restrictions and semantic peculiarities of various uses of out of the contexts of the underlying which is. 5.36 gives details.) For example in H. W retired the other day, he had sweet revenge on the alarm clock which had awakened him at 6 AM daily for 47 years, . . the revenge was on anything that had been an alarm clock and had awakened him&apos;, (page 243), a that is met by the underlying which is an clock. with the generic family is fast disappearing, is which is) is the subject of (loc. From these examples, one may perhaps get a sense of how unfamiliar H&apos;s derivations are — similar in form and intent to those of Wierzbicka (e.g. her 1982) though much more parsimonious — and perhaps even of how tightly interwoven the argumentation for each derivation is with that for other parts of the grammar. Some final comments on the book as a product are in order. It is fitting tribute to the publisher&apos;s published as it was in John Wiley &amp; Sons&apos; celebratory 175th year. It is well produced, with the attractive layout and good-quality printing, paper, and binding that one expects from Wiley. With due consideration for the costs of publication today, this helps to justify the steep price. The book is marred by some typographical errors, but they are generally not confusing. Cross-references and citations are inconveniently buried in the footnotes that follow each chapter, so that one must resort to the index to find them. Because of the complexity of the subject, and the repetition of particular topics in various locations, the index is an especially important help to the reader. It is comprehensive and well arranged, although might quibble about certain The book is well organized and laced with cross-references, showing the pains that author and publisher have taken to make this very important work accessible to readers willing to put similar care into reading it. Altogether, we have here an impressive analysis of the grammar of English as a whole, embodying a model of language and a paradigm for linguistics that is clear, explicit, and verifiable. Linguists of all would do well to study it carefully and on its own terms.</abstract>
<author confidence="0.994436">Bruce E Nevin</author>
<affiliation confidence="0.966967">Bolt Beranek and Newman Inc.</affiliation>
<abstract confidence="0.97740328">28An &apos;indefinite&apos; noun is one with exceptionally broad selection. This accounts for almost always the leftmost prenominal, explains precisely what may precede it and why (pp. 237, 29This in turn is derived from a base sentence containing the requisite metalinguistic identification of the noun with some explicit or implicit repetition: thing mentioned (nearby) — said thing is a small book fell. explicitly: thing [prior is same as mentioned (nearby)] — a [prior is same as penult] is a small book —fell. words in square brackets are interrupting metalinguistic asides about the words of the themselves, for convenience abbreviated by the first In this small and precisely defined metalanguage, to word before the current word (that is, the word before the word and to the word before the &apos;prior&apos; word. The two of refer to the other, yielding the &apos;peculiar self-reeffect of the restrictive which&apos; 95). All anaphoric and epiphoric reference is handled by the same sort of zeroable metalinguistic statement. So-called crossing coreference, as in man who he deserves it will get the prize he desires, not problematic for H&apos;s reduction rules, since they are not defined in terms of constituent structure. 30For example, the range 40-54 for indicator than the pages 40 and 54), operator on p. 69, p. 366, relative clause p. 162, zeroing of is pp. 201, 238, causative pp. 317 f, and numerous instances where a well placed f or If would help to indicate the major entry.</abstract>
<note confidence="0.856524847457627">Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles References Noam. 1964 Issues in Linguistic Theory. The Hague. John 1972 Harris on the Structures of Language. 1972. Maurice 1973 About the French verb TO KNOW. Maurice 1975 en Syntaxe Paris. Maurice 1979 On the Failure of Generative Grammar. 55(4): 859-885. Zellig S. 1965 Transformational Theory. 363-401. Zellig S. 1968 Structures of Language. Wiley &amp; Sons, New York. Harris, Zellig S. 1969 The Two Systems of Grammar: Report and TDAP, Philadelphia. Reprinted in in Structural Transformational Linguistics (1970). Dordrecht. Zellig S. 1976 A Theory of Language Structure. Phil. 237-55. Harris, Zellig S. 1982 Discourse and Sublanguage. In Kittredge and Lehrburger 1982, pp. 231-6. Hirschman, L. and Sager, N. 1981 Automataic Information-Formatting of a Medical Sublanguage. In Kittredge and Lehrberger 1982 pp. 27-80. Hiz, Henry 1979 On some general principles of semantics of a natural and Semantics 10: Otto 1909-1931 Modern English Grammar on Historical Allen and Unwin, Ltd., London. Kendall, Daythat Lee 1977 A Syntactic Analysis of Takeluma Texts. Ph.D. dissertation, Department of Linguistics, University of Pennsylvania, Philadelphia, Pennsylvania. Richard and Lehrburger, J., Eds. 1982 of Language in Restricted Semantic Domain. Gruyter, New York. Langerhans, Ilse 1971 Unrestricted Predicates as Transformational Sources in German. TDAP 86, University of Pennsylvania, Philadelphia. Munz, Jim 1972 Reflections on the Development of Transformational Theories. In Plotz 1972, pp. 251-74. Frederick J. 1980 Theory in America. Press, New York. Senta, Ed. 1972 Analyse. Frankfurt. Postal, Paul M. 1978 Traces and the Description of English Comple- Contraction. 1-30 Naomi 1981 Language Information Processing. Addison-Wesley, Reading, Massachusetts. Sager, Naomi; Claris, P.; and Clifford, J. 1970 French String Grammar, String Program Reports, No. 8. Linguistic String Project, New York University, New York. Morris 1973 Grammaire en Chaine du Francais. Paris. Eric S. 1984 Review of Zellig S. Harris, Grammar of English on Mathematical Principles. Computers in the the Humanities 17(3): 88-92. Anna 1982 Why Can You a Drink You Can&apos;t an Eat?. Lg. 753-99 L.A. 1965 Fuzzy Sets. and Control 338-353. Linguistics, Volume 10, Numbers 3-4, July-December 1984</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Current Issues in Linguistic Theory.</title>
<date>1964</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="13730" citStr="Chomsky (1964)" startWordPosition="2163" endWordPosition="2165">ed knowledge&apos; contextual and pragmatic information is expressed by conjoined sentences (including ordinary dictionary definitions) that are zeroable because of their redundancy. 6. The set of possible arguments for a given operator (or vice-versa) is graded as to acceptability. These gradings correspond with differences of meaning in the base sublanguage, and thence in the whole language. They diverge in detail from one sublan6 Weak paraphrases: see below. 7 Zeroing is similar to deletion with a condition of recoverability (but contrast the Constructive treatment at pp. 50 ff. and fn. 10 with Chomsky (1964), which introduced the latter notion). The term &apos;deletion&apos; is often applied to &apos;replacement&apos;, as in pronominalization. This and other points of difference between the two terms seem to be an artifact of generative derivations being defined on abstract phrase-structure trees and Constructive derivations being defined on lexical dependencies. For an example of problems with deletion as defined on nodes of phrase-structure trees, see for example Postal (1978), pp. 10-11, and ref. cit. For Postal (p. 23), there is no way to formulate in a transformational framework the fact that `to contraction&apos; —</context>
<context position="29501" citStr="Chomsky (1964)" startWordPosition="4649" endWordPosition="4650">ent some relation to the notion of simplicity in generative grammar, but is both more sensitive and intuitively more satisfying than gross measures of formalisms, such as symbol-counting. 19 For Korean, see e.g. Harris (1968:109-113). For French, see Sager (1981: 10.2); Sager, Claris, and Clifford (1970); and Salkoff (1973). For German, see Langerhans (1981). For Takelma, see Kendall (1977). 20 Except insofar as H&apos;s indefinites may be thought of as classifiers with the broadest selection. There is some parallelism between the notion of a `designated representative&apos; of a category introduced in Chomsky (1964) and H&apos;s use of indefinites where a zeroed argument is not specified by context. simple information-transmitting dependency structure of H&apos;s base is universal, and having been found for English (and in some measure for French, German, Korean, Takelma, and other languages) may now be presumed as an objective of work in other languages. And surely the major types of reduction involving permutation and zeroing, such as his length permutation, include many universals, though details of the &apos;morphophonemic&apos; types of reduction are more likely to be language specific.19 Comparison with generativist f</context>
</contexts>
<marker>Chomsky, 1964</marker>
<rawString>Chomsky, Noam. 1964 Current Issues in Linguistic Theory. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Corcoran</author>
</authors>
<title>Harris on the Structures of Language. In</title>
<date>1972</date>
<location>Plotz</location>
<contexts>
<context position="10729" citStr="Corcoran 1972" startWordPosition="1705" endWordPosition="1706">is apparently allied with intuitionist usage in mathematics. The term &apos;constructive&apos; provides a counterpoise to the (equally irrelevant) rhetorical overtones that have grown up around the term &apos;generative&apos; as a kind of trademark. A constructive grammar such as the one under review is of course generative in the technical sense of producing an explicit structural description for every sentence of the language. Furthermore, at least since his 1965 report in Language H has been explicitly concerned with generative rules for deriving sentences from base structures. (Newmeyer 1980:37 misinterprets Corcoran 1972:279 on this point: he ignores Corcoran&apos;s further discussion in that paper, as well as H&apos;s own later writings, in particular section 8.8 of Harris 1969, Corcoran&apos;s principal reference). This notwithstanding, few would claim that H is a generative grammarian, and a correlative term other than &apos;Harrisian grammar&apos; seems in order. 4 The suggestion outlined in Harris (1969), and exemplified in Gross (1973), that some of the base vocabulary may be suppletives factored from partial homonyms and synonyms, is not taken up systematically in this book. Echoes of this program may be found here and there, </context>
</contexts>
<marker>Corcoran, 1972</marker>
<rawString>Corcoran, John 1972 Harris on the Structures of Language. In Plotz 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>About the French verb TO KNOW.</title>
<date>1973</date>
<journal>Studia Linguistica</journal>
<pages>122--124</pages>
<contexts>
<context position="11133" citStr="Gross (1973)" startWordPosition="1767" endWordPosition="1768">nguage. Furthermore, at least since his 1965 report in Language H has been explicitly concerned with generative rules for deriving sentences from base structures. (Newmeyer 1980:37 misinterprets Corcoran 1972:279 on this point: he ignores Corcoran&apos;s further discussion in that paper, as well as H&apos;s own later writings, in particular section 8.8 of Harris 1969, Corcoran&apos;s principal reference). This notwithstanding, few would claim that H is a generative grammarian, and a correlative term other than &apos;Harrisian grammar&apos; seems in order. 4 The suggestion outlined in Harris (1969), and exemplified in Gross (1973), that some of the base vocabulary may be suppletives factored from partial homonyms and synonyms, is not taken up systematically in this book. Echoes of this program may be found here and there, however, as for example in H&apos;s observations on the derivation of -hood, -dom, which were once free words in English (suppletive forms of something like state and estate, respectively). Note here also the discussion (p. 73) of classifiers in the sublanguage of a science, in particular the sublanguage of grammar (which is the metalanguage of the language as whole). 5 A sublanguage is a subset of sentenc</context>
</contexts>
<marker>Gross, 1973</marker>
<rawString>Gross, Maurice 1973 About the French verb TO KNOW. Studia Linguistica 122-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Methodes en Syntaxe Hermann,</title>
<date>1975</date>
<location>Paris.</location>
<contexts>
<context position="34939" citStr="Gross (1975" startWordPosition="5500" endWordPosition="5501">onable, however, insofar as the presence of such zeroable sequences in sources may interact with the normal restrictions defined for the reductions and thus impose some significant constraints on derivations. (There is some evidence for this in certain island phenomena.) In a system with metalanguage statements of coreference and the like represented in a separate notation outside the language, these constraints have to be restated in the form of special rules or exceptions to rules, introducing extrinsic redundancy precisely of the sort that we wish to avoid. 22 See for example Sager (1981), Gross (1975, 1979), Salkoff (1973), and references cited in those works. 23 The first three examples were suggested by Wheeler 1984. 24 With other verbs, such as stand in He stood the box on end, The tree stood in the forest, and f He stood the tree in the forest, H sets up a causative source (Zero Causative, 6.8). In both cases, the higher verb imposes restrictions on the derived form, such as we see reflected in the limited acceptability of the sentence just cited with a dagger t (which might occur acceptably, forexample, in a myth or folk tale). These restrictions betray the zeroed presence of the hig</context>
</contexts>
<marker>Gross, 1975</marker>
<rawString>Gross, Maurice 1975 Methodes en Syntaxe Hermann, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>On the Failure of Generative Grammar.</title>
<date>1979</date>
<journal>Lg.</journal>
<volume>55</volume>
<issue>4</issue>
<pages>859--885</pages>
<contexts>
<context position="12852" citStr="Gross 1979" startWordPosition="2034" endWordPosition="2035">ntial to an understanding of semantics of natural language as opposed to formal languages. Kittredge and Lehrburger (1982) provides a brief survey. 204 Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 Book Review A Grammar of English on Mathematical Principles redundancies of the base sublanguage are recoverable.8 Linearization of the operator-argument dependencies — in English either &apos;normal&apos; SVO or a `topicalizing&apos; linear order — is accomplished by the reduction system, not the base. The reduction system includes much of what is in the lexicon in generative grammar (cf. Gross 1979). 5. Metalinguistic information required for many reductions, such as coreferentiality and lexical identity, is expressed within the language by conjoined metalanguage sentences, rather than by a separate grammatical mechanism such as subscripts.9 Similarly, &apos;shared knowledge&apos; contextual and pragmatic information is expressed by conjoined sentences (including ordinary dictionary definitions) that are zeroable because of their redundancy. 6. The set of possible arguments for a given operator (or vice-versa) is graded as to acceptability. These gradings correspond with differences of meaning in </context>
<context position="14886" citStr="Gross (1979" startWordPosition="2347" endWordPosition="2348">sformational framework the fact that `to contraction&apos; — phonetic reduction of to under the seven verb forms want, going, have, ought, used, got, and supposed — is determined by &apos;subject sharing&apos;, though that is precisely how H&apos;s transformational grammar did and his constructive grammar does account for these phenomena. 8 These traces, and those noted under 3 above, were the basis for reformulation of transformations as elementary sentence-differences, a crucial step in development of the constructive model. They are of course unrelated to the trace marker proposed in Chomsky (1973), which, as Gross (1979: 874) points out, is simply a computational device, a pointer to a memory address, such as is found in programming languages. 9 See Harris (1968: 17 ff.) for reasons why the metalanguage must be within the language. This property of language of course does not in itself preclude use of other types of notation for the metalanguage. Convenient notational reductions of other specialized sublanguages to formal symbol systems come to mind, as for example the notational systems of logic and mathematics, whose most complex formulae are nevertheless always stateable somehow in sentences of the corres</context>
</contexts>
<marker>Gross, 1979</marker>
<rawString>Gross, Maurice 1979 On the Failure of Generative Grammar. Lg. 55(4): 859-885.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Transformational Theory.</title>
<date>1965</date>
<journal>Lg.</journal>
<booktitle>Mathematical Structures of Language.</booktitle>
<volume>41</volume>
<pages>363--401</pages>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="9048" citStr="Harris 1965" startWordPosition="1452" endWordPosition="1453">t be met by some previously entering word or words, generally the last entry or entries, which must not already be in the argument of some other word. The base vocabulary has thus a few simple classes of words: base nouns with null argument On, Onn operators requiring base nouns as arguments 00, 000 requiring operators as arguments Ono, Oon requiring combinations of operators and base nouns In addition to these classes, almost all of the operators require morphophonemic insertion of &apos;argument indicators&apos; such as -ing and that. (These were termed the &apos;trace&apos; of &apos;incremental transformations&apos; in Harris 1965 and 1968.) 3. The base generates a sublanguage,5 which is informationally complete while containing no paraphrases. This is at the expense of redundancy and other stylistic awkwardness, so that utterances of any complexity in the base sublanguage are unlikely to be encountered in ordinary discourse. As in prior reports of H&apos;s work, base sentences are all assertions, other forms such as questions and imperatives being derived from underlying performatives I ask, I request, and the like. 4. A well-defined system of reductions yields the other sentences of the language as paraphrases of base sen</context>
</contexts>
<marker>Harris, 1965</marker>
<rawString>Harris, Zellig S. 1965 Transformational Theory. Lg. 41: 363-401. Harris, Zellig S. 1968 Mathematical Structures of Language. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>The Two Systems of Grammar: Report</title>
<date>1969</date>
<booktitle>Reprinted in Papers in Structural and Transformational Linguistics</booktitle>
<location>Reidel, Dordrecht.</location>
<contexts>
<context position="10880" citStr="Harris 1969" startWordPosition="1730" endWordPosition="1731">ones that have grown up around the term &apos;generative&apos; as a kind of trademark. A constructive grammar such as the one under review is of course generative in the technical sense of producing an explicit structural description for every sentence of the language. Furthermore, at least since his 1965 report in Language H has been explicitly concerned with generative rules for deriving sentences from base structures. (Newmeyer 1980:37 misinterprets Corcoran 1972:279 on this point: he ignores Corcoran&apos;s further discussion in that paper, as well as H&apos;s own later writings, in particular section 8.8 of Harris 1969, Corcoran&apos;s principal reference). This notwithstanding, few would claim that H is a generative grammarian, and a correlative term other than &apos;Harrisian grammar&apos; seems in order. 4 The suggestion outlined in Harris (1969), and exemplified in Gross (1973), that some of the base vocabulary may be suppletives factored from partial homonyms and synonyms, is not taken up systematically in this book. Echoes of this program may be found here and there, however, as for example in H&apos;s observations on the derivation of -hood, -dom, which were once free words in English (suppletive forms of something like</context>
</contexts>
<marker>Harris, 1969</marker>
<rawString>Harris, Zellig S. 1969 The Two Systems of Grammar: Report and Paraphrase. TDAP, Philadelphia. Reprinted in Papers in Structural and Transformational Linguistics (1970). Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1976</date>
<journal>A Theory of Language Structure. Am. Phil. Quart.</journal>
<volume>13</volume>
<issue>4</issue>
<pages>237--55</pages>
<contexts>
<context position="30750" citStr="Harris 1976" startWordPosition="4841" endWordPosition="4842"> problematic. Differences of acceptability or likelihood among the arguments of a given operator, or among the operators (or operators and co-arguments) for a given word, are notoriously variable and vague. For H, selection restrictions within the major dependency classes of base nouns and operators form at best fuzzy subsets that are inherently beyond the reach of the binary semantic features of interpretive semantics. To be sure, there are some hard yes/no selection restrictions that can be successfully encoded by binary semantic features, but these are produced by the reduction system (cf. Harris 1976:251 for a succinct statement), and therefore for H are not a problem of lexicon at all. H does little with the classifier words2° that are explicitly part of the vocabulary — the hierarchies of words that make up the taxonomies, both folk and scientific, that were the original motivation for componential analysis and semantic features. Classifier words among the operators (act is an example) go unmentioned except for the &apos;appropriate&apos; operator needed for the sources of certain derived nouns (5.25, p. 224). H does show how novel utterances may be related to familiar ones of known acceptability</context>
<context position="37103" citStr="Harris 1976" startWordPosition="5856" endWordPosition="5857">ities would use only classifier nouns, as in the onion-skin example. There would be a set of &apos;dictionary sentences&apos; for each classifier N giving acceptabilities for its co-argument N under be. (For compactness, many of these sentences could have classifiers in both argument positions. These acceptabilities are welldefined, or nearly so, only for the classifier words in sublanguages of science.) The grammar would draw upon these dictionary sentences to extrapolate the fuzzy set of acceptable arguments for a given operator, or of operators (and, where required, co-argument N) for a given N (see Harris 1976, p. 247 and fn 13). Metaphor and analogic extension fall naturally out of the same process. Finally, another type of dictionary sentence would be needed to meet the word-sharing requirement under conjunction that distinguishes coherent discourse from incoherent strings of sentences (pp. 13, 163-4; also Harris 1976, p. 241 and fn. 6; 1982, pp. 232-3). These &apos;shared knowledge&apos; sentences are normally zeroed since they add only information that the hearer is presumed already to have. The three sets of acceptability-model sentences would be unique for each definable sublanguage, presumably with gr</context>
<context position="42044" citStr="Harris 1976" startWordPosition="6668" endWordPosition="6669">heir subject a recipient (including have, need, undergo). As elsewhere in the grammar (such as the hit vs. break example, above), when this higher verb operates on a sentence containing an object N metalinguistically identified with its subject, the redundant lower object may be zeroed: He had a scolding of him by me. -• He had a scolding by me. 25 In some contexts, perhaps in the argument of capture or some other operator. Note that one may seek to do anything, but in most contexts only finding is so predictable as the purpose of one&apos;s seeking that the verb find is zeroable under seek. 26 In Harris 1976, argues that problems of intension vs. extension do not arise in his inscriptional approach to semantics, as they do in the more familiar interpretive and set-theoretic approaches. The second component of the passive is be . . . -en before the original subject. The -en occurs with stative meaning in the have . . . -en (perfect) construction, as well as in the diachronically moribund construction seen for example in He is risen. H takes these to be occurrences of the same suffix, a suppletive form of underlying be in a state: He is in the state of his scolding by me. The third component is by </context>
</contexts>
<marker>Harris, 1976</marker>
<rawString>Harris, Zellig S. 1976 A Theory of Language Structure. Am. Phil. Quart. 13(4): 237-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Discourse and Sublanguage.</title>
<date>1982</date>
<booktitle>In Kittredge and Lehrburger</booktitle>
<pages>231--6</pages>
<marker>Harris, 1982</marker>
<rawString>Harris, Zellig S. 1982 Discourse and Sublanguage. In Kittredge and Lehrburger 1982, pp. 231-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>N Sager</author>
</authors>
<title>Automataic Information-Formatting of a Medical Sublanguage.</title>
<date>1981</date>
<booktitle>In Kittredge and Lehrberger</booktitle>
<pages>27--80</pages>
<contexts>
<context position="32199" citStr="Hirschman and Sager 1981" startWordPosition="5077" endWordPosition="5080">h the air at a particular velocity; blue and mauve are colors; onion-skin is a solid object; 759.06 miles per second is a particular velocity. Some blue and mauve colored solid object consisting of onion-skin shot through the air at a velocity of 759.06 miles per second. It is not clear to the reviewer at least whether H can find further well-established reductions to derive the first sentence above from the last one (that appears not to be his aim in citing this example). Note, however, that a similar mechanism underlies Sager&apos;s computer formatting of sublanguage discourse (Sager 1981, 1982; Hirschman and Sager 1981). Some classifier words presumably embody semantic universals (e.g. animal, human, object) so that such use of classifiers could be a device for machine translation. One may hope that H or another Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 207 Book Review A Grammar of English on Mathematical Principles worker in the constructive paradigm will take up these undeveloped themes in some future report on the lexicon for a constructive grammar. Presumably, the next important phase of H&apos;s work includes a specification of the lexicon for his grammar and a return to discourse</context>
</contexts>
<marker>Hirschman, Sager, 1981</marker>
<rawString>Hirschman, L. and Sager, N. 1981 Automataic Information-Formatting of a Medical Sublanguage. In Kittredge and Lehrberger 1982 pp. 27-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Hiz</author>
</authors>
<title>On some general principles of semantics of a natural language.</title>
<date>1979</date>
<journal>Syntax and Semantics</journal>
<volume>10</volume>
<pages>343--352</pages>
<contexts>
<context position="24603" citStr="Hiz (1979)" startWordPosition="3869" endWordPosition="3870">e paradigm works rather from the outside in: an excessively powerful system of rules generates a superset of objects containing the set of sentences, and a major task is pruning and filtering; degrees of grammaticality and degrees of acceptability (the two are often not distinguished) are problematic rather than being fundamental data of linguistics; and dynamic processes of metaphor, analogy, and language change are difficult to integrate with one another and with synchronic descriptions of languages. 16 There is no mention of this &apos;weak semantics&apos; in the book proper, though it does occur in Hiz (1979). This &apos;weak semantics&apos; must be construed as a form of &apos; propositional meaning&apos; with the proviso that a great deal of pragmatics, implied meaning, and the like, may appear as conjoined &apos;common knowledge&apos; sentences that are zeroable because so utterly redundant (characteristic 5). This makes sense only in context of a knowledge base, a theme to which I will revert at the end of this review. Examples of H&apos;s treatment of some familiar semantic problems follow this comparison of the two paradigms. 17 See e.g. pp. 27-28 and 377-78 for discussion. However, H&apos;s derivation of please (p. 351 and n 7) c</context>
</contexts>
<marker>Hiz, 1979</marker>
<rawString>Hiz, Henry 1979 On some general principles of semantics of a natural language. Syntax and Semantics 10: 343-352.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Otto Jespersen</author>
</authors>
<title>1909-1931 A Modern English Grammar on Historical Principles. George Allen and Unwin,</title>
<location>Ltd., London.</location>
<marker>Jespersen, </marker>
<rawString>Jespersen, Otto 1909-1931 A Modern English Grammar on Historical Principles. George Allen and Unwin, Ltd., London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daythat Lee Kendall</author>
</authors>
<title>A Syntactic Analysis of Takeluma Texts.</title>
<date>1977</date>
<tech>Ph.D. dissertation,</tech>
<institution>Department of Linguistics, University of Pennsylvania,</institution>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="29280" citStr="Kendall (1977)" startWordPosition="4615" endWordPosition="4616">cture of other languages in constructive terms rather than making them a prerequisite for such work — one may draw some conclusions about them from this book. Surely, the very 18 H&apos;s reduction of redundancy has in its intent some relation to the notion of simplicity in generative grammar, but is both more sensitive and intuitively more satisfying than gross measures of formalisms, such as symbol-counting. 19 For Korean, see e.g. Harris (1968:109-113). For French, see Sager (1981: 10.2); Sager, Claris, and Clifford (1970); and Salkoff (1973). For German, see Langerhans (1981). For Takelma, see Kendall (1977). 20 Except insofar as H&apos;s indefinites may be thought of as classifiers with the broadest selection. There is some parallelism between the notion of a `designated representative&apos; of a category introduced in Chomsky (1964) and H&apos;s use of indefinites where a zeroed argument is not specified by context. simple information-transmitting dependency structure of H&apos;s base is universal, and having been found for English (and in some measure for French, German, Korean, Takelma, and other languages) may now be presumed as an objective of work in other languages. And surely the major types of reduction in</context>
</contexts>
<marker>Kendall, 1977</marker>
<rawString>Kendall, Daythat Lee 1977 A Syntactic Analysis of Takeluma Texts. Ph.D. dissertation, Department of Linguistics, University of Pennsylvania, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Kittredge</author>
<author>J Lehrburger</author>
<author>Eds</author>
</authors>
<date>1982</date>
<booktitle>Sublanguage: Studies of Language in Restricted Semantic Domain. de Gruyter,</booktitle>
<location>New York.</location>
<marker>Kittredge, Lehrburger, Eds, 1982</marker>
<rawString>Kittredge, Richard and Lehrburger, J., Eds. 1982 Sublanguage: Studies of Language in Restricted Semantic Domain. de Gruyter, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilse Langerhans</author>
</authors>
<title>Unrestricted Predicates as Transformational Sources in German.</title>
<date>1971</date>
<tech>TDAP 86,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<marker>Langerhans, 1971</marker>
<rawString>Langerhans, Ilse 1971 Unrestricted Predicates as Transformational Sources in German. TDAP 86, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Munz</author>
</authors>
<title>Reflections on the Development of Transformational Theories.</title>
<date>1972</date>
<booktitle>In Plotz</booktitle>
<pages>251--74</pages>
<contexts>
<context position="18765" citStr="Munz (1972)" startWordPosition="2950" endWordPosition="2951">nsequences for the surface forms generated by the grammar. H has always given primacy to semantics over syntax. Even in his earlier structural linguistics, H&apos;s distributionalism was a study of semantics and not the empty taxonomy of the &apos;structuralist&apos; stereotype; H&apos;s two empirical touchstones, contrast and differential acceptability, are both semantic notions; and H&apos;s aim in reducing redundancy in grammar (see below) is to get syntax out of the way of semantics. For Chomsky, on the other hand, syntax has always been central and semantics must be effected by a separate interpretive mechanism. Munz (1972) sketches the history of this divergence of the two paradigms. The mention of paraphrase under characteristic 4 may trouble some readers. The use of judgments of paraphrase as a criterion for grammatical relationship has given rise to endless confusion and dissension in linguistics. This is because paraphrase in any strong sense is an exceedingly rare phenomenon in natural language. For H, paraphrase (&apos;weak paraphrase&apos;) is rather an interpretation of the fact that the semantic dependencies of the base are preserved by (are recoverable under) the reductions. The reductions in turn are defined w</context>
</contexts>
<marker>Munz, 1972</marker>
<rawString>Munz, Jim 1972 Reflections on the Development of Transformational Theories. In Plotz 1972, pp. 251-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick J Newmeyer</author>
</authors>
<title>Linguistic Theory in</title>
<date>1980</date>
<publisher>America. Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="10698" citStr="Newmeyer 1980" startWordPosition="1702" endWordPosition="1703">968, pp. 17f, 20, 32, 89, 121) is apparently allied with intuitionist usage in mathematics. The term &apos;constructive&apos; provides a counterpoise to the (equally irrelevant) rhetorical overtones that have grown up around the term &apos;generative&apos; as a kind of trademark. A constructive grammar such as the one under review is of course generative in the technical sense of producing an explicit structural description for every sentence of the language. Furthermore, at least since his 1965 report in Language H has been explicitly concerned with generative rules for deriving sentences from base structures. (Newmeyer 1980:37 misinterprets Corcoran 1972:279 on this point: he ignores Corcoran&apos;s further discussion in that paper, as well as H&apos;s own later writings, in particular section 8.8 of Harris 1969, Corcoran&apos;s principal reference). This notwithstanding, few would claim that H is a generative grammarian, and a correlative term other than &apos;Harrisian grammar&apos; seems in order. 4 The suggestion outlined in Harris (1969), and exemplified in Gross (1973), that some of the base vocabulary may be suppletives factored from partial homonyms and synonyms, is not taken up systematically in this book. Echoes of this progra</context>
</contexts>
<marker>Newmeyer, 1980</marker>
<rawString>Newmeyer, Frederick J. 1980 Linguistic Theory in America. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Senta PlOtz</author>
<author>Ed</author>
</authors>
<title>Transformationelle Analyse.</title>
<date>1972</date>
<location>Athenaum, Frankfurt.</location>
<marker>PlOtz, Ed, 1972</marker>
<rawString>PlOtz, Senta, Ed. 1972 Transformationelle Analyse. Athenaum, Frankfurt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul M Postal</author>
</authors>
<title>Traces and the Description of English Complementizer Contraction.</title>
<date>1978</date>
<volume>19</volume>
<issue>1</issue>
<pages>1--30</pages>
<contexts>
<context position="14190" citStr="Postal (1978)" startWordPosition="2232" endWordPosition="2233">7 Zeroing is similar to deletion with a condition of recoverability (but contrast the Constructive treatment at pp. 50 ff. and fn. 10 with Chomsky (1964), which introduced the latter notion). The term &apos;deletion&apos; is often applied to &apos;replacement&apos;, as in pronominalization. This and other points of difference between the two terms seem to be an artifact of generative derivations being defined on abstract phrase-structure trees and Constructive derivations being defined on lexical dependencies. For an example of problems with deletion as defined on nodes of phrase-structure trees, see for example Postal (1978), pp. 10-11, and ref. cit. For Postal (p. 23), there is no way to formulate in a transformational framework the fact that `to contraction&apos; — phonetic reduction of to under the seven verb forms want, going, have, ought, used, got, and supposed — is determined by &apos;subject sharing&apos;, though that is precisely how H&apos;s transformational grammar did and his constructive grammar does account for these phenomena. 8 These traces, and those noted under 3 above, were the basis for reformulation of transformations as elementary sentence-differences, a crucial step in development of the constructive model. Th</context>
</contexts>
<marker>Postal, 1978</marker>
<rawString>Postal, Paul M. 1978 Traces and the Description of English Complementizer Contraction. L19(1): 1-30</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Sager</author>
</authors>
<title>Natural Language Information Processing.</title>
<date>1981</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="21274" citStr="Sager (1981)" startWordPosition="3343" endWordPosition="3344">ng from various valid substitutions of words for the variable are not all equally sayable, and this is a direct reflection of the meanings of the words. By this means, then, Harris&apos;s grammar captures what the dust jacket refers to as &apos;weak semantics&apos;16 without the cost of a separate semantic component. H&apos;s grammar is the product of long evolution, rather than of sudden revolution. His method of improving his grammar is to extend existing, established reductions to new argument domains, rather than to throw out the previous version of the grammatical apparatus for a revolutionary new one. (See Sager (1981) for examples of this within an earlier form of H&apos;s paradigm.) His method mirrors the way language itself changes over time, and indeed incorporates the mechanism of language change 12 It was an alternative to well-defined selection or &apos;cooccurrence sets&apos; as a criterion for two sentence-sets to be transforms in 1957, and the only criterion after I-1 determined that simple well-defined co-occurrence did not stand up, as he reported in his 1965 paper. 13 These more subtle aspects of meaning, which are more difficult to characterize formally, are thus segregated for study together with other esse</context>
<context position="29149" citStr="Sager (1981" startWordPosition="4596" endWordPosition="4597">rsals are not H&apos;s explicit concern — presumably, he would determine them empirically after working out the transformational structure of other languages in constructive terms rather than making them a prerequisite for such work — one may draw some conclusions about them from this book. Surely, the very 18 H&apos;s reduction of redundancy has in its intent some relation to the notion of simplicity in generative grammar, but is both more sensitive and intuitively more satisfying than gross measures of formalisms, such as symbol-counting. 19 For Korean, see e.g. Harris (1968:109-113). For French, see Sager (1981: 10.2); Sager, Claris, and Clifford (1970); and Salkoff (1973). For German, see Langerhans (1981). For Takelma, see Kendall (1977). 20 Except insofar as H&apos;s indefinites may be thought of as classifiers with the broadest selection. There is some parallelism between the notion of a `designated representative&apos; of a category introduced in Chomsky (1964) and H&apos;s use of indefinites where a zeroed argument is not specified by context. simple information-transmitting dependency structure of H&apos;s base is universal, and having been found for English (and in some measure for French, German, Korean, Takel</context>
<context position="32166" citStr="Sager 1981" startWordPosition="5074" endWordPosition="5075">object shot through the air at a particular velocity; blue and mauve are colors; onion-skin is a solid object; 759.06 miles per second is a particular velocity. Some blue and mauve colored solid object consisting of onion-skin shot through the air at a velocity of 759.06 miles per second. It is not clear to the reviewer at least whether H can find further well-established reductions to derive the first sentence above from the last one (that appears not to be his aim in citing this example). Note, however, that a similar mechanism underlies Sager&apos;s computer formatting of sublanguage discourse (Sager 1981, 1982; Hirschman and Sager 1981). Some classifier words presumably embody semantic universals (e.g. animal, human, object) so that such use of classifiers could be a device for machine translation. One may hope that H or another Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 207 Book Review A Grammar of English on Mathematical Principles worker in the constructive paradigm will take up these undeveloped themes in some future report on the lexicon for a constructive grammar. Presumably, the next important phase of H&apos;s work includes a specification of the lexicon for his </context>
<context position="34926" citStr="Sager (1981)" startWordPosition="5498" endWordPosition="5499">gram is questionable, however, insofar as the presence of such zeroable sequences in sources may interact with the normal restrictions defined for the reductions and thus impose some significant constraints on derivations. (There is some evidence for this in certain island phenomena.) In a system with metalanguage statements of coreference and the like represented in a separate notation outside the language, these constraints have to be restated in the form of special rules or exceptions to rules, introducing extrinsic redundancy precisely of the sort that we wish to avoid. 22 See for example Sager (1981), Gross (1975, 1979), Salkoff (1973), and references cited in those works. 23 The first three examples were suggested by Wheeler 1984. 24 With other verbs, such as stand in He stood the box on end, The tree stood in the forest, and f He stood the tree in the forest, H sets up a causative source (Zero Causative, 6.8). In both cases, the higher verb imposes restrictions on the derived form, such as we see reflected in the limited acceptability of the sentence just cited with a dagger t (which might occur acceptably, forexample, in a myth or folk tale). These restrictions betray the zeroed presen</context>
</contexts>
<marker>Sager, 1981</marker>
<rawString>Sager, Naomi 1981 Natural Language Information Processing. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Sager</author>
<author>P Claris</author>
<author>J Clifford</author>
</authors>
<title>French String Grammar,</title>
<date>1970</date>
<booktitle>String Program Reports, No. 8. Linguistic String Project,</booktitle>
<location>New York University, New York.</location>
<marker>Sager, Claris, Clifford, 1970</marker>
<rawString>Sager, Naomi; Claris, P.; and Clifford, J. 1970 French String Grammar, String Program Reports, No. 8. Linguistic String Project, New York University, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morris Salkoff</author>
</authors>
<title>Une Grammaire en Chaine du Francais.</title>
<date>1973</date>
<location>Dunod, Paris.</location>
<contexts>
<context position="29212" citStr="Salkoff (1973)" startWordPosition="4605" endWordPosition="4606">termine them empirically after working out the transformational structure of other languages in constructive terms rather than making them a prerequisite for such work — one may draw some conclusions about them from this book. Surely, the very 18 H&apos;s reduction of redundancy has in its intent some relation to the notion of simplicity in generative grammar, but is both more sensitive and intuitively more satisfying than gross measures of formalisms, such as symbol-counting. 19 For Korean, see e.g. Harris (1968:109-113). For French, see Sager (1981: 10.2); Sager, Claris, and Clifford (1970); and Salkoff (1973). For German, see Langerhans (1981). For Takelma, see Kendall (1977). 20 Except insofar as H&apos;s indefinites may be thought of as classifiers with the broadest selection. There is some parallelism between the notion of a `designated representative&apos; of a category introduced in Chomsky (1964) and H&apos;s use of indefinites where a zeroed argument is not specified by context. simple information-transmitting dependency structure of H&apos;s base is universal, and having been found for English (and in some measure for French, German, Korean, Takelma, and other languages) may now be presumed as an objective of</context>
<context position="34962" citStr="Salkoff (1973)" startWordPosition="5503" endWordPosition="5504">ofar as the presence of such zeroable sequences in sources may interact with the normal restrictions defined for the reductions and thus impose some significant constraints on derivations. (There is some evidence for this in certain island phenomena.) In a system with metalanguage statements of coreference and the like represented in a separate notation outside the language, these constraints have to be restated in the form of special rules or exceptions to rules, introducing extrinsic redundancy precisely of the sort that we wish to avoid. 22 See for example Sager (1981), Gross (1975, 1979), Salkoff (1973), and references cited in those works. 23 The first three examples were suggested by Wheeler 1984. 24 With other verbs, such as stand in He stood the box on end, The tree stood in the forest, and f He stood the tree in the forest, H sets up a causative source (Zero Causative, 6.8). In both cases, the higher verb imposes restrictions on the derived form, such as we see reflected in the limited acceptability of the sentence just cited with a dagger t (which might occur acceptably, forexample, in a myth or folk tale). These restrictions betray the zeroed presence of the higher verb and indicate w</context>
</contexts>
<marker>Salkoff, 1973</marker>
<rawString>Salkoff, Morris 1973 Une Grammaire en Chaine du Francais. Dunod, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Wheeler</author>
</authors>
<title>Review of Zellig S. Harris, A Grammar of</title>
<date>1984</date>
<booktitle>English on Mathematical Principles. Computers in the the Humanities</booktitle>
<volume>17</volume>
<issue>3</issue>
<pages>88--92</pages>
<contexts>
<context position="35059" citStr="Wheeler 1984" startWordPosition="5519" endWordPosition="5520">s defined for the reductions and thus impose some significant constraints on derivations. (There is some evidence for this in certain island phenomena.) In a system with metalanguage statements of coreference and the like represented in a separate notation outside the language, these constraints have to be restated in the form of special rules or exceptions to rules, introducing extrinsic redundancy precisely of the sort that we wish to avoid. 22 See for example Sager (1981), Gross (1975, 1979), Salkoff (1973), and references cited in those works. 23 The first three examples were suggested by Wheeler 1984. 24 With other verbs, such as stand in He stood the box on end, The tree stood in the forest, and f He stood the tree in the forest, H sets up a causative source (Zero Causative, 6.8). In both cases, the higher verb imposes restrictions on the derived form, such as we see reflected in the limited acceptability of the sentence just cited with a dagger t (which might occur acceptably, forexample, in a myth or folk tale). These restrictions betray the zeroed presence of the higher verb and indicate which form is primitive and which derived. Following H&apos;s practice, I am using a dagger to mark a s</context>
</contexts>
<marker>Wheeler, 1984</marker>
<rawString>Wheeler, Eric S. 1984 Review of Zellig S. Harris, A Grammar of English on Mathematical Principles. Computers in the the Humanities 17(3): 88-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wierzbicka</author>
</authors>
<title>Why Can You Have a Drink When You Can&apos;t Have an Eat?.</title>
<date>1982</date>
<journal>Lg.</journal>
<volume>58</volume>
<issue>4</issue>
<pages>753--99</pages>
<location>Anna</location>
<marker>Wierzbicka, 1982</marker>
<rawString>Wierzbicka, Anna 1982 Why Can You Have a Drink When You Can&apos;t Have an Eat?. Lg. 58(4): 753-99</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Zadeh</author>
</authors>
<title>Fuzzy Sets.</title>
<date>1965</date>
<journal>Information and Control</journal>
<volume>8</volume>
<pages>338--353</pages>
<contexts>
<context position="16325" citStr="Zadeh 1965" startWordPosition="2571" endWordPosition="2572">whole. There is also the open question whether the presence of low-information and metalanguage conjuncts in the base sublanguage constrains the reduction system, and whether subscripts and other computationally convenient notations might therefore give rise to some thorny problems as an artifact (see note 21 below). Furthermore, since sentences containing overt (unzeroed) assertions of metalinguistic information do occur in normal discourse, and cannot be excluded as ungrammatical, any grammar must account for them whether it uses them in this way or not, so it might as well use them. I° See Zadeh 1965. I I See also the discussion of analogic extension, below. guage or subject-matter domain to another. Equivalently, the fuzzy set&amp;quot;) of &apos;normal&apos; co-occurrents for a given word differs from one such domain to another within the base sublanguage. In informal, intuitive terms, a constructive grammar generates sentences from the bottom up, beginning with word entry, whereas a generative grammar generates sentences from the top down, beginning with the abstract symbol S. The grammatical apparatus of constructive grammar (the rules together with their requirements and exceptions) is very simple and </context>
</contexts>
<marker>Zadeh, 1965</marker>
<rawString>Zadeh, L.A. 1965 Fuzzy Sets. Information and Control 8: 338-353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Computational Linguistics</author>
</authors>
<date>1984</date>
<volume>10</volume>
<pages>211</pages>
<location>Numbers</location>
<marker>Linguistics, 1984</marker>
<rawString>Computational Linguistics, Volume 10, Numbers 3-4, July-December 1984 211</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>