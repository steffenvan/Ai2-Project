<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000636">
<title confidence="0.984068">
Discriminative Syntactic Language Modeling for Speech Recognition
</title>
<author confidence="0.835277">
Michael Collins Brian Roark Murat Saraclar
</author>
<affiliation confidence="0.378389">
MIT CSAIL OGI/OHSU Bogazici University
</affiliation>
<email confidence="0.992076">
mcollins@csail.mit.edu roark@cslu.ogi.edu murat.saraclar@boun.edu.tr
</email>
<sectionHeader confidence="0.997334" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999747285714286">
We describe a method for discriminative
training of a language model that makes
use of syntactic features. We follow
a reranking approach, where a baseline
recogniser is used to produce 1000-best
output for each acoustic input, and a sec-
ond “reranking” model is then used to
choose an utterance from these 1000-best
lists. The reranking model makes use of
syntactic features together with a parame-
ter estimation method that is based on the
perceptron algorithm. We describe exper-
iments on the Switchboard speech recog-
nition task. The syntactic features provide
an additional 0.3% reduction in test–set
error rate beyond the model of (Roark et
al., 2004a; Roark et al., 2004b) (signifi-
cant at p &lt; 0.001), which makes use of
a discriminatively trained n-gram model,
giving a total reduction of 1.2% over the
baseline Switchboard system.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998849">
The predominant approach within language model-
ing for speech recognition has been to use an n-
gram language model, within the “source-channel”
or “noisy-channel” paradigm. The language model
assigns a probability Pl(w) to each string w in the
language; the acoustic model assigns a conditional
probability Pa(a|w) to each pair (a, w) where a is a
sequence of acoustic vectors, and w is a string. For
a given acoustic input a, the highest scoring string
under the model is
</bodyText>
<equation confidence="0.906669">
w* = arg max (Q log Pl(w) + log Pa(a|w)) (1)
w
</equation>
<bodyText confidence="0.99995615625">
where Q &gt; 0 is some value that reflects the rela-
tive importance of the language model; Q is typi-
cally chosen by optimization on held-out data. In
an n-gram language model, a Markov assumption
is made, namely that each word depends only on
the previous (n − 1) words. The parameters of the
language model are usually estimated from a large
quantity of text data. See (Chen and Goodman,
1998) for an overview of estimation techniques for
n-gram models.
This paper describes a method for incorporating
syntactic features into the language model, using
discriminative parameter estimation techniques. We
build on the work in Roark et al. (2004a; 2004b),
which was summarized and extended in Roark et al.
(2005). These papers used discriminative methods
for n-gram language models. Our approach reranks
the 1000-best output from the Switchboard recog-
nizer of Ljolje et al. (2003).1 Each candidate string
w is parsed using the statistical parser of Collins
(1999) to give a parse tree T (w). Information from
the parse tree is incorporated in the model using
a feature-vector approach: we define 4b(a, w) to
be a d-dimensional feature vector which in princi-
ple could track arbitrary features of the string w
together with the acoustic input a. In this paper
we restrict 4b(a, w) to only consider the string w
and/or the parse tree T (w) for w. For example,
4b(a, w) might track counts of context-free rule pro-
ductions in T (w), or bigram lexical dependencies
within T (w). The optimal string under our new
model is defined as
</bodyText>
<equation confidence="0.998626">
w* � arg max (Q log P1(w) + (a, 4)(a, w))+
W
logP,,(alw)) (2)
</equation>
<bodyText confidence="0.977975777777778">
where the arg max is taken over all strings in the
1000-best list, and where a E Rd is a parameter
vector specifying the “weight” for each feature in
-b (note that we define (x, y) to be the inner, or dot
&apos;Note that (Roark et al., 2004a; Roark et al., 2004b) give
results for an n-gram approach on this data which makes use of
both lattices and 1000-best lists. The results on 1000-best lists
were very close to results on lattices for this domain, suggesting
that the 1000-best approximation is a reasonable one.
</bodyText>
<page confidence="0.948367">
507
</page>
<note confidence="0.9915385">
Proceedings of the 43rd Annual Meeting of the ACL, pages 507–514,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.999963388888889">
product, between vectors x and y). For this paper,
we train the parameter vector a using the perceptron
algorithm (Collins, 2004; Collins, 2002). The per-
ceptron algorithm is a very fast training method, in
practice requiring only a few passes over the train-
ing set, allowing for a detailed comparison of a wide
variety of feature sets.
A number of researchers have described work
that incorporates syntactic language models into a
speech recognizer. These methods have almost ex-
clusively worked within the noisy channel paradigm,
where the syntactic language model has the task
of modeling a distribution over strings in the lan-
guage, in a very similar way to traditional n-gram
language models. The Structured Language Model
(Chelba and Jelinek, 1998; Chelba and Jelinek,
2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003)
makes use of an incremental shift-reduce parser to
enable the probability of words to be conditioned on
k previous c-commanding lexical heads, rather than
simply on the previous k words. Incremental top-
down and left-corner parsing (Roark, 2001a; Roark,
2001b) and head-driven parsing (Charniak, 2001)
approaches have directly used generative PCFG
models as language models. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al., 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar were used to exploit syn-
tactic dependencies.
Our approach differs from previous work in a cou-
ple of important respects. First, through the feature-
vector representations 4b(a, w) we can essentially
incorporate arbitrary sources of information from
the string or parse tree into the model. We would ar-
gue that our method allows considerably more flexi-
bility in terms of the choice of features in the model;
in previous work features were incorporated in the
model through modification of the underlying gen-
erative parsing or tagging model, and modifying a
generative model is a rather indirect way of chang-
ing the features used by a model. In this respect, our
approach is similar to that advocated in Rosenfeld et
al. (2001), which used Maximum Entropy modeling
to allow for the use of shallow syntactic features for
language modeling.
A second contrast between our work and previ-
ous work, including that of Rosenfeld et al. (2001),
is in the use of discriminative parameter estimation
techniques. The criterion we use to optimize the pa-
rameter vector a is closely related to the end goal
in speech recognition, i.e., word error rate. Previ-
ous work (Roark et al., 2004a; Roark et al., 2004b)
has shown that discriminative methods within an n-
gram approach can lead to significant reductions in
WER, in spite of the features being of the same type
as the original language model. In this paper we ex-
tend this approach, by including syntactic features
that were not in the baseline speech recognizer.
This paper describe experiments using a variety
of syntactic features within this approach. We tested
the model on the Switchboard (SWB) domain, using
the recognizer of Ljolje et al. (2003). The discrim-
inative approach for n-gram modeling gave a 0.9%
reduction in WER on this domain; the syntactic fea-
tures we describe give a further 0.3% reduction.
In the remainder of this paper, section 2 describes
previous work, including the parameter estimation
methods we use, and section 3 describes the feature-
vector representations of parse trees that we used in
our experiments. Section 4 describes experiments
using the approach.
</bodyText>
<sectionHeader confidence="0.985531" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.851665">
2.1 Previous Work
</subsectionHeader>
<bodyText confidence="0.9998582">
Techniques for exploiting stochastic context-free
grammars for language modeling have been ex-
plored for more than a decade. Early approaches
included algorithms for efficiently calculating string
prefix probabilities (Jelinek and Lafferty, 1991; Stol-
cke, 1995) and approaches to exploit such algo-
rithms to produce n-gram models (Stolcke and Se-
gal, 1994; Jurafsky et al., 1995). The work of Chelba
and Jelinek (Chelba and Jelinek, 1998; Chelba and
Jelinek, 2000; Chelba, 2000) involved the use of a
shift-reduce parser trained on Penn treebank style
annotations, that maintains a weighted set of parses
as it traverses the string from left-to-right. Each
word is predicted by each candidate parse in this set
at the point when the word is shifted, and the con-
ditional probability of the word given the previous
words is taken as the weighted sum of the condi-
tional probabilities provided by each parse. In this
approach, the probability of a word is conditioned
by the top two lexical heads on the stack of the par-
</bodyText>
<page confidence="0.995648">
508
</page>
<bodyText confidence="0.999723272727273">
ticular parse. Enhancements in the feature set and
improved parameter estimation techniques have ex-
tended this approach in recent years (Xu et al., 2002;
Xu et al., 2003).
Roark (2001a; 2001b) pursued a different deriva-
tion strategy from Chelba and Jelinek, and used the
parse probabilities directly to calculate the string
probabilities. This work made use of a left-to-right,
top-down, beam-search parser, which exploits rich
lexico-syntactic features from the left context of
each derivation to condition derivation move proba-
bilities, leading to a very peaked distribution. Rather
than normalizing a prediction of the next word over
the beam of candidates, as in Chelba and Jelinek,
in this approach the string probability is derived by
simply summing the probabilities of all derivations
for that string in the beam.
Other work on syntactic language modeling in-
cludes that of Charniak (2001), which made use of
a non-incremental, head-driven statistical parser to
produce string probabilities. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al., 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar, were used to exploit syn-
tactic dependencies. The processing advantages of
the finite-state encoding of the model has allowed
for the use of probabilities calculated off-line from
this model to be used in the first pass of decoding,
which has provided additional benefits. Finally, Och
et al. (2004) use a reranking approach with syntactic
information within a machine translation system.
Rosenfeld et al. (2001) investigated the use of
syntactic features in a Maximum Entropy approach.
In their paper, they used a shallow parser to anno-
tate base constituents, and derived features from se-
quences of base constituents. The features were in-
dicator features that were either (1) exact matches
between a set or sequence of base constituents with
those annotated on the hypothesis transcription; or
(2) tri-tag features from the constituent sequence.
The generative model that resulted from their fea-
ture set resulted in only a very small improvement
in either perplexity or word-error-rate.
</bodyText>
<subsectionHeader confidence="0.996148">
2.2 Global Linear Models
</subsectionHeader>
<bodyText confidence="0.89408325">
We follow the framework of Collins (2002; 2004),
recently applied to language modeling in Roark et
al. (2004a; 2004b). The model we propose consists
of the following components:
</bodyText>
<listItem confidence="0.998369666666667">
• GEN(a) is a set of candidate strings for an
acoustic input a. In our case, GEN(a) is a set of
1000-best strings from a first-pass recognizer.
• T (w) is the parse tree for string w.
• -1)(a, w) E Rd is a feature-vector representation
of an acoustic input a together with a string w.
• a E Rd is a parameter vector.
• The output of the recognizer for an input a is
defined as
</listItem>
<equation confidence="0.9917545">
F (a) = argmax (`b(a, w), a) (3)
wEGEN(a)
</equation>
<bodyText confidence="0.961711925925926">
In principle, the feature vector 4b(a, w) could take
into account any features of the acoustic input a to-
gether with the utterance w. In this paper we make
a couple of restrictions. First, we define the first fea-
ture to be
-b1(a, w) = Q log Pl(w) + log P,,,(a|w)
where Pl(w) and P,,,(a|w) are language and acous-
tic model scores from the baseline speech recog-
nizer. In our experiments we kept Q fixed at the
value used in the baseline recogniser. It can then
be seen that our model is equivalent to the model
in Eq. 2. Second, we restrict the remaining features
&apos;b2(a, w) ...&apos;bd(a, w) to be sensitive to the string
w alone. In this sense, the scope of this paper is
limited to the language modeling problem. As one
example, the language modeling features might take
into account n-grams, for example through defini-
tions such as
4b2(a, w) = Count of the the in w
Previous work (Roark et al., 2004a; Roark et al.,
2004b) considered features of this type. In this pa-
per, we introduce syntactic features, which may be
sensitive to the parse tree for w, for example
4g(a, w) = Count of S —* NP VP in T (w)
where S —* NP VP is a context-free rule produc-
tion. Section 3 describes the full set of features used
in the empirical results presented in this paper.
</bodyText>
<footnote confidence="0.859228666666667">
2Future work may consider features of the acoustic sequence
a together with the string w, allowing the approach to be ap-
plied to acoustic modeling.
</footnote>
<page confidence="0.992088">
509
</page>
<subsubsectionHeader confidence="0.821118">
2.2.1 Parameter Estimation
</subsubsectionHeader>
<bodyText confidence="0.998896368421053">
We now describe how the parameter vector α� is
estimated from a set of training utterances. The
training set consists of examples (ai, wi) for i =
1... m, where ai is the i’th acoustic input, and wi
is the transcription of this input. We briefly review
the two training algorithms described in Roark et al.
(2004b), the perceptron algorithm and global condi-
tional log-linear models (GCLMs).
Figure 1 shows the perceptron algorithm. It is an
online algorithm, which makes several passes over
the training set, updating the parameter vector after
each training example. For a full description of the
algorithm, see Collins (2004; 2002).
A second parameter estimation method, which
was used in (Roark et al., 2004b), is to optimize
the log-likelihood under a log-linear model. Sim-
ilar approaches have been described in Johnson et
al. (1999) and Lafferty et al. (2001). The objective
function used in optimizing the parameters is
</bodyText>
<equation confidence="0.98117375">
L(a) = � �log P (si|ai, �α) − C α� (4)
i j
where P(si|ai, �α) = EwEGEN(ai) e(,D(ai,w),a) .
e(�(ai,si),�α�
</equation>
<bodyText confidence="0.996554625">
Here, each si is the member of GEN(ai) which
has lowest WER with respect to the target transcrip-
tion wi. The first term in L(a) is the log-likelihood
of the training data under a conditional log-linear
model. The second term is a regularization term
which penalizes large parameter values. C is a con-
stant that dictates the relative weighting given to the
two terms. The optimal parameters are defined as
</bodyText>
<equation confidence="0.8992675">
a* = arg max
a L(a)
</equation>
<bodyText confidence="0.999839888888889">
We refer to these models as global conditional log-
linear models (GCLMs).
Each of these algorithms has advantages. A num-
ber of results—e.g., in Sha and Pereira (2003) and
Roark et al. (2004b)—suggest that the GCLM ap-
proach leads to slightly higher accuracy than the per-
ceptron training method. However the perceptron
converges very quickly, often in just a few passes
over the training set—in comparison GCLM’s can
take tens or hundreds of gradient calculations before
convergence. In addition, the perceptron can be used
as an effective feature selection technique, in that
Input: A parameter specifying the number of iterations over
the training set, T. A value for the first parameter, α. A
feature-vector representation 4)(a, w) E Rd. Training exam-
ples (ai, wi) for i = 1 ... m. An n-best list GEN(ai) for each
training utterance. We take si to be the member of GEN(ai)
which has the lowest WER when compared to wi.
</bodyText>
<figure confidence="0.313347333333333">
Initialization: Set α1 = α, and αj = 0 for j =
2...d.
Algorithm: For t = 1... T, i = 1... m
•Calculate yi = arg maxwEGEN(ai) h4&apos;(ai, w), ai
• For j = 2 ... m, set aj = aj + 4bj(ai, si) −
-(Dj(ai, yi)
</figure>
<figureCaption confidence="0.808655444444444">
Output: Either the final parameters a, or the averaged pa-
rameters aavg defined as �vg = �t,i at,i/mT where at,i is
the parameter vector after training on the i’th training example
on the t’th pass through the training data.
Figure 1: The perceptron training algorithm. Following
Roark et al. (2004a), the parameter α1 is set to be some con-
stant α that is typically chosen through optimization over the
development set. Recall that α1 dictates the weight given to the
baseline recognizer score.
</figureCaption>
<bodyText confidence="0.999741058823529">
at each training example it only increments features
seen on si or yi, effectively ignoring all other fea-
tures seen on members of GEN(ai). For example,
in the experiments in Roark et al. (2004a), the per-
ceptron converged in around 3 passes over the train-
ing set, while picking non-zero values for around 1.4
million n-gram features out of a possible 41 million
n-gram features seen in the training set.
For the present paper, to get a sense of the relative
effectiveness of various kinds of syntactic features
that can be derived from the output of a parser, we
are reporting results using just the perceptron algo-
rithm. This has allowed us to explore more of the po-
tential feature space than we would have been able
to do using the more costly GCLM estimation tech-
niques. In future we plan to apply GLCM parameter
estimation methods to the task.
</bodyText>
<sectionHeader confidence="0.993991" genericHeader="method">
3 Parse Tree Features
</sectionHeader>
<bodyText confidence="0.9997296">
We tagged each candidate transcription with (1)
part-of-speech tags, using the tagger documented in
Collins (2002); and (2) a full parse tree, using the
parser documented in Collins (1999). The models
for both of these were trained on the Switchboard
</bodyText>
<page confidence="0.990833">
510
</page>
<figureCaption confidence="0.999294">
Figure 2: An example parse tree
</figureCaption>
<bodyText confidence="0.999748258064516">
treebank, and applied to candidate transcriptions in
both the training and test sets. Each transcription
received one POS-tag annotation and one parse tree
annotation, from which features were extracted.
Figure 2 shows a Penn Treebank style parse tree
that is of the sort produced by the parser. Given such
a structure, there is a tremendous amount of flexibil-
ity in selecting features. The first approach that we
follow is to map each parse tree to sequences encod-
ing part-of-speech (POS) decisions, and “shallow”
parsing decisions. Similar representations have been
used by (Rosenfeld et al., 2001; Wang and Harper,
2002). Figure 3 shows the sequential representations
that we used. The first simply makes use of the POS
tags for each word. The latter representations make
use of sequences of non-terminals associated with
lexical items. In 3(b), each word in the string is asso-
ciated with the beginning or continuation of a shal-
low phrase or “chunk” in the tree. We include any
non-terminals above the level of POS tags as poten-
tial chunks: a new “chunk” (VP, NP, PP etc.) begins
whenever we see the initial word of the phrase dom-
inated by the non-terminal. In 3(c), we show how
POS tags can be added to these sequences. The final
type of sequence mapping, shown in 3(d), makes a
similar use of chunks, but preserves only the head-
word seen with each chunk.3
From these sequences of categories, various fea-
tures can be extracted, to go along with the n-gram
features used in the baseline. These include n-tag
features, e.g. ti−2ti−1ti (where ti represents the
</bodyText>
<footnote confidence="0.8996975">
3It should be noted that for a very small percentage of hy-
potheses, the parser failed to return a full parse tree. At the
end of every shallow tag or category sequence, a special end of
sequence tag/word pair “&lt;/parse&gt; &lt;/parse&gt;” was emit-
ted. In contrast, when a parse failed, the sequence consisted of
solely “&lt;noparse&gt; &lt;noparse&gt;”.
</footnote>
<bodyText confidence="0.931636857142857">
we/PRP helped/VBD her/PRP paint/VB the/DT
house/NN
we/NPb helped/VPb her/NPb paint/VPb the/NPb
house/NPc
we/PRP-NPb helped/VBD-VPb her/PRP-NPb
paint/VB-VPb the/DT-NPb house/NN-NPc
we/NP helped/VP her/NP paint/VP house/NP
</bodyText>
<figureCaption confidence="0.9987108">
Figure 3: Sequences derived from a parse tree: (a) POS-tag
sequence; (b) Shallow parse tag sequence—the superscripts b
and c refer to the beginning and continuation of a phrase re-
spectively; (c) Shallow parse tag plus POS tag sequence; and
(d) Shallow category with lexical head sequence
</figureCaption>
<bodyText confidence="0.999921677419355">
tag in position i); and composite tag/word features,
e.g. tiwi (where wi represents the word in posi-
tion i) or, more complicated configurations, such as
ti−2ti−1wi−1tiwi. These features can be extracted
from whatever sort of tag/word sequence we pro-
vide for feature extraction, e.g. POS-tag sequences
or shallow parse tag sequences.
One variant that we performed in feature extrac-
tion had to do with how speech repairs (identified as
EDITED constituents in the Switchboard style parse
trees) and filled pauses or interjections (labeled with
the INTJ label) were dealt with. In the simplest ver-
sion, these are simply treated like other constituents
in the parse tree. However, these can disrupt what
may be termed the intended sequence of syntactic
categories in the utterance, so we also tried skipping
these constituents when mapping from the parse tree
to shallow parse sequences.
The second set of features we employed made
use of the full parse tree when extracting features.
For this paper, we examined several features tem-
plates of this type. First, we considered context-free
rule instances, extracted from each local node in the
tree. Second, we considered features based on lex-
ical heads within the tree. Let us first distinguish
between POS-tags and non-POS non-terminal cate-
gories by calling these latter constituents NTs. For
each constituent NT in the tree, there is an associ-
ated lexical head (HNT) and the POS-tag of that lex-
ical head (HPNT). Two simple features are NT/HNT
and NT/HPNT for every NT constituent in the tree.
</bodyText>
<figure confidence="0.999456944444445">
NP
VB
NP
VP
S
we
VBD
NP
VP
PRP
PRP
helped
her
paint
DT
the
NN
house
</figure>
<page confidence="0.928955">
511
</page>
<table confidence="0.9288234">
Feature Examples from figure 2
(P,HCP,Ci,{+,-}{1,2},HP,HCJ (VP,VB,NP,1,paint,house)
(S,VP,NP,-1,helped,we)
(P,HCP,Ci,{+,-}{1,2},HP,HPCJ (VP,VB,NP,1,paint,NN)
(S,VP,NP,-1,helped,PRP)
(P,HCP,Ci,{+,-}{1,2},HPP,HCJ (VP,VB,NP,1,VB,house)
VP,NP,-1, VBD,we)
((S,
(P,HCP,Ci,{+,-} { 1,2},HPP,HPcJ `VP,VB,NP,1,VB,NN)
(S,VP,NP,-1,VBD,PRP)
</table>
<tableCaption confidence="0.9768005">
Table 1: Examples of head-to-head features. The examples
are derived from the tree in figure 2.
</tableCaption>
<bodyText confidence="0.9997535">
Using the heads as identified in the parser, example
features from the tree in figure 2 would be S/VBD,
S/helped, NP/NN, and NP/house.
Beyond these constituent/head features, we can
look at the head-to-head dependencies of the sort
used by the parser. Consider each local tree, con-
sisting of a parent node (P), a head child (HCP), and
k non-head children (C1 ... CO. For each non-head
child CZ, it is either to the left or right of HCP, and is
either adjacent or non-adjacent to HCP. We denote
these positional features as an integer, positive if to
the right, negative if to the left, 1 if adjacent, and 2 if
non-adjacent. Table 1 shows four head-to-head fea-
tures that can be extracted for each non-head child
CZ. These features include dependencies between
pairs of lexical items, between a single lexical item
and the part-of-speech of another item, and between
pairs of part-of-speech tags in the parse.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999790384615385">
The experimental set-up we use is very similar to
that of Roark et al. (2004a; 2004b), and the exten-
sions to that work in Roark et al. (2005). We make
use of the Rich Transcription 2002 evaluation test
set (rt02) as our development set, and use the Rich
Transcription 2003 Spring evaluation CTS test set
(rt03) as test set. The rt02 set consists of 6081 sen-
tences (63804 words) and has three subsets: Switch-
board 1, Switchboard 2, Switchboard Cellular. The
rt03 set consists of 9050 sentences (76083 words)
and has two subsets: Switchboard and Fisher.
The training set consists of 297580 transcribed
utterances (3297579 words)4. For each utterance,
</bodyText>
<footnote confidence="0.940119666666667">
4Note that Roark et al. (2004a; 2004b; 2005) used 20854 of
these utterances (249774 words) as held out data. In this work
we simply use the rt02 test set as held out and development data.
</footnote>
<bodyText confidence="0.999973104166667">
a weighted word-lattice was produced, represent-
ing alternative transcriptions, from the ASR system.
The baseline ASR system that we are comparing
against then performed a rescoring pass on these first
pass lattices, allowing for better silence modeling,
and replaces the trigram language model score with
a 6-gram model. 1000-best lists were then extracted
from these lattices. For each candidate in the 1000-
best lists, we identified the number of edits (inser-
tions, deletions or substitutions) for that candidate,
relative to the “target” transcribed utterance. The or-
acle score for the 1000-best lists was 16.7%.
To produce the word-lattices, each training utter-
ance was processed by the baseline ASR system. In
a naive approach, we would simply train the base-
line system (i.e., an acoustic model and language
model) on the entire training set, and then decode
the training utterances with this system to produce
lattices. We would then use these lattices with the
perceptron algorithm. Unfortunately, this approach
is likely to produce a set of training lattices that are
very different from test lattices, in that they will have
very low word-error rates, given that the lattice for
each utterance was produced by a model that was
trained on that utterance. To somewhat control for
this, the training set was partitioned into 28 sets, and
baseline Katz backoff trigram models were built for
each set by including only transcripts from the other
27 sets. Lattices for each utterance were produced
with an acoustic model that had been trained on the
entire training set, but with a language model that
was trained on the 27 data portions that did not in-
clude the current utterance. Since language mod-
els are generally far more prone to overtraining than
standard acoustic models, this goes a long way to-
ward making the training conditions similar to test-
ing conditions. Similar procedures were used to
train the parsing and tagging models for the training
set, since the Switchboard treebank overlaps exten-
sively with the ASR training utterances.
Table 2 presents the word-error rates on rt02 and
rt03 of the baseline ASR system, 1000-best percep-
tron and GCLM results from Roark et al. (2005)
under this condition, and our 1000-best perceptron
results. Note that our n-best result, using just n-
gram features, improves upon the perceptron result
of (Roark et al., 2005) by 0.2 percent, putting us
within 0.1 percent of their GCLM result for that
</bodyText>
<page confidence="0.990802">
512
</page>
<table confidence="0.998340833333333">
Trial WER
rt02 rt03
ASR system output 37.1 36.4
Roark et al. (2005) perceptron 36.6 35.7
Roark et al. (2005) GCLM 36.3 35.4
n-gram perceptron 36.4 35.5
</table>
<tableCaption confidence="0.931946">
Table 2: Baseline word-error rates versus Roark et al. (2005)
</tableCaption>
<table confidence="0.999764571428571">
Trial rt02
WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS (1) perceptron 36.1
n-gram + POS (1,2) perceptron 36.1
n-gram + POS (1,3) perceptron 36.1
</table>
<tableCaption confidence="0.999899">
Table 3: Use of POS-tag sequence derived features
</tableCaption>
<bodyText confidence="0.9886196">
condition. (Note that the perceptron–trained n-gram
features were trigrams (i.e., n = 3).) This is due to
a larger training set being used in our experiments;
we have added data that was used as held-out data in
(Roark et al., 2005) to the training set that we use.
The first additional features that we experimented
with were POS-tag sequence derived features. Let
ti and wi be the POS tag and word at position i,
respectively. We experimented with the following
three feature definitions:
</bodyText>
<equation confidence="0.700826">
1. (ti−2ti−1ti), (ti−1ti), (ti), (tiwi)
2. (ti−2ti−1wi)
3. (ti−2wi−2ti−1wi−1tiwi), (ti−2ti−1wi−1tiwi),
(ti−1wi−1tiwi), (ti−1tiwi)
</equation>
<bodyText confidence="0.999246625">
Table 3 summarizes the results of these trials on
the held out set. Using the simple features (num-
ber 1 above) yielded an improvement beyond just
n-grams, but additional, more complicated features
failed to yield additional improvements.
Next, we considered features derived from shal-
low parsing sequences. Given the results from the
POS-tag sequence derived features, for any given se-
quence, we simply use n-tag and tag/word features
(number 1 above). The first sequence type from
which we extracted features was the shallow parse
tag sequence (S1), as shown in figure 3(b). Next,
we tried the composite shallow/POS tag sequence
(S2), as in figure 3(c). Finally, we tried extract-
ing features from the shallow constituent sequence
(S3), as shown in figure 3(d). When EDITED and
</bodyText>
<table confidence="0.958179090909091">
Trial rt02
WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS perceptron 36.1
n-gram + POS + S1 perceptron 36.1
n-gram + POS + S2 perceptron 36.0
n-gram + POS + S3 perceptron 36.0
n-gram + POS + S3-E perceptron 36.0
n-gram + POS + CF perceptron 36.1
n-gram + POS + H2H perceptron 36.0
</table>
<tableCaption confidence="0.9921745">
Table 4: Use of shallow parse sequence and full parse derived
features
</tableCaption>
<bodyText confidence="0.999901133333333">
INTJ nodes are ignored, we refer to this condition
as S3-E. For full-parse feature extraction, we tried
context-free rule features (CF) and head-to-head fea-
tures (H2H), of the kind shown in table 1. Table 4
shows the results of these trials on rt02.
Although the single digit precision in the table
does not show it, the H2H trial, using features ex-
tracted from the full parses along with n-grams and
POS-tag sequence features, was the best performing
model on the held out data, so we selected it for ap-
plication to the rt03 test data. This yielded 35.2%
WER, a reduction of 0.3% absolute over what was
achieved with just n-grams, which is significant at
p &lt; 0.001,5 reaching a total reduction of 1.2% over
the baseline recognizer.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999989466666667">
The results presented in this paper are a first step in
examining the potential utility of syntactic features
for discriminative language modeling for speech
recognition. We tried two possible sets of features
derived from the full annotation, as well as a va-
riety of possible feature sets derived from shallow
parse and POS tag sequences, the best of which
gave a small but significant improvement beyond
what was provided by the n-gram features. Future
work will include a further investigation of parser–
derived features. In addition, we plan to explore the
alternative parameter estimation methods described
in (Roark et al., 2004a; Roark et al., 2004b), which
were shown in this previous work to give further im-
provements over the perceptron.
</bodyText>
<footnote confidence="0.8504065">
5We use the Matched Pair Sentence Segment test for WER,
a standard measure of significance, to calculate this P-value.
</footnote>
<page confidence="0.992418">
513
</page>
<table confidence="0.654674">
References
Eugene Charniak. 2001. Immediate-head parsing for language
models. In Proc. ACL.
Brian Roark, Murat Saraclar, and Michael Collins. 2004a. Cor-
rective language modeling for large vocabulary ASR with the
perceptron algorithm. In Proc. ICASSP, pages 749–752.
</table>
<reference confidence="0.996087173469387">
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntac-
tic structure for language modeling. In Proceedings of the
36th Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Computa-
tional Linguistics, pages 225–231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283–332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for Nat-
ural Language Modeling. Ph.D. thesis, The Johns Hopkins
University.
Stanley Chen and Joshua Goodman. 1998. An empirical study
of smoothing techniques for language modeling. Technical
Report, TR-10-98, Harvard University.
Michael J. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proc. EMNLP, pages 1–8.
Michael Collins. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-free
methods. In Harry Bunt, John Carroll, and Giorgio Satta,
editors, New Developments in Parsing Technology. Kluwer
Academic Publishers, Dordrecht.
Frederick Jelinek and John Lafferty. 1991. Computation of
the probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315–323.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic “unification-
based” grammars. In Proc. ACL, pages 535–541.
Daniel Jurafsky, Chuck Wooters, Jonathan Segal, Andreas
Stolcke, Eric Fosler, Gary Tajchman, and Nelson Morgan.
1995. Using a stochastic context-free grammar as a lan-
guage model for speech recognition. In Proceedings of the
IEEE Conference on Acoustics, Speech, and Signal Process-
ing, pages 189–192.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML, pages
282–289, Williams College, Williamstown, MA, USA.
Andrej Ljolje, Enrico Bocchieri, Michael Riley, Brian Roark,
Murat Saraclar, and Izhak Shafran. 2003. The AT&amp;T 1xRT
CTS system. In Rich Transcription Workshop.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In Proceedings of HLT-NAACL
2004.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004b. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In Proc.
ACL.
Brian Roark, Murat Saraclar, and Michael Collins. 2005. Dis-
criminative n-gram language modeling. Computer Speech
and Language. submitted.
Brian Roark. 2001a. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27(2):249–
276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. 2001.
Whole-sentence exponential language models: a vehicle for
linguistic-statistical integration. In Computer Speech and
Language.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of the Human
Language Technology Conference and Meeting of the North
American Chapter ofthe Association for Computational Lin-
guistics (HLT-NAACL), Edmonton, Canada.
Andreas Stolcke and Jonathan Segal. 1994. Precise n-gram
probabilities from stochastic context-free grammars. In Pro-
ceedings of the 32nd Annual Meeting of the Association for
Computational Linguistics, pages 74–79.
Andreas Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities. Com-
putational Linguistics, 21(2):165–202.
Wen Wang and Mary P. Harper. 2002. The superARV language
model: Investigating the effectiveness of tightly integrating
multiple knowledge sources. In Proc. EMNLP, pages 238–
247.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004. The
use of a linguistically motivated language model in conver-
sational speech recognition. In Proc. ICASSP.
Wen Wang. 2003. Statistical parsing and language model-
ing based on constraint dependency grammar. Ph.D. thesis,
Purdue University.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A
study on richer syntactic dependencies for structured lan-
guage modeling. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics, pages
191–198.
Peng Xu, Ahmad Emami, and Frederick Jelinek. 2003. Train-
ing connectionist models for the structured language model.
In Proc. EMNLP, pages 160–167.
</reference>
<page confidence="0.997922">
514
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962304">
<title confidence="0.99973">Discriminative Syntactic Language Modeling for Speech Recognition</title>
<author confidence="0.999793">Michael Collins Brian Roark Murat Saraclar</author>
<affiliation confidence="0.999896">MIT CSAIL OGI/OHSU Bogazici University</affiliation>
<email confidence="0.982728">mcollins@csail.mit.eduroark@cslu.ogi.edumurat.saraclar@boun.edu.tr</email>
<abstract confidence="0.999063181818182">We describe a method for discriminative training of a language model that makes use of syntactic features. We follow where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second “reranking” model is then used to choose an utterance from these 1000-best lists. The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm. We describe experiments on the Switchboard speech recognition task. The syntactic features provide an additional 0.3% reduction in test–set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (signifiat &lt; which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>225--231</pages>
<contexts>
<context position="4582" citStr="Chelba and Jelinek, 1998" startWordPosition="756" endWordPosition="759">4; Collins, 2002). The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tag</context>
<context position="7807" citStr="Chelba and Jelinek, 1998" startWordPosition="1280" endWordPosition="1283"> and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the par508 ticular parse. En</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 225–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="4608" citStr="Chelba and Jelinek, 2000" startWordPosition="760" endWordPosition="763">ceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from th</context>
<context position="7833" citStr="Chelba and Jelinek, 2000" startWordPosition="1284" endWordPosition="1287">he featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the par508 ticular parse. Enhancements in the feature </context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>The Johns Hopkins University.</institution>
<contexts>
<context position="4622" citStr="Chelba, 2000" startWordPosition="764" endWordPosition="765">y fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar wer</context>
<context position="7848" citStr="Chelba, 2000" startWordPosition="1288" endWordPosition="1289">ations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the par508 ticular parse. Enhancements in the feature set and improve</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Ciprian Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report, TR-10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="1984" citStr="Chen and Goodman, 1998" startWordPosition="318" endWordPosition="321">itional probability Pa(a|w) to each pair (a, w) where a is a sequence of acoustic vectors, and w is a string. For a given acoustic input a, the highest scoring string under the model is w* = arg max (Q log Pl(w) + log Pa(a|w)) (1) w where Q &gt; 0 is some value that reflects the relative importance of the language model; Q is typically chosen by optimization on held-out data. In an n-gram language model, a Markov assumption is made, namely that each word depends only on the previous (n − 1) words. The parameters of the language model are usually estimated from a large quantity of text data. See (Chen and Goodman, 1998) for an overview of estimation techniques for n-gram models. This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques. We build on the work in Roark et al. (2004a; 2004b), which was summarized and extended in Roark et al. (2005). These papers used discriminative methods for n-gram language models. Our approach reranks the 1000-best output from the Switchboard recognizer of Ljolje et al. (2003).1 Each candidate string w is parsed using the statistical parser of Collins (1999) to give a parse tree T (w). Info</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report, TR-10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2551" citStr="Collins (1999)" startWordPosition="406" endWordPosition="407">tity of text data. See (Chen and Goodman, 1998) for an overview of estimation techniques for n-gram models. This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques. We build on the work in Roark et al. (2004a; 2004b), which was summarized and extended in Roark et al. (2005). These papers used discriminative methods for n-gram language models. Our approach reranks the 1000-best output from the Switchboard recognizer of Ljolje et al. (2003).1 Each candidate string w is parsed using the statistical parser of Collins (1999) to give a parse tree T (w). Information from the parse tree is incorporated in the model using a feature-vector approach: we define 4b(a, w) to be a d-dimensional feature vector which in principle could track arbitrary features of the string w together with the acoustic input a. In this paper we restrict 4b(a, w) to only consider the string w and/or the parse tree T (w) for w. For example, 4b(a, w) might track counts of context-free rule productions in T (w), or bigram lexical dependencies within T (w). The optimal string under our new model is defined as w* � arg max (Q log P1(w) + (a, 4)(a,</context>
<context position="16744" citStr="Collins (1999)" startWordPosition="2825" endWordPosition="2826"> the relative effectiveness of various kinds of syntactic features that can be derived from the output of a parser, we are reporting results using just the perceptron algorithm. This has allowed us to explore more of the potential feature space than we would have been able to do using the more costly GCLM estimation techniques. In future we plan to apply GLCM parameter estimation methods to the task. 3 Parse Tree Features We tagged each candidate transcription with (1) part-of-speech tags, using the tagger documented in Collins (2002); and (2) a full parse tree, using the parser documented in Collins (1999). The models for both of these were trained on the Switchboard 510 Figure 2: An example parse tree treebank, and applied to candidate transcriptions in both the training and test sets. Each transcription received one POS-tag annotation and one parse tree annotation, from which features were extracted. Figure 2 shows a Penn Treebank style parse tree that is of the sort produced by the parser. Given such a structure, there is a tremendous amount of flexibility in selecting features. The first approach that we follow is to map each parse tree to sequences encoding part-of-speech (POS) decisions, </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael J. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3975" citStr="Collins, 2002" startWordPosition="660" endWordPosition="661">e the inner, or dot &apos;Note that (Roark et al., 2004a; Roark et al., 2004b) give results for an n-gram approach on this data which makes use of both lattices and 1000-best lists. The results on 1000-best lists were very close to results on lattices for this domain, suggesting that the 1000-best approximation is a reasonable one. 507 Proceedings of the 43rd Annual Meeting of the ACL, pages 507–514, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics product, between vectors x and y). For this paper, we train the parameter vector a using the perceptron algorithm (Collins, 2004; Collins, 2002). The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jeline</context>
<context position="10629" citStr="Collins (2002" startWordPosition="1735" endWordPosition="1736">tactic features in a Maximum Entropy approach. In their paper, they used a shallow parser to annotate base constituents, and derived features from sequences of base constituents. The features were indicator features that were either (1) exact matches between a set or sequence of base constituents with those annotated on the hypothesis transcription; or (2) tri-tag features from the constituent sequence. The generative model that resulted from their feature set resulted in only a very small improvement in either perplexity or word-error-rate. 2.2 Global Linear Models We follow the framework of Collins (2002; 2004), recently applied to language modeling in Roark et al. (2004a; 2004b). The model we propose consists of the following components: • GEN(a) is a set of candidate strings for an acoustic input a. In our case, GEN(a) is a set of 1000-best strings from a first-pass recognizer. • T (w) is the parse tree for string w. • -1)(a, w) E Rd is a feature-vector representation of an acoustic input a together with a string w. • a E Rd is a parameter vector. • The output of the recognizer for an input a is defined as F (a) = argmax (`b(a, w), a) (3) wEGEN(a) In principle, the feature vector 4b(a, w) c</context>
<context position="16670" citStr="Collins (2002)" startWordPosition="2812" endWordPosition="2813">eatures seen in the training set. For the present paper, to get a sense of the relative effectiveness of various kinds of syntactic features that can be derived from the output of a parser, we are reporting results using just the perceptron algorithm. This has allowed us to explore more of the potential feature space than we would have been able to do using the more costly GCLM estimation techniques. In future we plan to apply GLCM parameter estimation methods to the task. 3 Parse Tree Features We tagged each candidate transcription with (1) part-of-speech tags, using the tagger documented in Collins (2002); and (2) a full parse tree, using the parser documented in Collins (1999). The models for both of these were trained on the Switchboard 510 Figure 2: An example parse tree treebank, and applied to candidate transcriptions in both the training and test sets. Each transcription received one POS-tag annotation and one parse tree annotation, from which features were extracted. Figure 2 shows a Penn Treebank style parse tree that is of the sort produced by the parser. Given such a structure, there is a tremendous amount of flexibility in selecting features. The first approach that we follow is to </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods.</title>
<date>2004</date>
<booktitle>New Developments in Parsing Technology.</booktitle>
<editor>In Harry Bunt, John Carroll, and Giorgio Satta, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="3959" citStr="Collins, 2004" startWordPosition="658" endWordPosition="659">ine (x, y) to be the inner, or dot &apos;Note that (Roark et al., 2004a; Roark et al., 2004b) give results for an n-gram approach on this data which makes use of both lattices and 1000-best lists. The results on 1000-best lists were very close to results on lattices for this domain, suggesting that the 1000-best approximation is a reasonable one. 507 Proceedings of the 43rd Annual Meeting of the ACL, pages 507–514, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics product, between vectors x and y). For this paper, we train the parameter vector a using the perceptron algorithm (Collins, 2004; Collins, 2002). The perceptron algorithm is a very fast training method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (C</context>
<context position="13243" citStr="Collins (2004" startWordPosition="2210" endWordPosition="2211">be how the parameter vector α� is estimated from a set of training utterances. The training set consists of examples (ai, wi) for i = 1... m, where ai is the i’th acoustic input, and wi is the transcription of this input. We briefly review the two training algorithms described in Roark et al. (2004b), the perceptron algorithm and global conditional log-linear models (GCLMs). Figure 1 shows the perceptron algorithm. It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example. For a full description of the algorithm, see Collins (2004; 2002). A second parameter estimation method, which was used in (Roark et al., 2004b), is to optimize the log-likelihood under a log-linear model. Similar approaches have been described in Johnson et al. (1999) and Lafferty et al. (2001). The objective function used in optimizing the parameters is L(a) = � �log P (si|ai, �α) − C α� (4) i j where P(si|ai, �α) = EwEGEN(ai) e(,D(ai,w),a) . e(�(ai,si),�α� Here, each si is the member of GEN(ai) which has lowest WER with respect to the target transcription wi. The first term in L(a) is the log-likelihood of the training data under a conditional log</context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Michael Collins. 2004. Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods. In Harry Bunt, John Carroll, and Giorgio Satta, editors, New Developments in Parsing Technology. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="7617" citStr="Jelinek and Lafferty, 1991" startWordPosition="1247" endWordPosition="1250"> domain; the syntactic features we describe give a further 0.3% reduction. In the remainder of this paper, section 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>Frederick Jelinek and John Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Steven Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unificationbased” grammars.</title>
<date>1999</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>535--541</pages>
<contexts>
<context position="13454" citStr="Johnson et al. (1999)" startWordPosition="2242" endWordPosition="2245">ion of this input. We briefly review the two training algorithms described in Roark et al. (2004b), the perceptron algorithm and global conditional log-linear models (GCLMs). Figure 1 shows the perceptron algorithm. It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example. For a full description of the algorithm, see Collins (2004; 2002). A second parameter estimation method, which was used in (Roark et al., 2004b), is to optimize the log-likelihood under a log-linear model. Similar approaches have been described in Johnson et al. (1999) and Lafferty et al. (2001). The objective function used in optimizing the parameters is L(a) = � �log P (si|ai, �α) − C α� (4) i j where P(si|ai, �α) = EwEGEN(ai) e(,D(ai,w),a) . e(�(ai,si),�α� Here, each si is the member of GEN(ai) which has lowest WER with respect to the target transcription wi. The first term in L(a) is the log-likelihood of the training data under a conditional log-linear model. The second term is a regularization term which penalizes large parameter values. C is a constant that dictates the relative weighting given to the two terms. The optimal parameters are defined as </context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unificationbased” grammars. In Proc. ACL, pages 535–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>Chuck Wooters</author>
<author>Jonathan Segal</author>
<author>Andreas Stolcke</author>
<author>Eric Fosler</author>
<author>Gary Tajchman</author>
<author>Nelson Morgan</author>
</authors>
<title>Using a stochastic context-free grammar as a language model for speech recognition.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="7749" citStr="Jurafsky et al., 1995" startWordPosition="1270" endWordPosition="1273">work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two </context>
</contexts>
<marker>Jurafsky, Wooters, Segal, Stolcke, Fosler, Tajchman, Morgan, 1995</marker>
<rawString>Daniel Jurafsky, Chuck Wooters, Jonathan Segal, Andreas Stolcke, Eric Fosler, Gary Tajchman, and Nelson Morgan. 1995. Using a stochastic context-free grammar as a language model for speech recognition. In Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML,</booktitle>
<pages>282--289</pages>
<location>Williams College, Williamstown, MA, USA.</location>
<contexts>
<context position="13481" citStr="Lafferty et al. (2001)" startWordPosition="2247" endWordPosition="2250">fly review the two training algorithms described in Roark et al. (2004b), the perceptron algorithm and global conditional log-linear models (GCLMs). Figure 1 shows the perceptron algorithm. It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example. For a full description of the algorithm, see Collins (2004; 2002). A second parameter estimation method, which was used in (Roark et al., 2004b), is to optimize the log-likelihood under a log-linear model. Similar approaches have been described in Johnson et al. (1999) and Lafferty et al. (2001). The objective function used in optimizing the parameters is L(a) = � �log P (si|ai, �α) − C α� (4) i j where P(si|ai, �α) = EwEGEN(ai) e(,D(ai,w),a) . e(�(ai,si),�α� Here, each si is the member of GEN(ai) which has lowest WER with respect to the target transcription wi. The first term in L(a) is the log-likelihood of the training data under a conditional log-linear model. The second term is a regularization term which penalizes large parameter values. C is a constant that dictates the relative weighting given to the two terms. The optimal parameters are defined as a* = arg max a L(a) We refe</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML, pages 282–289, Williams College, Williamstown, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Ljolje</author>
<author>Enrico Bocchieri</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Izhak Shafran</author>
</authors>
<date>2003</date>
<booktitle>The AT&amp;T 1xRT CTS system. In Rich Transcription Workshop.</booktitle>
<contexts>
<context position="2468" citStr="Ljolje et al. (2003)" startWordPosition="391" endWordPosition="394"> − 1) words. The parameters of the language model are usually estimated from a large quantity of text data. See (Chen and Goodman, 1998) for an overview of estimation techniques for n-gram models. This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques. We build on the work in Roark et al. (2004a; 2004b), which was summarized and extended in Roark et al. (2005). These papers used discriminative methods for n-gram language models. Our approach reranks the 1000-best output from the Switchboard recognizer of Ljolje et al. (2003).1 Each candidate string w is parsed using the statistical parser of Collins (1999) to give a parse tree T (w). Information from the parse tree is incorporated in the model using a feature-vector approach: we define 4b(a, w) to be a d-dimensional feature vector which in principle could track arbitrary features of the string w together with the acoustic input a. In this paper we restrict 4b(a, w) to only consider the string w and/or the parse tree T (w) for w. For example, 4b(a, w) might track counts of context-free rule productions in T (w), or bigram lexical dependencies within T (w). The opt</context>
<context position="6905" citStr="Ljolje et al. (2003)" startWordPosition="1141" endWordPosition="1144">lated to the end goal in speech recognition, i.e., word error rate. Previous work (Roark et al., 2004a; Roark et al., 2004b) has shown that discriminative methods within an ngram approach can lead to significant reductions in WER, in spite of the features being of the same type as the original language model. In this paper we extend this approach, by including syntactic features that were not in the baseline speech recognizer. This paper describe experiments using a variety of syntactic features within this approach. We tested the model on the Switchboard (SWB) domain, using the recognizer of Ljolje et al. (2003). The discriminative approach for n-gram modeling gave a 0.9% reduction in WER on this domain; the syntactic features we describe give a further 0.3% reduction. In the remainder of this paper, section 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early ap</context>
</contexts>
<marker>Ljolje, Bocchieri, Riley, Roark, Saraclar, Shafran, 2003</marker>
<rawString>Andrej Ljolje, Enrico Bocchieri, Michael Riley, Brian Roark, Murat Saraclar, and Izhak Shafran. 2003. The AT&amp;T 1xRT CTS system. In Rich Transcription Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. In Proceedings of HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="878" citStr="Roark et al., 2004" startWordPosition="124" endWordPosition="127">ative training of a language model that makes use of syntactic features. We follow a reranking approach, where a baseline recogniser is used to produce 1000-best output for each acoustic input, and a second “reranking” model is then used to choose an utterance from these 1000-best lists. The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm. We describe experiments on the Switchboard speech recognition task. The syntactic features provide an additional 0.3% reduction in test–set error rate beyond the model of (Roark et al., 2004a; Roark et al., 2004b) (significant at p &lt; 0.001), which makes use of a discriminatively trained n-gram model, giving a total reduction of 1.2% over the baseline Switchboard system. 1 Introduction The predominant approach within language modeling for speech recognition has been to use an ngram language model, within the “source-channel” or “noisy-channel” paradigm. The language model assigns a probability Pl(w) to each string w in the language; the acoustic model assigns a conditional probability Pa(a|w) to each pair (a, w) where a is a sequence of acoustic vectors, and w is a string. For a g</context>
<context position="2233" citStr="Roark et al. (2004" startWordPosition="355" endWordPosition="358">alue that reflects the relative importance of the language model; Q is typically chosen by optimization on held-out data. In an n-gram language model, a Markov assumption is made, namely that each word depends only on the previous (n − 1) words. The parameters of the language model are usually estimated from a large quantity of text data. See (Chen and Goodman, 1998) for an overview of estimation techniques for n-gram models. This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques. We build on the work in Roark et al. (2004a; 2004b), which was summarized and extended in Roark et al. (2005). These papers used discriminative methods for n-gram language models. Our approach reranks the 1000-best output from the Switchboard recognizer of Ljolje et al. (2003).1 Each candidate string w is parsed using the statistical parser of Collins (1999) to give a parse tree T (w). Information from the parse tree is incorporated in the model using a feature-vector approach: we define 4b(a, w) to be a d-dimensional feature vector which in principle could track arbitrary features of the string w together with the acoustic input a. I</context>
<context position="6386" citStr="Roark et al., 2004" startWordPosition="1055" endWordPosition="1058">ng a generative model is a rather indirect way of changing the features used by a model. In this respect, our approach is similar to that advocated in Rosenfeld et al. (2001), which used Maximum Entropy modeling to allow for the use of shallow syntactic features for language modeling. A second contrast between our work and previous work, including that of Rosenfeld et al. (2001), is in the use of discriminative parameter estimation techniques. The criterion we use to optimize the parameter vector a is closely related to the end goal in speech recognition, i.e., word error rate. Previous work (Roark et al., 2004a; Roark et al., 2004b) has shown that discriminative methods within an ngram approach can lead to significant reductions in WER, in spite of the features being of the same type as the original language model. In this paper we extend this approach, by including syntactic features that were not in the baseline speech recognizer. This paper describe experiments using a variety of syntactic features within this approach. We tested the model on the Switchboard (SWB) domain, using the recognizer of Ljolje et al. (2003). The discriminative approach for n-gram modeling gave a 0.9% reduction in WER on</context>
<context position="10697" citStr="Roark et al. (2004" startWordPosition="1744" endWordPosition="1747">they used a shallow parser to annotate base constituents, and derived features from sequences of base constituents. The features were indicator features that were either (1) exact matches between a set or sequence of base constituents with those annotated on the hypothesis transcription; or (2) tri-tag features from the constituent sequence. The generative model that resulted from their feature set resulted in only a very small improvement in either perplexity or word-error-rate. 2.2 Global Linear Models We follow the framework of Collins (2002; 2004), recently applied to language modeling in Roark et al. (2004a; 2004b). The model we propose consists of the following components: • GEN(a) is a set of candidate strings for an acoustic input a. In our case, GEN(a) is a set of 1000-best strings from a first-pass recognizer. • T (w) is the parse tree for string w. • -1)(a, w) E Rd is a feature-vector representation of an acoustic input a together with a string w. • a E Rd is a parameter vector. • The output of the recognizer for an input a is defined as F (a) = argmax (`b(a, w), a) (3) wEGEN(a) In principle, the feature vector 4b(a, w) could take into account any features of the acoustic input a together</context>
<context position="12080" citStr="Roark et al., 2004" startWordPosition="2006" endWordPosition="2009">nd P,,,(a|w) are language and acoustic model scores from the baseline speech recognizer. In our experiments we kept Q fixed at the value used in the baseline recogniser. It can then be seen that our model is equivalent to the model in Eq. 2. Second, we restrict the remaining features &apos;b2(a, w) ...&apos;bd(a, w) to be sensitive to the string w alone. In this sense, the scope of this paper is limited to the language modeling problem. As one example, the language modeling features might take into account n-grams, for example through definitions such as 4b2(a, w) = Count of the the in w Previous work (Roark et al., 2004a; Roark et al., 2004b) considered features of this type. In this paper, we introduce syntactic features, which may be sensitive to the parse tree for w, for example 4g(a, w) = Count of S —* NP VP in T (w) where S —* NP VP is a context-free rule production. Section 3 describes the full set of features used in the empirical results presented in this paper. 2Future work may consider features of the acoustic sequence a together with the string w, allowing the approach to be applied to acoustic modeling. 509 2.2.1 Parameter Estimation We now describe how the parameter vector α� is estimated from a</context>
<context position="13327" citStr="Roark et al., 2004" startWordPosition="2222" endWordPosition="2225">The training set consists of examples (ai, wi) for i = 1... m, where ai is the i’th acoustic input, and wi is the transcription of this input. We briefly review the two training algorithms described in Roark et al. (2004b), the perceptron algorithm and global conditional log-linear models (GCLMs). Figure 1 shows the perceptron algorithm. It is an online algorithm, which makes several passes over the training set, updating the parameter vector after each training example. For a full description of the algorithm, see Collins (2004; 2002). A second parameter estimation method, which was used in (Roark et al., 2004b), is to optimize the log-likelihood under a log-linear model. Similar approaches have been described in Johnson et al. (1999) and Lafferty et al. (2001). The objective function used in optimizing the parameters is L(a) = � �log P (si|ai, �α) − C α� (4) i j where P(si|ai, �α) = EwEGEN(ai) e(,D(ai,w),a) . e(�(ai,si),�α� Here, each si is the member of GEN(ai) which has lowest WER with respect to the target transcription wi. The first term in L(a) is the log-likelihood of the training data under a conditional log-linear model. The second term is a regularization term which penalizes large parame</context>
<context position="15490" citStr="Roark et al. (2004" startWordPosition="2607" endWordPosition="2610">EN(ai) for each training utterance. We take si to be the member of GEN(ai) which has the lowest WER when compared to wi. Initialization: Set α1 = α, and αj = 0 for j = 2...d. Algorithm: For t = 1... T, i = 1... m •Calculate yi = arg maxwEGEN(ai) h4&apos;(ai, w), ai • For j = 2 ... m, set aj = aj + 4bj(ai, si) − -(Dj(ai, yi) Output: Either the final parameters a, or the averaged parameters aavg defined as �vg = �t,i at,i/mT where at,i is the parameter vector after training on the i’th training example on the t’th pass through the training data. Figure 1: The perceptron training algorithm. Following Roark et al. (2004a), the parameter α1 is set to be some constant α that is typically chosen through optimization over the development set. Recall that α1 dictates the weight given to the baseline recognizer score. at each training example it only increments features seen on si or yi, effectively ignoring all other features seen on members of GEN(ai). For example, in the experiments in Roark et al. (2004a), the perceptron converged in around 3 passes over the training set, while picking non-zero values for around 1.4 million n-gram features out of a possible 41 million n-gram features seen in the training set. </context>
<context position="22279" citStr="Roark et al. (2004" startWordPosition="3711" endWordPosition="3714">head child CZ, it is either to the left or right of HCP, and is either adjacent or non-adjacent to HCP. We denote these positional features as an integer, positive if to the right, negative if to the left, 1 if adjacent, and 2 if non-adjacent. Table 1 shows four head-to-head features that can be extracted for each non-head child CZ. These features include dependencies between pairs of lexical items, between a single lexical item and the part-of-speech of another item, and between pairs of part-of-speech tags in the parse. 4 Experiments The experimental set-up we use is very similar to that of Roark et al. (2004a; 2004b), and the extensions to that work in Roark et al. (2005). We make use of the Rich Transcription 2002 evaluation test set (rt02) as our development set, and use the Rich Transcription 2003 Spring evaluation CTS test set (rt03) as test set. The rt02 set consists of 6081 sentences (63804 words) and has three subsets: Switchboard 1, Switchboard 2, Switchboard Cellular. The rt03 set consists of 9050 sentences (76083 words) and has two subsets: Switchboard and Fisher. The training set consists of 297580 transcribed utterances (3297579 words)4. For each utterance, 4Note that Roark et al. (20</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004b. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling. Computer Speech and Language.</title>
<date>2005</date>
<note>submitted.</note>
<contexts>
<context position="2300" citStr="Roark et al. (2005)" startWordPosition="366" endWordPosition="369">Q is typically chosen by optimization on held-out data. In an n-gram language model, a Markov assumption is made, namely that each word depends only on the previous (n − 1) words. The parameters of the language model are usually estimated from a large quantity of text data. See (Chen and Goodman, 1998) for an overview of estimation techniques for n-gram models. This paper describes a method for incorporating syntactic features into the language model, using discriminative parameter estimation techniques. We build on the work in Roark et al. (2004a; 2004b), which was summarized and extended in Roark et al. (2005). These papers used discriminative methods for n-gram language models. Our approach reranks the 1000-best output from the Switchboard recognizer of Ljolje et al. (2003).1 Each candidate string w is parsed using the statistical parser of Collins (1999) to give a parse tree T (w). Information from the parse tree is incorporated in the model using a feature-vector approach: we define 4b(a, w) to be a d-dimensional feature vector which in principle could track arbitrary features of the string w together with the acoustic input a. In this paper we restrict 4b(a, w) to only consider the string w and</context>
<context position="22344" citStr="Roark et al. (2005)" startWordPosition="3724" endWordPosition="3727">either adjacent or non-adjacent to HCP. We denote these positional features as an integer, positive if to the right, negative if to the left, 1 if adjacent, and 2 if non-adjacent. Table 1 shows four head-to-head features that can be extracted for each non-head child CZ. These features include dependencies between pairs of lexical items, between a single lexical item and the part-of-speech of another item, and between pairs of part-of-speech tags in the parse. 4 Experiments The experimental set-up we use is very similar to that of Roark et al. (2004a; 2004b), and the extensions to that work in Roark et al. (2005). We make use of the Rich Transcription 2002 evaluation test set (rt02) as our development set, and use the Rich Transcription 2003 Spring evaluation CTS test set (rt03) as test set. The rt02 set consists of 6081 sentences (63804 words) and has three subsets: Switchboard 1, Switchboard 2, Switchboard Cellular. The rt03 set consists of 9050 sentences (76083 words) and has two subsets: Switchboard and Fisher. The training set consists of 297580 transcribed utterances (3297579 words)4. For each utterance, 4Note that Roark et al. (2004a; 2004b; 2005) used 20854 of these utterances (249774 words) a</context>
<context position="25236" citStr="Roark et al. (2005)" startWordPosition="4198" endWordPosition="4201">t with a language model that was trained on the 27 data portions that did not include the current utterance. Since language models are generally far more prone to overtraining than standard acoustic models, this goes a long way toward making the training conditions similar to testing conditions. Similar procedures were used to train the parsing and tagging models for the training set, since the Switchboard treebank overlaps extensively with the ASR training utterances. Table 2 presents the word-error rates on rt02 and rt03 of the baseline ASR system, 1000-best perceptron and GCLM results from Roark et al. (2005) under this condition, and our 1000-best perceptron results. Note that our n-best result, using just ngram features, improves upon the perceptron result of (Roark et al., 2005) by 0.2 percent, putting us within 0.1 percent of their GCLM result for that 512 Trial WER rt02 rt03 ASR system output 37.1 36.4 Roark et al. (2005) perceptron 36.6 35.7 Roark et al. (2005) GCLM 36.3 35.4 n-gram perceptron 36.4 35.5 Table 2: Baseline word-error rates versus Roark et al. (2005) Trial rt02 WER ASR system output 37.1 n-gram perceptron 36.4 n-gram + POS (1) perceptron 36.1 n-gram + POS (1,2) perceptron 36.1 </context>
</contexts>
<marker>Roark, Saraclar, Collins, 2005</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2005. Discriminative n-gram language modeling. Computer Speech and Language. submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<pages>276</pages>
<contexts>
<context position="4901" citStr="Roark, 2001" startWordPosition="810" endWordPosition="811">hods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations 4b(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the m</context>
<context position="8573" citStr="Roark (2001" startWordPosition="1414" endWordPosition="1415">et of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the par508 ticular parse. Enhancements in the feature set and improved parameter estimation techniques have extended this approach in recent years (Xu et al., 2002; Xu et al., 2003). Roark (2001a; 2001b) pursued a different derivation strategy from Chelba and Jelinek, and used the parse probabilities directly to calculate the string probabilities. This work made use of a left-to-right, top-down, beam-search parser, which exploits rich lexico-syntactic features from the left context of each derivation to condition derivation move probabilities, leading to a very peaked distribution. Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summing the probabilities of all de</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001a. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249– 276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Robust Probabilistic Predictive Syntactic Processing.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<note>http://arXiv.org/abs/cs/0105019.</note>
<contexts>
<context position="4901" citStr="Roark, 2001" startWordPosition="810" endWordPosition="811">hods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations 4b(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the m</context>
<context position="8573" citStr="Roark (2001" startWordPosition="1414" endWordPosition="1415">et of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the par508 ticular parse. Enhancements in the feature set and improved parameter estimation techniques have extended this approach in recent years (Xu et al., 2002; Xu et al., 2003). Roark (2001a; 2001b) pursued a different derivation strategy from Chelba and Jelinek, and used the parse probabilities directly to calculate the string probabilities. This work made use of a left-to-right, top-down, beam-search parser, which exploits rich lexico-syntactic features from the left context of each derivation to condition derivation move probabilities, leading to a very peaked distribution. Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summing the probabilities of all de</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001b. Robust Probabilistic Predictive Syntactic Processing. Ph.D. thesis, Brown University. http://arXiv.org/abs/cs/0105019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
<author>Stanley Chen</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Whole-sentence exponential language models: a vehicle for linguistic-statistical integration.</title>
<date>2001</date>
<booktitle>In Computer Speech and Language.</booktitle>
<contexts>
<context position="5942" citStr="Rosenfeld et al. (2001)" startWordPosition="980" endWordPosition="983">important respects. First, through the featurevector representations 4b(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modification of the underlying generative parsing or tagging model, and modifying a generative model is a rather indirect way of changing the features used by a model. In this respect, our approach is similar to that advocated in Rosenfeld et al. (2001), which used Maximum Entropy modeling to allow for the use of shallow syntactic features for language modeling. A second contrast between our work and previous work, including that of Rosenfeld et al. (2001), is in the use of discriminative parameter estimation techniques. The criterion we use to optimize the parameter vector a is closely related to the end goal in speech recognition, i.e., word error rate. Previous work (Roark et al., 2004a; Roark et al., 2004b) has shown that discriminative methods within an ngram approach can lead to significant reductions in WER, in spite of the features b</context>
<context position="9988" citStr="Rosenfeld et al. (2001)" startWordPosition="1632" endWordPosition="1635">e string probabilities. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar, were used to exploit syntactic dependencies. The processing advantages of the finite-state encoding of the model has allowed for the use of probabilities calculated off-line from this model to be used in the first pass of decoding, which has provided additional benefits. Finally, Och et al. (2004) use a reranking approach with syntactic information within a machine translation system. Rosenfeld et al. (2001) investigated the use of syntactic features in a Maximum Entropy approach. In their paper, they used a shallow parser to annotate base constituents, and derived features from sequences of base constituents. The features were indicator features that were either (1) exact matches between a set or sequence of base constituents with those annotated on the hypothesis transcription; or (2) tri-tag features from the constituent sequence. The generative model that resulted from their feature set resulted in only a very small improvement in either perplexity or word-error-rate. 2.2 Global Linear Models</context>
<context position="17442" citStr="Rosenfeld et al., 2001" startWordPosition="2936" endWordPosition="2939"> An example parse tree treebank, and applied to candidate transcriptions in both the training and test sets. Each transcription received one POS-tag annotation and one parse tree annotation, from which features were extracted. Figure 2 shows a Penn Treebank style parse tree that is of the sort produced by the parser. Given such a structure, there is a tremendous amount of flexibility in selecting features. The first approach that we follow is to map each parse tree to sequences encoding part-of-speech (POS) decisions, and “shallow” parsing decisions. Similar representations have been used by (Rosenfeld et al., 2001; Wang and Harper, 2002). Figure 3 shows the sequential representations that we used. The first simply makes use of the POS tags for each word. The latter representations make use of sequences of non-terminals associated with lexical items. In 3(b), each word in the string is associated with the beginning or continuation of a shallow phrase or “chunk” in the tree. We include any non-terminals above the level of POS tags as potential chunks: a new “chunk” (VP, NP, PP etc.) begins whenever we see the initial word of the phrase dominated by the non-terminal. In 3(c), we show how POS tags can be a</context>
</contexts>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. 2001. Whole-sentence exponential language models: a vehicle for linguistic-statistical integration. In Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL),</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="14239" citStr="Sha and Pereira (2003)" startWordPosition="2381" endWordPosition="2384"> e(,D(ai,w),a) . e(�(ai,si),�α� Here, each si is the member of GEN(ai) which has lowest WER with respect to the target transcription wi. The first term in L(a) is the log-likelihood of the training data under a conditional log-linear model. The second term is a regularization term which penalizes large parameter values. C is a constant that dictates the relative weighting given to the two terms. The optimal parameters are defined as a* = arg max a L(a) We refer to these models as global conditional loglinear models (GCLMs). Each of these algorithms has advantages. A number of results—e.g., in Sha and Pereira (2003) and Roark et al. (2004b)—suggest that the GCLM approach leads to slightly higher accuracy than the perceptron training method. However the perceptron converges very quickly, often in just a few passes over the training set—in comparison GCLM’s can take tens or hundreds of gradient calculations before convergence. In addition, the perceptron can be used as an effective feature selection technique, in that Input: A parameter specifying the number of iterations over the training set, T. A value for the first parameter, α. A feature-vector representation 4)(a, w) E Rd. Training examples (ai, wi) </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jonathan Segal</author>
</authors>
<title>Precise n-gram probabilities from stochastic context-free grammars.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>74--79</pages>
<contexts>
<context position="7725" citStr="Stolcke and Segal, 1994" startWordPosition="1265" endWordPosition="1269">ion 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is con</context>
</contexts>
<marker>Stolcke, Segal, 1994</marker>
<rawString>Andreas Stolcke and Jonathan Segal. 1994. Precise n-gram probabilities from stochastic context-free grammars. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 74–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="7633" citStr="Stolcke, 1995" startWordPosition="1251" endWordPosition="1253">res we describe give a further 0.3% reduction. In the remainder of this paper, section 2 describes previous work, including the parameter estimation methods we use, and section 3 describes the featurevector representations of parse trees that we used in our experiments. Section 4 describes experiments using the approach. 2 Background 2.1 Previous Work Techniques for exploiting stochastic context-free grammars for language modeling have been explored for more than a decade. Early approaches included algorithms for efficiently calculating string prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995) and approaches to exploit such algorithms to produce n-gram models (Stolcke and Segal, 1994; Jurafsky et al., 1995). The work of Chelba and Jelinek (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000) involved the use of a shift-reduce parser trained on Penn treebank style annotations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditio</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Mary P Harper</author>
</authors>
<title>The superARV language model: Investigating the effectiveness of tightly integrating multiple knowledge sources.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>238--247</pages>
<contexts>
<context position="5094" citStr="Wang and Harper, 2002" startWordPosition="838" endWordPosition="841">ry similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations 4b(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modif</context>
<context position="9451" citStr="Wang and Harper, 2002" startWordPosition="1547" endWordPosition="1550">ctic features from the left context of each derivation to condition derivation move probabilities, leading to a very peaked distribution. Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summing the probabilities of all derivations for that string in the beam. Other work on syntactic language modeling includes that of Charniak (2001), which made use of a non-incremental, head-driven statistical parser to produce string probabilities. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar, were used to exploit syntactic dependencies. The processing advantages of the finite-state encoding of the model has allowed for the use of probabilities calculated off-line from this model to be used in the first pass of decoding, which has provided additional benefits. Finally, Och et al. (2004) use a reranking approach with syntactic information within a machine translation system. Rosenfeld et al. (2001) investigated the use of syntactic features in a Maximum Entrop</context>
<context position="17466" citStr="Wang and Harper, 2002" startWordPosition="2940" endWordPosition="2943">reebank, and applied to candidate transcriptions in both the training and test sets. Each transcription received one POS-tag annotation and one parse tree annotation, from which features were extracted. Figure 2 shows a Penn Treebank style parse tree that is of the sort produced by the parser. Given such a structure, there is a tremendous amount of flexibility in selecting features. The first approach that we follow is to map each parse tree to sequences encoding part-of-speech (POS) decisions, and “shallow” parsing decisions. Similar representations have been used by (Rosenfeld et al., 2001; Wang and Harper, 2002). Figure 3 shows the sequential representations that we used. The first simply makes use of the POS tags for each word. The latter representations make use of sequences of non-terminals associated with lexical items. In 3(b), each word in the string is associated with the beginning or continuation of a shallow phrase or “chunk” in the tree. We include any non-terminals above the level of POS tags as potential chunks: a new “chunk” (VP, NP, PP etc.) begins whenever we see the initial word of the phrase dominated by the non-terminal. In 3(c), we show how POS tags can be added to these sequences.</context>
</contexts>
<marker>Wang, Harper, 2002</marker>
<rawString>Wen Wang and Mary P. Harper. 2002. The superARV language model: Investigating the effectiveness of tightly integrating multiple knowledge sources. In Proc. EMNLP, pages 238– 247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Andreas Stolcke</author>
<author>Mary P Harper</author>
</authors>
<title>The use of a linguistically motivated language model in conversational speech recognition.</title>
<date>2004</date>
<booktitle>In Proc. ICASSP.</booktitle>
<contexts>
<context position="5126" citStr="Wang et al., 2004" startWordPosition="844" endWordPosition="847">m language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations 4b(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modification of the underlying genera</context>
<context position="9483" citStr="Wang et al., 2004" startWordPosition="1553" endWordPosition="1556"> of each derivation to condition derivation move probabilities, leading to a very peaked distribution. Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summing the probabilities of all derivations for that string in the beam. Other work on syntactic language modeling includes that of Charniak (2001), which made use of a non-incremental, head-driven statistical parser to produce string probabilities. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar, were used to exploit syntactic dependencies. The processing advantages of the finite-state encoding of the model has allowed for the use of probabilities calculated off-line from this model to be used in the first pass of decoding, which has provided additional benefits. Finally, Och et al. (2004) use a reranking approach with syntactic information within a machine translation system. Rosenfeld et al. (2001) investigated the use of syntactic features in a Maximum Entropy approach. In their paper, they</context>
</contexts>
<marker>Wang, Stolcke, Harper, 2004</marker>
<rawString>Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004. The use of a linguistically motivated language model in conversational speech recognition. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
</authors>
<title>Statistical parsing and language modeling based on constraint dependency grammar.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Purdue University.</institution>
<contexts>
<context position="5106" citStr="Wang, 2003" startWordPosition="842" endWordPosition="843">tional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic dependencies. Our approach differs from previous work in a couple of important respects. First, through the featurevector representations 4b(a, w) we can essentially incorporate arbitrary sources of information from the string or parse tree into the model. We would argue that our method allows considerably more flexibility in terms of the choice of features in the model; in previous work features were incorporated in the model through modification of t</context>
<context position="9463" citStr="Wang, 2003" startWordPosition="1551" endWordPosition="1552">left context of each derivation to condition derivation move probabilities, leading to a very peaked distribution. Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summing the probabilities of all derivations for that string in the beam. Other work on syntactic language modeling includes that of Charniak (2001), which made use of a non-incremental, head-driven statistical parser to produce string probabilities. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar, were used to exploit syntactic dependencies. The processing advantages of the finite-state encoding of the model has allowed for the use of probabilities calculated off-line from this model to be used in the first pass of decoding, which has provided additional benefits. Finally, Och et al. (2004) use a reranking approach with syntactic information within a machine translation system. Rosenfeld et al. (2001) investigated the use of syntactic features in a Maximum Entropy approach. </context>
</contexts>
<marker>Wang, 2003</marker>
<rawString>Wen Wang. 2003. Statistical parsing and language modeling based on constraint dependency grammar. Ph.D. thesis, Purdue University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>191--198</pages>
<contexts>
<context position="4639" citStr="Xu et al., 2002" startWordPosition="766" endWordPosition="769">g method, in practice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit</context>
<context position="8542" citStr="Xu et al., 2002" startWordPosition="1406" endWordPosition="1409">ations, that maintains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the par508 ticular parse. Enhancements in the feature set and improved parameter estimation techniques have extended this approach in recent years (Xu et al., 2002; Xu et al., 2003). Roark (2001a; 2001b) pursued a different derivation strategy from Chelba and Jelinek, and used the parse probabilities directly to calculate the string probabilities. This work made use of a left-to-right, top-down, beam-search parser, which exploits rich lexico-syntactic features from the left context of each derivation to condition derivation move probabilities, leading to a very peaked distribution. Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summ</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 191–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ahmad Emami</author>
<author>Frederick Jelinek</author>
</authors>
<title>Training connectionist models for the structured language model.</title>
<date>2003</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="4657" citStr="Xu et al., 2003" startWordPosition="770" endWordPosition="773">tice requiring only a few passes over the training set, allowing for a detailed comparison of a wide variety of feature sets. A number of researchers have described work that incorporates syntactic language models into a speech recognizer. These methods have almost exclusively worked within the noisy channel paradigm, where the syntactic language model has the task of modeling a distribution over strings in the language, in a very similar way to traditional n-gram language models. The Structured Language Model (Chelba and Jelinek, 1998; Chelba and Jelinek, 2000; Chelba, 2000; Xu et al., 2002; Xu et al., 2003) makes use of an incremental shift-reduce parser to enable the probability of words to be conditioned on k previous c-commanding lexical heads, rather than simply on the previous k words. Incremental topdown and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models. In the work of Wen Wang and Mary Harper (Wang and Harper, 2002; Wang, 2003; Wang et al., 2004), a constraint dependency grammar and a finite-state tagging model derived from that grammar were used to exploit syntactic depende</context>
<context position="8560" citStr="Xu et al., 2003" startWordPosition="1410" endWordPosition="1413">tains a weighted set of parses as it traverses the string from left-to-right. Each word is predicted by each candidate parse in this set at the point when the word is shifted, and the conditional probability of the word given the previous words is taken as the weighted sum of the conditional probabilities provided by each parse. In this approach, the probability of a word is conditioned by the top two lexical heads on the stack of the par508 ticular parse. Enhancements in the feature set and improved parameter estimation techniques have extended this approach in recent years (Xu et al., 2002; Xu et al., 2003). Roark (2001a; 2001b) pursued a different derivation strategy from Chelba and Jelinek, and used the parse probabilities directly to calculate the string probabilities. This work made use of a left-to-right, top-down, beam-search parser, which exploits rich lexico-syntactic features from the left context of each derivation to condition derivation move probabilities, leading to a very peaked distribution. Rather than normalizing a prediction of the next word over the beam of candidates, as in Chelba and Jelinek, in this approach the string probability is derived by simply summing the probabilit</context>
</contexts>
<marker>Xu, Emami, Jelinek, 2003</marker>
<rawString>Peng Xu, Ahmad Emami, and Frederick Jelinek. 2003. Training connectionist models for the structured language model. In Proc. EMNLP, pages 160–167.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>