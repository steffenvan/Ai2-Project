<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000909">
<title confidence="0.998344">
Efficient Algorithms for Richer Formalisms:
Parsing and Machine Translation
</title>
<author confidence="0.998591">
Liang Huang
</author>
<affiliation confidence="0.998255">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<email confidence="0.968891">
lhuang3@cis.upenn.edu
</email>
<bodyText confidence="0.999738384615385">
My PhD research has been on the algorithmic and
formal aspects of computational linguistics, esp. in
the areas of parsing and machine translation. I am
interested in developing efficient algorithms for for-
malisms with rich expressive power, so that we can
have a better modeling of human languages without
sacrificing efficiency. In doing so, I hope to help in-
tegrating more linguistic and structural knowledge
with modern statistical techniques, and in particular,
for syntax-based machine translation (MT) systems.
Among other projects, I have been working on k-
best parsing, synchronous binarization, and syntax-
directed translation.
</bodyText>
<sectionHeader confidence="0.620572" genericHeader="method">
1 k-best Parsing and Hypergraphs
</sectionHeader>
<bodyText confidence="0.99985036">
NLP systems are often cascades of several modules,
e.g., part-of-speech tagging, then syntactic parsing,
and finally semantic interpretation. It is often the
case that the 1-best output from one module is not
always optimal for the next module. So one might
want to postpone some disambiguation by propa-
gating k-best lists (instead of 1-best solutions) to
subsequent phases, as in joint parsing and seman-
tic role-labeling (Gildea and Jurafsky, 2002). This
is also true for reranking and discriminative train-
ing, where the k-best list of candidates serves as an
approximation of the full set (Collins, 2000; Och,
2003; McDonald et al., 2005). In this way we can
optimize some complicated objective function on
the k-best set, rather than on the full search space
which is usually exponentially large.
Previous algorithms for k-best parsing (Collins,
2000; Charniak and Johnson, 2005) are either sub-
optimal or slow and rely significantly on prun-
ing techniques to make them tractable. So I co-
developed several fast and exact algorithms for k-
best parsing in the general framework of directed
monotonic hypergraphs (Huang and Chiang, 2005).
This formulation extends and refines Klein and
Manning’s work (2001) by introducing monotonic
</bodyText>
<figure confidence="0.694859">
1 10 100 1000 10000
k
</figure>
<figureCaption confidence="0.9948555">
Figure 1: Average parsing speed on the Section 23
of Penn Treebank (Algorithms 0, 1, and 3, log-log).
</figureCaption>
<bodyText confidence="0.99512308">
weightfunctions, which is closely related to the opti-
mal subproblem property in dynamic programming.
We first generalize the classical 1-best Viterbi al-
gorithm to hypergraphs, and then present four k-best
algorithms, each improving its predessor by delay-
ing more work until necessary. The final one, Al-
gorithm 3, starts with a normal 1-best search for
each vertex (or item, as in deductive frameworks),
and then works backwards from the target vertex (fi-
nal item) for its 2nd, 3rd, ..., kth best derivations,
calling itself recursively only on demand, being the
laziest of the four algorithms. When tested on top
of two state-of-the-art systems, the Collins/Bikel
parser (Bikel, 2004) and Chiang’s CKY-based Hiero
decoder (Chiang, 2005), this algorithm is shown to
have very little overhead even for quite large k (say,
106) (See Fig. 1 for experiments on Bikel parser).
These algorithms have been re-implemented by
other researchers in the field, including Eugene
Charniak for his n-best parser, Ryan McDonald for
his dependency parser (McDonald et al., 2005), Mi-
crosoft Research NLP group (Simon Corston-Oliver
and Kevin Duh, p.c.) for a similar model, Jonathan
Graehl for the ISI syntax-based MT decoder, David
A. Smith for the Dyna language (Eisner et al., 2005),
</bodyText>
<figure confidence="0.996509909090909">
Average Parsing Time (seconds)
4.5
2.5
7.5
6.5
5.5
3.5
1.5
Algorithm 0
Algorithm 1
Algorithm 3
</figure>
<page confidence="0.976969">
223
</page>
<note confidence="0.8710425">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223–226,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.997507333333333">
and Jonathan May for ISI’s tree automata package
Tiburon. All of these experiments confirmed the
findings in our work.
</bodyText>
<sectionHeader confidence="0.824543" genericHeader="method">
2 Synchronous Binarization for MT
</sectionHeader>
<bodyText confidence="0.99991925">
Machine Translation has made very good progress
in recent times, especially, the so-called “phrase-
based” statistical systems (Och and Ney, 2004). In
order to take a substantial next-step it will be neces-
sary to incorporate several aspects of syntax. Many
researchers have explored syntax-based methods,
for instance, Wu (1996) and Chiang (2005) both uses
binary-branching synchronous context-free gram-
mars (SCFGs). However, to be more expressive
and flexible, it is often easier to start with a gen-
eral SCFG or tree-transducer (Galley et al., 2004).
In this case, binarization of the input grammar is
required for the use of the CKY algorithm (in or-
der to get cubic-time complexity), just as we convert
a CFG into the Chomsky Normal Form (CNF) for
monolingual parsing. For synchronous grammars,
however, different binarization schemes may result
in very different-looking chart items that greatly af-
fect decoding efficiency. For example, consider the
following SCFG rule:
</bodyText>
<equation confidence="0.95889025">
(1) S , NP(1) VP(2) PP(3), NP(1) PP(3) VP(2)
We can binarize it either left-to-right or right-to-left:
S NP VPP-VP
VPP-VP PP VP
</equation>
<bodyText confidence="0.999901941176471">
The intermediate symbols (e.g. VPP-VP) are called vir-
tual nonterminals. We would certainly prefer the
right-to-left binarization because the virtual nonter-
minal has consecutive span (see Fig. 2). The left-to-
right binarization causes discontinuities on the target
side, which results in an exponential time complex-
ity when decoding with an integrated n-gram model.
We develop this intuition into a technique called
synchronous binarization (Zhang et al., 2006)
which binarizes a synchronous production or tree-
tranduction rule on both source and target sides si-
multaneously. It essentially converts an SCFG into
an equivalent ITG (the synchronous extension of
CNF) if possible. We reduce this problem to the
binarization of the permutation of nonterminal sym-
bols between the source and target sides of a syn-
chronous rule and devise a linear-time algorithm
</bodyText>
<figureCaption confidence="0.998941">
Figure 2: The alignment pattern (left) and alignment
matrix (right) of the SCFG rule.
</figureCaption>
<bodyText confidence="0.539087666666667">
system BLEU
monolingual binarization
synchronous binarization
</bodyText>
<tableCaption confidence="0.766123">
Table 1: Synchronous vs. monolingual binarization
in terms of translation quality (BLEU score).
</tableCaption>
<bodyText confidence="0.999466285714286">
for it. Experiments show that the resulting rule set
significantly improves the speed and accuracy over
monolingual binarization (see Table 1) in a state-
of-the-art syntax-based machine translation system
(Galley et al., 2004). We also propose another trick
(hook) for further speeding up the decoding with in-
tegrated n-gram models (Huang et al., 2005).
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="method">
3 Syntax-Directed Translation
</sectionHeader>
<bodyText confidence="0.999877875">
Syntax-directed translation was originally proposed
for compiling programming languages (Irons, 1961;
Lewis and Stearns, 1968), where the source pro-
gram is parsed into a syntax-tree that guides the
generation of the object code. These translations
have been formalized as a synchronous context-free
grammar (SCFG) that generates two languages si-
multaneously (Aho and Ullman, 1972), and equiv-
alently, as a top-down tree-to-string transducer
(G´ecseg and Steinby, 1984). We adapt this syntax-
directed transduction process to statistical MT by
applying stochastic operations at each node of the
source-language parse-tree and searching for the
best derivation (a sequence of translation steps) that
converts the whole tree into some target-language
string (Huang et al., 2006).
</bodyText>
<subsectionHeader confidence="0.985072">
3.1 Extended Domain of Locality
</subsectionHeader>
<bodyText confidence="0.997192">
From a modeling perspective, however, the struc-
tural divergence across languages results in non-
isomorphic parse-trees that are not captured by
</bodyText>
<figure confidence="0.988913222222222">
source (Chinese)
target (English)
NP
NP
VP
PP
VPP-VP
VP
PP
English boundary words
meeting
Sharon
Powell
Powell
with
held
1 2 4 7
PP
NP
Chinese indices
VPP-VP
VP
S — VNP-PP VP
VNP-PP--+ NP PP
or
36.25
38.44
</figure>
<page confidence="0.892098">
224
</page>
<bodyText confidence="0.972877444444445">
◦
SCFGs. For example, the S(VO) structure in En-
glish is translated into a VSO order in Arabic, an
instance of complex re-ordering (Fig. 4).
To alleviate this problem, grammars with richer
expressive power have been proposed which can
grab larger fragments of the tree. Following Galley
et al. (2004), we use an extended tree-to-string trans-
ducer (xRs) with multi-level left-hand-side (LHS)
trees.&apos; Since the right-hand-side (RHS) string can
be viewed as a flat one-level tree with the same non-
terminal root from LHS (Fig. 4), this framework is
closely related to STSGs in having extended domain
of locality on the source-side except for remain-
ing a CFG on the target-side. These rules can be
learned from a parallel corpus using English parse-
trees, Chinese strings, and word alignment (Galley
et al., 2004).
</bodyText>
<subsectionHeader confidence="0.996568">
3.2 A Running Example
</subsectionHeader>
<bodyText confidence="0.976941666666667">
Consider the following English sentence and its Chi-
nese translation (note the reordering in the passive
construction):
</bodyText>
<figure confidence="0.972375586206896">
(c) qiangshou
(a) the gunman was killed by the police .
parser ⇓ S
VP
PUNC
NP-C
DT
.
VP-C
VBD
NN
(b) the gunman was VBN PP
killed IN NP-C
by DT NN
r1, r2 ⇓ VP the police
killed
IN
NP-C
VBD
was
VP-C
PP
VBN
(2) the gunman was killed by the police .
by
DT
NN
qiangshou bei jingfang jibi ◦ r3 ⇓ the police
[gunman] [passive] [police] [killed] . NP-C
</figure>
<bodyText confidence="0.987765428571429">
Figure 3 shows how the translator works. The En-
glish sentence (a) is first parsed into the tree in (b),
which is then recursively converted into the Chinese
string in (e) through five steps. First, at the root
node, we apply the rule r1 which preserves the top-
level word-order and translates the English period
into its Chinese counterpart:
</bodyText>
<equation confidence="0.850491">
(r1) S (x1:NP-C x2:VP PUNC (.) ) → x1 x2 ◦
</equation>
<bodyText confidence="0.9962215">
Then, the rule r2 grabs the whole sub-tree for “the
gunman” and translates it as a phrase:
</bodyText>
<listItem confidence="0.696522">
(r2) NP-C ( DT (the) NN (gunman) ) → qiangshou
</listItem>
<bodyText confidence="0.965031888888889">
Now we get a “partial Chinese, partial English” sen-
tence “qiangshou VP ◦” as shown in Fig. 3 (c). Our
recursion goes on to translate the VP sub-tree. Here
we use the rule r3 for the passive construction:
&apos;we will use LHS and source-side interchangeably (so are
RHS and target-side). In accordance with our experiments, we
also use English and Chinese as the source and target languages,
opposite to the Foreign-to-English convention of Brown et al.
(1993).
</bodyText>
<figure confidence="0.9685635">
(d) qiangshou bei
killed
the police
r5 ⇓ r4 ⇓
</figure>
<figureCaption confidence="0.893641">
(e) qiangshou bei jingfang jibi ◦
Figure 3: A synatx-directed translation process.
</figureCaption>
<equation confidence="0.834167">
, S
VB(2) NP(1) NP(3)
↓ ↓ ↓
</equation>
<figureCaption confidence="0.997025">
Figure 4: An example of complex re-ordering.
</figureCaption>
<figure confidence="0.981731666666667">
S
NP(1) ↓VP
VB(2) NP(3)
↓ ↓
VBN
DT NN ◦
</figure>
<page confidence="0.974025">
225
</page>
<note confidence="0.838142">
VP David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of the 43rd ACL.
</note>
<sectionHeader confidence="0.356653" genericHeader="method">
VBD VP-C
</sectionHeader>
<bodyText confidence="0.8555575">
(r3) was x1:VBN PP __+ bei x2 x1
IN x2:NP-C
by
which captures the fact that the agent (NP-C, “the
police”) and the verb (VBN, “killed”) are always
inverted between English and Chinese in a passive
voice. Finally, we apply rules r4 and r5 which per-
form phrasal translations for the two remaining sub-
trees in (d), respectively, and get the completed Chi-
nese string in (e).
</bodyText>
<subsectionHeader confidence="0.989287">
3.3 Translation Algorithm
</subsectionHeader>
<bodyText confidence="0.999957117647059">
Given a fixed parse-tree T*, the search for the best
derivation (as a sequence of conversion steps) can
be done by a simple top-down traversal (or depth-
first search) from the root of the tree. With memo-
izationm, we get a dynamic programming algorithm
that is guaranteed to run in O(n) time where n is the
length of the input string, since the size of the parse-
tree is proportional to n. Similar algorithms have
also been proposed for dependency-based transla-
tion (Lin, 2004; Ding and Palmer, 2005).
I am currently performing large-scale experi-
ments on English-to-Chinese translation using the
zRs rules. We are not doing the usual direction of
Chinese-to-English partly due to the lack of a suf-
ficiently good Chinese parser. Initial results show
promising translation quality (in terms of BLEU
scores) and fast translation speed.
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999651147540984">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of
Parsing, Translation, and Compiling, volume I: Parsing of
Series in Automatic Computation. Prentice Hall, Englewood
Cliffs, New Jersey.
Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model.
Computational Linguistics, 30(4):479–511, December.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263–311.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine-
grained n-best parsing and discriminative reranking. In Pro-
ceedings of the 43rd ACL.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings ofICML, pages 175–182.
Yuan Ding and Martha Palmer. 2005. Machine translation
using probablisitic synchronous dependency insertion gram-
mars. In Proceedings of the 43rd ACL.
Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005. Com-
piling comp ling: Weighted dynamic programming and the
dyna language. In Proceedings of HLT-EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLT-NAACL.
F. G´ecseg and M. Steinby. 1984. Tree Automata. Akad´emiai
Kiad´o, Budapest.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling
of semantic roles. Computational Linguistics, 28(3):245–
288.
Liang Huang and David Chiang. 2005. Better k-best Pars-
ing. In Proceedings of 9th International Workshop of Pars-
ing Technologies (IWPT).
Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Machine
translation as lexicalized parsing with hooks. In Proceed-
ings of 9th International Workshop of Parsing Technologies
(IWPT).
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Syntax-
directed translation with extended domain of locality. In
submission.
E. T. Irons. 1961. A syntax-directed compiler for ALGOL 60.
Comm. ACM, 4(1):51–55.
Dan Klein and Christopher D. Manning. 2001. Parsing and
Hypergraphs. In Proceedings of the Seventh International
Workshop on Parsing Technologies (IWPT-2001), 17-19 Oc-
tober 2001, Beijing, China.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduc-
tion. Journal of the ACM, 15(3):465–488.
Dekang Lin. 2004. A path-based transfer model for machine
translation. In Proceedings of the 20th COLING.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Online large-margin training of dependency parsers. In Pro-
ceedings of the 43rd ACL.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguis-
tics, 30:417–449.
Franz Och. 2003. Minimum error rate training for statistical
machine translation. In Proc. ofACL.
Dekai Wu. 1996. A polynomial-time algorithm for statistical
machine translation. In Proceedings of the 34th ACL.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proceedings of HLT-NAACL.
</reference>
<page confidence="0.998841">
226
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9997275">Efficient Algorithms for Richer Parsing and Machine Translation</title>
<author confidence="0.982596">Liang</author>
<affiliation confidence="0.999802">Department of Computer and Information University of</affiliation>
<email confidence="0.97945">lhuang3@cis.upenn.edu</email>
<abstract confidence="0.974237634920635">My PhD research has been on the algorithmic and formal aspects of computational linguistics, esp. in the areas of parsing and machine translation. I am interested in developing efficient algorithms for formalisms with rich expressive power, so that we can have a better modeling of human languages without sacrificing efficiency. In doing so, I hope to help integrating more linguistic and structural knowledge with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems. other projects, I have been working on best parsing, synchronous binarization, and syntaxdirected translation. Parsing and Hypergraphs NLP systems are often cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the that the 1-best output from one module is always optimal for the next module. So one might want to postpone some disambiguation by propalists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This also true for trainwhere the list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on set, rather than on the full search space which is usually exponentially large. algorithms for parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I coseveral fast and exact algorithms for best parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and work (2001) by introducing 1 10 100 1000 10000 k Figure 1: Average parsing speed on the Section 23 of Penn Treebank (Algorithms 0, 1, and 3, log-log). which is closely related to the optimal subproblem property in dynamic programming. We first generalize the classical 1-best Viterbi alto hypergraphs, and then present four algorithms, each improving its predessor by delaying more work until necessary. The final one, Algorithm 3, starts with a normal 1-best search for vertex (or as in deductive frameworks), and then works backwards from the target vertex (fiitem) for its 2nd, 3rd, best derivations, calling itself recursively only on demand, being the laziest of the four algorithms. When tested on top of two state-of-the-art systems, the Collins/Bikel parser (Bikel, 2004) and Chiang’s CKY-based Hiero decoder (Chiang, 2005), this algorithm is shown to very little overhead even for quite large (See Fig. 1 for experiments on Bikel parser). These algorithms have been re-implemented by other researchers in the field, including Eugene for his parser, Ryan McDonald for</abstract>
<note confidence="0.7164518">his dependency parser (McDonald et al., 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c.) for a similar model, Jonathan Graehl for the ISI syntax-based MT decoder, David A. Smith for the Dyna language (Eisner et al., 2005),</note>
<abstract confidence="0.911974165217392">Average Parsing Time (seconds) 4.5 2.5 7.5 6.5 5.5 3.5 1.5 Algorithm Algorithm Algorithm 3 223 of the Human Language Technology Conference of the North American Chapter of the pages York, June 2006. Association for Computational Linguistics and Jonathan May for ISI’s tree automata package Tiburon. All of these experiments confirmed the findings in our work. 2 Synchronous Binarization for MT Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs). However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al., 2004). this case, the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF) for monolingual parsing. For synchronous grammars, however, different binarization schemes may result in very different-looking chart items that greatly affect decoding efficiency. For example, consider the following SCFG rule: S We can binarize it either left-to-right or right-to-left: S NP VP intermediate symbols (e.g. are called vir- We would certainly prefer the right-to-left binarization because the virtual nonterminal has consecutive span (see Fig. 2). The left-toright binarization causes discontinuities on the target side, which results in an exponential time complexwhen decoding with an integrated model. We develop this intuition into a technique called binarization et al., 2006) which binarizes a synchronous production or treerule on both source and target sides si- It essentially converts an SCFG into an equivalent ITG (the synchronous extension of CNF) if possible. We reduce this problem to the binarization of the permutation of nonterminal symbols between the source and target sides of a synchronous rule and devise a linear-time algorithm Figure 2: The alignment pattern (left) and alignment matrix (right) of the SCFG rule. system BLEU monolingual binarization synchronous binarization Table 1: Synchronous vs. monolingual binarization in terms of translation quality (BLEU score). for it. Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al., 2004). We also propose another trick (hook) for further speeding up the decoding with inmodels (Huang et al., 2005). 3 Syntax-Directed Translation Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operations at each node of the source-language parse-tree and searching for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string (Huang et al., 2006). 3.1 Extended Domain of Locality From a modeling perspective, however, the structural divergence across languages results in nonisomorphic parse-trees that are not captured by source (Chinese) target (English) NP NP VP PP VP PP English boundary words meeting Sharon Powell Powell with held 1 2 4 7 PP NP Chinese indices VP PP or 36.25 38.44 224 ◦ SCFGs. For example, the S(VO) structure in English is translated into a VSO order in Arabic, an of re-ordering 4). To alleviate this problem, grammars with richer expressive power have been proposed which can grab larger fragments of the tree. Following Galley et al. (2004), we use an extended tree-to-string transwith multi-level left-hand-side (LHS) the right-hand-side (RHS) string can be viewed as a flat one-level tree with the same nonterminal root from LHS (Fig. 4), this framework is closely related to STSGs in having extended domain of locality on the source-side except for remaining a CFG on the target-side. These rules can be learned from a parallel corpus using English parsetrees, Chinese strings, and word alignment (Galley et al., 2004). 3.2 A Running Example Consider the following English sentence and its Chinese translation (note the reordering in the passive construction): (a) the gunman was killed by the police . VP PUNC NP-C DT . VP-C VBD NN gunman was VBN PP killed IN NP-C by DT NN police killed IN NP-C VBD was VP-C PP VBN (2) the gunman was killed by the police . by DT NN bei jingfang jibi police [passive] [police] [killed] . Figure 3 shows how the translator works. The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps. First, at the root we apply the rule preserves the toplevel word-order and translates the English period into its Chinese counterpart: S PUNC (.) ) the rule the whole sub-tree for “the gunman” and translates it as a phrase: NP-C ( DT (the) NN (gunman) ) Now we get a “partial Chinese, partial English” senas shown in Fig. 3 (c). Our recursion goes on to translate the VP sub-tree. Here use the rule the passive construction: will use LHS and source-side interchangeably (so are RHS and target-side). In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993). bei killed the police bei jingfang jibi Figure 3: A synatx-directed translation process. , S ↓ ↓ ↓ Figure 4: An example of complex re-ordering. S ↓ ↓ VBN NN 225 VP David Chiang. 2005. A hierarchical phrase-based model for machine translation. In of the 43rd VBD VP-C PP by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive Finally, we apply rules perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e). 3.3 Translation Algorithm a fixed parse-tree the search for the best derivation (as a sequence of conversion steps) can be done by a simple top-down traversal (or depthfirst search) from the root of the tree. With memoizationm, we get a dynamic programming algorithm is guaranteed to run in where the length of the input string, since the size of the parseis proportional to Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005). I am currently performing large-scale experiments on English-to-Chinese translation using the We are not doing the usual direction of Chinese-to-English partly due to the lack of a sufficiently good Chinese parser. Initial results show promising translation quality (in terms of BLEU scores) and fast translation speed.</abstract>
<note confidence="0.82504511627907">References V. Aho and Jeffrey D. Ullman. 1972. Theory of Translation, and volume I: Parsing of in Automatic Prentice Hall, Englewood Cliffs, New Jersey. Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. 30(4):479–511, December. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical translation: Parameter estimation. 19:263–311. Eugene Charniak and Mark Johnson. 2005. Coarse-to-fineparsing and discriminative reranking. In Proof the 43rd Michael Collins. 2000. Discriminative reranking for natural parsing. In pages 175–182. Yuan Ding and Martha Palmer. 2005. Machine translation using probablisitic synchronous dependency insertion gram- In of the 43rd Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005. Compiling comp ling: Weighted dynamic programming and the language. In of Michel Galley, Mark Hopkins, Kevin Knight, and Daniel 2004. What’s in a translation rule? In G´ecseg and M. Steinby. 1984. Akad´emiai Kiad´o, Budapest. Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling semantic roles. 28(3):245– 288. Huang and David Chiang. 2005. Better Pars- In of 9th International Workshop of Pars- Technologies Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Machine as lexicalized parsing with hooks. In Proceedings of 9th International Workshop of Parsing Technologies Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Syntaxdirected translation with extended domain of locality. In submission. E. T. Irons. 1961. A syntax-directed compiler for ALGOL 60. 4(1):51–55. Dan Klein and Christopher D. Manning. 2001. Parsing and In of the Seventh International Workshop on Parsing Technologies (IWPT-2001), 17-19 Oc-</note>
<address confidence="0.802662">2001, Beijing,</address>
<abstract confidence="0.857289764705882">P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transducof the 15(3):465–488. Dekang Lin. 2004. A path-based transfer model for machine In of the 20th Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. large-margin training of dependency parsers. In Proof the 43rd F. J. Och and H. Ney. 2004. The alignment template approach statistical machine translation. Linguis- 30:417–449. Franz Och. 2003. Minimum error rate training for statistical translation. In Dekai Wu. 1996. A polynomial-time algorithm for statistical translation. In of the 34th Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In of</abstract>
<intro confidence="0.252856">226</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="6870" citStr="Aho and Ullman, 1972" startWordPosition="1051" endWordPosition="1054"> in a stateof-the-art syntax-based machine translation system (Galley et al., 2004). We also propose another trick (hook) for further speeding up the decoding with integrated n-gram models (Huang et al., 2005). 3 Syntax-Directed Translation Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operations at each node of the source-language parse-tree and searching for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string (Huang et al., 2006). 3.1 Extended Domain of Locality From a modeling perspective, however, the structural divergence across languages results in nonisomorphic parse-trees that are not captured by source (Chinese) target (Englis</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume I: Parsing of Series in Automatic Computation. Prentice Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="2890" citStr="Bikel, 2004" startWordPosition="445" endWordPosition="446">erty in dynamic programming. We first generalize the classical 1-best Viterbi algorithm to hypergraphs, and then present four k-best algorithms, each improving its predessor by delaying more work until necessary. The final one, Algorithm 3, starts with a normal 1-best search for each vertex (or item, as in deductive frameworks), and then works backwards from the target vertex (final item) for its 2nd, 3rd, ..., kth best derivations, calling itself recursively only on demand, being the laziest of the four algorithms. When tested on top of two state-of-the-art systems, the Collins/Bikel parser (Bikel, 2004) and Chiang’s CKY-based Hiero decoder (Chiang, 2005), this algorithm is shown to have very little overhead even for quite large k (say, 106) (See Fig. 1 for experiments on Bikel parser). These algorithms have been re-implemented by other researchers in the field, including Eugene Charniak for his n-best parser, Ryan McDonald for his dependency parser (McDonald et al., 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c.) for a similar model, Jonathan Graehl for the ISI syntax-based MT decoder, David A. Smith for the Dyna language (Eisner et al., 2005), Average Parsing </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="9921" citStr="Brown et al. (1993)" startWordPosition="1578" endWordPosition="1581">2:VP PUNC (.) ) → x1 x2 ◦ Then, the rule r2 grabs the whole sub-tree for “the gunman” and translates it as a phrase: (r2) NP-C ( DT (the) NN (gunman) ) → qiangshou Now we get a “partial Chinese, partial English” sentence “qiangshou VP ◦” as shown in Fig. 3 (c). Our recursion goes on to translate the VP sub-tree. Here we use the rule r3 for the passive construction: &apos;we will use LHS and source-side interchangeably (so are RHS and target-side). In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993). (d) qiangshou bei killed the police r5 ⇓ r4 ⇓ (e) qiangshou bei jingfang jibi ◦ Figure 3: A synatx-directed translation process. , S VB(2) NP(1) NP(3) ↓ ↓ ↓ Figure 4: An example of complex re-ordering. S NP(1) ↓VP VB(2) NP(3) ↓ ↓ VBN DT NN ◦ 225 VP David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of the 43rd ACL. VBD VP-C (r3) was x1:VBN PP __+ bei x2 x1 IN x2:NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply r</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-finegrained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="1734" citStr="Charniak and Johnson, 2005" startWordPosition="256" endWordPosition="259">ht want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) by introducing monotonic 1 10 100 1000 10000 k Figure 1: Average parsing speed on the Section 23 of Penn Treebank (Algorithms 0, 1, and 3, log-log). weightfunctions, which is closely related to the optimal subproblem property in dynamic programming. We first generalize the cla</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-finegrained n-best parsing and discriminative reranking. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="1457" citStr="Collins, 2000" startWordPosition="215" endWordPosition="216">rgraphs NLP systems are often cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the case that the 1-best output from one module is not always optimal for the next module. So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) </context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings ofICML, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probablisitic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probablisitic synchronous dependency insertion grammars. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Eric Goldlust</author>
<author>Noah A Smith</author>
</authors>
<title>Compiling comp ling: Weighted dynamic programming and the dyna language.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="3472" citStr="Eisner et al., 2005" startWordPosition="536" endWordPosition="539">the Collins/Bikel parser (Bikel, 2004) and Chiang’s CKY-based Hiero decoder (Chiang, 2005), this algorithm is shown to have very little overhead even for quite large k (say, 106) (See Fig. 1 for experiments on Bikel parser). These algorithms have been re-implemented by other researchers in the field, including Eugene Charniak for his n-best parser, Ryan McDonald for his dependency parser (McDonald et al., 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c.) for a similar model, Jonathan Graehl for the ISI syntax-based MT decoder, David A. Smith for the Dyna language (Eisner et al., 2005), Average Parsing Time (seconds) 4.5 2.5 7.5 6.5 5.5 3.5 1.5 Algorithm 0 Algorithm 1 Algorithm 3 223 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223–226, New York, June 2006. c�2006 Association for Computational Linguistics and Jonathan May for ISI’s tree automata package Tiburon. All of these experiments confirmed the findings in our work. 2 Synchronous Binarization for MT Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take </context>
</contexts>
<marker>Eisner, Goldlust, Smith, 2005</marker>
<rawString>Jason Eisner, Eric Goldlust, and Noah A. Smith. 2005. Compiling comp ling: Weighted dynamic programming and the dyna language. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="4457" citStr="Galley et al., 2004" startWordPosition="688" endWordPosition="691">s confirmed the findings in our work. 2 Synchronous Binarization for MT Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs). However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al., 2004). In this case, binarization of the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF) for monolingual parsing. For synchronous grammars, however, different binarization schemes may result in very different-looking chart items that greatly affect decoding efficiency. For example, consider the following SCFG rule: (1) S , NP(1) VP(2) PP(3), NP(1) PP(3) VP(2) We can binarize it either left-to-right or right-to-left: S NP VPP-VP VPP-VP PP VP The intermediate symbols (e.g. VPP-VP) are calle</context>
<context position="6332" citStr="Galley et al., 2004" startWordPosition="973" endWordPosition="976">he binarization of the permutation of nonterminal symbols between the source and target sides of a synchronous rule and devise a linear-time algorithm Figure 2: The alignment pattern (left) and alignment matrix (right) of the SCFG rule. system BLEU monolingual binarization synchronous binarization Table 1: Synchronous vs. monolingual binarization in terms of translation quality (BLEU score). for it. Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al., 2004). We also propose another trick (hook) for further speeding up the decoding with integrated n-gram models (Huang et al., 2005). 3 Syntax-Directed Translation Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G</context>
<context position="7948" citStr="Galley et al. (2004)" startWordPosition="1225" endWordPosition="1228">ever, the structural divergence across languages results in nonisomorphic parse-trees that are not captured by source (Chinese) target (English) NP NP VP PP VPP-VP VP PP English boundary words meeting Sharon Powell Powell with held 1 2 4 7 PP NP Chinese indices VPP-VP VP S — VNP-PP VP VNP-PP--+ NP PP or 36.25 38.44 224 ◦ SCFGs. For example, the S(VO) structure in English is translated into a VSO order in Arabic, an instance of complex re-ordering (Fig. 4). To alleviate this problem, grammars with richer expressive power have been proposed which can grab larger fragments of the tree. Following Galley et al. (2004), we use an extended tree-to-string transducer (xRs) with multi-level left-hand-side (LHS) trees.&apos; Since the right-hand-side (RHS) string can be viewed as a flat one-level tree with the same nonterminal root from LHS (Fig. 4), this framework is closely related to STSGs in having extended domain of locality on the source-side except for remaining a CFG on the target-side. These rules can be learned from a parallel corpus using English parsetrees, Chinese strings, and word alignment (Galley et al., 2004). 3.2 A Running Example Consider the following English sentence and its Chinese translation (</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F G´ecseg</author>
<author>M Steinby</author>
</authors>
<title>Tree Automata. Akad´emiai Kiad´o,</title>
<date>1984</date>
<location>Budapest.</location>
<marker>G´ecseg, Steinby, 1984</marker>
<rawString>F. G´ecseg and M. Steinby. 1984. Tree Automata. Akad´emiai Kiad´o, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<pages>288</pages>
<contexts>
<context position="1301" citStr="Gildea and Jurafsky, 2002" startWordPosition="187" endWordPosition="190">translation (MT) systems. Among other projects, I have been working on kbest parsing, synchronous binarization, and syntaxdirected translation. 1 k-best Parsing and Hypergraphs NLP systems are often cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the case that the 1-best output from one module is not always optimal for the next module. So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsin</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245– 288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of 9th International Workshop of Parsing Technologies (IWPT).</booktitle>
<contexts>
<context position="1986" citStr="Huang and Chiang, 2005" startWordPosition="298" endWordPosition="301"> where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) by introducing monotonic 1 10 100 1000 10000 k Figure 1: Average parsing speed on the Section 23 of Penn Treebank (Algorithms 0, 1, and 3, log-log). weightfunctions, which is closely related to the optimal subproblem property in dynamic programming. We first generalize the classical 1-best Viterbi algorithm to hypergraphs, and then present four k-best algorithms, each improving its predessor by delaying more work until necessary. The final one, Algorithm 3, starts with a normal 1-best search for each vertex (or item, as in </context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing. In Proceedings of 9th International Workshop of Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Machine translation as lexicalized parsing with hooks.</title>
<date>2005</date>
<booktitle>In Proceedings of 9th International Workshop of Parsing Technologies (IWPT).</booktitle>
<contexts>
<context position="6458" citStr="Huang et al., 2005" startWordPosition="994" endWordPosition="997">a linear-time algorithm Figure 2: The alignment pattern (left) and alignment matrix (right) of the SCFG rule. system BLEU monolingual binarization synchronous binarization Table 1: Synchronous vs. monolingual binarization in terms of translation quality (BLEU score). for it. Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al., 2004). We also propose another trick (hook) for further speeding up the decoding with integrated n-gram models (Huang et al., 2005). 3 Syntax-Directed Translation Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operatio</context>
</contexts>
<marker>Huang, Zhang, Gildea, 2005</marker>
<rawString>Liang Huang, Hao Zhang, and Daniel Gildea. 2005. Machine translation as lexicalized parsing with hooks. In Proceedings of 9th International Workshop of Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Syntaxdirected translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In submission.</booktitle>
<contexts>
<context position="7262" citStr="Huang et al., 2006" startWordPosition="1108" endWordPosition="1111">is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operations at each node of the source-language parse-tree and searching for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string (Huang et al., 2006). 3.1 Extended Domain of Locality From a modeling perspective, however, the structural divergence across languages results in nonisomorphic parse-trees that are not captured by source (Chinese) target (English) NP NP VP PP VPP-VP VP PP English boundary words meeting Sharon Powell Powell with held 1 2 4 7 PP NP Chinese indices VPP-VP VP S — VNP-PP VP VNP-PP--+ NP PP or 36.25 38.44 224 ◦ SCFGs. For example, the S(VO) structure in English is translated into a VSO order in Arabic, an instance of complex re-ordering (Fig. 4). To alleviate this problem, grammars with richer expressive power have bee</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Syntaxdirected translation with extended domain of locality. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Irons</author>
</authors>
<title>A syntax-directed compiler for</title>
<date>1961</date>
<journal>ALGOL 60. Comm. ACM,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="6590" citStr="Irons, 1961" startWordPosition="1010" endWordPosition="1011">zation synchronous binarization Table 1: Synchronous vs. monolingual binarization in terms of translation quality (BLEU score). for it. Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al., 2004). We also propose another trick (hook) for further speeding up the decoding with integrated n-gram models (Huang et al., 2005). 3 Syntax-Directed Translation Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operations at each node of the source-language parse-tree and searching for the best derivation (a sequence of translation steps) that conve</context>
</contexts>
<marker>Irons, 1961</marker>
<rawString>E. T. Irons. 1961. A syntax-directed compiler for ALGOL 60. Comm. ACM, 4(1):51–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT-2001),</booktitle>
<pages>17--19</pages>
<location>Beijing, China.</location>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT-2001), 17-19 October 2001, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis</author>
<author>R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="6616" citStr="Lewis and Stearns, 1968" startWordPosition="1012" endWordPosition="1015">onous binarization Table 1: Synchronous vs. monolingual binarization in terms of translation quality (BLEU score). for it. Experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization (see Table 1) in a stateof-the-art syntax-based machine translation system (Galley et al., 2004). We also propose another trick (hook) for further speeding up the decoding with integrated n-gram models (Huang et al., 2005). 3 Syntax-Directed Translation Syntax-directed translation was originally proposed for compiling programming languages (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a syntax-tree that guides the generation of the object code. These translations have been formalized as a synchronous context-free grammar (SCFG) that generates two languages simultaneously (Aho and Ullman, 1972), and equivalently, as a top-down tree-to-string transducer (G´ecseg and Steinby, 1984). We adapt this syntaxdirected transduction process to statistical MT by applying stochastic operations at each node of the source-language parse-tree and searching for the best derivation (a sequence of translation steps) that converts the whole tree into so</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduction. Journal of the ACM, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING.</booktitle>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of the 20th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="1492" citStr="McDonald et al., 2005" startWordPosition="219" endWordPosition="222">ten cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the case that the 1-best output from one module is not always optimal for the next module. So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) by introducing monotonic 1 10 100 1</context>
<context position="3266" citStr="McDonald et al., 2005" startWordPosition="502" endWordPosition="505">arget vertex (final item) for its 2nd, 3rd, ..., kth best derivations, calling itself recursively only on demand, being the laziest of the four algorithms. When tested on top of two state-of-the-art systems, the Collins/Bikel parser (Bikel, 2004) and Chiang’s CKY-based Hiero decoder (Chiang, 2005), this algorithm is shown to have very little overhead even for quite large k (say, 106) (See Fig. 1 for experiments on Bikel parser). These algorithms have been re-implemented by other researchers in the field, including Eugene Charniak for his n-best parser, Ryan McDonald for his dependency parser (McDonald et al., 2005), Microsoft Research NLP group (Simon Corston-Oliver and Kevin Duh, p.c.) for a similar model, Jonathan Graehl for the ISI syntax-based MT decoder, David A. Smith for the Dyna language (Eisner et al., 2005), Average Parsing Time (seconds) 4.5 2.5 7.5 6.5 5.5 3.5 1.5 Algorithm 0 Algorithm 1 Algorithm 3 223 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223–226, New York, June 2006. c�2006 Association for Computational Linguistics and Jonathan May for ISI’s tree automata package Tiburon. All of these experiments confirmed the findings in o</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="4053" citStr="Och and Ney, 2004" startWordPosition="625" endWordPosition="628">e Dyna language (Eisner et al., 2005), Average Parsing Time (seconds) 4.5 2.5 7.5 6.5 5.5 3.5 1.5 Algorithm 0 Algorithm 1 Algorithm 3 223 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 223–226, New York, June 2006. c�2006 Association for Computational Linguistics and Jonathan May for ISI’s tree automata package Tiburon. All of these experiments confirmed the findings in our work. 2 Synchronous Binarization for MT Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs). However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al., 2004). In this case, binarization of the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF) </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="1468" citStr="Och, 2003" startWordPosition="217" endWordPosition="218">tems are often cascades of several modules, e.g., part-of-speech tagging, then syntactic parsing, and finally semantic interpretation. It is often the case that the 1-best output from one module is not always optimal for the next module. So one might want to postpone some disambiguation by propagating k-best lists (instead of 1-best solutions) to subsequent phases, as in joint parsing and semantic role-labeling (Gildea and Jurafsky, 2002). This is also true for reranking and discriminative training, where the k-best list of candidates serves as an approximation of the full set (Collins, 2000; Och, 2003; McDonald et al., 2005). In this way we can optimize some complicated objective function on the k-best set, rather than on the full search space which is usually exponentially large. Previous algorithms for k-best parsing (Collins, 2000; Charniak and Johnson, 2005) are either suboptimal or slow and rely significantly on pruning techniques to make them tractable. So I codeveloped several fast and exact algorithms for kbest parsing in the general framework of directed monotonic hypergraphs (Huang and Chiang, 2005). This formulation extends and refines Klein and Manning’s work (2001) by introduc</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th ACL.</booktitle>
<contexts>
<context position="4235" citStr="Wu (1996)" startWordPosition="655" endWordPosition="656">ce of the North American Chapter of the ACL, pages 223–226, New York, June 2006. c�2006 Association for Computational Linguistics and Jonathan May for ISI’s tree automata package Tiburon. All of these experiments confirmed the findings in our work. 2 Synchronous Binarization for MT Machine Translation has made very good progress in recent times, especially, the so-called “phrasebased” statistical systems (Och and Ney, 2004). In order to take a substantial next-step it will be necessary to incorporate several aspects of syntax. Many researchers have explored syntax-based methods, for instance, Wu (1996) and Chiang (2005) both uses binary-branching synchronous context-free grammars (SCFGs). However, to be more expressive and flexible, it is often easier to start with a general SCFG or tree-transducer (Galley et al., 2004). In this case, binarization of the input grammar is required for the use of the CKY algorithm (in order to get cubic-time complexity), just as we convert a CFG into the Chomsky Normal Form (CNF) for monolingual parsing. For synchronous grammars, however, different binarization schemes may result in very different-looking chart items that greatly affect decoding efficiency. F</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>Dekai Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proceedings of the 34th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="5468" citStr="Zhang et al., 2006" startWordPosition="844" endWordPosition="847">nsider the following SCFG rule: (1) S , NP(1) VP(2) PP(3), NP(1) PP(3) VP(2) We can binarize it either left-to-right or right-to-left: S NP VPP-VP VPP-VP PP VP The intermediate symbols (e.g. VPP-VP) are called virtual nonterminals. We would certainly prefer the right-to-left binarization because the virtual nonterminal has consecutive span (see Fig. 2). The left-toright binarization causes discontinuities on the target side, which results in an exponential time complexity when decoding with an integrated n-gram model. We develop this intuition into a technique called synchronous binarization (Zhang et al., 2006) which binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously. It essentially converts an SCFG into an equivalent ITG (the synchronous extension of CNF) if possible. We reduce this problem to the binarization of the permutation of nonterminal symbols between the source and target sides of a synchronous rule and devise a linear-time algorithm Figure 2: The alignment pattern (left) and alignment matrix (right) of the SCFG rule. system BLEU monolingual binarization synchronous binarization Table 1: Synchronous vs. monolingual binarization in term</context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of HLT-NAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>