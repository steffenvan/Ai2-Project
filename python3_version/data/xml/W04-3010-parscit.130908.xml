<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030727">
<title confidence="0.9988105">
Speech Recognition Models of the Interdependence Among Syntax, Prosody,
and Segmental Acoustics
</title>
<author confidence="0.997372">
Mark Hasegawa-Johnson, Jennifer Cole, ChilM Shih, Ken Chen, Aaron Cohen,
Sandra Chavarria, Heejin Kim, Taejin Yoon, Sarah Borys, and Jeung-Yoon Choi
</author>
<affiliation confidence="0.999673">
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.966552">
fjhasegaw,jscole,c1s,kenchen,ascohenl@uilic.edu
fchavarri,hkim17,tyoon,sborys,choijyl@uiuc.edu
</email>
<sectionHeader confidence="0.995569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953233333333">
This paper describes results from several dozen
experimental systems, and draws conclusions
about the ability of speech recognition mod-
els to represent the relationship among syntax,
prosody, and segmental acoustics. Prosody-
dependent allophone modeling can reduce the
word error rate (WER) of a speech recognizer,
but only if both the language model and the
acoustic model encode explicit dependence on
prosody. Word error rate is improved mainly
because the observed prosody is linguistically
unlikely to co-occur with any incorrect word
string. Additional improvements, in both per-
plexity and WER, can be obtained using a
semi-factored language model, in which the re-
lationship between prosody and the word se-
quence is at least partly mediated by syntactic
tags. Careful analysis of the relationship be-
tween prosody and syntax indicates that syn-
tactic phrase boundaries are the most important
cue for prosodic phrase boundary recognition,
while part of speech is the most important cue
for locating pitch accents, but that neither of
these cues is entirely sufficient for either clas-
sification task. Experiments to port this sys-
tem from Radio News to the Switchboard cor-
pus are currently under way, but preliminary re-
sults suggest that the prosody of Switchboard is
profoundly different from the prosody of Radio
News.
</bodyText>
<sectionHeader confidence="0.998905" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994013">
In prosody-dependent speech recognition, acoustic phone
models and acoustic prosody models are interdepen-
dent, so one cannot be searched without simultaneously
searching the other. Our first experiments in prosody-
dependent recognition did not explicitly model syntactic
structure, but we have discovered that the relationship be-
tween word sequence and prosodic tag sequence is most
accurately learned by a factored language model with an
explicit representation of syntactic class and, if possible,
syntactic phrase structure. Thus our systems require an
integrated probabilistic model of the relationship among
prosodic, syntactic, lexical, and acoustic features, sum-
marized as
IT,)= arg rrj iax p(W, S, P, 0) (1)
where 0 = [51, • • • , 5T] is a sequence of acoustic ob-
servations, W = [wi, , wm] is a sequence of words,
and each word is tagged with both syntactic informa-
tion, S = [si, , Al] and prosodic information P =
pl, . . ,pm]. As in most large vocabulary speech recog-
nition systems, Eq. 1 is implemented by way of an inter-
mediate sequence of allophone labels, Q = [q,. , qL],
thus
</bodyText>
<equation confidence="0.999097">
P(147, S, P, 0) maxQ p(010P(Q1W,
P(W, P1S)P(S) (2)
</equation>
<bodyText confidence="0.999961342105263">
Enabling technologies for the recognition of prosody
include prosody-dependent allophones and prosody-
sensitive acoustic observations, discussed in Sec. 2. En-
abling technology for the simultaneous recognition of
syntax is a factored prosody-dependent language model,
with factors representing part of speech and (in a rescor-
ing pass) CFG parse structure, discussed in Sections 3
and 5. The system has been trained and tested using the
Radio News Corpus (Ostendorf et al., 1995). The Radio
News Corpus is the largest publicly available corpus la-
beled with the tones and break indices (TOBI) prosodic
labeling standard (Beckman and Elam, 1994). The Radio
News Corpus was designed for speech synthesis studies.
By speech recognition standards, it is an extremely small
corpus (a bit over 3 hours of speech, read by seven pro-
fessional radio announcers). To our knowledge, no other
research group has reported speech recognition word er-
ror rate for this corpus, but two studies have reported au-
tomatic pitch accent recognition results for this corpus.
(Ostendorf and Ross, 1997) achieved 89% accent recog-
nition correctness given known word alignment. (Taylor,
2000) reported 72.7% accent recognition correctness, and
47.7% accent recognition accuracy, based purely on ob-
servation of FO (without lexical sequence information or
MFCC observations). Our experiments are not directly
comparable to either of these previous studies; like Tay-
lor, we do not assume a priori knowledge of word bound-
ary times, but like Ostendorf and Ross, we use word se-
quence information to aid us in the automatic labeling of
prosodic tags.
Current experiments seek to extend our system to the
Switchboard corpus. In order to train on Switchboard,
it is necessary, first, to both manually and automatically
generate TOM labels for a certain amount of Switch-
board data, and second, to develop acoustic and language
models of disfluency. Sec. 6 describes our preliminary at-
tempts to transcribe and model the prosody of disfluency
in Switchboard.
</bodyText>
<sectionHeader confidence="0.966062" genericHeader="method">
2 Prosody-Dependent Allophones
</sectionHeader>
<bodyText confidence="0.999781428571429">
In the notation of Eq. 2, recognition of prosody is en-
abled by the use of prosody-dependent allophone models
and prosody-sensitive acoustic observations. Prosody-
dependent allophones are similar to the clustered tri-
phones used in standard LVCSR, except that clusters may
be defined on the basis of prosodic as well as phonetic
context. Each allophone model is a three-state HMM
with an explicit duration PMF, and with two observation
streams. The first observation stream carries acoustic-
phonetic observations (currently MFCCs and energy).
The second observation stream carries acoustic-prosodic
observations (pitch). Allophone cluster definitions are
created separately for the duration PMFs, acoustic pho-
netic PDFs, and acoustic prosodic PDFs, thus each type
of observation is used to distinguish only those context
variables with which it is most highly correlated.
Allophone clusters may be defined by any of the fol-
lowing five context variables: left phonetic context, right
phonetic context, pitch accent, intonational phrase po-
sition, syntactic category. A complete specification of
these five context variables may be encoded using the
notation shown in Table I. An allophone is considered
to be phrase-final if it is part of the rhyme of the sylla-
ble preceding an intonational phrase boundary, and non-
final otherwise (Wightman et al., 1992). An allophone
is considered to be accented if it is part of the lexically
stressed syllable of a word transcribed as containing a
pitch accent, and unaccented otherwise. Other prosodic
</bodyText>
<tableCaption confidence="0.924626">
Table 1: Context variables that may be used to deter-
mine an allophone cluster. A fully specified allophone
of phoneme PH takes the form L-PH+R_AP S.
</tableCaption>
<table confidence="0.999619166666667">
Variable Meaning Allowed Settings
L Left Phoneme (vwl, gld, nsl, fric, stop)
R Right Phoneme (vwl, gld, nsl, fric, stop)
A Accent (unaccented, accented)
P Phrase (non-final, final)
S Syntax (content, function)
</table>
<bodyText confidence="0.999852276595744">
distinctions that we have tested include the distinction be-
tween phrase-initial and non-initial allophones, and the
distinction between consonants in the onset and coda of
an accented syllable (Borys, 2003a; Chen et al., 2004);
our best-performing systems implement only the context
variables listed in Table 1. Our best-performing systems
currently only distinguish the manner class of phonemes
to the left and right, and not their place, voicing, or vo-
calic features (Borys, 2003b; Chen et al., 2004). As
place, voicing, and vocalic features have often proven
to be useful in other studies of allophonic variation, we
suspect that the uselessness of these features in our stud-
ies may be an artifact of the relatively small speech cor-
pus that we use to train and test our models. Finally, the
&amp;quot;syntactic category&amp;quot; tag may carry any syntactic features
that can modify allophone pronunciation without produc-
ing a pitch accent or phrase boundary; our current system
distinguishes allophones in content words vs. function
words.
Each allophone model is a three-state hidden Markov
model with an explicit duration probability mass function
(PMF). Modifications to HTK necessary in order to im-
plement an explicit duration PMF are described in (Chen
et al., 2004); diff files creating the modified functions
HDRest, HDERest, HDInit, and HDVite are available at
http://www.ifp.uiuc.edu/speech/software/. Parameters of
the duration PMF and observation PDFs may be tied in-
dependently of one another.
Each state in the allophone model observes two
streams of data: an acoustic-phonetic stream, intended
to carry information primarily about the shape of
the vocal tract, and an acoustic-prosodic stream, in-
tended to carry information primarily about the voice
source. The acoustic-prosodic observation stream mod-
els a smoothed, nonlinearly transformed pitch frequency,
based on the pitch frequency fo and probability of voic-
ing (PV) estimated by the formant program in En-
tropic XWAVES. In order to remove pitch doubling and
halving errors, we use a method similar to that pro-
posed in (Kompe, 1997): a 3 mixture Gassian classifier
is trained on the fo data from each utterance, with mix-
ture component means constrained to equal 1/2, 1, and 2
times the utterance mean pitch. Measured fo candidates
classified as apparently equal to 2f0 or fo /2 are elimi-
nated, as are fo measurements with small PVs. Remain-
ing fo measurements are normalized and converted to log
scale using the formula
</bodyText>
<equation confidence="0.950531">
= log (L) ± 1) , (3)
</equation>
<bodyText confidence="0.999217535714286">
where au is the utterance mean pitch. Eq. 3 is intended to
mimic Fujisaki&apos;s log( fo / min Jo) parameterization (Fu-
jisaki and Hirose, 1984; Hirai et al., 1997); in our ex-
periments we found that estimates of the mean pitch are
less sensitive to pitch tracking errors than estimates of the
min fo, thus we find that Eq. 3 is less sensitive to pitch
tracking errors than Fuji saki&apos;s parameterization. Frames
with missing fo are filled by linearly interpolating fo be-
tween available frames, resulting in a smoothed normal-
ized pitch waveform fo (t). The acoustic-prosodic ob-
servation stream models a scalar observation, Y(t) =
9( Lfo (t — 20ms), , fo (t + 20ms)]). The function g(.)
is a multilayer perceptron, trained so that Y(t) is an esti-
mate of the a posteriori probability that frame t is part of
a pitch-accented syllable (Kim et al., in press).
In our best-performing systems, the duration PMF de-
pends on intonational phrase position, and the FO stream
depends on pitch accent. Splitting the duration PMF and
the FO stream seems to be effective, despite the small size
of the database, because each of these PDFs requires a
very small number of trainable parameters. The duration
PMF is stored as a discrete distribution, with 10-15 train-
able parameters per state. A Gaussian model of the scalar
FO stream works as well, in our experiments, as a mixture
Gaussian model, thus the FO stream requires only 2 train-
able parameters per state. The MFCC stream, by com-
parison, requires 237 trainable parameters per state for
a 3-mixture model of a 39-dimensional acoustic feature
vector. Because of the relatively high parameter dimen-
sion of the MFCC PDF, all of our attempts to condition
the MFCC stream on prosodic context have been stymied
by data sparsity problems.
Our approach to prosodic conditioning of the MFCC
stream is similar to that proposed in (Ostendorf et al.,
1997). Either individual HMM states (as in (Ostendorf
et al., 1997)) or entire allophone models are first split
into prosody-dependent allophones (as shown in Table 1),
then clustered using a standard cross-entropy-based hi-
erarchical clustering algorithm. Two baselines are used:
a recognizer composed of clustered prosody-independent
triphone models, and a recognizer composed of mono-
phone models. First, we attempted to cluster individ-
ual HMM states using the HTK hierarchical clustering
routines (HERest, HLStats, and HHEd); embedded re-
estimation using HERest failed repeatedly to converge,
apparently because the training database is just too small.
Second, we wrote our own code to cluster entire al-
lophone models using a cross-entropy metric compara-
ble to that used by HTK (Borys, 2003b; Borys, 2003a).
In order to guarantee convergence, the clustering algo-
rithm was constrained to generate a pre-specified num-
ber of clustered allophone models, regardless of the re-
sulting change in WER. Hierarchical clustering of tri-
phone or allophone models successfully improved the
cross-entropy of the test data, but WER of the clustered-
allophone recognizer was substantially worse than WER
of a 48-monophone baseline recognizer. WER of the
monophone recognizer was 24.8%; WER of the prosody-
independent clustered triphone model was 36.2%; WER
of the prosody-dependent clustered allophone model was
25.2%. Because clustered allophones failed to outper-
form a monophone model, all results reported in Sec. 4 of
this paper will be based on a 48-monophone recognizer,
with prosodic splitting only of the duration PMF and the
FO stream.
Although monophones outperform any triphone or al-
lophone model of this database, there are tendencies in
the clustering result that support prior phonetic literature
in interesting ways. Of the questions selected by the
clustering algorithm, slightly more questions concerned
intonational phrase position than pitch accent (21% vs.
16%) (Borys, 2003b), in agreement with a number of
phonetic studies that suggest important articulatory cor-
relates of intonational phrase boundary (Fougeron and
Keating, 1997; Dilley etal., 1996; Cho, 2001). Although
vowels were sensitive to all possible prosodic distinc-
tions, consonants were sensitive only to the &amp;quot;lengthen-
ing vs. strengthening vs. neutral&amp;quot; three-way distinction
proposed by Fougeron and Keating (Fougeron and Keat-
ing, 1997): phrase-final consonants (&amp;quot;lengthened&amp;quot;) were
insensitive to pitch accent, while consonants at the begin-
ning of a phrase-medial accented syllable were grouped
together with both accented and unaccented phrase-initial
consonants (&amp;quot;strengthened&amp;quot;) (Borys, 2003a).
</bodyText>
<sectionHeader confidence="0.908149" genericHeader="method">
3 Prosody-Dependent Language Models
</sectionHeader>
<bodyText confidence="0.999975956521739">
The relationship between syntax, prosody, and the word
string is modeled by a tagged language model. A
tagged language model is an estimate of the probabil-
ity p(wm, pm, sm history) where u),,, is the mth word
in the sentence, and pm and 5, are its prosodic and
syntactic tags, respectively. The amount of prosodi-
cally labeled data in the English language is not nearly
sufficient to create a reliable maximum likelihood esti-
mate of p(wm, pm, smIhistory), therefore we have exper-
imented with three methods for estimating the language
model probability: a backed-off prosodically-labeled bi-
gram (with no encoding of syntax), and two factored lan-
guage models.
A prosody-dependent bigram is an estimate of
The prosodic label pm car-
ries two types of information: the pitch accent status of
word Wm, and the position of Wm within an intonational
phrase. There are eight possible settings of pm: a word
may be accented or unaccented; the same word may be
phrase-initial, phrase-final, phrase-medial, or it may be
a one-word intonational phrase (both phrase-initial and
phrase-final). A prosodically tagged word may be en-
coded in the form W_AP, where w is the word label, A
takes the values &amp;quot;a&amp;quot; or &amp;quot;u&amp;quot; (accented or unaccented),
and P takes the values &amp;quot;i,m,f,o&amp;quot; (initial, medial, final,
one-word phrase). The sequence pm_i, pm] takes on
lp 2 = 64 possible values, so in theory, a prosody-
dependent bigram model learns 64 times as many param-
eters as a prosody-independent bigram model. In prac-
tice, most possible combinations of wm and pm never oc-
cur, so their probabilities are estimated by backing off to
1-gram and 0-gram (uniform) distributions; in our experi-
ments, the actual parameter count of a prosody-dependent
bigram model is slightly less than three times that of a
prosody-independent bigram.
An empirically superior estimate of the prosody-
dependent bigram probability may be trained by ex-
plicitly modeling the relationship between the prosodic
tag, pc, and the syntactic tag, sk (Chen and Hasegawa-
Johnson, 2003). The syntactic tag sk specifies the part
of speech of word wk, and during second-pass decoding
(given a complete sentence hypothesis), may also spec-
ify the position of word wk relative to syntactic phrase
and clause boundaries. By explicitly modeling syntactic
tags, the prosody-dependent bigram probability may be
written as
</bodyText>
<equation confidence="0.816154">
19(tv.i wi , Pi) = (4)
</equation>
<bodyText confidence="0.989128979591837">
p(w3,pi,s3,silwi,pi)is proportional to the bigram prob-
ability of a syntactically and prosodically tagged vocab-
ulary. This tagged bigram probability may be computed
as
sj,
sz,A)P(s3, si 1w3, wi)P(wi Iwi,Pi) (5)
The approximation in Eq. 5 is valid if we assume that,
first, prosody is independent of the word string given
knowledge of syntax (reasonable because neither side
of the equation has any explicit representation of dia-
log context), and second, that the syntactic tags are inde-
pendent of prosody given knowledge of the word string
(reasonable except for those cases when prosody may be
used to resolve syntactic ambiguity, (Price et al., 1991)).
Under these assumptions, the tagged bigram probability
factors into three terms. The first term, P(Pi si si, Pi ),
may be robustly estimated from a relatively small cor-
pus, because the syntactic tagset and the prosodic tagset
are both much smaller than the vocabulary. The sec-
ond term,P (i&apos; is the probability that a word
sequence ( si v
, Isizl)wiirUZents syntactic tag sequence
(si , si). Computation of this probability is simplified by
appropriate choice of the syntactic tagset. During first-
pass recognition, the syntactic tag sz encodes only the
part of speech of word wi. In most cases, the word se-
quence (wi , wi) uniquely determines the POS sequence
(si , 83); the few common exceptions can be robustly es-
timated from a large text database with manual or au-
tomatic POS tags. During second-pass recognition, in
an N-best rescoring paradigm, it is possible to assume
that the recognizer is computing the prosody-dependent
and syntax-dependent probability of a complete sentence
transcription, W = [wi, , wm]. Given a complete
transcription, it is possible to compute the maximum like-
lihood phrase-level parse of the sentence using a context-
free grammar, and to augment the syntactic tag sz with in-
formation about the position of the word in its surround-
ing phrase and clause. Like POS, this new syntactic in-
formation may be treated, by the prosody-dependent lan-
guage model, as information uniquely determined by the
hypothesized word sequence (wi, w3).
The third term in Eq. 5, P(wilwi,Pi), is a prosody-
dependent semi-bigram probability. We have tested
two variants of Eq. 5: one in which the probability
is estimated directly from the Radio News
corpus, using backed-off ML estimation, and one in
which the probability is estimated using the following ap-
proximation:
</bodyText>
<table confidence="0.794775666666667">
p(tvi wi pi) P(Pi IWi ,W0P(Wi IWi (6)
P(Pd Wi)
• P(p/ Isj )P(Si 19j *ii,Vj)P(Wj1Wi)
</table>
<sectionHeader confidence="0.999627" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.997948083333334">
Table 2 describes performance of six different recogniz-
ers, each based on 48 monophone HMMs, each com-
posed of an MFCC observation stream (3-mixture Gaus-
sian) and a pitch observation stream (Gaussian), with
explicit representation of duration probability density.
Each row was created by training the named recognizer
on about 90% of the TOBI-transcribed data in the Ra-
dio News corpus (six talkers), and testing on the re-
maining 10% (from the same six talkers). During test-
ing, each recognizer output its best estimate of the com-
plete lexical and prosodic transcription of the utterance.
Word error rate was computed by comparing the lexi-
cal transcription to a reference using the program HRe-
sults, without considering the prosodic transcription; ac-
cent and boundary recognition error rates were com-
puted by ignoring the lexical transcription. The sys-
tem in the first row has no explicit representation of
Table 2: Word error rate (WER), accent error rate (AER),
and intonational phrase boundary error rate (BER, in per-
cent) with six different combinations of acoustic model
(AM) and language model (LM). PI=prosody indepen-
dent (baseline), PD=prosody dependent. Accent and
boundary error rates of the system with no prosody de-
pendence are at chance.
</bodyText>
<table confidence="0.999344571428571">
AM LM WER AER BER
PI PI 24.8 44.6 15.6
PD PI 24.0 45.9 15.0
PI PD Bigram 24.3 23.1 14.5
PD PD Bigram 23.4 20.3 14.3
PD PD Semi-factored 21.7 20.3 14.2
PD PD Factored 22.9 19.7 13.4
</table>
<bodyText confidence="0.957329046511628">
prosody. Accent and boundary recognition error rates
of the first system are at chance for this database: 45%
of words in this database are unaccented (55% are ac-
cented), and 16% are phrase-final. In the second sys-
tem, the FO stream is accent-dependent and the duration
PMF is phrase-position dependent; all systems in this ta-
ble use a prosody-independent MFCC stream. The third
system uses a prosody-dependent bigram language model
with no model of the acoustic correlates of prosody. The
fourth system uses a prosody-dependent bigram, plus
explicit models of accent-dependent pitch variation and
phrase-final lengthening. The fifth system uses a semi-
factored language model, meaning that P(w3,Pilwi,P2)
is factored, but( I
pktui,wi, pi) is not (Eq. 5 is used, but not
Eq. 6). The last system uses both Eq. 5 and 6.
The results of Table 2 indicate that word error rate
is only significantly improved if a prosody-dependent
acoustic model and a prosody-dependent language model
are combined. Prosody-dependent language model-
ing, alone, is sufficient for better-than-chance recogni-
tion of accents and boundaries; a prosody-dependent
acoustic model, alone, is insufficient for any type of
gain. Chen and Hasegawa-Johnson (Chen and Hasegawa-
Johnson, 2004) have presented a formal, information-
theoretic hypothesis explaining the necessity of simulta-
neous prosody-dependent language modeling and acous-
tic modeling. The core of the argument is the observa-
tion that word error rate is improved only if the observed
prosody (the prosody that maximizes the acoustic obser-
vation PDF) is linguistically unlikely to co-occur with
any incorrect word string.
The last two rows of Table 2 present results obtained
using the semi-factored and factored bigram language
models. The word perplexities of the bigram, semi-
factored, and factored language models, using the same
test corpus as in Table 2, are 60, 54, and 47, respectively.
The semi-factored model has significantly lower WER
Table 3: Accent error rate (AER) and boundary error rate
(BER) of five machine learning algorithms in the task of
automatic prosodic transcription of the Radio News cor-
pus based on word sequence information. NN=Neural
Network. From (Cohen, 2004).
</bodyText>
<table confidence="0.999679615384615">
Features: Word-based POS
Learning Algorithm AER BER
None (Chance) 42.6 19.1
C4.5 Rules 18.1 12.1
SLIPPER 18.4 11.5
QUEST Univariate 12.1
Features: Full Syntactic Parse
C4.5 Rules 17.3 1L2
SLIPPER 17.7 10.2
QUEST Univariate 17.5 10.6
QUEST Linear 17.4 11.0
NN, All Features 17.1 10.8
NN, Category Features 16.9 10.4
</table>
<bodyText confidence="0.997204333333333">
than the baseline bigram (21.7% vs. 23.4%), but not sig-
nificantly lower boundary error rate (14.2% vs. 14.3%)
or accent error rate (20.3% vs. 20.3%). The factored
model has significantly improved boundary recognition
error (13.4% vs. 14.3%), but not significantly improved
WER (22.9% vs. 23.4%).
</bodyText>
<sectionHeader confidence="0.563174" genericHeader="method">
5 Prediction of Prosody from Syntax
</sectionHeader>
<bodyText confidence="0.999912787037037">
Table 2 demonstrates that prosody is most useful when its
acoustic and word sequence correlates are jointly mod-
eled, and that, for the purpose of modeling prosody, syn-
tactically inspired language models significantly outper-
form a baseline bigram model. In order to better under-
stand the relationship between prosody and syntax, Co-
hen (Cohen, 2004) performed a series of experiments in
which automatic syntactic parsers, tree-based learners,
and neural networks were used to predict the prosodic
tags on each word in the Radio News corpus. Nine ma-
chine learning algorithms were tested, using seven differ-
ent syntactic feature sets, for prediction of two prosodic
tag variables. Table 3 presents results from several repre-
sentative experiments, including the most successful.
All classifiers in Table 3 used a two-stage classification
algorithm: word sequence was first automatically parsed
to produce syntactic tags, and syntactic tags were then
classified in order to determine prosodic tags. Two types
of binary prosodic tags were estimated: accent recog-
nition marked the target word as either accented or un-
accented, while boundary recognition marked the target
word as either intonational-phrase-initial or non-initial.
Two types of syntactic parsers were used. The top half
of the table, marked &amp;quot;Word-Based POS,&amp;quot; describes ac-
cent recognition experiments using part of speech (POS)
information generated by the Roth-Zelenko (RZ) shal-
low parsing algorithm (Roth and Zelenko, 1998), and
prosodic boundary recognition experiments using the RZ
algorithm plus seven syntactic phrase boundary features.
Each prosodic tag is computed based on observation of
the POS of three consecutive words (the target word plus
two prior words). Each POS tag, in turn, is computed
by the RZ algorithm based on lexical features in a five
word window, thus the total system computes prosodic
tags of one word based on lexical features of 3+5-1=7
consecutive words. Recognition of intonational phrase
boundary based only on POS information was found to
be quite poor, therefore boundary recognition results in
this half of the table also use a small set of full-parse in-
formation: seven features labeling the type of the syn-
tactic phrase boundary beginning on the target word, as
determined by the Charniak parser. The columns marked
&amp;quot;Full Syntactic Parse&amp;quot; use both POS and phrasal parse
information generated by Charniak&apos;s parser (Charniak,
1994). Charniak&apos;s parser computes the maximum like-
lihood parse of an entire breath group using a stochastic
context-free grammar, including opening and closing of
every phrase, clause, and fragment, and the part of speech
of every word. Prosodic taggers based on full-parse fea-
tures observed POS in a four-word window, and parse
features in a two-word window (the target word and the
word to its left). Parse features of a word include indi-
cator features marking the types of phrases that open or
close with the given word, as well as two integer fea-
tures counting the number of phrases, of any type, that
the word opens or closes. Two types of indicator fea-
tures were tested: the &amp;quot;all features&amp;quot; condition used sepa-
rate indicator features for every type of phrase or clause
defined by the Charniak parser, while the &amp;quot;category fea-
tures&amp;quot; used indicator features to mark the onset and offset
of heuristically designed categories. All learners except
the neural network had better performance in the &amp;quot;all fea-
tures&amp;quot; condition, thus results for the &amp;quot;category features&amp;quot;
are only shown for the neural network.
Five learners were tested. The neural network was a
sigmoidal feedforward network trained using error back-
propagation. SLIPPER is a boosting algorithm based
on the RIPPER rule learner (Cohen, 1995). C4.5 and
QUEST are tree-based learners (Quinlan, 1993; Loh and
Shih, 1997). Trees learned by the C4.5 algorithm were
generally found to have better test-corpus accuracy if the
tree was first post-processed in order to generate a series
of rules; the resulting rules were also considerably more
human-legible than the raw learned trees. QUEST was
tested using either univariate nodes or &amp;quot;linear&amp;quot; nodes; the
linear-node configuration implements a linear discrimi-
nant combination of all features at each node in the tree.
Rulesets learned by univariate QUEST were generally
more concise and more legible, to a human expert, than
those learned by any other algorithm.
In some cases, different learners discovered quite dif-
ferent patterns of information. The C4.5 learner classifies
pitch accent on the basis of both phrase and POS infor-
mation, if both are available: for example, with certain
exceptions, words are marked as accented if they close
a subordinate clause but not a prepositional phrase. The
QUEST learner, on the other hand, determines pitch ac-
cent using rules that consider only the POS of the current
word and next word. The first QUEST rule places a pitch
accent on every noun, adjective, gerund, or participle, re-
gardless of context. It must be noted, however, that even
though the QUEST learner uses only POS to determine
pitch accent, the QUEST classifier learned using a full
CFG parse outperforms the classifier learned using local
word-based POS tags. Apparently, POS tags generated
by the CFG parser are more useful, for the purpose of
prosody recognition, than POS tags generated by a local
word-sequence-based tagger.
The best predictor of intonational phrase boundary
is the simultaneous closure of more than one syntactic
phrase. if syntactic parse information is unavailable, the
best predictor is the presence of an audible breath; in this
corpus, breath can be pretty reliably labeled based on du-
ration of the pause. After these cues, the next several
levels in both trees are primarily occupied by POS fea-
tures. For example, an intonational phrase boundary is
likely after a noun or interjection, or before an auxiliary,
coordinating conjunction, modal verb, or the word &amp;quot;to.&amp;quot;
</bodyText>
<sectionHeader confidence="0.99192" genericHeader="method">
6 Switchboard
</sectionHeader>
<bodyText confidence="0.999921272727273">
Our current research seeks to extend these results to
spontaneous speech. Using the neural network classi-
fier whose performance is listed in Table 3, we have
tagged syntactically predicted accent and intonational
phrase boundary positions in the Switchboard conversa-
tional telephone speech corpus (Godfrey et al., 1992).
In order to test these results, and in order to learn
about the differences between conversational speech and
read speech, we have started to manually transcribe the
prosody and disfluency segments in the WS97 subset of
Switchboard (Greenberg and Hitchcock, 2001; Chavarria
et al., 2004).
Preliminary results from this corpus indicate that sta-
tistical models trained to represent the prosody of Radio
News speech are unable to predict the prosody of Switch-
board speech. The models listed in Table 3 do not pre-
dict prosodic phrase boundary position at rates better than
chance. Pitch accent is predicted with an accuracy better
than chance, but still insufficient to be of any use for the
clustering of allophone HMMs, thus when clustering ex-
periments are repeated on the Switchboard corpus, error
rates of the prosody-dependent and prosody-independent
</bodyText>
<figure confidence="0.658202">
1,1 . „ .11114ilailla L L Li. LiLliaL liith
</figure>
<figureCaption confidence="0.9891995">
Figure 1: Transcription of prosody and disfluencies in the
phrase &amp;quot;I, I, one of the...&amp;quot;
</figureCaption>
<bodyText confidence="0.995947525423729">
systems are identical.
Manual transcription suggests many reasons why the
syntactic and acoustic correlates of prosody in the
Switchboard corpus may be significantly different from
their correlates in Radio News. First, few Switchboard ut-
terances contain complete, well-formed sentences. Sec-
ond, Radio News speech is characterized by clear FO
markers for all kinds of pitch accent and phrase boundary
tones, while FO contours extracted from the Switchboard
corpus are comparatively monotone. Comparison of L-
(intermediate phrase boundary tones) and L-L% (intona-
tional phrase boundary tones) on the Switchboard cor-
pus discovered that phrase-final lengthening is the only
reliable acoustic correlate of this distinction; FO corre-
lates seem not to reliably mark this distinction in Switch-
board (Chavarria et al., 2004).
Disfluency is the third reason that Switchboard
prosody is unlike Radio News prosody. Although we be-
gan transcribing Switchboard with the goal of only an-
notating prosody, we discovered almost immediately that
it is impossible to annotate prosodic phrase boundaries in
Switchboard without devising some sort of annotation for
disfluencies. We have adopted the annotation system of
Heeman and Allen (Heeman and Allen, 1999), accord-
ing to which the words being corrected are called the
&amp;quot;reparandum&amp;quot; or REP, the correction is called the &amp;quot;alter-
ation&amp;quot; (ALT), and filled pauses or meta-dialog between
REP and ALT are called the &amp;quot;edit&amp;quot; (EDT).
Fig. 1 shows a disfluency with a double reparandum:
&amp;quot;I, I, one of the things I...&amp;quot; The first reparandum is re-
peated, then finally replaced by the alteration. Fig. 1
shows two characteristics of disfluency that have not been
extensively studied. First, both of the reparanda end in
glottalization, clearly visible in the form of extremely
low-frequency or low-amplitude glottal excitation. Sec-
ond, the prosody of the reparandum is &amp;quot;mimicked&amp;quot; in the
alteration, despite dramatically different lexical content.
In this corpus, words in the reparandum or alteration of
a disfluency are as likely to bear a pitch accent as any
other words in the sentence: 40%, compared to 39.9%
of all words. About two thirds of the accented words in
the reparandum are replaced by accented words in the al-
teration (10/16); about two thirds of unaccented words
are replaced by unaccented words (15/21). Repetition
of prosody is perceptually salient: some listeners report
that the alteration &amp;quot;mimics&amp;quot; the intonational contour of
the reparandum.
Disfluency is common in Switchboard. Of 1100 words
we have transcribed, 40 are part of a reparandum, 37 are
filled pauses, and 41 are part of an alteration, thus 10% of
the words we have transcribed are part of a disfluency.
This estimate is higher than most published estimates,
perhaps because we include all words that are part of the
reparandum or alteration, but most published studies esti-
mate that at least 5% of the words in Switchboard are part
of a disfluency (e.g., (Shriberg, 2001)). Any complete de-
scription of the prosody of Switchboard will necessarily
include, as one component, a theory about the prosody of
disfluency.
</bodyText>
<sectionHeader confidence="0.998542" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999913">
This paper has reviewed results from a number of ex-
perimental systems that simultaneously recognize the
prosodic and lexical transcriptions of an utterance. It
has been demonstrated, first, that prosody-dependent al-
lophone modeling can reduce the word error rate of
a speech recognizer, but that reliable WER reductions
depend on the simultaneous use of both a prosody-
dependent acoustic model and a prosody-dependent lan-
guage model. Additional improvements, in both perplex-
ity and WER, can be obtained using a semi-factored lan-
guage model, in which the relationship between prosody
and the word sequence is at least partly mediated by syn-
tactic tags. Careful analysis of the relationship between
prosody and syntax indicates that syntactic phrase bound-
aries are the most important cue for prosodic phrase
boundary recognition, while part of speech is the most
important cue for locating pitch accents, but that neither
of these cues is entirely sufficient for either classification
task. Even if a pitch accent recognizer completely ignores
phrase information (as does the QUEST learner), its error
rate can be reduced by deriving POS information from a
complete CFG parse of the sentence, rather than from a
local lexical-feature-based classifier.
The prosody of conversational telephone speech is sig-
nificantly different, in important ways, from the prosody
of Radio News speech. Preliminary results suggest that
important differences include the use of incomplete sen-
tences, relatively greater use of duration to cue prosody
(and correspondingly less use of pitch), and, perhaps
most importantly, the frequent occurrence of disfluency.
</bodyText>
<sectionHeader confidence="0.973131" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99563512037037">
M. E. Beckman and G. A. Elam. 1994. Guide-
lines for ToBI labelling. Technical report,
Ohio State University. http://www.ling.ohio-
state.eduiresearch/phonetics/E_ToBI/singer_tobi.html.
Sarah Borys. 2003a. The importance of prosodic fac-
tors in phoneme modeling with applications to speech
recognition. In HLT/NAACL student session, Edmon-
ton.
Sarah Borys. 2003b. Recognition of prosodic fac-
tors and detection of landmarks for automatic speech
recognition. Bachelor&apos;s thesis, University of Illinois at
Urbana-Champaign.
Eugene Charniak. 1994. Statistical Language Learning.
MIT Press, Cambridge, MA.
Sandra Chavarria, Taejin Yoon, Jennifer Cole, and Mark
Hasegawa-Johnson. 2004. Acoustic differentiation of
ip and TP boundary levels: Comparison of and
L% in the switchboard corpus. In ISCA Internat Conf
Speech Prosody, Nara, Japan.
Ken Chen and Mark Hasegawa-Johnson. 2003. Im-
proving the robustness of prosody dependent language
modeling based on prosody syntax cross-correlation.
In IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
Ken Chen and Mark Hasegawa-Johnson. 2004. How
prosody improves word recognition. In ISCA Internat.
Coq: Speech Prosody, Nara, Japan.
Ken Chen, Mark Hasegawa-Johnson, Aaron Cohen,
Sarah Borys, Sung-Suk Kim, Jennifer Cole, and Jeung-
Yoon Choi. 2004. Prosody dependent speech recogni-
tion on radio news. (in review).
T. Cho. 2001. Effects of Prosody on Articulation in En-
glish. Ph.D. thesis, UCLA.
William W. Cohen. 1995. Fast effective rule induction.
In Proc. International Conference on Machine Learn-
ing.
Aaron Cohen. 2004. A survey of machine learning meth-
ods for predicting prosody in radio speech. Master&apos;s
thesis, University of Illinois at Urbana-Champaign.
Laura Dilley, Stefanie Shattuck-Hufnagel, and Mari Os-
tendorf. 1996. Glottalization of word-initial vowels
as a function of prosodic structure. J. of Phonetics,
24:423-444.
Cecile Fougeron and Patricia A. Keating. 1997. Articu-
latory strengthening at edges of prosodic domains. J.
Acoust. Soc. Am, 101(6):3728-3740.
H. Fujisaki and K. Hirose. 1984. Analysis of voice fun-
damental frequency contours for declarative sentence
of Japanese. J. Acoust Soc. Japan, 5(4):233-242.
J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: telephone speech corpus for re-
search and development. In Proc. ICASSP, pages 517-
520.
Steven Greenberg and Leah Hitchcock. 2001. Stress-
accent and vowel quality in the Switchboard corpus.
In NIST Large Vocabulary Continuous Speech Recog-
nition Workshop, Linthicum Heights, MD, May.
Peter A. Heeman and James F. Allen. 1999. Speech
repairs, intonational phrases and discourse markers:
Modeling speakers&apos; utterances in spoken dialogue.
Computational Linguistics, 25(4).
Toshio Hirai, Naoto Iwahashi, Norio Higuchi, and Yoshi-
nori Sagisaka. 1997. Automatic extraction of ft con-
trol rules using statistical analysis. In Jan P. H. van
Santen, Richard W. Sproat, Joseph P. Olive, and Ju-
lia Hirschberg, editors, Progress in Speech Synthesis,
pages 333-346. Springer-Verlag, New York.
Sung-Suk Kim, Mark Hasegawa-Johnson, and Ken Chen.
(in press). Automatic recognition of pitch movements
using multi-layer perceptron and time-delay recursive
neural network. IEEE Signal Processing Letters.
R. Kompe. 1997. Prosody in Speech Understanding Sys-
tems. Springer-Verlag.
W.-Y. Loh and Y.-S. Shih. 1997. Split selection methods
for classification trees. Statistica Sinica, 7:815-840.
M. Ostendorf and K. Ross. 1997. A multi-level model
for recognition of intonation labels. In Computing
prosody: computational models for processing spon-
taneous speech. Springer-Verlag New York, Inc.
M. Ostendorf, P.J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University Radio News Corpus.
Linguistic Data Consortium.
M. Ostendorf, B. Byrne, M. Fink, A. Gunawardana,
K. Ross, S. Roweis, E. Shriberg, D. Talkin, A. Waibel,
B. Wheatley, and T. Zeppenfield. 1997. Modeling
systematic variations in pronunciation via a language-
dependent hidden speaking mode: Final report. Tech-
nical Report WS96, Johns Hopkins University Center
for Language and Speech Processing.
P.J. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic dis-
ambiguation. J. Acoust. Soc. Am, 90(6):2956-2970,
Dec.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufman, Boston.
Dan Roth and Dmitry Zelenko. 1998. Part of speech tag-
ging using a network of linear separators. In COLING-
ACL.
Elizabeth Shriberg. 2001. To &apos;erre is human: ecology
and acoustics of speech disfluencies. Journal of the
International Phonetic Association, 31(1):153-164.
Paul Taylor. 2000. Analysis and synthesis of intonation
using the Tilt model. J. Acoust Soc. Am, 107(3):1697-
1714.
Colin Wightman, Stefanie Shattuck-Hufnagel, and
Mari Ostendorf andPatti Price. 1992. Segmental dura-
tions in the vicinity of prosodic phrase boundaries. J.
Acoust. Soc. Am, 91(3):1707-1717, March.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.426545">
<title confidence="0.9886525">Speech Recognition Models of the Interdependence Among Syntax, Prosody, and Segmental Acoustics</title>
<author confidence="0.970163">Mark Hasegawa-Johnson</author>
<author confidence="0.970163">Jennifer Cole</author>
<author confidence="0.970163">ChilM Shih</author>
<author confidence="0.970163">Ken Chen</author>
<author confidence="0.970163">Aaron Sandra Chavarria</author>
<author confidence="0.970163">Heejin Kim</author>
<author confidence="0.970163">Taejin Yoon</author>
<author confidence="0.970163">Sarah Borys</author>
<author confidence="0.970163">Jeung-Yoon</author>
<affiliation confidence="0.997287">University of Illinois at</affiliation>
<email confidence="0.994773">fchavarri,hkim17,tyoon,sborys,choijyl@uiuc.edu</email>
<abstract confidence="0.9991866">This paper describes results from several dozen experimental systems, and draws conclusions about the ability of speech recognition models to represent the relationship among syntax, prosody, and segmental acoustics. Prosodydependent allophone modeling can reduce the word error rate (WER) of a speech recognizer, but only if both the language model and the acoustic model encode explicit dependence on prosody. Word error rate is improved mainly because the observed prosody is linguistically unlikely to co-occur with any incorrect word string. Additional improvements, in both perplexity and WER, can be obtained using a semi-factored language model, in which the relationship between prosody and the word sequence is at least partly mediated by syntactic tags. Careful analysis of the relationship between prosody and syntax indicates that syntactic phrase boundaries are the most important cue for prosodic phrase boundary recognition, while part of speech is the most important cue for locating pitch accents, but that neither of these cues is entirely sufficient for either classification task. Experiments to port this system from Radio News to the Switchboard corpus are currently under way, but preliminary results suggest that the prosody of Switchboard is profoundly different from the prosody of Radio</abstract>
<intro confidence="0.470339">News.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M E Beckman</author>
<author>G A Elam</author>
</authors>
<title>Guidelines for ToBI labelling.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>Ohio State University.</institution>
<note>http://www.ling.ohiostate.eduiresearch/phonetics/E_ToBI/singer_tobi.html.</note>
<contexts>
<context position="3509" citStr="Beckman and Elam, 1994" startWordPosition="535" endWordPosition="538">gies for the recognition of prosody include prosody-dependent allophones and prosodysensitive acoustic observations, discussed in Sec. 2. Enabling technology for the simultaneous recognition of syntax is a factored prosody-dependent language model, with factors representing part of speech and (in a rescoring pass) CFG parse structure, discussed in Sections 3 and 5. The system has been trained and tested using the Radio News Corpus (Ostendorf et al., 1995). The Radio News Corpus is the largest publicly available corpus labeled with the tones and break indices (TOBI) prosodic labeling standard (Beckman and Elam, 1994). The Radio News Corpus was designed for speech synthesis studies. By speech recognition standards, it is an extremely small corpus (a bit over 3 hours of speech, read by seven professional radio announcers). To our knowledge, no other research group has reported speech recognition word error rate for this corpus, but two studies have reported automatic pitch accent recognition results for this corpus. (Ostendorf and Ross, 1997) achieved 89% accent recognition correctness given known word alignment. (Taylor, 2000) reported 72.7% accent recognition correctness, and 47.7% accent recognition accu</context>
</contexts>
<marker>Beckman, Elam, 1994</marker>
<rawString>M. E. Beckman and G. A. Elam. 1994. Guidelines for ToBI labelling. Technical report, Ohio State University. http://www.ling.ohiostate.eduiresearch/phonetics/E_ToBI/singer_tobi.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Borys</author>
</authors>
<title>The importance of prosodic factors in phoneme modeling with applications to speech recognition.</title>
<date>2003</date>
<booktitle>In HLT/NAACL student session,</booktitle>
<location>Edmonton.</location>
<contexts>
<context position="7012" citStr="Borys, 2003" startWordPosition="1081" endWordPosition="1082">cent, and unaccented otherwise. Other prosodic Table 1: Context variables that may be used to determine an allophone cluster. A fully specified allophone of phoneme PH takes the form L-PH+R_AP S. Variable Meaning Allowed Settings L Left Phoneme (vwl, gld, nsl, fric, stop) R Right Phoneme (vwl, gld, nsl, fric, stop) A Accent (unaccented, accented) P Phrase (non-final, final) S Syntax (content, function) distinctions that we have tested include the distinction between phrase-initial and non-initial allophones, and the distinction between consonants in the onset and coda of an accented syllable (Borys, 2003a; Chen et al., 2004); our best-performing systems implement only the context variables listed in Table 1. Our best-performing systems currently only distinguish the manner class of phonemes to the left and right, and not their place, voicing, or vocalic features (Borys, 2003b; Chen et al., 2004). As place, voicing, and vocalic features have often proven to be useful in other studies of allophonic variation, we suspect that the uselessness of these features in our studies may be an artifact of the relatively small speech corpus that we use to train and test our models. Finally, the &amp;quot;syntactic </context>
<context position="11963" citStr="Borys, 2003" startWordPosition="1888" endWordPosition="1889">red using a standard cross-entropy-based hierarchical clustering algorithm. Two baselines are used: a recognizer composed of clustered prosody-independent triphone models, and a recognizer composed of monophone models. First, we attempted to cluster individual HMM states using the HTK hierarchical clustering routines (HERest, HLStats, and HHEd); embedded reestimation using HERest failed repeatedly to converge, apparently because the training database is just too small. Second, we wrote our own code to cluster entire allophone models using a cross-entropy metric comparable to that used by HTK (Borys, 2003b; Borys, 2003a). In order to guarantee convergence, the clustering algorithm was constrained to generate a pre-specified number of clustered allophone models, regardless of the resulting change in WER. Hierarchical clustering of triphone or allophone models successfully improved the cross-entropy of the test data, but WER of the clusteredallophone recognizer was substantially worse than WER of a 48-monophone baseline recognizer. WER of the monophone recognizer was 24.8%; WER of the prosodyindependent clustered triphone model was 36.2%; WER of the prosody-dependent clustered allophone model wa</context>
<context position="13830" citStr="Borys, 2003" startWordPosition="2159" endWordPosition="2160">t articulatory correlates of intonational phrase boundary (Fougeron and Keating, 1997; Dilley etal., 1996; Cho, 2001). Although vowels were sensitive to all possible prosodic distinctions, consonants were sensitive only to the &amp;quot;lengthening vs. strengthening vs. neutral&amp;quot; three-way distinction proposed by Fougeron and Keating (Fougeron and Keating, 1997): phrase-final consonants (&amp;quot;lengthened&amp;quot;) were insensitive to pitch accent, while consonants at the beginning of a phrase-medial accented syllable were grouped together with both accented and unaccented phrase-initial consonants (&amp;quot;strengthened&amp;quot;) (Borys, 2003a). 3 Prosody-Dependent Language Models The relationship between syntax, prosody, and the word string is modeled by a tagged language model. A tagged language model is an estimate of the probability p(wm, pm, sm history) where u),,, is the mth word in the sentence, and pm and 5, are its prosodic and syntactic tags, respectively. The amount of prosodically labeled data in the English language is not nearly sufficient to create a reliable maximum likelihood estimate of p(wm, pm, smIhistory), therefore we have experimented with three methods for estimating the language model probability: a backed</context>
</contexts>
<marker>Borys, 2003</marker>
<rawString>Sarah Borys. 2003a. The importance of prosodic factors in phoneme modeling with applications to speech recognition. In HLT/NAACL student session, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Borys</author>
</authors>
<title>Recognition of prosodic factors and detection of landmarks for automatic speech recognition. Bachelor&apos;s thesis,</title>
<date>2003</date>
<institution>University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="7012" citStr="Borys, 2003" startWordPosition="1081" endWordPosition="1082">cent, and unaccented otherwise. Other prosodic Table 1: Context variables that may be used to determine an allophone cluster. A fully specified allophone of phoneme PH takes the form L-PH+R_AP S. Variable Meaning Allowed Settings L Left Phoneme (vwl, gld, nsl, fric, stop) R Right Phoneme (vwl, gld, nsl, fric, stop) A Accent (unaccented, accented) P Phrase (non-final, final) S Syntax (content, function) distinctions that we have tested include the distinction between phrase-initial and non-initial allophones, and the distinction between consonants in the onset and coda of an accented syllable (Borys, 2003a; Chen et al., 2004); our best-performing systems implement only the context variables listed in Table 1. Our best-performing systems currently only distinguish the manner class of phonemes to the left and right, and not their place, voicing, or vocalic features (Borys, 2003b; Chen et al., 2004). As place, voicing, and vocalic features have often proven to be useful in other studies of allophonic variation, we suspect that the uselessness of these features in our studies may be an artifact of the relatively small speech corpus that we use to train and test our models. Finally, the &amp;quot;syntactic </context>
<context position="11963" citStr="Borys, 2003" startWordPosition="1888" endWordPosition="1889">red using a standard cross-entropy-based hierarchical clustering algorithm. Two baselines are used: a recognizer composed of clustered prosody-independent triphone models, and a recognizer composed of monophone models. First, we attempted to cluster individual HMM states using the HTK hierarchical clustering routines (HERest, HLStats, and HHEd); embedded reestimation using HERest failed repeatedly to converge, apparently because the training database is just too small. Second, we wrote our own code to cluster entire allophone models using a cross-entropy metric comparable to that used by HTK (Borys, 2003b; Borys, 2003a). In order to guarantee convergence, the clustering algorithm was constrained to generate a pre-specified number of clustered allophone models, regardless of the resulting change in WER. Hierarchical clustering of triphone or allophone models successfully improved the cross-entropy of the test data, but WER of the clusteredallophone recognizer was substantially worse than WER of a 48-monophone baseline recognizer. WER of the monophone recognizer was 24.8%; WER of the prosodyindependent clustered triphone model was 36.2%; WER of the prosody-dependent clustered allophone model wa</context>
<context position="13830" citStr="Borys, 2003" startWordPosition="2159" endWordPosition="2160">t articulatory correlates of intonational phrase boundary (Fougeron and Keating, 1997; Dilley etal., 1996; Cho, 2001). Although vowels were sensitive to all possible prosodic distinctions, consonants were sensitive only to the &amp;quot;lengthening vs. strengthening vs. neutral&amp;quot; three-way distinction proposed by Fougeron and Keating (Fougeron and Keating, 1997): phrase-final consonants (&amp;quot;lengthened&amp;quot;) were insensitive to pitch accent, while consonants at the beginning of a phrase-medial accented syllable were grouped together with both accented and unaccented phrase-initial consonants (&amp;quot;strengthened&amp;quot;) (Borys, 2003a). 3 Prosody-Dependent Language Models The relationship between syntax, prosody, and the word string is modeled by a tagged language model. A tagged language model is an estimate of the probability p(wm, pm, sm history) where u),,, is the mth word in the sentence, and pm and 5, are its prosodic and syntactic tags, respectively. The amount of prosodically labeled data in the English language is not nearly sufficient to create a reliable maximum likelihood estimate of p(wm, pm, smIhistory), therefore we have experimented with three methods for estimating the language model probability: a backed</context>
</contexts>
<marker>Borys, 2003</marker>
<rawString>Sarah Borys. 2003b. Recognition of prosodic factors and detection of landmarks for automatic speech recognition. Bachelor&apos;s thesis, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1994</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="25475" citStr="Charniak, 1994" startWordPosition="4023" endWordPosition="4024"> a five word window, thus the total system computes prosodic tags of one word based on lexical features of 3+5-1=7 consecutive words. Recognition of intonational phrase boundary based only on POS information was found to be quite poor, therefore boundary recognition results in this half of the table also use a small set of full-parse information: seven features labeling the type of the syntactic phrase boundary beginning on the target word, as determined by the Charniak parser. The columns marked &amp;quot;Full Syntactic Parse&amp;quot; use both POS and phrasal parse information generated by Charniak&apos;s parser (Charniak, 1994). Charniak&apos;s parser computes the maximum likelihood parse of an entire breath group using a stochastic context-free grammar, including opening and closing of every phrase, clause, and fragment, and the part of speech of every word. Prosodic taggers based on full-parse features observed POS in a four-word window, and parse features in a two-word window (the target word and the word to its left). Parse features of a word include indicator features marking the types of phrases that open or close with the given word, as well as two integer features counting the number of phrases, of any type, that</context>
</contexts>
<marker>Charniak, 1994</marker>
<rawString>Eugene Charniak. 1994. Statistical Language Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Chavarria</author>
<author>Taejin Yoon</author>
<author>Jennifer Cole</author>
<author>Mark Hasegawa-Johnson</author>
</authors>
<title>Acoustic differentiation of ip and TP boundary levels: Comparison of and L% in the switchboard corpus.</title>
<date>2004</date>
<booktitle>In ISCA Internat Conf Speech Prosody,</booktitle>
<location>Nara, Japan.</location>
<contexts>
<context position="29598" citStr="Chavarria et al., 2004" startWordPosition="4682" endWordPosition="4685"> 6 Switchboard Our current research seeks to extend these results to spontaneous speech. Using the neural network classifier whose performance is listed in Table 3, we have tagged syntactically predicted accent and intonational phrase boundary positions in the Switchboard conversational telephone speech corpus (Godfrey et al., 1992). In order to test these results, and in order to learn about the differences between conversational speech and read speech, we have started to manually transcribe the prosody and disfluency segments in the WS97 subset of Switchboard (Greenberg and Hitchcock, 2001; Chavarria et al., 2004). Preliminary results from this corpus indicate that statistical models trained to represent the prosody of Radio News speech are unable to predict the prosody of Switchboard speech. The models listed in Table 3 do not predict prosodic phrase boundary position at rates better than chance. Pitch accent is predicted with an accuracy better than chance, but still insufficient to be of any use for the clustering of allophone HMMs, thus when clustering experiments are repeated on the Switchboard corpus, error rates of the prosody-dependent and prosody-independent 1,1 . „ .11114ilailla L L Li. LiLli</context>
<context position="31109" citStr="Chavarria et al., 2004" startWordPosition="4916" endWordPosition="4919">elates in Radio News. First, few Switchboard utterances contain complete, well-formed sentences. Second, Radio News speech is characterized by clear FO markers for all kinds of pitch accent and phrase boundary tones, while FO contours extracted from the Switchboard corpus are comparatively monotone. Comparison of L(intermediate phrase boundary tones) and L-L% (intonational phrase boundary tones) on the Switchboard corpus discovered that phrase-final lengthening is the only reliable acoustic correlate of this distinction; FO correlates seem not to reliably mark this distinction in Switchboard (Chavarria et al., 2004). Disfluency is the third reason that Switchboard prosody is unlike Radio News prosody. Although we began transcribing Switchboard with the goal of only annotating prosody, we discovered almost immediately that it is impossible to annotate prosodic phrase boundaries in Switchboard without devising some sort of annotation for disfluencies. We have adopted the annotation system of Heeman and Allen (Heeman and Allen, 1999), according to which the words being corrected are called the &amp;quot;reparandum&amp;quot; or REP, the correction is called the &amp;quot;alteration&amp;quot; (ALT), and filled pauses or meta-dialog between REP </context>
</contexts>
<marker>Chavarria, Yoon, Cole, Hasegawa-Johnson, 2004</marker>
<rawString>Sandra Chavarria, Taejin Yoon, Jennifer Cole, and Mark Hasegawa-Johnson. 2004. Acoustic differentiation of ip and TP boundary levels: Comparison of and L% in the switchboard corpus. In ISCA Internat Conf Speech Prosody, Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chen</author>
<author>Mark Hasegawa-Johnson</author>
</authors>
<title>Improving the robustness of prosody dependent language modeling based on prosody syntax cross-correlation.</title>
<date>2003</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<marker>Chen, Hasegawa-Johnson, 2003</marker>
<rawString>Ken Chen and Mark Hasegawa-Johnson. 2003. Improving the robustness of prosody dependent language modeling based on prosody syntax cross-correlation. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chen</author>
<author>Mark Hasegawa-Johnson</author>
</authors>
<title>How prosody improves word recognition.</title>
<date>2004</date>
<booktitle>In ISCA Internat. Coq: Speech Prosody,</booktitle>
<location>Nara, Japan.</location>
<marker>Chen, Hasegawa-Johnson, 2004</marker>
<rawString>Ken Chen and Mark Hasegawa-Johnson. 2004. How prosody improves word recognition. In ISCA Internat. Coq: Speech Prosody, Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chen</author>
<author>Mark Hasegawa-Johnson</author>
<author>Aaron Cohen</author>
<author>Sarah Borys</author>
<author>Sung-Suk Kim</author>
<author>Jennifer Cole</author>
<author>JeungYoon Choi</author>
</authors>
<title>Prosody dependent speech recognition on radio news. (in review).</title>
<date>2004</date>
<contexts>
<context position="7033" citStr="Chen et al., 2004" startWordPosition="1083" endWordPosition="1086">cented otherwise. Other prosodic Table 1: Context variables that may be used to determine an allophone cluster. A fully specified allophone of phoneme PH takes the form L-PH+R_AP S. Variable Meaning Allowed Settings L Left Phoneme (vwl, gld, nsl, fric, stop) R Right Phoneme (vwl, gld, nsl, fric, stop) A Accent (unaccented, accented) P Phrase (non-final, final) S Syntax (content, function) distinctions that we have tested include the distinction between phrase-initial and non-initial allophones, and the distinction between consonants in the onset and coda of an accented syllable (Borys, 2003a; Chen et al., 2004); our best-performing systems implement only the context variables listed in Table 1. Our best-performing systems currently only distinguish the manner class of phonemes to the left and right, and not their place, voicing, or vocalic features (Borys, 2003b; Chen et al., 2004). As place, voicing, and vocalic features have often proven to be useful in other studies of allophonic variation, we suspect that the uselessness of these features in our studies may be an artifact of the relatively small speech corpus that we use to train and test our models. Finally, the &amp;quot;syntactic category&amp;quot; tag may car</context>
</contexts>
<marker>Chen, Hasegawa-Johnson, Cohen, Borys, Kim, Cole, Choi, 2004</marker>
<rawString>Ken Chen, Mark Hasegawa-Johnson, Aaron Cohen, Sarah Borys, Sung-Suk Kim, Jennifer Cole, and JeungYoon Choi. 2004. Prosody dependent speech recognition on radio news. (in review).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cho</author>
</authors>
<date>2001</date>
<booktitle>Effects of Prosody on Articulation in English. Ph.D. thesis, UCLA.</booktitle>
<contexts>
<context position="13336" citStr="Cho, 2001" startWordPosition="2093" endWordPosition="2094">er, with prosodic splitting only of the duration PMF and the FO stream. Although monophones outperform any triphone or allophone model of this database, there are tendencies in the clustering result that support prior phonetic literature in interesting ways. Of the questions selected by the clustering algorithm, slightly more questions concerned intonational phrase position than pitch accent (21% vs. 16%) (Borys, 2003b), in agreement with a number of phonetic studies that suggest important articulatory correlates of intonational phrase boundary (Fougeron and Keating, 1997; Dilley etal., 1996; Cho, 2001). Although vowels were sensitive to all possible prosodic distinctions, consonants were sensitive only to the &amp;quot;lengthening vs. strengthening vs. neutral&amp;quot; three-way distinction proposed by Fougeron and Keating (Fougeron and Keating, 1997): phrase-final consonants (&amp;quot;lengthened&amp;quot;) were insensitive to pitch accent, while consonants at the beginning of a phrase-medial accented syllable were grouped together with both accented and unaccented phrase-initial consonants (&amp;quot;strengthened&amp;quot;) (Borys, 2003a). 3 Prosody-Dependent Language Models The relationship between syntax, prosody, and the word string is m</context>
</contexts>
<marker>Cho, 2001</marker>
<rawString>T. Cho. 2001. Effects of Prosody on Articulation in English. Ph.D. thesis, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proc. International Conference on Machine Learning.</booktitle>
<contexts>
<context position="26769" citStr="Cohen, 1995" startWordPosition="4236" endWordPosition="4237">eatures&amp;quot; condition used separate indicator features for every type of phrase or clause defined by the Charniak parser, while the &amp;quot;category features&amp;quot; used indicator features to mark the onset and offset of heuristically designed categories. All learners except the neural network had better performance in the &amp;quot;all features&amp;quot; condition, thus results for the &amp;quot;category features&amp;quot; are only shown for the neural network. Five learners were tested. The neural network was a sigmoidal feedforward network trained using error backpropagation. SLIPPER is a boosting algorithm based on the RIPPER rule learner (Cohen, 1995). C4.5 and QUEST are tree-based learners (Quinlan, 1993; Loh and Shih, 1997). Trees learned by the C4.5 algorithm were generally found to have better test-corpus accuracy if the tree was first post-processed in order to generate a series of rules; the resulting rules were also considerably more human-legible than the raw learned trees. QUEST was tested using either univariate nodes or &amp;quot;linear&amp;quot; nodes; the linear-node configuration implements a linear discriminant combination of all features at each node in the tree. Rulesets learned by univariate QUEST were generally more concise and more legib</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William W. Cohen. 1995. Fast effective rule induction. In Proc. International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Cohen</author>
</authors>
<title>A survey of machine learning methods for predicting prosody in radio speech. Master&apos;s thesis,</title>
<date>2004</date>
<institution>University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="22399" citStr="Cohen, 2004" startWordPosition="3547" endWordPosition="3548">o co-occur with any incorrect word string. The last two rows of Table 2 present results obtained using the semi-factored and factored bigram language models. The word perplexities of the bigram, semifactored, and factored language models, using the same test corpus as in Table 2, are 60, 54, and 47, respectively. The semi-factored model has significantly lower WER Table 3: Accent error rate (AER) and boundary error rate (BER) of five machine learning algorithms in the task of automatic prosodic transcription of the Radio News corpus based on word sequence information. NN=Neural Network. From (Cohen, 2004). Features: Word-based POS Learning Algorithm AER BER None (Chance) 42.6 19.1 C4.5 Rules 18.1 12.1 SLIPPER 18.4 11.5 QUEST Univariate 12.1 Features: Full Syntactic Parse C4.5 Rules 17.3 1L2 SLIPPER 17.7 10.2 QUEST Univariate 17.5 10.6 QUEST Linear 17.4 11.0 NN, All Features 17.1 10.8 NN, Category Features 16.9 10.4 than the baseline bigram (21.7% vs. 23.4%), but not significantly lower boundary error rate (14.2% vs. 14.3%) or accent error rate (20.3% vs. 20.3%). The factored model has significantly improved boundary recognition error (13.4% vs. 14.3%), but not significantly improved WER (22.9%</context>
</contexts>
<marker>Cohen, 2004</marker>
<rawString>Aaron Cohen. 2004. A survey of machine learning methods for predicting prosody in radio speech. Master&apos;s thesis, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Dilley</author>
<author>Stefanie Shattuck-Hufnagel</author>
<author>Mari Ostendorf</author>
</authors>
<title>Glottalization of word-initial vowels as a function of prosodic structure.</title>
<date>1996</date>
<journal>J. of Phonetics,</journal>
<pages>24--423</pages>
<marker>Dilley, Shattuck-Hufnagel, Ostendorf, 1996</marker>
<rawString>Laura Dilley, Stefanie Shattuck-Hufnagel, and Mari Ostendorf. 1996. Glottalization of word-initial vowels as a function of prosodic structure. J. of Phonetics, 24:423-444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile Fougeron</author>
<author>Patricia A Keating</author>
</authors>
<title>Articulatory strengthening at edges of prosodic domains.</title>
<date>1997</date>
<journal>J. Acoust. Soc. Am,</journal>
<pages>101--6</pages>
<contexts>
<context position="13304" citStr="Fougeron and Keating, 1997" startWordPosition="2086" endWordPosition="2089">s paper will be based on a 48-monophone recognizer, with prosodic splitting only of the duration PMF and the FO stream. Although monophones outperform any triphone or allophone model of this database, there are tendencies in the clustering result that support prior phonetic literature in interesting ways. Of the questions selected by the clustering algorithm, slightly more questions concerned intonational phrase position than pitch accent (21% vs. 16%) (Borys, 2003b), in agreement with a number of phonetic studies that suggest important articulatory correlates of intonational phrase boundary (Fougeron and Keating, 1997; Dilley etal., 1996; Cho, 2001). Although vowels were sensitive to all possible prosodic distinctions, consonants were sensitive only to the &amp;quot;lengthening vs. strengthening vs. neutral&amp;quot; three-way distinction proposed by Fougeron and Keating (Fougeron and Keating, 1997): phrase-final consonants (&amp;quot;lengthened&amp;quot;) were insensitive to pitch accent, while consonants at the beginning of a phrase-medial accented syllable were grouped together with both accented and unaccented phrase-initial consonants (&amp;quot;strengthened&amp;quot;) (Borys, 2003a). 3 Prosody-Dependent Language Models The relationship between syntax, p</context>
</contexts>
<marker>Fougeron, Keating, 1997</marker>
<rawString>Cecile Fougeron and Patricia A. Keating. 1997. Articulatory strengthening at edges of prosodic domains. J. Acoust. Soc. Am, 101(6):3728-3740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fujisaki</author>
<author>K Hirose</author>
</authors>
<title>Analysis of voice fundamental frequency contours for declarative sentence of Japanese.</title>
<date>1984</date>
<journal>J. Acoust Soc. Japan,</journal>
<pages>5--4</pages>
<contexts>
<context position="9438" citStr="Fujisaki and Hirose, 1984" startWordPosition="1471" endWordPosition="1475">ng and halving errors, we use a method similar to that proposed in (Kompe, 1997): a 3 mixture Gassian classifier is trained on the fo data from each utterance, with mixture component means constrained to equal 1/2, 1, and 2 times the utterance mean pitch. Measured fo candidates classified as apparently equal to 2f0 or fo /2 are eliminated, as are fo measurements with small PVs. Remaining fo measurements are normalized and converted to log scale using the formula = log (L) ± 1) , (3) where au is the utterance mean pitch. Eq. 3 is intended to mimic Fujisaki&apos;s log( fo / min Jo) parameterization (Fujisaki and Hirose, 1984; Hirai et al., 1997); in our experiments we found that estimates of the mean pitch are less sensitive to pitch tracking errors than estimates of the min fo, thus we find that Eq. 3 is less sensitive to pitch tracking errors than Fuji saki&apos;s parameterization. Frames with missing fo are filled by linearly interpolating fo between available frames, resulting in a smoothed normalized pitch waveform fo (t). The acoustic-prosodic observation stream models a scalar observation, Y(t) = 9( Lfo (t — 20ms), , fo (t + 20ms)]). The function g(.) is a multilayer perceptron, trained so that Y(t) is an estim</context>
</contexts>
<marker>Fujisaki, Hirose, 1984</marker>
<rawString>H. Fujisaki and K. Hirose. 1984. Analysis of voice fundamental frequency contours for declarative sentence of Japanese. J. Acoust Soc. Japan, 5(4):233-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Godfrey</author>
<author>E C Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="29309" citStr="Godfrey et al., 1992" startWordPosition="4637" endWordPosition="4640"> on duration of the pause. After these cues, the next several levels in both trees are primarily occupied by POS features. For example, an intonational phrase boundary is likely after a noun or interjection, or before an auxiliary, coordinating conjunction, modal verb, or the word &amp;quot;to.&amp;quot; 6 Switchboard Our current research seeks to extend these results to spontaneous speech. Using the neural network classifier whose performance is listed in Table 3, we have tagged syntactically predicted accent and intonational phrase boundary positions in the Switchboard conversational telephone speech corpus (Godfrey et al., 1992). In order to test these results, and in order to learn about the differences between conversational speech and read speech, we have started to manually transcribe the prosody and disfluency segments in the WS97 subset of Switchboard (Greenberg and Hitchcock, 2001; Chavarria et al., 2004). Preliminary results from this corpus indicate that statistical models trained to represent the prosody of Radio News speech are unable to predict the prosody of Switchboard speech. The models listed in Table 3 do not predict prosodic phrase boundary position at rates better than chance. Pitch accent is predi</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J.J. Godfrey, E.C. Holliman, and J. McDaniel. 1992. SWITCHBOARD: telephone speech corpus for research and development. In Proc. ICASSP, pages 517-520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Greenberg</author>
<author>Leah Hitchcock</author>
</authors>
<title>Stressaccent and vowel quality in the Switchboard corpus.</title>
<date>2001</date>
<booktitle>In NIST Large Vocabulary Continuous Speech Recognition Workshop,</booktitle>
<location>Linthicum Heights, MD,</location>
<contexts>
<context position="29573" citStr="Greenberg and Hitchcock, 2001" startWordPosition="4678" endWordPosition="4681">, modal verb, or the word &amp;quot;to.&amp;quot; 6 Switchboard Our current research seeks to extend these results to spontaneous speech. Using the neural network classifier whose performance is listed in Table 3, we have tagged syntactically predicted accent and intonational phrase boundary positions in the Switchboard conversational telephone speech corpus (Godfrey et al., 1992). In order to test these results, and in order to learn about the differences between conversational speech and read speech, we have started to manually transcribe the prosody and disfluency segments in the WS97 subset of Switchboard (Greenberg and Hitchcock, 2001; Chavarria et al., 2004). Preliminary results from this corpus indicate that statistical models trained to represent the prosody of Radio News speech are unable to predict the prosody of Switchboard speech. The models listed in Table 3 do not predict prosodic phrase boundary position at rates better than chance. Pitch accent is predicted with an accuracy better than chance, but still insufficient to be of any use for the clustering of allophone HMMs, thus when clustering experiments are repeated on the Switchboard corpus, error rates of the prosody-dependent and prosody-independent 1,1 . „ .1</context>
</contexts>
<marker>Greenberg, Hitchcock, 2001</marker>
<rawString>Steven Greenberg and Leah Hitchcock. 2001. Stressaccent and vowel quality in the Switchboard corpus. In NIST Large Vocabulary Continuous Speech Recognition Workshop, Linthicum Heights, MD, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>James F Allen</author>
</authors>
<title>Speech repairs, intonational phrases and discourse markers: Modeling speakers&apos; utterances in spoken dialogue.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="31532" citStr="Heeman and Allen, 1999" startWordPosition="4980" endWordPosition="4983">s discovered that phrase-final lengthening is the only reliable acoustic correlate of this distinction; FO correlates seem not to reliably mark this distinction in Switchboard (Chavarria et al., 2004). Disfluency is the third reason that Switchboard prosody is unlike Radio News prosody. Although we began transcribing Switchboard with the goal of only annotating prosody, we discovered almost immediately that it is impossible to annotate prosodic phrase boundaries in Switchboard without devising some sort of annotation for disfluencies. We have adopted the annotation system of Heeman and Allen (Heeman and Allen, 1999), according to which the words being corrected are called the &amp;quot;reparandum&amp;quot; or REP, the correction is called the &amp;quot;alteration&amp;quot; (ALT), and filled pauses or meta-dialog between REP and ALT are called the &amp;quot;edit&amp;quot; (EDT). Fig. 1 shows a disfluency with a double reparandum: &amp;quot;I, I, one of the things I...&amp;quot; The first reparandum is repeated, then finally replaced by the alteration. Fig. 1 shows two characteristics of disfluency that have not been extensively studied. First, both of the reparanda end in glottalization, clearly visible in the form of extremely low-frequency or low-amplitude glottal excitatio</context>
</contexts>
<marker>Heeman, Allen, 1999</marker>
<rawString>Peter A. Heeman and James F. Allen. 1999. Speech repairs, intonational phrases and discourse markers: Modeling speakers&apos; utterances in spoken dialogue. Computational Linguistics, 25(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshio Hirai</author>
</authors>
<title>Naoto Iwahashi, Norio Higuchi, and Yoshinori Sagisaka.</title>
<date>1997</date>
<booktitle>Progress in Speech Synthesis,</booktitle>
<pages>333--346</pages>
<editor>In Jan P. H. van Santen, Richard W. Sproat, Joseph P. Olive, and Julia Hirschberg, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<marker>Hirai, 1997</marker>
<rawString>Toshio Hirai, Naoto Iwahashi, Norio Higuchi, and Yoshinori Sagisaka. 1997. Automatic extraction of ft control rules using statistical analysis. In Jan P. H. van Santen, Richard W. Sproat, Joseph P. Olive, and Julia Hirschberg, editors, Progress in Speech Synthesis, pages 333-346. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung-Suk Kim</author>
<author>Mark Hasegawa-Johnson</author>
<author>Ken Chen</author>
</authors>
<title>(in press). Automatic recognition of pitch movements using multi-layer perceptron and time-delay recursive neural network.</title>
<date>1997</date>
<journal>IEEE Signal Processing</journal>
<publisher>Springer-Verlag.</publisher>
<marker>Kim, Hasegawa-Johnson, Chen, 1997</marker>
<rawString>Sung-Suk Kim, Mark Hasegawa-Johnson, and Ken Chen. (in press). Automatic recognition of pitch movements using multi-layer perceptron and time-delay recursive neural network. IEEE Signal Processing Letters. R. Kompe. 1997. Prosody in Speech Understanding Systems. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-Y Loh</author>
<author>Y-S Shih</author>
</authors>
<title>Split selection methods for classification trees. Statistica Sinica,</title>
<date>1997</date>
<pages>7--815</pages>
<contexts>
<context position="26845" citStr="Loh and Shih, 1997" startWordPosition="4246" endWordPosition="4249"> phrase or clause defined by the Charniak parser, while the &amp;quot;category features&amp;quot; used indicator features to mark the onset and offset of heuristically designed categories. All learners except the neural network had better performance in the &amp;quot;all features&amp;quot; condition, thus results for the &amp;quot;category features&amp;quot; are only shown for the neural network. Five learners were tested. The neural network was a sigmoidal feedforward network trained using error backpropagation. SLIPPER is a boosting algorithm based on the RIPPER rule learner (Cohen, 1995). C4.5 and QUEST are tree-based learners (Quinlan, 1993; Loh and Shih, 1997). Trees learned by the C4.5 algorithm were generally found to have better test-corpus accuracy if the tree was first post-processed in order to generate a series of rules; the resulting rules were also considerably more human-legible than the raw learned trees. QUEST was tested using either univariate nodes or &amp;quot;linear&amp;quot; nodes; the linear-node configuration implements a linear discriminant combination of all features at each node in the tree. Rulesets learned by univariate QUEST were generally more concise and more legible, to a human expert, than those learned by any other algorithm. In some ca</context>
</contexts>
<marker>Loh, Shih, 1997</marker>
<rawString>W.-Y. Loh and Y.-S. Shih. 1997. Split selection methods for classification trees. Statistica Sinica, 7:815-840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>K Ross</author>
</authors>
<title>A multi-level model for recognition of intonation labels. In Computing prosody: computational models for processing spontaneous speech.</title>
<date>1997</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="3941" citStr="Ostendorf and Ross, 1997" startWordPosition="605" endWordPosition="608">rpus (Ostendorf et al., 1995). The Radio News Corpus is the largest publicly available corpus labeled with the tones and break indices (TOBI) prosodic labeling standard (Beckman and Elam, 1994). The Radio News Corpus was designed for speech synthesis studies. By speech recognition standards, it is an extremely small corpus (a bit over 3 hours of speech, read by seven professional radio announcers). To our knowledge, no other research group has reported speech recognition word error rate for this corpus, but two studies have reported automatic pitch accent recognition results for this corpus. (Ostendorf and Ross, 1997) achieved 89% accent recognition correctness given known word alignment. (Taylor, 2000) reported 72.7% accent recognition correctness, and 47.7% accent recognition accuracy, based purely on observation of FO (without lexical sequence information or MFCC observations). Our experiments are not directly comparable to either of these previous studies; like Taylor, we do not assume a priori knowledge of word boundary times, but like Ostendorf and Ross, we use word sequence information to aid us in the automatic labeling of prosodic tags. Current experiments seek to extend our system to the Switchbo</context>
</contexts>
<marker>Ostendorf, Ross, 1997</marker>
<rawString>M. Ostendorf and K. Ross. 1997. A multi-level model for recognition of intonation labels. In Computing prosody: computational models for processing spontaneous speech. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>P J Price</author>
<author>S Shattuck-Hufnagel</author>
</authors>
<date>1995</date>
<institution>The Boston University Radio News Corpus. Linguistic Data Consortium.</institution>
<contexts>
<context position="3345" citStr="Ostendorf et al., 1995" startWordPosition="509" endWordPosition="512">. 1 is implemented by way of an intermediate sequence of allophone labels, Q = [q,. , qL], thus P(147, S, P, 0) maxQ p(010P(Q1W, P(W, P1S)P(S) (2) Enabling technologies for the recognition of prosody include prosody-dependent allophones and prosodysensitive acoustic observations, discussed in Sec. 2. Enabling technology for the simultaneous recognition of syntax is a factored prosody-dependent language model, with factors representing part of speech and (in a rescoring pass) CFG parse structure, discussed in Sections 3 and 5. The system has been trained and tested using the Radio News Corpus (Ostendorf et al., 1995). The Radio News Corpus is the largest publicly available corpus labeled with the tones and break indices (TOBI) prosodic labeling standard (Beckman and Elam, 1994). The Radio News Corpus was designed for speech synthesis studies. By speech recognition standards, it is an extremely small corpus (a bit over 3 hours of speech, read by seven professional radio announcers). To our knowledge, no other research group has reported speech recognition word error rate for this corpus, but two studies have reported automatic pitch accent recognition results for this corpus. (Ostendorf and Ross, 1997) ach</context>
</contexts>
<marker>Ostendorf, Price, Shattuck-Hufnagel, 1995</marker>
<rawString>M. Ostendorf, P.J. Price, and S. Shattuck-Hufnagel. 1995. The Boston University Radio News Corpus. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>B Byrne</author>
<author>M Fink</author>
<author>A Gunawardana</author>
<author>K Ross</author>
<author>S Roweis</author>
<author>E Shriberg</author>
<author>D Talkin</author>
<author>A Waibel</author>
<author>B Wheatley</author>
<author>T Zeppenfield</author>
</authors>
<title>Modeling systematic variations in pronunciation via a languagedependent hidden speaking mode: Final report.</title>
<date>1997</date>
<tech>Technical Report WS96,</tech>
<institution>Johns Hopkins University Center for Language and Speech Processing.</institution>
<contexts>
<context position="11177" citStr="Ostendorf et al., 1997" startWordPosition="1769" endWordPosition="1772">rs per state. A Gaussian model of the scalar FO stream works as well, in our experiments, as a mixture Gaussian model, thus the FO stream requires only 2 trainable parameters per state. The MFCC stream, by comparison, requires 237 trainable parameters per state for a 3-mixture model of a 39-dimensional acoustic feature vector. Because of the relatively high parameter dimension of the MFCC PDF, all of our attempts to condition the MFCC stream on prosodic context have been stymied by data sparsity problems. Our approach to prosodic conditioning of the MFCC stream is similar to that proposed in (Ostendorf et al., 1997). Either individual HMM states (as in (Ostendorf et al., 1997)) or entire allophone models are first split into prosody-dependent allophones (as shown in Table 1), then clustered using a standard cross-entropy-based hierarchical clustering algorithm. Two baselines are used: a recognizer composed of clustered prosody-independent triphone models, and a recognizer composed of monophone models. First, we attempted to cluster individual HMM states using the HTK hierarchical clustering routines (HERest, HLStats, and HHEd); embedded reestimation using HERest failed repeatedly to converge, apparently </context>
</contexts>
<marker>Ostendorf, Byrne, Fink, Gunawardana, Ross, Roweis, Shriberg, Talkin, Waibel, Wheatley, Zeppenfield, 1997</marker>
<rawString>M. Ostendorf, B. Byrne, M. Fink, A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, D. Talkin, A. Waibel, B. Wheatley, and T. Zeppenfield. 1997. Modeling systematic variations in pronunciation via a languagedependent hidden speaking mode: Final report. Technical Report WS96, Johns Hopkins University Center for Language and Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Price</author>
<author>M Ostendorf</author>
<author>S Shattuck-Hufnagel</author>
<author>C Fong</author>
</authors>
<title>The use of prosody in syntactic disambiguation.</title>
<date>1991</date>
<journal>J. Acoust. Soc. Am,</journal>
<pages>90--6</pages>
<contexts>
<context position="16884" citStr="Price et al., 1991" startWordPosition="2651" endWordPosition="2654">nal to the bigram probability of a syntactically and prosodically tagged vocabulary. This tagged bigram probability may be computed as sj, sz,A)P(s3, si 1w3, wi)P(wi Iwi,Pi) (5) The approximation in Eq. 5 is valid if we assume that, first, prosody is independent of the word string given knowledge of syntax (reasonable because neither side of the equation has any explicit representation of dialog context), and second, that the syntactic tags are independent of prosody given knowledge of the word string (reasonable except for those cases when prosody may be used to resolve syntactic ambiguity, (Price et al., 1991)). Under these assumptions, the tagged bigram probability factors into three terms. The first term, P(Pi si si, Pi ), may be robustly estimated from a relatively small corpus, because the syntactic tagset and the prosodic tagset are both much smaller than the vocabulary. The second term,P (i&apos; is the probability that a word sequence ( si v , Isizl)wiirUZents syntactic tag sequence (si , si). Computation of this probability is simplified by appropriate choice of the syntactic tagset. During firstpass recognition, the syntactic tag sz encodes only the part of speech of word wi. In most cases, the</context>
</contexts>
<marker>Price, Ostendorf, Shattuck-Hufnagel, Fong, 1991</marker>
<rawString>P.J. Price, M. Ostendorf, S. Shattuck-Hufnagel, and C. Fong. 1991. The use of prosody in syntactic disambiguation. J. Acoust. Soc. Am, 90(6):2956-2970, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan</publisher>
<location>Kaufman, Boston.</location>
<contexts>
<context position="26824" citStr="Quinlan, 1993" startWordPosition="4244" endWordPosition="4245">r every type of phrase or clause defined by the Charniak parser, while the &amp;quot;category features&amp;quot; used indicator features to mark the onset and offset of heuristically designed categories. All learners except the neural network had better performance in the &amp;quot;all features&amp;quot; condition, thus results for the &amp;quot;category features&amp;quot; are only shown for the neural network. Five learners were tested. The neural network was a sigmoidal feedforward network trained using error backpropagation. SLIPPER is a boosting algorithm based on the RIPPER rule learner (Cohen, 1995). C4.5 and QUEST are tree-based learners (Quinlan, 1993; Loh and Shih, 1997). Trees learned by the C4.5 algorithm were generally found to have better test-corpus accuracy if the tree was first post-processed in order to generate a series of rules; the resulting rules were also considerably more human-legible than the raw learned trees. QUEST was tested using either univariate nodes or &amp;quot;linear&amp;quot; nodes; the linear-node configuration implements a linear discriminant combination of all features at each node in the tree. Rulesets learned by univariate QUEST were generally more concise and more legible, to a human expert, than those learned by any other </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufman, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Dmitry Zelenko</author>
</authors>
<title>Part of speech tagging using a network of linear separators.</title>
<date>1998</date>
<booktitle>In COLINGACL.</booktitle>
<contexts>
<context position="24530" citStr="Roth and Zelenko, 1998" startWordPosition="3870" endWordPosition="3873">as first automatically parsed to produce syntactic tags, and syntactic tags were then classified in order to determine prosodic tags. Two types of binary prosodic tags were estimated: accent recognition marked the target word as either accented or unaccented, while boundary recognition marked the target word as either intonational-phrase-initial or non-initial. Two types of syntactic parsers were used. The top half of the table, marked &amp;quot;Word-Based POS,&amp;quot; describes accent recognition experiments using part of speech (POS) information generated by the Roth-Zelenko (RZ) shallow parsing algorithm (Roth and Zelenko, 1998), and prosodic boundary recognition experiments using the RZ algorithm plus seven syntactic phrase boundary features. Each prosodic tag is computed based on observation of the POS of three consecutive words (the target word plus two prior words). Each POS tag, in turn, is computed by the RZ algorithm based on lexical features in a five word window, thus the total system computes prosodic tags of one word based on lexical features of 3+5-1=7 consecutive words. Recognition of intonational phrase boundary based only on POS information was found to be quite poor, therefore boundary recognition res</context>
</contexts>
<marker>Roth, Zelenko, 1998</marker>
<rawString>Dan Roth and Dmitry Zelenko. 1998. Part of speech tagging using a network of linear separators. In COLINGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
</authors>
<title>To &apos;erre is human: ecology and acoustics of speech disfluencies.</title>
<date>2001</date>
<journal>Journal of the International Phonetic Association,</journal>
<pages>31--1</pages>
<contexts>
<context position="33263" citStr="Shriberg, 2001" startWordPosition="5267" endWordPosition="5268">y is perceptually salient: some listeners report that the alteration &amp;quot;mimics&amp;quot; the intonational contour of the reparandum. Disfluency is common in Switchboard. Of 1100 words we have transcribed, 40 are part of a reparandum, 37 are filled pauses, and 41 are part of an alteration, thus 10% of the words we have transcribed are part of a disfluency. This estimate is higher than most published estimates, perhaps because we include all words that are part of the reparandum or alteration, but most published studies estimate that at least 5% of the words in Switchboard are part of a disfluency (e.g., (Shriberg, 2001)). Any complete description of the prosody of Switchboard will necessarily include, as one component, a theory about the prosody of disfluency. 7 Conclusions This paper has reviewed results from a number of experimental systems that simultaneously recognize the prosodic and lexical transcriptions of an utterance. It has been demonstrated, first, that prosody-dependent allophone modeling can reduce the word error rate of a speech recognizer, but that reliable WER reductions depend on the simultaneous use of both a prosodydependent acoustic model and a prosody-dependent language model. Additiona</context>
</contexts>
<marker>Shriberg, 2001</marker>
<rawString>Elizabeth Shriberg. 2001. To &apos;erre is human: ecology and acoustics of speech disfluencies. Journal of the International Phonetic Association, 31(1):153-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Taylor</author>
</authors>
<title>Analysis and synthesis of intonation using the Tilt model.</title>
<date>2000</date>
<journal>J. Acoust Soc. Am,</journal>
<pages>107--3</pages>
<contexts>
<context position="4028" citStr="Taylor, 2000" startWordPosition="619" endWordPosition="620">ed with the tones and break indices (TOBI) prosodic labeling standard (Beckman and Elam, 1994). The Radio News Corpus was designed for speech synthesis studies. By speech recognition standards, it is an extremely small corpus (a bit over 3 hours of speech, read by seven professional radio announcers). To our knowledge, no other research group has reported speech recognition word error rate for this corpus, but two studies have reported automatic pitch accent recognition results for this corpus. (Ostendorf and Ross, 1997) achieved 89% accent recognition correctness given known word alignment. (Taylor, 2000) reported 72.7% accent recognition correctness, and 47.7% accent recognition accuracy, based purely on observation of FO (without lexical sequence information or MFCC observations). Our experiments are not directly comparable to either of these previous studies; like Taylor, we do not assume a priori knowledge of word boundary times, but like Ostendorf and Ross, we use word sequence information to aid us in the automatic labeling of prosodic tags. Current experiments seek to extend our system to the Switchboard corpus. In order to train on Switchboard, it is necessary, first, to both manually </context>
</contexts>
<marker>Taylor, 2000</marker>
<rawString>Paul Taylor. 2000. Analysis and synthesis of intonation using the Tilt model. J. Acoust Soc. Am, 107(3):1697-1714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Wightman</author>
<author>Stefanie Shattuck-Hufnagel</author>
<author>Mari Ostendorf andPatti Price</author>
</authors>
<title>Segmental durations in the vicinity of prosodic phrase boundaries.</title>
<date>1992</date>
<journal>J. Acoust. Soc. Am,</journal>
<pages>91--3</pages>
<contexts>
<context position="6262" citStr="Wightman et al., 1992" startWordPosition="961" endWordPosition="964">tic prosodic PDFs, thus each type of observation is used to distinguish only those context variables with which it is most highly correlated. Allophone clusters may be defined by any of the following five context variables: left phonetic context, right phonetic context, pitch accent, intonational phrase position, syntactic category. A complete specification of these five context variables may be encoded using the notation shown in Table I. An allophone is considered to be phrase-final if it is part of the rhyme of the syllable preceding an intonational phrase boundary, and nonfinal otherwise (Wightman et al., 1992). An allophone is considered to be accented if it is part of the lexically stressed syllable of a word transcribed as containing a pitch accent, and unaccented otherwise. Other prosodic Table 1: Context variables that may be used to determine an allophone cluster. A fully specified allophone of phoneme PH takes the form L-PH+R_AP S. Variable Meaning Allowed Settings L Left Phoneme (vwl, gld, nsl, fric, stop) R Right Phoneme (vwl, gld, nsl, fric, stop) A Accent (unaccented, accented) P Phrase (non-final, final) S Syntax (content, function) distinctions that we have tested include the distinctio</context>
</contexts>
<marker>Wightman, Shattuck-Hufnagel, Price, 1992</marker>
<rawString>Colin Wightman, Stefanie Shattuck-Hufnagel, and Mari Ostendorf andPatti Price. 1992. Segmental durations in the vicinity of prosodic phrase boundaries. J. Acoust. Soc. Am, 91(3):1707-1717, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>