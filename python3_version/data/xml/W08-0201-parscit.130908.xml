<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.141339">
<title confidence="0.981347">
Teaching computational linguistics to a large, diverse student body:
courses, tools, and interdepartmental interaction
</title>
<author confidence="0.997322">
Jason Baldridge and Katrin Erk
</author>
<affiliation confidence="0.999468">
Department of Linguistics
The University of Texas at Austin
</affiliation>
<email confidence="0.999388">
{jbaldrid,erk}@mail.utexas.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999490777777778">
We describe course adaptation and develop-
ment for teaching computational linguistics
for the diverse body of undergraduate and
graduate students the Department of Linguis-
tics at the University of Texas at Austin. We
also discuss classroom tools and teaching aids
we have used and created, and we mention
our efforts to develop a campus-wide compu-
tational linguistics program.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981940926829268">
We teach computational linguistics courses in the
linguistics department of the University of Texas
at Austin, one of the largest American universi-
ties. This presents many challenges and opportu-
nities; in this paper, we discuss issues and strate-
gies for designing courses in our context and build-
ing a campus-wide program.&apos; The main theme of
our experience is that courses should be targeted to-
ward specific groups of students whenever possible.
This means identifying specific needs and design-
ing the course around them rather than trying to sat-
isfy a diverse set of students in each course. To this
end, we have split general computational linguistics
courses into more specific ones, e.g., working with
corpora, a non-technical overview of language tech-
nology applications, and natural language process-
ing. In section 2, we outline how we have stratified
our courses at both the graduate and undergraduate
levels.
&apos;Links to the courses, tools, and resources described in this
paper can be found on our main website:
http://comp.ling.utexas.edu
As part of this strategy, it is crucial to ensure that
the appropriate student populations are reached and
that the courses fulfill degree requirements. For ex-
ample, our Language and Computers course fulfills
a Liberal Arts science requirement and our Natural
Language Processing is cross-listed with computer
science. This is an excellent way to get students in
the door and ensure that courses meet or exceed min-
imum enrollments. We find that many get hooked
and go on to specialize in computational linguistics.
Even for targeted CL courses, there is still usually
significant diversity of backgrounds among the stu-
dents taking them. Thus, it is still important to care-
fully consider the teaching tools that are used; in sec-
tion 3, we discuss our experience with several stan-
dard tools and two of our own. Finally, we describe
our efforts to build a campus-wide CL program that
provides visibility for CL across the university and
provides coherence in our course offerings.
</bodyText>
<sectionHeader confidence="0.998689" genericHeader="introduction">
2 Courses
</sectionHeader>
<bodyText confidence="0.999963285714285">
Our courses are based on those initiated by Jonas
Kuhn between 2002 and 2005. Since 2005, we have
created several spin-off courses for students with
different backgrounds. Our broad goals for these
courses are to communicate both the practical util-
ity of computational linguistics and its promise for
improving our understanding of human languages.
</bodyText>
<subsectionHeader confidence="0.882213">
2.1 Graduate
</subsectionHeader>
<bodyText confidence="0.99990675">
We started with two primary graduate courses, Com-
putational Linguistics I and II. The first introduces
central algorithms and data structures in computa-
tional linguistics, while the second focuses on learn-
</bodyText>
<page confidence="0.824027">
1
</page>
<note confidence="0.838947">
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 1–9,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9997455">
Figure 1: Flow for non-seminar courses. Left: graduate
courses, right: undergraduate courses.
</figureCaption>
<bodyText confidence="0.9999716625">
ing and disambiguation. This served a computation-
ally savvy segment of the student population quite
well. However, we view one of our key teaching
contributions as computational linguists in a linguis-
tics department to be providing non-computational
students with technical and formal skills useful for
their research. We discovered quickly that our first
computational linguistics course did not fill these
needs, and the second is not even accessible to most
students. The graduate linguistics students did put
in the effort to learn Python for Computational Lin-
guistics I, but many would have preferred a (much)
gentler introduction and also more coverage of is-
sues connected to their linguistic concerns. This led
us to create a new course, Working with Corpora.
Still, there is a need for the primary courses,
which receive interest from many students in com-
puter science and also graduate students from other
departments such as German and English. One of
the great surprises for us in our graduate courses has
been the interest from excellent linguistics and com-
puter science undergraduates.
We have sought to encourage our students to be
active in the academic community outside of UT
Austin. One way we do this is to have a final
project for each course (and most seminars) that has
four distinct stages: (i) a project proposal halfway
through the semester, (ii) a progress report three-
quarters of the way through, (iii) a 10-minute pre-
sentation during the last week of class, and (iv) a
final report at the end. We have found that having
course projects done in this staged manner ensures
that students think very thoroughly about what their
topic is early on, receive significant feedback from
us, and then still have enough time to do significant
implementation for their project, rather than rushing
everything in last minute. Also, by having students
do presentations on their work before they hand in
the final report, they can incorporate feedback from
other students. A useful strategy we have found for
scoring these projects is to use standard conference
reviews in Computational Linguistics II. The final
projects have led to several workshops and confer-
ence publications for the students so far, as well
as honors theses. The topics have been quite var-
ied (in line with our varied student body), including
lexicon induction using genetic algorithms (Ponvert,
2007), alignment-and-transfer for bootstrapping tag-
gers (Moon and Baldridge, 2007), lemmatization us-
ing parallel corpora (Moon and Erk, 2008), graphi-
cal visualization of articles using syntactic depen-
dencies (Jeff Rego, CS honors thesis), and feature
extraction for semantic role labeling (Trevor Foun-
tain, CS honors thesis).
Working with corpora. Computational linguis-
tics skills and techniques are tremendously valuable
for linguists using corpora. Ideally, a linguist should
be able to extract the relevant data, count occur-
rences of phenomena, and do statistical analyses.
The intersection of these skills and needs is the core
of this course, which covers corpus formats (XML,
bracket formats for syntax, “word/POS” formats for
part-of-speech information), query languages and
tools (regular expressions, cqp, tregex), and some
statistical analysis techniques. It also teaches Python
gently for liberal arts students who have never pro-
grammed and have only limited or no knowledge of
text processing. Other main topics are the compi-
lation of corpora and corpus annotation, with issues
like representativity and what meta-data to include.
At the end of this course, students are prepared for
our primary computational courses.
We observed the tremendous teaching potential
of effective visualization in this course with the R
statistics package. It was used for statistical anal-
yses: students loved it because they could produce
meaningful results immediately and visualize them.
The course includes only a very short two-session
introduction to working with R. We were worried
that this would overtax students because R is its own
</bodyText>
<page confidence="0.988284">
2
</page>
<bodyText confidence="0.999933384615385">
programming language. But interestingly they had
no problems with learning this second programming
language (after Python). This is particularly striking
as most of the students had no programming experi-
ence prior to the class.
We have not yet used the Natural Language
Toolkit (Loper and Bird, 2002) (see Section 3.1) in
this course. But as it, too, offers visualization and
rapid access to meaningful results, we intend to use
it in the future. In particular, the NLTK allows very
easy access to Toolbox data (Robinson et al., 2007),
which we feel will greatly improve the utility and
appeal of the course for the significant number of
documentary linguistics students in the department.
Seminars. We also offer several seminars in
our areas of interest. These include Categorial
Grammar, Computational Syntax, and Lexical Ac-
quisition. These courses have attracted “non-
computational” linguistics students with related in-
terests, and have served as the launching point for
several qualifying papers and masters theses. It
is important to offer these courses so that these
students gain a view into computational linguistics
from the standpoint of a topic with which they al-
ready have some mastery; it also ensures healthier
enrollments from students in our own department.
We are currently co-teaching a seminar called
Spinning Straw into Gold: Automated Syntax-
Semantics Analysis, that is designed to overlap with
the CoNLL-2008 shared task on joint dependency
parsing and semantic role labeling. The entire class
is participating in the actual competition, and we
have been particularly pleased with how this exter-
nal facet of the course motivates students to consider
the topics we cover very carefully – the papers truly
matter for the system we are building. It provides an
excellent framework with which to judge the contri-
butions of recent research in both areas and compu-
tational linguistics more generally.
</bodyText>
<subsectionHeader confidence="0.995183">
2.2 Undergraduate
</subsectionHeader>
<bodyText confidence="0.997925773584906">
Our first undergraduate course was Introduction to
Computational Linguistics in Fall 2006. Our expe-
rience with this course, which had to deal with the
classic divide in computational linguistics courses
between students with liberal arts versus computer
science backgrounds, led us to split it into two
courses. We briefly outline some of the missteps
with this first course (and what worked well) and
how we are addressing them with new courses.
Introduction to Computational Linguistics.
This course is a boiled-down version of the graduate
Computational Linguistics I taught in Fall 2006.
Topics included Python programming, regular
expressions, finite-state transducers, part-of-speech
tagging, context-free grammar, categorial grammar,
meaning representations, and machine translation.
Overall, the course went well, but enrollment
dropped after the mid-term. As many have found
teaching such courses, some students truly struggled
with the course material while others were ready for
it to go much faster. Several students had interpreted
“introduction” to mean that it was going to be about
computational linguistics, but that they would not
actually have to do computational linguistics. Many
stayed with it, but there were still others who could
have gone much further if it had not been necessary
to slow down to cover basic material like for loops.
Note that several linguistics majors were among the
compationally savvy students.
In fairness to the students who struggled, it was
certainly ill-advised to ask students with no previous
background to learn Python and XFST in a single
semester. One of the key points of confusion was
regular expression syntax. The syntax used in the
textbook (Jurafsky and Martin, 2000) transfers eas-
ily to regular expressions in Python, but is radically
different from that of XFST. For students who had
never coded anything in their life, this proved ex-
tremely frustrating. On the other hand, for computa-
tionally savvy students, XFST was great fun, and it
was an interesting new challenge after having to sit
through very basic Python lectures.
On the other hand, the use of NLTK to drive learn-
ing about Python and NLP tasks (like building POS-
taggers) significantly eased the burden for new pro-
grammers. Many of them were highly satisfied that
they could build interesting programs and experi-
ment with their behavior so easily.
Language and Computers. We had fortunately
already planned the first replacement course: Lan-
guage and Computers, based on the course designed
at the Department of Linguistics at the Ohio State
University (Brew et al., 2005). This course intro-
</bodyText>
<page confidence="0.994951">
3
</page>
<bodyText confidence="0.999767511111111">
duces computational linguistics to a general audi-
ence and is ideal for students who want exposure
to computational methods without having to learn
to program. We designed and taught it jointly, and
added several new aspects to the course. Whereas
OSU’s course fulfills a Mathematical and Logical
Analysis requirement, our course fulfills a Science
requirement for liberal arts majors. These require-
ments were met by course content that requires un-
derstanding and thinking about formal methods.
The topics we added to our course were question
answering, cryptography,2 and world knowledge.
The course provides ample opportunity to discuss
high-level issues in language technology with low-
level aspects such as understanding particular algo-
rithms (e.g., computing edit distance with dynamic
programming) and fundamental concepts (such as
regular languages and frequency distributions).
In addition to its target audience, the course
nonetheless attracts students who are already well-
versed in many of the low-level concepts. The high-
level material plays an important role for such stu-
dents: while they find the low-level problems quite
easy, many find a new challenge in thinking about
and communicating clearly the wider role that such
technologies play. The high-level material is even
more crucial for holding the interest of less formally
minded students. It gives them the motivation to
work through and understand calculations and com-
putations that might otherwise bore them. Finally,
it provides an excellent way to encourage class dis-
cussion. For example, this year’s class became very
animated on the question of “Can a machine think?”
that we discussed with respect to dialogue systems.
Though the course does not require students to
do any programming, we do show them short pro-
grams that accomplish (simplified versions of) some
of the tasks discussed in the course; for example,
short programs for document retrieval and creating
a list of email address from US census data. The
goal is to give students a glimpse into such applica-
tions, demonstrate that they are not hugely compli-
cated magical systems, and hopefully entice some of
them to learn how to do it for themselves.
The 2007 course was quite successful: it filled
</bodyText>
<footnote confidence="0.695723">
2The idea to cover cryptography came from a discussion
with Chris Brew; he now teaches an entire course on it at OSU.
</footnote>
<bodyText confidence="0.999693288888889">
up (40 students) and received very positive feedback
from the students. It filled up again for this year’s
Spring 2008 offering. The major challenge is the
lack of a textbook, which means that students must
rely heavily on lecture slides and notes.
Words in a Haystack: Methods and Tools for
Working with Corpora. This advanced under-
graduate version of Working with corpora was of-
fered because we felt that graduate and undergrad-
uate linguistics students were actually on an equal
footing in their prior knowledge, and could profit
equally from a gentle introduction to programming.
Although the undergraduate students were active
and engaged in the class, they did not benefit as
much from it as the graduate students. This is likely
because graduate students had already experienced
the need for extracting information from corpora for
their research and the consequent frustration when
they did not have the skills to do so.
Natural Language Processing. This is an de-
manding course that will be taught in Fall 2008. It
is cross-listed with computer science and assumes
knowledge of programming and formal methods in
computer science, mathematics, or linguistics. It is
designed for the significant number of students who
wish to carry on further from the courses described
previously. It is also an appropriate course for un-
dergraduates who have ended up taking our graduate
courses for lack of such an option.
Much of the material from Introduction to Com-
putational Linguistics will be covered in this course,
but it will be done at a faster pace and in greater
detail since programming and appropriate thinking
skills are assumed. A significant portion of the grad-
uate course Computational Linguistics II also forms
part of the syllabus, including machine learning
methods for classification tasks, language modeling,
hidden Markov models, and probabilistic parsing.
We see cross-listing the course with computer sci-
ence as key to its success. Though there are many
computationally savvy students in our liberal arts
college, we expect cross-listing to encourage signif-
icantly more computer science students to try out a
course that they would otherwise overlook or be un-
able to use for fulfilling degree requirements.
</bodyText>
<page confidence="0.995252">
4
</page>
<sectionHeader confidence="0.956604" genericHeader="method">
3 Teaching Tools and Tutorials
</sectionHeader>
<bodyText confidence="0.9999466">
We have used a range of external tools and have
adapted tools from our own research for various as-
pects of our courses. In this section, we describe our
experience using these as part of our courses.
We have used Python as the common language in
our courses. We are pleased with it: it is straight-
forward for beginning programmers to learn, its in-
teractive prompt facilitates in-class instruction, it is
text-processing friendly, and it is useful for gluing
together other (e.g., Java and C++) applications.
</bodyText>
<subsectionHeader confidence="0.999532">
3.1 External tools and resources
</subsectionHeader>
<bodyText confidence="0.999957307692308">
NLTK. We use the Natural Language Toolkit
(NLTK) (Loper and Bird, 2002; Bird et al., 2008) in
both undergraduate and graduate courses for in-class
demos, tutorials, and homework assignments. We
use the toolkit and tutorials for several course com-
ponents, including introductory Python program-
ming, text processing, rule-based part-of-speech tag-
ging and chunking, and grammars and parsing.
NLTK is ideal for both novice and advanced pro-
grammers. The tutorials and extensive documenta-
tion provide novices with plenty of support outside
of the classroom, and the toolkit is powerful enough
to give plenty of room for advanced students to play.
The demos are also very useful in classes and serve
to make many of the concepts, e.g. parsing algo-
rithms, much more concrete and apparent. Some
students also use NLTK for course projects. In all,
NLTK has made course development and execution
significantly easier and more effective.
XFST. A core part of several courses is finite-state
transducers. FSTs have unique qualities for courses
about computational linguistics that are taught in
linguistics department. They are an elegant exten-
sion of finite-state automata and are simple enough
that their core aspects and capabilities can be ex-
pressed in just a few lectures. Computer science stu-
dents immediately get excited about being able to
relate string languages rather than just recognizing
them. More importantly, they can be used to ele-
gantly solve problems in phonology and morphol-
ogy that linguistics students can readily appreciate.
We use the Xerox Finite State Toolkit (XFST)
(Beesley and Karttunen, 2003) for in-class demon-
strations and homeworks for FSTs. A great aspect of
using XFST is that it can be used to show that differ-
ent representations (e.g., two-level rules versus cas-
caded rules) can be used to define the same regular
relation. This exercise injects some healthy skepti-
cism into linguistics students who may have to deal
with formalism wars in their own linguistic subfield.
Also, XFST allows one to use lenient composition to
encode Optimality Theory constraints and in so do-
ing show interesting and direct contrasts and com-
parisons between paper-and-pencil linguistics and
rigorous computational implementations.
As with other implementation-oriented activities
in our classes, we created a wiki page for XFST tu-
torials.3 These were adapted and expanded from Xe-
rox PARC materials and Mark Gawron’s examples.
Eisner’s HMM Materials. Simply put: the
spreadsheet designed by Jason Eisner (Eisner, 2002)
for teaching hidden Markov models is fantastic. We
used that plus Eisner’s HMM homework assignment
for Computational Linguistics II in Fall 2007. The
spreadsheet is great for interactive classroom explo-
ration of HMMs—students were very engaged. The
homework allows students to implement an HMM
from scratch, giving enough detail to alleviate much
of the needless frustration that could occur with this
task while ensuring that students need to put in sig-
nificant effort and understand the concepts in order
to make it work. It also helps that the new edition
of Jurafsky and Martin’s textbook discusses Eisner’s
ice cream scenario as part of its much improved
explanation of HMMs. Students had very positive
feedback on the use of all these materials.
Unix command line. We feel it is important to
make sure students are well aware of the mighty
Unix command line and the tools that are available
for it. We usually have at least one homework as-
signment per course that involves doing the same
task with a Python script versus a pipeline using
command line tools like tr, sort, grep and awk.
This gives students students an appreciation for the
power of these tools and for the fact that they are at
times preferable to writing scripts that handle every-
thing, and they can see how scripts they write can
form part of such pipelines. As part of this module,
</bodyText>
<footnote confidence="0.9599595">
3http://comp.ling.utexas.edu/wiki/doku.
php/xfst
</footnote>
<page confidence="0.995847">
5
</page>
<bodyText confidence="0.998424">
we have students work through the exercises in the
draft chapter on command line tools in Chris Brew
and Marc Moens’ Data-Intensive Linguistics course
notes or Ken Church’s Unix for Poets tutorial.4
</bodyText>
<subsectionHeader confidence="0.994448">
3.2 Internal tools
</subsectionHeader>
<bodyText confidence="0.999804384615384">
Grammar engineering with OpenCCG. The
grammar engineering component of Computational
Syntax in Spring 2006 used OpenCCG,5 a catego-
rial grammar parsing system that Baldridge created
with Gann Bierner and Michael White. The prob-
lem with using OpenCCG is that its native grammar
specification format is XML designed for machines,
not people. Students in the course persevered and
managed to complete the assignments; nonetheless,
it became glaringly apparent that the non-intuitive
XML specification language was a major stumbling
block that held students back from more interesting
aspects of grammar engineering.
One student, Ben Wing, was unhappy enough us-
ing the XML format that he devised a new specifica-
tion language, DotCCG, and a converter to generate
the XML from it. DotCCG is not only simpler—it
also uses several interesting devices, including sup-
port for regular expressions and string expansions.
This expressivity makes it possible to encode a sig-
nificant amount of morphology in the specification
language and reduce redundancy in the grammar.
The DotCCG specification language and con-
verter became the core of a project funded by UT
Austin’s Liberal Arts Instructional Technology Ser-
vices to create a web and graphical user interface,
VisCCG, and develop instructional materials for
grammar engineering. The goal was to provide suit-
able interfaces and a graduated series of activities
and assignments that would allow students to learn
very basic grammar engineering and then grow into
the full capabilities of an established parsing system.
A web interface provided an initial stage that al-
lowed students in the undergraduate Introduction to
Computational Linguistics course (Fall 2006) to test
their grammars in a grammar writing assignment.
This simple interface allows students to first write
out a grammar on paper and then implement it and
test it on a set of sentences. Students grasped the
</bodyText>
<footnote confidence="0.999454">
4http://research.microsoft.com/users/church/
wwwfiles/tutorials/unix for poets.ps
5http://openccg.sf.net
</footnote>
<bodyText confidence="0.999809833333333">
concepts and seemed to enjoy seeing the grammar’s
coverage improve as they added more lexical entries
or added features to constrain them appropriately. A
major advantage of this interface, of course, is that
it was not necessary for students to come to the lab
or install any software on their own computers.
The second major development was VisCCG,
a graphical user interface for writing full-fledged
OpenCCG grammars. It has special support for
DotCCG, including error checking, and it displays
grammatical information at various levels of granu-
larity while still allowing direct source text editing
of the grammar.
The third component was several online
tutorials—written on as publicly available wiki
pages—for writing grammars with VisCCG and
DotCCG. A pleasant discovery was the tremendous
utility of the wiki-based tutorials. It was very easy
to quickly create tutorial drafts and improve them
with the graduate assistant employed for creating
instructional materials for the project, regardless of
where we were. More importantly, it was possible
to fix bugs or add clarifications while students were
following the tutorials in the lab. Furthermore,
students could add their own tips for other students
and share their grammars on the wiki.
These tools and tutorials were used for two grad-
uate courses in Spring 2007, Categorial Grammar
and Computational Linguistics I. Students caught on
quickly to using VisCCG and DotCCG, which was a
huge contrast over the previous year. Students were
able to create and test grammars of reasonable com-
plexity very quickly and with much greater ease. We
are continuing to develop and improve these materi-
als for current courses.
The resources we created have been not only ef-
fective for classroom instruction: they are also be-
ing used by researchers that use OpenCCG for pars-
ing and realization. The work we did produced sev-
eral innovations for grammar engineering that we
reported at the workshop on Grammar Engineering
Across the Frameworks (Baldridge et al., 2007).
</bodyText>
<subsectionHeader confidence="0.9976395">
3.3 A lexical semantics workbench:
Shalmaneser
</subsectionHeader>
<bodyText confidence="0.997574666666667">
In the lexical semantics sections of our classes, word
sense and predicate-argument structure are core top-
ics. Until now, we had only discussed word sense
</bodyText>
<page confidence="0.998561">
6
</page>
<bodyText confidence="0.999944642857143">
disambiguation and semantic role labeling theoret-
ically. However, it would be preferable to give the
students hands-on experience with the tasks, as well
as a sense of what does and does not work, and why
the tasks are difficult. So, we are now extending
Shalmaneser (Erk and Pado, 2006), a SHALlow se-
MANtic parSER that does word sense and semantic
role assignment using FrameNet frames and roles,
to be a teaching tool. Shalmaneser already offers a
graphical representation of the assigned predicate-
argument structure. Supported by an instructional
technology grant from UT Austin, we are extend-
ing the system with two graphical interfaces that
will allow students to experiment with a variety of
features, settings and machine learning paradigms.
Courses that only do a short segment on lexical se-
mantic analysis will be able to use the web inter-
face, which does not offer the full functionality of
Shalmaneser (in particular, no training of new clas-
sifiers), but does not require any setup. In addition,
there will be a stand-alone graphical user interface
for a more in-depth treatment of lexical semantic
analysis. We plan to have the new platform ready
for use for Fall 2008.
Besides a GUI and tutorial documents, there is
one more component to the new Shalmaneser sys-
tem, an adaptation of the idea of grammar engi-
neering workbenches to predicate-argument struc-
ture. Grammar engineering workbenches allow stu-
dents to specify grammars declaratively. For seman-
tic role labeling, the only possibility that has been
available so far for experimenting with new features
is to program. But, since semantic role labeling fea-
tures typically refer to parts of the syntactic struc-
ture, it should be possible to describe them declar-
atively using a tree description language. We are
now developing such a language and workbench as
part of Shalmaneser. We aim for a system that will
be usable not only in the classroom but also by re-
searchers who develop semantic role labeling sys-
tems or who need an automatic predicate-argument
structure analysis system.
</bodyText>
<sectionHeader confidence="0.990725" genericHeader="method">
4 University-wide program
</sectionHeader>
<bodyText confidence="0.999962166666667">
The University of Texas at Austin has a long tra-
dition in the field of computational linguistics that
goes back to 1961, when a major machine transla-
tion project was undertaken at the university’s Lin-
guistics Research Center under the direction of Win-
fred Lehman. Lauri Karttunen, Stan Peters, and
Bob Wall were all on the faculty of the linguistics
department in the late 1960’s, and Bob Simmons
was in the computer science department during this
time. Overall activity was quite strong throughout
the 1970’s and 1980’s. After Bob Wall retired in the
mid-1990’s, there was virtually no computational
work in the linguistics department, but Ray Mooney
and his students in computer science remained very
active during this period.6
The linguistics department decided in 2000 to
revive computational linguistics in the department,
and consequently hired Jonas Kuhn in 2002. His
efforts, along with those of Hans Boas in the Ger-
man department, succeeded in producing a com-
putational linguistics curriculum, funding research,
(re)establishing links with computer science, and at-
tracting an enthusiastic group of linguistics students.
Nonetheless, there is still no formal interdepart-
mental program in computational linguistics at UT
Austin. Altogether, we have a sizable group of
faculty and students working on topics related to
computational linguistics, including many other lin-
guists, computer scientists, psychologists and oth-
ers who have interests directly related to issues in
computational linguistics, including our strong arti-
ficial intelligence group. Despite this, it was easy
to overlook if one was considering only an individ-
ual department. We thus set up a site7 to improve
the visibility of our CL-related faculty and research
across the university. There are plans to create an ac-
tual program spanning the various departments and
drawing on the strengths of UT Austin’s language
departments. For now, the web site is a low-cost and
low-effort but effective starting point.
As part of these efforts, we are working to in-
tegrate our course offerings, including the cross-
listing of the undergraduate NLP course. Our stu-
dents regularly take Machine Learning and other
courses from the computer science department. Ray
Mooney will teach a graduate NLP course in Fall
2008 that will offer students a different perspective
and we hope that it will drum up further interest
</bodyText>
<footnote confidence="0.999754">
6For a detailed account, see: http://comp.ling.
utexas.edu/wiki/doku.php/austin compling history
7http://comp.ling.utexas.edu
</footnote>
<page confidence="0.999283">
7
</page>
<bodyText confidence="0.999910928571428">
in CL in the computer science department and thus
lead to further interest in our other courses.
As part of the web page, we also created a wiki.8
We have already mentioned its use in teaching and
tutorials. Other uses include lab information, a
repository of programming tips and tricks, list of im-
portant NLP papers, collaboration areas for projects,
and general information about computational lin-
guistics. We see the wiki as an important reposi-
tory of knowledge that will accumulate over time
and continue to benefit us and our students as it
grows. It simplifies our job since we answer many
student questions on the wiki: when questions get
asked again, we just point to the relevant page.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999637066666667">
Our experience as computational linguists teaching
and doing research in a linguistics department at a
large university has given us ample opportunity to
learn a number of general lessons for teaching com-
putational linguistics to a diverse audience.
The main lesson is to stratify courses according
to the backgrounds different populations of students
have with respect to programming and formal think-
ing. A key component of this is to make expec-
tations about the level of technical difficulty of a
course clear before the start of classes and restate
this information on the first day of class. This is im-
portant not only to ensure students do not take too
challenging a course: other reasons include (a) re-
assuring programming-wary students that a course
will introduce them to programming gently, (b) en-
suring that programming-savvy students know when
there will be little programming involved or formal
problem solving they are likely to have already ac-
quired, and (c) providing awareness of other courses
students may be more interested in right away or af-
ter they have completed the current course.
Another key lesson we have learned is that the for-
mal categorization of a course within a university
course schedule and departmental degree program
are massive factors in enrollment, both at the under-
graduate and graduate level. Computational linguis-
tics is rarely a required course, but when taught in a
liberal arts college it can easily satisify undergradu-
ate math and/or science requirements (as Language
</bodyText>
<footnote confidence="0.723198">
8http://comp.ling.utexas.edu/wiki/doku.php
</footnote>
<bodyText confidence="0.999628829787234">
and Computers does at OSU and UT Austin, respec-
tively). However, for highly technical courses taught
in a liberal arts college (e.g., Natural Language Pro-
cessing) it is useful to cross-list them with computer
science or related areas in order to ensure that the ap-
propriate student population is reached. At the grad-
uate level, it is also important to provide structure
and context for each course. We are now coordinat-
ing with Ray Mooney to define a core set of com-
putational linguistics courses that we offer regularly
and can suggest to incoming graduate students. This
will not be part of a formal degree program per se,
but will provide necessary structure for students to
progress through either the linguistics or computer
science program in a timely fashion while taking
courses relevant to their research interests.
One of the big questions that hovers over nearly
all discussions of teaching computational linguistics
is: how do we teach the computer science to the
linguistics students and teach the linguistics to the
computer science students? Or, rather, the question
is how to teach both groups computational linguis-
tics. This involves getting students to understand the
importance of a strong formal basis, ranging from
understanding what a tight syntax-semantics inter-
face really means to how machine learning mod-
els relate to questions of actual language acquisi-
tion to how corpus data can or should inform lin-
guistic analyses. It also involves revealing the cre-
ativity and complexity of language to students who
think it should be easy to deal with. And it involves
showing linguistics students how familiar concepts
from linguistics translate to technical questions (for
example, addressing agreement using feature log-
ics), and showing computer science students how
familiar friends like finite-state automata and dy-
namic programming are crucial for analyzing nat-
ural language phenomena and managing complexity
and ambiguity. The key is to target the courses so
that the background needs of each type of student
can be met appropriately without needing to skimp
on linguistic or computational complexity for those
who are ready to learn about it.
Acknowledgments. We would like to thank Hans
Boas, Bob Harms, Ray Mooney, Elias Ponvert, Tony
Woodbury, and the anonymous reviewers for their
help and feedback.
</bodyText>
<page confidence="0.996992">
8
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894172413793">
Jason Baldridge, Sudipta Chatterjee, Alexis Palmer, and
Ben Wing. 2007. DotCCG and VisCCG: Wiki and
programming paradigms for improved grammar engi-
neering with OpenCCG. In Proceeings of the GEAF
2007 Workshop.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI Publications.
Steven Bird, Ewan Klein, Edward Loper, and Jason
Baldridge. 2008. Multidisciplinary instruction with
the Natural Language Toolkit. In Proceedings of the
Third Workshop on Issues in Teaching Computational
Linguistics. Association for Computational Linguis-
tics.
C. Brew, M. Dickinson, and W. D. Meurers. 2005. Lan-
guage and computers: Creating an introduction for a
general undergraduate audience. In Proceedings of the
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing And Compu-
tational Linguistics, Ann Arbor, Michigan.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Dragomir
Radev and Chris Brew, editors, Proceedings of the
ACL Workshop on Effective Tools and Methodologies
for Teaching NLP and CL, pages 10–18.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser – a
flexible toolbox for semantic role assignment. In Pro-
ceedings of LREC-2006, Genoa, Italy.
D. Jurafsky and J. H. Martin. 2000. Speech and language
processing: An Introduction to Natural Language
Processing, Computational Linguistics, and Speech
Recognition. Prentice-Hall, Upper Saddle River, NJ.
Edward Loper and Steven Bird. 2002. NLTK: The natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics, pages 62–69. Somerset, NJ: Association
for Computational Linguistics.
Taesun Moon and Jason Baldridge. 2007. Part-of-speech
tagging for middle English through alignment and pro-
jection of parallel diachronic texts. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
390–399.
Taesun Moon and Katrin Erk. 2008. Minimally super-
vised lemmatization scheme induction through bilin-
gual parallel corpora. In Proceedings of the Interna-
tional Conference on Global Interoperability for Lan-
guage Resources.
Elias Ponvert. 2007. Inducing Combinatory Categorial
Grammars with genetic algorithms. In Proceedings
of the ACL 2007 Student Research Workshop, pages
7–12, Prague, Czech Republic, June. Association for
Computational Linguistics.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolkit. Language Documentation and
Conservation, 1:44–57.
</reference>
<page confidence="0.99711">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858551">
<title confidence="0.991679">Teaching computational linguistics to a large, diverse student courses, tools, and interdepartmental interaction</title>
<author confidence="0.998713">Jason Baldridge</author>
<author confidence="0.998713">Katrin</author>
<affiliation confidence="0.955393">Department of The University of Texas at</affiliation>
<abstract confidence="0.9950308">We describe course adaptation and development for teaching computational linguistics for the diverse body of undergraduate and graduate students the Department of Linguistics at the University of Texas at Austin. We also discuss classroom tools and teaching aids we have used and created, and we mention our efforts to develop a campus-wide computational linguistics program.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Sudipta Chatterjee</author>
<author>Alexis Palmer</author>
<author>Ben Wing</author>
</authors>
<title>DotCCG and VisCCG: Wiki and programming paradigms for improved grammar engineering with OpenCCG.</title>
<date>2007</date>
<booktitle>In Proceeings of the GEAF</booktitle>
<note>Workshop.</note>
<contexts>
<context position="25426" citStr="Baldridge et al., 2007" startWordPosition="3997" endWordPosition="4000">ught on quickly to using VisCCG and DotCCG, which was a huge contrast over the previous year. Students were able to create and test grammars of reasonable complexity very quickly and with much greater ease. We are continuing to develop and improve these materials for current courses. The resources we created have been not only effective for classroom instruction: they are also being used by researchers that use OpenCCG for parsing and realization. The work we did produced several innovations for grammar engineering that we reported at the workshop on Grammar Engineering Across the Frameworks (Baldridge et al., 2007). 3.3 A lexical semantics workbench: Shalmaneser In the lexical semantics sections of our classes, word sense and predicate-argument structure are core topics. Until now, we had only discussed word sense 6 disambiguation and semantic role labeling theoretically. However, it would be preferable to give the students hands-on experience with the tasks, as well as a sense of what does and does not work, and why the tasks are difficult. So, we are now extending Shalmaneser (Erk and Pado, 2006), a SHALlow seMANtic parSER that does word sense and semantic role assignment using FrameNet frames and rol</context>
</contexts>
<marker>Baldridge, Chatterjee, Palmer, Wing, 2007</marker>
<rawString>Jason Baldridge, Sudipta Chatterjee, Alexis Palmer, and Ben Wing. 2007. DotCCG and VisCCG: Wiki and programming paradigms for improved grammar engineering with OpenCCG. In Proceeings of the GEAF 2007 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth R Beesley</author>
<author>Lauri Karttunen</author>
</authors>
<title>Finite State Morphology.</title>
<date>2003</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="18857" citStr="Beesley and Karttunen, 2003" startWordPosition="2965" endWordPosition="2968">ansducers. FSTs have unique qualities for courses about computational linguistics that are taught in linguistics department. They are an elegant extension of finite-state automata and are simple enough that their core aspects and capabilities can be expressed in just a few lectures. Computer science students immediately get excited about being able to relate string languages rather than just recognizing them. More importantly, they can be used to elegantly solve problems in phonology and morphology that linguistics students can readily appreciate. We use the Xerox Finite State Toolkit (XFST) (Beesley and Karttunen, 2003) for in-class demonstrations and homeworks for FSTs. A great aspect of using XFST is that it can be used to show that different representations (e.g., two-level rules versus cascaded rules) can be used to define the same regular relation. This exercise injects some healthy skepticism into linguistics students who may have to deal with formalism wars in their own linguistic subfield. Also, XFST allows one to use lenient composition to encode Optimality Theory constraints and in so doing show interesting and direct contrasts and comparisons between paper-and-pencil linguistics and rigorous compu</context>
</contexts>
<marker>Beesley, Karttunen, 2003</marker>
<rawString>Kenneth R. Beesley and Lauri Karttunen. 2003. Finite State Morphology. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
<author>Jason Baldridge</author>
</authors>
<title>Multidisciplinary instruction with the Natural Language Toolkit.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17338" citStr="Bird et al., 2008" startWordPosition="2729" endWordPosition="2732">a range of external tools and have adapted tools from our own research for various aspects of our courses. In this section, we describe our experience using these as part of our courses. We have used Python as the common language in our courses. We are pleased with it: it is straightforward for beginning programmers to learn, its interactive prompt facilitates in-class instruction, it is text-processing friendly, and it is useful for gluing together other (e.g., Java and C++) applications. 3.1 External tools and resources NLTK. We use the Natural Language Toolkit (NLTK) (Loper and Bird, 2002; Bird et al., 2008) in both undergraduate and graduate courses for in-class demos, tutorials, and homework assignments. We use the toolkit and tutorials for several course components, including introductory Python programming, text processing, rule-based part-of-speech tagging and chunking, and grammars and parsing. NLTK is ideal for both novice and advanced programmers. The tutorials and extensive documentation provide novices with plenty of support outside of the classroom, and the toolkit is powerful enough to give plenty of room for advanced students to play. The demos are also very useful in classes and ser</context>
</contexts>
<marker>Bird, Klein, Loper, Baldridge, 2008</marker>
<rawString>Steven Bird, Ewan Klein, Edward Loper, and Jason Baldridge. 2008. Multidisciplinary instruction with the Natural Language Toolkit. In Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brew</author>
<author>M Dickinson</author>
<author>W D Meurers</author>
</authors>
<title>Language and computers: Creating an introduction for a general undergraduate audience.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing And Computational Linguistics,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="12076" citStr="Brew et al., 2005" startWordPosition="1882" endWordPosition="1885"> was great fun, and it was an interesting new challenge after having to sit through very basic Python lectures. On the other hand, the use of NLTK to drive learning about Python and NLP tasks (like building POStaggers) significantly eased the burden for new programmers. Many of them were highly satisfied that they could build interesting programs and experiment with their behavior so easily. Language and Computers. We had fortunately already planned the first replacement course: Language and Computers, based on the course designed at the Department of Linguistics at the Ohio State University (Brew et al., 2005). This course intro3 duces computational linguistics to a general audience and is ideal for students who want exposure to computational methods without having to learn to program. We designed and taught it jointly, and added several new aspects to the course. Whereas OSU’s course fulfills a Mathematical and Logical Analysis requirement, our course fulfills a Science requirement for liberal arts majors. These requirements were met by course content that requires understanding and thinking about formal methods. The topics we added to our course were question answering, cryptography,2 and world k</context>
</contexts>
<marker>Brew, Dickinson, Meurers, 2005</marker>
<rawString>C. Brew, M. Dickinson, and W. D. Meurers. 2005. Language and computers: Creating an introduction for a general undergraduate audience. In Proceedings of the Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing And Computational Linguistics, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>An interactive spreadsheet for teaching the forward-backward algorithm.</title>
<date>2002</date>
<booktitle>In Dragomir Radev</booktitle>
<pages>10--18</pages>
<editor>and Chris Brew, editors,</editor>
<contexts>
<context position="19769" citStr="Eisner, 2002" startWordPosition="3109" endWordPosition="3110">cs students who may have to deal with formalism wars in their own linguistic subfield. Also, XFST allows one to use lenient composition to encode Optimality Theory constraints and in so doing show interesting and direct contrasts and comparisons between paper-and-pencil linguistics and rigorous computational implementations. As with other implementation-oriented activities in our classes, we created a wiki page for XFST tutorials.3 These were adapted and expanded from Xerox PARC materials and Mark Gawron’s examples. Eisner’s HMM Materials. Simply put: the spreadsheet designed by Jason Eisner (Eisner, 2002) for teaching hidden Markov models is fantastic. We used that plus Eisner’s HMM homework assignment for Computational Linguistics II in Fall 2007. The spreadsheet is great for interactive classroom exploration of HMMs—students were very engaged. The homework allows students to implement an HMM from scratch, giving enough detail to alleviate much of the needless frustration that could occur with this task while ensuring that students need to put in significant effort and understand the concepts in order to make it work. It also helps that the new edition of Jurafsky and Martin’s textbook discus</context>
</contexts>
<marker>Eisner, 2002</marker>
<rawString>Jason Eisner. 2002. An interactive spreadsheet for teaching the forward-backward algorithm. In Dragomir Radev and Chris Brew, editors, Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Shalmaneser – a flexible toolbox for semantic role assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="25919" citStr="Erk and Pado, 2006" startWordPosition="4078" endWordPosition="4081">for grammar engineering that we reported at the workshop on Grammar Engineering Across the Frameworks (Baldridge et al., 2007). 3.3 A lexical semantics workbench: Shalmaneser In the lexical semantics sections of our classes, word sense and predicate-argument structure are core topics. Until now, we had only discussed word sense 6 disambiguation and semantic role labeling theoretically. However, it would be preferable to give the students hands-on experience with the tasks, as well as a sense of what does and does not work, and why the tasks are difficult. So, we are now extending Shalmaneser (Erk and Pado, 2006), a SHALlow seMANtic parSER that does word sense and semantic role assignment using FrameNet frames and roles, to be a teaching tool. Shalmaneser already offers a graphical representation of the assigned predicateargument structure. Supported by an instructional technology grant from UT Austin, we are extending the system with two graphical interfaces that will allow students to experiment with a variety of features, settings and machine learning paradigms. Courses that only do a short segment on lexical semantic analysis will be able to use the web interface, which does not offer the full fun</context>
</contexts>
<marker>Erk, Pado, 2006</marker>
<rawString>Katrin Erk and Sebastian Pado. 2006. Shalmaneser – a flexible toolbox for semantic role assignment. In Proceedings of LREC-2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J H Martin</author>
</authors>
<title>Speech and language processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall, Upper Saddle River,</title>
<date>2000</date>
<location>NJ.</location>
<contexts>
<context position="11209" citStr="Jurafsky and Martin, 2000" startWordPosition="1738" endWordPosition="1741">ational linguistics, but that they would not actually have to do computational linguistics. Many stayed with it, but there were still others who could have gone much further if it had not been necessary to slow down to cover basic material like for loops. Note that several linguistics majors were among the compationally savvy students. In fairness to the students who struggled, it was certainly ill-advised to ask students with no previous background to learn Python and XFST in a single semester. One of the key points of confusion was regular expression syntax. The syntax used in the textbook (Jurafsky and Martin, 2000) transfers easily to regular expressions in Python, but is radically different from that of XFST. For students who had never coded anything in their life, this proved extremely frustrating. On the other hand, for computationally savvy students, XFST was great fun, and it was an interesting new challenge after having to sit through very basic Python lectures. On the other hand, the use of NLTK to drive learning about Python and NLP tasks (like building POStaggers) significantly eased the burden for new programmers. Many of them were highly satisfied that they could build interesting programs an</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>D. Jurafsky and J. H. Martin. 2000. Speech and language processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,</booktitle>
<pages>62--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Somerset, NJ:</location>
<contexts>
<context position="7845" citStr="Loper and Bird, 2002" startWordPosition="1217" endWordPosition="1220">e with the R statistics package. It was used for statistical analyses: students loved it because they could produce meaningful results immediately and visualize them. The course includes only a very short two-session introduction to working with R. We were worried that this would overtax students because R is its own 2 programming language. But interestingly they had no problems with learning this second programming language (after Python). This is particularly striking as most of the students had no programming experience prior to the class. We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. But as it, too, offers visualization and rapid access to meaningful results, we intend to use it in the future. In particular, the NLTK allows very easy access to Toolbox data (Robinson et al., 2007), which we feel will greatly improve the utility and appeal of the course for the significant number of documentary linguistics students in the department. Seminars. We also offer several seminars in our areas of interest. These include Categorial Grammar, Computational Syntax, and Lexical Acquisition. These courses have attracted “noncomputational” linguistics st</context>
<context position="17318" citStr="Loper and Bird, 2002" startWordPosition="2725" endWordPosition="2728">utorials We have used a range of external tools and have adapted tools from our own research for various aspects of our courses. In this section, we describe our experience using these as part of our courses. We have used Python as the common language in our courses. We are pleased with it: it is straightforward for beginning programmers to learn, its interactive prompt facilitates in-class instruction, it is text-processing friendly, and it is useful for gluing together other (e.g., Java and C++) applications. 3.1 External tools and resources NLTK. We use the Natural Language Toolkit (NLTK) (Loper and Bird, 2002; Bird et al., 2008) in both undergraduate and graduate courses for in-class demos, tutorials, and homework assignments. We use the toolkit and tutorials for several course components, including introductory Python programming, text processing, rule-based part-of-speech tagging and chunking, and grammars and parsing. NLTK is ideal for both novice and advanced programmers. The tutorials and extensive documentation provide novices with plenty of support outside of the classroom, and the toolkit is powerful enough to give plenty of room for advanced students to play. The demos are also very usefu</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The natural language toolkit. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62–69. Somerset, NJ: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taesun Moon</author>
<author>Jason Baldridge</author>
</authors>
<title>Part-of-speech tagging for middle English through alignment and projection of parallel diachronic texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>390--399</pages>
<contexts>
<context position="5989" citStr="Moon and Baldridge, 2007" startWordPosition="935" endWordPosition="938">e. Also, by having students do presentations on their work before they hand in the final report, they can incorporate feedback from other students. A useful strategy we have found for scoring these projects is to use standard conference reviews in Computational Linguistics II. The final projects have led to several workshops and conference publications for the students so far, as well as honors theses. The topics have been quite varied (in line with our varied student body), including lexicon induction using genetic algorithms (Ponvert, 2007), alignment-and-transfer for bootstrapping taggers (Moon and Baldridge, 2007), lemmatization using parallel corpora (Moon and Erk, 2008), graphical visualization of articles using syntactic dependencies (Jeff Rego, CS honors thesis), and feature extraction for semantic role labeling (Trevor Fountain, CS honors thesis). Working with corpora. Computational linguistics skills and techniques are tremendously valuable for linguists using corpora. Ideally, a linguist should be able to extract the relevant data, count occurrences of phenomena, and do statistical analyses. The intersection of these skills and needs is the core of this course, which covers corpus formats (XML, </context>
</contexts>
<marker>Moon, Baldridge, 2007</marker>
<rawString>Taesun Moon and Jason Baldridge. 2007. Part-of-speech tagging for middle English through alignment and projection of parallel diachronic texts. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 390–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taesun Moon</author>
<author>Katrin Erk</author>
</authors>
<title>Minimally supervised lemmatization scheme induction through bilingual parallel corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Global Interoperability for Language Resources.</booktitle>
<contexts>
<context position="6048" citStr="Moon and Erk, 2008" startWordPosition="944" endWordPosition="947"> they hand in the final report, they can incorporate feedback from other students. A useful strategy we have found for scoring these projects is to use standard conference reviews in Computational Linguistics II. The final projects have led to several workshops and conference publications for the students so far, as well as honors theses. The topics have been quite varied (in line with our varied student body), including lexicon induction using genetic algorithms (Ponvert, 2007), alignment-and-transfer for bootstrapping taggers (Moon and Baldridge, 2007), lemmatization using parallel corpora (Moon and Erk, 2008), graphical visualization of articles using syntactic dependencies (Jeff Rego, CS honors thesis), and feature extraction for semantic role labeling (Trevor Fountain, CS honors thesis). Working with corpora. Computational linguistics skills and techniques are tremendously valuable for linguists using corpora. Ideally, a linguist should be able to extract the relevant data, count occurrences of phenomena, and do statistical analyses. The intersection of these skills and needs is the core of this course, which covers corpus formats (XML, bracket formats for syntax, “word/POS” formats for part-of-</context>
</contexts>
<marker>Moon, Erk, 2008</marker>
<rawString>Taesun Moon and Katrin Erk. 2008. Minimally supervised lemmatization scheme induction through bilingual parallel corpora. In Proceedings of the International Conference on Global Interoperability for Language Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Ponvert</author>
</authors>
<title>Inducing Combinatory Categorial Grammars with genetic algorithms.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Student Research Workshop,</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5912" citStr="Ponvert, 2007" startWordPosition="928" endWordPosition="929">on for their project, rather than rushing everything in last minute. Also, by having students do presentations on their work before they hand in the final report, they can incorporate feedback from other students. A useful strategy we have found for scoring these projects is to use standard conference reviews in Computational Linguistics II. The final projects have led to several workshops and conference publications for the students so far, as well as honors theses. The topics have been quite varied (in line with our varied student body), including lexicon induction using genetic algorithms (Ponvert, 2007), alignment-and-transfer for bootstrapping taggers (Moon and Baldridge, 2007), lemmatization using parallel corpora (Moon and Erk, 2008), graphical visualization of articles using syntactic dependencies (Jeff Rego, CS honors thesis), and feature extraction for semantic role labeling (Trevor Fountain, CS honors thesis). Working with corpora. Computational linguistics skills and techniques are tremendously valuable for linguists using corpora. Ideally, a linguist should be able to extract the relevant data, count occurrences of phenomena, and do statistical analyses. The intersection of these sk</context>
</contexts>
<marker>Ponvert, 2007</marker>
<rawString>Elias Ponvert. 2007. Inducing Combinatory Categorial Grammars with genetic algorithms. In Proceedings of the ACL 2007 Student Research Workshop, pages 7–12, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Robinson</author>
<author>Greg Aumann</author>
<author>Steven Bird</author>
</authors>
<title>Managing fieldwork data with Toolbox and the Natural Language Toolkit. Language Documentation and Conservation,</title>
<date>2007</date>
<pages>1--44</pages>
<contexts>
<context position="8079" citStr="Robinson et al., 2007" startWordPosition="1258" endWordPosition="1261">working with R. We were worried that this would overtax students because R is its own 2 programming language. But interestingly they had no problems with learning this second programming language (after Python). This is particularly striking as most of the students had no programming experience prior to the class. We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. But as it, too, offers visualization and rapid access to meaningful results, we intend to use it in the future. In particular, the NLTK allows very easy access to Toolbox data (Robinson et al., 2007), which we feel will greatly improve the utility and appeal of the course for the significant number of documentary linguistics students in the department. Seminars. We also offer several seminars in our areas of interest. These include Categorial Grammar, Computational Syntax, and Lexical Acquisition. These courses have attracted “noncomputational” linguistics students with related interests, and have served as the launching point for several qualifying papers and masters theses. It is important to offer these courses so that these students gain a view into computational linguistics from the </context>
</contexts>
<marker>Robinson, Aumann, Bird, 2007</marker>
<rawString>Stuart Robinson, Greg Aumann, and Steven Bird. 2007. Managing fieldwork data with Toolbox and the Natural Language Toolkit. Language Documentation and Conservation, 1:44–57.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>