<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002199">
<title confidence="0.962799">
Underspecifying and Predicting Voice for Surface Realisation Ranking
</title>
<author confidence="0.900791">
Sina ZarrieB, Aoife Cahill and Jonas Kuhn
</author>
<affiliation confidence="0.705762">
Institut f¨ur maschinelle Sprachverarbeitung
Universit¨at Stuttgart, Germany
</affiliation>
<email confidence="0.989105">
{sina.zarriess,aoife.cahill,jonas.kuhn}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.997254" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998673473684211">
This paper addresses a data-driven surface
realisation model based on a large-scale re-
versible grammar of German. We investigate
the relationship between the surface realisa-
tion performance and the character of the in-
put to generation, i.e. its degree of underspec-
ification. We extend a syntactic surface reali-
sation system, which can be trained to choose
among word order variants, such that the can-
didate set includes active and passive variants.
This allows us to study the interaction of voice
and word order alternations in realistic Ger-
man corpus data. We show that with an ap-
propriately underspecified input, a linguisti-
cally informed realisation model trained to re-
generate strings from the underlying semantic
representation achieves 91.5% accuracy (over
a baseline of 82.5%) in the prediction of the
original voice.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999717428571429">
This paper1 presents work on modelling the usage
of voice and word order alternations in a free word
order language. Given a set of meaning-equivalent
candidate sentences, such as in the simplified En-
glish Example (1), our model makes predictions
about which candidate sentence is most appropriate
or natural given the context.
</bodyText>
<listItem confidence="0.9369044">
(1) Context: The Parliament started the debate about the state
budget in April.
a. It wasn’t until June that the Parliament approved it.
b. It wasn’t until June that it was approved by the Parliament.
c. It wasn’t until June that it was approved.
</listItem>
<bodyText confidence="0.9926985">
We address the problem of predicting the usage of
linguistic alternations in the framework of a surface
</bodyText>
<footnote confidence="0.6954115">
1This work has been supported by the Deutsche Forschungs-
gemeinschaft (DFG; German Research Foundation) in SFB 732
Incremental specification in context, project D2 (PIs: Jonas
Kuhn and Christian Rohrer).
</footnote>
<bodyText confidence="0.999888">
realisation ranking system. Such ranking systems
are practically relevant for the real-world applica-
tion of grammar-based generators that usually gen-
erate several grammatical surface sentences from a
given abstract input, e.g. (Velldal and Oepen, 2006).
Moreover, this framework allows for detailed exper-
imental studies of the interaction of specific linguis-
tic features. Thus it has been demonstrated that for
free word order languages like German, word or-
der prediction quality can be improved with care-
fully designed, linguistically informed models cap-
turing information-structural strategies (Filippova
and Strube, 2007; Cahill and Riester, 2009).
This paper is situated in the same framework, us-
ing rich linguistic representations over corpus data
for machine learning of realisation ranking. How-
ever, we go beyond the task of finding the correct or-
dering for an almost fixed set of word forms. Quite
obviously, word order is only one of the means at
a speaker’s disposal for expressing some content in
a contextually appropriate form; we add systematic
alternations like the voice alternation (active vs. pas-
sive) to the picture. As an alternative way of pro-
moting or demoting the prominence of a syntactic
argument, its interaction with word ordering strate-
gies in real corpus data is of high theoretical interest
(Aissen, 1999; Aissen, 2003; Bresnan et al., 2001).
Our main goals are (i) to establish a corpus-based
surface realisation framework for empirically inves-
tigating interactions of voice and word order in Ger-
man, (ii) to design an input representation for gen-
eration capturing voice alternations in a variety of
contexts, (iii) to better understand the relationship
between the performance of a generation ranking
model and the type of realisation candidates avail-
able in its input. In working towards these goals,
this paper addresses the question of evaluation. We
conduct a pilot human evaluation on the voice al-
</bodyText>
<page confidence="0.949928">
1007
</page>
<note confidence="0.979274">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1007–1017,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.99991612244898">
ternation data and relate our findings to our results
established in the automatic ranking experiments.
Addressing interactions among a range of gram-
matical and discourse phenomena on realistic corpus
data turns out to be a major methodological chal-
lenge for data-driven surface realisation. The set of
candidate realisations available for ranking will in-
fluence the findings, and here, existing surface re-
alisers vary considerably. Belz et al. (2010) point
out the differences across approaches in the type of
syntactic and semantic information present and ab-
sent in the input representation; and it is the type of
underspecification that determines the number (and
character) of available candidate realisations and,
hence, the complexity of the realisation task.
We study the effect of varying degrees of under-
specification explicitly, extending a syntactic gen-
eration system by a semantic component capturing
voice alternations. In regeneration studies involving
underspecified underlying representations, corpus-
oriented work reveals an additional methodological
challenge. When using standard semantic represen-
tations, as common in broad-coverage work in se-
mantic parsing (i.e., from the point of view of analy-
sis), alternative variants for sentence realisation will
often receive slightly different representations: In
the context of (1), the continuation (1-c) is presum-
ably more natural than (1-b), but with a standard
sentence-bounded semantic analysis, only (1-a) and
(1-b) would receive equivalent representations.
Rather than waiting for the availability of robust
and reliable techniques for detecting the reference of
implicit arguments in analysis (or for contextually
aware reasoning components), we adopt a relatively
simple heuristic approach (see Section 3.1) that ap-
proximates the desired equivalences by augmented
representations for examples like (1-c). This way
we can overcome an extremely skewed distribution
in the naturally occurring meaning-equivalent active
vs. passive sentences, a factor which we believe jus-
tifies taking the risk of occasional overgeneration.
The paper is structured as follows: Section 2 situ-
ates our methodology with respect to other work on
surface realisation and briefly summarises the rele-
vant theoretical linguistic background. In Section 3,
we present our generation architecture and the de-
sign of the input representation. Section 4 describes
the setup for the experiments in Section 5. In Section
6, we present the results from the human evaluation.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999246">
2.1 Generation Background
</subsectionHeader>
<bodyText confidence="0.999992583333334">
The first widely known data-driven approach to
surface realisation, or tactical generation, (Langk-
ilde and Knight, 1998) used language-model n-
gram statistics on a word lattice of candidate re-
alisations to guide a ranker. Subsequent work ex-
plored ways of exploiting linguistically annotated
data for trainable generation models (Ratnaparkhi,
2000; Marciniak and Strube, 2005; Belz, 2005, a.o.).
Work on data-driven approaches has led to insights
into the importance of linguistic features for sen-
tence linearisation decisions (Ringger et al., 2004;
Filippova and Strube, 2009). The availability of dis-
criminative learning techniques for the ranking of
candidate analyses output by broad-coverage gram-
mars with rich linguistic representations, originally
in parsing (Riezler et al., 2000; Riezler et al., 2002),
has also led to a revival of interest in linguistically
sophisticated reversible grammars as the basis for
surface realisation (Velldal and Oepen, 2006; Cahill
et al., 2007). The grammar generates candidate
analyses for an underlying representation and the
ranker’s task is to predict the contextually appropri-
ate realisation.
The work that is most closely related to ours is
Velldal (2008). He uses an MRS representation
derived by an HPSG grammar that can be under-
specified for information status. In his case, the
underspecification is encoded in the grammar and
not directly controlled. In multilingually oriented
linearisation work, Bohnet et al. (2010) generate
from semantic corpus annotations included in the
CoNLL’09 shared task data. However, they note that
these annotations are not suitable for full generation
since they are often incomplete. Thus, it is not clear
to which degree these annotations are actually un-
derspecified for certain paraphrases.
</bodyText>
<subsectionHeader confidence="0.999312">
2.2 Linguistic Background
</subsectionHeader>
<bodyText confidence="0.9996226">
In competition-based linguistic theories (Optimal-
ity Theory and related frameworks), the use of
argument alternations is construed as an effect
of markedness hierarchies (Aissen, 1999; Aissen,
2003). Argument functions (subject, object, ...) on
</bodyText>
<page confidence="0.988906">
1008
</page>
<bodyText confidence="0.999860413793103">
the one hand and the various properties that argu-
ment phrases can bear (person, animacy, definite-
ness) on the other are organised in markedness hi-
erarchies. Wherever possible, there is a tendency to
align the hierarchies, i.e., use prominent functions to
realise prominently marked argument phrases. For
instance, Bresnan et al. (2001) find that there is a sta-
tistical tendency in English to passivise a verb if the
patient is higher on the person scale than the agent,
but an active is grammatically possible.
Bresnan et al. (2007) correlate the use of the En-
glish dative alternation to a number of features such
as givenness, pronominalisation, definiteness, con-
stituent length, animacy of the involved verb argu-
ments. These features are assumed to reflect the dis-
course acessibility of the arguments.
Interestingly, the properties that have been used
to model argument alternations in strict word or-
der languages like English have been identified as
factors that influence word order in free word or-
der languages like German, see Filippova and Strube
(2007) for a number of pointers. Cahill and Riester
(2009) implement a model for German word or-
der variation that approximates the information sta-
tus of constituents through morphological features
like definiteness, pronominalisation etc. We are not
aware of any corpus-based generation studies inves-
tigating how these properties relate to argument al-
ternations in free word order languages.
</bodyText>
<sectionHeader confidence="0.98769" genericHeader="method">
3 Generation Architecture
</sectionHeader>
<bodyText confidence="0.999301769230769">
Our data-driven methodology for investigating fac-
tors relevant to surface realisation uses a regen-
eration set-up2 with two main components: a) a
grammar-based component used to parse a corpus
sentence and map it to all its meaning-equivalent
surface realisations, b) a statistical ranking compo-
nent used to select the correct, i.e. contextually most
appropriate surface realisation. Two variants of this
set-up that we use are sketched in Figure 1.
We generally use a hand-crafted, broad-coverage
LFG for German (Rohrer and Forst, 2006) to parse
a corpus sentence into a f(unctional) structure3
and generate all surface realisations from a given
</bodyText>
<footnote confidence="0.99915825">
2Compare the bidirectional competition set-up in some
Optimality-Theoretic work, e.g., (Kuhn, 2003).
3The choice among alternative f-structures is done with a
discriminative model (Forst, 2007).
</footnote>
<figure confidence="0.965612625">
Snty
SVM Ranker
Sntb, Snta, Sntaz... Sntb,
LFG Grammar
FSa
Reverse Sem. Rules
LFG Grammar
Snti
</figure>
<figureCaption confidence="0.999991">
Figure 1: Generation pipelines
</figureCaption>
<bodyText confidence="0.999948333333333">
f-structure, following the generation approach of
Cahill et al. (2007). F-structures are attribute-
value matrices representing grammatical functions
and morphosyntactic features; their theoretical mo-
tivation lies in the abstraction over details of sur-
face realisation. The grammar is implemented in the
XLE framework (Crouch et al., 2006), which allows
for reversible use of the same declarative grammar
in the parsing and generation direction.
To obtain a more abstract underlying representa-
tion (in the pipeline on the right-hand side of Fig-
ure 1), the present work uses an additional seman-
tic construction component (Crouch and King, 2006;
Zarrieß, 2009) to map LFG f-structures to meaning
representations. For the reverse direction, the mean-
ing representations are mapped to f-structures which
can then be mapped to surface strings by the XLE
generator (Zarrieß and Kuhn, 2010).
For the final realisation ranking step in both
pipelines, we used SVMrank, a Support Vector
Machine-based learning tool (Joachims, 1996). The
ranking step is thus technically independent from the
LFG-based component. However, the grammar is
used to produce the training data, pairs of corpus
sentences and the possible alternations.
The two pipelines allow us to vary the degree to
which the generation input is underspecified. An f-
structure abstracts away from word order, i.e. the
candidate set will contain just word order alterna-
tions. In the semantic input, syntactic function and
voice are underspecified, so a larger set of surface
realisation candidates is generated. Figure 2 illus-
trates the two representation levels for an active and
</bodyText>
<figure confidence="0.995328181818182">
FSa
LFG grammar
Snti
Sntx
SVM Ranker
Snta, Snta2 ... Snta_
LFG grammar
SEM
Sem. Rules
FS1
FSb
</figure>
<page confidence="0.983949">
1009
</page>
<bodyText confidence="0.999110666666667">
a passive sentence. The subject of the passive and
the object of the active f-structure are mapped to the
same role (patient) in the meaning representation.
</bodyText>
<subsectionHeader confidence="0.999163">
3.1 Issues with “naive” underspecification
</subsectionHeader>
<bodyText confidence="0.99704525">
In order to create an underspecified voice represen-
tation that does indeed leave open the realisation op-
tions available to the speaker/writer, it is often not
sufficient to remove just the syntactic function in-
formation. For instance, the subject of the active
sentence (2) is an arbitrary reference pronoun man
“one” which cannot be used as an oblique agent in
a passive, sentence (2-b) is ungrammatical.
</bodyText>
<figure confidence="0.911939">
(2) a. Man
One
b. *Der Kanzler
The chancellor
</figure>
<bodyText confidence="0.997698142857143">
So, when combined with the grammar, the mean-
ing representation for (2) in Figure 2 contains im-
plicit information about the voice of the original cor-
pus sentence; the candidate set will not include any
passive realisations. However, a passive realisation
without the oblique agent in the by-phrase, as in Ex-
ample (3), is a very natural variant.
</bodyText>
<listItem confidence="0.734634">
(3) Der
</listItem>
<bodyText confidence="0.981071826086957">
The
The reverse situation arises frequently too: pas-
sive sentences where the agent role is not overtly
realised. Given the standard, “analysis-oriented”
meaning representation for Sentence (4) in Figure
2, the realiser will not generate an active realisation
since the agent role cannot be instantiated by any
phrase in the grammar. However, depending on the
exact context there are typically options for realis-
ing the subject phrase in an active with very little
descriptive content.
Ideally, one would like to account for these phe-
nomena in a meaning representation that under-
specifies the lexicalisation of discourse referents,
and also captures the reference of implicit argu-
ments. Especially the latter task has hardly been
addressed in NLP applications (but see Gerber and
Chai (2010)). In order to work around that problem,
we implemented some simple heuristics which un-
derspecify the realisation of certain verb arguments.
These rules define: 1. a set of pronouns (generic and
neutral pronouns, universal quantifiers) that corre-
spond to “trivial” agents in active and implicit agents
</bodyText>
<table confidence="0.981480333333333">
Active Passive
2-role trans. 71% (82%) 10% (2%)
1-role trans. 11% (0%) 8% (16%)
</table>
<tableCaption confidence="0.999533">
Table 1: Distribution of voices in SEM� (SEMS,)
</tableCaption>
<bodyText confidence="0.999982028571428">
in passive sentences; 2. a set of prepositional ad-
juncts in passive sentences that correspond to sub-
jects in active sentence (e.g. causative and instru-
mental prepositions like durch “by means of”); 3.
certain syntactic contexts where special underspec-
ification devices are needed, e.g. coordinations or
embeddings, see Zarrieß and Kuhn (2010) for ex-
amples. In the following, we will distinguish 1-role
transitives where the agent is “trivial” or implicit
from 2-role transitives with a non-implicit agent.
By means of the extended underspecification rules
for voice, the sentences in (2) and (3) receive an
identical meaning representation. As a result, our
surface realiser can produce an active alternation for
(3) and a passive alternation for (2). In the follow-
ing, we will refer to the extended representations as
SEM� (“heuristic semantics”), and to the original
representations as SEM,,, (“naive semantics”).
We are aware of the fact that these approximations
introduce some noise into the data and do not always
represent the underlying referents correctly. For in-
stance, the implicit agent in a passive need not be
“trivial” but can correspond to an actual discourse
referent. However, we consider these heuristics as
a first step towards capturing an important discourse
function of the passive alternation, namely the dele-
tion of the agent role. If we did not treat the passives
with an implicit agent on a par with certain actives,
we would have to ignore a major portion of the pas-
sives occurring in corpus data.
Table 1 summarises the distribution of the voices
for the heuristic meaning representation SEM� on
the data-set we will introduce in Section 4, with
the distribution for the naive representation SEM,,,
in parentheses.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="method">
4 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.999531833333333">
Data To obtain a sizable set of realistic corpus ex-
amples for our experiments on voice alternations, we
created our own dataset of input sentences and rep-
resentations, instead of building on treebank exam-
ples as Cahill et al. (2007) do. We extracted 19,905
sentences, all containing at least one transitive verb,
</bodyText>
<figure confidence="0.723237166666667">
hat den Kanzler gesehen.
has the chancellor seen.
von
by
gesehen.
seen.
wurde
was
man
one
Kanzler wurde gesehen.
chancellor was seen.
</figure>
<page confidence="0.729471">
1010
</page>
<table confidence="0.9923807">
f-structure 2 PRED ′see &lt; (I SUBJ)(I OBJ) &gt;′ 3 f-structure 2 PRED ′see &lt; NULL (I SUBJ) &gt;′ 3
Example (2) 6 6 6 6 4 SUBJ PRED ′one′ ] 57777 Example (3) 6 6 4 SUBJ PRED ′chancellor′ ] 577
OBJ PRED ′chancellor′ ] TOPIC ′chancellor′ ]
TOPIC ′one′ ] PASS +
PASS −
semantics HEAD (see) semantics HEAD (see)
Example (2) Example (3)
PAST (see) PAST (see)
ROLE (agent,see,one) ROLE (agent,see,implicit)
ROLE(patient,see,chancellor) ROLE(patient,see,chancellor)
</table>
<figureCaption confidence="0.987295">
Figure 2: F-structure pair for passive-active alternation
</figureCaption>
<bodyText confidence="0.999808954545455">
from the HGC, a huge German corpus of newspa-
per text (204.5 million tokens). The sentences are
automatically parsed with the German LFG gram-
mar. The resulting f-structure parses are transferred
to meaning representations and mapped back to f-
structure charts. For our generation experiments,
we only use those f-structure charts that the XLE
generator can map back to a set of surface realisa-
tions. This results in a total of 1236 test sentences
and 8044 sentences in our training set. The data loss
is mostly due to the fact the XLE generator often
fails on incomplete parses, and on very long sen-
tences. Nevertheless, the average sentence length
(17.28) and number of surface realisations (see Ta-
ble 2) are higher than in Cahill et al. (2007).
Labelling For the training of our ranking model,
we have to tell the learner how closely each surface
realisation candidate resembles the original corpus
sentence. We distinguish the rank categories: “1”
identical to the corpus string, “2” identical to the
corpus string ignoring punctuation, “3” small edit
distance (&lt; 4) to the corpus string ignoring punc-
tuation, “4” different from the corpus sentence. In
one of our experiments (Section 5.1), we used the
rank category “5” to explicitly label the surface real-
isations derived from the alternation f-structure that
does not correspond to the parse of the original cor-
pus sentence. The intermediate rank categories “2”
and “3” are useful since the grammar does not al-
ways regenerate the exact corpus string, see Cahill
et al. (2007) for explanation.
Features The linguistic theories sketched in Sec-
tion 2.2 correlate morphological, syntactic and se-
mantic properties of constituents (or discourse ref-
erents) with their order and argument realisation. In
our system, this correlation is modelled by a combi-
nation of linguistic properties that can be extracted
from the f-structure or meaning representation and
of the surface order that is read off the sentence
string. Standard n-gram features are also used as
features.4 The feature model is built as follows:
for every lemma in the f-structure, we extract a set
of morphological properties (definiteness, person,
pronominal status etc.), the voice of the verbal head,
its syntactic and semantic role, and a set of infor-
mations status features following Cahill and Riester
(2009). These properties are combined in two ways:
a) Precedence features: relative order of properties
in the surface string, e.g. “theme &lt; agent in pas-
sive”, “1st person &lt; 3rd person”; b) “scale align-
ment” features (ScalAl.): combinations of voice and
role properties with morphological properties, e.g.
“subject is singular”, “agent is 3rd person in active
voice” (these are surface-independent, identical for
each alternation candidate).
The model for which we present our results is
based on sentence-internal features only; as Cahill
and Riester (2009) showed, these feature carry a
considerable amount of implicit information about
the discourse context (e.g. in the shape of referring
expressions). We also implemented a set of explic-
itly inter-sentential features, inspired by Centering
Theory (Grosz et al., 1995). This model did not im-
prove over the intra-sentential model.
Evaluation Measures In order to assess the gen-
eral quality of our generation ranking models, we
</bodyText>
<footnote confidence="0.935943333333333">
4The language model is trained on the German data release
for the 2009 ACL Workshop on Machine Translation shared
task, 11,991,277 total sentences.
</footnote>
<page confidence="0.791803">
1011
</page>
<table confidence="0.999848333333333">
FS SEMn SEMh
Avg. # strings 36.7 68.2 75.8
Random Match 16.98 10.72 7.28
Match 15.45 15.04 11.89
LM BLEU 0.68 0.68 0.65
NIST 13.01 12.95 12.69
Match 27.91 27.66 26.38
Ling. Model BLEU 0.764 0.759 0.747
NIST 13.18 13.14 13.01
</table>
<tableCaption confidence="0.999535">
Table 2: Evaluation of Experiment 1
</tableCaption>
<bodyText confidence="0.99989525">
use several standard measures: a) exact match:
how often does the model select the original cor-
pus sentence, b) BLEU: n-gram overlap between
top-ranked and original sentence, c) NIST: modifi-
cation of BLEU giving more weight to less frequent
n-grams. Second, we are interested in the model’s
performance wrt. specific linguistic criteria. We re-
port the following accuracies: d) Voice: how often
does the model select a sentence realising the correct
voice, e) Precedence: how often does the model gen-
erate the right order of the verb arguments (agent and
patient), and f) Vorfeld: how often does the model
correctly predict the verb arguments to appear in the
sentence initial position before the finite verb, the
so-called Vorfeld. See Sections 5.3 and 6 for a dis-
cussion of these measures.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.975784">
5.1 Exp. 1: Effect of Underspecified Input
</subsectionHeader>
<bodyText confidence="0.999863">
We investigate the effect of the input’s underspecifi-
cation on a state-of-the-art surface realisation rank-
ing model. This model implements the entire fea-
ture set described in Section 4 (it is further analysed
in the subsequent experiments). We built 3 datasets
from our alternation data: FS - candidates generated
from the f-structure; SEMn - realisations from the
naive meaning representations; SEMh - candidates
from the heuristically underspecified meaning rep-
resentation. Thus, we keep the set of original cor-
pus sentences (=the target realisations) constant, but
train and test the model on different candidate sets.
In Table 2, we compare the performance of the
linguistically informed model described in Section 4
on the candidates sets against a random choice and
a language model (LM) baseline. The differences in
BLEU between the candidate sets and models are
</bodyText>
<table confidence="0.9997761">
FS SEMn SEMh
Voice Acc. 100 98.06 91.05
Voice Spec. 100 22.8 0
Majority BL 82.4
Voice Acc. 100 97.7 91.8
Voice Spec. 100 8.33 0
Majority BL 88.5
Voice Acc. 100 100 90.0
Voice Spec. 100 100 0
Majority BL 53.9
</table>
<tableCaption confidence="0.9939475">
Table 3: Accuracy of Voice Prediction by Ling. Model in
Experiment 1
</tableCaption>
<bodyText confidence="0.999328909090909">
statistically significant.5 In general, the linguistic
model largely outperforms the LM and is less sen-
sitive to the additional confusion introduced by the
SEMh input. Its BLEU score and match accuracy
decrease only slightly (though statistically signifi-
cantly).
In Table 3, we report the performance of the lin-
guistic model on the different candidate sets with re-
spect to voice accuracy. Since the candidate sets dif-
fer in the proportion of items that underspecify the
voice (see “Voice Spec.” in Table 3), we also report
the accuracy on the SEMn* test set, which is a sub-
set of SEMn excluding the items where the voice is
specified. Table 3 shows that the proportion of active
realisations for the SEMn* input is very high, and
the model does not outperform the majority baseline
(which always selects active). In contrast, the SEMh
model clearly outperforms the majority baseline.
Example (4) is a case from our development set
where the SEMn model incorrectly predicts an ac-
tive (4-a), and the SEMh correctly predicts a passive
(4-b).
</bodyText>
<figure confidence="0.915792541666667">
(4) a. 26 kostspielige Studien erw¨ahnten die Finanzierung.
26 expensive studies mentioned the funding.
26 kostspieligen Studien
26 expensive studies
erw¨ahnt.
mentioned.
This prediction is according to the markedness hier-
archy: the patient is singular and definite, the agent
5According to a bootstrap resampling test, P &lt; 0.05
All Trans.
2-role Trans.
1-role Trans.
SEMn*
97.59
0
98.1
97.59
0
98.1
-
-
-
b. Die Finanzierung wurde von
The funding was by
</figure>
<page confidence="0.941265">
1012
</page>
<table confidence="0.999398">
Features Match BLEU Voice Prec. VF
Prec. 16.3 0.70 88.43 64.1 59.1
ScalAl. 10.4 0.64 90.37 58.9 56.3
Union 26.4 0.75 91.50 80.2 70.9
</table>
<tableCaption confidence="0.99984">
Table 4: Evaluation of Experiment 2
</tableCaption>
<bodyText confidence="0.99997425">
is plural and indefinite. Counterexamples are possi-
ble, but there is a clear statistical preference – which
the model was able to pick up.
On the one hand, the rankers can cope surpris-
ingly well with the additional realisations obtained
from the meaning representations. According to the
global sentence overlap measures, their quality is
not seriously impaired. On the other hand, the de-
sign of the representations has a substantial effect
on the prediction of the alternations. The SEM,,
does not seem to learn certain preferences because
of the extremely imbalanced distribution in the in-
put data. This confirms the hypothesis sketched in
Section 3.1, according to which the degree of the
input’s underspecification can crucially change the
behaviour of the ranking model.
</bodyText>
<subsectionHeader confidence="0.9566">
5.2 Exp. 2: Word Order and Voice
</subsectionHeader>
<bodyText confidence="0.997689468085106">
We examine the impact of certain feature types on
the prediction of the variation types in our data. We
are particularly interested in the interaction of voice
and word order (precedence) since linguistic theo-
ries (see Section 2.2) predict similar information-
structural factors guiding their use, but usually do
not consider them in conjunction.
In Table 4, we report the performance of ranking
models trained on the different feature subsets intro-
duced in Section 4. The union of the features corre-
sponds to the model trained on SEMh in Experiment
1. At a very broad level, the results suggest that the
precedence and the scale alignment features interact
both in the prediction of voice and word order.
The most pronounced effect on voice accuracy
can be seen when comparing the precedence model
to the union model. Adding the surface-independent
scale alignment features to the precedence features
leads to a big improvement in the prediction of word
order. This is not a trivial observation since a) the
surface-independent features do not discriminate be-
tween the word orders and b) the precedence fea-
tures are built from the same properties (see Sec-
tion 4). Thus, the SVM learner discovers depen-
dencies between relative precedence preferences and
abstract properties of a verb argument which cannot
be encoded in the precedence alone.
It is worth noting that the precedence features im-
prove the voice prediction. This indicates that wher-
ever the application context allows it, voice should
not be specified at a stage prior to word order. Ex-
ample (5) is taken from our development set, illus-
trating a case where the union model predicted the
correct voice and word order (5-a), and the scale
alignment model top-ranked the incorrect voice and
word order. The active verb arguments in (5-b) are
both case-ambigous and placed in the non-canonical
order (object &lt; subject), so the semantic relation can
be easily misunderstood. The passive in (5-a) is un-
ambiguous since the agent is realised in a PP (and
placed in the Vorfeld).
deutschen Medien.
German media.
Moreover, our results confirm Filippova and
Strube (2007) who find that it is harder to predict
the correct Vorfeld occupant in a German sentence,
than to predict the relative order of the constituents.
</bodyText>
<subsectionHeader confidence="0.902764">
5.3 Exp. 3: Capturing Flexible Variation
</subsectionHeader>
<bodyText confidence="0.999926944444444">
The previous experiment has shown that there is a
certain inter-dependence between word order and
voice. This experiment addresses this interaction
by varying the way the training data for the ranker
is labelled. We contrast two ways of labelling the
sentences (see Section 4): a) all sentences that are
not (nearly) identical to the reference sentence have
the rank category “4”, irrespective of their voice (re-
ferred to as unlabelled model), b) the sentences that
do not realise the correct voice are ranked lower than
sentences with the correct voice (“4” vs. “5”), re-
ferred to as labelled model. Intuitively, the latter
way of labelling tells the ranker that all sentences
in the incorrect voice are worse than all sentences
in the correct voice, independent of the word order.
Given the first labelling strategy, the ranker can de-
cide in an unsupervised way which combinations of
word order and voice are to be preferred.
</bodyText>
<figure confidence="0.9953014">
(5) a. Von den
By the
deutschen
German
Medien
media
wurden
die
the
were
Ausl¨ander
foreigners
wenn es
mentioned, when there
Zoff
trouble
nur
only
gab.
was.
erw¨ahnt,
b. Wenn es
When there
Zoff
trouble
erw¨ahnten die
mentioned the
Ausl¨ander
foreigners
gab,
was,
die
the
nur
only
</figure>
<page confidence="0.904008">
1013
</page>
<table confidence="0.9984468">
Top 1 Top 1 Top 1 Top 2 Top 3
Model Match BLEU NIST Voice Prec. Prec.+Voice Prec.+Voice Prec.+Voice
Labelled, no LM 21.52 0.73 12.93 91.9 76.25 71.01 78.35 82.31
Unlabelled, no LM 26.83 0.75 13.01 91.5 80.19 74.51 84.28 88.59
Unlabeled + LM 27.35 0.75 13.08 91.5 79.6 73.92 79.74 82.89
</table>
<tableCaption confidence="0.999758">
Table 5: Evaluation of Experiment 3
</tableCaption>
<bodyText confidence="0.999986064516129">
In Table 5, it can be seen that the unlabelled model
improves over the labelled on all the sentence over-
lap measures. The improvements are statistically
significant. Moreover, we compare the n-best ac-
curacies achieved by the models for the joint pre-
diction of voice and argument order. The unla-
belled model is very flexible with respect to the word
order-voice interaction: the accuracy dramatically
improves when looking at the top 3 sentences. Ta-
ble 5 also reports the performance of an unlabelled
model that additionally integrates LM scores. Sur-
prisingly, these scores have a very small positive ef-
fect on the sentence overlap features and no positive
effect on the voice and precedence accuracy. The
n-best evaluations even suggest that the LM scores
negatively impact the ranker: the accuracy for the
top 3 sentences increases much less as compared to
the model that does not integrate LM scores.6
The n-best performance of a realisation ranker is
practically relevant for re-ranking applications such
as Velldal (2008). We think that it is also concep-
tually interesting. Previous evaluation studies sug-
gest that the original corpus sentence is not always
the only optimal realisation of a given linguistic in-
put (Cahill and Forst, 2010; Belz and Kow, 2010).
Humans seem to have varying preferences for word
order contrasts in certain contexts. The n-best evalu-
ation could reflect the behaviour of a ranking model
with respect to the range of variations encountered
in real discourse. The pilot human evaluation in the
next Section deals with this question.
</bodyText>
<sectionHeader confidence="0.974486" genericHeader="method">
6 Human Evaluation
</sectionHeader>
<bodyText confidence="0.995386666666667">
Our experiment in Section 5.3 has shown that the ac-
curacy of our linguistically informed ranking model
dramatically increases when we consider the three
</bodyText>
<footnote confidence="0.988561">
6(Nakanishi et al., 2005) also note a negative effect of in-
cluding LM scores in their model, pointing out that the LM was
not trained on enough data. The corpus used for training our
LM might also have been too small or distinct in genre.
</footnote>
<bodyText confidence="0.999908725">
best sentences rather than only the top-ranked sen-
tence. This means that the model sometimes predicts
almost equal naturalness for different voice realisa-
tions. Moreover, in the case of word order, we know
from previous evaluation studies, that humans some-
times prefer different realisations than the original
corpus sentences. This Section investigates agree-
ment in human judgements of voice realisation.
Whereas previous studies in generation mainly
used human evaluation to compare different sys-
tems, or to correlate human and automatic evalua-
tions, our primary interest is the agreement or cor-
relation between human rankings. In particular, we
explore the hypothesis that this agreement is higher
in certain contexts than in others. In order to select
these contexts, we use the predictions made by our
ranking model.
The questionnaire for our experiment comprised
24 items falling into 3 classes: a) items where the
3 best sentences predicted by the model have the
same voice as the original sentence (“Correct”), b)
items where the 3 top-ranked sentences realise dif-
ferent voices (“Mixed”), c) items where the model
predicted the incorrect voice in all 3 top sentences
(“False”). Each item is composed of the original
sentence, the 3 top-ranked sentences (if not identical
to the corpus sentence) and 2 further sentences such
that each item contains different voices. For each
item, we presented the previous context sentence.
The experiment was completed by 8 participants,
all native speakers of German, 5 had a linguistic
background. The participants were asked to rank
each sentence on a scale from 1-6 according to its
naturalness and plausibility in the given context. The
participants were explicitly allowed to use the same
rank for sentences they find equally natural. The par-
ticipants made heavy use of this option: out of the
192 annotated items, only 8 are ranked such that no
two sentences have the same rank.
We compare the human judgements by correlat-
</bodyText>
<page confidence="0.987103">
1014
</page>
<bodyText confidence="0.999674380952381">
ing them with Spearman’s p. This measure is con-
sidered appropriate for graded annotation tasks in
general (Erk and McCarthy, 2009), and has also
been used for analysing human realisation rankings
(Velldal, 2008; Cahill and Forst, 2010). We nor-
malise the ranks according to the procedure in Vell-
dal (2008). In Table 6, we report the correlations
obtained from averaging over all pairwise correla-
tions between the participants and the correlations
restricted to the item and sentence classes. We used
bootstrap re-sampling on the pairwise correlations to
test that the correlations on the different item classes
significantly differ from each other.
The correlations in Table 6 suggest that the agree-
ment between annotators is highest on the false
items, and lowest on the mixed items. Humans
tended to give the best rank to the original sentence
more often on the false items (91%) than on the oth-
ers. Moreover, the agreement is generally higher on
the sentences realising the correct voice.
These results seem to confirm our hypothesis that
the general level of agreement between humans dif-
fers depending on the context. However, one has to
be careful in relating the effects in our data solely to
voice preferences. Since the sentences were chosen
automatically, some examples contain very unnatu-
ral word orders that probably guided the annotators’
decisions more than the voice. This is illustrated
by Example (6) showing two passive sentences from
our questionnaire which differ only in the position of
the adverb besser “better”. Sentence (6-a) is com-
pletely implausible for a native speaker of German,
whereas Sentence (6-b) sounds very natural.
besser gesch¨utzt werden.
better protected be.
This observation brings us back to our initial point
that the surface realisation task is especially chal-
lenging due to the interaction of a range of semantic
and discourse phenomena. Obviously, this interac-
tion makes it difficult to single out preferences for a
specific alternation type. Future work will have to
establish how this problem should be dealt with in
</bodyText>
<table confidence="0.998946714285714">
All Items
Correct Mixed False
“All” sent. 0.58 0.6 0.54 0.62
“Correct” sent. 0.64 0.63 0.56 0.72
“False” sent. 0.47 0.57 0.48 0.44
Top-ranked 84% 78% 83% 91%
corpus sent.
</table>
<tableCaption confidence="0.8332495">
Table 6: Human Evaluation
the design of human evaluation experiments.
</tableCaption>
<sectionHeader confidence="0.997791" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999976666666667">
We have presented a grammar-based generation ar-
chitecture which implements the surface realisation
of meaning representations abstracting from voice
and word order. In order to be able to study voice
alternations in a variety of contexts, we designed
heuristic underspecification rules which establish,
for instance, the alternation relation between an ac-
tive with a generic agent and a passive that does
not overtly realise the agent. This strategy leads
to a better balanced distribution of the alternations
in the training data, such that our linguistically
informed generation ranking model achieves high
BLEU scores and accurately predicts active and pas-
sive. In future work, we will extend our experiments
to a wider range of alternations and try to capture
inter-sentential context more explicitly. Moreover, it
would be interesting to carry over our methodology
to a purely statistical linearisation system where the
relation between an input representation and a set of
candidate realisations is not so clearly defined as in
a grammar-based system.
Our study also addressed the interaction of dif-
ferent linguistic variation types, i.e. word order
and voice, by looking at different types of linguis-
tic features and exploring different ways of labelling
the training data. However, our SVM-based learn-
ing framework is not well-suited to directly assess
the correlation between a certain feature (or fea-
ture combination) and the occurrence of an alterna-
tion. Therefore, it would be interesting to relate our
work to the techniques used in theoretical papers,
e.g. (Bresnan et al., 2007), where these correlations
are analysed more directly.
</bodyText>
<figure confidence="0.9979848125">
(6) a. Durch
By
Eigenheimbesitzer gesch¨utzt werden.
house owners protected be.
b. Durch das neue Gesetz sollen Eigenheimbesitzer
By the new law should house owners
das
the
Gesetz
law
besser
better
sollen
should
neue
new
</figure>
<page confidence="0.979863">
1015
</page>
<sectionHeader confidence="0.994642" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998145380952381">
Judith Aissen. 1999. Markedness and subject choice in
optimality theory. Natural Language and Linguistic
Theory, 17(4):673–711.
Judith Aissen. 2003. Differential Object Marking:
Iconicity vs. Economy. Natural Language and Lin-
guistic Theory, 21:435–483.
Anja Belz and Eric Kow. 2010. Comparing rating
scales and preference judgements in language evalu-
ation. In Proceedings of the 6th International Natural
Language Generation Conference (INLG’10).
Anja Belz, Mike White, Josef van Genabith, Deirdre
Hogan, and Amanda Stent. 2010. Finding common
ground: Towards a surface realisation shared task.
In Proceedings of the 6th International Natural Lan-
guage Generation Conference (INLG’10).
Anja Belz. 2005. Statistical generation: Three meth-
ods compared and evaluated. In Proceedings of Tenth
European Workshop on Natural Language Generation
(ENLG-05), pages 15–23.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING 2010), Bei-
jing, China.
Joan Bresnan, Shipra Dingare, and Christopher D. Man-
ning. 2001. Soft Constraints Mirror Hard Constraints:
Voice and Person in English and Lummi. In Proceed-
ings of the LFG ’01 Conference.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald
Baayen. 2007. Predicting the Dative Alternation. In
G. Boume, I. Kraemer, and J. Zwarts, editors, Cogni-
tive Foundations ofInterpretation. Amsterdam: Royal
Netherlands Academy of Science.
Aoife Cahill and Martin Forst. 2010. Human Evaluation
of a German Surface Realisation Ranker. In Proceed-
ings of the 12th Conference of the European Chapter
of the ACL (EACL 2009), pages 112 – 120, Athens,
Greece. Association for Computational Linguistics.
Aoife Cahill and Arndt Riester. 2009. Incorporating
Information Status into Generation Ranking. In Pro-
ceedings of the 47th Annual Meeting of the ACL, pages
817–825, Suntec, Singapore, August. Association for
Computational Linguistics.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Stochastic realisation ranking for a free word order
language. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
17–24, Saarbr¨ucken, Germany, June. DFKI GmbH.
Document D-07-01.
Dick Crouch and Tracy Holloway King. 2006. Se-
mantics via F-Structure Rewriting. In Miriam Butt
and Tracy Holloway King, editors, Proceedings of the
LFG06 Conference.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2006. XLE Docu-
mentation. Technical report, Palo Alto Research Cen-
ter, CA.
Katrin Erk and Diana McCarthy. 2009. Graded Word
Sense Assignment. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 440 – 449, Singapore.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics (ACL 07), Prague, Czech
Republic.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Companion Volume to the Pro-
ceedings of Human Language Technologies Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT 09,
short)., Boulder, Colorado.
Martin Forst. 2007. Filling Statistics with Linguistics
– Property Design for the Disambiguation of German
LFG Parses. In ACL 2007 Workshop on Deep Linguis-
tic Processing, pages 17–24, Prague, Czech Republic,
June. Association for Computational Linguistics.
Matthew Gerber and Joyce Chai. 2010. Beyond nom-
bank: A study of implicit argumentation for nominal
predicates. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Thorsten Joachims. 1996. Training linear svms in linear
time. In M. Butt and T. H. King, editors, Proceedings
of the ACM Conference on Knowledge Discovery and
Data Mining (KDD), CSLI Proceedings Online.
Jonas Kuhn. 2003. Optimality-Theoretic Syntax—A
Declarative Approach. CSLI Publications, Stanford,
CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the ACL/COLING-98, pages 704–710,
Montreal, Quebec.
Tomasz Marciniak and Michael Strube. 2005. Using
an annotated corpus as a knowledge source for lan-
guage generation. In Proceedings of Workshop on Us-
ing Corpora for Natural Language Generation, pages
19–24, Birmingham, UK.
Hiroko Nakanishi, Yusuke Miyao, and Junichi Tsujii.
2005. Probabilistic models for disambiguation of an
</reference>
<page confidence="0.803267">
1016
</page>
<reference confidence="0.9997418">
HPSG-based chart generator. In Proceedings ofIWPT
2005.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL 2000, pages 194–201, Seattle, WA.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of the 38th Annual
Meeting ofthe Association for Computational Linguis-
tics (ACL’00), Hong Kong, pages 480–487.
Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King,
John Maxwell, and Mark Johnson. 2002. Parsing the
Wall Street Journal using a Lexical-Functional Gram-
mar and discriminative estimation techniques. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’02), Pennsyl-
vania, Philadelphia.
Eric K. Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically Informed Statistical
Models of Constituent Structure for Ordering in Sen-
tence Realization. In Proceedings of the 2004 In-
ternational Conference on Computational Linguistics,
Geneva, Switzerland.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG for
German. In Proceedings ofLREC-2006.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia.
Erik Velldal. 2008. Empirical Realization Ranking.
Ph.D. thesis, University of Oslo, Department of Infor-
matics.
Sina ZarrieB and Jonas Kuhn. 2010. Reversing F-
structure Rewriting for Generation from Meaning Rep-
resentations. In Proceedings of the LFG10 Confer-
ence, Ottawa.
Sina ZarrieB. 2009. Developing German Semantics on
the basis of Parallel LFG Grammars. In Proceed-
ings of the 2009 Workshop on Grammar Engineering
Across Frameworks (GEAF 2009), pages 10–18, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
</reference>
<page confidence="0.994093">
1017
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941285">
<title confidence="0.999884">Underspecifying and Predicting Voice for Surface Realisation Ranking</title>
<author confidence="0.97451">Sina ZarrieB</author>
<author confidence="0.97451">Aoife Cahill</author>
<author confidence="0.97451">Jonas</author>
<affiliation confidence="0.994225">Institut f¨ur maschinelle Universit¨at Stuttgart,</affiliation>
<abstract confidence="0.9985144">This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Judith Aissen</author>
</authors>
<title>Markedness and subject choice in optimality theory.</title>
<date>1999</date>
<journal>Natural Language and Linguistic Theory,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="3315" citStr="Aissen, 1999" startWordPosition="502" endWordPosition="503">presentations over corpus data for machine learning of realisation ranking. However, we go beyond the task of finding the correct ordering for an almost fixed set of word forms. Quite obviously, word order is only one of the means at a speaker’s disposal for expressing some content in a contextually appropriate form; we add systematic alternations like the voice alternation (active vs. passive) to the picture. As an alternative way of promoting or demoting the prominence of a syntactic argument, its interaction with word ordering strategies in real corpus data is of high theoretical interest (Aissen, 1999; Aissen, 2003; Bresnan et al., 2001). Our main goals are (i) to establish a corpus-based surface realisation framework for empirically investigating interactions of voice and word order in German, (ii) to design an input representation for generation capturing voice alternations in a variety of contexts, (iii) to better understand the relationship between the performance of a generation ranking model and the type of realisation candidates available in its input. In working towards these goals, this paper addresses the question of evaluation. We conduct a pilot human evaluation on the voice al</context>
<context position="8645" citStr="Aissen, 1999" startWordPosition="1286" endWordPosition="1287">and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these annotations are not suitable for full generation since they are often incomplete. Thus, it is not clear to which degree these annotations are actually underspecified for certain paraphrases. 2.2 Linguistic Background In competition-based linguistic theories (Optimality Theory and related frameworks), the use of argument alternations is construed as an effect of markedness hierarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, ...) on 1008 the one hand and the various properties that argument phrases can bear (person, animacy, definiteness) on the other are organised in markedness hierarchies. Wherever possible, there is a tendency to align the hierarchies, i.e., use prominent functions to realise prominently marked argument phrases. For instance, Bresnan et al. (2001) find that there is a statistical tendency in English to passivise a verb if the patient is higher on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) </context>
</contexts>
<marker>Aissen, 1999</marker>
<rawString>Judith Aissen. 1999. Markedness and subject choice in optimality theory. Natural Language and Linguistic Theory, 17(4):673–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Aissen</author>
</authors>
<title>Differential Object Marking: Iconicity vs.</title>
<date>2003</date>
<booktitle>Economy. Natural Language and Linguistic Theory,</booktitle>
<pages>21--435</pages>
<contexts>
<context position="3329" citStr="Aissen, 2003" startWordPosition="504" endWordPosition="505">over corpus data for machine learning of realisation ranking. However, we go beyond the task of finding the correct ordering for an almost fixed set of word forms. Quite obviously, word order is only one of the means at a speaker’s disposal for expressing some content in a contextually appropriate form; we add systematic alternations like the voice alternation (active vs. passive) to the picture. As an alternative way of promoting or demoting the prominence of a syntactic argument, its interaction with word ordering strategies in real corpus data is of high theoretical interest (Aissen, 1999; Aissen, 2003; Bresnan et al., 2001). Our main goals are (i) to establish a corpus-based surface realisation framework for empirically investigating interactions of voice and word order in German, (ii) to design an input representation for generation capturing voice alternations in a variety of contexts, (iii) to better understand the relationship between the performance of a generation ranking model and the type of realisation candidates available in its input. In working towards these goals, this paper addresses the question of evaluation. We conduct a pilot human evaluation on the voice al1007 Proceedin</context>
<context position="8660" citStr="Aissen, 2003" startWordPosition="1288" endWordPosition="1289">ly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these annotations are not suitable for full generation since they are often incomplete. Thus, it is not clear to which degree these annotations are actually underspecified for certain paraphrases. 2.2 Linguistic Background In competition-based linguistic theories (Optimality Theory and related frameworks), the use of argument alternations is construed as an effect of markedness hierarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, ...) on 1008 the one hand and the various properties that argument phrases can bear (person, animacy, definiteness) on the other are organised in markedness hierarchies. Wherever possible, there is a tendency to align the hierarchies, i.e., use prominent functions to realise prominently marked argument phrases. For instance, Bresnan et al. (2001) find that there is a statistical tendency in English to passivise a verb if the patient is higher on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) correlate the u</context>
</contexts>
<marker>Aissen, 2003</marker>
<rawString>Judith Aissen. 2003. Differential Object Marking: Iconicity vs. Economy. Natural Language and Linguistic Theory, 21:435–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>Comparing rating scales and preference judgements in language evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference (INLG’10).</booktitle>
<contexts>
<context position="31119" citStr="Belz and Kow, 2010" startWordPosition="4908" endWordPosition="4911">no positive effect on the voice and precedence accuracy. The n-best evaluations even suggest that the LM scores negatively impact the ranker: the accuracy for the top 3 sentences increases much less as compared to the model that does not integrate LM scores.6 The n-best performance of a realisation ranker is practically relevant for re-ranking applications such as Velldal (2008). We think that it is also conceptually interesting. Previous evaluation studies suggest that the original corpus sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6(Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not t</context>
</contexts>
<marker>Belz, Kow, 2010</marker>
<rawString>Anja Belz and Eric Kow. 2010. Comparing rating scales and preference judgements in language evaluation. In Proceedings of the 6th International Natural Language Generation Conference (INLG’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Mike White</author>
<author>Josef van Genabith</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>Finding common ground: Towards a surface realisation shared task.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Natural Language Generation Conference (INLG’10).</booktitle>
<marker>Belz, White, van Genabith, Hogan, Stent, 2010</marker>
<rawString>Anja Belz, Mike White, Josef van Genabith, Deirdre Hogan, and Amanda Stent. 2010. Finding common ground: Towards a surface realisation shared task. In Proceedings of the 6th International Natural Language Generation Conference (INLG’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Statistical generation: Three methods compared and evaluated.</title>
<date>2005</date>
<booktitle>In Proceedings of Tenth European Workshop on Natural Language Generation (ENLG-05),</booktitle>
<pages>15--23</pages>
<contexts>
<context position="7046" citStr="Belz, 2005" startWordPosition="1050" endWordPosition="1051">rchitecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). </context>
</contexts>
<marker>Belz, 2005</marker>
<rawString>Anja Belz. 2005. Statistical generation: Three methods compared and evaluated. In Proceedings of Tenth European Workshop on Natural Language Generation (ENLG-05), pages 15–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Leo Wanner</author>
<author>Simon Mill</author>
<author>Alicia Burga</author>
</authors>
<title>Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="8129" citStr="Bohnet et al. (2010)" startWordPosition="1210" endWordPosition="1213">in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these annotations are not suitable for full generation since they are often incomplete. Thus, it is not clear to which degree these annotations are actually underspecified for certain paraphrases. 2.2 Linguistic Background In competition-based linguistic theories (Optimality Theory and related frameworks), the use of argument alternations is construed as an effect of markedness hierarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, ...) on 1008 the one hand and </context>
</contexts>
<marker>Bohnet, Wanner, Mill, Burga, 2010</marker>
<rawString>Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Shipra Dingare</author>
<author>Christopher D Manning</author>
</authors>
<title>Soft Constraints Mirror Hard Constraints: Voice and Person in English and Lummi.</title>
<date>2001</date>
<booktitle>In Proceedings of the LFG ’01 Conference.</booktitle>
<contexts>
<context position="3352" citStr="Bresnan et al., 2001" startWordPosition="506" endWordPosition="509">ta for machine learning of realisation ranking. However, we go beyond the task of finding the correct ordering for an almost fixed set of word forms. Quite obviously, word order is only one of the means at a speaker’s disposal for expressing some content in a contextually appropriate form; we add systematic alternations like the voice alternation (active vs. passive) to the picture. As an alternative way of promoting or demoting the prominence of a syntactic argument, its interaction with word ordering strategies in real corpus data is of high theoretical interest (Aissen, 1999; Aissen, 2003; Bresnan et al., 2001). Our main goals are (i) to establish a corpus-based surface realisation framework for empirically investigating interactions of voice and word order in German, (ii) to design an input representation for generation capturing voice alternations in a variety of contexts, (iii) to better understand the relationship between the performance of a generation ranking model and the type of realisation candidates available in its input. In working towards these goals, this paper addresses the question of evaluation. We conduct a pilot human evaluation on the voice al1007 Proceedings of the 49th Annual M</context>
<context position="9047" citStr="Bresnan et al. (2001)" startWordPosition="1345" endWordPosition="1348">aphrases. 2.2 Linguistic Background In competition-based linguistic theories (Optimality Theory and related frameworks), the use of argument alternations is construed as an effect of markedness hierarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, ...) on 1008 the one hand and the various properties that argument phrases can bear (person, animacy, definiteness) on the other are organised in markedness hierarchies. Wherever possible, there is a tendency to align the hierarchies, i.e., use prominent functions to realise prominently marked argument phrases. For instance, Bresnan et al. (2001) find that there is a statistical tendency in English to passivise a verb if the patient is higher on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have b</context>
</contexts>
<marker>Bresnan, Dingare, Manning, 2001</marker>
<rawString>Joan Bresnan, Shipra Dingare, and Christopher D. Manning. 2001. Soft Constraints Mirror Hard Constraints: Voice and Person in English and Lummi. In Proceedings of the LFG ’01 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
<author>Anna Cueni</author>
<author>Tatiana Nikitina</author>
<author>Harald Baayen</author>
</authors>
<title>Predicting the Dative Alternation. In</title>
<date>2007</date>
<booktitle>Cognitive Foundations ofInterpretation. Amsterdam: Royal Netherlands Academy of Science.</booktitle>
<editor>G. Boume, I. Kraemer, and J. Zwarts, editors,</editor>
<contexts>
<context position="9244" citStr="Bresnan et al. (2007)" startWordPosition="1381" endWordPosition="1384">rarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, ...) on 1008 the one hand and the various properties that argument phrases can bear (person, animacy, definiteness) on the other are organised in markedness hierarchies. Wherever possible, there is a tendency to align the hierarchies, i.e., use prominent functions to realise prominently marked argument phrases. For instance, Bresnan et al. (2001) find that there is a statistical tendency in English to passivise a verb if the patient is higher on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model f</context>
</contexts>
<marker>Bresnan, Cueni, Nikitina, Baayen, 2007</marker>
<rawString>Joan Bresnan, Anna Cueni, Tatiana Nikitina, and Harald Baayen. 2007. Predicting the Dative Alternation. In G. Boume, I. Kraemer, and J. Zwarts, editors, Cognitive Foundations ofInterpretation. Amsterdam: Royal Netherlands Academy of Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
</authors>
<title>Human Evaluation of a German Surface Realisation Ranker.</title>
<date>2010</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>112--120</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="31098" citStr="Cahill and Forst, 2010" startWordPosition="4904" endWordPosition="4907">ce overlap features and no positive effect on the voice and precedence accuracy. The n-best evaluations even suggest that the LM scores negatively impact the ranker: the accuracy for the top 3 sentences increases much less as compared to the model that does not integrate LM scores.6 The n-best performance of a realisation ranker is practically relevant for re-ranking applications such as Velldal (2008). We think that it is also conceptually interesting. Previous evaluation studies suggest that the original corpus sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6(Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out </context>
<context position="34043" citStr="Cahill and Forst, 2010" startWordPosition="5381" endWordPosition="5384">nce on a scale from 1-6 according to its naturalness and plausibility in the given context. The participants were explicitly allowed to use the same rank for sentences they find equally natural. The participants made heavy use of this option: out of the 192 annotated items, only 8 are ranked such that no two sentences have the same rank. We compare the human judgements by correlat1014 ing them with Spearman’s p. This measure is considered appropriate for graded annotation tasks in general (Erk and McCarthy, 2009), and has also been used for analysing human realisation rankings (Velldal, 2008; Cahill and Forst, 2010). We normalise the ranks according to the procedure in Velldal (2008). In Table 6, we report the correlations obtained from averaging over all pairwise correlations between the participants and the correlations restricted to the item and sentence classes. We used bootstrap re-sampling on the pairwise correlations to test that the correlations on the different item classes significantly differ from each other. The correlations in Table 6 suggest that the agreement between annotators is highest on the false items, and lowest on the mixed items. Humans tended to give the best rank to the original</context>
</contexts>
<marker>Cahill, Forst, 2010</marker>
<rawString>Aoife Cahill and Martin Forst. 2010. Human Evaluation of a German Surface Realisation Ranker. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 112 – 120, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Arndt Riester</author>
</authors>
<title>Incorporating Information Status into Generation Ranking.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL,</booktitle>
<pages>817--825</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2631" citStr="Cahill and Riester, 2009" startWordPosition="386" endWordPosition="389">m. Such ranking systems are practically relevant for the real-world application of grammar-based generators that usually generate several grammatical surface sentences from a given abstract input, e.g. (Velldal and Oepen, 2006). Moreover, this framework allows for detailed experimental studies of the interaction of specific linguistic features. Thus it has been demonstrated that for free word order languages like German, word order prediction quality can be improved with carefully designed, linguistically informed models capturing information-structural strategies (Filippova and Strube, 2007; Cahill and Riester, 2009). This paper is situated in the same framework, using rich linguistic representations over corpus data for machine learning of realisation ranking. However, we go beyond the task of finding the correct ordering for an almost fixed set of word forms. Quite obviously, word order is only one of the means at a speaker’s disposal for expressing some content in a contextually appropriate form; we add systematic alternations like the voice alternation (active vs. passive) to the picture. As an alternative way of promoting or demoting the prominence of a syntactic argument, its interaction with word o</context>
<context position="9824" citStr="Cahill and Riester (2009)" startWordPosition="1473" endWordPosition="1476">grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalen</context>
<context position="20154" citStr="Cahill and Riester (2009)" startWordPosition="3109" endWordPosition="3112">s) with their order and argument realisation. In our system, this correlation is modelled by a combination of linguistic properties that can be extracted from the f-structure or meaning representation and of the surface order that is read off the sentence string. Standard n-gram features are also used as features.4 The feature model is built as follows: for every lemma in the f-structure, we extract a set of morphological properties (definiteness, person, pronominal status etc.), the voice of the verbal head, its syntactic and semantic role, and a set of informations status features following Cahill and Riester (2009). These properties are combined in two ways: a) Precedence features: relative order of properties in the surface string, e.g. “theme &lt; agent in passive”, “1st person &lt; 3rd person”; b) “scale alignment” features (ScalAl.): combinations of voice and role properties with morphological properties, e.g. “subject is singular”, “agent is 3rd person in active voice” (these are surface-independent, identical for each alternation candidate). The model for which we present our results is based on sentence-internal features only; as Cahill and Riester (2009) showed, these feature carry a considerable amou</context>
</contexts>
<marker>Cahill, Riester, 2009</marker>
<rawString>Aoife Cahill and Arndt Riester. 2009. Incorporating Information Status into Generation Ranking. In Proceedings of the 47th Annual Meeting of the ACL, pages 817–825, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Martin Forst</author>
<author>Christian Rohrer</author>
</authors>
<title>Stochastic realisation ranking for a free word order language.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh European Workshop on Natural Language Generation,</booktitle>
<pages>17--24</pages>
<publisher>DFKI GmbH. Document</publisher>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="7644" citStr="Cahill et al., 2007" startWordPosition="1136" endWordPosition="1139">rube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these</context>
<context position="11221" citStr="Cahill et al. (2007)" startWordPosition="1678" endWordPosition="1681">e use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3The choice among alternative f-structures is done with a discriminative model (Forst, 2007). Snty SVM Ranker Sntb, Snta, Sntaz... Sntb, LFG Grammar FSa Reverse Sem. Rules LFG Grammar Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the same declarative grammar in the parsing and generation direction. To obtain a more abstract underlying representation (in the pipeline on the right-hand side of Figure 1), the present work uses an additional semantic construction component (Crouch and King, 2006; Zarrieß, 2009) to map LFG f-</context>
<context position="17099" citStr="Cahill et al. (2007)" startWordPosition="2614" endWordPosition="2617">he passives with an implicit agent on a par with certain actives, we would have to ignore a major portion of the passives occurring in corpus data. Table 1 summarises the distribution of the voices for the heuristic meaning representation SEM� on the data-set we will introduce in Section 4, with the distribution for the naive representation SEM,,, in parentheses. 4 Experimental Set-up Data To obtain a sizable set of realistic corpus examples for our experiments on voice alternations, we created our own dataset of input sentences and representations, instead of building on treebank examples as Cahill et al. (2007) do. We extracted 19,905 sentences, all containing at least one transitive verb, hat den Kanzler gesehen. has the chancellor seen. von by gesehen. seen. wurde was man one Kanzler wurde gesehen. chancellor was seen. 1010 f-structure 2 PRED ′see &lt; (I SUBJ)(I OBJ) &gt;′ 3 f-structure 2 PRED ′see &lt; NULL (I SUBJ) &gt;′ 3 Example (2) 6 6 6 6 4 SUBJ PRED ′one′ ] 57777 Example (3) 6 6 4 SUBJ PRED ′chancellor′ ] 577 OBJ PRED ′chancellor′ ] TOPIC ′chancellor′ ] TOPIC ′one′ ] PASS + PASS − semantics HEAD (see) semantics HEAD (see) Example (2) Example (3) PAST (see) PAST (see) ROLE (agent,see,one) ROLE (agent,s</context>
<context position="18571" citStr="Cahill et al. (2007)" startWordPosition="2859" endWordPosition="2862">e German LFG grammar. The resulting f-structure parses are transferred to meaning representations and mapped back to fstructure charts. For our generation experiments, we only use those f-structure charts that the XLE generator can map back to a set of surface realisations. This results in a total of 1236 test sentences and 8044 sentences in our training set. The data loss is mostly due to the fact the XLE generator often fails on incomplete parses, and on very long sentences. Nevertheless, the average sentence length (17.28) and number of surface realisations (see Table 2) are higher than in Cahill et al. (2007). Labelling For the training of our ranking model, we have to tell the learner how closely each surface realisation candidate resembles the original corpus sentence. We distinguish the rank categories: “1” identical to the corpus string, “2” identical to the corpus string ignoring punctuation, “3” small edit distance (&lt; 4) to the corpus string ignoring punctuation, “4” different from the corpus sentence. In one of our experiments (Section 5.1), we used the rank category “5” to explicitly label the surface realisations derived from the alternation f-structure that does not correspond to the par</context>
</contexts>
<marker>Cahill, Forst, Rohrer, 2007</marker>
<rawString>Aoife Cahill, Martin Forst, and Christian Rohrer. 2007. Stochastic realisation ranking for a free word order language. In Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 17–24, Saarbr¨ucken, Germany, June. DFKI GmbH. Document D-07-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Tracy Holloway King</author>
</authors>
<title>Semantics via F-Structure Rewriting.</title>
<date>2006</date>
<booktitle>Proceedings of the LFG06 Conference.</booktitle>
<editor>In Miriam Butt and Tracy Holloway King, editors,</editor>
<contexts>
<context position="11791" citStr="Crouch and King, 2006" startWordPosition="1764" endWordPosition="1767">ing the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the same declarative grammar in the parsing and generation direction. To obtain a more abstract underlying representation (in the pipeline on the right-hand side of Figure 1), the present work uses an additional semantic construction component (Crouch and King, 2006; Zarrieß, 2009) to map LFG f-structures to meaning representations. For the reverse direction, the meaning representations are mapped to f-structures which can then be mapped to surface strings by the XLE generator (Zarrieß and Kuhn, 2010). For the final realisation ranking step in both pipelines, we used SVMrank, a Support Vector Machine-based learning tool (Joachims, 1996). The ranking step is thus technically independent from the LFG-based component. However, the grammar is used to produce the training data, pairs of corpus sentences and the possible alternations. The two pipelines allow u</context>
</contexts>
<marker>Crouch, King, 2006</marker>
<rawString>Dick Crouch and Tracy Holloway King. 2006. Semantics via F-Structure Rewriting. In Miriam Butt and Tracy Holloway King, editors, Proceedings of the LFG06 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Paula Newman</author>
</authors>
<title>XLE Documentation.</title>
<date>2006</date>
<tech>Technical report,</tech>
<location>Palo Alto Research Center, CA.</location>
<contexts>
<context position="11488" citStr="Crouch et al., 2006" startWordPosition="1715" endWordPosition="1718"> set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3The choice among alternative f-structures is done with a discriminative model (Forst, 2007). Snty SVM Ranker Sntb, Snta, Sntaz... Sntb, LFG Grammar FSa Reverse Sem. Rules LFG Grammar Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the same declarative grammar in the parsing and generation direction. To obtain a more abstract underlying representation (in the pipeline on the right-hand side of Figure 1), the present work uses an additional semantic construction component (Crouch and King, 2006; Zarrieß, 2009) to map LFG f-structures to meaning representations. For the reverse direction, the meaning representations are mapped to f-structures which can then be mapped to surface strings by the XLE generator (Zarrieß and Kuhn, 2010). For the final realisation ranking step in both pipeline</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2006</marker>
<rawString>Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King, John Maxwell, and Paula Newman. 2006. XLE Documentation. Technical report, Palo Alto Research Center, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
</authors>
<title>Graded Word Sense Assignment.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>440--449</pages>
<contexts>
<context position="33938" citStr="Erk and McCarthy, 2009" startWordPosition="5365" endWordPosition="5368"> native speakers of German, 5 had a linguistic background. The participants were asked to rank each sentence on a scale from 1-6 according to its naturalness and plausibility in the given context. The participants were explicitly allowed to use the same rank for sentences they find equally natural. The participants made heavy use of this option: out of the 192 annotated items, only 8 are ranked such that no two sentences have the same rank. We compare the human judgements by correlat1014 ing them with Spearman’s p. This measure is considered appropriate for graded annotation tasks in general (Erk and McCarthy, 2009), and has also been used for analysing human realisation rankings (Velldal, 2008; Cahill and Forst, 2010). We normalise the ranks according to the procedure in Velldal (2008). In Table 6, we report the correlations obtained from averaging over all pairwise correlations between the participants and the correlations restricted to the item and sentence classes. We used bootstrap re-sampling on the pairwise correlations to test that the correlations on the different item classes significantly differ from each other. The correlations in Table 6 suggest that the agreement between annotators is highe</context>
</contexts>
<marker>Erk, McCarthy, 2009</marker>
<rawString>Katrin Erk and Diana McCarthy. 2009. Graded Word Sense Assignment. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 440 – 449, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Generating constituent order in German clauses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 07),</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2604" citStr="Filippova and Strube, 2007" startWordPosition="382" endWordPosition="385">). realisation ranking system. Such ranking systems are practically relevant for the real-world application of grammar-based generators that usually generate several grammatical surface sentences from a given abstract input, e.g. (Velldal and Oepen, 2006). Moreover, this framework allows for detailed experimental studies of the interaction of specific linguistic features. Thus it has been demonstrated that for free word order languages like German, word order prediction quality can be improved with carefully designed, linguistically informed models capturing information-structural strategies (Filippova and Strube, 2007; Cahill and Riester, 2009). This paper is situated in the same framework, using rich linguistic representations over corpus data for machine learning of realisation ranking. However, we go beyond the task of finding the correct ordering for an almost fixed set of word forms. Quite obviously, word order is only one of the means at a speaker’s disposal for expressing some content in a contextually appropriate form; we add systematic alternations like the voice alternation (active vs. passive) to the picture. As an alternative way of promoting or demoting the prominence of a syntactic argument, </context>
<context position="9772" citStr="Filippova and Strube (2007)" startWordPosition="1464" endWordPosition="1467"> on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a cor</context>
<context position="28136" citStr="Filippova and Strube (2007)" startWordPosition="4411" endWordPosition="4414">d not be specified at a stage prior to word order. Example (5) is taken from our development set, illustrating a case where the union model predicted the correct voice and word order (5-a), and the scale alignment model top-ranked the incorrect voice and word order. The active verb arguments in (5-b) are both case-ambigous and placed in the non-canonical order (object &lt; subject), so the semantic relation can be easily misunderstood. The passive in (5-a) is unambiguous since the agent is realised in a PP (and placed in the Vorfeld). deutschen Medien. German media. Moreover, our results confirm Filippova and Strube (2007) who find that it is harder to predict the correct Vorfeld occupant in a German sentence, than to predict the relative order of the constituents. 5.3 Exp. 3: Capturing Flexible Variation The previous experiment has shown that there is a certain inter-dependence between word order and voice. This experiment addresses this interaction by varying the way the training data for the ranker is labelled. We contrast two ways of labelling the sentences (see Section 4): a) all sentences that are not (nearly) identical to the reference sentence have the rank category “4”, irrespective of their voice (ref</context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Generating constituent order in German clauses. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 07), Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Tree linearization in English: Improving language model based approaches.</title>
<date>2009</date>
<booktitle>In Companion Volume to the Proceedings of Human Language Technologies Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 09,</booktitle>
<location>short)., Boulder, Colorado.</location>
<contexts>
<context position="7236" citStr="Filippova and Strube, 2009" startWordPosition="1076" endWordPosition="1079">valuation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related </context>
</contexts>
<marker>Filippova, Strube, 2009</marker>
<rawString>Katja Filippova and Michael Strube. 2009. Tree linearization in English: Improving language model based approaches. In Companion Volume to the Proceedings of Human Language Technologies Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 09, short)., Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
</authors>
<title>Filling Statistics with Linguistics – Property Design for the Disambiguation of German LFG Parses.</title>
<date>2007</date>
<booktitle>In ACL 2007 Workshop on Deep Linguistic Processing,</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11023" citStr="Forst, 2007" startWordPosition="1650" endWordPosition="1651">ing-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3The choice among alternative f-structures is done with a discriminative model (Forst, 2007). Snty SVM Ranker Sntb, Snta, Sntaz... Sntb, LFG Grammar FSa Reverse Sem. Rules LFG Grammar Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the same declarative grammar in the parsing and generation direction. To obtain a more abstract un</context>
</contexts>
<marker>Forst, 2007</marker>
<rawString>Martin Forst. 2007. Filling Statistics with Linguistics – Property Design for the Disambiguation of German LFG Parses. In ACL 2007 Workshop on Deep Linguistic Processing, pages 17–24, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Chai</author>
</authors>
<title>Beyond nombank: A study of implicit argumentation for nominal predicates.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</booktitle>
<contexts>
<context position="14671" citStr="Gerber and Chai (2010)" startWordPosition="2225" endWordPosition="2228">ntation for Sentence (4) in Figure 2, the realiser will not generate an active realisation since the agent role cannot be instantiated by any phrase in the grammar. However, depending on the exact context there are typically options for realising the subject phrase in an active with very little descriptive content. Ideally, one would like to account for these phenomena in a meaning representation that underspecifies the lexicalisation of discourse referents, and also captures the reference of implicit arguments. Especially the latter task has hardly been addressed in NLP applications (but see Gerber and Chai (2010)). In order to work around that problem, we implemented some simple heuristics which underspecify the realisation of certain verb arguments. These rules define: 1. a set of pronouns (generic and neutral pronouns, universal quantifiers) that correspond to “trivial” agents in active and implicit agents Active Passive 2-role trans. 71% (82%) 10% (2%) 1-role trans. 11% (0%) 8% (16%) Table 1: Distribution of voices in SEM� (SEMS,) in passive sentences; 2. a set of prepositional adjuncts in passive sentences that correspond to subjects in active sentence (e.g. causative and instrumental prepositions</context>
</contexts>
<marker>Gerber, Chai, 2010</marker>
<rawString>Matthew Gerber and Joyce Chai. 2010. Beyond nombank: A study of implicit argumentation for nominal predicates. In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="20971" citStr="Grosz et al., 1995" startWordPosition="3231" endWordPosition="3234">nt” features (ScalAl.): combinations of voice and role properties with morphological properties, e.g. “subject is singular”, “agent is 3rd person in active voice” (these are surface-independent, identical for each alternation candidate). The model for which we present our results is based on sentence-internal features only; as Cahill and Riester (2009) showed, these feature carry a considerable amount of implicit information about the discourse context (e.g. in the shape of referring expressions). We also implemented a set of explicitly inter-sentential features, inspired by Centering Theory (Grosz et al., 1995). This model did not improve over the intra-sentential model. Evaluation Measures In order to assess the general quality of our generation ranking models, we 4The language model is trained on the German data release for the 2009 ACL Workshop on Machine Translation shared task, 11,991,277 total sentences. 1011 FS SEMn SEMh Avg. # strings 36.7 68.2 75.8 Random Match 16.98 10.72 7.28 Match 15.45 15.04 11.89 LM BLEU 0.68 0.68 0.65 NIST 13.01 12.95 12.69 Match 27.91 27.66 26.38 Ling. Model BLEU 0.764 0.759 0.747 NIST 13.18 13.14 13.01 Table 2: Evaluation of Experiment 1 use several standard measure</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>1996</date>
<booktitle>Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), CSLI Proceedings Online.</booktitle>
<editor>In M. Butt and T. H. King, editors,</editor>
<contexts>
<context position="12169" citStr="Joachims, 1996" startWordPosition="1823" endWordPosition="1824">in the parsing and generation direction. To obtain a more abstract underlying representation (in the pipeline on the right-hand side of Figure 1), the present work uses an additional semantic construction component (Crouch and King, 2006; Zarrieß, 2009) to map LFG f-structures to meaning representations. For the reverse direction, the meaning representations are mapped to f-structures which can then be mapped to surface strings by the XLE generator (Zarrieß and Kuhn, 2010). For the final realisation ranking step in both pipelines, we used SVMrank, a Support Vector Machine-based learning tool (Joachims, 1996). The ranking step is thus technically independent from the LFG-based component. However, the grammar is used to produce the training data, pairs of corpus sentences and the possible alternations. The two pipelines allow us to vary the degree to which the generation input is underspecified. An fstructure abstracts away from word order, i.e. the candidate set will contain just word order alternations. In the semantic input, syntactic function and voice are underspecified, so a larger set of surface realisation candidates is generated. Figure 2 illustrates the two representation levels for an ac</context>
</contexts>
<marker>Joachims, 1996</marker>
<rawString>Thorsten Joachims. 1996. Training linear svms in linear time. In M. Butt and T. H. King, editors, Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), CSLI Proceedings Online.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Kuhn</author>
</authors>
<title>Optimality-Theoretic Syntax—A Declarative Approach.</title>
<date>2003</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="10929" citStr="Kuhn, 2003" startWordPosition="1637" endWordPosition="1638">ents: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3The choice among alternative f-structures is done with a discriminative model (Forst, 2007). Snty SVM Ranker Sntb, Snta, Sntaz... Sntb, LFG Grammar FSa Reverse Sem. Rules LFG Grammar Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the </context>
</contexts>
<marker>Kuhn, 2003</marker>
<rawString>Jonas Kuhn. 2003. Optimality-Theoretic Syntax—A Declarative Approach. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACL/COLING-98,</booktitle>
<pages>704--710</pages>
<location>Montreal, Quebec.</location>
<contexts>
<context position="6781" citStr="Langkilde and Knight, 1998" startWordPosition="1008" endWordPosition="1012">es taking the risk of occasional overgeneration. The paper is structured as follows: Section 2 situates our methodology with respect to other work on surface realisation and briefly summarises the relevant theoretical linguistic background. In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich lingui</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proceedings of the ACL/COLING-98, pages 704–710, Montreal, Quebec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasz Marciniak</author>
<author>Michael Strube</author>
</authors>
<title>Using an annotated corpus as a knowledge source for language generation.</title>
<date>2005</date>
<booktitle>In Proceedings of Workshop on Using Corpora for Natural Language Generation,</booktitle>
<pages>pages</pages>
<location>Birmingham, UK.</location>
<contexts>
<context position="7034" citStr="Marciniak and Strube, 2005" startWordPosition="1046" endWordPosition="1049"> we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et </context>
</contexts>
<marker>Marciniak, Strube, 2005</marker>
<rawString>Tomasz Marciniak and Michael Strube. 2005. Using an annotated corpus as a knowledge source for language generation. In Proceedings of Workshop on Using Corpora for Natural Language Generation, pages 19–24, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Junichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWPT</booktitle>
<contexts>
<context position="31617" citStr="Nakanishi et al., 2005" startWordPosition="4987" endWordPosition="4990"> sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6(Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not trained on enough data. The corpus used for training our LM might also have been too small or distinct in genre. best sentences rather than only the top-ranked sentence. This means that the model sometimes predicts almost equal naturalness for different voice realisations. Moreover, in the case of word order, we know from previous evaluation studies, that humans sometimes prefer different realisations than the original corpus sentences. This Section investigates agreement in human judgements of</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Junichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings ofIWPT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL 2000,</booktitle>
<pages>194--201</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="7006" citStr="Ratnaparkhi, 2000" startWordPosition="1044" endWordPosition="1045">ound. In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Adwait Ratnaparkhi. 2000. Trainable methods for surface natural language generation. In Proceedings of NAACL 2000, pages 194–201, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting ofthe Association for Computational Linguistics (ACL’00), Hong Kong,</booktitle>
<pages>480--487</pages>
<contexts>
<context position="7446" citStr="Riezler et al., 2000" startWordPosition="1105" endWordPosition="1108">rd lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not direc</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proceedings of the 38th Annual Meeting ofthe Association for Computational Linguistics (ACL’00), Hong Kong, pages 480–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Dick Crouch</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<location>Pennsylvania, Philadelphia.</location>
<contexts>
<context position="7469" citStr="Riezler et al., 2002" startWordPosition="1109" endWordPosition="1112">e realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In mult</context>
</contexts>
<marker>Riezler, Crouch, Kaplan, King, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King, John Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric K Ringger</author>
<author>Michael Gamon</author>
<author>Robert C Moore</author>
<author>David Rojas</author>
<author>Martine Smets</author>
<author>Simon CorstonOliver</author>
</authors>
<title>Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 International Conference on Computational Linguistics,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7207" citStr="Ringger et al., 2004" startWordPosition="1072" endWordPosition="1075">sults from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work </context>
</contexts>
<marker>Ringger, Gamon, Moore, Rojas, Smets, CorstonOliver, 2004</marker>
<rawString>Eric K. Ringger, Michael Gamon, Robert C. Moore, David Rojas, Martine Smets, and Simon CorstonOliver. 2004. Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization. In Proceedings of the 2004 International Conference on Computational Linguistics, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Rohrer</author>
<author>Martin Forst</author>
</authors>
<title>Improving coverage and parsing quality of a large-scale LFG for German.</title>
<date>2006</date>
<booktitle>In Proceedings ofLREC-2006.</booktitle>
<contexts>
<context position="10720" citStr="Rohrer and Forst, 2006" startWordPosition="1606" endWordPosition="1609">elate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3The choice among alternative f-structures is done with a discriminative model (Forst, 2007). Snty SVM Ranker Sntb, Snta, Sntaz... Sntb, LFG Grammar FSa Reverse Sem. Rules LFG Grammar Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic f</context>
</contexts>
<marker>Rohrer, Forst, 2006</marker>
<rawString>Christian Rohrer and Martin Forst. 2006. Improving coverage and parsing quality of a large-scale LFG for German. In Proceedings ofLREC-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Statistical ranking in tactical generation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2233" citStr="Velldal and Oepen, 2006" startWordPosition="329" endWordPosition="332">by the Parliament. c. It wasn’t until June that it was approved. We address the problem of predicting the usage of linguistic alternations in the framework of a surface 1This work has been supported by the Deutsche Forschungsgemeinschaft (DFG; German Research Foundation) in SFB 732 Incremental specification in context, project D2 (PIs: Jonas Kuhn and Christian Rohrer). realisation ranking system. Such ranking systems are practically relevant for the real-world application of grammar-based generators that usually generate several grammatical surface sentences from a given abstract input, e.g. (Velldal and Oepen, 2006). Moreover, this framework allows for detailed experimental studies of the interaction of specific linguistic features. Thus it has been demonstrated that for free word order languages like German, word order prediction quality can be improved with carefully designed, linguistically informed models capturing information-structural strategies (Filippova and Strube, 2007; Cahill and Riester, 2009). This paper is situated in the same framework, using rich linguistic representations over corpus data for machine learning of realisation ranking. However, we go beyond the task of finding the correct </context>
<context position="7622" citStr="Velldal and Oepen, 2006" startWordPosition="1132" endWordPosition="1135">i, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However</context>
</contexts>
<marker>Velldal, Oepen, 2006</marker>
<rawString>Erik Velldal and Stephan Oepen. 2006. Statistical ranking in tactical generation. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
</authors>
<title>Empirical Realization Ranking.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oslo, Department of Informatics.</institution>
<contexts>
<context position="7861" citStr="Velldal (2008)" startWordPosition="1171" endWordPosition="1172">ability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these annotations are not suitable for full generation since they are often incomplete. Thus, it is not clear to which degree these annotations are actually underspecified for certain paraphrases. 2.2 Linguistic Background</context>
<context position="30881" citStr="Velldal (2008)" startWordPosition="4870" endWordPosition="4871">en looking at the top 3 sentences. Table 5 also reports the performance of an unlabelled model that additionally integrates LM scores. Surprisingly, these scores have a very small positive effect on the sentence overlap features and no positive effect on the voice and precedence accuracy. The n-best evaluations even suggest that the LM scores negatively impact the ranker: the accuracy for the top 3 sentences increases much less as compared to the model that does not integrate LM scores.6 The n-best performance of a realisation ranker is practically relevant for re-ranking applications such as Velldal (2008). We think that it is also conceptually interesting. Previous evaluation studies suggest that the original corpus sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown th</context>
<context position="34018" citStr="Velldal, 2008" startWordPosition="5379" endWordPosition="5380">rank each sentence on a scale from 1-6 according to its naturalness and plausibility in the given context. The participants were explicitly allowed to use the same rank for sentences they find equally natural. The participants made heavy use of this option: out of the 192 annotated items, only 8 are ranked such that no two sentences have the same rank. We compare the human judgements by correlat1014 ing them with Spearman’s p. This measure is considered appropriate for graded annotation tasks in general (Erk and McCarthy, 2009), and has also been used for analysing human realisation rankings (Velldal, 2008; Cahill and Forst, 2010). We normalise the ranks according to the procedure in Velldal (2008). In Table 6, we report the correlations obtained from averaging over all pairwise correlations between the participants and the correlations restricted to the item and sentence classes. We used bootstrap re-sampling on the pairwise correlations to test that the correlations on the different item classes significantly differ from each other. The correlations in Table 6 suggest that the agreement between annotators is highest on the false items, and lowest on the mixed items. Humans tended to give the </context>
</contexts>
<marker>Velldal, 2008</marker>
<rawString>Erik Velldal. 2008. Empirical Realization Ranking. Ph.D. thesis, University of Oslo, Department of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sina ZarrieB</author>
<author>Jonas Kuhn</author>
</authors>
<title>Reversing Fstructure Rewriting for Generation from Meaning Representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the LFG10 Conference,</booktitle>
<location>Ottawa.</location>
<marker>ZarrieB, Kuhn, 2010</marker>
<rawString>Sina ZarrieB and Jonas Kuhn. 2010. Reversing Fstructure Rewriting for Generation from Meaning Representations. In Proceedings of the LFG10 Conference, Ottawa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sina ZarrieB</author>
</authors>
<title>Developing German Semantics on the basis of Parallel LFG Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<marker>ZarrieB, 2009</marker>
<rawString>Sina ZarrieB. 2009. Developing German Semantics on the basis of Parallel LFG Grammars. In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF 2009), pages 10–18, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>