<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9744445">
Fast and Robust Compressive Summarization
with Dual Decomposition and Multi-Task Learning
</title>
<note confidence="0.458663333333333">
Miguel B. Almeida*† Andr´e F. T. Martins*†
*Priberam Labs, Alameda D. Afonso Henriques, 41, 2°, 1000-123 Lisboa, Portugal
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal
</note>
<email confidence="0.990452">
{mba,atm}@priberam.pt
</email>
<sectionHeader confidence="0.994536" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982235294118">
We present a dual decomposition frame-
work for multi-document summarization,
using a model that jointly extracts and
compresses sentences. Compared with
previous work based on integer linear pro-
gramming, our approach does not require
external solvers, is significantly faster, and
is modular in the three qualities a sum-
mary should have: conciseness, informa-
tiveness, and grammaticality. In addition,
we propose a multi-task learning frame-
work to take advantage of existing data
for extractive summarization and sentence
compression. Experiments in the TAC-
2008 dataset yield the highest published
ROUGE scores to date, with runtimes that
rival those of extractive summarizers.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845254237288">
Automatic text summarization is a seminal prob-
lem in information retrieval and natural language
processing (Luhn, 1958; Baxendale, 1958; Ed-
mundson, 1969). Today, with the overwhelming
amount of information available on the Web, the
demand for fast, robust, and scalable summariza-
tion systems is stronger than ever.
Up to now, extractive systems have been the
most popular in multi-document summarization.
These systems produce a summary by extracting
a representative set of sentences from the origi-
nal documents (Kupiec et al., 1995; Carbonell and
Goldstein, 1998; Radev et al., 2000; Gillick et al.,
2008). This approach has obvious advantages: it
reduces the search space by letting decisions be
made for each sentence as a whole (avoiding fine-
grained text generation), and it ensures a grammat-
ical summary, assuming the original sentences are
well-formed. The typical trade-offs in these mod-
els (maximizing relevance, and penalizing redun-
dancy) lead to submodular optimization problems
(Lin and Bilmes, 2010), which are NP-hard but ap-
proximable through greedy algorithms; learning is
possible with standard structured prediction algo-
rithms (Sipos et al., 2012; Lin and Bilmes, 2012).
Probabilistic models have also been proposed to
capture the problem structure, such as determinan-
tal point processes (Gillenwater et al., 2012).
However, extractive systems are rather limited
in the summaries they can produce. Long, partly
relevant sentences tend not to appear in the sum-
mary, or to block the inclusion of other sen-
tences. This has motivated research in compres-
sive summarization (Lin, 2003; Zajic et al., 2006;
Daum´e, 2006), where summaries are formed by
compressed sentences (Knight and Marcu, 2000),
not necessarily extracts. While promising results
have been achieved by models that simultaneously
extract and compress (Martins and Smith, 2009;
Woodsend and Lapata, 2010; Berg-Kirkpatrick et
al., 2011), there are still obstacles that need to
be surmounted for these systems to enjoy wide
adoption. All approaches above are based on in-
teger linear programming (ILP), suffering from
slow runtimes, when compared to extractive sys-
tems. For example, Woodsend and Lapata (2012)
report 55 seconds on average to produce a sum-
mary; Berg-Kirkpatrick et al. (2011) report sub-
stantially faster runtimes, but fewer compressions
are allowed. Having a compressive summarizer
which is both fast and expressive remains an open
problem. A second inconvenience of ILP-based
approaches is that they do not exploit the modu-
larity of the problem, since the declarative specifi-
cation required by ILP solvers discards important
structural information. For example, such solvers
are unable to take advantage of efficient dynamic
programming routines for sentence compression
(McDonald, 2006).
</bodyText>
<page confidence="0.984281">
196
</page>
<note confidence="0.9157705">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 196–206,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.966026">
This paper makes progress in two fronts:
</bodyText>
<listItem confidence="0.944708769230769">
• We derive a dual decomposition framework for
extractive and compressive summarization (§2–
3). Not only is this framework orders of mag-
nitude more efficient than the ILP-based ap-
proaches, it also allows the three well-known
metrics of summaries—conciseness, informa-
tiveness, and grammaticality—to be treated sep-
arately in a modular fashion (see Figure 1). We
also contribute with a novel knapsack factor,
along with a linear-time algorithm for the corre-
sponding dual decomposition subproblem.
• We propose multi-task learning (§4) as a prin-
cipled way to train compressive summarizers,
</listItem>
<bodyText confidence="0.846882888888889">
using auxiliary data for extractive summariza-
tion and sentence compression. To this end,
we adapt the framework of Evgeniou and Pon-
til (2004) and Daum´e (2007) to train structured
predictors that share some of their parts.
Experiments on TAC data (§5) yield state-of-the-
art results, with runtimes similar to that of extrac-
tive systems. To our best knowledge, this had
never been achieved by compressive summarizers.
</bodyText>
<sectionHeader confidence="0.995292" genericHeader="method">
2 Extractive Summarization
</sectionHeader>
<bodyText confidence="0.999743923076923">
In extractive summarization, we are given a set
of sentences D := {s1, ... , sN} belonging to one
or more documents, and the goal is to extract a
subset S ⊆ D that conveys a good summary of D
and whose total number of words does not exceed
a prespecified budget B.
We use an indicator vector y := hyniNn=1 to rep-
resent an extractive summary, where yn = 1 if
sn ∈ S, and yn = 0 otherwise. Let Ln be the
number of words of the nth sentence. By design-
ing a quality score function g : {0,1}N → R, this
can be cast as a global optimization problem with
a knapsack constraint:
</bodyText>
<equation confidence="0.964347">
maximize g(y)
w.r.t. y ∈ {0,1}N
s.t. ENn=1 Lnyn ≤ B. (1)
</equation>
<bodyText confidence="0.9996275">
Intuitively, a good summary is one which selects
sentences that individually convey “relevant” in-
formation, while collectively having small “re-
dundancy.” This trade-off was explicitly mod-
eled in early works through the notion of max-
imal marginal relevance (Carbonell and Gold-
stein, 1998; McDonald, 2007). An alternative
are coverage-based models (§2.1; Filatova and
Hatzivassiloglou, 2004; Yih et al., 2007; Gillick
et al., 2008), which seek a set of sentences that
covers as many diverse “concepts” as possible; re-
dundancy is automatically penalized since redun-
dant sentences cover fewer concepts. Both models
can be framed under the framework of submodular
optimization (Lin and Bilmes, 2010), leading to
greedy algorithms that have approximation guar-
antees. However, extending these models to allow
for sentence compression (as will be detailed in
§3) breaks the diminishing returns property, mak-
ing submodular optimization no longer applicable.
</bodyText>
<subsectionHeader confidence="0.901827">
2.1 Coverage-Based Summarization
</subsectionHeader>
<bodyText confidence="0.998696125">
Coverage-based extractive summarization can be
formalized as follows. Let C(D) := {c1, ... , cM}
be a set of relevant concept types which are
present in the original documents D.1 Let σm be a
relevance score assigned to the mth concept, and
let the set Jm ⊆ {1, ... , N} contain the indices of
the sentences in which this concept occurs. Then,
the following quality score function is defined:
</bodyText>
<equation confidence="0.997928">
g(y) = EMm=1 σmum(y), (2)
</equation>
<bodyText confidence="0.99986225">
where um(y) := Vn∈Jm yn is a Boolean function
that indicates whether the mth concept is present
in the summary. Plugging this into Eq. 1, one ob-
tains the following Boolean optimization problem:
</bodyText>
<equation confidence="0.9572275">
maximize EMm=1 σmum
w.r.t. y ∈ {0,1}N, u ∈ {0,1}M
s.t. um = V n∈Jm yn, ∀m ∈ [M]
ENn=1 Lnyn ≤ B, (3)
</equation>
<bodyText confidence="0.9999707">
where we used the notation [M] := {1, ... , M}.
This can be converted into an ILP and addressed
with off-the-shelf solvers (Gillick et al., 2008). A
drawback of this approach is that solving an ILP
exactly is NP-hard. Even though existing commer-
cial solvers can solve most instances with a mod-
erate speed, they still exhibit poor worst-case be-
haviour; this is exacerbated when there is the need
to combine an extractive component with other
modules, as in compressive summarization (§3).
</bodyText>
<footnote confidence="0.999511666666667">
1Previous work has modeled concepts as events (Filatova
and Hatzivassiloglou, 2004), salient words (Lin and Bilmes,
2010), and word bigrams (Gillick et al., 2008). In the sequel,
we assume concepts are word k-grams, but our model can
handle other representations, such as phrases or predicate-
argument structures.
</footnote>
<page confidence="0.994793">
197
</page>
<subsectionHeader confidence="0.996757">
2.2 A Dual Decomposition Formulation
</subsectionHeader>
<bodyText confidence="0.999772592592593">
We next describe how the problem in Eq. 3 can be
addressed with dual decomposition, a class of op-
timization techniques that tackle the dual of com-
binatorial problems in a modular, extensible, and
parallelizable manner (Komodakis et al., 2007;
Rush et al., 2010). In particular, we employ al-
ternating directions dual decomposition (AD3;
Martins et al., 2011a, 2012) for solving a linear re-
laxation of Eq. 3. AD3 resembles the subgradient-
based algorithm of Rush et al. (2010), but it enjoys
a faster convergence rate. Both algorithms split
the original problem into several components,
and then iterate between solving independent lo-
cal subproblems at each component and adjusting
multipliers to promote an agreement.2 The differ-
ence between the two methods is that the AD3 lo-
cal subproblems, instead of requiring the compu-
tation of a locally optimal configuration, require
solving a local quadratic problem. Martins et al.
(2011b) provided linear-time solutions for several
logic constraints, with applications to syntax and
frame-semantic parsing (Das et al., 2012). We will
see that AD3 can also handle budget and knapsack
constraints efficiently.
To tackle Eq. 3 with dual decomposition, we
split the coverage-based summarizer into the fol-
lowing M + 1 components (one per constraint):
</bodyText>
<listItem confidence="0.989101272727273">
1. For each of the M concepts in C(D), one
component for imposing the logic constraint
in Eq. 3. This corresponds to the OR-WITH-
OUTPUT factor described by Martins et al.
(2011b); the AD3 subproblem for the mth factor
can be solved in time O(|Im|).
2. Another component for the knapsack con-
straint. This corresponds to a (novel) KNAP-
SACK factor, whose AD3 subproblem is solv-
able in time O(N). The actual algorithm is de-
scribed in the appendix (Algorithm 1).3
</listItem>
<sectionHeader confidence="0.977638" genericHeader="method">
3 Compressive Summarization
</sectionHeader>
<bodyText confidence="0.975901333333333">
We now turn to compressive summarization,
which does not limit the summary sentences to be
verbatim extracts from the original documents; in-
</bodyText>
<footnote confidence="0.897122">
2For details about dual decomposition and Lagrangian re-
laxation, see the recent tutorial by Rush and Collins (2012).
3The AD3 subproblem in this case corresponds to com-
puting an Euclidean projection onto the knapsack polytope
(Eq. 11). Others addressed the related, but much harder, inte-
ger quadratic knapsack problem (McDonald, 2007).
</footnote>
<bodyText confidence="0.982552304347826">
stead, it allows the extraction of compressed sen-
tences where some words can be deleted.
Formally, let us express each sentence of D
as a sequence of word tokens, sn := htn,`iLn
`=0,
where tn,0 ≡ $ is a dummy symbol. We rep-
resent a compression of sn as an indicator vec-
tor zn := hzn,`iLn `=0, where zn,` = 1 if the Eth
word is included in the compression. By conven-
tion, the dummy symbol is included if and only if
the remaining compression is non-empty. A com-
pressive summary can then be represented by an
indicator vector z which is the concatenation of
N such vectors, z = hz1,... , zNi; each position
in this indicator vector is indexed by a sentence
n ∈ [N] and a word position E ∈ {0} ∪ [Ln].
Models for compressive summarization were
proposed by Martins and Smith (2009) and Berg-
Kirkpatrick et al. (2011) by combining extraction
and compression scores. Here, we follow the lat-
ter work, by combining a coverage score function
g with sentence-level compression score functions
h1, ... , hN. This yields the decoding problem:
</bodyText>
<equation confidence="0.9913168">
maximize g(z) + PNn=1 hn(zn)
w.r.t. zn ∈ {0,1}Ln, ∀n ∈ [N]
s.t. PN PLn
`=1 zn,` ≤ B. (4)
n=1
</equation>
<subsectionHeader confidence="0.981387">
3.1 Coverage Model
</subsectionHeader>
<bodyText confidence="0.999959">
We use a coverage function similar to Eq. 2, but
taking a compressive summary z as argument:
</bodyText>
<equation confidence="0.989748">
g(z) = PMm=1 Qmum(z), (5)
</equation>
<bodyText confidence="0.9999468">
where we redefine um as follows. First, we
parametrize each occurrence of the mth concept
(assumed to be a k-gram) as a triple hn, Es, Eei,
where n indexes a sentence, Es indexes a start po-
sition within the sentence, and Ee indexes the end
position. We denote by Tm the set of triples repre-
senting all occurrences of the mth concept in the
original text, and we associate an indicator vari-
able zn,`s:`e to each member of this set. We then
define um(z) via the following logic constraints:
</bodyText>
<listItem confidence="0.9608684">
• A concept type is selected if some of its k-gram
tokens are selected:
um(y) := W (n,`s,`e)∈Tm zn,`s:`e. (6)
• A k-gram concept token is selected if all its
words are selected:
</listItem>
<equation confidence="0.8282805">
zn,`s:`e := V`e
`=`s zn,`. (7)
</equation>
<page confidence="0.989996">
198
</page>
<figureCaption confidence="0.999676">
Figure 1: Components of our compressive summarizer. Factors depicted in blue belong to the compres-
sion model, and aim to enforce grammaticality. The logic factors in red form the coverage component.
Finally, the budget factor, in green, is connected to the word nodes; it ensures that the summary fits the
word limit. Shaded circles represent active variables while white circles represent inactive variables.
</figureCaption>
<figure confidence="0.944471857142857">
Concept type
Budget
&amp;quot;Kashmiri separatists&amp;quot;
Concept tokens
$ Talks with Kashmiri separatists began last year ...
Sentences
$ The leader of moderate Kashmiri separatists warned Thursday that ...
</figure>
<bodyText confidence="0.999603666666667">
We set concept scores as Qm := w · Φcov(D, cm),
where Φcov(D, cm) is a vector of features (de-
scribed in §3.5) and w the corresponding weights.
</bodyText>
<subsectionHeader confidence="0.984295">
3.2 Compression Model
</subsectionHeader>
<bodyText confidence="0.995943">
For the compression score function, we follow
Martins and Smith (2009) and decompose it as a
sum of local score functions Pn,` defined on de-
pendency arcs:
</bodyText>
<equation confidence="0.9997965">
hn(zn) := �Ln
`=1 Pn,`(zn,`, zn,π(`)), (8)
</equation>
<bodyText confidence="0.999988866666667">
where 7r(E) denotes the index of the word which
is the parent of the Eth word in the dependency
tree (by convention, the root of the tree is the
dummy symbol). To model the event that an
arc is “cut” by disconnecting a child from its
head, we define arc-deletion scores Pn,`(0,1) :=
w · Φcomp(sn, E, 7r(E)), where Φcomp is a feature
map, which is described in detail in §3.5. We set
Pn,`(0, 0) = Pn,`(1,1) = 0, and Pn,`(1, 0) = −∞,
to allow only the deletion of entire subtrees.
A crucial fact is that one can maximize Eq. 8
efficiently with dynamic programming (using the
Viterbi algorithm for trees); the total cost is linear
in Ln. We will exploit this fact in the dual decom-
position framework described next.4
</bodyText>
<subsectionHeader confidence="0.998617">
3.3 A Dual Decomposition Formulation
</subsectionHeader>
<bodyText confidence="0.9997258">
In previous work, the optimization problem in
Eq. 4 was converted into an ILP and fed to an off-
the-shelf solver (Martins and Smith, 2009; Berg-
Kirkpatrick et al., 2011; Woodsend and Lapata,
2012). Here, we employ the AD3 algorithm, in a
</bodyText>
<footnote confidence="0.57924">
4The same framework can be readily adapted to other
compression models that are efficiently decodable, such as
the semi-Markov model of McDonald (2006), which would
allow incorporating a language model for the compression.
</footnote>
<bodyText confidence="0.935182777777778">
similar manner as described in §2, but with an ad-
ditional component for the sentence compressor,
and slight modifications in the other components.
We have the following N + M + EMm=1 |Tm |+ 1
components in total, illustrated in Figure 1:
1. For each of the N sentences, one component
for the compression model. The AD3 quadratic
subproblem for this factor can be addressed by
solving a sequence of linear subproblems, as de-
scribed by Martins et al. (2012). Each of these
subproblems corresponds to maximizing an ob-
jective function of the same form as Eq. 8; this
can be done in O(Ln) time with dynamic pro-
gramming, as discussed in §3.2.
2. For each of the M concept types in C(D),
one OR-WITH-OUTPUT factor for the logic con-
straint in Eq. 6. This is analogous to the one
described for the extractive case.
</bodyText>
<listItem confidence="0.9459123">
3. For each k-gram concept token in Tm, one
AND-WITH-OUTPUT factor that imposes the
constraint in Eq. 7. This factor was described
by Martins et al. (2011b) and its AD3 subprob-
lem can be solved in time linear in k.
4. Another component linked to all the words im-
posing that at most B words can be selected;
this is done via a BUDGET factor, a particular
case of KNAPSACK. The runtime of this AD3
subproblem is linear in the number of words.
</listItem>
<bodyText confidence="0.9793285">
In addition, we found it useful to add a second
BUDGET factor limiting the number of sentences
that can be selected to a prescribed value K. We
set K = 6 in our experiments.
</bodyText>
<page confidence="0.993634">
199
</page>
<subsectionHeader confidence="0.997413">
3.4 Rounding Strategy
</subsectionHeader>
<bodyText confidence="0.999961153846154">
Recall that the problem in Eq. 4 is NP-hard, and
that AD3 is solving a linear relaxation. While
there are ways of wrapping AD3 in an exact search
algorithm (Das et al., 2012), such strategies work
best when the solution of the relaxation has few
fractional components, which is typical of pars-
ing and translation problems (Rush et al., 2010;
Chang and Collins, 2011), and attractive networks
(Taskar et al., 2004). Unfortunately, this is not the
case in summarization, where concepts “compete”
with each other for inclusion in the summary, lead-
ing to frustrated cycles. We chose instead to adopt
a fast and simple rounding procedure for obtaining
a summary from a fractional solution.
The procedure works as follows. First, solve
the LP relaxation using AD3, as described above.
This yields a solution z*, where each component
lies in the unit interval [0, 1]. If these components
are all integer, then we have a certificate that this
is the optimal solution. Otherwise, we collect the
K sentences with the highest values of z*,,,,0 (“pos-
teriors” on sentences), and seek the feasible sum-
mary which is the closest (in Euclidean distance)
to z*, while only containing those sentences. This
can be computed exactly in time O(B EKk=1 L,,,k),
through dynamic programming.5
</bodyText>
<subsectionHeader confidence="0.658354">
3.5 Features and Hard Constraints
</subsectionHeader>
<bodyText confidence="0.9982245">
As Berg-Kirkpatrick et al. (2011), we used
stemmed word bigrams as concepts, to which we
associate the following concept features (Φcov):
indicators for document counts, features indicat-
ing if each of the words in the bigram is a stop-
word, the earliest position in a document each con-
cept occurs, as well as two and three-way conjunc-
tions of these features.
For the compression model, we include the fol-
lowing arc-deletion features (Φcomp):
</bodyText>
<listItem confidence="0.986256142857143">
• the dependency label of the arc being deleted, as
well as its conjunction with the part-of-speech
tag of the head, of the modifier, and of both;
• the dependency labels of the arc being deleted
and of its parent arc;
• the modifier tag, if the modifier is a function
word modifying a verb ;
</listItem>
<bodyText confidence="0.85813325">
5Briefly, if we link the roots of the K sentences to a super-
root node, the problem above can be transformed into that
of finding the best configuration in the resulting binary tree
subject to a budget constraint. We omit details for space.
</bodyText>
<listItem confidence="0.9746005">
• a feature indicating whether the modifier or any
of its descendants is a negation word;
• indicators of whether the modifier is a temporal
word (e.g., Friday) or a preposition pointing to
</listItem>
<bodyText confidence="0.979295684210527">
a temporal word (e.g., on Friday).
In addition, we included hard constraints to pre-
vent the deletion of certain arcs, following pre-
vious work in sentence compression (Clarke and
Lapata, 2008). We never delete arcs whose de-
pendency label is SUB, OBJ, PMOD, SBAR, VC, or
PRD (this makes sure we preserve subjects and ob-
jects of verbs, arcs departing from prepositions or
complementizers, and that we do not break verb
chains or predicative complements); arcs linking
to a conjunction word or siblings of such arcs (to
prevent inconsistencies in handling coordinative
conjunctions); arcs linking verbs to other verbs,
to adjectives (e.g., make available), to verb parti-
cles (e.g., settle down), to the word that (e.g., said
that), or to the word to if it is a leaf (e.g., allowed
to come); arcs pointing to negation words, cardinal
numbers, or determiners; and arcs connecting two
proper nouns or words within quotation marks.
</bodyText>
<sectionHeader confidence="0.99796" genericHeader="method">
4 Multi-Task Learning
</sectionHeader>
<bodyText confidence="0.99992352">
We next turn to the problem of learning the model
from training data. Prior work in compressive
summarization has followed one of two strategies:
Martins and Smith (2009) and Woodsend and La-
pata (2012) learn the extraction and compression
models separately, and then post-combine them,
circumventing the lack of fully annotated data.
Berg-Kirkpatrick et al. (2011) gathered a small
dataset of manually compressed summaries, and
trained with full supervision. While the latter
approach is statistically more principled, it has
the disadvantage of requiring fully annotated data,
which is difficult to obtain in large quantities. On
the other hand, there is plenty of data contain-
ing manually written abstracts (from the DUC and
TAC conferences) and user-generated text (from
Wikipedia) that may provide useful weak supervi-
sion.
With this in mind, we put together a multi-task
learning framework for compressive summariza-
tion (which we name task #1). The goal is to
take advantage of existing data for related tasks,
such as extractive summarization (task #2), and
sentence compression (task #3). The three tasks
are instances of structured predictors (Bakır et
</bodyText>
<page confidence="0.984196">
200
</page>
<table confidence="0.61044025">
Tasks Features Decoder
Comp. summ. (#1) ΦCOV,ΦCOMP AD3 (solve Eq. 4)
Extr. summ. (#2) ΦCOV AD3 (solve Eq. 3)
Sent. comp. (#3) ΦCOMP dyn. prg. (max. Eq. 8)
</table>
<tableCaption confidence="0.998641">
Table 1: Features and decoders used for each task.
</tableCaption>
<bodyText confidence="0.986918">
al., 2007), and for all of them we assume feature-
based models that decompose over “parts”:
</bodyText>
<listItem confidence="0.980835857142857">
• For the compressive summarization task, the
parts correspond to concept features (§3.1) and
to arc-deletion features (§3.2).
• For the extractive summarization task, there are
parts for concept features only.
• For the sentence compression task, the parts
correspond to arc-deletion features only.
</listItem>
<bodyText confidence="0.996448571428571">
This is summarized in Table 1. Features for
the three tasks are populated into feature vectors
Φ1(x, y), Φ2(x, y), and Φ3(x, y), respectively,
where (x, y) denotes a task-specific input-output
pair. We assume the feature vectors are all D di-
mensional, where we place zeros in entries cor-
responding to parts that are absent. Note that
this setting is very general and applies to arbi-
trary structured prediction problems (not just sum-
marization), the only assumption being that some
parts are shared between different tasks.
Next, we associate weight vectors v1, v2, v3 E
RD to each task, along with a “shared” vector w.
Each task makes predictions according to the rule:
</bodyText>
<equation confidence="0.970308">
y := arg max (w + vk) - Φk(x, y), (9)
y
</equation>
<bodyText confidence="0.999943727272727">
where k E 11, 2, 3}. This setting is equiva-
lent to the approach of Daum´e (2007) for domain
adaptation, which consists in splitting each fea-
ture into task-component features and a shared
feature; but here we do not duplicate features ex-
plicitly. To learn the weights, we regularize the
weight vectors separately, and assume that each
task has its own loss function Lk, so that the to-
tal loss L is a weighted sum L(w, v1, v2, v3) :=
E3k=1 σkLk(w + vk). This yields the following
objective function to be minimized:
</bodyText>
<equation confidence="0.978990666666667">
F(w, v1, v2, v3) = 2 IIwII2 +
λ 3 λk2 IIvkII2
k=1
</equation>
<bodyText confidence="0.9999826">
where λ and the λk’s are regularization constants,
and N is the total number of training instances.6 In
our experiments (§5), we let the Lk’s be structured
hinge losses (Taskar et al., 2003; Tsochantaridis et
al., 2004), where the corresponding cost functions
are concept recall (for task #2), precision of arc
deletions (for task #3), and a combination thereof
(for task #1).7 These losses were normalized, and
we set σk = N/Nk, where Nk is the number of
training instances for the kth task. This ensures
all tasks are weighted evenly. We used the same
rationale to set λ = λ1 = λ2 = λ3, choosing this
value through cross-validation in the dev set.
We optimize Eq. 10 with stochastic subgradient
descent. This leads to update rules of the form
</bodyText>
<equation confidence="0.9964635">
w +-- (1 − ηtλ)w − ηtσk ˜tLk(w + vk)
vj +-- (1 − ηtλj)vj − ηtδjkσk ˜tLk(w + vk),
</equation>
<bodyText confidence="0.998778108108108">
where ˜tLk are stochastic subgradients for the kth
task, that take only a single instance into account,
and δjk = 1 if and only if j = k. Stochastic sub-
gradients can be computed via cost-augmented de-
coding (see footnote 7).
Interestingly, Eq. 10 subsumes previous ap-
proaches to train compressive summarizers. The
limit λ -+ oc (keeping the λk’s fixed) forces w -+
0, decoupling all the tasks. In this limit, inference
for task #1 (compressive summarization) is based
solely on the model learned from that task’s data,
recovering the approach of Berg-Kirkpatrick et al.
(2011). In the other extreme, setting σ1 = 0 sim-
ply ignores task #1’s training data. As a result, the
optimal v1 will be a vector of zeros; since tasks
#2 and #3 have no parts in common, the objective
will decouple into a sum of two independent terms
6Note that, by substituting uk := w + vk and solving for
w, the problem in Eq. 10 becomes that of minimizing the sum
of the losses with a penalty for the (weighted) variance of the
vectors {0, u1, u2, u3}, regularizing the difference towards
their average, as in Evgeniou and Pontil (2004). This is sim-
ilar to the hierarchical joint learning approach of Finkel and
Manning (2010), except that our goal is to learn a new task
(compressive summarization) instead of combining tasks.
7Let Yk denote the output set for the kth task. Given
a task-specific cost function Ak : Yk x Yk → R,
and letting (xt, yt)Tt=1 be the labeled dataset for this
task, the structured// hinge loss takes the form Lk(uk) :=
Etmaxy&apos;∈Y (uk · \Φk(xt, y&apos;) − Φk(xt, yt)) + Ak(y&apos;, yt)).
The inner maximization over y&apos; is called the cost-augmented
decoding problem: it differs from Eq. 9 by the inclusion
of the cost term Ak(y&apos;, yt). Our costs decompose over the
model’s factors, hence any decoder for Eq. 9 can be used
for the maximization above: for tasks #1–#2, we solve a
relaxation by running AD3 without rounding, and for task #3
we use dynamic programming; see Table 1.
</bodyText>
<equation confidence="0.903774">
σkLk(w + vk), (10)
</equation>
<page confidence="0.634860571428571">
3
1
+
N
�
k=1
201
</page>
<bodyText confidence="0.999799333333333">
involving v2 and v3, which is equivalent to train-
ing the two tasks separately and post-combining
the models, as Martins and Smith (2009) did.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996467">
5.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999952421052631">
We evaluated our compressive summarizers on
data from the Text Analysis Conference (TAC)
evaluations. We use the same splits as previ-
ous work (Berg-Kirkpatrick et al., 2011; Wood-
send and Lapata, 2012): the non-update portions
of TAC-2009 for training and TAC-2008 for test-
ing. In addition, we reserved TAC-2010 as a dev-
set. The test partition contains 48 multi-document
summarization problems; each provides 10 related
news articles as input, and asks for a summary
with up to 100 words, which is evaluated against
four manually written abstracts. We ignored all
the query information present in the TAC datasets.
Single-Task Learning. In the single-task exper-
iments, we trained a compressive summarizer on
the dataset disclosed by Berg-Kirkpatrick et al.
(2011), which contains manual compressive sum-
maries for the TAC-2009 data. We trained a struc-
tured SVM with stochastic subgradient descent;
the cost-augmented inference problems are re-
laxed and solved with AD3, as described in §3.3.8
We followed the procedure described in Berg-
Kirkpatrick et al. (2011) to reduce the number of
candidate sentences: scores were defined for each
sentence (the sum of the scores of the concepts
they cover), and the best-scored sentences were
greedily selected up to a limit of 1,000 words. We
then tagged and parsed the selected sentences with
TurboParser.9 Our choice of a dependency parser
was motivated by our will for a fast system; in par-
ticular, TurboParser attains top accuracies at a rate
of 1,200 words per second, keeping parsing times
below 1 second for each summarization problem.
Multi-Task Learning. For the multi-task ex-
periments, we also used the dataset of Berg-
Kirkpatrick et al. (2011), but we augmented the
training data with extractive summarization and
sentence compression datasets, to help train the
</bodyText>
<footnote confidence="0.9191145">
8We use the AD3 implementation in http://www.
ark.cs.cmu.edu/AD3, setting the maximum number of
iterations to 200 at training time and 1000 at test time. We
extended the code to handle the knapsack and budget factors;
the modified code will be part of the next release (AD3 2.1).
9http://www.ark.cs.cmu.edu/TurboParser
</footnote>
<bodyText confidence="0.999859307692308">
compressive summarizer. For extractive sum-
marization, we used the DUC 2003 and 2004
datasets (a total of 80 multi-document summariza-
tion problems). We generated oracle extracts by
maximizing bigram recall with respect to the man-
ual abstracts, as described in Berg-Kirkpatrick et
al. (2011). For sentence compression, we adapted
the Simple English Wikipedia dataset of Wood-
send and Lapata (2011), containing aligned sen-
tences for 15,000 articles from the English and
Simple English Wikipedias. We kept only the
4,481 sentence pairs corresponding to deletion-
based compressions.
</bodyText>
<subsectionHeader confidence="0.592324">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999976066666667">
Table 2 shows the results. The top rows refer
to three strong baselines: the ICSI-1 extractive
coverage-based system of Gillick et al. (2008),
which achieved the best ROUGE scores in the
TAC-2008 evaluation; the compressive summa-
rizer of Berg-Kirkpatrick et al. (2011), denoted
BGK’11; and the multi-aspect compressive sum-
marizer of Woodsend and Lapata (2012), denoted
WL’12. All these systems require ILP solvers.
The bottom rows show the results achieved by
our implementation of a pure extractive system
(similar to the learned extractive summarizer of
Berg-Kirkpatrick et al., 2011); a system that post-
combines extraction and compression components
trained separately, as in Martins and Smith (2009);
and our compressive summarizer trained as a sin-
gle task, and in the multi-task setting.
The ROUGE and Pyramid scores show that the
compressive summarizers (when properly trained)
yield considerable benefits in content coverage
over extractive systems, confirming the results of
Berg-Kirkpatrick et al. (2011). Comparing the
two bottom rows, we see a clear benefit by train-
ing in the multi-task setting, with a consistent
gain in both coverage and linguistic quality. Our
ROUGE-2 score (12.30%) is, to our knowledge,
the highest reported on the TAC-2008 dataset,
with little harm in grammaticality with respect to
an extractive system that preserves the original
sentences. Figure 2 shows an example summary.
</bodyText>
<subsectionHeader confidence="0.990626">
5.3 Runtimes
</subsectionHeader>
<bodyText confidence="0.9989582">
We conducted another set of experiments to com-
pare the runtime of our compressive summarizer
based on AD3 with the runtimes achieved by
GLPK, the ILP solver used by Berg-Kirkpatrick et
al. (2011). We varied the maximum number of it-
</bodyText>
<page confidence="0.993992">
202
</page>
<table confidence="0.999885375">
System R-2 R-SU4 Pyr LQ
ICSI-1 11.03 13.96 34.51 –
BGK’11 11.71 14.47 41.31 –
WL’12 11.37 14.47 – –
Extractive 11.16 14.07 36.0 4.6
Post-comb. 11.07 13.85 38.4 4.1
Single-task 11.88 14.86 41.0 3.8
Multi-task 12.30 15.18 42.6 4.2
</table>
<tableCaption confidence="0.969856">
Table 2: Results for compressive summarization.
</tableCaption>
<bodyText confidence="0.978701636363636">
Shown are the ROUGE-2 and ROUGE SU-4 re-
calls with the default options from the ROUGE
toolkit (Lin, 2004); Pyramid scores (Nenkova and
Passonneau, 2004); and linguistic quality scores,
scored between 1 (very bad) to 5 (very good). For
Pyramid, the evaluation was performed by two
annotators, each evaluating half of the problems;
scores marked with † were computed by different
annotators and are not directly comparable. Lin-
guistic quality was evaluated by two linguists; we
show the average of the reported scores.
</bodyText>
<table confidence="0.999294714285714">
Solver Runtime (sec.) ROUGE-2
ILP Exact 10.394 12.40
LP-Relax. 2.265 12.38
AD3-5000 0.952 12.38
AD3-1000 0.406 12.30
AD3-200 0.159 12.15
Extractive (ILP) 0.265 11.16
</table>
<tableCaption confidence="0.99401">
Table 3: Runtimes of several decoders on a Intel
</tableCaption>
<bodyText confidence="0.921193590909091">
Core i7 processor @2.8 GHz, with 8GB RAM. For
each decoder, we show the average time taken to
solve a summarization problem in TAC-2008. The
reported runtimes of AD3 and LP-Relax include
the time taken to round the solution (§3.4), which
is 0.029 seconds on average.
erations of AD3 in {200, 1000, 5000}, and clocked
the time spent by GLPK to solve the exact ILPs
and their relaxations. Table 3 depicts the results.10
We see that our proposed configuration (AD3-
1000) is orders of magnitude faster than the ILP
solver, and 5 times faster than its relaxed variant,
while keeping similar accuracy levels.11 The gain
when the number of iterations in AD3 is increased
to 5000 is small, given that the runtime is more
10Within dual decomposition algorithms, we verified ex-
perimentally that AD3 is substantially faster than the subgra-
dient algorithm, which is consistent with previous findings
(Martins et al., 2011b).
11The runtimes obtained with the exact ILP solver seem
slower than those reported by Berg-Kirkpatrick et al. (2011).
(around 1.5 sec. on average, according to their Fig. 3). We
conjecture that this difference is due to the restricted set of
subtrees that can be deleted by Berg-Kirkpatrick et al. (2011),
which greatly reduces their search space.
Japan dispatched four military ships to help Russia res-
cue seven crew members aboard a small submarine
trapped on the seabed in the Far East. The Russian
Pacific Fleet said the crew had 120 hours of oxygen
reserves on board when the submarine submerged at
midday Thursday (2300 GMT Wednesday) off the Kam-
chatka peninsula, the stretch of Far Eastern Russia fac-
ing the Bering Sea. The submarine, used in rescue,
research and intelligence-gathering missions, became
stuck at the bottom of the Bay of Berezovaya off Rus-
sia’s Far East coast when its propeller was caught in a
fishing net. The Russian submarine had been tending
an underwater antenna mounted to the sea floor when it
became snagged on a wire helping to stabilize a ventila-
tion cable attached to the antenna. Rescue crews low-
ered a British remote-controlled underwater vehicle to a
Russian mini-submarine trapped deep under the Pacific
Ocean, hoping to free the vessel and its seven trapped
crewmen before their air supply ran out.
</bodyText>
<figureCaption confidence="0.996567">
Figure 2: Example summary from our compres-
sive system. Removed text is grayed out.
</figureCaption>
<bodyText confidence="0.978210153846154">
than doubled; accuracy starts to suffer, however, if
the number of iterations is reduced too much. In
practice, we observed that the final rounding pro-
cedure was crucial, as only 2 out of the 48 test
problems had integral solutions (arguably because
of the “repulsive” nature of the network, as hinted
in §3.4). For comparison, we also report in the bot-
tom row the average runtime of the learned extrac-
tive baseline. We can see that our system’s runtime
is competitive with this baseline. To our knowl-
edge, this is the first time a compressive sum-
marizer achieves such a favorable accuracy/speed
tradeoff.
</bodyText>
<sectionHeader confidence="0.996491" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9998928125">
We presented a multi-task learning framework for
compressive summarization, leveraging data for
related tasks in a principled manner. We decode
with AD3, a fast and modular dual decomposition
algorithm which is orders of magnitude faster than
ILP-based approaches. Results show that the state
of the art is improved in automatic and manual
metrics, with speeds close to extractive systems.
Our approach is modular and easy to extend.
For example, a different compression model could
incorporate rewriting rules to enable compres-
sions that go beyond word deletion, as in Cohn
and Lapata (2008). Other aspects may be added
as additional components in our dual decom-
position framework, such as query information
(Schilder and Kondadadi, 2008), discourse con-
</bodyText>
<page confidence="0.996133">
203
</page>
<bodyText confidence="0.999916166666667">
straints (Clarke and Lapata, 2007), or lexical pref-
erences (Woodsend and Lapata, 2012). Our multi-
task approach may be used to jointly learn pa-
rameters for these aspects; the dual decomposi-
tion algorithm ensures that optimization remains
tractable even with many components.
</bodyText>
<subsectionHeader confidence="0.552693">
A Projection Onto Knapsack
</subsectionHeader>
<bodyText confidence="0.9958165">
This section describes a linear-time algorithm (Al-
gorithm 1) for solving the following problem:
</bodyText>
<equation confidence="0.805252666666667">
minimize 11z − a112
w.r.t. zn E [0, 1], bn E [N],
s.t. ENn=1 Lnzn &lt; B, (11)
</equation>
<bodyText confidence="0.999712944444444">
where a E RN and Ln &gt; 0, bn E [N]. This in-
cludes as special cases the problems of projecting
onto a budget constraint (Ln = 1, bn) and onto
the simplex (same, plus B = 1).
Let clip(t) := max{0, min{1, t}}. Algorithm 1
starts by clipping a to the unit interval; if that
yields a z satisfying ENn=1 Lnzn &lt; B, we are
done. Otherwise, the solution of Eq. 11 must sat-
isfy ENn=1 Lnzn = B. It can be shown from the
KKT conditions that the solution is of the form
z*n := clip(an +T*Ln) for a constant T* lying in a
particular interval of split-points (line 11). To seek
this constant, we use an algorithm due to Pardalos
and Kovoor (1990) which iteratively shrinks this
interval. The algorithm requires computing medi-
ans as a subroutine, which can be done in linear
time (Blum et al., 1973). The overall complexity
in O(N) (Pardalos and Kovoor, 1990).
</bodyText>
<sectionHeader confidence="0.996982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998909">
We thank all reviewers for their insightful com-
ments; Trevor Cohn for helpful discussions about
multi-task learning; Taylor Berg-Kirkpatrick for
answering questions about their summarizer and
for providing code; and Helena Figueira and Pedro
Mendes for helping with manual evaluation. This
work was partially supported by the EU/FEDER
programme, QREN/POR Lisboa (Portugal), under
the Discooperio project (contract 2011/18501),
and by a FCT grant PTDC/EEI-SII/2312/2012.
</bodyText>
<sectionHeader confidence="0.998456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.852130692307692">
G. Bakır, T. Hofmann, B. Sch¨olkopf, A. Smola,
B. Taskar, and S. Vishwanathan. 2007. Predicting
Structured Data. The MIT Press.
Algorithm 1 Projection Onto Knapsack.
1: input: a := (an)Nn=1, costs (Ln)Nn=1, maximum cost B
3: {Try to clip into unit interval:}
4: Set zn +- clip(an) for n E [N]
5: if ENn=1 Lnzn &lt; B then
6: Return z and stop.
7: end if
9: {Run Pardalos and Kovoor (1990)’s algorithm:}
10: Initialize working set W +- {1, ... ,K}
11: Initialize set of split points:
</reference>
<figure confidence="0.938876">
P +- {−an/Ln, (1 − an)/Ln}Nn=1 U {foo}
12: Initialize TL +- −oo, TR +- oo, stight +- 0, ξ +- 0.
13: while W =� ∅ do
14: Compute T +- Median(P)
15: Set s +- stight + ξT + En∈W Lnclip(an + TLn)
16: If s &lt; B, set TL +- T; if s &gt; B, set TR +- T
17: Reduce set of split points: P +- P n [TL, TR]
18: Define the sets:
WL := {n E W  |(1 − an)/Ln &lt; TL}
WR := {n E W  |− an/Ln &gt; TR}
an &lt; TL ∧ 1 − an &gt; TR
Ln Ln
}
19: Update working set: W +- W \ (WL U WR U WM)
20: Update tight-sum:
stight +- stight+En∈WL Ln(1−an)−En∈WR Lnan
</figure>
<reference confidence="0.977777172413793">
21: Update slack-sum: ξ +- ξ + En∈WM L2n
22: end while
23: Define T∗ +- (B − ENi=1 Liai − stight)/ξ
24: Set zn +- clip(an + T∗Ln), `dn E [N]
25: output: z := (zn)Nn=1.
P. B. Baxendale. 1958. Machine-made index for tech-
nical literature—an experiment. IBM Journal of Re-
search Development, 2(4):354–361.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proc. ofAnnual Meeting of the Association for Com-
putational Linguistics.
Manuel Blum, Robert W Floyd, Vaughan Pratt,
Ronald L Rivest, and Robert E Tarjan. 1973. Time
bounds for selection. Journal of Computer and Sys-
tem Sciences, 7(4):448–461.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In SIGIR.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through lagrangian
relaxation. In Proc. of Empirical Methods for Natu-
ral Language Processing.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Proc. of
Empirical Methods in Natural Language Process-
ing.
J. Clarke and M. Lapata. 2008. Global Inference for
Sentence Compression An Integer Linear Program-
</reference>
<figure confidence="0.5101175">
I ����
WM := n E W
</figure>
<page confidence="0.994336">
204
</page>
<bodyText confidence="0.82918">
ming Approach. Journal of Artificial Intelligence
Research, 31:399–429.
H. Lin and J. Bilmes. 2012. Learning mixtures of sub-
modular shells with application to document sum-
marization. In Proc. of Uncertainty in Artificial In-
telligence.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proc. COLING.
</bodyText>
<reference confidence="0.999745042105263">
D. Das, A. F. T. Martins, and N. A. Smith. 2012. An
Exact Dual Decomposition Algorithm for Shallow
Semantic Parsing with Constraints. In Proc. of First
Joint Conference on Lexical and Computational Se-
mantics (*SEM).
H. Daum´e. 2006. Practical Structured Learning Tech-
niques for Natural Language Processing. Ph.D. the-
sis, University of Southern California.
H. Daum´e. 2007. Frustratingly easy domain adapta-
tion. In Proc. of Annual Meeting of the Association
for Computational Linguistics.
H. P. Edmundson. 1969. New methods in automatic
extracting. Journal of the ACM, 16(2):264–285.
T. Evgeniou and M. Pontil. 2004. Regularized multi–
task learning. In Proc. of ACM SIGKDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 109–117. ACM.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proc. of International
Conference on Computational Linguistics.
J.R. Finkel and C.D. Manning. 2010. Hierarchical
joint learning: Improving joint parsing and named
entity recognition with non-jointly labeled data. In
Proc. ofAnnual Meeting of the Association for Com-
putational Linguistics.
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Dis-
covering diverse and salient threads in document
collections. In Proc. of Empirical Methods in Natu-
ral Language Processing.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.
2008. The icsi summarization system at tac 2008.
In Proc. of Text Understanding Conference.
K. Knight and D. Marcu. 2000. Statistics-based
summarization—step one: Sentence compression.
In AAAI/IAAI.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition:
Message-passing revisited. In Proc. of International
Conference on Computer Vision.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In SIGIR.
H. Lin and J. Bilmes. 2010. Multi-document summa-
rization via budgeted maximization of submodular
functions. In Proc. of Annual Meeting of the North
American chapter of the Association for Computa-
tional Linguistics.
C.-Y. Lin. 2003. Improving summarization perfor-
mance by sentence compression-a pilot study. In the
Int. Workshop on Inf. Ret. with Asian Languages.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research Development,
2(2):159–165.
A. F. T. Martins and N. A. Smith. 2009. Summariza-
tion with a Joint Model for Sentence Extraction and
Compression. In North American Chapter of the As-
sociation for Computational Linguistics: Workshop
on Integer Linear Programming for NLP.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2011a. An Aug-
mented Lagrangian Approach to Constrained MAP
Inference. In Proc. of International Conference on
Machine Learning.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011b. Dual Decomposition
with Many Overlapping Components. In Proc. of
Empirical Methods for Natural Language Process-
ing.
Andre F. T. Martins, Mario A. T. Figueiredo, Pedro
M. Q. Aguiar, Noah A. Smith, and Eric P. Xing.
2012. Alternating Directions Dual Decomposition.
Arxiv preprint arXiv:1212.6550.
R. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proc. of
Annual Meeting of the European Chapter of the As-
sociation for Computational Linguistics.
R. McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In ECIR.
A. Nenkova and R. Passonneau. 2004. Evaluating
content selection in summarization: The pyramid
method. In Proceedings of NAACL, pages 145–152.
Panos M. Pardalos and Naina Kovoor. 1990. An al-
gorithm for a singly constrained class of quadratic
programs subject to upper and lower bounds. Math-
ematical Programming, 46(1):321–328.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evaluation,
and user studies. In the NAACL-ANLP Workshop on
Automatic Summarization.
</reference>
<page confidence="0.979557">
205
</page>
<reference confidence="0.999741829787234">
A.M. Rush and M. Collins. 2012. A Tutorial on Dual
Decomposition and Lagrangian Relaxation for In-
ference in Natural Language Processing. Journal of
Artificial Intelligence Research, 45:305–362.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming re-
laxations for natural language processing. In Proc.
of Empirical Methods for Natural Language Pro-
cessing.
Frank Schilder and Ravikumar Kondadadi. 2008. Fast-
sum: Fast and accurate query-based multi-document
summarization. In Proc. of Annual Meeting of the
Association for Computational Linguistics.
R. Sipos, P. Shivaswamy, and T. Joachims. 2012.
Large-margin learning of submodular summariza-
tion models.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-
margin Markov networks. In Proc. of Neural Infor-
mation Processing Systems.
B. Taskar, V. Chatalbashev, and D. Koller. 2004.
Learning associative Markov networks. In Proc. of
International Conference of Machine Learning.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Proc.
of International Conference of Machine Learning.
K. Woodsend and M. Lapata. 2010. Automatic gener-
ation of story highlights. In Proc. of Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 565–574.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proc. of Em-
pirical Methods in Natural Language Processing.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proc. of Empirical Methods in Natu-
ral Language Processing.
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proc. of International Joint Conference on Artifi-
cal Intelligence.
D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006.
Sentence compression as a component of a multi-
document summarization system. In the ACL DUC
Workshop.
</reference>
<page confidence="0.998888">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226544">
<title confidence="0.990984">Fast and Robust Compressive with Dual Decomposition and Multi-Task Learning</title>
<author confidence="0.7128">B F T</author>
<note confidence="0.4200155">Labs, Alameda D. Afonso Henriques, 41, 2°, 1000-123 Lisboa, Portugal de Instituto Superior T´ecnico, 1049-001 Lisboa,</note>
<abstract confidence="0.999831444444445">We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC- 2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Bakır</author>
<author>T Hofmann</author>
<author>B Sch¨olkopf</author>
<author>A Smola</author>
<author>B Taskar</author>
<author>S Vishwanathan</author>
</authors>
<title>Predicting Structured Data.</title>
<date>2007</date>
<publisher>The MIT Press.</publisher>
<marker>Bakır, Hofmann, Sch¨olkopf, Smola, Taskar, Vishwanathan, 2007</marker>
<rawString>G. Bakır, T. Hofmann, B. Sch¨olkopf, A. Smola, B. Taskar, and S. Vishwanathan. 2007. Predicting Structured Data. The MIT Press.</rawString>
</citation>
<citation valid="false">
<title>Algorithm 1 Projection Onto Knapsack. 1: input: a := (an)Nn=1, costs (Ln)Nn=1, maximum cost B 3: {Try to clip into unit interval:} 4: Set zn +- clip(an) for n E [N] 5: if ENn=1 Lnzn &lt; B then 6: Return z and stop. 7: end if 9: {Run Pardalos and Kovoor (1990)’s algorithm:} 10: Initialize working set W +- {1, ... ,K} 11: Initialize set of split points: 21: Update slack-sum: ξ +- ξ + En∈WM L2n 22: end while 23: Define T∗ +- (B − ENi=1 Liai − stight)/ξ 24: Set zn +- clip(an + T∗Ln), `dn E [N] 25: output: z :=</title>
<date></date>
<marker></marker>
<rawString>Algorithm 1 Projection Onto Knapsack. 1: input: a := (an)Nn=1, costs (Ln)Nn=1, maximum cost B 3: {Try to clip into unit interval:} 4: Set zn +- clip(an) for n E [N] 5: if ENn=1 Lnzn &lt; B then 6: Return z and stop. 7: end if 9: {Run Pardalos and Kovoor (1990)’s algorithm:} 10: Initialize working set W +- {1, ... ,K} 11: Initialize set of split points: 21: Update slack-sum: ξ +- ξ + En∈WM L2n 22: end while 23: Define T∗ +- (B − ENi=1 Liai − stight)/ξ 24: Set zn +- clip(an + T∗Ln), `dn E [N] 25: output: z := (zn)Nn=1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P B Baxendale</author>
</authors>
<title>Machine-made index for technical literature—an experiment.</title>
<date>1958</date>
<journal>IBM Journal of Research Development,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="1158" citStr="Baxendale, 1958" startWordPosition="158" endWordPosition="159">oach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers. 1 Introduction Automatic text summarization is a seminal problem in information retrieval and natural language processing (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, with the overwhelming amount of information available on the Web, the demand for fast, robust, and scalable summarization systems is stronger than ever. Up to now, extractive systems have been the most popular in multi-document summarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (a</context>
</contexts>
<marker>Baxendale, 1958</marker>
<rawString>P. B. Baxendale. 1958. Machine-made index for technical literature—an experiment. IBM Journal of Research Development, 2(4):354–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Gillick</author>
<author>Dan Klein</author>
</authors>
<title>Jointly learning to extract and compress.</title>
<date>2011</date>
<booktitle>In Proc. ofAnnual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2935" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="426" endWordPosition="429">nantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still obstacles that need to be surmounted for these systems to enjoy wide adoption. All approaches above are based on integer linear programming (ILP), suffering from slow runtimes, when compared to extractive systems. For example, Woodsend and Lapata (2012) report 55 seconds on average to produce a summary; Berg-Kirkpatrick et al. (2011) report substantially faster runtimes, but fewer compressions are allowed. Having a compressive summarizer which is both fast and expressive remains an open problem. A second inconvenience of ILP-based approaches is that they do not exploit the mo</context>
<context position="17400" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="2862" endWordPosition="2865">as follows. First, solve the LP relaxation using AD3, as described above. This yields a solution z*, where each component lies in the unit interval [0, 1]. If these components are all integer, then we have a certificate that this is the optimal solution. Otherwise, we collect the K sentences with the highest values of z*,,,,0 (“posteriors” on sentences), and seek the feasible summary which is the closest (in Euclidean distance) to z*, while only containing those sentences. This can be computed exactly in time O(B EKk=1 L,,,k), through dynamic programming.5 3.5 Features and Hard Constraints As Berg-Kirkpatrick et al. (2011), we used stemmed word bigrams as concepts, to which we associate the following concept features (Φcov): indicators for document counts, features indicating if each of the words in the bigram is a stopword, the earliest position in a document each concept occurs, as well as two and three-way conjunctions of these features. For the compression model, we include the following arc-deletion features (Φcomp): • the dependency label of the arc being deleted, as well as its conjunction with the part-of-speech tag of the head, of the modifier, and of both; • the dependency labels of the arc being dele</context>
<context position="19841" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="3273" endWordPosition="3276">ttle down), to the word that (e.g., said that), or to the word to if it is a leaf (e.g., allowed to come); arcs pointing to negation words, cardinal numbers, or determiners; and arcs connecting two proper nouns or words within quotation marks. 4 Multi-Task Learning We next turn to the problem of learning the model from training data. Prior work in compressive summarization has followed one of two strategies: Martins and Smith (2009) and Woodsend and Lapata (2012) learn the extraction and compression models separately, and then post-combine them, circumventing the lack of fully annotated data. Berg-Kirkpatrick et al. (2011) gathered a small dataset of manually compressed summaries, and trained with full supervision. While the latter approach is statistically more principled, it has the disadvantage of requiring fully annotated data, which is difficult to obtain in large quantities. On the other hand, there is plenty of data containing manually written abstracts (from the DUC and TAC conferences) and user-generated text (from Wikipedia) that may provide useful weak supervision. With this in mind, we put together a multi-task learning framework for compressive summarization (which we name task #1). The goal is to </context>
<context position="23913" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="3972" endWordPosition="3975">Lk(w + vk) vj +-- (1 − ηtλj)vj − ηtδjkσk ˜tLk(w + vk), where ˜tLk are stochastic subgradients for the kth task, that take only a single instance into account, and δjk = 1 if and only if j = k. Stochastic subgradients can be computed via cost-augmented decoding (see footnote 7). Interestingly, Eq. 10 subsumes previous approaches to train compressive summarizers. The limit λ -+ oc (keeping the λk’s fixed) forces w -+ 0, decoupling all the tasks. In this limit, inference for task #1 (compressive summarization) is based solely on the model learned from that task’s data, recovering the approach of Berg-Kirkpatrick et al. (2011). In the other extreme, setting σ1 = 0 simply ignores task #1’s training data. As a result, the optimal v1 will be a vector of zeros; since tasks #2 and #3 have no parts in common, the objective will decouple into a sum of two independent terms 6Note that, by substituting uk := w + vk and solving for w, the problem in Eq. 10 becomes that of minimizing the sum of the losses with a penalty for the (weighted) variance of the vectors {0, u1, u2, u3}, regularizing the difference towards their average, as in Evgeniou and Pontil (2004). This is similar to the hierarchical joint learning approach of F</context>
<context position="25691" citStr="Berg-Kirkpatrick et al., 2011" startWordPosition="4292" endWordPosition="4295">r costs decompose over the model’s factors, hence any decoder for Eq. 9 can be used for the maximization above: for tasks #1–#2, we solve a relaxation by running AD3 without rounding, and for task #3 we use dynamic programming; see Table 1. σkLk(w + vk), (10) 3 1 + N � k=1 201 involving v2 and v3, which is equivalent to training the two tasks separately and post-combining the models, as Martins and Smith (2009) did. 5 Experiments 5.1 Experimental setup We evaluated our compressive summarizers on data from the Text Analysis Conference (TAC) evaluations. We use the same splits as previous work (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012): the non-update portions of TAC-2009 for training and TAC-2008 for testing. In addition, we reserved TAC-2010 as a devset. The test partition contains 48 multi-document summarization problems; each provides 10 related news articles as input, and asks for a summary with up to 100 words, which is evaluated against four manually written abstracts. We ignored all the query information present in the TAC datasets. Single-Task Learning. In the single-task experiments, we trained a compressive summarizer on the dataset disclosed by Berg-Kirkpatrick et al. (2011), which co</context>
<context position="27937" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="4644" endWordPosition="4647">ts, to help train the 8We use the AD3 implementation in http://www. ark.cs.cmu.edu/AD3, setting the maximum number of iterations to 200 at training time and 1000 at test time. We extended the code to handle the knapsack and budget factors; the modified code will be part of the next release (AD3 2.1). 9http://www.ark.cs.cmu.edu/TurboParser compressive summarizer. For extractive summarization, we used the DUC 2003 and 2004 datasets (a total of 80 multi-document summarization problems). We generated oracle extracts by maximizing bigram recall with respect to the manual abstracts, as described in Berg-Kirkpatrick et al. (2011). For sentence compression, we adapted the Simple English Wikipedia dataset of Woodsend and Lapata (2011), containing aligned sentences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletionbased compressions. 5.2 Results Table 2 shows the results. The top rows refer to three strong baselines: the ICSI-1 extractive coverage-based system of Gillick et al. (2008), which achieved the best ROUGE scores in the TAC-2008 evaluation; the compressive summarizer of Berg-Kirkpatrick et al. (2011), denoted BGK’11; and the multi-a</context>
<context position="29249" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="4839" endWordPosition="4842">se systems require ILP solvers. The bottom rows show the results achieved by our implementation of a pure extractive system (similar to the learned extractive summarizer of Berg-Kirkpatrick et al., 2011); a system that postcombines extraction and compression components trained separately, as in Martins and Smith (2009); and our compressive summarizer trained as a single task, and in the multi-task setting. The ROUGE and Pyramid scores show that the compressive summarizers (when properly trained) yield considerable benefits in content coverage over extractive systems, confirming the results of Berg-Kirkpatrick et al. (2011). Comparing the two bottom rows, we see a clear benefit by training in the multi-task setting, with a consistent gain in both coverage and linguistic quality. Our ROUGE-2 score (12.30%) is, to our knowledge, the highest reported on the TAC-2008 dataset, with little harm in grammaticality with respect to an extractive system that preserves the original sentences. Figure 2 shows an example summary. 5.3 Runtimes We conducted another set of experiments to compare the runtime of our compressive summarizer based on AD3 with the runtimes achieved by GLPK, the ILP solver used by Berg-Kirkpatrick et al</context>
<context position="31934" citStr="Berg-Kirkpatrick et al. (2011)" startWordPosition="5278" endWordPosition="5281">cts the results.10 We see that our proposed configuration (AD3- 1000) is orders of magnitude faster than the ILP solver, and 5 times faster than its relaxed variant, while keeping similar accuracy levels.11 The gain when the number of iterations in AD3 is increased to 5000 is small, given that the runtime is more 10Within dual decomposition algorithms, we verified experimentally that AD3 is substantially faster than the subgradient algorithm, which is consistent with previous findings (Martins et al., 2011b). 11The runtimes obtained with the exact ILP solver seem slower than those reported by Berg-Kirkpatrick et al. (2011). (around 1.5 sec. on average, according to their Fig. 3). We conjecture that this difference is due to the restricted set of subtrees that can be deleted by Berg-Kirkpatrick et al. (2011), which greatly reduces their search space. Japan dispatched four military ships to help Russia rescue seven crew members aboard a small submarine trapped on the seabed in the Far East. The Russian Pacific Fleet said the crew had 120 hours of oxygen reserves on board when the submarine submerged at midday Thursday (2300 GMT Wednesday) off the Kamchatka peninsula, the stretch of Far Eastern Russia facing the B</context>
</contexts>
<marker>Berg-Kirkpatrick, Gillick, Klein, 2011</marker>
<rawString>Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein. 2011. Jointly learning to extract and compress. In Proc. ofAnnual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Blum</author>
<author>Robert W Floyd</author>
<author>Vaughan Pratt</author>
<author>Ronald L Rivest</author>
<author>Robert E Tarjan</author>
</authors>
<title>Time bounds for selection.</title>
<date>1973</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>7</volume>
<issue>4</issue>
<marker>Blum, Floyd, Pratt, Rivest, Tarjan, 1973</marker>
<rawString>Manuel Blum, Robert W Floyd, Vaughan Pratt, Ronald L Rivest, and Robert E Tarjan. 1973. Time bounds for selection. Journal of Computer and System Sciences, 7(4):448–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="1587" citStr="Carbonell and Goldstein, 1998" startWordPosition="222" endWordPosition="225">times that rival those of extractive summarizers. 1 Introduction Automatic text summarization is a seminal problem in information retrieval and natural language processing (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, with the overwhelming amount of information available on the Web, the demand for fast, robust, and scalable summarization systems is stronger than ever. Up to now, extractive systems have been the most popular in multi-document summarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (avoiding finegrained text generation), and it ensures a grammatical summary, assuming the original sentences are well-formed. The typical trade-offs in these models (maximizing relevance, and penalizing redundancy) lead to submodular optimization problems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012;</context>
<context position="5979" citStr="Carbonell and Goldstein, 1998" startWordPosition="917" endWordPosition="921">vector y := hyniNn=1 to represent an extractive summary, where yn = 1 if sn ∈ S, and yn = 0 otherwise. Let Ln be the number of words of the nth sentence. By designing a quality score function g : {0,1}N → R, this can be cast as a global optimization problem with a knapsack constraint: maximize g(y) w.r.t. y ∈ {0,1}N s.t. ENn=1 Lnyn ≤ B. (1) Intuitively, a good summary is one which selects sentences that individually convey “relevant” information, while collectively having small “redundancy.” This trade-off was explicitly modeled in early works through the notion of maximal marginal relevance (Carbonell and Goldstein, 1998; McDonald, 2007). An alternative are coverage-based models (§2.1; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2008), which seek a set of sentences that covers as many diverse “concepts” as possible; redundancy is automatically penalized since redundant sentences cover fewer concepts. Both models can be framed under the framework of submodular optimization (Lin and Bilmes, 2010), leading to greedy algorithms that have approximation guarantees. However, extending these models to allow for sentence compression (as will be detailed in §3) breaks the diminishing returns </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-W Chang</author>
<author>M Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="16431" citStr="Chang and Collins, 2011" startWordPosition="2706" endWordPosition="2709"> of this AD3 subproblem is linear in the number of words. In addition, we found it useful to add a second BUDGET factor limiting the number of sentences that can be selected to a prescribed value K. We set K = 6 in our experiments. 199 3.4 Rounding Strategy Recall that the problem in Eq. 4 is NP-hard, and that AD3 is solving a linear relaxation. While there are ways of wrapping AD3 in an exact search algorithm (Das et al., 2012), such strategies work best when the solution of the relaxation has few fractional components, which is typical of parsing and translation problems (Rush et al., 2010; Chang and Collins, 2011), and attractive networks (Taskar et al., 2004). Unfortunately, this is not the case in summarization, where concepts “compete” with each other for inclusion in the summary, leading to frustrated cycles. We chose instead to adopt a fast and simple rounding procedure for obtaining a summary from a fractional solution. The procedure works as follows. First, solve the LP relaxation using AD3, as described above. This yields a solution z*, where each component lies in the unit interval [0, 1]. If these components are all integer, then we have a certificate that this is the optimal solution. Otherw</context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Y.-W. Chang and M. Collins. 2011. Exact decoding of phrase-based translation models through lagrangian relaxation. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Modelling compression with discourse constraints.</title>
<date>2007</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="34653" citStr="Clarke and Lapata, 2007" startWordPosition="5724" endWordPosition="5727">ular dual decomposition algorithm which is orders of magnitude faster than ILP-based approaches. Results show that the state of the art is improved in automatic and manual metrics, with speeds close to extractive systems. Our approach is modular and easy to extend. For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). Other aspects may be added as additional components in our dual decomposition framework, such as query information (Schilder and Kondadadi, 2008), discourse con203 straints (Clarke and Lapata, 2007), or lexical preferences (Woodsend and Lapata, 2012). Our multitask approach may be used to jointly learn parameters for these aspects; the dual decomposition algorithm ensures that optimization remains tractable even with many components. A Projection Onto Knapsack This section describes a linear-time algorithm (Algorithm 1) for solving the following problem: minimize 11z − a112 w.r.t. zn E [0, 1], bn E [N], s.t. ENn=1 Lnzn &lt; B, (11) where a E RN and Ln &gt; 0, bn E [N]. This includes as special cases the problems of projecting onto a budget constraint (Ln = 1, bn) and onto the simplex (same, pl</context>
</contexts>
<marker>Clarke, Lapata, 2007</marker>
<rawString>James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proc. of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clarke</author>
<author>M Lapata</author>
</authors>
<title>Global Inference for Sentence Compression An Integer Linear Program-</title>
<date>2008</date>
<contexts>
<context position="18722" citStr="Clarke and Lapata, 2008" startWordPosition="3093" endWordPosition="3096"> ; 5Briefly, if we link the roots of the K sentences to a superroot node, the problem above can be transformed into that of finding the best configuration in the resulting binary tree subject to a budget constraint. We omit details for space. • a feature indicating whether the modifier or any of its descendants is a negation word; • indicators of whether the modifier is a temporal word (e.g., Friday) or a preposition pointing to a temporal word (e.g., on Friday). In addition, we included hard constraints to prevent the deletion of certain arcs, following previous work in sentence compression (Clarke and Lapata, 2008). We never delete arcs whose dependency label is SUB, OBJ, PMOD, SBAR, VC, or PRD (this makes sure we preserve subjects and objects of verbs, arcs departing from prepositions or complementizers, and that we do not break verb chains or predicative complements); arcs linking to a conjunction word or siblings of such arcs (to prevent inconsistencies in handling coordinative conjunctions); arcs linking verbs to other verbs, to adjectives (e.g., make available), to verb particles (e.g., settle down), to the word that (e.g., said that), or to the word to if it is a leaf (e.g., allowed to come); arcs</context>
</contexts>
<marker>Clarke, Lapata, 2008</marker>
<rawString>J. Clarke and M. Lapata. 2008. Global Inference for Sentence Compression An Integer Linear Program-</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>A F T Martins</author>
<author>N A Smith</author>
</authors>
<title>An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints.</title>
<date>2012</date>
<booktitle>In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="9295" citStr="Das et al., 2012" startWordPosition="1453" endWordPosition="1456">thm of Rush et al. (2010), but it enjoys a faster convergence rate. Both algorithms split the original problem into several components, and then iterate between solving independent local subproblems at each component and adjusting multipliers to promote an agreement.2 The difference between the two methods is that the AD3 local subproblems, instead of requiring the computation of a locally optimal configuration, require solving a local quadratic problem. Martins et al. (2011b) provided linear-time solutions for several logic constraints, with applications to syntax and frame-semantic parsing (Das et al., 2012). We will see that AD3 can also handle budget and knapsack constraints efficiently. To tackle Eq. 3 with dual decomposition, we split the coverage-based summarizer into the following M + 1 components (one per constraint): 1. For each of the M concepts in C(D), one component for imposing the logic constraint in Eq. 3. This corresponds to the OR-WITHOUTPUT factor described by Martins et al. (2011b); the AD3 subproblem for the mth factor can be solved in time O(|Im|). 2. Another component for the knapsack constraint. This corresponds to a (novel) KNAPSACK factor, whose AD3 subproblem is solvable </context>
<context position="16239" citStr="Das et al., 2012" startWordPosition="2675" endWordPosition="2678">ime linear in k. 4. Another component linked to all the words imposing that at most B words can be selected; this is done via a BUDGET factor, a particular case of KNAPSACK. The runtime of this AD3 subproblem is linear in the number of words. In addition, we found it useful to add a second BUDGET factor limiting the number of sentences that can be selected to a prescribed value K. We set K = 6 in our experiments. 199 3.4 Rounding Strategy Recall that the problem in Eq. 4 is NP-hard, and that AD3 is solving a linear relaxation. While there are ways of wrapping AD3 in an exact search algorithm (Das et al., 2012), such strategies work best when the solution of the relaxation has few fractional components, which is typical of parsing and translation problems (Rush et al., 2010; Chang and Collins, 2011), and attractive networks (Taskar et al., 2004). Unfortunately, this is not the case in summarization, where concepts “compete” with each other for inclusion in the summary, leading to frustrated cycles. We chose instead to adopt a fast and simple rounding procedure for obtaining a summary from a fractional solution. The procedure works as follows. First, solve the LP relaxation using AD3, as described ab</context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>D. Das, A. F. T. Martins, and N. A. Smith. 2012. An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints. In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Practical Structured Learning Techniques for Natural Language Processing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Southern California.</institution>
<marker>Daum´e, 2006</marker>
<rawString>H. Daum´e. 2006. Practical Structured Learning Techniques for Natural Language Processing. Ph.D. thesis, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>H. Daum´e. 2007. Frustratingly easy domain adaptation. In Proc. of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1176" citStr="Edmundson, 1969" startWordPosition="160" endWordPosition="162">uire external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers. 1 Introduction Automatic text summarization is a seminal problem in information retrieval and natural language processing (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, with the overwhelming amount of information available on the Web, the demand for fast, robust, and scalable summarization systems is stronger than ever. Up to now, extractive systems have been the most popular in multi-document summarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (avoiding finegraine</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic extracting. Journal of the ACM, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Evgeniou</author>
<author>M Pontil</author>
</authors>
<title>Regularized multi– task learning.</title>
<date>2004</date>
<booktitle>In Proc. of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>109--117</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4763" citStr="Evgeniou and Pontil (2004)" startWordPosition="700" endWordPosition="704">nly is this framework orders of magnitude more efficient than the ILP-based approaches, it also allows the three well-known metrics of summaries—conciseness, informativeness, and grammaticality—to be treated separately in a modular fashion (see Figure 1). We also contribute with a novel knapsack factor, along with a linear-time algorithm for the corresponding dual decomposition subproblem. • We propose multi-task learning (§4) as a principled way to train compressive summarizers, using auxiliary data for extractive summarization and sentence compression. To this end, we adapt the framework of Evgeniou and Pontil (2004) and Daum´e (2007) to train structured predictors that share some of their parts. Experiments on TAC data (§5) yield state-of-theart results, with runtimes similar to that of extractive systems. To our best knowledge, this had never been achieved by compressive summarizers. 2 Extractive Summarization In extractive summarization, we are given a set of sentences D := {s1, ... , sN} belonging to one or more documents, and the goal is to extract a subset S ⊆ D that conveys a good summary of D and whose total number of words does not exceed a prespecified budget B. We use an indicator vector y := h</context>
<context position="24447" citStr="Evgeniou and Pontil (2004)" startWordPosition="4073" endWordPosition="4076">del learned from that task’s data, recovering the approach of Berg-Kirkpatrick et al. (2011). In the other extreme, setting σ1 = 0 simply ignores task #1’s training data. As a result, the optimal v1 will be a vector of zeros; since tasks #2 and #3 have no parts in common, the objective will decouple into a sum of two independent terms 6Note that, by substituting uk := w + vk and solving for w, the problem in Eq. 10 becomes that of minimizing the sum of the losses with a penalty for the (weighted) variance of the vectors {0, u1, u2, u3}, regularizing the difference towards their average, as in Evgeniou and Pontil (2004). This is similar to the hierarchical joint learning approach of Finkel and Manning (2010), except that our goal is to learn a new task (compressive summarization) instead of combining tasks. 7Let Yk denote the output set for the kth task. Given a task-specific cost function Ak : Yk x Yk → R, and letting (xt, yt)Tt=1 be the labeled dataset for this task, the structured// hinge loss takes the form Lk(uk) := Etmaxy&apos;∈Y (uk · \Φk(xt, y&apos;) − Φk(xt, yt)) + Ak(y&apos;, yt)). The inner maximization over y&apos; is called the cost-augmented decoding problem: it differs from Eq. 9 by the inclusion of the cost term</context>
</contexts>
<marker>Evgeniou, Pontil, 2004</marker>
<rawString>T. Evgeniou and M. Pontil. 2004. Regularized multi– task learning. In Proc. of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 109–117. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence text extraction.</title>
<date>2004</date>
<booktitle>In Proc. of International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="6081" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="930" endWordPosition="933">rwise. Let Ln be the number of words of the nth sentence. By designing a quality score function g : {0,1}N → R, this can be cast as a global optimization problem with a knapsack constraint: maximize g(y) w.r.t. y ∈ {0,1}N s.t. ENn=1 Lnyn ≤ B. (1) Intuitively, a good summary is one which selects sentences that individually convey “relevant” information, while collectively having small “redundancy.” This trade-off was explicitly modeled in early works through the notion of maximal marginal relevance (Carbonell and Goldstein, 1998; McDonald, 2007). An alternative are coverage-based models (§2.1; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2008), which seek a set of sentences that covers as many diverse “concepts” as possible; redundancy is automatically penalized since redundant sentences cover fewer concepts. Both models can be framed under the framework of submodular optimization (Lin and Bilmes, 2010), leading to greedy algorithms that have approximation guarantees. However, extending these models to allow for sentence compression (as will be detailed in §3) breaks the diminishing returns property, making submodular optimization no longer applicable. 2.1 Coverage-Based Summarization Covera</context>
<context position="7959" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="1244" endWordPosition="1247">}N, u ∈ {0,1}M s.t. um = V n∈Jm yn, ∀m ∈ [M] ENn=1 Lnyn ≤ B, (3) where we used the notation [M] := {1, ... , M}. This can be converted into an ILP and addressed with off-the-shelf solvers (Gillick et al., 2008). A drawback of this approach is that solving an ILP exactly is NP-hard. Even though existing commercial solvers can solve most instances with a moderate speed, they still exhibit poor worst-case behaviour; this is exacerbated when there is the need to combine an extractive component with other modules, as in compressive summarization (§3). 1Previous work has modeled concepts as events (Filatova and Hatzivassiloglou, 2004), salient words (Lin and Bilmes, 2010), and word bigrams (Gillick et al., 2008). In the sequel, we assume concepts are word k-grams, but our model can handle other representations, such as phrases or predicateargument structures. 197 2.2 A Dual Decomposition Formulation We next describe how the problem in Eq. 3 can be addressed with dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular, extensible, and parallelizable manner (Komodakis et al., 2007; Rush et al., 2010). In particular, we employ alternating directions dual decomposition</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence text extraction. In Proc. of International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data. In</title>
<date>2010</date>
<booktitle>Proc. ofAnnual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24537" citStr="Finkel and Manning (2010)" startWordPosition="4088" endWordPosition="4091">). In the other extreme, setting σ1 = 0 simply ignores task #1’s training data. As a result, the optimal v1 will be a vector of zeros; since tasks #2 and #3 have no parts in common, the objective will decouple into a sum of two independent terms 6Note that, by substituting uk := w + vk and solving for w, the problem in Eq. 10 becomes that of minimizing the sum of the losses with a penalty for the (weighted) variance of the vectors {0, u1, u2, u3}, regularizing the difference towards their average, as in Evgeniou and Pontil (2004). This is similar to the hierarchical joint learning approach of Finkel and Manning (2010), except that our goal is to learn a new task (compressive summarization) instead of combining tasks. 7Let Yk denote the output set for the kth task. Given a task-specific cost function Ak : Yk x Yk → R, and letting (xt, yt)Tt=1 be the labeled dataset for this task, the structured// hinge loss takes the form Lk(uk) := Etmaxy&apos;∈Y (uk · \Φk(xt, y&apos;) − Φk(xt, yt)) + Ak(y&apos;, yt)). The inner maximization over y&apos; is called the cost-augmented decoding problem: it differs from Eq. 9 by the inclusion of the cost term Ak(y&apos;, yt). Our costs decompose over the model’s factors, hence any decoder for Eq. 9 can</context>
</contexts>
<marker>Finkel, Manning, 2010</marker>
<rawString>J.R. Finkel and C.D. Manning. 2010. Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data. In Proc. ofAnnual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillenwater</author>
<author>A Kulesza</author>
<author>B Taskar</author>
</authors>
<title>Discovering diverse and salient threads in document collections.</title>
<date>2012</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2354" citStr="Gillenwater et al., 2012" startWordPosition="338" endWordPosition="341">r each sentence as a whole (avoiding finegrained text generation), and it ensures a grammatical summary, assuming the original sentences are well-formed. The typical trade-offs in these models (maximizing relevance, and penalizing redundancy) lead to submodular optimization problems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 2012). Probabilistic models have also been proposed to capture the problem structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still o</context>
</contexts>
<marker>Gillenwater, Kulesza, Taskar, 2012</marker>
<rawString>J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Discovering diverse and salient threads in document collections. In Proc. of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>The icsi summarization system at tac</title>
<date>2008</date>
<booktitle>In Proc. of Text Understanding Conference.</booktitle>
<contexts>
<context position="1630" citStr="Gillick et al., 2008" startWordPosition="230" endWordPosition="233"> Introduction Automatic text summarization is a seminal problem in information retrieval and natural language processing (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, with the overwhelming amount of information available on the Web, the demand for fast, robust, and scalable summarization systems is stronger than ever. Up to now, extractive systems have been the most popular in multi-document summarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (avoiding finegrained text generation), and it ensures a grammatical summary, assuming the original sentences are well-formed. The typical trade-offs in these models (maximizing relevance, and penalizing redundancy) lead to submodular optimization problems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 2012). Probabilistic model</context>
<context position="6122" citStr="Gillick et al., 2008" startWordPosition="938" endWordPosition="941">e. By designing a quality score function g : {0,1}N → R, this can be cast as a global optimization problem with a knapsack constraint: maximize g(y) w.r.t. y ∈ {0,1}N s.t. ENn=1 Lnyn ≤ B. (1) Intuitively, a good summary is one which selects sentences that individually convey “relevant” information, while collectively having small “redundancy.” This trade-off was explicitly modeled in early works through the notion of maximal marginal relevance (Carbonell and Goldstein, 1998; McDonald, 2007). An alternative are coverage-based models (§2.1; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2008), which seek a set of sentences that covers as many diverse “concepts” as possible; redundancy is automatically penalized since redundant sentences cover fewer concepts. Both models can be framed under the framework of submodular optimization (Lin and Bilmes, 2010), leading to greedy algorithms that have approximation guarantees. However, extending these models to allow for sentence compression (as will be detailed in §3) breaks the diminishing returns property, making submodular optimization no longer applicable. 2.1 Coverage-Based Summarization Coverage-based extractive summarization can be </context>
<context position="7533" citStr="Gillick et al., 2008" startWordPosition="1177" endWordPosition="1180">and let the set Jm ⊆ {1, ... , N} contain the indices of the sentences in which this concept occurs. Then, the following quality score function is defined: g(y) = EMm=1 σmum(y), (2) where um(y) := Vn∈Jm yn is a Boolean function that indicates whether the mth concept is present in the summary. Plugging this into Eq. 1, one obtains the following Boolean optimization problem: maximize EMm=1 σmum w.r.t. y ∈ {0,1}N, u ∈ {0,1}M s.t. um = V n∈Jm yn, ∀m ∈ [M] ENn=1 Lnyn ≤ B, (3) where we used the notation [M] := {1, ... , M}. This can be converted into an ILP and addressed with off-the-shelf solvers (Gillick et al., 2008). A drawback of this approach is that solving an ILP exactly is NP-hard. Even though existing commercial solvers can solve most instances with a moderate speed, they still exhibit poor worst-case behaviour; this is exacerbated when there is the need to combine an extractive component with other modules, as in compressive summarization (§3). 1Previous work has modeled concepts as events (Filatova and Hatzivassiloglou, 2004), salient words (Lin and Bilmes, 2010), and word bigrams (Gillick et al., 2008). In the sequel, we assume concepts are word k-grams, but our model can handle other representa</context>
<context position="28377" citStr="Gillick et al. (2008)" startWordPosition="4711" endWordPosition="4714">multi-document summarization problems). We generated oracle extracts by maximizing bigram recall with respect to the manual abstracts, as described in Berg-Kirkpatrick et al. (2011). For sentence compression, we adapted the Simple English Wikipedia dataset of Woodsend and Lapata (2011), containing aligned sentences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletionbased compressions. 5.2 Results Table 2 shows the results. The top rows refer to three strong baselines: the ICSI-1 extractive coverage-based system of Gillick et al. (2008), which achieved the best ROUGE scores in the TAC-2008 evaluation; the compressive summarizer of Berg-Kirkpatrick et al. (2011), denoted BGK’11; and the multi-aspect compressive summarizer of Woodsend and Lapata (2012), denoted WL’12. All these systems require ILP solvers. The bottom rows show the results achieved by our implementation of a pure extractive system (similar to the learned extractive summarizer of Berg-Kirkpatrick et al., 2011); a system that postcombines extraction and compression components trained separately, as in Martins and Smith (2009); and our compressive summarizer train</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, 2008</marker>
<rawString>Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur. 2008. The icsi summarization system at tac 2008. In Proc. of Text Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization—step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI.</booktitle>
<contexts>
<context position="2730" citStr="Knight and Marcu, 2000" startWordPosition="398" endWordPosition="401"> is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 2012). Probabilistic models have also been proposed to capture the problem structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still obstacles that need to be surmounted for these systems to enjoy wide adoption. All approaches above are based on integer linear programming (ILP), suffering from slow runtimes, when compared to extractive systems. For example, Woodsend and Lapata (2012) report 55 seconds on average to produce a summary; Berg-Kirkpatrick et al. (2011) report substantially faster runtimes, but</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-based summarization—step one: Sentence compression. In AAAI/IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Komodakis</author>
<author>N Paragios</author>
<author>G Tziritas</author>
</authors>
<title>MRF optimization via dual decomposition: Message-passing revisited.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Computer Vision.</booktitle>
<contexts>
<context position="8471" citStr="Komodakis et al., 2007" startWordPosition="1326" endWordPosition="1329">essive summarization (§3). 1Previous work has modeled concepts as events (Filatova and Hatzivassiloglou, 2004), salient words (Lin and Bilmes, 2010), and word bigrams (Gillick et al., 2008). In the sequel, we assume concepts are word k-grams, but our model can handle other representations, such as phrases or predicateargument structures. 197 2.2 A Dual Decomposition Formulation We next describe how the problem in Eq. 3 can be addressed with dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular, extensible, and parallelizable manner (Komodakis et al., 2007; Rush et al., 2010). In particular, we employ alternating directions dual decomposition (AD3; Martins et al., 2011a, 2012) for solving a linear relaxation of Eq. 3. AD3 resembles the subgradientbased algorithm of Rush et al. (2010), but it enjoys a faster convergence rate. Both algorithms split the original problem into several components, and then iterate between solving independent local subproblems at each component and adjusting multipliers to promote an agreement.2 The difference between the two methods is that the AD3 local subproblems, instead of requiring the computation of a locally </context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF optimization via dual decomposition: Message-passing revisited. In Proc. of International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="1556" citStr="Kupiec et al., 1995" startWordPosition="218" endWordPosition="221">res to date, with runtimes that rival those of extractive summarizers. 1 Introduction Automatic text summarization is a seminal problem in information retrieval and natural language processing (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, with the overwhelming amount of information available on the Web, the demand for fast, robust, and scalable summarization systems is stronger than ever. Up to now, extractive systems have been the most popular in multi-document summarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (avoiding finegrained text generation), and it ensures a grammatical summary, assuming the original sentences are well-formed. The typical trade-offs in these models (maximizing relevance, and penalizing redundancy) lead to submodular optimization problems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction </context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable document summarizer. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lin</author>
<author>J Bilmes</author>
</authors>
<title>Multi-document summarization via budgeted maximization of submodular functions.</title>
<date>2010</date>
<booktitle>In Proc. of Annual Meeting of the North American chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2035" citStr="Lin and Bilmes, 2010" startWordPosition="291" endWordPosition="294">ummarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (avoiding finegrained text generation), and it ensures a grammatical summary, assuming the original sentences are well-formed. The typical trade-offs in these models (maximizing relevance, and penalizing redundancy) lead to submodular optimization problems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 2012). Probabilistic models have also been proposed to capture the problem structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2</context>
<context position="6387" citStr="Lin and Bilmes, 2010" startWordPosition="979" endWordPosition="982">lly convey “relevant” information, while collectively having small “redundancy.” This trade-off was explicitly modeled in early works through the notion of maximal marginal relevance (Carbonell and Goldstein, 1998; McDonald, 2007). An alternative are coverage-based models (§2.1; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2008), which seek a set of sentences that covers as many diverse “concepts” as possible; redundancy is automatically penalized since redundant sentences cover fewer concepts. Both models can be framed under the framework of submodular optimization (Lin and Bilmes, 2010), leading to greedy algorithms that have approximation guarantees. However, extending these models to allow for sentence compression (as will be detailed in §3) breaks the diminishing returns property, making submodular optimization no longer applicable. 2.1 Coverage-Based Summarization Coverage-based extractive summarization can be formalized as follows. Let C(D) := {c1, ... , cM} be a set of relevant concept types which are present in the original documents D.1 Let σm be a relevance score assigned to the mth concept, and let the set Jm ⊆ {1, ... , N} contain the indices of the sentences in w</context>
<context position="7997" citStr="Lin and Bilmes, 2010" startWordPosition="1250" endWordPosition="1253">yn ≤ B, (3) where we used the notation [M] := {1, ... , M}. This can be converted into an ILP and addressed with off-the-shelf solvers (Gillick et al., 2008). A drawback of this approach is that solving an ILP exactly is NP-hard. Even though existing commercial solvers can solve most instances with a moderate speed, they still exhibit poor worst-case behaviour; this is exacerbated when there is the need to combine an extractive component with other modules, as in compressive summarization (§3). 1Previous work has modeled concepts as events (Filatova and Hatzivassiloglou, 2004), salient words (Lin and Bilmes, 2010), and word bigrams (Gillick et al., 2008). In the sequel, we assume concepts are word k-grams, but our model can handle other representations, such as phrases or predicateargument structures. 197 2.2 A Dual Decomposition Formulation We next describe how the problem in Eq. 3 can be addressed with dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular, extensible, and parallelizable manner (Komodakis et al., 2007; Rush et al., 2010). In particular, we employ alternating directions dual decomposition (AD3; Martins et al., 2011a, 2012) fo</context>
</contexts>
<marker>Lin, Bilmes, 2010</marker>
<rawString>H. Lin and J. Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proc. of Annual Meeting of the North American chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>Improving summarization performance by sentence compression-a pilot study.</title>
<date>2003</date>
<booktitle>In the Int. Workshop on Inf. Ret. with Asian Languages.</booktitle>
<contexts>
<context position="2618" citStr="Lin, 2003" startWordPosition="383" endWordPosition="384">ems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 2012). Probabilistic models have also been proposed to capture the problem structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still obstacles that need to be surmounted for these systems to enjoy wide adoption. All approaches above are based on integer linear programming (ILP), suffering from slow runtimes, when compared to extractive systems. For example, Woodsend and Lapata (2012) report 55 s</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>C.-Y. Lin. 2003. Improving summarization performance by sentence compression-a pilot study. In the Int. Workshop on Inf. Ret. with Asian Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="30278" citStr="Lin, 2004" startWordPosition="5013" endWordPosition="5014">ducted another set of experiments to compare the runtime of our compressive summarizer based on AD3 with the runtimes achieved by GLPK, the ILP solver used by Berg-Kirkpatrick et al. (2011). We varied the maximum number of it202 System R-2 R-SU4 Pyr LQ ICSI-1 11.03 13.96 34.51 – BGK’11 11.71 14.47 41.31 – WL’12 11.37 14.47 – – Extractive 11.16 14.07 36.0 4.6 Post-comb. 11.07 13.85 38.4 4.1 Single-task 11.88 14.86 41.0 3.8 Multi-task 12.30 15.18 42.6 4.2 Table 2: Results for compressive summarization. Shown are the ROUGE-2 and ROUGE SU-4 recalls with the default options from the ROUGE toolkit (Lin, 2004); Pyramid scores (Nenkova and Passonneau, 2004); and linguistic quality scores, scored between 1 (very bad) to 5 (very good). For Pyramid, the evaluation was performed by two annotators, each evaluating half of the problems; scores marked with † were computed by different annotators and are not directly comparable. Linguistic quality was evaluated by two linguists; we show the average of the reported scores. Solver Runtime (sec.) ROUGE-2 ILP Exact 10.394 12.40 LP-Relax. 2.265 12.38 AD3-5000 0.952 12.38 AD3-1000 0.406 12.30 AD3-200 0.159 12.15 Extractive (ILP) 0.265 11.16 Table 3: Runtimes of s</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1141" citStr="Luhn, 1958" startWordPosition="156" endWordPosition="157">ng, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers. 1 Introduction Automatic text summarization is a seminal problem in information retrieval and natural language processing (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, with the overwhelming amount of information available on the Web, the demand for fast, robust, and scalable summarization systems is stronger than ever. Up to now, extractive systems have been the most popular in multi-document summarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sente</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
</authors>
<title>Summarization with a Joint Model for Sentence Extraction and Compression.</title>
<date>2009</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics: Workshop on Integer Linear Programming for NLP.</booktitle>
<contexts>
<context position="2876" citStr="Martins and Smith, 2009" startWordPosition="418" endWordPosition="421">ed to capture the problem structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still obstacles that need to be surmounted for these systems to enjoy wide adoption. All approaches above are based on integer linear programming (ILP), suffering from slow runtimes, when compared to extractive systems. For example, Woodsend and Lapata (2012) report 55 seconds on average to produce a summary; Berg-Kirkpatrick et al. (2011) report substantially faster runtimes, but fewer compressions are allowed. Having a compressive summarizer which is both fast and expressive remains an open problem. A second inconvenience</context>
<context position="11256" citStr="Martins and Smith (2009)" startWordPosition="1793" endWordPosition="1796">d tokens, sn := htn,`iLn `=0, where tn,0 ≡ $ is a dummy symbol. We represent a compression of sn as an indicator vector zn := hzn,`iLn `=0, where zn,` = 1 if the Eth word is included in the compression. By convention, the dummy symbol is included if and only if the remaining compression is non-empty. A compressive summary can then be represented by an indicator vector z which is the concatenation of N such vectors, z = hz1,... , zNi; each position in this indicator vector is indexed by a sentence n ∈ [N] and a word position E ∈ {0} ∪ [Ln]. Models for compressive summarization were proposed by Martins and Smith (2009) and BergKirkpatrick et al. (2011) by combining extraction and compression scores. Here, we follow the latter work, by combining a coverage score function g with sentence-level compression score functions h1, ... , hN. This yields the decoding problem: maximize g(z) + PNn=1 hn(zn) w.r.t. zn ∈ {0,1}Ln, ∀n ∈ [N] s.t. PN PLn `=1 zn,` ≤ B. (4) n=1 3.1 Coverage Model We use a coverage function similar to Eq. 2, but taking a compressive summary z as argument: g(z) = PMm=1 Qmum(z), (5) where we redefine um as follows. First, we parametrize each occurrence of the mth concept (assumed to be a k-gram) a</context>
<context position="13280" citStr="Martins and Smith (2009)" startWordPosition="2141" endWordPosition="2144">dget factor, in green, is connected to the word nodes; it ensures that the summary fits the word limit. Shaded circles represent active variables while white circles represent inactive variables. Concept type Budget &amp;quot;Kashmiri separatists&amp;quot; Concept tokens $ Talks with Kashmiri separatists began last year ... Sentences $ The leader of moderate Kashmiri separatists warned Thursday that ... We set concept scores as Qm := w · Φcov(D, cm), where Φcov(D, cm) is a vector of features (described in §3.5) and w the corresponding weights. 3.2 Compression Model For the compression score function, we follow Martins and Smith (2009) and decompose it as a sum of local score functions Pn,` defined on dependency arcs: hn(zn) := �Ln `=1 Pn,`(zn,`, zn,π(`)), (8) where 7r(E) denotes the index of the word which is the parent of the Eth word in the dependency tree (by convention, the root of the tree is the dummy symbol). To model the event that an arc is “cut” by disconnecting a child from its head, we define arc-deletion scores Pn,`(0,1) := w · Φcomp(sn, E, 7r(E)), where Φcomp is a feature map, which is described in detail in §3.5. We set Pn,`(0, 0) = Pn,`(1,1) = 0, and Pn,`(1, 0) = −∞, to allow only the deletion of entire sub</context>
<context position="19647" citStr="Martins and Smith (2009)" startWordPosition="3245" endWordPosition="3248">blings of such arcs (to prevent inconsistencies in handling coordinative conjunctions); arcs linking verbs to other verbs, to adjectives (e.g., make available), to verb particles (e.g., settle down), to the word that (e.g., said that), or to the word to if it is a leaf (e.g., allowed to come); arcs pointing to negation words, cardinal numbers, or determiners; and arcs connecting two proper nouns or words within quotation marks. 4 Multi-Task Learning We next turn to the problem of learning the model from training data. Prior work in compressive summarization has followed one of two strategies: Martins and Smith (2009) and Woodsend and Lapata (2012) learn the extraction and compression models separately, and then post-combine them, circumventing the lack of fully annotated data. Berg-Kirkpatrick et al. (2011) gathered a small dataset of manually compressed summaries, and trained with full supervision. While the latter approach is statistically more principled, it has the disadvantage of requiring fully annotated data, which is difficult to obtain in large quantities. On the other hand, there is plenty of data containing manually written abstracts (from the DUC and TAC conferences) and user-generated text (f</context>
<context position="25476" citStr="Martins and Smith (2009)" startWordPosition="4259" endWordPosition="4262">) := Etmaxy&apos;∈Y (uk · \Φk(xt, y&apos;) − Φk(xt, yt)) + Ak(y&apos;, yt)). The inner maximization over y&apos; is called the cost-augmented decoding problem: it differs from Eq. 9 by the inclusion of the cost term Ak(y&apos;, yt). Our costs decompose over the model’s factors, hence any decoder for Eq. 9 can be used for the maximization above: for tasks #1–#2, we solve a relaxation by running AD3 without rounding, and for task #3 we use dynamic programming; see Table 1. σkLk(w + vk), (10) 3 1 + N � k=1 201 involving v2 and v3, which is equivalent to training the two tasks separately and post-combining the models, as Martins and Smith (2009) did. 5 Experiments 5.1 Experimental setup We evaluated our compressive summarizers on data from the Text Analysis Conference (TAC) evaluations. We use the same splits as previous work (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012): the non-update portions of TAC-2009 for training and TAC-2008 for testing. In addition, we reserved TAC-2010 as a devset. The test partition contains 48 multi-document summarization problems; each provides 10 related news articles as input, and asks for a summary with up to 100 words, which is evaluated against four manually written abstracts. We ignore</context>
<context position="28939" citStr="Martins and Smith (2009)" startWordPosition="4794" endWordPosition="4797">SI-1 extractive coverage-based system of Gillick et al. (2008), which achieved the best ROUGE scores in the TAC-2008 evaluation; the compressive summarizer of Berg-Kirkpatrick et al. (2011), denoted BGK’11; and the multi-aspect compressive summarizer of Woodsend and Lapata (2012), denoted WL’12. All these systems require ILP solvers. The bottom rows show the results achieved by our implementation of a pure extractive system (similar to the learned extractive summarizer of Berg-Kirkpatrick et al., 2011); a system that postcombines extraction and compression components trained separately, as in Martins and Smith (2009); and our compressive summarizer trained as a single task, and in the multi-task setting. The ROUGE and Pyramid scores show that the compressive summarizers (when properly trained) yield considerable benefits in content coverage over extractive systems, confirming the results of Berg-Kirkpatrick et al. (2011). Comparing the two bottom rows, we see a clear benefit by training in the multi-task setting, with a consistent gain in both coverage and linguistic quality. Our ROUGE-2 score (12.30%) is, to our knowledge, the highest reported on the TAC-2008 dataset, with little harm in grammaticality w</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>A. F. T. Martins and N. A. Smith. 2009. Summarization with a Joint Model for Sentence Extraction and Compression. In North American Chapter of the Association for Computational Linguistics: Workshop on Integer Linear Programming for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>M A T Figueiredo</author>
<author>P M Q Aguiar</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>An Augmented Lagrangian Approach to Constrained MAP Inference. In</title>
<date>2011</date>
<booktitle>Proc. of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="8586" citStr="Martins et al., 2011" startWordPosition="1344" endWordPosition="1347">nt words (Lin and Bilmes, 2010), and word bigrams (Gillick et al., 2008). In the sequel, we assume concepts are word k-grams, but our model can handle other representations, such as phrases or predicateargument structures. 197 2.2 A Dual Decomposition Formulation We next describe how the problem in Eq. 3 can be addressed with dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular, extensible, and parallelizable manner (Komodakis et al., 2007; Rush et al., 2010). In particular, we employ alternating directions dual decomposition (AD3; Martins et al., 2011a, 2012) for solving a linear relaxation of Eq. 3. AD3 resembles the subgradientbased algorithm of Rush et al. (2010), but it enjoys a faster convergence rate. Both algorithms split the original problem into several components, and then iterate between solving independent local subproblems at each component and adjusting multipliers to promote an agreement.2 The difference between the two methods is that the AD3 local subproblems, instead of requiring the computation of a locally optimal configuration, require solving a local quadratic problem. Martins et al. (2011b) provided linear-time solut</context>
<context position="15578" citStr="Martins et al. (2011" startWordPosition="2548" endWordPosition="2551">for this factor can be addressed by solving a sequence of linear subproblems, as described by Martins et al. (2012). Each of these subproblems corresponds to maximizing an objective function of the same form as Eq. 8; this can be done in O(Ln) time with dynamic programming, as discussed in §3.2. 2. For each of the M concept types in C(D), one OR-WITH-OUTPUT factor for the logic constraint in Eq. 6. This is analogous to the one described for the extractive case. 3. For each k-gram concept token in Tm, one AND-WITH-OUTPUT factor that imposes the constraint in Eq. 7. This factor was described by Martins et al. (2011b) and its AD3 subproblem can be solved in time linear in k. 4. Another component linked to all the words imposing that at most B words can be selected; this is done via a BUDGET factor, a particular case of KNAPSACK. The runtime of this AD3 subproblem is linear in the number of words. In addition, we found it useful to add a second BUDGET factor limiting the number of sentences that can be selected to a prescribed value K. We set K = 6 in our experiments. 199 3.4 Rounding Strategy Recall that the problem in Eq. 4 is NP-hard, and that AD3 is solving a linear relaxation. While there are ways of</context>
<context position="31815" citStr="Martins et al., 2011" startWordPosition="5260" endWordPosition="5263">, 1000, 5000}, and clocked the time spent by GLPK to solve the exact ILPs and their relaxations. Table 3 depicts the results.10 We see that our proposed configuration (AD3- 1000) is orders of magnitude faster than the ILP solver, and 5 times faster than its relaxed variant, while keeping similar accuracy levels.11 The gain when the number of iterations in AD3 is increased to 5000 is small, given that the runtime is more 10Within dual decomposition algorithms, we verified experimentally that AD3 is substantially faster than the subgradient algorithm, which is consistent with previous findings (Martins et al., 2011b). 11The runtimes obtained with the exact ILP solver seem slower than those reported by Berg-Kirkpatrick et al. (2011). (around 1.5 sec. on average, according to their Fig. 3). We conjecture that this difference is due to the restricted set of subtrees that can be deleted by Berg-Kirkpatrick et al. (2011), which greatly reduces their search space. Japan dispatched four military ships to help Russia rescue seven crew members aboard a small submarine trapped on the seabed in the Far East. The Russian Pacific Fleet said the crew had 120 hours of oxygen reserves on board when the submarine submer</context>
</contexts>
<marker>Martins, Figueiredo, Aguiar, Smith, Xing, 2011</marker>
<rawString>A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing. 2011a. An Augmented Lagrangian Approach to Constrained MAP Inference. In Proc. of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Dual Decomposition with Many Overlapping Components.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="8586" citStr="Martins et al., 2011" startWordPosition="1344" endWordPosition="1347">nt words (Lin and Bilmes, 2010), and word bigrams (Gillick et al., 2008). In the sequel, we assume concepts are word k-grams, but our model can handle other representations, such as phrases or predicateargument structures. 197 2.2 A Dual Decomposition Formulation We next describe how the problem in Eq. 3 can be addressed with dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular, extensible, and parallelizable manner (Komodakis et al., 2007; Rush et al., 2010). In particular, we employ alternating directions dual decomposition (AD3; Martins et al., 2011a, 2012) for solving a linear relaxation of Eq. 3. AD3 resembles the subgradientbased algorithm of Rush et al. (2010), but it enjoys a faster convergence rate. Both algorithms split the original problem into several components, and then iterate between solving independent local subproblems at each component and adjusting multipliers to promote an agreement.2 The difference between the two methods is that the AD3 local subproblems, instead of requiring the computation of a locally optimal configuration, require solving a local quadratic problem. Martins et al. (2011b) provided linear-time solut</context>
<context position="15578" citStr="Martins et al. (2011" startWordPosition="2548" endWordPosition="2551">for this factor can be addressed by solving a sequence of linear subproblems, as described by Martins et al. (2012). Each of these subproblems corresponds to maximizing an objective function of the same form as Eq. 8; this can be done in O(Ln) time with dynamic programming, as discussed in §3.2. 2. For each of the M concept types in C(D), one OR-WITH-OUTPUT factor for the logic constraint in Eq. 6. This is analogous to the one described for the extractive case. 3. For each k-gram concept token in Tm, one AND-WITH-OUTPUT factor that imposes the constraint in Eq. 7. This factor was described by Martins et al. (2011b) and its AD3 subproblem can be solved in time linear in k. 4. Another component linked to all the words imposing that at most B words can be selected; this is done via a BUDGET factor, a particular case of KNAPSACK. The runtime of this AD3 subproblem is linear in the number of words. In addition, we found it useful to add a second BUDGET factor limiting the number of sentences that can be selected to a prescribed value K. We set K = 6 in our experiments. 199 3.4 Rounding Strategy Recall that the problem in Eq. 4 is NP-hard, and that AD3 is solving a linear relaxation. While there are ways of</context>
<context position="31815" citStr="Martins et al., 2011" startWordPosition="5260" endWordPosition="5263">, 1000, 5000}, and clocked the time spent by GLPK to solve the exact ILPs and their relaxations. Table 3 depicts the results.10 We see that our proposed configuration (AD3- 1000) is orders of magnitude faster than the ILP solver, and 5 times faster than its relaxed variant, while keeping similar accuracy levels.11 The gain when the number of iterations in AD3 is increased to 5000 is small, given that the runtime is more 10Within dual decomposition algorithms, we verified experimentally that AD3 is substantially faster than the subgradient algorithm, which is consistent with previous findings (Martins et al., 2011b). 11The runtimes obtained with the exact ILP solver seem slower than those reported by Berg-Kirkpatrick et al. (2011). (around 1.5 sec. on average, according to their Fig. 3). We conjecture that this difference is due to the restricted set of subtrees that can be deleted by Berg-Kirkpatrick et al. (2011), which greatly reduces their search space. Japan dispatched four military ships to help Russia rescue seven crew members aboard a small submarine trapped on the seabed in the Far East. The Russian Pacific Fleet said the crew had 120 hours of oxygen reserves on board when the submarine submer</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2011b. Dual Decomposition with Many Overlapping Components. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre F T Martins</author>
<author>Mario A T Figueiredo</author>
<author>Pedro M Q Aguiar</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Alternating Directions Dual Decomposition. Arxiv preprint arXiv:1212.6550.</title>
<date>2012</date>
<contexts>
<context position="15073" citStr="Martins et al. (2012)" startWordPosition="2456" endWordPosition="2459">ession models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. similar manner as described in §2, but with an additional component for the sentence compressor, and slight modifications in the other components. We have the following N + M + EMm=1 |Tm |+ 1 components in total, illustrated in Figure 1: 1. For each of the N sentences, one component for the compression model. The AD3 quadratic subproblem for this factor can be addressed by solving a sequence of linear subproblems, as described by Martins et al. (2012). Each of these subproblems corresponds to maximizing an objective function of the same form as Eq. 8; this can be done in O(Ln) time with dynamic programming, as discussed in §3.2. 2. For each of the M concept types in C(D), one OR-WITH-OUTPUT factor for the logic constraint in Eq. 6. This is analogous to the one described for the extractive case. 3. For each k-gram concept token in Tm, one AND-WITH-OUTPUT factor that imposes the constraint in Eq. 7. This factor was described by Martins et al. (2011b) and its AD3 subproblem can be solved in time linear in k. 4. Another component linked to all</context>
</contexts>
<marker>Martins, Figueiredo, Aguiar, Smith, Xing, 2012</marker>
<rawString>Andre F. T. Martins, Mario A. T. Figueiredo, Pedro M. Q. Aguiar, Noah A. Smith, and Eric P. Xing. 2012. Alternating Directions Dual Decomposition. Arxiv preprint arXiv:1212.6550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic constraints.</title>
<date>2006</date>
<booktitle>In Proc. of Annual Meeting of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3801" citStr="McDonald, 2006" startWordPosition="559" endWordPosition="560">nd and Lapata (2012) report 55 seconds on average to produce a summary; Berg-Kirkpatrick et al. (2011) report substantially faster runtimes, but fewer compressions are allowed. Having a compressive summarizer which is both fast and expressive remains an open problem. A second inconvenience of ILP-based approaches is that they do not exploit the modularity of the problem, since the declarative specification required by ILP solvers discards important structural information. For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). 196 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 196–206, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics This paper makes progress in two fronts: • We derive a dual decomposition framework for extractive and compressive summarization (§2– 3). Not only is this framework orders of magnitude more efficient than the ILP-based approaches, it also allows the three well-known metrics of summaries—conciseness, informativeness, and grammaticality—to be treated separately in a modular fashion (see Figure 1). We also </context>
<context position="14546" citStr="McDonald (2006)" startWordPosition="2368" endWordPosition="2369">8 efficiently with dynamic programming (using the Viterbi algorithm for trees); the total cost is linear in Ln. We will exploit this fact in the dual decomposition framework described next.4 3.3 A Dual Decomposition Formulation In previous work, the optimization problem in Eq. 4 was converted into an ILP and fed to an offthe-shelf solver (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012). Here, we employ the AD3 algorithm, in a 4The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. similar manner as described in §2, but with an additional component for the sentence compressor, and slight modifications in the other components. We have the following N + M + EMm=1 |Tm |+ 1 components in total, illustrated in Figure 1: 1. For each of the N sentences, one component for the compression model. The AD3 quadratic subproblem for this factor can be addressed by solving a sequence of linear subproblems, as described by Martins et al. (2012). Each of these subproblems corresponds to maximizing an objective functi</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative sentence compression with soft syntactic constraints. In Proc. of Annual Meeting of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In ECIR.</booktitle>
<contexts>
<context position="5996" citStr="McDonald, 2007" startWordPosition="922" endWordPosition="923">nt an extractive summary, where yn = 1 if sn ∈ S, and yn = 0 otherwise. Let Ln be the number of words of the nth sentence. By designing a quality score function g : {0,1}N → R, this can be cast as a global optimization problem with a knapsack constraint: maximize g(y) w.r.t. y ∈ {0,1}N s.t. ENn=1 Lnyn ≤ B. (1) Intuitively, a good summary is one which selects sentences that individually convey “relevant” information, while collectively having small “redundancy.” This trade-off was explicitly modeled in early works through the notion of maximal marginal relevance (Carbonell and Goldstein, 1998; McDonald, 2007). An alternative are coverage-based models (§2.1; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2008), which seek a set of sentences that covers as many diverse “concepts” as possible; redundancy is automatically penalized since redundant sentences cover fewer concepts. Both models can be framed under the framework of submodular optimization (Lin and Bilmes, 2010), leading to greedy algorithms that have approximation guarantees. However, extending these models to allow for sentence compression (as will be detailed in §3) breaks the diminishing returns property, making </context>
<context position="10477" citStr="McDonald, 2007" startWordPosition="1648" endWordPosition="1649">whose AD3 subproblem is solvable in time O(N). The actual algorithm is described in the appendix (Algorithm 1).3 3 Compressive Summarization We now turn to compressive summarization, which does not limit the summary sentences to be verbatim extracts from the original documents; in2For details about dual decomposition and Lagrangian relaxation, see the recent tutorial by Rush and Collins (2012). 3The AD3 subproblem in this case corresponds to computing an Euclidean projection onto the knapsack polytope (Eq. 11). Others addressed the related, but much harder, integer quadratic knapsack problem (McDonald, 2007). stead, it allows the extraction of compressed sentences where some words can be deleted. Formally, let us express each sentence of D as a sequence of word tokens, sn := htn,`iLn `=0, where tn,0 ≡ $ is a dummy symbol. We represent a compression of sn as an indicator vector zn := hzn,`iLn `=0, where zn,` = 1 if the Eth word is included in the compression. By convention, the dummy symbol is included if and only if the remaining compression is non-empty. A compressive summary can then be represented by an indicator vector z which is the concatenation of N such vectors, z = hz1,... , zNi; each po</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In ECIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="30325" citStr="Nenkova and Passonneau, 2004" startWordPosition="5017" endWordPosition="5020">ments to compare the runtime of our compressive summarizer based on AD3 with the runtimes achieved by GLPK, the ILP solver used by Berg-Kirkpatrick et al. (2011). We varied the maximum number of it202 System R-2 R-SU4 Pyr LQ ICSI-1 11.03 13.96 34.51 – BGK’11 11.71 14.47 41.31 – WL’12 11.37 14.47 – – Extractive 11.16 14.07 36.0 4.6 Post-comb. 11.07 13.85 38.4 4.1 Single-task 11.88 14.86 41.0 3.8 Multi-task 12.30 15.18 42.6 4.2 Table 2: Results for compressive summarization. Shown are the ROUGE-2 and ROUGE SU-4 recalls with the default options from the ROUGE toolkit (Lin, 2004); Pyramid scores (Nenkova and Passonneau, 2004); and linguistic quality scores, scored between 1 (very bad) to 5 (very good). For Pyramid, the evaluation was performed by two annotators, each evaluating half of the problems; scores marked with † were computed by different annotators and are not directly comparable. Linguistic quality was evaluated by two linguists; we show the average of the reported scores. Solver Runtime (sec.) ROUGE-2 ILP Exact 10.394 12.40 LP-Relax. 2.265 12.38 AD3-5000 0.952 12.38 AD3-1000 0.406 12.30 AD3-200 0.159 12.15 Extractive (ILP) 0.265 11.16 Table 3: Runtimes of several decoders on a Intel Core i7 processor @2</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of NAACL, pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panos M Pardalos</author>
<author>Naina Kovoor</author>
</authors>
<title>An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds.</title>
<date>1990</date>
<booktitle>Mathematical Programming,</booktitle>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="35722" citStr="Pardalos and Kovoor (1990)" startWordPosition="5923" endWordPosition="5926">e a E RN and Ln &gt; 0, bn E [N]. This includes as special cases the problems of projecting onto a budget constraint (Ln = 1, bn) and onto the simplex (same, plus B = 1). Let clip(t) := max{0, min{1, t}}. Algorithm 1 starts by clipping a to the unit interval; if that yields a z satisfying ENn=1 Lnzn &lt; B, we are done. Otherwise, the solution of Eq. 11 must satisfy ENn=1 Lnzn = B. It can be shown from the KKT conditions that the solution is of the form z*n := clip(an +T*Ln) for a constant T* lying in a particular interval of split-points (line 11). To seek this constant, we use an algorithm due to Pardalos and Kovoor (1990) which iteratively shrinks this interval. The algorithm requires computing medians as a subroutine, which can be done in linear time (Blum et al., 1973). The overall complexity in O(N) (Pardalos and Kovoor, 1990). Acknowledgments We thank all reviewers for their insightful comments; Trevor Cohn for helpful discussions about multi-task learning; Taylor Berg-Kirkpatrick for answering questions about their summarizer and for providing code; and Helena Figueira and Pedro Mendes for helping with manual evaluation. This work was partially supported by the EU/FEDER programme, QREN/POR Lisboa (Portuga</context>
</contexts>
<marker>Pardalos, Kovoor, 1990</marker>
<rawString>Panos M. Pardalos and Naina Kovoor. 1990. An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds. Mathematical Programming, 46(1):321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In the NAACL-ANLP Workshop on Automatic Summarization.</booktitle>
<contexts>
<context position="1607" citStr="Radev et al., 2000" startWordPosition="226" endWordPosition="229">ctive summarizers. 1 Introduction Automatic text summarization is a seminal problem in information retrieval and natural language processing (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, with the overwhelming amount of information available on the Web, the demand for fast, robust, and scalable summarization systems is stronger than ever. Up to now, extractive systems have been the most popular in multi-document summarization. These systems produce a summary by extracting a representative set of sentences from the original documents (Kupiec et al., 1995; Carbonell and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (avoiding finegrained text generation), and it ensures a grammatical summary, assuming the original sentences are well-formed. The typical trade-offs in these models (maximizing relevance, and penalizing redundancy) lead to submodular optimization problems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 201</context>
</contexts>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>D. R. Radev, H. Jing, and M. Budzikowska. 2000. Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies. In the NAACL-ANLP Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Rush</author>
<author>M Collins</author>
</authors>
<title>A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>45--305</pages>
<contexts>
<context position="10258" citStr="Rush and Collins (2012)" startWordPosition="1613" endWordPosition="1616">he OR-WITHOUTPUT factor described by Martins et al. (2011b); the AD3 subproblem for the mth factor can be solved in time O(|Im|). 2. Another component for the knapsack constraint. This corresponds to a (novel) KNAPSACK factor, whose AD3 subproblem is solvable in time O(N). The actual algorithm is described in the appendix (Algorithm 1).3 3 Compressive Summarization We now turn to compressive summarization, which does not limit the summary sentences to be verbatim extracts from the original documents; in2For details about dual decomposition and Lagrangian relaxation, see the recent tutorial by Rush and Collins (2012). 3The AD3 subproblem in this case corresponds to computing an Euclidean projection onto the knapsack polytope (Eq. 11). Others addressed the related, but much harder, integer quadratic knapsack problem (McDonald, 2007). stead, it allows the extraction of compressed sentences where some words can be deleted. Formally, let us express each sentence of D as a sequence of word tokens, sn := htn,`iLn `=0, where tn,0 ≡ $ is a dummy symbol. We represent a compression of sn as an indicator vector zn := hzn,`iLn `=0, where zn,` = 1 if the Eth word is included in the compression. By convention, the dumm</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>A.M. Rush and M. Collins. 2012. A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing. Journal of Artificial Intelligence Research, 45:305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="8491" citStr="Rush et al., 2010" startWordPosition="1330" endWordPosition="1333">). 1Previous work has modeled concepts as events (Filatova and Hatzivassiloglou, 2004), salient words (Lin and Bilmes, 2010), and word bigrams (Gillick et al., 2008). In the sequel, we assume concepts are word k-grams, but our model can handle other representations, such as phrases or predicateargument structures. 197 2.2 A Dual Decomposition Formulation We next describe how the problem in Eq. 3 can be addressed with dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular, extensible, and parallelizable manner (Komodakis et al., 2007; Rush et al., 2010). In particular, we employ alternating directions dual decomposition (AD3; Martins et al., 2011a, 2012) for solving a linear relaxation of Eq. 3. AD3 resembles the subgradientbased algorithm of Rush et al. (2010), but it enjoys a faster convergence rate. Both algorithms split the original problem into several components, and then iterate between solving independent local subproblems at each component and adjusting multipliers to promote an agreement.2 The difference between the two methods is that the AD3 local subproblems, instead of requiring the computation of a locally optimal configuratio</context>
<context position="16405" citStr="Rush et al., 2010" startWordPosition="2702" endWordPosition="2705">APSACK. The runtime of this AD3 subproblem is linear in the number of words. In addition, we found it useful to add a second BUDGET factor limiting the number of sentences that can be selected to a prescribed value K. We set K = 6 in our experiments. 199 3.4 Rounding Strategy Recall that the problem in Eq. 4 is NP-hard, and that AD3 is solving a linear relaxation. While there are ways of wrapping AD3 in an exact search algorithm (Das et al., 2012), such strategies work best when the solution of the relaxation has few fractional components, which is typical of parsing and translation problems (Rush et al., 2010; Chang and Collins, 2011), and attractive networks (Taskar et al., 2004). Unfortunately, this is not the case in summarization, where concepts “compete” with each other for inclusion in the summary, leading to frustrated cycles. We chose instead to adopt a fast and simple rounding procedure for obtaining a summary from a fractional solution. The procedure works as follows. First, solve the LP relaxation using AD3, as described above. This yields a solution z*, where each component lies in the unit interval [0, 1]. If these components are all integer, then we have a certificate that this is th</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Schilder</author>
<author>Ravikumar Kondadadi</author>
</authors>
<title>Fastsum: Fast and accurate query-based multi-document summarization.</title>
<date>2008</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="34600" citStr="Schilder and Kondadadi, 2008" startWordPosition="5716" endWordPosition="5719">in a principled manner. We decode with AD3, a fast and modular dual decomposition algorithm which is orders of magnitude faster than ILP-based approaches. Results show that the state of the art is improved in automatic and manual metrics, with speeds close to extractive systems. Our approach is modular and easy to extend. For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). Other aspects may be added as additional components in our dual decomposition framework, such as query information (Schilder and Kondadadi, 2008), discourse con203 straints (Clarke and Lapata, 2007), or lexical preferences (Woodsend and Lapata, 2012). Our multitask approach may be used to jointly learn parameters for these aspects; the dual decomposition algorithm ensures that optimization remains tractable even with many components. A Projection Onto Knapsack This section describes a linear-time algorithm (Algorithm 1) for solving the following problem: minimize 11z − a112 w.r.t. zn E [0, 1], bn E [N], s.t. ENn=1 Lnzn &lt; B, (11) where a E RN and Ln &gt; 0, bn E [N]. This includes as special cases the problems of projecting onto a budget c</context>
</contexts>
<marker>Schilder, Kondadadi, 2008</marker>
<rawString>Frank Schilder and Ravikumar Kondadadi. 2008. Fastsum: Fast and accurate query-based multi-document summarization. In Proc. of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sipos</author>
<author>P Shivaswamy</author>
<author>T Joachims</author>
</authors>
<title>Large-margin learning of submodular summarization models.</title>
<date>2012</date>
<contexts>
<context position="2186" citStr="Sipos et al., 2012" startWordPosition="313" endWordPosition="316">and Goldstein, 1998; Radev et al., 2000; Gillick et al., 2008). This approach has obvious advantages: it reduces the search space by letting decisions be made for each sentence as a whole (avoiding finegrained text generation), and it ensures a grammatical summary, assuming the original sentences are well-formed. The typical trade-offs in these models (maximizing relevance, and penalizing redundancy) lead to submodular optimization problems (Lin and Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 2012). Probabilistic models have also been proposed to capture the problem structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have</context>
</contexts>
<marker>Sipos, Shivaswamy, Joachims, 2012</marker>
<rawString>R. Sipos, P. Shivaswamy, and T. Joachims. 2012. Large-margin learning of submodular summarization models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Maxmargin Markov networks.</title>
<date>2003</date>
<booktitle>In Proc. of Neural Information Processing Systems.</booktitle>
<contexts>
<context position="22701" citStr="Taskar et al., 2003" startWordPosition="3758" endWordPosition="3761">ch feature into task-component features and a shared feature; but here we do not duplicate features explicitly. To learn the weights, we regularize the weight vectors separately, and assume that each task has its own loss function Lk, so that the total loss L is a weighted sum L(w, v1, v2, v3) := E3k=1 σkLk(w + vk). This yields the following objective function to be minimized: F(w, v1, v2, v3) = 2 IIwII2 + λ 3 λk2 IIvkII2 k=1 where λ and the λk’s are regularization constants, and N is the total number of training instances.6 In our experiments (§5), we let the Lk’s be structured hinge losses (Taskar et al., 2003; Tsochantaridis et al., 2004), where the corresponding cost functions are concept recall (for task #2), precision of arc deletions (for task #3), and a combination thereof (for task #1).7 These losses were normalized, and we set σk = N/Nk, where Nk is the number of training instances for the kth task. This ensures all tasks are weighted evenly. We used the same rationale to set λ = λ1 = λ2 = λ3, choosing this value through cross-validation in the dev set. We optimize Eq. 10 with stochastic subgradient descent. This leads to update rules of the form w +-- (1 − ηtλ)w − ηtσk ˜tLk(w + vk) vj +-- </context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Maxmargin Markov networks. In Proc. of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>V Chatalbashev</author>
<author>D Koller</author>
</authors>
<title>Learning associative Markov networks.</title>
<date>2004</date>
<booktitle>In Proc. of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="16478" citStr="Taskar et al., 2004" startWordPosition="2713" endWordPosition="2716">words. In addition, we found it useful to add a second BUDGET factor limiting the number of sentences that can be selected to a prescribed value K. We set K = 6 in our experiments. 199 3.4 Rounding Strategy Recall that the problem in Eq. 4 is NP-hard, and that AD3 is solving a linear relaxation. While there are ways of wrapping AD3 in an exact search algorithm (Das et al., 2012), such strategies work best when the solution of the relaxation has few fractional components, which is typical of parsing and translation problems (Rush et al., 2010; Chang and Collins, 2011), and attractive networks (Taskar et al., 2004). Unfortunately, this is not the case in summarization, where concepts “compete” with each other for inclusion in the summary, leading to frustrated cycles. We chose instead to adopt a fast and simple rounding procedure for obtaining a summary from a fractional solution. The procedure works as follows. First, solve the LP relaxation using AD3, as described above. This yields a solution z*, where each component lies in the unit interval [0, 1]. If these components are all integer, then we have a certificate that this is the optimal solution. Otherwise, we collect the K sentences with the highes</context>
</contexts>
<marker>Taskar, Chatalbashev, Koller, 2004</marker>
<rawString>B. Taskar, V. Chatalbashev, and D. Koller. 2004. Learning associative Markov networks. In Proc. of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proc. of International Conference of Machine Learning.</booktitle>
<contexts>
<context position="22731" citStr="Tsochantaridis et al., 2004" startWordPosition="3762" endWordPosition="3765">component features and a shared feature; but here we do not duplicate features explicitly. To learn the weights, we regularize the weight vectors separately, and assume that each task has its own loss function Lk, so that the total loss L is a weighted sum L(w, v1, v2, v3) := E3k=1 σkLk(w + vk). This yields the following objective function to be minimized: F(w, v1, v2, v3) = 2 IIwII2 + λ 3 λk2 IIvkII2 k=1 where λ and the λk’s are regularization constants, and N is the total number of training instances.6 In our experiments (§5), we let the Lk’s be structured hinge losses (Taskar et al., 2003; Tsochantaridis et al., 2004), where the corresponding cost functions are concept recall (for task #2), precision of arc deletions (for task #3), and a combination thereof (for task #1).7 These losses were normalized, and we set σk = N/Nk, where Nk is the number of training instances for the kth task. This ensures all tasks are weighted evenly. We used the same rationale to set λ = λ1 = λ2 = λ3, choosing this value through cross-validation in the dev set. We optimize Eq. 10 with stochastic subgradient descent. This leads to update rules of the form w +-- (1 − ηtλ)w − ηtσk ˜tLk(w + vk) vj +-- (1 − ηtλj)vj − ηtδjkσk ˜tLk(w </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proc. of International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Woodsend</author>
<author>M Lapata</author>
</authors>
<title>Automatic generation of story highlights.</title>
<date>2010</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>565--574</pages>
<contexts>
<context position="2903" citStr="Woodsend and Lapata, 2010" startWordPosition="422" endWordPosition="425"> structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still obstacles that need to be surmounted for these systems to enjoy wide adoption. All approaches above are based on integer linear programming (ILP), suffering from slow runtimes, when compared to extractive systems. For example, Woodsend and Lapata (2012) report 55 seconds on average to produce a summary; Berg-Kirkpatrick et al. (2011) report substantially faster runtimes, but fewer compressions are allowed. Having a compressive summarizer which is both fast and expressive remains an open problem. A second inconvenience of ILP-based approaches is</context>
</contexts>
<marker>Woodsend, Lapata, 2010</marker>
<rawString>K. Woodsend and M. Lapata. 2010. Automatic generation of story highlights. In Proc. of Annual Meeting of the Association for Computational Linguistics, pages 565–574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to simplify sentences with quasi-synchronous grammar and integer programming.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="28042" citStr="Woodsend and Lapata (2011)" startWordPosition="4659" endWordPosition="4663">umber of iterations to 200 at training time and 1000 at test time. We extended the code to handle the knapsack and budget factors; the modified code will be part of the next release (AD3 2.1). 9http://www.ark.cs.cmu.edu/TurboParser compressive summarizer. For extractive summarization, we used the DUC 2003 and 2004 datasets (a total of 80 multi-document summarization problems). We generated oracle extracts by maximizing bigram recall with respect to the manual abstracts, as described in Berg-Kirkpatrick et al. (2011). For sentence compression, we adapted the Simple English Wikipedia dataset of Woodsend and Lapata (2011), containing aligned sentences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletionbased compressions. 5.2 Results Table 2 shows the results. The top rows refer to three strong baselines: the ICSI-1 extractive coverage-based system of Gillick et al. (2008), which achieved the best ROUGE scores in the TAC-2008 evaluation; the compressive summarizer of Berg-Kirkpatrick et al. (2011), denoted BGK’11; and the multi-aspect compressive summarizer of Woodsend and Lapata (2012), denoted WL’12. All these systems require ILP </context>
</contexts>
<marker>Woodsend, Lapata, 2011</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2011. Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proc. of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Multiple aspect summarization using integer linear programming.</title>
<date>2012</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="3206" citStr="Woodsend and Lapata (2012)" startWordPosition="469" endWordPosition="472">n compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still obstacles that need to be surmounted for these systems to enjoy wide adoption. All approaches above are based on integer linear programming (ILP), suffering from slow runtimes, when compared to extractive systems. For example, Woodsend and Lapata (2012) report 55 seconds on average to produce a summary; Berg-Kirkpatrick et al. (2011) report substantially faster runtimes, but fewer compressions are allowed. Having a compressive summarizer which is both fast and expressive remains an open problem. A second inconvenience of ILP-based approaches is that they do not exploit the modularity of the problem, since the declarative specification required by ILP solvers discards important structural information. For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). 196</context>
<context position="14353" citStr="Woodsend and Lapata, 2012" startWordPosition="2335" endWordPosition="2338"> is a feature map, which is described in detail in §3.5. We set Pn,`(0, 0) = Pn,`(1,1) = 0, and Pn,`(1, 0) = −∞, to allow only the deletion of entire subtrees. A crucial fact is that one can maximize Eq. 8 efficiently with dynamic programming (using the Viterbi algorithm for trees); the total cost is linear in Ln. We will exploit this fact in the dual decomposition framework described next.4 3.3 A Dual Decomposition Formulation In previous work, the optimization problem in Eq. 4 was converted into an ILP and fed to an offthe-shelf solver (Martins and Smith, 2009; BergKirkpatrick et al., 2011; Woodsend and Lapata, 2012). Here, we employ the AD3 algorithm, in a 4The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. similar manner as described in §2, but with an additional component for the sentence compressor, and slight modifications in the other components. We have the following N + M + EMm=1 |Tm |+ 1 components in total, illustrated in Figure 1: 1. For each of the N sentences, one component for the compression model. The AD3 quadratic subpro</context>
<context position="19678" citStr="Woodsend and Lapata (2012)" startWordPosition="3250" endWordPosition="3254">nt inconsistencies in handling coordinative conjunctions); arcs linking verbs to other verbs, to adjectives (e.g., make available), to verb particles (e.g., settle down), to the word that (e.g., said that), or to the word to if it is a leaf (e.g., allowed to come); arcs pointing to negation words, cardinal numbers, or determiners; and arcs connecting two proper nouns or words within quotation marks. 4 Multi-Task Learning We next turn to the problem of learning the model from training data. Prior work in compressive summarization has followed one of two strategies: Martins and Smith (2009) and Woodsend and Lapata (2012) learn the extraction and compression models separately, and then post-combine them, circumventing the lack of fully annotated data. Berg-Kirkpatrick et al. (2011) gathered a small dataset of manually compressed summaries, and trained with full supervision. While the latter approach is statistically more principled, it has the disadvantage of requiring fully annotated data, which is difficult to obtain in large quantities. On the other hand, there is plenty of data containing manually written abstracts (from the DUC and TAC conferences) and user-generated text (from Wikipedia) that may provide</context>
<context position="25719" citStr="Woodsend and Lapata, 2012" startWordPosition="4296" endWordPosition="4300">l’s factors, hence any decoder for Eq. 9 can be used for the maximization above: for tasks #1–#2, we solve a relaxation by running AD3 without rounding, and for task #3 we use dynamic programming; see Table 1. σkLk(w + vk), (10) 3 1 + N � k=1 201 involving v2 and v3, which is equivalent to training the two tasks separately and post-combining the models, as Martins and Smith (2009) did. 5 Experiments 5.1 Experimental setup We evaluated our compressive summarizers on data from the Text Analysis Conference (TAC) evaluations. We use the same splits as previous work (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012): the non-update portions of TAC-2009 for training and TAC-2008 for testing. In addition, we reserved TAC-2010 as a devset. The test partition contains 48 multi-document summarization problems; each provides 10 related news articles as input, and asks for a summary with up to 100 words, which is evaluated against four manually written abstracts. We ignored all the query information present in the TAC datasets. Single-Task Learning. In the single-task experiments, we trained a compressive summarizer on the dataset disclosed by Berg-Kirkpatrick et al. (2011), which contains manual compressive su</context>
<context position="28595" citStr="Woodsend and Lapata (2012)" startWordPosition="4743" endWordPosition="4746">dapted the Simple English Wikipedia dataset of Woodsend and Lapata (2011), containing aligned sentences for 15,000 articles from the English and Simple English Wikipedias. We kept only the 4,481 sentence pairs corresponding to deletionbased compressions. 5.2 Results Table 2 shows the results. The top rows refer to three strong baselines: the ICSI-1 extractive coverage-based system of Gillick et al. (2008), which achieved the best ROUGE scores in the TAC-2008 evaluation; the compressive summarizer of Berg-Kirkpatrick et al. (2011), denoted BGK’11; and the multi-aspect compressive summarizer of Woodsend and Lapata (2012), denoted WL’12. All these systems require ILP solvers. The bottom rows show the results achieved by our implementation of a pure extractive system (similar to the learned extractive summarizer of Berg-Kirkpatrick et al., 2011); a system that postcombines extraction and compression components trained separately, as in Martins and Smith (2009); and our compressive summarizer trained as a single task, and in the multi-task setting. The ROUGE and Pyramid scores show that the compressive summarizers (when properly trained) yield considerable benefits in content coverage over extractive systems, co</context>
<context position="34705" citStr="Woodsend and Lapata, 2012" startWordPosition="5732" endWordPosition="5735">of magnitude faster than ILP-based approaches. Results show that the state of the art is improved in automatic and manual metrics, with speeds close to extractive systems. Our approach is modular and easy to extend. For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). Other aspects may be added as additional components in our dual decomposition framework, such as query information (Schilder and Kondadadi, 2008), discourse con203 straints (Clarke and Lapata, 2007), or lexical preferences (Woodsend and Lapata, 2012). Our multitask approach may be used to jointly learn parameters for these aspects; the dual decomposition algorithm ensures that optimization remains tractable even with many components. A Projection Onto Knapsack This section describes a linear-time algorithm (Algorithm 1) for solving the following problem: minimize 11z − a112 w.r.t. zn E [0, 1], bn E [N], s.t. ENn=1 Lnzn &lt; B, (11) where a E RN and Ln &gt; 0, bn E [N]. This includes as special cases the problems of projecting onto a budget constraint (Ln = 1, bn) and onto the simplex (same, plus B = 1). Let clip(t) := max{0, min{1, t}}. Algorit</context>
</contexts>
<marker>Woodsend, Lapata, 2012</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proc. of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Joshua Goodman</author>
<author>Lucy Vanderwende</author>
<author>Hisami Suzuki</author>
</authors>
<title>Multi-document summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In Proc. of International Joint Conference on Artifical Intelligence.</booktitle>
<contexts>
<context position="6099" citStr="Yih et al., 2007" startWordPosition="934" endWordPosition="937">of the nth sentence. By designing a quality score function g : {0,1}N → R, this can be cast as a global optimization problem with a knapsack constraint: maximize g(y) w.r.t. y ∈ {0,1}N s.t. ENn=1 Lnyn ≤ B. (1) Intuitively, a good summary is one which selects sentences that individually convey “relevant” information, while collectively having small “redundancy.” This trade-off was explicitly modeled in early works through the notion of maximal marginal relevance (Carbonell and Goldstein, 1998; McDonald, 2007). An alternative are coverage-based models (§2.1; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2008), which seek a set of sentences that covers as many diverse “concepts” as possible; redundancy is automatically penalized since redundant sentences cover fewer concepts. Both models can be framed under the framework of submodular optimization (Lin and Bilmes, 2010), leading to greedy algorithms that have approximation guarantees. However, extending these models to allow for sentence compression (as will be detailed in §3) breaks the diminishing returns property, making submodular optimization no longer applicable. 2.1 Coverage-Based Summarization Coverage-based extractiv</context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and Hisami Suzuki. 2007. Multi-document summarization by maximizing informative content-words. In Proc. of International Joint Conference on Artifical Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zajic</author>
<author>B Dorr</author>
<author>J Lin</author>
<author>R Schwartz</author>
</authors>
<title>Sentence compression as a component of a multidocument summarization system.</title>
<date>2006</date>
<booktitle>In the ACL DUC Workshop.</booktitle>
<contexts>
<context position="2638" citStr="Zajic et al., 2006" startWordPosition="385" endWordPosition="388">d Bilmes, 2010), which are NP-hard but approximable through greedy algorithms; learning is possible with standard structured prediction algorithms (Sipos et al., 2012; Lin and Bilmes, 2012). Probabilistic models have also been proposed to capture the problem structure, such as determinantal point processes (Gillenwater et al., 2012). However, extractive systems are rather limited in the summaries they can produce. Long, partly relevant sentences tend not to appear in the summary, or to block the inclusion of other sentences. This has motivated research in compressive summarization (Lin, 2003; Zajic et al., 2006; Daum´e, 2006), where summaries are formed by compressed sentences (Knight and Marcu, 2000), not necessarily extracts. While promising results have been achieved by models that simultaneously extract and compress (Martins and Smith, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), there are still obstacles that need to be surmounted for these systems to enjoy wide adoption. All approaches above are based on integer linear programming (ILP), suffering from slow runtimes, when compared to extractive systems. For example, Woodsend and Lapata (2012) report 55 seconds on average to</context>
</contexts>
<marker>Zajic, Dorr, Lin, Schwartz, 2006</marker>
<rawString>D. Zajic, B. Dorr, J. Lin, and R. Schwartz. 2006. Sentence compression as a component of a multidocument summarization system. In the ACL DUC Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>