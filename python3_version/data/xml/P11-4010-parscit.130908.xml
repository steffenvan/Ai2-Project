<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008266">
<title confidence="0.992161">
BLAST: A Tool for Error Analysis of Machine Translation Output
</title>
<author confidence="0.995969">
Sara Stymne
</author>
<affiliation confidence="0.9809715">
Department of Computer and Information Science
Link¨oping University, Link¨oping, Sweden
</affiliation>
<email confidence="0.993751">
sara.stymne@liu.se
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990124928571429">
We present BLAST, an open source tool for er-
ror analysis of machine translation (MT) out-
put. We believe that error analysis, i.e., to
identify and classify MT errors, should be an
integral part of MT development, since it gives
a qualitative view, which is not obtained by
standard evaluation methods. BLAST can aid
MT researchers and users in this process, by
providing an easy-to-use graphical user inter-
face. It is designed to be flexible, and can be
used with any MT system, language pair, and
error typology. The annotation task can be
aided by highlighting similarities with a ref-
erence translation.
</bodyText>
<sectionHeader confidence="0.998934" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999675357142857">
Machine translation evaluation is a difficult task,
since there is not only one correct translation of a
sentence, but many equally good translation options.
Often, machine translation (MT) systems are only
evaluated quantitatively, e.g. by the use of automatic
metrics, which is fast and cheap, but does not give
any indication of the specific problems of a MT sys-
tem. Thus, we advocate human error analysis of MT
output, where humans identify and classify the prob-
lems in machine translated sentences.
In this paper we present BLAST,1 a graphical tool
for performing human error analysis, from any MT
system and for any language pair. BLAST has a
graphical user interface, and is designed to be easy
</bodyText>
<footnote confidence="0.955006333333333">
1The BiLingual Annotation/Annotator/Analysis Support
Tool, available for download at http://www.ida.liu.
se/~sarst/blast/
</footnote>
<bodyText confidence="0.999509272727273">
and intuitive to work with. It can aid the user by
highlighting similarities with a reference sentence.
BLAST is flexible in that it can be used with out-
put from any MT system, and with any hierarchical
error typology. It has a modular design, allowing
easy extension with new modules. To the best of our
knowledge, there is no other publicly available tool
for MT error annotation. Since we believe that error
analysis is a vital complement to MT evaluation, we
think that BLAST can be useful for many other MT
researchers and developers.
</bodyText>
<sectionHeader confidence="0.736559" genericHeader="method">
2 MT Evaluation and Error Analysis
</sectionHeader>
<bodyText confidence="0.999931047619048">
Hovy et al. (2002) discussed the complexity of MT
evaluation, and stressed the importance of adjusting
evaluation to the purpose and context of the trans-
lation. However, MT is very often only evaluated
quantitatively using a single metric, especially in re-
search papers. Quantitative evaluations can be au-
tomatic, using metrics such as Bleu (Papineni et
al., 2002) or Meteor (Denkowski and Lavie, 2010),
where the MT output is compared to one or more hu-
man reference translations. Metrics, however, only
give a single quantitative score, and do not give any
information about the strengths and weaknesses of
the system. Comparing scores from different met-
rics can give a very rough indication of some major
problems, especially in combination with a part-of-
speech analysis (Popovi´c et al., 2006).
Human evaluation is also often quantitative, for
instance in the form of estimates of values such as
adequacy and fluency, or by ranking sentences from
different systems (e.g. Callison-Burch et al. (2007)).
A combination of human and automatic metrics is
</bodyText>
<page confidence="0.965204">
56
</page>
<note confidence="0.3123025">
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 56–61,
Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999658">
human-targeted metrics such as HTER, where a hu-
man corrects the output of a system to the clos-
est correct translation, on which standard metrics
such as TER is then computed (Snover et al., 2006).
While these types of evaluation are certainly useful,
they are expensive and time-consuming, and still do
not tell us anything about the particular errors of a
system.2
Thus, we think that qualitative evaluation is an
important complement, and that error analysis, the
identification and classification of MT errors, is an
important task. There have been several suggestions
for general MT error typologies (Flanagan, 1994;
Vilar et al., 2006; Farr´us et al., 2010), targeted at
different user groups and purposes, focused on either
evaluation of single systems, or comparison between
systems. It is also possible to focus error analysis at
a specific problem, such as verb form errors (Murata
et al., 2005).
We have not been able to find any other freely
available tool for error analysis of MT. Vilar et al.
(2006) mentioned in a footnote that “a tool for high-
lighting the differences [between the MT system and
a correct translation] also proved to be quite useful”
for error analysis. They do not describe this tool any
further, and do not discuss if it was also used to mark
and store the error annotations themselves.
Some tools for post-editing of MT output, a re-
lated activity to error analysis, have been described
in the literature. Font Llitj´os and Carbonell (2004)
presented an online tool for eliciting information
from the user when post-editing sentences, in or-
der to improve a rule-based translation system. The
post-edit operations were labeled with error cate-
gories, making it a type of error analysis. This tool
was highly connected to their translation system,
and it required users to post-edit sentences by mod-
ifying word alignments, something that many users
found difficult. Glenn et al. (2008) described a post-
editing tool used for HTER calculation, which has
been used in large evaluation campaigns. The tool
is a pure post-editing tool and the edits are not clas-
sified. Graphical tools have also successfully been
used to aid humans in other MT-related tasks, such
as human MT evaluation of adequacy, fluency and
</bodyText>
<footnote confidence="0.87099">
2Though it does, at least in principle, seem possible to mine
HTER annotations for more information
</footnote>
<bodyText confidence="0.987853">
system comparison (Callison-Burch et al., 2007),
and word alignment (Ahrenberg et al., 2003).
</bodyText>
<sectionHeader confidence="0.966864" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.999982461538462">
BLAST is a tool for human annotations of bilingual
material. Its main purpose is error analysis for ma-
chine translation. BLAST is designed for use in any
MT evaluation project. It is not tied to the informa-
tion provided by specific MT systems, or to specific
languages, and it can be used with any hierarchi-
cal error typology. It has a preprocessing module
for automatically aiding the annotator by highlight-
ing similarities between the MT output and a refer-
ence. Its modular design allows easy integration of
new modules for preprocessing. BLAST has three
working modes for handling error annotations: for
adding new annotations, for editing existing annota-
tions, and for searching among annotations.
BLAST can handle two types of annotations: er-
ror annotations and support annotations. Error an-
notations are based on a hierarchical error typology,
and are used to annotate errors in MT output. Error
annotations are added by the users of BLAST. Sup-
port annotations are used as a support to the user,
currently to mark similarities in the system and ref-
erence sentences. The support annotations are nor-
mally created automatically by BLAST, but they can
also be modified by the user. Both annotation types
are stored with the indices of the words they apply
to.
Figure 1 shows a screenshot of BLAST. The MT
output is shown to the annotator one segment at a
time, in the upper part of the screen. A segment nor-
mally consists of a sentence and the MT output can
be accompanied by a source sentence, a reference
sentence, or both. Error annotations are marked in
the segments by bold, underlined, colored text, and
support annotations are marked by light background
colors. The bottom part of the tool, contains the er-
ror typology, and controls for updating annotations
and navigation. The error typology is shown using
a menu structure, where submenus are activated by
the user clicking on higher levels.
</bodyText>
<subsectionHeader confidence="0.997771">
3.1 Design goals
</subsectionHeader>
<bodyText confidence="0.999977">
We created BLAST with the goal that it should be
flexible, and allow maximum freedom for the user,
</bodyText>
<page confidence="0.997622">
57
</page>
<figureCaption confidence="0.999959">
Figure 1: Screenshot of BLAST
</figureCaption>
<bodyText confidence="0.993612">
based on the following goals:
</bodyText>
<listItem confidence="0.952585928571429">
• Independent of the MT system being analyzed,
particularly not dependent on specific informa-
tion given by a particular MT system, such as
alignment information
• Compatible with any error typology
• Language pair independent
• Possible to mark where in a sentence an error
occurs
• Possible to view either source or reference sen-
tences, or both
• Possible to automatically highlight similarities
between the system and the reference sentences
• Containing a search function for errors
• Simple to understand and use
</listItem>
<bodyText confidence="0.999945692307692">
The current implementation of BLAST fulfils all
these goals, with the possible small limitation that
the error typology has to be hierarchical. We believe
this limitation is minor, however, since it is possible
to have a relatively flat structure if desired, and to
re-use the same submenu in many places, allowing
cross-classification within a hierarchical typology.
The flexibility of the tool gives users a lot of free-
dom in how to use it in their evaluation projects.
However, we believe that it is important within ev-
ery error annotation project to use a set error typol-
ogy and guidelines for annotation, but the annotation
tool should not limit users in making these choices.
</bodyText>
<subsectionHeader confidence="0.977577">
3.2 Error Typologies
</subsectionHeader>
<bodyText confidence="0.9990365">
As described above, BLAST is easily configurable
with new typologies for annotation, with the only
restriction that the typology is hierarchical. BLAST
currently comes with the following implemented ty-
pologies, some of which are general, and some of
which are targeted at specific language (pairs):
</bodyText>
<listItem confidence="0.983822111111111">
• Vilar et al. (2006)
– General
– Chinese
– Spanish
• Farr´us et al. (2010)
– Catalan–Spanish
• Flanagan (1994) (slightly modified into a hier-
archical structure)
– French
</listItem>
<page confidence="0.889279">
58
</page>
<listItem confidence="0.955403">
– German
• Our own tentative fine-grained typology
– General
– Swedish
</listItem>
<bodyText confidence="0.999972705882353">
The error typologies can be very big, and it is hard
to fit an arbitrarily large typology into a graphical
tool. BLAST thus uses a menu structure which al-
ways shows the categories in the first level of the ty-
pology. Lower subtypologies are only shown when
they are activated by the user clicking on a higher
level. In Figure 1, the subtypologies to Word order
were activated by the user first clicking on Word or-
der, then on Phrase level.
It is important that typologies are easy to extend
and modify, especially in order to cover new target
languages, since the translation problems to some
extent will be dependent on the target language, for
instance with regard to the different agreement phe-
nomena in languages. The typologies that come with
BLAST can serve as a starting point for adjusting ty-
pologies, especially to new target languages.
</bodyText>
<subsectionHeader confidence="0.993205">
3.3 Implementation
</subsectionHeader>
<bodyText confidence="0.999947461538462">
BLAST is implemented as a Java application using
Swing for the graphical user interface. Using Java
makes it platform independent, and it is currently
tested on Unix, Linux, Mac, and Windows. BLAST
has an object-oriented design, with a particular fo-
cus on modular design, to allow it to be easily ex-
tendible with new modules for preprocessing, read-
ing and writing to different file formats, and present-
ing statistics. Unicode is used in order to allow a
high number of languages, and sentences can be dis-
played both right to left, and left to right. BLAST
is open source and is released under the LGPL li-
cense.3
</bodyText>
<subsectionHeader confidence="0.97432">
3.4 File formats
</subsectionHeader>
<bodyText confidence="0.99939625">
The main file types used in BLAST is the annotation
file, containing the translation segments and annota-
tions, and the typology file. These files are stored
in a simple text file format. There is also a configu-
ration file, which can be used for program settings,
besides using command line options, for instance to
configure color schemes, and to change preprocess-
ing settings. The statistics of an annotation project
</bodyText>
<footnote confidence="0.599467">
3http://www.gnu.org/copyleft/lesser.html
</footnote>
<bodyText confidence="0.998681285714286">
are printed in a text file in a human-readable format
(see Section 4.5).
The annotation file contains the translation seg-
ments for the MT system, and possibly for the
source and reference sentences, and all error and
support annotations. The annotations are stored with
the indices of the word(s) in the segments that were
marked, and a label identifying the error type. The
annotation file is initially created automatically by
BLAST based on sentence aligned files. It is then
updated by BLAST with the annotations added by
the user.
The typology file has a header with main informa-
tion, and then an item for each menu containing:
</bodyText>
<listItem confidence="0.8683868">
• The name of the menu
• A list of menu items, containing:
– Display name
– Internal name (used in annotation file, and
internally in BLAST)
</listItem>
<bodyText confidence="0.938713">
– The name of its submenu (if any)
The typology files have to be specified by the user,
but BLAST comes with several typology files, as de-
scribed in Section 3.2.
</bodyText>
<sectionHeader confidence="0.874778" genericHeader="method">
4 Working with BLAST
</sectionHeader>
<bodyText confidence="0.999970111111111">
BLAST has three different working modes: annota-
tion, edit and search. The main mode is annotation,
which allows the user to add new error annotations.
The edit mode allows the user to edit and remove er-
ror annotations. The search mode allows the user to
search for errors of different types. BLAST can also
create support annotations, that can later be updated
by the user, and calculate and print statistics of an
annotation project.
</bodyText>
<subsectionHeader confidence="0.994555">
4.1 Annotation
</subsectionHeader>
<bodyText confidence="0.9999945">
The annotation mode is the main working mode in
BLAST, and it is active in Figure 1. In annotation
mode a segment is shown with all its current er-
ror annotations. The annotations are marked with
bold and colored text, where the color depends on
the main type of the error. For each new annotation
the user selects the word or words that are wrong,
and selects an error type. In figure 1, the words no
television, and the error type Word order—*Phrase
level—*Long are selected in order to add a new error
</bodyText>
<page confidence="0.997521">
59
</page>
<bodyText confidence="0.999880333333333">
annotation. BLAST ignores identical annotations,
and warns the user if they try to add an annotation
for the exact same words as another annotation.
</bodyText>
<subsectionHeader confidence="0.989436">
4.2 Edit
</subsectionHeader>
<bodyText confidence="0.999973666666666">
In edit mode the user can change existing error an-
notations. In this mode only one annotation at a time
is shown, and the user can switch between them. For
each annotation affected words are highlighted, and
the error typology area shows the type of the error.
The currently shown error can be changed to a dif-
ferent error type, or it can be removed. The edit
mode is useful for revising annotations, and for cor-
recting annotation errors.
</bodyText>
<subsectionHeader confidence="0.999465">
4.3 Search
</subsectionHeader>
<bodyText confidence="0.999980153846154">
In search mode, it is possible to search for errors of
a certain type. To search, users choose the error type
they want to search for in the error typology, and
then search backwards or forwards for error annota-
tions of that type. It is possible both to search for
specific errors deep in the typology, and to search
for all errors of a type higher in the typology, for
instance, to search for all word order errors, regard-
less of subclassification. Search is active between all
segments, not only for the currently shown segment.
Search is useful for controlling the consistency of
annotations, and for finding instances of specific er-
rors.
</bodyText>
<subsectionHeader confidence="0.999677">
4.4 Support annotations
</subsectionHeader>
<bodyText confidence="0.99998503125">
Error annotation is a hard task for humans, and thus
we try to aid it by including automatic preprocess-
ing, where similarities between the system and refer-
ence sentences are marked at different levels of sim-
ilarity. Even if the goal of the error analysis often is
not to compare the MT output to a single reference,
but to the closest correct equivalent, it can still be
useful to be able to see the similarities to one ref-
erence sentence, to be able to identify problematic
parts easier.
For this module we have adapted the code
for alignment used in the Meteor-NEXT metric
(Denkowski and Lavie, 2010) to BLAST. In Meteor-
NEXT the system and reference sentences are
aligned at the levels of exact matching, stemmed
matching, synonyms, and paraphrases. All these
modules work on lower-cased data, so we added a
module for exact matching with the original casing
kept. The exact and lower-cased matching works
for most languages, and stemming for 15 languages.
The synonym module uses WordNet, and is only
available for English. The paraphrase module is
based on an automatic paraphrase induction method
(Bannard and Callison-Burch, 2005), it is currently
trained for five languages, but the Meteor-NEXT
code for training it for additional languages is in-
cluded.
Support annotations are normally only created au-
tomatically, but BLAST allows the user to edit them.
The mechanism for adding, removing or changing
support annotations is separate from error annota-
tions, and can be used regardless of mode.
</bodyText>
<subsectionHeader confidence="0.998383">
4.5 Create Statistics
</subsectionHeader>
<bodyText confidence="0.99999525">
The statistics module prints statistics about the cur-
rently loaded annotation project. The statistics are
printed to a file, in a human-readable format. It con-
tains information about the number of sentences and
errors in the project, average number of errors per
sentence, and how many sentences there are with
certain numbers of errors. The main part of the
statistics is the number and percentage of errors for
each node in the error typology. It is also possible to
get the number of errors for cross-classifications, by
specifying regular expressions for the categories to
cross-classify in the configuration file.
</bodyText>
<sectionHeader confidence="0.996697" genericHeader="method">
5 Future Extensions
</sectionHeader>
<bodyText confidence="0.9999606875">
BLAST is under active development, and we plan to
add new features. Most importantly we want to add
the possibility to annotate two MT systems in paral-
lel, which can be useful if the purpose of the annota-
tion is to compare MT systems. We are also working
on refining and developing the existing proposals for
error typologies, which is an important complement
to the tool itself. We intend to define a new fine-
grained general error typology, with extensions to a
number of target languages.
The modularity of BLAST also makes it possible
to add new modules, for instance for preprocess-
ing and to support other file formats. One example
would be to support error annotation of only specific
phenomena, such as verb errors, by adding a prepro-
cessing module for highlighting verbs with support
</bodyText>
<page confidence="0.99465">
60
</page>
<bodyText confidence="0.999951933333333">
annotations, and a suitable verb-focused error typol-
ogy. We are also working on a preprocessing module
based on grammar checker techniques (Stymne and
Ahrenberg, 2010), that highlights parts of the MT
output that it suspects are non-grammatical.
Even though the main purpose of BLAST is for
error annotation of machine translation output, the
freedom in the use of error typologies and support
annotations also makes it suitable for other tasks
where bilingual material is used, such as for anno-
tations of named entities in bilingual texts, or for
analyzing human translations, e.g. giving feedback
to second language learners, with only the addition
of a suitable typology, and possibly a preprocessing
module.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99973325">
We presented BLAST; a flexible tool for annotation
of bilingual segments, specifically intended for error
analysis of MT. BLAST facilitates the error analysis
task, which we believe is vital for MT researchers,
and could also be useful for other users of MT. Its
flexibility makes it possible to annotate translations
from any MT system and between any language
pairs, using any hierarchical error typology.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999854984848485">
Lars Ahrenberg, Magnus Merkel, and Michael Petterst-
edt. 2003. Interactive word alignment for language
engineering. In Proceedings of EACL, pages 49–52,
Budapest, Hungary.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597–604, Ann Arbor, Michigan,
USA.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
WMT, pages 136–158, Prague, Czech Republic, June.
Michael Denkowski and Alon Lavie. 2010. METEOR-
NEXT and the METEOR paraphrase tables: Improved
evaluation support for five target languages. In Pro-
ceedings of WMT and MetricsMATR, pages 339–342,
Uppsala, Sweden.
Mireia Farr´us, Marta R. Costa-juss`a, Jos´e B. Mari˜no, and
Jos´e A. R. Fonollosa. 2010. Linguistic-based evalu-
ation criteria to identify statistical machine translation
errors. In Proceedings of EAMT, pages 52–57, Saint
Rapha¨el, France.
Mary Flanagan. 1994. Error classification for MT
evaluation. In Proceedings of AMTA, pages 65–72,
Columbia, Maryland, USA.
Ariadna Font Llitj´os and Jaime Carbonell. 2004. The
translation correction tool: English-Spanish user stud-
ies. In Proceedings of LREC, pages 347–350, Lisbon,
Portugal.
Meghan Lammie Glenn, Stephanie Strassel, Lauren
Friedman, and Haejoong Lee. 2008. Management
of large annotation projects involving multiple human
judges: a case study of GALE machine translation
post-editing. In Proceedings of LREC, pages 2957–
2960, Marrakech, Morocco.
Eduard Hovy, Margaret King, and Andrei Popescu-Belis.
2002. Principles of context-based machine translation
evaluation. Machine Translation, 17(1):43–75.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, Toshiyuki
Kanamaru, and Hitoshi Isahara. 2005. Analysis of
machine translation systems’ errors in tense, aspect,
and modality. In Proceedings of PACLIC 19, pages
155–166, Taipei, Taiwan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311–318, Philadelphia, Pennsylvania, USA.
Maja Popovi´c, Adri`a de Gisper, Deepa Gupta, Patrik
Lambert, Hermann Ney, Jos´e Mari˜no, and Rafael
Banchs. 2006. Morpho-syntactic information for au-
tomatic error analysis of statistical machine translation
output. In Proceedings of WMT, pages 1–6, New York
City, New York, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human notation.
In Proceedings of AMTA, pages 223–231, Cambridge,
Massachusetts, USA.
Sara Stymne and Lars Ahrenberg. 2010. Using a gram-
mar checker for evaluation and postprocessing of sta-
tistical machine translation. In Proceedings of LREC,
pages 2175–2181, Valetta, Malta.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error analysis of machine transla-
tion output. In Proceedings of LREC, pages 697–702,
Genoa, Italy.
</reference>
<page confidence="0.999272">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892744">
<title confidence="0.998938">A Tool for Error Analysis of Machine Translation Output</title>
<author confidence="0.992352">Sara</author>
<affiliation confidence="0.99924">Department of Computer and Information Link¨oping University, Link¨oping,</affiliation>
<email confidence="0.971066">sara.stymne@liu.se</email>
<abstract confidence="0.995204666666667">present an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part of MT development, since it gives a qualitative view, which is not obtained by evaluation methods. aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lars Ahrenberg</author>
<author>Magnus Merkel</author>
<author>Michael Petterstedt</author>
</authors>
<title>Interactive word alignment for language engineering.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>49--52</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="5852" citStr="Ahrenberg et al., 2003" startWordPosition="938" endWordPosition="941">t-edit sentences by modifying word alignments, something that many users found difficult. Glenn et al. (2008) described a postediting tool used for HTER calculation, which has been used in large evaluation campaigns. The tool is a pure post-editing tool and the edits are not classified. Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and 2Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison (Callison-Burch et al., 2007), and word alignment (Ahrenberg et al., 2003). 3 System Overview BLAST is a tool for human annotations of bilingual material. Its main purpose is error analysis for machine translation. BLAST is designed for use in any MT evaluation project. It is not tied to the information provided by specific MT systems, or to specific languages, and it can be used with any hierarchical error typology. It has a preprocessing module for automatically aiding the annotator by highlighting similarities between the MT output and a reference. Its modular design allows easy integration of new modules for preprocessing. BLAST has three working modes for handl</context>
</contexts>
<marker>Ahrenberg, Merkel, Petterstedt, 2003</marker>
<rawString>Lars Ahrenberg, Magnus Merkel, and Michael Petterstedt. 2003. Interactive word alignment for language engineering. In Proceedings of EACL, pages 49–52, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>597--604</pages>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="15994" citStr="Bannard and Callison-Burch, 2005" startWordPosition="2661" endWordPosition="2664">e we have adapted the code for alignment used in the Meteor-NEXT metric (Denkowski and Lavie, 2010) to BLAST. In MeteorNEXT the system and reference sentences are aligned at the levels of exact matching, stemmed matching, synonyms, and paraphrases. All these modules work on lower-cased data, so we added a module for exact matching with the original casing kept. The exact and lower-cased matching works for most languages, and stemming for 15 languages. The synonym module uses WordNet, and is only available for English. The paraphrase module is based on an automatic paraphrase induction method (Bannard and Callison-Burch, 2005), it is currently trained for five languages, but the Meteor-NEXT code for training it for additional languages is included. Support annotations are normally only created automatically, but BLAST allows the user to edit them. The mechanism for adding, removing or changing support annotations is separate from error annotations, and can be used regardless of mode. 4.5 Create Statistics The statistics module prints statistics about the currently loaded annotation project. The statistics are printed to a file, in a human-readable format. It contains information about the number of sentences and er</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL, pages 597–604, Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>136--158</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3214" citStr="Callison-Burch et al. (2007)" startWordPosition="511" endWordPosition="514"> (Denkowski and Lavie, 2010), where the MT output is compared to one or more human reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system. Comparing scores from different metrics can give a very rough indication of some major problems, especially in combination with a part-ofspeech analysis (Popovi´c et al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al. (2007)). A combination of human and automatic metrics is 56 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 56–61, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics human-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed (Snover et al., 2006). While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualit</context>
<context position="5807" citStr="Callison-Burch et al., 2007" startWordPosition="931" endWordPosition="934">r translation system, and it required users to post-edit sentences by modifying word alignments, something that many users found difficult. Glenn et al. (2008) described a postediting tool used for HTER calculation, which has been used in large evaluation campaigns. The tool is a pure post-editing tool and the edits are not classified. Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and 2Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison (Callison-Burch et al., 2007), and word alignment (Ahrenberg et al., 2003). 3 System Overview BLAST is a tool for human annotations of bilingual material. Its main purpose is error analysis for machine translation. BLAST is designed for use in any MT evaluation project. It is not tied to the information provided by specific MT systems, or to specific languages, and it can be used with any hierarchical error typology. It has a preprocessing module for automatically aiding the annotator by highlighting similarities between the MT output and a reference. Its modular design allows easy integration of new modules for preproces</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of WMT, pages 136–158, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>METEORNEXT and the METEOR paraphrase tables: Improved evaluation support for five target languages.</title>
<date>2010</date>
<booktitle>In Proceedings of WMT and MetricsMATR,</booktitle>
<pages>339--342</pages>
<location>Uppsala,</location>
<contexts>
<context position="2614" citStr="Denkowski and Lavie, 2010" startWordPosition="415" endWordPosition="418">e tool for MT error annotation. Since we believe that error analysis is a vital complement to MT evaluation, we think that BLAST can be useful for many other MT researchers and developers. 2 MT Evaluation and Error Analysis Hovy et al. (2002) discussed the complexity of MT evaluation, and stressed the importance of adjusting evaluation to the purpose and context of the translation. However, MT is very often only evaluated quantitatively using a single metric, especially in research papers. Quantitative evaluations can be automatic, using metrics such as Bleu (Papineni et al., 2002) or Meteor (Denkowski and Lavie, 2010), where the MT output is compared to one or more human reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system. Comparing scores from different metrics can give a very rough indication of some major problems, especially in combination with a part-ofspeech analysis (Popovi´c et al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al. (2007)</context>
<context position="15460" citStr="Denkowski and Lavie, 2010" startWordPosition="2578" endWordPosition="2581">4.4 Support annotations Error annotation is a hard task for humans, and thus we try to aid it by including automatic preprocessing, where similarities between the system and reference sentences are marked at different levels of similarity. Even if the goal of the error analysis often is not to compare the MT output to a single reference, but to the closest correct equivalent, it can still be useful to be able to see the similarities to one reference sentence, to be able to identify problematic parts easier. For this module we have adapted the code for alignment used in the Meteor-NEXT metric (Denkowski and Lavie, 2010) to BLAST. In MeteorNEXT the system and reference sentences are aligned at the levels of exact matching, stemmed matching, synonyms, and paraphrases. All these modules work on lower-cased data, so we added a module for exact matching with the original casing kept. The exact and lower-cased matching works for most languages, and stemming for 15 languages. The synonym module uses WordNet, and is only available for English. The paraphrase module is based on an automatic paraphrase induction method (Bannard and Callison-Burch, 2005), it is currently trained for five languages, but the Meteor-NEXT </context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. METEORNEXT and the METEOR paraphrase tables: Improved evaluation support for five target languages. In Proceedings of WMT and MetricsMATR, pages 339–342, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mireia Farr´us</author>
<author>Marta R Costa-juss`a</author>
<author>Jos´e B Mari˜no</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Linguistic-based evaluation criteria to identify statistical machine translation errors.</title>
<date>2010</date>
<booktitle>In Proceedings of EAMT,</booktitle>
<pages>52--57</pages>
<location>Saint Rapha¨el, France.</location>
<marker>Farr´us, Costa-juss`a, Mari˜no, Fonollosa, 2010</marker>
<rawString>Mireia Farr´us, Marta R. Costa-juss`a, Jos´e B. Mari˜no, and Jos´e A. R. Fonollosa. 2010. Linguistic-based evaluation criteria to identify statistical machine translation errors. In Proceedings of EAMT, pages 52–57, Saint Rapha¨el, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Flanagan</author>
</authors>
<title>Error classification for MT evaluation.</title>
<date>1994</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>65--72</pages>
<location>Columbia, Maryland, USA.</location>
<contexts>
<context position="4041" citStr="Flanagan, 1994" startWordPosition="641" endWordPosition="642">-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed (Snover et al., 2006). While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al., 2006; Farr´us et al., 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. It is also possible to focus error analysis at a specific problem, such as verb form errors (Murata et al., 2005). We have not been able to find any other freely available tool for error analysis of MT. Vilar et al. (2006) mentioned in a footnote that “a tool for highlighting the differences [between the MT system and a correct translation] also proved to be quite useful” for error analysis. They do not describe this tool an</context>
<context position="9584" citStr="Flanagan (1994)" startWordPosition="1561" endWordPosition="1562">t is important within every error annotation project to use a set error typology and guidelines for annotation, but the annotation tool should not limit users in making these choices. 3.2 Error Typologies As described above, BLAST is easily configurable with new typologies for annotation, with the only restriction that the typology is hierarchical. BLAST currently comes with the following implemented typologies, some of which are general, and some of which are targeted at specific language (pairs): • Vilar et al. (2006) – General – Chinese – Spanish • Farr´us et al. (2010) – Catalan–Spanish • Flanagan (1994) (slightly modified into a hierarchical structure) – French 58 – German • Our own tentative fine-grained typology – General – Swedish The error typologies can be very big, and it is hard to fit an arbitrarily large typology into a graphical tool. BLAST thus uses a menu structure which always shows the categories in the first level of the typology. Lower subtypologies are only shown when they are activated by the user clicking on a higher level. In Figure 1, the subtypologies to Word order were activated by the user first clicking on Word order, then on Phrase level. It is important that typolo</context>
</contexts>
<marker>Flanagan, 1994</marker>
<rawString>Mary Flanagan. 1994. Error classification for MT evaluation. In Proceedings of AMTA, pages 65–72, Columbia, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Font Llitj´os</author>
<author>Jaime Carbonell</author>
</authors>
<title>The translation correction tool: English-Spanish user studies.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>347--350</pages>
<location>Lisbon, Portugal.</location>
<marker>Llitj´os, Carbonell, 2004</marker>
<rawString>Ariadna Font Llitj´os and Jaime Carbonell. 2004. The translation correction tool: English-Spanish user studies. In Proceedings of LREC, pages 347–350, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meghan Lammie Glenn</author>
<author>Stephanie Strassel</author>
<author>Lauren Friedman</author>
<author>Haejoong Lee</author>
</authors>
<title>Management of large annotation projects involving multiple human judges: a case study of GALE machine translation post-editing.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2957--2960</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="5338" citStr="Glenn et al. (2008)" startWordPosition="855" endWordPosition="858">annotations themselves. Some tools for post-editing of MT output, a related activity to error analysis, have been described in the literature. Font Llitj´os and Carbonell (2004) presented an online tool for eliciting information from the user when post-editing sentences, in order to improve a rule-based translation system. The post-edit operations were labeled with error categories, making it a type of error analysis. This tool was highly connected to their translation system, and it required users to post-edit sentences by modifying word alignments, something that many users found difficult. Glenn et al. (2008) described a postediting tool used for HTER calculation, which has been used in large evaluation campaigns. The tool is a pure post-editing tool and the edits are not classified. Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and 2Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison (Callison-Burch et al., 2007), and word alignment (Ahrenberg et al., 2003). 3 System Overview BLAST is a tool for human annotations of bilingual material. Its m</context>
</contexts>
<marker>Glenn, Strassel, Friedman, Lee, 2008</marker>
<rawString>Meghan Lammie Glenn, Stephanie Strassel, Lauren Friedman, and Haejoong Lee. 2008. Management of large annotation projects involving multiple human judges: a case study of GALE machine translation post-editing. In Proceedings of LREC, pages 2957– 2960, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Margaret King</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Principles of context-based machine translation evaluation.</title>
<date>2002</date>
<journal>Machine Translation,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="2230" citStr="Hovy et al. (2002)" startWordPosition="355" endWordPosition="358">da.liu. se/~sarst/blast/ and intuitive to work with. It can aid the user by highlighting similarities with a reference sentence. BLAST is flexible in that it can be used with output from any MT system, and with any hierarchical error typology. It has a modular design, allowing easy extension with new modules. To the best of our knowledge, there is no other publicly available tool for MT error annotation. Since we believe that error analysis is a vital complement to MT evaluation, we think that BLAST can be useful for many other MT researchers and developers. 2 MT Evaluation and Error Analysis Hovy et al. (2002) discussed the complexity of MT evaluation, and stressed the importance of adjusting evaluation to the purpose and context of the translation. However, MT is very often only evaluated quantitatively using a single metric, especially in research papers. Quantitative evaluations can be automatic, using metrics such as Bleu (Papineni et al., 2002) or Meteor (Denkowski and Lavie, 2010), where the MT output is compared to one or more human reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system</context>
</contexts>
<marker>Hovy, King, Popescu-Belis, 2002</marker>
<rawString>Eduard Hovy, Margaret King, and Andrei Popescu-Belis. 2002. Principles of context-based machine translation evaluation. Machine Translation, 17(1):43–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Toshiyuki Kanamaru</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Analysis of machine translation systems’ errors in tense, aspect, and modality.</title>
<date>2005</date>
<booktitle>In Proceedings of PACLIC 19,</booktitle>
<pages>155--166</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4326" citStr="Murata et al., 2005" startWordPosition="686" endWordPosition="689">ming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al., 2006; Farr´us et al., 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. It is also possible to focus error analysis at a specific problem, such as verb form errors (Murata et al., 2005). We have not been able to find any other freely available tool for error analysis of MT. Vilar et al. (2006) mentioned in a footnote that “a tool for highlighting the differences [between the MT system and a correct translation] also proved to be quite useful” for error analysis. They do not describe this tool any further, and do not discuss if it was also used to mark and store the error annotations themselves. Some tools for post-editing of MT output, a related activity to error analysis, have been described in the literature. Font Llitj´os and Carbonell (2004) presented an online tool for </context>
</contexts>
<marker>Murata, Uchimoto, Ma, Kanamaru, Isahara, 2005</marker>
<rawString>Masaki Murata, Kiyotaka Uchimoto, Qing Ma, Toshiyuki Kanamaru, and Hitoshi Isahara. 2005. Analysis of machine translation systems’ errors in tense, aspect, and modality. In Proceedings of PACLIC 19, pages 155–166, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="2576" citStr="Papineni et al., 2002" startWordPosition="409" endWordPosition="412">here is no other publicly available tool for MT error annotation. Since we believe that error analysis is a vital complement to MT evaluation, we think that BLAST can be useful for many other MT researchers and developers. 2 MT Evaluation and Error Analysis Hovy et al. (2002) discussed the complexity of MT evaluation, and stressed the importance of adjusting evaluation to the purpose and context of the translation. However, MT is very often only evaluated quantitatively using a single metric, especially in research papers. Quantitative evaluations can be automatic, using metrics such as Bleu (Papineni et al., 2002) or Meteor (Denkowski and Lavie, 2010), where the MT output is compared to one or more human reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system. Comparing scores from different metrics can give a very rough indication of some major problems, especially in combination with a part-ofspeech analysis (Popovi´c et al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different syst</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Adri`a de Gisper</author>
<author>Deepa Gupta</author>
<author>Patrik Lambert</author>
<author>Hermann Ney</author>
<author>Jos´e Mari˜no</author>
<author>Rafael Banchs</author>
</authors>
<title>Morpho-syntactic information for automatic error analysis of statistical machine translation output.</title>
<date>2006</date>
<booktitle>In Proceedings of WMT,</booktitle>
<pages>1--6</pages>
<location>New York City, New York, USA.</location>
<marker>Popovi´c, de Gisper, Gupta, Lambert, Ney, Mari˜no, Banchs, 2006</marker>
<rawString>Maja Popovi´c, Adri`a de Gisper, Deepa Gupta, Patrik Lambert, Hermann Ney, Jos´e Mari˜no, and Rafael Banchs. 2006. Morpho-syntactic information for automatic error analysis of statistical machine translation output. In Proceedings of WMT, pages 1–6, New York City, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human notation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="3617" citStr="Snover et al., 2006" startWordPosition="575" endWordPosition="578"> al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al. (2007)). A combination of human and automatic metrics is 56 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 56–61, Portland, Oregon, USA, 21 June 2011. c�2011 Association for Computational Linguistics human-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed (Snover et al., 2006). While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al., 2006; Farr´us et al., 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. It i</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human notation. In Proceedings of AMTA, pages 223–231, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Using a grammar checker for evaluation and postprocessing of statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2175--2181</pages>
<location>Valetta,</location>
<contexts>
<context position="17981" citStr="Stymne and Ahrenberg, 2010" startWordPosition="2986" endWordPosition="2989">s an important complement to the tool itself. We intend to define a new finegrained general error typology, with extensions to a number of target languages. The modularity of BLAST also makes it possible to add new modules, for instance for preprocessing and to support other file formats. One example would be to support error annotation of only specific phenomena, such as verb errors, by adding a preprocessing module for highlighting verbs with support 60 annotations, and a suitable verb-focused error typology. We are also working on a preprocessing module based on grammar checker techniques (Stymne and Ahrenberg, 2010), that highlights parts of the MT output that it suspects are non-grammatical. Even though the main purpose of BLAST is for error annotation of machine translation output, the freedom in the use of error typologies and support annotations also makes it suitable for other tasks where bilingual material is used, such as for annotations of named entities in bilingual texts, or for analyzing human translations, e.g. giving feedback to second language learners, with only the addition of a suitable typology, and possibly a preprocessing module. 6 Conclusion We presented BLAST; a flexible tool for an</context>
</contexts>
<marker>Stymne, Ahrenberg, 2010</marker>
<rawString>Sara Stymne and Lars Ahrenberg. 2010. Using a grammar checker for evaluation and postprocessing of statistical machine translation. In Proceedings of LREC, pages 2175–2181, Valetta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error analysis of machine translation output.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>697--702</pages>
<location>Genoa, Italy.</location>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error analysis of machine translation output. In Proceedings of LREC, pages 697–702, Genoa, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>