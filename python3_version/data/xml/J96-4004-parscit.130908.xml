<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9923115">
A Statistically Emergent Approach
for Language Processing: Application to
Modeling Context Effects in Ambiguous
Chinese Word Boundary Perception
</title>
<author confidence="0.994724">
Kok-Wee Gan. Martha Palmert
</author>
<affiliation confidence="0.9414415">
Hong Kong University of Science and University of Pennsylvania
Technology
</affiliation>
<author confidence="0.972179">
Kim-Teng Luat
</author>
<affiliation confidence="0.99567">
National University of Singapore
</affiliation>
<bodyText confidence="0.99742">
This paper proposes that the process of language understanding can be modeled as a collective
phenomenon that emerges from a myriad of microscopic and diverse activities. The process is
analogous to the crystallization process in chemistry. The essential features of this model are:
asynchronous parallelism; temperature-controlled randomness; and statistically emergent active
symbols. A computer program that tests this model on the task of capturing the effect of context
on the perception of ambiguous word boundaries in Chinese sentences is presented. The program
adopts a holistic approach in which word identification forms an integral component of sentence
analysis. Various types of knowledge, from statistics to linguistics, are seamlessly integrated
for the tasks of word boundary disambiguation as well as sentential analysis. Our experimental
results showed that the model is able to address the word boundary ambiguity problems effectively.
</bodyText>
<sectionHeader confidence="0.992132" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999109454545455">
This paper suggests that the language understanding process can be effectively mod-
eled as the statistical outcome of a large number of independent activities occurring
in parallel. There is no global controller deciding which processes to run next. All pro-
cessing is done locally by many simple, independent agents that make their decisions
stochastically. The system is self-organizing, with coherent behavior being a statisti-
cally emergent property of the system as a whole. The model, in a nutshell, simulates
language understanding as a crystallization process. This process consists of a series
of hierarchical, structure-building activities in which high-level linguistic structures
are formed from their constituents and get properly hooked up to each other as the
process converges.
The essential features of the model are:
</bodyText>
<listItem confidence="0.998855666666667">
• The process of sentence analysis is a series of computational activities
that determine how various constituents in a sentence can be
meaningfully related.
</listItem>
<affiliation confidence="0.9979968">
* Department of Computer Science, Hong Kong University of Science and Technology, Clear Water Bay,
Kowloon, Hong Kong
f Department of Computer Information Science, University of Pennsylvania, Philadelphia, PA 19104-6389
t Department of Information Systems &amp; Computer Science, National University of Singapore, Lower
Kent Ridge Road, Singapore 119260, Republic of Singapore
</affiliation>
<note confidence="0.6583755">
© 1996 Association for Computational Linguistics
Computational Linguistics Volume 22, Number 4
</note>
<listItem confidence="0.9985480625">
• All computational activities are carried out by a large number of
procedures known as codelets.
• A linguistic structure is not built by a single codelet. Rather, it is
constructed by a sequence of codelets. The execution of this sequence of
codelets is interleaved with other codelets that are responsible for
building other structures.
• The order by which structures are built is not explicitly programmed,
but is an emergent outcome of chains of codelets working in an
asynchronous parallel mode.
• Computational activities are a combination of top-down and bottom-up
activities.
• Computational activities are indirectly guided by a semantic network of
linguistic concepts, which ensures that these activities do not operate
independently of the system&apos;s representation of the context of a sentence.
• Decision making is stochastic, with the amount of randomness being
controlled by a parameter known as the computational temperature.
</listItem>
<bodyText confidence="0.99992796">
We have applied our model to the task of capturing the effect of context on the
perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our ap-
proach differs from existing work on Chinese word segmentation (Liang 1983; Wang,
Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al.
1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat
et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence inter-
pretation, in addition to word boundary identification. Our system figures out where
the word boundaries of a sentence are by determining how various constituents in
a sentence can be meaningfully related. The relations the system builds represent its
interpretation of the sentence. In the initial stage of a run, the system constructs re-
lations between characters of a sentence. Through a spreading activation mechanism,
the system gradually shifts to the construction of words and of relations between
words. Later, the system progresses to identifying and constructing chunks (in other
words, phrases), and to establishing connections between chunks. Note that there is
no top-level executive that decides the order of these activities. At any given time, the
system stochastically selects one action to execute. Therefore, efforts toward building
different structures are interleaved, sometimes cooperating and sometimes competing.
The system&apos;s high-level behavior, therefore, arises from its low-level stochastic actions.
We will give a detailed description of this application in this paper. In Section 2, we
introduce the problem of ambiguous Chinese word boundary perception, and follow,
in Section 3, with a summary of the current practices in Chinese word identification. We
describe our model in Section 4, showing a sample run of our program in Section 5 to
illustrate the behavior of the model. Finally, some discussions of the model are covered
in Section 6. In Section 7, we compare our model with others, and explore areas for
future research in Section 8.
</bodyText>
<sectionHeader confidence="0.818935" genericHeader="keywords">
2. Ambiguous Chinese Word Boundary Perception
</sectionHeader>
<bodyText confidence="0.900337666666667">
A written Chinese sentence consists of a series of evenly spaced Chinese characters.
Each character corresponds to one syllable. A word in Chinese can be made up of
a single character, such as fR fan &apos;rice&apos;, or it can be a combination of two or more
</bodyText>
<page confidence="0.982779">
532
</page>
<note confidence="0.891401">
Gan, Palmer, and Lua A Statistically Emergent Approach
</note>
<bodyText confidence="0.997025789473684">
characters, such as *W shutguo &apos;fruit&apos;. It is possible that the component characters of
a word are free&apos;, such as * shut and W guel of the word )EW shutgu6 &apos;fruit&apos;, which
mean &apos;water&apos; and &apos;fruit&apos; respectively. For any two Chinese characters in a sentence,
denoted as x and y, if xy cannot be combined together to function as a word, a single
word boundary exists between these two characters. If x and y can be constituents of
the same word, yet at the same time may also be free, then word boundary ambiguity
exists in these two characters. If there is a unique word boundary before x and after
y, we refer to the ambiguity existing in xy as a combination ambiguity. On the other
hand, if there is a word boundary ambiguity between the characters xy and the char-
acter that precedes or follows them, say z, and these three characters can be grouped
into either xy z or x yz, then we say that an overlap ambiguity exists. A sentence
that allows an ambiguous fragment to have multiple word boundaries will end up
with more than one interpretation. This type of ambiguity is called global ambiguity
with respect to the sentence. On the other hand, if only one way of segmenting the
word boundary of an ambiguous fragment is allowed in a sentence, we call this local
ambiguity with respect to the sentence. Global ambiguity can only be resolved with
discourse knowledge. An example for each category is shown in (1) to (4).2 Through-
out this paper, we follow the guidelines on Chinese word segmentation adopted in
China.3
</bodyText>
<note confidence="0.2099875">
Overlap, Local Ambiguity
(1) LA 42 itairt Erg Et IR
</note>
<bodyText confidence="0.998937916666667">
zhe wei zhiyuan gOngzub de yall hen da
this CL4 worker work STRUC pressure very great
&apos;This worker faces great pressure in his work.&apos;
The underlined fragment&apos;Ai i&apos;F yuan gOngzue, in (1) has overlap, local ambiguity.
The middle character I king can combine with the previous character 1%. yucin to
form the word I yuangong &apos;worker&apos;, leaving the third character functioning as a
monosyllabic word fr zua &apos;do&apos;. The middle character can also combine with the next
character to form the word IfF gongzuo &apos;work&apos;, leaving the first character alone.
The sentence containing this fragment allows only one way of segmenting the word
boundary, which is shown in (1). The character Ot&apos; yuim combines with the character
preceding it, fa zhi, to form the bisyllabic word plat zhiywin &apos;worker&apos;, and the two
characters 2 gong and 41F zub form a word.
</bodyText>
<subsectionHeader confidence="0.665171">
Overlap, Global Ambiguity
</subsectionHeader>
<bodyText confidence="0.9757765">
(2)a. fall W ill &apos;d f4 17 sa
women ylw xuesheng huo de you yiyi
we want student live CSC&apos; have meaning
&apos;We want our students to have a meaningful life.&apos;
</bodyText>
<footnote confidence="0.959760875">
1 A free character is one which can occur independently as a word (Li and Thompson 1981).
2 The characters underlined in sentences (1) to (4) are the locations of word boundary ambiguities we
would like to focus on. This convention will be used throughout in this paper.
3 See Contemporary Chinese Language Words Segmentation Standard Used for Information Processing, fifth
edition, 1988, published in China.
4 CL stands for a CLassifier.
5 STRUC stands for the STRUCture word frg de.
6 CSC stands for the Complex Stative Construction word t4 de.
</footnote>
<page confidence="0.995202">
533
</page>
<figure confidence="0.646915333333333">
Computational Linguistics Volume 22, Number 4
b. flIfl 5 /a
, tiA i 4 A‘ 21t
</figure>
<bodyText confidence="0.993874">
women yao xue shenghuo de you yiyi
we want learn life CSC have meaning
&apos;We want to learn how to lead a meaningful life.&apos;
The fragment f. it xue sheng huO also has overlap ambiguity, where the middle
character can either combine with the first character to form a word, or combine with
the last character to form a word. The sentence containing this fragment has two
plausible interpretations as shown in (2a) and (2b). Both alternations:14.°1 &apos;d xuesheng
huo &apos;student live&apos; (2a) and44.1 &apos;t&amp; xue shenghuo &apos;learn life&apos; are acceptable.
</bodyText>
<sectionHeader confidence="0.668656" genericHeader="introduction">
Combination, Local Ambiguity
</sectionHeader>
<subsectionHeader confidence="0.131177">
(3) g n
</subsectionHeader>
<bodyText confidence="0.841772285714286">
ni de bidoqing shifen hufiji
you STRUC look very funny
&apos;You look very funny.&apos;
In (3), the two characters in the fragment -H-} shifen can either function as two au-
tonomous words + sh1 &apos;ten&apos; and3-)- fen &apos;mark&apos;, or they can combine together to function
as a bisyllabic word +3} shifen &apos;very&apos;. Given the sentential context of (3), however,
only the second alternation is correct.
</bodyText>
<sectionHeader confidence="0.696769" genericHeader="method">
Combination, Global Ambiguity
</sectionHeader>
<listItem confidence="0.349427125">
(4)a. fill N ttl NI A
women dou hen nan gub
we all very hard live
&apos;We all have a hard life.&apos;
b. MI X 111 ffbig
women dou hen nangub
we all very sad
&apos;We all feel very sad.&apos;
</listItem>
<bodyText confidence="0.998818888888889">
The fragment ffig nangub also has combination ambiguity. It differs from (3) in that the
sentence in which it appears has two plausible interpretations. Hence, this fragment
can either be segmented as fit nan &apos;hard&apos; and A gut) &apos;live&apos; in (4a), or as Ntig nangua
&apos;sad&apos; in (4b).
Word boundary ambiguity is a very common phenomenon in written Chinese,
due to the fact that a large number of words in modern Chinese are formed from
free characters (Chao 1957). The problem also exists in continuous speech recognition
research, where correct interpretation of word boundaries in an utterance requires lin-
guistic and nonlinguistic information. However, people have a fascinating ability to
fluidly perceive groups of characters as words in one context but break these groups
apart in a different context. This human capability highlights the fact that there is a
continual interaction between word identification and sentence interpretation. We are
therefore motivated to study how our statistically emergent model can be used to sim-
ulate the interactions between word identification and sentence analysis. In particular,
we want to study how the model (i) handles fragments with local ambiguities, such
as those in sentences (1) and (3), when they appear in different sentential contexts and
(ii) handles fragments with global ambiguities, such as those in sentences (2) and (4),
when there is no discourse information.
</bodyText>
<page confidence="0.997013">
534
</page>
<note confidence="0.73108">
Gan, Palmer, and Lua A Statistically Emergent Approach
</note>
<sectionHeader confidence="0.945069" genericHeader="method">
3. Existing Approaches
</sectionHeader>
<bodyText confidence="0.994550952380953">
Traditionally, word identification has been treated as a preprocessing issue, distinct
from sentence analysis. We will therefore only discuss current practices in word iden-
tification, leaving sentence analysis aside. Several techniques have been used in word
identification, ranging from simple pattern matching, to statistical approaches, to rule-
based methods. The most popular pattern-matching method is based on the Maximum
Matching heuristics, commonly known as the MM method (Liang 1983; Wang, Wang,
and Bai 1991). This method scans a sentence from left to right. In each step, the longest
matched substring is selected as a word by dictionary look-up. For example, in sen-
tence (5),
(5)-3-1-ntil (1&apos;3
jisuanjr de faming YIY1 zhengcla
computer STRUC invention implication profound
&apos;The invention of the computer has profound implications.&apos;
the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it
is the longest matched substring found in a word dictionary. With the same reasoning,
the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It
zhOngda &apos;profound&apos; are identified.
Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang,
Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat
and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai
et al. 1992). These approaches make use of co-occurrence frequencies of characters
in a large corpus of written texts to achieve word segmentation without getting into
deep syntactic and semantic analysis. For example, the relaxation approach uses the
usage frequencies of words and the adjacency constraints among words to iteratively
derive the most plausible assignment of characters into word classes. First, all possi-
ble words in a sentence are identified and assigned initial probabilities based on their
usage frequency. These probabilities are updated iteratively by employing the consis-
tency constraints among neighboring words. Impossible combinations are gradually
filtered out, leading to the identification of the most likely combination. The mutual
information approach is similar to the relaxation approach in principle. Here, mutual
information is used to measure how strongly two characters are associated. The mu-
tual information score is derived from the ratio of the co-occurrence frequency of two
characters to the frequency of each character. In a sentence, the mutual information
score for each pair of adjacent characters is determined. The pair having the highest
score is grouped together. The sentence is split into two parts by the two characters
just grouped. The same procedure is applied to each part recursively. Eventually, all
word boundaries will be identified.
Both the pattern-matching and the statistical approaches are simple and easy to
implement. It is well known, however, that they perform poorly when presented with
ambiguous fragments that have alternate word boundaries in different sentential con-
texts. For instance, the fragment ± shifen, which is a bisyllabic word in sentence
(3a), functions as two separate word s in sentence (6).
</bodyText>
<listItem confidence="0.65522">
(6) ft p, NJ 1- 53.
</listItem>
<bodyText confidence="0.983672333333333">
hi zhi kao dao shi fen
he only score ASP ten mark
&apos;He scores only ten marks.&apos;
</bodyText>
<page confidence="0.993213">
535
</page>
<note confidence="0.421652">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.996500083333333">
The MM method will regard this fragment as a bisyllabic word +3} shifen &apos;very&apos;
regardless of the sentential context in (3a) and (6), since this word is longer than the
lengths of the two monosyllabic words + shi &apos;ten&apos; and 3)- fen &apos;mark&apos;. As a result,
this method fails to correctly identify the word boundaries in sentence (6). Within
statistical approaches, considering, for example, the mutual information method (Lua
and Can 1994), the same fragment is identified as a bisyllabic word in both sentences
(3a) and (6)7.
By checking the structural relationships among words in a sentence, rule-based
approaches aim to overcome limitations faced by pattern-matching and statistical ap-
proaches. However, many of the rules in existing rule-based systems (Huang 1989;
Yao, Zheng, and Wu 1990; Yeh and Lee 1991; He, Xu, and Sun 1991; Chen and Liu
1992) are either arbitrary and word-specific, or overly general. For example,
</bodyText>
<subsectionHeader confidence="0.770535">
Rule
</subsectionHeader>
<bodyText confidence="0.9757305">
Given an ambiguous fragment xyz where x, z, xy, and yz are all possible words, if
x can be analyzed as a so-called direction word, segment the fragment as x yz, else
segment it as xy z (Liang 1990).
This syntactic rule works in sentence (7).
</bodyText>
<sectionHeader confidence="0.383314" genericHeader="method">
(7) ft flff
</sectionHeader>
<bodyText confidence="0.8963645">
ta fü xia shenzi
he bend down body
&apos;He bends down his body.&apos;
The fragment -F4T- xia shen zi in sentence (7) is ambiguous. As -F xia &apos;down&apos; is a
direction word, the fragment is segmented as -F giT xia shenzi &apos;down body&apos;, which
is as desired.
Similarly, this rule will segment the fragment 31-131A wai guo ren as 3,1- IA wai
guOren &apos;out citizen&apos;, since 3$ wai &apos;out&apos; is also a direction word. Therefore, when this
fragment appears in sentence (8a),
(8)a. ft 3&apos;FLIA
ta shi waiguOren
he COPULA foreigner
&apos;He is a foreigner.&apos;
the word boundaries identified will be:
b. ft 5&apos;F MA
ta shi i guoren
he COPULA out citizen
which is incorrect.
Examples (7) and (8) illustrate that although syntactic information has been incor-
porated in word segmentation, there are still errors. In contrast, people are extremely
flexible in their perception of word boundaries of ambiguous fragments appearing in
different sentential contexts. We believe that the separation of word identification from
the task of analysis accounts for the difference in performance. This has motivated us
to study how word identification and sentence analysis can be integrated.
</bodyText>
<page confidence="0.9044745">
7 This result is reported in Can (1994).
536
</page>
<note confidence="0.731186">
Gan, Palmer, and Lua A Statistically Emergent Approach
</note>
<sectionHeader confidence="0.733809" genericHeader="method">
4. The Statistically Emergent Model
</sectionHeader>
<bodyText confidence="0.999922444444444">
This model is inspired by the work done in the Fluid Analogies Research Group (Hof-
stadter 1983; Meredith 1986; Mitchell 1990; French 1992). There are four main compo-
nents in this model. Namely, (i) the conceptual network, which is a network of nodes
and links representing some permanent linguistic concepts; (ii) the workspace, which
is the working area in which high-level linguistic structures representing the system&apos;s
current understanding of a sentence are built and modified; (iii) the
coderack, which is a pool of structure-building agents (codelets) waiting to run; and
(iv) the computational temperature, which is an approximate measure of the amount
of disorganization in the system&apos;s understanding of a sentence.
</bodyText>
<subsectionHeader confidence="0.999578">
4.1 The Conceptual Network
</subsectionHeader>
<bodyText confidence="0.982137296296296">
This is a network of nodes and links representing some permanent linguistic concepts
(Figure 1).
In the network, a node represents a concept. For example, the node labeled charac-
ter represents the concept of character; the node word represents the concept of word;
the node chunk represents the concept of chunk; the nodes character-1, character-2, up
to character-n represent the actual characters in a sentence; the affix and affinity nodes
represent the concepts of relations between characters; the nodes classifier, reflexive ad-
jective, structure, etc., represent the concepts of relations between words; the nodes
agent, patient, theme, etc., represent the concepts of relations between chunks.
A link represents an association between two nodes. There are four types of links:
(i) category-of links, or is-a links, which connect instances to types, for example,
the connections from character-1, character-2, up to character-n to the character node;
(ii) has-instance links, the converse of category-of links; (iii) has-relation links, which
associate a node with the relations it contributes, for example, the connection from
the character node to the affix node represents that the character node contributes to
the character-based relation named as affix; (iv) part-of links, which represent part-of
relations between two nodes. The direction of a part-of link, for instance, the link from
the character node to the word node, is interpreted as &apos;the character is part of the word&apos;.
During a run of the program, nodes become activated when perceived to be rele-
vant, and decay when no longer perceived to be relevant. Nodes also spread activation
to their neighbors, and thus concepts closely associated with relevant concepts also
become relevant. The activation levels of nodes can be affected by processes that take
place in the workspace. Several nodes in the network (e.g., agent, patient, word, chunk,
etc.), when activated, are able to exert top-down influences on the types of activities
that may occur in the workspace in subsequent processing. The context-dependent
activation of nodes enables the system to dynamically decide what is relevant at a
given point in time, and influences what types of actions the system engages in.
</bodyText>
<subsectionHeader confidence="0.999513">
4.2 The Workspace
</subsectionHeader>
<bodyText confidence="0.999320714285714">
The workspace is meant to be the region where the system does the parsing and
construction required to understand a sentence. This area can be thought of as corre-
sponding to the locus of the creation and modification of mental representations that
occurs in the mind as one tries to form a coherent understanding of a sentence. The
construction process is done by a large number of processing agents.
Figure 2 shows an example of a possible state of the workspace when the system
is processing sentence (9).
</bodyText>
<page confidence="0.95131">
537
</page>
<figure confidence="0.9905195">
Volume 22, Number 4
Computational Linguistics
character-1
character-2
character-3
•
•
•
character-n character
lexical marker
affix
affinity
word
/
&lt; &gt;
adjective
predicate
) agent
4. /A patient
1/
&apos;(
\\-4
object name N
I
theme / )
7
goal
classifier
source
a
reflexive
adjective
tirne /
/,
/„.-
---
structure
/&apos; --- ___,I coordination
C
..
- ----
\ ----
&amp;quot;--.N
\\ \N
V4&apos;, \N
\\ \ judgment
BA
quantity
manner
degree
complex
stative
construction
Legends:
</figure>
<listItem confidence="0.720294666666667">
&lt;---&gt; has-instance &amp; category-of link
— —&gt;. has-relation link
- - - -&gt; part-of link
</listItem>
<figureCaption confidence="0.917972">
Figure 1
</figureCaption>
<bodyText confidence="0.78707575">
The conceptual network.
aspect
direction
demonstrative
</bodyText>
<equation confidence="0.709796333333333">
I
question
(9) kt 2K A T =
</equation>
<bodyText confidence="0.978436916666667">
ta benren sheng le san ge haizi
she self give birth ASP three CL child
&apos;She herself has given birth to three children.&apos;
There are three types of objects that may exist in the workspace: character objects,
word objects, and chunk objects. The Chinese characters in Figure 2 not enclosed by
rectangles, namely, the characters E. stin and {I4 ge, are character objects. When a few
Chinese characters are enclosed by a rectangle, for example *A benren, it indicates
that these characters make up a word object. The constituent characters of the word
still exist in the workspace but they become less explicit in the figure. If a group
of characters is enclosed by two rectangles, for example, the character sheng, it
indicates that a chunk object exists, made up of word objects. In short, the immediate
constituents of a word object are character objects, and those of a chunk object are
</bodyText>
<page confidence="0.971773">
538
</page>
<figure confidence="0.889510833333333">
Gan, Palmer, and Lua
A Statistically Emergent Approach
affinity
rr
affix
* A =7
reflexive adjective aspectual
Figure 2
A possible state of the workspace.
Non-linguistic — statistical
Types of relations
— Linguistic
</figure>
<figureCaption confidence="0.821476">
Figure 3
</figureCaption>
<bodyText confidence="0.96967092">
An overview of the types of relations.
between character objects
between word objects
between chunk objects
word objects. It is possible to have unitary constituency whereby one object is the
only part of another object. The chunk object sheng &apos;give birth&apos; is an example.
Each object in the workspace has a list of descriptions not shown in Figure 2.
For example, descriptions of character objects include their morphological category
(stem/affix) and whether they are bound or unbound.&apos; Descriptions of word objects
include their categorial information and sense. Descriptions of chunk objects may also
include these two descriptions, except that here, these two descriptions are derived
from the category and the sense of the word that is the governor.
The directed arc connecting two objects in Figure 2 denotes a linguistic relation
between the objects connected. We adopt the dependency grammar notation (Tesniere
1959; Mead( 1988) in which the object pointed to by an arrow is the dependent while
the object where the arrow originates is the governor. The undirected arc connecting
the characters hal and 7- zi in Figure 2 represents a statistical relation, and statistical
relations are undirected in our representation.
An overview of our classification of relations is shown in Figure 3.
A list of all types of relations is summarized in Table 1; a detailed exposition can
be found in Can (1994).
In Figure 2, the connection between the word objects #t ta &apos;she&apos; and *A benren
&apos;self&apos; is a reflexive adjective relation, the connection between the word objects sheng
&apos;give birth&apos; and 7 le &apos;ASP&apos; is an aspectual relation, and the two arcs connecting the
character objects TA. hal and 7 zi are affix and affinity relations.
</bodyText>
<page confidence="0.8974255">
8 A bound character cannot occur independently as a word.
539
</page>
<table confidence="0.442093">
Computational Linguistics Volume 22, Number 4
</table>
<tableCaption confidence="0.996353">
Table 1
</tableCaption>
<table confidence="0.861118">
A list of all types of relations.
Object Type Relation Type Example
Object 1 Object 2
character affinity relation ,Figt
-÷.
character affix relation M
word classifier relation M &apos;CL&apos; it &apos;snake&apos;
word reflexive adjective relation tifi &apos;they&apos; *ft &apos;self&apos;
word structure relation 01 `STRUC&apos; 93E &apos;father&apos;
word coordination relation fli &apos;and&apos; F!11 &apos;Lisi&apos;
word adjective relation k &apos;blue&apos; R &apos;sky&apos;
word complex stative relation .(4 &apos;STRUC&apos; q &apos;good&apos;
word attitude relation fos &apos;really&apos; &apos;go&apos;
word disposal relation E &apos;BA&apos; 19 &apos;door&apos;
word quantity relation WM &apos;we&apos; All3 &apos;all&apos;
word manner relation * &apos;able&apos; Pg &apos;sing&apos;
word degree relation 11/ &apos;very&apos; 9A4 &apos;nervous&apos;
word aspectual relation IM &apos;sleep&apos; y &apos;ASP&apos;
word direction relation *7- &apos;table&apos; _h &apos;on&apos;
word demonstrative relation 2 &apos;this&apos; gt, &apos;fish&apos;
word interrogative relation ite &apos;what&apos; 1144R &apos;time&apos;
chunk agent relation It &apos;he&apos; fI4e7&apos; &apos;broke&apos;
chunk patient relation 19 &apos;door&apos; MT &apos;broke&apos;
chunk theme relation ef, &apos;chant&apos; &apos;scripture&apos;
chunk source relation MP al &apos;from China&apos; riv &apos;return&apos;
chunk goal relation MI &apos;to room&apos; * &apos;get&apos;
chunk time relation 4&apos;R &apos;today&apos; *NE &apos;not well&apos;
</table>
<subsectionHeader confidence="0.997221">
4.3 The Coderack
</subsectionHeader>
<bodyText confidence="0.999939666666667">
The building of linguistic structures (e.g., word and chunk objects, descriptions of
objects, relations between objects) is carried out by a large number of agents known
as codelets. These codelets reside in a data structure called the coderack. A codelet is
a piece of code that carries out some small, local task that is part of the process of
building a linguistic structure. For example, one codelet may check for the possibility
of building an aspectual relation between the words sheng &apos;give birth&apos; and 7 le
&apos;ASP&apos; of sentence (9). There are several codelet types. Each type is responsible for
building one of the relations shown in Table 1. In addition, there are word and chunk
codelet types, which are responsible for the construction of words and chunks. Two
special codelet types, namely, breaker and answer, will be explained in Section 5. Here,
we make a distinction between codelets and codelet type. The latter is a prewritten
piece of code while the former are instances of the latter.
In the initial stage when the program is presented with a sentence, the default
codelets initialized in the coderack are affix and affinity codelets. They will construct
relations between character objects. Some default bottom-up word codelets are also
posted to determine whether monosyllabic words could be constructed from character
objects. When the word node in the conceptual network becomes activated by activation
spreading from the character node, more top-down word codelets will be posted. When
word objects are constructed, nodes denoting relevant relations between words will be
activated. These nodes in turn cause the posting of codelets that will build relations
between word objects. Again, by activation spreading to the chunk node, codelets
</bodyText>
<page confidence="0.984679">
540
</page>
<note confidence="0.924451">
Can, Palmer, and Lua A Statistically Emergent Approach
</note>
<bodyText confidence="0.999791666666667">
building chunk objects will be posted, which will further lead to the posting of codelets
that determine how chunk objects can be related.
Note that there is no top-level executive deciding the order in which codelets
are executed. At any given time, one of the existing codelets is selected to execute.
The selection is a stochastic one, and it is a function of the relative urgencies of all
existing codelets. The urgency of a codelet is a number assigned at the time of its
creation to represent the importance of the task that it is supposed to carry out (this
is an integer between 1 to 7, with 1 as the least urgent and 7 as the most urgent).
Many codelets are independent and they run in parallel. Therefore, efforts towards
building different structures are interleaved, sometimes co-operating and sometimes
competing. The rate at which a structure is built is a function of the urgencies of its
dedicated codelets. More promising structures are explored at high speeds and others
at lower speeds. Almost all codelets make one or more stochastic decisions, and the
high-level behavior of the program arises from the combination of thousands of these
very small choices. In other words, the system&apos;s high-level behavior arises from its
low-level stochastic substrate. To summarize, the macroscopic behavior of the system
is not preprogrammed; the details of how it emerges from the low-level stochastic
architecture of the system are given in Sections 5.2 and 5.3.
</bodyText>
<subsectionHeader confidence="0.999191">
4.4 The Computational Temperature
</subsectionHeader>
<bodyText confidence="0.999995">
The computational temperature is an approximate measure of the amount of coherency
in the system&apos;s interpretation of a sentence: the value at a given time is a function of
the amount and quality of linguistic structures that have been built in the workspace.
The computational temperature is in turn used to control the amount of randomness
in the local action of codelets. If many good linguistic structures have been built, the
temperature will be low, and the system will make decisions less randomly. When
few good linguistic structures have been found, the temperature will be high, leading
to many more random decisions and hence to more diverse paths being explored by
codelets.9
The notion of temperature used here is similar to that in simulated annealing
(Kirkpatrick, Gelatt, and Vecchi 1983). Both start with a high temperature, allowing all
sorts of random steps to be taken, and slowly cool the system down by lowering the
temperature. However, the decrease in temperature in our system is not necessarily
monotonic. It varies according to the amount of coherency in the system&apos;s interpreta-
tion of a sentence. Thus, our system has an extra degree of flexibility, which allows
uphill steps in temperature; in effect, this means that the system is annealing at the
metalevel as well.
</bodyText>
<sectionHeader confidence="0.699143" genericHeader="method">
5. An Example
</sectionHeader>
<bodyText confidence="0.988729571428571">
We will use a sample run of the program on sentence (9) to illustrate many central
features of the model, including the selection of a codelet; the selection of competing
alternatives; the interaction between the workspace and the conceptual network; etc.
Note that this section would be overwhelmed with details if a step-by-step explanation
were given. A detailed trace of the system&apos;s execution on this sentence can be found
in Can (1994), and a short description of the program&apos;s behavior can be found in Gan
(1993). Here, only selected snapshots are highlighted.
</bodyText>
<footnote confidence="0.610405">
Sentence (9) is an example with local, overlap, and combination ambiguities in the
9 &amp;quot;Diverse paths&amp;quot; refers to different ways of analyzing the structure of a sentence.
</footnote>
<page confidence="0.941756">
541
</page>
<table confidence="0.458755">
Computational Linguistics Volume 22, Number 4
</table>
<tableCaption confidence="0.988536">
Table 2
</tableCaption>
<table confidence="0.925134666666667">
Initial state of the coderack.
Codelet Type Urgency (U) Temperature-regulated Urgency Quantity
Ut = 100 Ut = 0
word 2 2 16 14
affinity 3 2 81 20
affix 3 2 81 8
</table>
<bodyText confidence="0.98379575">
fragment *AA ben ren sheng. Without considering the sentential context, these three
characters have three possible word boundaries: * A Ii ben ren sheng &apos;CL human give
birth&apos;, *A benren sheng &apos;self give birth&apos; or * AI ben rensheng &apos;CL life&apos;. Given the
sentential context of (9), however, only the second alternative is correct.
</bodyText>
<subsectionHeader confidence="0.982197">
5.1 Initial Setup
</subsectionHeader>
<bodyText confidence="0.999983533333333">
When the parsing process starts, the program is presented with the sentence. The
temperature is clamped at 100 for the first 80 cycles to ensure that diverse paths
are explored initially (the range of the temperature varies between 0 and 100). A
cycle is the execution of one codelet. The number 80 is decided based on intuition
and trial-and-error; it is not necessarily optimal. The workspace is initialized with
nine character objects, each corresponding to a character of the sentence. Since the
workspace contains only character objects, the only relevant concepts are: character,
affinity, affix, and each character of the sentence. The corresponding nodes in the
conceptual network, namely: character, affinity, affix, t ta, * ben, up to T- zi, are set
to full activation. Fourteen instances of word codelet are posted to the coderack. They
are responsible for identifying and constructing monosyllabic words. Twenty instances
of affinity codelet are also posted to identify and construct affinity relations between
characters. Eight instances of affix codelet are posted to identify and construct affix
relations between characters. In general, the number of codelets posted is a function
of the length of a sentence.
</bodyText>
<subsectionHeader confidence="0.999993">
5.2 Selection of a Codelet
</subsectionHeader>
<bodyText confidence="0.989136666666667">
Among all codelet instances that exist in the coderack, only one of them is stochas-
tically selected to execute each time. The choice of which codelet instance to execute
depends on three factors: (i) its urgency, (ii) the number of codelet instances in the
coderack that are of the same type as the individual instance, and (iii) the current
temperature. At cycle 0, the coderack contains the statistics as shown in Table 2.
The temperature-regulated urgency (Ut) is derived in the following way:
</bodyText>
<equation confidence="0.983298">
ut = u(120- 0/30 (1)
</equation>
<bodyText confidence="0.999931">
where t denotes the temperature, which ranges between [0,100]. This equation is used
to magnify differences in urgency values when the temperature is low. Conversely, at
high temperatures, it will minimize differences in urgency values. The idea is to let
the system explore diverse paths when the temperature is high, while always stick to
one search path when the temperature is low.
At cycle 0 where the temperature is 100, the temperature-regulated urgencies of
the three codelet types are the same. The probability of selecting an instance of a word
codelet, an affinity codelet, and an affix codelet is 33.3%, 47.6%, and 19.1% respectively.
</bodyText>
<page confidence="0.990097">
542
</page>
<note confidence="0.540498">
Gan, Palmer, and Lua A Statistically Emergent Approach
</note>
<figure confidence="0.990769666666667">
th r
114 N
affinity
</figure>
<figureCaption confidence="0.994884">
Figure 4
</figureCaption>
<bodyText confidence="0.925928">
State of the workspace at cycle 17.
These probabilities are derived as follows:
</bodyText>
<equation confidence="0.891455">
x Qi (2)
Pt(c), i(uu x Qi)
</equation>
<bodyText confidence="0.999953571428571">
where Q, and (21 are the quantities of codelet types C, and Cj respectively, U,,t and Um
are the urgencies of codelet types C, and C., at temperature t respectively, and n is the
total number of codelet types.
Supposing that the coderack contains the same types of codelets with the same
quantities, but the temperature is 0, the probability of selecting an instance of a word
codelet, an affinity codelet, and an affix codelet becomes 8.99%, 65.01%, and 26.00%
respectively. Therefore, at low temperatures, codelets with high urgency are preferred.
</bodyText>
<subsectionHeader confidence="0.999979">
5.3 Construction of Linguistic Structures
</subsectionHeader>
<bodyText confidence="0.9977854">
Linguistic structures include high-level objects (words and chunks) and relations be-
tween two objects (see Table 1). In this run, for example, an affinity relation between
the character objects 4 ben and A ren is constructed by an instance of an affinity
codelet at cycle 17 (Figure 4).
An affinity codelet works on any two adjacent character objects to evaluate whether
an affinity relation should be built between these two characters. The affinity relation
is a quantitative measure that reflects how strongly two characters co-occur statis-
tically. It is derived from mutual information (Fano 1961), which is the probability
that two characters occur together versus the probability that they are independent.
Mathematically, it is:
</bodyText>
<equation confidence="0.9994745">
A(a,b) = log2 (3)
P(a)P(b)
</equation>
<bodyText confidence="0.999961545454546">
where A(a, b) is the affinity relation between the character objects a and b, P (a, b) is
the probability that the two character objects co-occur consecutively, P(a) and P(b) are
the probabilities that a and b occur independently. To derive affinity relations between
characters, we have the usage frequencies of 6,768 Chinese characters specified in the
GB2312-80 standard, and the usage frequencies of 46,520 words derived from a corpus.
The total usage frequency of these words is 13,019,814. (The data was obtained from
Liang Nanyuan, Beijing University of Aeronautics and Astronautics.)
Note that efforts towards building different structures are interleaved, as many
codelets are independent and they run in parallel. Apart from the initial set of codelets
present at the onset of processing, new codelets are sometimes created by old codelets
to continue working on a task in progress, and these codelets may in turn create other
</bodyText>
<page confidence="0.995575">
543
</page>
<note confidence="0.622828">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.9756155">
codelets, and so on. The cycle in which a structure is built is not preprogrammed.
Rather, it emerges from the statistics of the interaction of all codelets in the coderack.
</bodyText>
<subsectionHeader confidence="0.999815">
5.4 Selection of Competing Structures
</subsectionHeader>
<bodyText confidence="0.999374142857143">
It may happen that a structure being constructed is in conflict with an existing struc-
ture. In this run, for example, an affinity relation between the characters A ren and
sheng is being considered at cycle 79. This structure is in conflict with the previously
constructed affinity relation between the characters * ben and A ren. The decision
about which competing structure should win is decided stochastically as a function
of two factors: (i) the strengths of the competing structures, and (ii) the temperature.
The strength of a structure is an approximate measure of how promising the structure
is. It is an integer ranging between 0 and 100, inclusive. The strengths of different
structures are derived according to either linguistic knowledge encoded in the lexicon
or certain statistical measures. Equation (3) is a key factor in deriving the strength of
an affinity relation. In this run, the strength of the proposed affinity relation between
the characters A ren and sheng is 55, while that of the existing affinity relation
between the characters * ben and A ren is 56. These two values are adjusted by the
temperature according to equation (4).
</bodyText>
<equation confidence="0.992719">
St = s(120-t)/40 (4)
</equation>
<bodyText confidence="0.9998289375">
where St is the temperature-regulated strength, S is the original strength, and t is
the temperature. The effect of equation (4) is similar to equation (1): to maximize
differences in strength values at low temperatures, and to minimize differences at
high temperatures. At cycle 79, the temperature is still clamped at 100, and hence the
temperature-regulated strengths of these two competing structures are both 7 (rounded
up to the nearest integer). The decision about which structure should win is therefore
a random one, as both have an equal probability of success. According to equation (4),
at low temperatures, it is increasingly difficult for a new structure of lesser strength to
win in competition against existing structures of greater strength. Since the system&apos;s
behavior is more random at high temperatures, it is able to explore diverse paths in
the initial stage when little structure has been built. When a large number of structures
deemed to be good have been found, which entails a low temperature, the system will
proceed in a more deterministic fashion, always preferring good paths to bad ones.
Indeed, in this case, the new affinity relation between the characters A ren and
sheng has won. Instead of destroying the affinity relation between the characters *
ben and A ren, this structure is retained, but it becomes dormant in the workspace.
</bodyText>
<subsectionHeader confidence="0.980818">
5.5 The interaction between the Workspace and the Conceptual Network
</subsectionHeader>
<bodyText confidence="0.999931">
Activated nodes in the conceptual network spread activation to their neighbors, and
thus concepts closely related to relevant concepts also become relevant. In this run,
for example, the nodes word and chunk become activated at cycle 80 due to activation
spreading from the character node. Activated nodes influence what tasks the system
will focus on subsequently through the posting of top-down codelets. For example, at
cycle 80, the activated word node causes the proportion of word codelets to increase
to 93%. This is an important feature of the system: the context-dependent activation
of nodes, which enables the system to dynamically decide what is relevant at a given
point in time, and influences what actions to take through the posting of top-down
codelets.
</bodyText>
<page confidence="0.994939">
544
</page>
<note confidence="0.557367">
Gan, Palmer, and Lua A Statistically Emergent Approach
</note>
<figure confidence="0.9352645">
affinity
Atli F-1 7
I
dormant affinity affinity affix
</figure>
<figureCaption confidence="0.977842">
Figure 5
</figureCaption>
<bodyText confidence="0.948728">
State of the workspace at cycle 180.
</bodyText>
<subsectionHeader confidence="0.994241">
5.6 Detection and Resolution of Erroneous Structures
</subsectionHeader>
<bodyText confidence="0.994698">
By the end of cycle 180, the following structures have been built (Figure 5):
</bodyText>
<listItem confidence="0.963163">
• active relations: an affinity relation between the characters A ren and t
shjng, hai and zi, an affix relation between the characters TA hai and
zi;
• active word objects: A7 haizi &apos;child&apos;, At rensheng &apos;life&apos;, and * ben &apos;CL&apos;;
• active chunk objects: At rensheng &apos;life&apos;, and 1.-A7 haizi &apos;child&apos;;
• dormant relations: an affinity relation between the characters * ben and
A ren.
</listItem>
<bodyText confidence="0.999959947368421">
Among them, the word * ben &apos;CL&apos; is a classifier. This word has activated the
classifier node in the conceptual network, which in turn causes the posting of classifier
codelets to the coderack. The responsibility of this type of codelet is to explore the
possibility of establishing a classifier relation between a classifier and an object name.1°
The use of a classifier is in general idiosyncratic. This type of idiosyncrasy is encoded
in the lexicon. Since * ben cannot be the classifier of the object name At rensheng
&apos;life&apos;, a special type of codelet known as a breaker codelet is posted to the coderack.
The role of a breaker is to identify erroneous linguistic structures, and set them to
dormant, restoring any dormant competing structure when necessary
At cycle 187, a breaker codelet is executed that examines structures that are &amp;quot;in-
trouble&amp;quot;, namely, the words * ben and At rensheng &apos;life&apos;. Since the component
characters of the second word can be free, the breaker codelet concludes that this is
an erroneous grouping. The word A t rensheng &apos;life&apos; is made dormant. The other
structures that support the word At rensheng &apos;life&apos;, namely the affinity relation be-
tween the characters A ren and t sheng and the chunk At rensheng &apos;life&apos;, are also
made dormant. The competing alternative, the affinity relation between the characters
* ben and A ren, is reactivated. This snapshot also illustrates an important feature
of the system: syntactic analysis can be performed without waiting for the system to
complete the task of word identification.
</bodyText>
<footnote confidence="0.807016">
10 The term object name is borrowed from Meaning-Text linguistics (Me1&apos;6uk 1988). It refers to words that
cannot have a semantic dependent. A more formal attempt to define this term can be found in
Polguere (to appear).
</footnote>
<page confidence="0.994344">
545
</page>
<figure confidence="0.756402">
Computational Linguistics Volume 22, Number 4
</figure>
<figureCaption confidence="0.976653">
Figure 6
</figureCaption>
<bodyText confidence="0.915403">
State of the workspace at cycle 373.
</bodyText>
<subsectionHeader confidence="0.996869">
5.7 The Final State
</subsectionHeader>
<bodyText confidence="0.87318225">
Figure 6 shows the state of the workspace at the end of cycle 373.
For easy reference, sentence (9) is repeated here:
(9) kt4A Ji 7
ti IA
benren sheng le san ge hciizi
she self give birth ASP three CL child
&apos;She herself has given birth to three children.&apos;
The list of structures built are:
</bodyText>
<listItem confidence="0.642624">
• active relations: an affinity relation between the characters * ben and A
</listItem>
<bodyText confidence="0.908011166666667">
ren,Tf.&lt; hai and zi, an affix relation between the charactersIP, .&lt;hái and
7- zi, a reflexive adjective relation between the words kt ta &apos;she&apos; and
*A benren &apos;self&apos;, a classifier relation between the words SI ge &apos;CL&apos; and
A 7- hciizi &apos;child&apos;, a quantity relation between the words E. san &apos;three&apos;
and A 7- haizi &apos;child&apos;, an aspectual relation between the words sheng
&apos;give birth&apos; and 7 le &apos;ASP&apos;;
</bodyText>
<listItem confidence="0.998668625">
• active words: kt ta &apos;she&apos;, *A benren &apos;self&apos;, sheng &apos;give birth&apos;, 7 le
&apos;ASP&apos;,&apos;El- san &apos;three&apos;, 1111 ge &apos;CU, and A 7- haizi &apos;child&apos;;
• active chunks: kt*Ata benren &apos;she herself&apos;, sheng &apos;give birth&apos;, and
fi?P san ge haizi &apos;three CL children&apos;;
• dormant relations: an affinity relation between the characters A ren and
sheng;
• dormant words: A rensheng &apos;life&apos;;
• dormant chunks: ./A rensheng &apos;life&apos;.
</listItem>
<figureCaption confidence="0.729281666666667">
Comparing the above structures with the complete analysis of the sentence in
Figure 7 (for simplicity, we have omitted relations between characters in Figure 7),
it is observed that the system has not yet constructed the agent and theme relations.
</figureCaption>
<bodyText confidence="0.7462805">
They were not identified because the system has come to a stop at cycle 381, after
an instance of answer codelet was executed. This type of codelet reports on the word
</bodyText>
<page confidence="0.986103">
546
</page>
<figure confidence="0.946456555555556">
Gan, Palmer, and Lua A Statistically Emergent Approach
agent
theme
* A
reflexive adjective aspect
#61
1
classifier
quantity
</figure>
<figureCaption confidence="0.995176">
Figure 7
A complete analysis of sentence (9).
Figure 8
</figureCaption>
<bodyText confidence="0.970683928571429">
A graph of structures constructed against cycle number.
boundaries of a sentence. The system currently adopts a greedy approach and starts
posting large numbers of this type of codelet as soon as it has identified a plausible
interpretation of the word boundaries of a sentence. Hence, although instances of agent
and theme codelets were present in the coderack, they were being overwhelmed by
the ubiquitous answer codelets.
Figure 8 summarizes the cycle number in which various types of structures were
constructed during this run. In this figure we see that affinity relations are built earlier
than words, reflecting the system&apos;s preference for words of greater lengths. The system
makes use of statistical information (the mutual information scores) to make quick and
reliable guesses of the locations of these words. It can also be observed that overall,
there is a gradual shift in the types of operations executed, from being character-
centered initially, to word-centered, and then to chunk-centered. From time to time,
however, the construction of different types of structures is interleaved.
</bodyText>
<listItem confidence="0.483984">
6. System Performance and Discussions
</listItem>
<bodyText confidence="0.962964666666667">
Thirty ambiguous fragments that have alternating word boundaries in different sen-
tential contexts were presented to the system and the system was able to resolve all
the ambiguities. The test set covers the four types of word boundary ambiguities de-
</bodyText>
<figure confidence="0.999551952380952">
•
a
I
•
•
•
•
•
•
_
_
-
•
8
400
300
200
u
100
affinity affix word chunk refl. adj. classifier quantity aspect ttal
0
</figure>
<page confidence="0.682687">
547
</page>
<note confidence="0.49309">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.996875333333333">
scribed in Section 2. When the sentential contexts of locally ambiguous fragments (both
the overlap and combination type) were varied, our system was able to identify the
correct word boundaries. When the system was presented with sentences with global
ambiguities, it produced all the plausible alternative word boundaries. However, at
any run of such a sentence, only one alternative is generated. The system&apos;s behavior is
similar to human performance in the goblet/faces recognition problem in perception
(Hoffman and Richards 1984). We cannot see both the goblet and the faces at the same
time, but we are able to switch back and forth between these two interpretations.
The frequencies of generating all the alternatives vary from one sentence to another.
It is important to note that such frequencies are not meant to indicate some kind of
&amp;quot;goodness&amp;quot; measure of alternative word boundary interpretations. Neither are they
meant to reflect the preferences of a human. They are merely a reflection of the usage
frequencies of Chinese characters and words in our dictionary.
The system&apos;s ability to generate different word boundaries for a globally ambigu-
ous sentence arises from its stochastic search mechanism, which does not rule out
a priori certain possibilities. This feature enables the system to occasionally discover
less-obvious interpretations of word boundaries. For example, in addition to the two
apparent ways of aligning the fragment Et i yi&amp;quot; jrng gue) as either Et yijingguO
&apos;already over&apos; or E yijrngguO &apos;already go through&apos; in sentences (10a) and (10b),
a less-obvious possibility that the system has identified is: E fff yljing gub &apos;already
experience over&apos;, where i gue• &apos;over&apos; is the complement of jfng &apos;experience&apos;.
</bodyText>
<equation confidence="0.749693">
(10)a. ft E
wo yijing gub le xuesheng shIdài
I already over ASP student period
&apos;My student days are over.&apos;
b. ft E ffflja T nit
w6 yr jinggite le xuesheng shIdài
I -already go through ASP student period
&apos;I have already gone through the period as a student.&apos;
c. ft E A 7 i4t- nit
wo yl ling gni) le
_ xuesheng shiclai
</equation>
<bodyText confidence="0.996869307692308">
I already experience over ASP student period
&apos;I have already experienced student life.&apos;
The system rarely produces the less-obvious interpretations. This demonstrates that
its mechanisms are able to strike an effective balance between random search and
deterministic search, imbuing it with both flexibility and robustness.
An issue that arises from the nondeterministic feature of the system is: will the
word boundaries of a locally ambiguous sentence vary at different runs? To address
this, we ran the program with each sentence 20 times. We found that for sentences cov-
ered by our current set of linguistic descriptions, the system arrived at the same word
boundaries despite different paths being taken at each run. For linguistic phenom-
ena not yet covered, suboptimal solutions may sometimes be generated. For example,
when the program worked on sentence (10), it produced sentence (11) once as the
answer.
</bodyText>
<page confidence="0.992326">
548
</page>
<note confidence="0.777984">
Gan, Palmer, and Lua A Statistically Emergent Approach
</note>
<equation confidence="0.310165">
(11) ita 6 RJR fa AI *
zhOngguO yi kaifa he shang wei
</equation>
<bodyText confidence="0.8622005">
China already exploit and yet not
RJR Erg kiff, tli 411
kaifa de ziyuan clOu hen duo
exploit STRUC resource all very many
&apos;China has many resources which have either been exploited
or not yet been exploited.&apos;
</bodyText>
<equation confidence="0.5462625">
(12)* rM 6
zhanggue• yl kaifa he shang wei
</equation>
<bodyText confidence="0.996236375">
China already exploit and yet not
fa de ziyuan dOu hen3 duo
open distribute STRUC resource all very many
In this run, the bisyllabic word Dm icaifa &apos;develop&apos; has been wrongly identified as
two monosyllabic words NI kai &apos;open&apos; and4%). fa &apos;distribute&apos;. To determine the proper
use of two juxtaposed predicates, such as Du kai &apos;open&apos; and 4;4_, fa &apos;distribute&apos; in this case,
requires a careful study of serial verb constructions. It is inevitable that the system
would make such a mistake as our linguistic descriptions have not yet covered this
phenomenon.
In comparison, consider the performance of a strictly statistical approach based
on mutual information (Lua and Gan 1994): the latter wrongly identified the word
boundaries in 11 out of the 30 ambiguous fragments. For the 6 fragments that appear
in globally ambiguous sentences, the mutual information approach gave only one
interpretation of the word boundaries. In terms of processing speed, the mutual infor-
mation approach took an average of 110.4 ms to process one character; our approach
took 1.7 s.11 The extra time in our approach is spent in parsing sentences.
</bodyText>
<sectionHeader confidence="0.793285" genericHeader="method">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.999517">
In this paper, we reported on a stochastically emergent model for language processing
and described its application to the modeling of context effects in ambiguous Chinese
word boundary interpretation. The model simulates language processing as a collective
phenomenon that emerges from a myriad of microscopic and diverse activities. The
proposed mechanism, whereby word objects and chunk objects are formed by the
hooking up of character objects as the latter are gradually cooled down, is analagous
to the crystallization process in chemistry.
Our application is distinct from existing work in two main respects:
</bodyText>
<listItem confidence="0.826750666666667">
• Word identification: We show that the full power of natural language
processing can be brought to bear on the issue of word identification
effectively and seamlessly. The model is able to resolve ambiguities
</listItem>
<bodyText confidence="0.7347985">
appearing in different sentential contexts. This is an improvement over
statistical approaches such as the relaxation method (Fan and Tsai 1988),
which generates all possible ways of grouping the characters of a
sentence into words, and then uses some scoring function to select the
11 The mutual information approach was written in Borland C, version 2.0 while the new approach was
written in Borland C++, version 3.0. Both ran on a 33 MHz, 386 machine.
</bodyText>
<page confidence="0.991826">
549
</page>
<note confidence="0.689779">
Computational Linguistics Volume 22, Number 4
</note>
<bodyText confidence="0.999669375">
best combination. At the same time, this model eliminates the use of ad
hoc rules, as syntactic and semantic analysis are interleaved with word
identification. This application is diametrically opposed to the
reductionist approach of separating word segmentation and sentence
analysis into two distinct stages. We have argued that our approach can
avoid the computational problem of combinatorial explosion as the
architecture has appropriate mechanisms to regulate run-time resources
dynamically.
</bodyText>
<listItem confidence="0.908739">
• Sentence analysis: We show that a sentence can be analyzed without
</listItem>
<bodyText confidence="0.99031697368421">
assuming a presegmented input. The main feature is that there is no
fixed, predetermined order of morphological, syntactic, and semantic
analysis, since the control mechanism is a nondeterministic one.
Essentially, the order in which these analyses are carried out is
dependent on what has been discovered so far by the system, and the
system&apos;s perception of what is relevant to the task it is currently
investigating.
The essential idea of the proposed model is that of stochastically guided conver-
gence to what is called a globally optimum state. This model shares some features with
APRIL (Annealing Parser for Realistic Input Language) (Sampson, Haigh, and Atwell
1989). APRIL uses simulated annealing to determine the most plausible parse tree of
a sentence. It begins with an arbitrary tree. Many local modifications are generated
randomly. They are either adopted or rejected according to their effect on a plausibil-
ity measure. Modifications that improve the plausibility measure are always accepted;
while unfavorable modifications are rejected only if the loss of merit exceeds a certain
threshold. The threshold value is generated randomly but its mean value decreases
according to some predefined schedule. This differs from the behavior of the compu-
tational temperature in our system, which does not have a monotonically decreasing
property. Our system further differs from APRIL in the following aspects: (i) APRIL
begins with an arbitrary parse tree whereas our system begins with no parse structure;
(ii) APRIL&apos;s plausibility measure is defined using statistics collected from a treebank
of manually parsed English text while ours is derived from mutual information statis-
tics and linguistic constraints; (iii) parse trees in APRIL are immediate-constituency
type while ours are dependency-based. That is, nodes in our system are either char-
acters, words, or chunks. There are no nonterminal nodes defined with grammatical
categories.
Our model also shares some features with connectionist models, such as fine-
grained parallelism, local actions, competition, spreading activation, and statistically
emergent effects from a large number of small, subcognitive events. On the other
hand, the representation of concepts is quite different: they are encoded as atomic,
symbolic primitives instead of distributed as weighted connections between nodes in
a network, which is common in connectionist systems. Therefore, in terms of the degree
to which concepts are distributed, our representation has a strong symbolic flavor; in
terms of the extent to which high-level behavior emerges from lower-level processes,
ours has a strong subsymbolic orientation. By providing an account of the language
understanding process at such an intermediate level of description, it is hoped that our
results will provide a guide to connectionists studying how such intermediate-level
structures can emerge from neurons or cell-assemblies in the brain.
</bodyText>
<page confidence="0.994943">
550
</page>
<note confidence="0.967779">
Can, Palmer, and Lua A Statistically Emergent Approach
</note>
<sectionHeader confidence="0.882857" genericHeader="discussions">
8. Future Work
</sectionHeader>
<bodyText confidence="0.997311533333333">
Our application, which handles only thirty sentences at present, has enabled us to
focus on the mechanisms that underlie the process of sentence comprehension, and
their interactions. With the progress made in this study, which would not have been
possible if we had plunged straight into large-scale unrestricted texts, our next concern
would be to address the issue of scalability. There are two aspects to this issue.
• The effect of various parameter values chosen for the formulae shown in
Section 5 on the operation of the program: These values are set by
trial-and-error. They are not specifically tailored to our test set. To finesse
these parameters in order to completely weed out unpromising search
paths is impossible, since decision making in the system is stochastic. We
therefore do not anticipate that the setting of the various parameter
values is an issue during scaling up. The values of the parameters may
affect the rate of convergence, but they will not affect the accuracy of the
system in terms of the analysis results.
• The possibility of generating thousands of codelets as a result of using a
large lexicon: We do not expect such a scenario to occur. Instead, having
a large lexicon means that the system is able to handle more sentences.
The number of codelets spawned to process a sentence is determined by
the number of characters and words in the sentence, and the types of
words and chunks in the sentence, not by the size of the lexicon. In
addition, there are built-in mechanisms to manage the growth of
codelets. We have demonstrated in Section 5 how we have made use of
statistics (the maximum matching heuristics and mutual information) to
avoid generating all possible word boundary combinations. The sample
run in Section 5 has also demonstrated that the program need not finish
executing all codelets in the coderack before it is allowed to stop, and
that simpler and more clear-cut decisions tend to be made before the
more subtle ones. Furthermore, certain features of the system, namely,
the stochastic selection of a codelet by relative urgencies, the use of the
conceptual network as a top-down controller, the interactions between
the conceptual network and the workspace, enable the system to
dynamically decide on the number and the types of codelets to be
generated.
The real bottleneck when scaling-up is the acquisition of linguistic descriptions,
as our current work has limited breadth and depth of coverage. Therefore, the cur-
rent system has less practical value to people working on the word segmentation
problem, where the main concern is to develop algorithms that work for large-scale
text. However, the proposed model provides a useful architecture for us to study the
root of what people do when they encounter unknown words in text. This issue of
unknown-word resolution has been the single major problem in the segmentation of
unrestricted text. Understanding how higher-level knowledge is brought to bear on
this issue is essential to the design of an effective solution. Hence, our next goal is to
apply the model to handle the unknown-word problem, including treatments of un-
known compounds such as personal names, previously unseen place names, foreign
names in transliteration, and company names.
</bodyText>
<page confidence="0.993088">
551
</page>
<note confidence="0.757111">
Computational Linguistics Volume 22, Number 4
</note>
<sectionHeader confidence="0.973351" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.868714111111111">
Throughout the course of this work, we
have benefited from discussions with Alain
Polguere, Melanie Mitchell, Robert French,
Ngai-lai Cheng, Chew Lim Tan, Loke Soo
Hsu, Gee Kim Yeo, Guojin, Zhibiao Wu, and
Paul Wu. We would like to express our
thanks to them. We are also grateful to the
reviewers for their insightful comments and
suggestions.
</bodyText>
<sectionHeader confidence="0.975691" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999560663461539">
Chang, Jyun-Sheng, C. D. Chen; and S. D.
Chen. 1991. Chinese word segmentation
through constraint satisfaction and
statistical optimization (in Chinese). In
Proceedings of ROCLING-IV, R.O.C.
Computational Linguistics Conference,
pages 147-165.
Chao, Yuen-Ren. 1957. Formal and semantic
discrepancies between different levels of
Chinese structure. Bulletin of The Institute
of History and Philosophy, XXVIII: 1-16.
Chen, Keh-Jiann and Shing-Huan Liu. 1992.
Word identification for Mandarin Chinese
sentences. In Proceedings of COLING-92,
pages 101-107.
Chiang, Tung-Hui, Jing-Shin Chang,
Ming-Yu Lin, and Keh-Yih Su. 1992.
Statistical models for word segmentation
and unknown resolution. In Proceedings of
ROCLING V. R.O.C. Computational
Linguistics Conference, pages 121-146.
Fan, Charng-Kang and Wen-Hsiang Tsai.
1988. Automatic word identification in
Chinese sentences by the relaxation
technique. Computer Processing of Chinese
and Oriental Languages, 4(1): 33-56.
Fano, Robert M. 1961. Transmission of
Information. MIT Press, Cambridge, MA.
French, Robert M. 1992. Tabletop: An
Emergent, Stochastic Computer Model of
Analogy-Making. Ph.D. thesis, University
of Michigan.
Gan, Kok-Wee. 1993. Integrating word
boundary identification with sentence
understanding. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 301-303.
Ohio State University, June.
Gan, Kok-Wee. 1994. Integrating Word
Boundary Disambiguation with Sentence
Understanding. Ph.D. thesis, Department
of Information Systems &amp; Computer
Science, National University of Singapore.
He, Ke-Kang, Hui Xu, and Bo Sun. 1991.
Design principle of expert system for
automatic word segmentation in written
Chinese (in Chinese). Journal of Chinese
Information Processing, 5(2): 1-14.
Hoffman, Donald D. and Whitman A.
Richards. 1984. Parts of recognition.
Cognition, 18: 65-96.
Hofstadter, Douglas R. 1983. The
architecture of JUMBO. In Proceedings of
the International Machine Learning Workshop,
edited by Ryszard Michalski.
Huang, Xiang-Xi. 1989. A produce-test
approach to automatic segmentation of
written Chinese (in Chinese). Journal of
Chinese Information Processing, 3(4): 42-48.
Kirkpatrick, S., C. D. Gelatt Jr., and
M. P. Vecchi. 1983. Optimization by
simulated annealing. Science, 220: 671-680.
Lai, T. B. Y., S. C. Lun, C. F. Sun, and M. S.
Sun. 1992. A tagging-based first-order
Markov model approach to automatic
word identification for Chinese sentences.
In Proceedings of the 1992 International
Conference on Computer Processing of Chinese
&amp; Oriental Languages, pages 17-23.
Li, Charles N. and Sandra A. Thompson.
1981. Mandarin Chinese: A Functional
Reference Grammar. University of
California Press.
Liang, Nan-Yuan. 1983. Automatic word
segmentation in written Chinese and an
automatic word segmentation
system-CDWS (in Chinese). In
Proceedings of the National Chinese Language
Processing System.
Liang, Nan-Yuan. 1990. The knowledge of
Chinese words segmentation (in Chinese).
Journal of Chinese Information Processing,
4(2): 29-33.
Lua, Kim-Teng and Kok-Wee Gan. 1994. An
application of information theory in
Chinese word segmentation. Computer
Processing of Chinese &amp; Oriental Languages,
8(1): 115-123.
MelZuk, Igor A. 1988. Dependency Syntax:
Theory And Practice. State University Press
of New York.
Meredith, Marsha J. 1986. Seek-Whence: A
model of pattern perception. Technical
Report 214, Computer Science
Department, Indiana University,
Bloomington, IN.
Mitchell, Melanie. 1990. Copycat: A Computer
Model of High-Level Perception and
Conceptual Slippage in Analogy-Making.
Ph.D. thesis, University of Michigan.
Polguere, Alain. To appear. Meaning-text
semantic networks as a formal language.
In Current Issues In Meaning-Text
Linguistics, edited by Leo Wanner.
</reference>
<page confidence="0.930007">
552
</page>
<reference confidence="0.989545770833333">
Gan, Palmer, and Lua A Statistically Emergent Approach
Sampson, Geoffrey, Robin Haigh, and Eric
Atwell. 1989. Natural language analysis
by stochastic optimization: A progress
report on project APRIL. Journal of
Experimental and Theoretical Artificial
Intelligence, 1(4): 271-287.
Sproat, Richard and Chilin Shih. 1990. A
statistical method for finding word
boundaries in Chinese text. Computer
Processing of Chinese &amp; Oriental Languages,
4(4): 336-351.
Sproat, Richard, Chilin Shih, William Gale,
and Nancy Chang. 1994. A stochastic
finite-state word-segmentation algorithm
for Chinese. In Proceedings of the 32nd
Annual Meeting of the Association for
Computational Linguistics, pages 66-73.
Sproat, Richard, Chain Shih, William Gale,
and Nancy Chang. 1996. A stochastic
finite-state word-segmentation algorithm
for Chinese. Computational Linguistics,
22(3).
Tesniere, Lucien. 1959. Ele&apos;ments de la syntaxe
structurale. Klincksieck, Paris.
Wang, Xiaog-Long, Kai-Zhu Wang, and
Xiao-Hua Bai. 1991. Separating syllables
and characters into words in natural
language understanding (in Chinese).
Journal of Chinese Information Processing,
5(3): 48-58.
Wu, Ming-Wen and Keh-Yih Su. 1993.
Corpus-based automatic compound
extraction with mutual information and
relative frequency count. In Proceedings of
R.O.C. Computational Linguistics Conference
VI, pages 207-216.
Yao, Tian-Shun Gui-Ping Zhang, and
Ying-Ming Wu. 1990. A rule-based
Chinese automatic segmenting system (in
Chinese). Journal of Chinese Information
Processing, 4(1): 37-43.
Yeh, Ching-Long and Hsi-Jian Lee. 1991.
Rule-based word identification for
Mandarin Chinese
sentences—A unification approach.
Computer Processing of Chinese &amp; Oriental
Languages, 5(2): 97-118.
</reference>
<page confidence="0.999025">
553
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196086">
<title confidence="0.998429">A Statistically Emergent Approach for Language Processing: Application to Modeling Context Effects in Ambiguous Chinese Word Boundary Perception</title>
<author confidence="0.637865333333333">Martha Palmert Hong Kong University of Science</author>
<author confidence="0.637865333333333">University of Pennsylvania Technology</author>
<affiliation confidence="0.771032">Kim-Teng Luat National University of Singapore</affiliation>
<abstract confidence="0.9978729">This paper proposes that the process of language understanding can be modeled as a collective phenomenon that emerges from a myriad of microscopic and diverse activities. The process is analogous to the crystallization process in chemistry. The essential features of this model are: asynchronous parallelism; temperature-controlled randomness; and statistically emergent active symbols. A computer program that tests this model on the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences is presented. The program adopts a holistic approach in which word identification forms an integral component of sentence analysis. Various types of knowledge, from statistics to linguistics, are seamlessly integrated for the tasks of word boundary disambiguation as well as sentential analysis. Our experimental results showed that the model is able to address the word boundary ambiguity problems effectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jyun-Sheng Chang</author>
<author>C D Chen</author>
<author>S D Chen</author>
</authors>
<title>Chinese word segmentation through constraint satisfaction and statistical optimization (in Chinese).</title>
<date>1991</date>
<booktitle>In Proceedings of ROCLING-IV, R.O.C. Computational Linguistics Conference,</booktitle>
<pages>147--165</pages>
<marker>Chang, Chen, Chen, 1991</marker>
<rawString>Chang, Jyun-Sheng, C. D. Chen; and S. D. Chen. 1991. Chinese word segmentation through constraint satisfaction and statistical optimization (in Chinese). In Proceedings of ROCLING-IV, R.O.C. Computational Linguistics Conference, pages 147-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuen-Ren Chao</author>
</authors>
<title>Formal and semantic discrepancies between different levels of Chinese structure.</title>
<date>1957</date>
<booktitle>Bulletin of The Institute of History and Philosophy, XXVIII:</booktitle>
<pages>1--16</pages>
<contexts>
<context position="10900" citStr="Chao 1957" startWordPosition="1771" endWordPosition="1772"> NI A women dou hen nan gub we all very hard live &apos;We all have a hard life.&apos; b. MI X 111 ffbig women dou hen nangub we all very sad &apos;We all feel very sad.&apos; The fragment ffig nangub also has combination ambiguity. It differs from (3) in that the sentence in which it appears has two plausible interpretations. Hence, this fragment can either be segmented as fit nan &apos;hard&apos; and A gut) &apos;live&apos; in (4a), or as Ntig nangua &apos;sad&apos; in (4b). Word boundary ambiguity is a very common phenomenon in written Chinese, due to the fact that a large number of words in modern Chinese are formed from free characters (Chao 1957). The problem also exists in continuous speech recognition research, where correct interpretation of word boundaries in an utterance requires linguistic and nonlinguistic information. However, people have a fascinating ability to fluidly perceive groups of characters as words in one context but break these groups apart in a different context. This human capability highlights the fact that there is a continual interaction between word identification and sentence interpretation. We are therefore motivated to study how our statistically emergent model can be used to simulate the interactions betw</context>
</contexts>
<marker>Chao, 1957</marker>
<rawString>Chao, Yuen-Ren. 1957. Formal and semantic discrepancies between different levels of Chinese structure. Bulletin of The Institute of History and Philosophy, XXVIII: 1-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Jiann Chen</author>
<author>Shing-Huan Liu</author>
</authors>
<title>Word identification for Mandarin Chinese sentences.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>101--107</pages>
<contexts>
<context position="16087" citStr="Chen and Liu 1992" startWordPosition="2571" endWordPosition="2574"> &apos;mark&apos;. As a result, this method fails to correctly identify the word boundaries in sentence (6). Within statistical approaches, considering, for example, the mutual information method (Lua and Can 1994), the same fragment is identified as a bisyllabic word in both sentences (3a) and (6)7. By checking the structural relationships among words in a sentence, rule-based approaches aim to overcome limitations faced by pattern-matching and statistical approaches. However, many of the rules in existing rule-based systems (Huang 1989; Yao, Zheng, and Wu 1990; Yeh and Lee 1991; He, Xu, and Sun 1991; Chen and Liu 1992) are either arbitrary and word-specific, or overly general. For example, Rule Given an ambiguous fragment xyz where x, z, xy, and yz are all possible words, if x can be analyzed as a so-called direction word, segment the fragment as x yz, else segment it as xy z (Liang 1990). This syntactic rule works in sentence (7). (7) ft flff ta fü xia shenzi he bend down body &apos;He bends down his body.&apos; The fragment -F4T- xia shen zi in sentence (7) is ambiguous. As -F xia &apos;down&apos; is a direction word, the fragment is segmented as -F giT xia shenzi &apos;down body&apos;, which is as desired. Similarly, this rule will s</context>
</contexts>
<marker>Chen, Liu, 1992</marker>
<rawString>Chen, Keh-Jiann and Shing-Huan Liu. 1992. Word identification for Mandarin Chinese sentences. In Proceedings of COLING-92, pages 101-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tung-Hui Chiang</author>
<author>Jing-Shin Chang</author>
<author>Ming-Yu Lin</author>
<author>Keh-Yih Su</author>
</authors>
<title>Statistical models for word segmentation and unknown resolution.</title>
<date>1992</date>
<booktitle>In Proceedings of ROCLING V. R.O.C. Computational Linguistics Conference,</booktitle>
<pages>121--146</pages>
<contexts>
<context position="3991" citStr="Chiang et al. 1992" startWordPosition="586" endWordPosition="589">etwork of linguistic concepts, which ensures that these activities do not operate independently of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system</context>
<context position="13193" citStr="Chiang et al. 1992" startWordPosition="2112" endWordPosition="2115">ord by dictionary look-up. For example, in sentence (5), (5)-3-1-ntil (1&apos;3 jisuanjr de faming YIY1 zhengcla computer STRUC invention implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It zhOngda &apos;profound&apos; are identified. Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai et al. 1992). These approaches make use of co-occurrence frequencies of characters in a large corpus of written texts to achieve word segmentation without getting into deep syntactic and semantic analysis. For example, the relaxation approach uses the usage frequencies of words and the adjacency constraints among words to iteratively derive the most plausible assignment of characters into word classes. First, all possible words in a sentence are identified and assigned initial </context>
</contexts>
<marker>Chiang, Chang, Lin, Su, 1992</marker>
<rawString>Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin, and Keh-Yih Su. 1992. Statistical models for word segmentation and unknown resolution. In Proceedings of ROCLING V. R.O.C. Computational Linguistics Conference, pages 121-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charng-Kang Fan</author>
<author>Wen-Hsiang Tsai</author>
</authors>
<title>Automatic word identification in Chinese sentences by the relaxation technique.</title>
<date>1988</date>
<booktitle>Computer Processing of Chinese and Oriental Languages,</booktitle>
<volume>4</volume>
<issue>1</issue>
<pages>33--56</pages>
<contexts>
<context position="3943" citStr="Fan and Tsai 1988" startWordPosition="577" endWordPosition="580">ctivities are indirectly guided by a semantic network of linguistic concepts, which ensures that these activities do not operate independently of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Thro</context>
<context position="13144" citStr="Fan and Tsai 1988" startWordPosition="2103" endWordPosition="2106">he longest matched substring is selected as a word by dictionary look-up. For example, in sentence (5), (5)-3-1-ntil (1&apos;3 jisuanjr de faming YIY1 zhengcla computer STRUC invention implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It zhOngda &apos;profound&apos; are identified. Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai et al. 1992). These approaches make use of co-occurrence frequencies of characters in a large corpus of written texts to achieve word segmentation without getting into deep syntactic and semantic analysis. For example, the relaxation approach uses the usage frequencies of words and the adjacency constraints among words to iteratively derive the most plausible assignment of characters into word classes. First, all possible words i</context>
<context position="51736" citStr="Fan and Tsai 1988" startWordPosition="8391" endWordPosition="8394">anism, whereby word objects and chunk objects are formed by the hooking up of character objects as the latter are gradually cooled down, is analagous to the crystallization process in chemistry. Our application is distinct from existing work in two main respects: • Word identification: We show that the full power of natural language processing can be brought to bear on the issue of word identification effectively and seamlessly. The model is able to resolve ambiguities appearing in different sentential contexts. This is an improvement over statistical approaches such as the relaxation method (Fan and Tsai 1988), which generates all possible ways of grouping the characters of a sentence into words, and then uses some scoring function to select the 11 The mutual information approach was written in Borland C, version 2.0 while the new approach was written in Borland C++, version 3.0. Both ran on a 33 MHz, 386 machine. 549 Computational Linguistics Volume 22, Number 4 best combination. At the same time, this model eliminates the use of ad hoc rules, as syntactic and semantic analysis are interleaved with word identification. This application is diametrically opposed to the reductionist approach of separ</context>
</contexts>
<marker>Fan, Tsai, 1988</marker>
<rawString>Fan, Charng-Kang and Wen-Hsiang Tsai. 1988. Automatic word identification in Chinese sentences by the relaxation technique. Computer Processing of Chinese and Oriental Languages, 4(1): 33-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Fano</author>
</authors>
<title>Transmission of Information.</title>
<date>1961</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="35673" citStr="Fano 1961" startWordPosition="5762" endWordPosition="5763">ructures Linguistic structures include high-level objects (words and chunks) and relations between two objects (see Table 1). In this run, for example, an affinity relation between the character objects 4 ben and A ren is constructed by an instance of an affinity codelet at cycle 17 (Figure 4). An affinity codelet works on any two adjacent character objects to evaluate whether an affinity relation should be built between these two characters. The affinity relation is a quantitative measure that reflects how strongly two characters co-occur statistically. It is derived from mutual information (Fano 1961), which is the probability that two characters occur together versus the probability that they are independent. Mathematically, it is: A(a,b) = log2 (3) P(a)P(b) where A(a, b) is the affinity relation between the character objects a and b, P (a, b) is the probability that the two character objects co-occur consecutively, P(a) and P(b) are the probabilities that a and b occur independently. To derive affinity relations between characters, we have the usage frequencies of 6,768 Chinese characters specified in the GB2312-80 standard, and the usage frequencies of 46,520 words derived from a corpus</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, Robert M. 1961. Transmission of Information. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M French</author>
</authors>
<title>Tabletop: An Emergent, Stochastic Computer Model of Analogy-Making.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Michigan.</institution>
<contexts>
<context position="17829" citStr="French 1992" startWordPosition="2868" endWordPosition="2869">ly flexible in their perception of word boundaries of ambiguous fragments appearing in different sentential contexts. We believe that the separation of word identification from the task of analysis accounts for the difference in performance. This has motivated us to study how word identification and sentence analysis can be integrated. 7 This result is reported in Can (1994). 536 Gan, Palmer, and Lua A Statistically Emergent Approach 4. The Statistically Emergent Model This model is inspired by the work done in the Fluid Analogies Research Group (Hofstadter 1983; Meredith 1986; Mitchell 1990; French 1992). There are four main components in this model. Namely, (i) the conceptual network, which is a network of nodes and links representing some permanent linguistic concepts; (ii) the workspace, which is the working area in which high-level linguistic structures representing the system&apos;s current understanding of a sentence are built and modified; (iii) the coderack, which is a pool of structure-building agents (codelets) waiting to run; and (iv) the computational temperature, which is an approximate measure of the amount of disorganization in the system&apos;s understanding of a sentence. 4.1 The Conce</context>
</contexts>
<marker>French, 1992</marker>
<rawString>French, Robert M. 1992. Tabletop: An Emergent, Stochastic Computer Model of Analogy-Making. Ph.D. thesis, University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
</authors>
<title>Integrating word boundary identification with sentence understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>301--303</pages>
<institution>Ohio State University,</institution>
<contexts>
<context position="3816" citStr="Gan 1993" startWordPosition="557" endWordPosition="558">ous parallel mode. • Computational activities are a combination of top-down and bottom-up activities. • Computational activities are indirectly guided by a semantic network of linguistic concepts, which ensures that these activities do not operate independently of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interp</context>
<context position="31086" citStr="Gan (1993)" startWordPosition="5015" endWordPosition="5016">in effect, this means that the system is annealing at the metalevel as well. 5. An Example We will use a sample run of the program on sentence (9) to illustrate many central features of the model, including the selection of a codelet; the selection of competing alternatives; the interaction between the workspace and the conceptual network; etc. Note that this section would be overwhelmed with details if a step-by-step explanation were given. A detailed trace of the system&apos;s execution on this sentence can be found in Can (1994), and a short description of the program&apos;s behavior can be found in Gan (1993). Here, only selected snapshots are highlighted. Sentence (9) is an example with local, overlap, and combination ambiguities in the 9 &amp;quot;Diverse paths&amp;quot; refers to different ways of analyzing the structure of a sentence. 541 Computational Linguistics Volume 22, Number 4 Table 2 Initial state of the coderack. Codelet Type Urgency (U) Temperature-regulated Urgency Quantity Ut = 100 Ut = 0 word 2 2 16 14 affinity 3 2 81 20 affix 3 2 81 8 fragment *AA ben ren sheng. Without considering the sentential context, these three characters have three possible word boundaries: * A Ii ben ren sheng &apos;CL human gi</context>
</contexts>
<marker>Gan, 1993</marker>
<rawString>Gan, Kok-Wee. 1993. Integrating word boundary identification with sentence understanding. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 301-303. Ohio State University, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kok-Wee Gan</author>
</authors>
<title>Integrating Word Boundary Disambiguation with Sentence Understanding.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Information Systems &amp; Computer Science, National University of Singapore.</institution>
<contexts>
<context position="13283" citStr="Gan 1994" startWordPosition="2130" endWordPosition="2131">hengcla computer STRUC invention implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It zhOngda &apos;profound&apos; are identified. Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai et al. 1992). These approaches make use of co-occurrence frequencies of characters in a large corpus of written texts to achieve word segmentation without getting into deep syntactic and semantic analysis. For example, the relaxation approach uses the usage frequencies of words and the adjacency constraints among words to iteratively derive the most plausible assignment of characters into word classes. First, all possible words in a sentence are identified and assigned initial probabilities based on their usage frequency. These probabilities are updated iteratively </context>
<context position="50304" citStr="Gan 1994" startWordPosition="8171" endWordPosition="8172">istribute STRUC resource all very many In this run, the bisyllabic word Dm icaifa &apos;develop&apos; has been wrongly identified as two monosyllabic words NI kai &apos;open&apos; and4%). fa &apos;distribute&apos;. To determine the proper use of two juxtaposed predicates, such as Du kai &apos;open&apos; and 4;4_, fa &apos;distribute&apos; in this case, requires a careful study of serial verb constructions. It is inevitable that the system would make such a mistake as our linguistic descriptions have not yet covered this phenomenon. In comparison, consider the performance of a strictly statistical approach based on mutual information (Lua and Gan 1994): the latter wrongly identified the word boundaries in 11 out of the 30 ambiguous fragments. For the 6 fragments that appear in globally ambiguous sentences, the mutual information approach gave only one interpretation of the word boundaries. In terms of processing speed, the mutual information approach took an average of 110.4 ms to process one character; our approach took 1.7 s.11 The extra time in our approach is spent in parsing sentences. 7. Conclusion In this paper, we reported on a stochastically emergent model for language processing and described its application to the modeling of con</context>
</contexts>
<marker>Gan, 1994</marker>
<rawString>Gan, Kok-Wee. 1994. Integrating Word Boundary Disambiguation with Sentence Understanding. Ph.D. thesis, Department of Information Systems &amp; Computer Science, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ke-Kang He</author>
<author>Hui Xu</author>
<author>Bo Sun</author>
</authors>
<title>Design principle of expert system for automatic word segmentation in written Chinese (in Chinese).</title>
<date>1991</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>1--14</pages>
<marker>He, Xu, Sun, 1991</marker>
<rawString>He, Ke-Kang, Hui Xu, and Bo Sun. 1991. Design principle of expert system for automatic word segmentation in written Chinese (in Chinese). Journal of Chinese Information Processing, 5(2): 1-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald D Hoffman</author>
<author>Whitman A Richards</author>
</authors>
<title>Parts of recognition.</title>
<date>1984</date>
<journal>Cognition,</journal>
<volume>18</volume>
<pages>65--96</pages>
<contexts>
<context position="46826" citStr="Hoffman and Richards 1984" startWordPosition="7592" endWordPosition="7595">. adj. classifier quantity aspect ttal 0 547 Computational Linguistics Volume 22, Number 4 scribed in Section 2. When the sentential contexts of locally ambiguous fragments (both the overlap and combination type) were varied, our system was able to identify the correct word boundaries. When the system was presented with sentences with global ambiguities, it produced all the plausible alternative word boundaries. However, at any run of such a sentence, only one alternative is generated. The system&apos;s behavior is similar to human performance in the goblet/faces recognition problem in perception (Hoffman and Richards 1984). We cannot see both the goblet and the faces at the same time, but we are able to switch back and forth between these two interpretations. The frequencies of generating all the alternatives vary from one sentence to another. It is important to note that such frequencies are not meant to indicate some kind of &amp;quot;goodness&amp;quot; measure of alternative word boundary interpretations. Neither are they meant to reflect the preferences of a human. They are merely a reflection of the usage frequencies of Chinese characters and words in our dictionary. The system&apos;s ability to generate different word boundarie</context>
</contexts>
<marker>Hoffman, Richards, 1984</marker>
<rawString>Hoffman, Donald D. and Whitman A. Richards. 1984. Parts of recognition. Cognition, 18: 65-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas R Hofstadter</author>
</authors>
<title>The architecture of JUMBO.</title>
<date>1983</date>
<booktitle>In Proceedings of the International Machine Learning Workshop,</booktitle>
<note>edited by Ryszard Michalski.</note>
<contexts>
<context position="17785" citStr="Hofstadter 1983" startWordPosition="2861" endWordPosition="2863">e still errors. In contrast, people are extremely flexible in their perception of word boundaries of ambiguous fragments appearing in different sentential contexts. We believe that the separation of word identification from the task of analysis accounts for the difference in performance. This has motivated us to study how word identification and sentence analysis can be integrated. 7 This result is reported in Can (1994). 536 Gan, Palmer, and Lua A Statistically Emergent Approach 4. The Statistically Emergent Model This model is inspired by the work done in the Fluid Analogies Research Group (Hofstadter 1983; Meredith 1986; Mitchell 1990; French 1992). There are four main components in this model. Namely, (i) the conceptual network, which is a network of nodes and links representing some permanent linguistic concepts; (ii) the workspace, which is the working area in which high-level linguistic structures representing the system&apos;s current understanding of a sentence are built and modified; (iii) the coderack, which is a pool of structure-building agents (codelets) waiting to run; and (iv) the computational temperature, which is an approximate measure of the amount of disorganization in the system&apos;</context>
</contexts>
<marker>Hofstadter, 1983</marker>
<rawString>Hofstadter, Douglas R. 1983. The architecture of JUMBO. In Proceedings of the International Machine Learning Workshop, edited by Ryszard Michalski.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang-Xi Huang</author>
</authors>
<title>A produce-test approach to automatic segmentation of written Chinese (in Chinese).</title>
<date>1989</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>42--48</pages>
<contexts>
<context position="16002" citStr="Huang 1989" startWordPosition="2555" endWordPosition="2556">longer than the lengths of the two monosyllabic words + shi &apos;ten&apos; and 3)- fen &apos;mark&apos;. As a result, this method fails to correctly identify the word boundaries in sentence (6). Within statistical approaches, considering, for example, the mutual information method (Lua and Can 1994), the same fragment is identified as a bisyllabic word in both sentences (3a) and (6)7. By checking the structural relationships among words in a sentence, rule-based approaches aim to overcome limitations faced by pattern-matching and statistical approaches. However, many of the rules in existing rule-based systems (Huang 1989; Yao, Zheng, and Wu 1990; Yeh and Lee 1991; He, Xu, and Sun 1991; Chen and Liu 1992) are either arbitrary and word-specific, or overly general. For example, Rule Given an ambiguous fragment xyz where x, z, xy, and yz are all possible words, if x can be analyzed as a so-called direction word, segment the fragment as x yz, else segment it as xy z (Liang 1990). This syntactic rule works in sentence (7). (7) ft flff ta fü xia shenzi he bend down body &apos;He bends down his body.&apos; The fragment -F4T- xia shen zi in sentence (7) is ambiguous. As -F xia &apos;down&apos; is a direction word, the fragment is segment</context>
</contexts>
<marker>Huang, 1989</marker>
<rawString>Huang, Xiang-Xi. 1989. A produce-test approach to automatic segmentation of written Chinese (in Chinese). Journal of Chinese Information Processing, 3(4): 42-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kirkpatrick</author>
<author>C D Gelatt Jr</author>
<author>M P Vecchi</author>
</authors>
<title>Optimization by simulated annealing.</title>
<date>1983</date>
<journal>Science,</journal>
<volume>220</volume>
<pages>671--680</pages>
<marker>Kirkpatrick, Jr, Vecchi, 1983</marker>
<rawString>Kirkpatrick, S., C. D. Gelatt Jr., and M. P. Vecchi. 1983. Optimization by simulated annealing. Science, 220: 671-680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T B Y Lai</author>
<author>S C Lun</author>
<author>C F Sun</author>
<author>M S Sun</author>
</authors>
<title>A tagging-based first-order Markov model approach to automatic word identification for Chinese sentences.</title>
<date>1992</date>
<booktitle>In Proceedings of the 1992 International Conference on Computer Processing of Chinese &amp; Oriental Languages,</booktitle>
<pages>17--23</pages>
<contexts>
<context position="4064" citStr="Lai et al. 1992" startWordPosition="602" endWordPosition="605">erate independently of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to the construction of words and of relations between w</context>
<context position="13323" citStr="Lai et al. 1992" startWordPosition="2136" endWordPosition="2139">implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It zhOngda &apos;profound&apos; are identified. Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai et al. 1992). These approaches make use of co-occurrence frequencies of characters in a large corpus of written texts to achieve word segmentation without getting into deep syntactic and semantic analysis. For example, the relaxation approach uses the usage frequencies of words and the adjacency constraints among words to iteratively derive the most plausible assignment of characters into word classes. First, all possible words in a sentence are identified and assigned initial probabilities based on their usage frequency. These probabilities are updated iteratively by employing the consistency constraints</context>
</contexts>
<marker>Lai, Lun, Sun, Sun, 1992</marker>
<rawString>Lai, T. B. Y., S. C. Lun, C. F. Sun, and M. S. Sun. 1992. A tagging-based first-order Markov model approach to automatic word identification for Chinese sentences. In Proceedings of the 1992 International Conference on Computer Processing of Chinese &amp; Oriental Languages, pages 17-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles N Li</author>
<author>Sandra A Thompson</author>
</authors>
<title>Mandarin Chinese: A Functional Reference Grammar.</title>
<date>1981</date>
<publisher>University of California Press.</publisher>
<contexts>
<context position="8758" citStr="Li and Thompson 1981" startWordPosition="1393" endWordPosition="1396"> word IfF gongzuo &apos;work&apos;, leaving the first character alone. The sentence containing this fragment allows only one way of segmenting the word boundary, which is shown in (1). The character Ot&apos; yuim combines with the character preceding it, fa zhi, to form the bisyllabic word plat zhiywin &apos;worker&apos;, and the two characters 2 gong and 41F zub form a word. Overlap, Global Ambiguity (2)a. fall W ill &apos;d f4 17 sa women ylw xuesheng huo de you yiyi we want student live CSC&apos; have meaning &apos;We want our students to have a meaningful life.&apos; 1 A free character is one which can occur independently as a word (Li and Thompson 1981). 2 The characters underlined in sentences (1) to (4) are the locations of word boundary ambiguities we would like to focus on. This convention will be used throughout in this paper. 3 See Contemporary Chinese Language Words Segmentation Standard Used for Information Processing, fifth edition, 1988, published in China. 4 CL stands for a CLassifier. 5 STRUC stands for the STRUCture word frg de. 6 CSC stands for the Complex Stative Construction word t4 de. 533 Computational Linguistics Volume 22, Number 4 b. flIfl 5 /a , tiA i 4 A‘ 21t women yao xue shenghuo de you yiyi we want learn life CSC ha</context>
</contexts>
<marker>Li, Thompson, 1981</marker>
<rawString>Li, Charles N. and Sandra A. Thompson. 1981. Mandarin Chinese: A Functional Reference Grammar. University of California Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan-Yuan Liang</author>
</authors>
<title>Automatic word segmentation in written Chinese and an automatic word segmentation system-CDWS (in Chinese).</title>
<date>1983</date>
<booktitle>In Proceedings of the National Chinese Language Processing System.</booktitle>
<contexts>
<context position="3898" citStr="Liang 1983" startWordPosition="570" endWordPosition="571">ottom-up activities. • Computational activities are indirectly guided by a semantic network of linguistic concepts, which ensures that these activities do not operate independently of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs rel</context>
<context position="12434" citStr="Liang 1983" startWordPosition="1996" endWordPosition="1997">n there is no discourse information. 534 Gan, Palmer, and Lua A Statistically Emergent Approach 3. Existing Approaches Traditionally, word identification has been treated as a preprocessing issue, distinct from sentence analysis. We will therefore only discuss current practices in word identification, leaving sentence analysis aside. Several techniques have been used in word identification, ranging from simple pattern matching, to statistical approaches, to rulebased methods. The most popular pattern-matching method is based on the Maximum Matching heuristics, commonly known as the MM method (Liang 1983; Wang, Wang, and Bai 1991). This method scans a sentence from left to right. In each step, the longest matched substring is selected as a word by dictionary look-up. For example, in sentence (5), (5)-3-1-ntil (1&apos;3 jisuanjr de faming YIY1 zhengcla computer STRUC invention implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and I</context>
</contexts>
<marker>Liang, 1983</marker>
<rawString>Liang, Nan-Yuan. 1983. Automatic word segmentation in written Chinese and an automatic word segmentation system-CDWS (in Chinese). In Proceedings of the National Chinese Language Processing System.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan-Yuan Liang</author>
</authors>
<title>The knowledge of Chinese words segmentation (in Chinese).</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>29--33</pages>
<contexts>
<context position="16362" citStr="Liang 1990" startWordPosition="2623" endWordPosition="2624">and (6)7. By checking the structural relationships among words in a sentence, rule-based approaches aim to overcome limitations faced by pattern-matching and statistical approaches. However, many of the rules in existing rule-based systems (Huang 1989; Yao, Zheng, and Wu 1990; Yeh and Lee 1991; He, Xu, and Sun 1991; Chen and Liu 1992) are either arbitrary and word-specific, or overly general. For example, Rule Given an ambiguous fragment xyz where x, z, xy, and yz are all possible words, if x can be analyzed as a so-called direction word, segment the fragment as x yz, else segment it as xy z (Liang 1990). This syntactic rule works in sentence (7). (7) ft flff ta fü xia shenzi he bend down body &apos;He bends down his body.&apos; The fragment -F4T- xia shen zi in sentence (7) is ambiguous. As -F xia &apos;down&apos; is a direction word, the fragment is segmented as -F giT xia shenzi &apos;down body&apos;, which is as desired. Similarly, this rule will segment the fragment 31-131A wai guo ren as 3,1- IA wai guOren &apos;out citizen&apos;, since 3$ wai &apos;out&apos; is also a direction word. Therefore, when this fragment appears in sentence (8a), (8)a. ft 3&apos;FLIA ta shi waiguOren he COPULA foreigner &apos;He is a foreigner.&apos; the word boundaries ide</context>
</contexts>
<marker>Liang, 1990</marker>
<rawString>Liang, Nan-Yuan. 1990. The knowledge of Chinese words segmentation (in Chinese). Journal of Chinese Information Processing, 4(2): 29-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim-Teng Lua</author>
<author>Kok-Wee Gan</author>
</authors>
<title>An application of information theory in Chinese word segmentation.</title>
<date>1994</date>
<journal>Computer Processing of Chinese &amp; Oriental Languages,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>115--123</pages>
<contexts>
<context position="13283" citStr="Lua and Gan 1994" startWordPosition="2128" endWordPosition="2131">g YIY1 zhengcla computer STRUC invention implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It zhOngda &apos;profound&apos; are identified. Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai et al. 1992). These approaches make use of co-occurrence frequencies of characters in a large corpus of written texts to achieve word segmentation without getting into deep syntactic and semantic analysis. For example, the relaxation approach uses the usage frequencies of words and the adjacency constraints among words to iteratively derive the most plausible assignment of characters into word classes. First, all possible words in a sentence are identified and assigned initial probabilities based on their usage frequency. These probabilities are updated iteratively </context>
<context position="50304" citStr="Lua and Gan 1994" startWordPosition="8169" endWordPosition="8172">o open distribute STRUC resource all very many In this run, the bisyllabic word Dm icaifa &apos;develop&apos; has been wrongly identified as two monosyllabic words NI kai &apos;open&apos; and4%). fa &apos;distribute&apos;. To determine the proper use of two juxtaposed predicates, such as Du kai &apos;open&apos; and 4;4_, fa &apos;distribute&apos; in this case, requires a careful study of serial verb constructions. It is inevitable that the system would make such a mistake as our linguistic descriptions have not yet covered this phenomenon. In comparison, consider the performance of a strictly statistical approach based on mutual information (Lua and Gan 1994): the latter wrongly identified the word boundaries in 11 out of the 30 ambiguous fragments. For the 6 fragments that appear in globally ambiguous sentences, the mutual information approach gave only one interpretation of the word boundaries. In terms of processing speed, the mutual information approach took an average of 110.4 ms to process one character; our approach took 1.7 s.11 The extra time in our approach is spent in parsing sentences. 7. Conclusion In this paper, we reported on a stochastically emergent model for language processing and described its application to the modeling of con</context>
</contexts>
<marker>Lua, Gan, 1994</marker>
<rawString>Lua, Kim-Teng and Kok-Wee Gan. 1994. An application of information theory in Chinese word segmentation. Computer Processing of Chinese &amp; Oriental Languages, 8(1): 115-123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A MelZuk</author>
</authors>
<title>Dependency Syntax: Theory And Practice.</title>
<date>1988</date>
<publisher>State University Press of</publisher>
<location>New York.</location>
<marker>MelZuk, 1988</marker>
<rawString>MelZuk, Igor A. 1988. Dependency Syntax: Theory And Practice. State University Press of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marsha J Meredith</author>
</authors>
<title>Seek-Whence: A model of pattern perception.</title>
<date>1986</date>
<tech>Technical Report 214,</tech>
<institution>Computer Science Department, Indiana University,</institution>
<location>Bloomington, IN.</location>
<contexts>
<context position="17800" citStr="Meredith 1986" startWordPosition="2864" endWordPosition="2865">n contrast, people are extremely flexible in their perception of word boundaries of ambiguous fragments appearing in different sentential contexts. We believe that the separation of word identification from the task of analysis accounts for the difference in performance. This has motivated us to study how word identification and sentence analysis can be integrated. 7 This result is reported in Can (1994). 536 Gan, Palmer, and Lua A Statistically Emergent Approach 4. The Statistically Emergent Model This model is inspired by the work done in the Fluid Analogies Research Group (Hofstadter 1983; Meredith 1986; Mitchell 1990; French 1992). There are four main components in this model. Namely, (i) the conceptual network, which is a network of nodes and links representing some permanent linguistic concepts; (ii) the workspace, which is the working area in which high-level linguistic structures representing the system&apos;s current understanding of a sentence are built and modified; (iii) the coderack, which is a pool of structure-building agents (codelets) waiting to run; and (iv) the computational temperature, which is an approximate measure of the amount of disorganization in the system&apos;s understanding</context>
</contexts>
<marker>Meredith, 1986</marker>
<rawString>Meredith, Marsha J. 1986. Seek-Whence: A model of pattern perception. Technical Report 214, Computer Science Department, Indiana University, Bloomington, IN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Mitchell</author>
</authors>
<title>Copycat: A Computer Model of High-Level Perception and Conceptual Slippage in Analogy-Making.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Michigan.</institution>
<contexts>
<context position="17815" citStr="Mitchell 1990" startWordPosition="2866" endWordPosition="2867">ple are extremely flexible in their perception of word boundaries of ambiguous fragments appearing in different sentential contexts. We believe that the separation of word identification from the task of analysis accounts for the difference in performance. This has motivated us to study how word identification and sentence analysis can be integrated. 7 This result is reported in Can (1994). 536 Gan, Palmer, and Lua A Statistically Emergent Approach 4. The Statistically Emergent Model This model is inspired by the work done in the Fluid Analogies Research Group (Hofstadter 1983; Meredith 1986; Mitchell 1990; French 1992). There are four main components in this model. Namely, (i) the conceptual network, which is a network of nodes and links representing some permanent linguistic concepts; (ii) the workspace, which is the working area in which high-level linguistic structures representing the system&apos;s current understanding of a sentence are built and modified; (iii) the coderack, which is a pool of structure-building agents (codelets) waiting to run; and (iv) the computational temperature, which is an approximate measure of the amount of disorganization in the system&apos;s understanding of a sentence.</context>
</contexts>
<marker>Mitchell, 1990</marker>
<rawString>Mitchell, Melanie. 1990. Copycat: A Computer Model of High-Level Perception and Conceptual Slippage in Analogy-Making. Ph.D. thesis, University of Michigan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alain Polguere</author>
</authors>
<title>To appear. Meaning-text semantic networks as a formal language. In Current Issues In Meaning-Text Linguistics, edited by Leo Wanner.</title>
<marker>Polguere, </marker>
<rawString>Polguere, Alain. To appear. Meaning-text semantic networks as a formal language. In Current Issues In Meaning-Text Linguistics, edited by Leo Wanner.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Palmer Gan</author>
</authors>
<title>and Lua A Statistically Emergent Approach</title>
<marker>Gan, </marker>
<rawString>Gan, Palmer, and Lua A Statistically Emergent Approach</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
<author>Robin Haigh</author>
<author>Eric Atwell</author>
</authors>
<title>Natural language analysis by stochastic optimization: A progress report on project APRIL.</title>
<date>1989</date>
<journal>Journal of Experimental and Theoretical Artificial Intelligence,</journal>
<volume>1</volume>
<issue>4</issue>
<pages>271--287</pages>
<marker>Sampson, Haigh, Atwell, 1989</marker>
<rawString>Sampson, Geoffrey, Robin Haigh, and Eric Atwell. 1989. Natural language analysis by stochastic optimization: A progress report on project APRIL. Journal of Experimental and Theoretical Artificial Intelligence, 1(4): 271-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
</authors>
<title>A statistical method for finding word boundaries</title>
<date>1990</date>
<booktitle>in Chinese text. Computer Processing of Chinese &amp; Oriental Languages,</booktitle>
<volume>4</volume>
<issue>4</issue>
<pages>336--351</pages>
<contexts>
<context position="4013" citStr="Sproat and Shih 1990" startWordPosition="590" endWordPosition="593"> concepts, which ensures that these activities do not operate independently of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to t</context>
<context position="13248" citStr="Sproat and Shih 1990" startWordPosition="2120" endWordPosition="2123">), (5)-3-1-ntil (1&apos;3 jisuanjr de faming YIY1 zhengcla computer STRUC invention implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It zhOngda &apos;profound&apos; are identified. Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai et al. 1992). These approaches make use of co-occurrence frequencies of characters in a large corpus of written texts to achieve word segmentation without getting into deep syntactic and semantic analysis. For example, the relaxation approach uses the usage frequencies of words and the adjacency constraints among words to iteratively derive the most plausible assignment of characters into word classes. First, all possible words in a sentence are identified and assigned initial probabilities based on their usage frequency. These pro</context>
</contexts>
<marker>Sproat, Shih, 1990</marker>
<rawString>Sproat, Richard and Chilin Shih. 1990. A statistical method for finding word boundaries in Chinese text. Computer Processing of Chinese &amp; Oriental Languages, 4(4): 336-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state word-segmentation algorithm for Chinese.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>66--73</pages>
<contexts>
<context position="4084" citStr="Sproat et al. 1994" startWordPosition="606" endWordPosition="609">ly of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to the construction of words and of relations between words. Later, the sys</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1994</marker>
<rawString>Sproat, Richard, Chilin Shih, William Gale, and Nancy Chang. 1994. A stochastic finite-state word-segmentation algorithm for Chinese. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 66-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chain Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-state word-segmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="4105" citStr="Sproat et al. 1996" startWordPosition="610" endWordPosition="613">epresentation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to the construction of words and of relations between words. Later, the system progresses to ide</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Sproat, Richard, Chain Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics, 22(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesniere</author>
</authors>
<title>Ele&apos;ments de la syntaxe structurale.</title>
<date>1959</date>
<location>Klincksieck, Paris.</location>
<contexts>
<context position="23882" citStr="Tesniere 1959" startWordPosition="3840" endWordPosition="3841">t of descriptions not shown in Figure 2. For example, descriptions of character objects include their morphological category (stem/affix) and whether they are bound or unbound.&apos; Descriptions of word objects include their categorial information and sense. Descriptions of chunk objects may also include these two descriptions, except that here, these two descriptions are derived from the category and the sense of the word that is the governor. The directed arc connecting two objects in Figure 2 denotes a linguistic relation between the objects connected. We adopt the dependency grammar notation (Tesniere 1959; Mead( 1988) in which the object pointed to by an arrow is the dependent while the object where the arrow originates is the governor. The undirected arc connecting the characters hal and 7- zi in Figure 2 represents a statistical relation, and statistical relations are undirected in our representation. An overview of our classification of relations is shown in Figure 3. A list of all types of relations is summarized in Table 1; a detailed exposition can be found in Can (1994). In Figure 2, the connection between the word objects #t ta &apos;she&apos; and *A benren &apos;self&apos; is a reflexive adjective relati</context>
</contexts>
<marker>Tesniere, 1959</marker>
<rawString>Tesniere, Lucien. 1959. Ele&apos;ments de la syntaxe structurale. Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaog-Long Wang</author>
<author>Kai-Zhu Wang</author>
<author>Xiao-Hua Bai</author>
</authors>
<title>Separating syllables and characters into words in natural language understanding (in Chinese).</title>
<date>1991</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>5</volume>
<issue>3</issue>
<pages>48--58</pages>
<marker>Wang, Wang, Bai, 1991</marker>
<rawString>Wang, Xiaog-Long, Kai-Zhu Wang, and Xiao-Hua Bai. 1991. Separating syllables and characters into words in natural language understanding (in Chinese). Journal of Chinese Information Processing, 5(3): 48-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wen Wu</author>
<author>Keh-Yih Su</author>
</authors>
<title>Corpus-based automatic compound extraction with mutual information and relative frequency count.</title>
<date>1993</date>
<booktitle>In Proceedings of R.O.C. Computational Linguistics Conference VI,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="4029" citStr="Wu and Su 1993" startWordPosition="594" endWordPosition="597">es that these activities do not operate independently of the system&apos;s representation of the context of a sentence. • Decision making is stochastic, with the amount of randomness being controlled by a parameter known as the computational temperature. We have applied our model to the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences (Gan 1993). Our approach differs from existing work on Chinese word segmentation (Liang 1983; Wang, Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Can 1994; Lai et al. 1992; Sproat et al. 1994; Sproat et al. 1996) primarily in that our system performs sentence interpretation, in addition to word boundary identification. Our system figures out where the word boundaries of a sentence are by determining how various constituents in a sentence can be meaningfully related. The relations the system builds represent its interpretation of the sentence. In the initial stage of a run, the system constructs relations between characters of a sentence. Through a spreading activation mechanism, the system gradually shifts to the construction </context>
<context position="13264" citStr="Wu and Su 1993" startWordPosition="2124" endWordPosition="2127">isuanjr de faming YIY1 zhengcla computer STRUC invention implication profound &apos;The invention of the computer has profound implications.&apos; the first three characters are identified as the word ESN jisuanfr &apos;computer&apos; because it is the longest matched substring found in a word dictionary. With the same reasoning, the words n de &apos;STRUC&apos;, pining &apos;invention&apos;, TA yiyi &apos;implication&apos;, and It zhOngda &apos;profound&apos; are identified. Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al. 1992), the mutual information approach (Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Lai et al. 1992). These approaches make use of co-occurrence frequencies of characters in a large corpus of written texts to achieve word segmentation without getting into deep syntactic and semantic analysis. For example, the relaxation approach uses the usage frequencies of words and the adjacency constraints among words to iteratively derive the most plausible assignment of characters into word classes. First, all possible words in a sentence are identified and assigned initial probabilities based on their usage frequency. These probabilities are u</context>
</contexts>
<marker>Wu, Su, 1993</marker>
<rawString>Wu, Ming-Wen and Keh-Yih Su. 1993. Corpus-based automatic compound extraction with mutual information and relative frequency count. In Proceedings of R.O.C. Computational Linguistics Conference VI, pages 207-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tian-Shun Gui-Ping Zhang Yao</author>
<author>Ying-Ming Wu</author>
</authors>
<title>A rule-based Chinese automatic segmenting system (in Chinese).</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>37--43</pages>
<marker>Yao, Wu, 1990</marker>
<rawString>Yao, Tian-Shun Gui-Ping Zhang, and Ying-Ming Wu. 1990. A rule-based Chinese automatic segmenting system (in Chinese). Journal of Chinese Information Processing, 4(1): 37-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ching-Long Yeh</author>
<author>Hsi-Jian Lee</author>
</authors>
<title>Rule-based word identification for Mandarin Chinese sentences—A unification approach.</title>
<date>1991</date>
<journal>Computer Processing of Chinese &amp; Oriental Languages,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>97--118</pages>
<contexts>
<context position="16045" citStr="Yeh and Lee 1991" startWordPosition="2562" endWordPosition="2565">nosyllabic words + shi &apos;ten&apos; and 3)- fen &apos;mark&apos;. As a result, this method fails to correctly identify the word boundaries in sentence (6). Within statistical approaches, considering, for example, the mutual information method (Lua and Can 1994), the same fragment is identified as a bisyllabic word in both sentences (3a) and (6)7. By checking the structural relationships among words in a sentence, rule-based approaches aim to overcome limitations faced by pattern-matching and statistical approaches. However, many of the rules in existing rule-based systems (Huang 1989; Yao, Zheng, and Wu 1990; Yeh and Lee 1991; He, Xu, and Sun 1991; Chen and Liu 1992) are either arbitrary and word-specific, or overly general. For example, Rule Given an ambiguous fragment xyz where x, z, xy, and yz are all possible words, if x can be analyzed as a so-called direction word, segment the fragment as x yz, else segment it as xy z (Liang 1990). This syntactic rule works in sentence (7). (7) ft flff ta fü xia shenzi he bend down body &apos;He bends down his body.&apos; The fragment -F4T- xia shen zi in sentence (7) is ambiguous. As -F xia &apos;down&apos; is a direction word, the fragment is segmented as -F giT xia shenzi &apos;down body&apos;, which </context>
</contexts>
<marker>Yeh, Lee, 1991</marker>
<rawString>Yeh, Ching-Long and Hsi-Jian Lee. 1991. Rule-based word identification for Mandarin Chinese sentences—A unification approach. Computer Processing of Chinese &amp; Oriental Languages, 5(2): 97-118.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>