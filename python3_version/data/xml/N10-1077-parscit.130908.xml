<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.986654">
Urdu Word Segmentation
</title>
<author confidence="0.991105">
Nadir Durrani Sarmad Hussain
</author>
<affiliation confidence="0.9807275">
Institute for NLP Center for Research in Urdu Language Processing
Universität Stuttgart National University of Computer and Emerging Sciences
</affiliation>
<email confidence="0.954797">
durrani@ims.uni-stuttgart.de sarmad.hussain@nu.edu.pk
</email>
<sectionHeader confidence="0.99368" genericHeader="abstract">
Abstract
</sectionHeader>
<construct confidence="0.6437946">
Word Segmentation is the foremost obligatory task in
almost all the NLP applications where the initial phase
requires tokenization of input into words. Urdu is
amongst the Asian languages that face word segmenta-
tion challenge. However, unlike other Asian languages,
word segmentation in Urdu not only has space omission
errors but also space insertion errors. This paper dis-
cusses how orthographic and linguistic features in Urdu
trigger these two problems. It also discusses the work
that has been done to tokenize input text. We employ a
hybrid solution that performs an n-gram ranking on top
of rule based maximum matching heuristic. Our best
technique gives an error detection of 85.8% and over-
all accuracy of 95.8%. Further issues and possible fu-
ture directions are also discussed.
</construct>
<sectionHeader confidence="0.999206" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938941176471">
All language processing applications require input
text to be tokenized into words for further
processing. Languages like English normally use
white spaces or punctuation marks to identify word
boundaries, though with some complications, e.g.
the word “e.g.” uses a period in between and thus
the period does not indicate a word boundary.
However, many Asian languages like Thai, Khmer,
Lao and Dzongkha do not have word boundaries
and thus do not use white space to consistently
mark word endings. This makes the process of
tokenization of input into words for such languages
very challenging.
Urdu is spoken by more than 100 million people,
mostly in Pakistan and India1. It is an Indo-Aryan
language, written using Arabic script from right to
left, and Nastalique writing style (Hussain, 2003).
</bodyText>
<footnote confidence="0.518508">
1 Ethnologue.com
http://www.ethnologue.com/14/show language.asp?code=UR
D
</footnote>
<bodyText confidence="0.9997783125">
Nastalique is a cursive writing system, which also
does not have a concept of space. Thus, though
space is used in typing the language, it serves other
purposes, as discussed later in this paper. This en-
tails that space cannot be used as a reliable delimi-
ter for words. Therefore, Urdu shares the word
segmentation challenge for language processing,
like other Asian languages.
This paper explains the problem of word segmen-
tation in Urdu. It gives details of work done to
investigate linguistic typology of words and moti-
vation of using space in Urdu. The paper then
presents an algorithm developed to automatically
process the input to produce consistent word seg-
mentation, and finally discusses the results and
future directions.
</bodyText>
<sectionHeader confidence="0.971357" genericHeader="introduction">
2 Urdu Writing System
</sectionHeader>
<bodyText confidence="0.99986215">
Urdu is written in cursive Arabic script. Characters
in general join with the neighbors within a word
and in doing so acquire different shapes. Logically,
a character can acquire up to four shapes, i.e. ini-
tial, medial, final position in a connected sequence
or an isolated form. The characters having this
four-way shaping are known as joiners. However,
another set of characters only join with characters
before them but do not join with character after
them, and are termed as non-joiners. The non-
joiners only have final and isolated forms. For
example Arabic Letter Farsi Yeh s is a joiner and
has four shapes�, �, s and s respectively and
Arabic letter Dal � is a non-joiner and has two
forms � and a only. The shape that these characters
acquire depends upon the context.
Table 1 lists the orthographic rules that Urdu cha-
racters follow. For example, the table shows that in
the middle of a word, if the character is a non-
joiner, it acquires final shape when following a
</bodyText>
<page confidence="0.959587">
528
</page>
<note confidence="0.758032">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 528–536,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.974144111111111">
Word J-Shape Example NJ-Shape Example
Start I 1./o Is 0L1~
Middle M after J 2ro.; F after J J-:.!
I after NJ Lo, Is after J ��L;
End F after J 5.6 F after J _L:3
Is after NJ 715 Is after NJ NJ
J = Joiners, NJ = Non-Joiners
I = Initial, Is = Isolated, M = Medial, F = Final
Underlined = Shape in Consideration
</figure>
<bodyText confidence="0.75803675">
joiner and isolated shape when following a non-
joiner. This joining behavior results in formation
of multiple connected portions within a word, each
called a ligature.
</bodyText>
<tableCaption confidence="0.990986">
Table 1: Orthographic Rules for Urdu
</tableCaption>
<bodyText confidence="0.999719071428571">
The concept of space as a word boundary marker is
not present in Urdu writing. As an Urdu learner, a
person is not taught to leave a space between
words, but only to generate correct shaping while
writing. Thus, the concept of space is only learnt
later on when the person learns how to use a com-
puter. However, space is introduced as a tool to
control the correct letter shaping and not to consis-
tently separate words. For example, the native
speaker learns to insert a space within the word
_,:o &amp;quot;,9,.b (“needy”) to generate the correct shape
of &amp;quot;. Without space it appears as A1oj,9,.b which
is visually incorrect. On contrary, the user finds it
unnecessary to insert a space between the two
words ;5jo9�,1 (“Urdu Center”), because the cor-
rect shaping is produced automatically as the first
word ends with a non-joiner. Therefore ��������
and ;S,o 9�,1 look identical.
Though space character is not present in Urdu,
with increasing usage of computer it is now being
used, both to generate correct shaping (as dis-
cussed above) and also to separate words (a habit
being carried over to Urdu from English literate
computer users). This makes space an unreliable
cue for word boundary. The problem is further ob-
fuscated by the lack of a clear definition of a work
in Urdu in some contexts. The next section dis-
cusses these issues.
</bodyText>
<sectionHeader confidence="0.911016" genericHeader="method">
3 Segmentation Issues in Urdu Text
</sectionHeader>
<bodyText confidence="0.998616666666667">
The segmentation challenges can be divided into
two categories, challenges caused due to joiner and
non-joiner characters.
</bodyText>
<subsectionHeader confidence="0.999529">
3.1 Space Omission
</subsectionHeader>
<bodyText confidence="0.999934909090909">
As discussed, for words ending with non-joiners
correct shaping is generated even when space is
not typed and thus, many times a user omits the
space. Though there is no visible implication,
from the perspective of computational processing
not typing a space merges current word with the
next word. Figure 1 below illustrates an example,
where the phrase has eight words (or tokens) each
ending with a non-joiner and thus the whole
phrase can be written without a space and is still
visibly same and equally readable.
</bodyText>
<figure confidence="0.60747">
��� �� ���� ���-a&gt;I��.o��Lsl9
��������������������������
</figure>
<figureCaption confidence="0.980796666666667">
Figure 1: All Words Ending with Non-Joiners (a)
with Spaces, (b) without Spaces between Words
(“Troop Leader Ahmed Sher Dogar Said”)
</figureCaption>
<bodyText confidence="0.999890916666666">
Another frequent set of space omissions are caused
due to variation in the definition of a word in Urdu.
There are certain function words in Urdu which
may be combined with other function words and
content words by some writers but may be written
separately by others. Shape variation may also
occur in some of these cases, but is overlooked by
the writers. Table 2 gives some examples of such
cases. Though the merged form is not considered
correct diction, it is still frequently used and thus
has to be handled. It is not considered spelling
error but a writing variation.
</bodyText>
<table confidence="0.999621125">
POS Combined Separated Translation
Pn+CM L !&amp;quot; L5 #&amp;quot; Yours
D+ NN $s%&amp;l $s9 &apos;J at that time
CM+ NN (,)S (,* s.5 Towards
V+TA 1+~5 15S ,,5 will do
CM + P ����5 I�- ___ 5 For
Pn = Pronoun, D = Demonstrative, NN = Noun, CM
= Case Marker, V=Verb, P = Particle
</table>
<tableCaption confidence="0.993383">
Table 2: Multiple Words Written in Connected
</tableCaption>
<subsectionHeader confidence="0.973291">
Form Causing Shaping Changes
</subsectionHeader>
<bodyText confidence="0.999635">
Due to reasonable frequency of such cases, these
may be considered as acceptable alternatives, and
thus Urdu word segmentation system would need
to deal with both forms and consider them equiva-
lent. This process is productively applicable and
</bodyText>
<page confidence="0.997213">
529
</page>
<bodyText confidence="0.999875384615384">
not limited to a few pre-determined cases. Addi-
tional complication in the process arises from the
fact that in some cases (last two cases in Table 2)
the spellings also change when two words are writ-
ten in combined form, due to the way these charac-
ters are encoded. Urdu considers Ls and , both
logically same characters at a certain level, though
with different shapes to indicated different vowels
(Hussain, 2004). In combined form they render the
same shape. However, Unicode terms , as a non-
joiner with no medial shape. Thus, the Urdu writ-
ers use Ls to generate the medial position of , in
combined form.
</bodyText>
<subsectionHeader confidence="0.999418">
3.2 Space Insertion
</subsectionHeader>
<bodyText confidence="0.990455590909091">
When multiple morphemes are juxtaposed within a
word, many of them tend to retain their shaping as
separate ligatures. If ending characters are joiners,
space is usually inserted by writers to prevent them
from joining and thus to retain the separate ligature
identity. This causes an extra space within a word.
Though this creates the visually acceptable form, it
creates two tokens from a single word in the con-
text of its processing. If the writers do not type a
space between these two morphemes within a word
they would join and create a visually incorrect
shape. Such examples are common in Urdu2. Few
of these cases are given in Table 3.
Class A B Translation
i 21-ai s&amp;quot; 218u&amp;quot; Married
ii py93 7%o LS94�%4 Candle
iii 2l%:o 2l%; 2l%:o&lt;l%; Unnecessarily
iv =%~ ~4.&gt; =%?~~~&gt; Telephone
v s&apos;N @�I &apos;N. sAB�L�! PhD
i= Affixation, ii = Compounding ,
iii = Reduplication, iv = Foreign Word,
v = Abbreviations
</bodyText>
<tableCaption confidence="0.802838666666667">
Table 3: (A) Separated Form (Correct Shaping, but
Two Tokens), (B) Combined Form (Erroneous
Shaping, with one Token)
</tableCaption>
<bodyText confidence="0.997098714285714">
As categorized in Table 3, the space insertion
problem is caused due to multiple reasons. Data
analyzed shows that space is inserted (i) to keep
affixes separate from the stem, (ii) in some cases,
2 Though Unicode recommends using Zero Width Non-Joiner
character in these context, this is not generally known by Urdu
typists and thus not practiced; Further, this character is not
available on most Urdu keyboards.
to keep two words compounded together from vi-
sually merging, (iii) to keep reduplicated words
from combining, (iv) to enhance readability of
some foreign words written in Urdu, and (v) to
keep English letters separate and readable when
English abbreviations are transliterated.
</bodyText>
<subsectionHeader confidence="0.999979">
3.3 Extent of Segmentation Issues in Urdu
</subsectionHeader>
<bodyText confidence="0.999972583333333">
In an earlier work on Urdu spell checking (Naseem
and Hussain, 2007) report that a significant number
of spelling errors3 are due to irregular use of space,
as discussed above. The study does a spelling
check of an Urdu corpus. The errors reported by
the spelling checker are manually analyzed. A to-
tal of 975 errors are found and of which 736 errors
were due to irregular use of space (75.5%) and 239
errors are non-space-related errors (24.5%). Of the
space related errors, majority of errors (672 or 70%
of total errors) are due to space omission and 53
errors (5%) were due to space insertion. Thus irre-
gular use of space causes an extremely high per-
centage of all errors and has to be addressed for all
language processing applications for Urdu.
A study of Urdu words was also conducted as part
of the current work. Text was used from popular
Urdu online news sites (www.bbc.co.uk/urdu and
http://search.jang.com.pk/). A data of 5,000 words
from both corpora was observed and space inser-
tion and omission cases were counted. These
counts are given in Table 4. Counts for Space In-
sertion are sub-divided into the four categories
identified earlier.
</bodyText>
<table confidence="0.99973625">
Problem BBC Jang Total
Space Omission 373 563 936
Space Insertion
Affixation 298 467 765
Reduplication 52 76 128
Compounding 133 218 351
Abbreviation 263 199 462
Total 1119 1523 2642
</table>
<tableCaption confidence="0.886548">
Table 4: Space Omission and Insertion Counts
from Online BBC and Jang Urdu News Websites
</tableCaption>
<bodyText confidence="0.9971425">
The data shows that a significantly high percentage
of errors related to space, with significant errors
</bodyText>
<sectionHeader confidence="0.292773" genericHeader="method">
3 Errors based on tokenization on space and punctuation mark-
ers
</sectionHeader>
<page confidence="0.981845">
530
</page>
<bodyText confidence="0.999738111111111">
related to both omission and insertion. Within in-
sertion errors, affixation, compounding and ab-
breviation related errors are more significant
(because reduplication is a less frequent phenome-
non).
In summary, the space related errors are significant
and must be addressed as a precursor to any signif-
icant language and speech processing of the lan-
guage
</bodyText>
<subsectionHeader confidence="0.993332">
3.4 Ambiguity in Defining Urdu Word
</subsectionHeader>
<bodyText confidence="0.983392210526316">
Another confounding factor in this context it that
there is no clear agreement on word boundaries of
Urdu in some cases.
Compound words are hard to categorize as single
or multiple words. Urdu forms compounds in
three ways: (i) by placing two words together, e.g.
# CLo (“parents”, literally “father mother”), (ii)
by putting a combining mark between them4, e.g.
5D6l (“prime minister”), and (iii) by putting
the conjunction 9 between two words, e.g. 9 5D;
ﻂ4.o (“Discipline”).
Similarly certain cases of reduplication are also
considered a single word by a native speaker, e.g.
&gt;9&gt;9 (“fluently”) and rlr (“equal”), while others
are not, e.g. G9/&lt;&amp;quot; G9/&lt;&amp;quot; (“slowly”). There are also
cases which are ambiguous, as there is no agree-
ment even within native speakers.
Moreover, certain function words, normally case
markers, postpositions and auxiliaries, may be
written joined with other words in context or sepa-
rately. The words like �.- 5 may also be written
in joined form �.l�S, and the different forms may
be perceived as multiple or single words respec-
tively.
This is demonstrated by the results of a study done
with 30 native speakers of Urdu (including univer-
sity students, language researchers and language
teachers). The subjects were asked to mark wheth-
er they considered some text a single word or a
sequence of two words. Some relevant results are
given in Table 5. The table indicates that for the
types of phenomena in Table 4, the native speakers
4 The diacritics (called zer-e-izafat or hamza-e-izafat) are op-
tional, and are not written in the example given.
do not always agree on the word boundary, that
certain cases are very ambiguous, and that writing
with or without space also changes the perception
of where the word boundary should lie.
</bodyText>
<table confidence="0.999821142857143">
Word(s) # of Words Category
1 2
$ I0,,yE9 24 6 Compounding with
conjunctive diacritic
=L9/5L H$.o% s 17 13 -do-
06 H�J%.0 28 2 -do-
0LI�,%.o 28 2 -do-
=619 Jol 25 5 Compounding with
conjunctive character 9
L-9 %8~ 29 1 -do-
���� 30 0 Suffixation
~~K6
G8~! 5L~1 22 8 -do-
G+1G+1 3 27 Reduplication
M�L&amp; M�L., 3 27 -do-
~~%&lt; 15 15 Space omission between
two auxiliaries
~+~L~1 18 12 Space omission between
verb and auxiliary
LS :=LL1 5 25 Same as above but
without space omission
</table>
<tableCaption confidence="0.999736">
Table 5: Survey on Word Definition
</tableCaption>
<bodyText confidence="0.999982181818182">
As the word boundary is ambiguously perceived, it
is not always clear when to mark it. To develop a
more consistent solution, the current work tags the
different levels of boundaries, and it is left up to
the application provider using the output to decide
which tags to translate to word level boundaries.
Free morphemes are placed and identified at first
level. At second level we identify strings that are
clearly lexicalized as a single word. Compounds,
reduplication and abbreviations are dealt at third
level.
</bodyText>
<sectionHeader confidence="0.969037" genericHeader="method">
4 Summary of Existing Techniques
</sectionHeader>
<bodyText confidence="0.999736363636364">
Rule based techniques have been extensively used
for word segmentation. Techniques including
longest matching (Poowarawan, 1986; Rarunrom,
1991) try to match longest possible dictionary
look-up. If a match is found at nth letter next look-
up is performed starting from n+1 index. Longest
matching with word binding force is used for Chi-
nese word segmentation (Wong and Chang, 1997).
However, the problem with this technique is that it
consistently segments a letter sequence the same
way, and does not take the context into account.
</bodyText>
<page confidence="0.989407">
531
</page>
<bodyText confidence="0.999958">
Thus, shorter word sequences are never generated,
even where they are intended.
Maximum matching is another rule based tech-
nique that was proposed to solve the shortcomings
of longest matching. It generates all possible seg-
mentations out of a given sequence of characters
using dynamic programming. It then selects the
best segmentation based on some heuristics. Most
popularly used heuristic selects the segmentation
with minimum number of words. This heuristic
fails when alternatives have same number of
words. Some additional heuristics are then often
applied, including longest match (Sornlertlamva-
nich, 1995). Many variants of maximum matching
have been applied (Liang, 1986; Li et al., 1991; Gu
and Mao, 1994; Nie et al., 1994).
There is a third category of rule based techniques,
which also use additional linguistic information for
generating intermediate solutions which are then
eventually mapped onto words. For example, rule
based techniques have also been applied to lan-
guages like Thai and Lao to determine syllables,
before syllables are eventually mapped onto words,
e.g. see (Phissamy et al., 2007).
There has been an increasing application of statis-
tical methods, including n-grams, to solve word
segmentation. These techniques are based at let-
ters, syllables and words, and use contextual in-
formation to resolve segmentation ambiguities, e.g.
(Aroonmanakul, 2002; Krawtrakul et al., 1997).
The limitation of statistical methods is that they
only use immediate context and long distance de-
pendencies cannot be directly handled. Also the
performance is based on training corpus. Neverthe-
less, statistical methods are considered to be very
effective to solve segmentation ambiguities.
Finally, another class of segmentation techniques
applies several types of features, e.g. Winnow and
RIPPER algorithms (Meknavin et al., 1997; Blum
1997). The idea is to learn several sources of fea-
tures that characterize the context in which each
word tends to occur. Then these features are com-
bined to remove the segmentation ambiguities
(Charoenpornsawat and Kijsirikul 1998).
</bodyText>
<sectionHeader confidence="0.992974" genericHeader="method">
5 Segmentation Model for Urdu
</sectionHeader>
<bodyText confidence="0.999968930232558">
Although many other languages share the same
problem of word boundary identification for lan-
guage processing, Urdu problem is unique due to
its cursive script and its irregular use of space to
create proper shaping. Though other languages
only have space omission challenge, Urdu has both
omission and insertion problems further confound-
ing the issue.
We employ a combination of techniques to inves-
tigate an effective algorithm to achieve Urdu seg-
mentation. These techniques are incorporated
based on knowledge of Urdu linguistic and writing
system specific information for effective segmen-
tation. For space omission problem a rule based
maximum matching technique is used to generate
all the possible segmentations. The resulting possi-
bilities are ranked using three different heuristics,
namely min-word, unigram and bigram techniques.
For space insertion, we first sub-classify the prob-
lem based on linguistic information, and then use
different techniques for the different cases. Space
insertion between affixes is done by extracting all
possible affixes from Urdu corpus. Some affixes in
Urdu are also free morphemes so it is important to
identify in segmentation process whether or not
they are part of preceding or following word. For
example SSL� is also a free morpheme (“nose”) and
a suffix that makes adjective from noun, e.g. in
word �SL� ,x:�, (“dangerous”). This is done based
on the part of speech information of the words in
the context.
Reduplication is handled using edit distance algo-
rithm. In Urdu the reduplicated morpheme is either
the same or a single edit-distance from the base
morpheme, e.g. ,9,9 has same string repeated, ,�l,�
has one insertion, and �.SLP has one substitu-
tion. Thus, if a string is less than two edits from its
neighbor it is an instance of reduplication5. As the
examples suggest, the reduplication may not only
be limited to word initial position and may also
occur word medially. However, if the length of
base word is less than four, it is further to avoid
function words (case markers, postpositions, aux-
</bodyText>
<note confidence="0.592501">
5 Insertion, deletion and substitution are all considered contri-
buting a single edit distance here.
</note>
<page confidence="0.995919">
532
</page>
<bodyText confidence="0.999842222222222">
iliaries, etc.) from being mistakenly identified as a
case of reduplication, e.g. LS LS (“was done”) has
two words with a single edit distance but is not a
reduplicated sequence.
Urdu does not abbreviate strings, but abbreviations
from English are frequently transliterated into Ur-
du. This sequence can be effectively recognized by
developing a simple finite automaton. The automa-
ton treats marks all such co-occurring morphemes
because they are likely to be an English abbrevia-
tion transliterated into Urdu, e.g. �&apos;s @�1 ,5y,
(“PhD”). If such morphemes are preceding proper
names then these are not combined as they are
more likely to be the initials of an abbreviated
name, e.g. ,5L.0 Ls&apos; J, I (“N. D. Shakir”). This ap-
proach confuses the morpheme _5 (genitive case
marker) of Urdu with the transliteration of English
letter “k”. If we write ~P3 5 Ls&apos; @~I ~! (“after
PhD”), it is interpreted as “P H D K after”. This
has to be explicitly handled.
As classification of compounds into one or two
word sequences is unclear, unambiguous cases are
explicitly handled via lexical look-up. An initial
lexicon of 1850 compound words has been devel-
oped for the system based on a corpus of Urdu.
Common foreign words are also included in this
list.
</bodyText>
<subsectionHeader confidence="0.961311">
5.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.967364029411765">
The segmentation process starts with pre-
processing, which involves removing diacritics (as
they are optionally used in Urdu and not consi-
dered in the current algorithm because they are
frequently incorrectly marked by users6) and nor-
malizing the input text to remove encoding ambi-
guities7. Input is then tokenized based on space
and punctuation characters in the input stream. As
has been discussed, space does not necessarily in-
dicate word boundary. However presence of space
does imply word or morpheme boundary in many
6 The word Q��6� is written with the super-script Alef placed
on Lam and Yay characters. The latter variation is correct but
the former incorrect variation is also common in the corpus.
7 Unicode provides multiple codes for a few letters, and both
composed and decomposed forms for others. These have to be
mapped onto same underlying encoding sequence for further
processing. See
http://www.crulp.org/software/langproc/urdunormalization.ht
m for details.
cases, which can still be useful. The tokenization
process gives what we call an Orthographic Word
(OW). OW is used instead of “word” because one
OW may eventually give multiple words and mul-
tiple OWs may combine to give a single word.
Keeping space related information also keeps the
extent of problem to be solved within a reasonable
computational complexity. For example input
string 5Jj_-, =6 (the name of the first author)
with spaces giving three OWs, creates 2 x 1 x 7 =
14 possible segmentations when sent separately to
the maximum matching module (space omission
error removal - see Figure 2). However, if we re-
move the spaces from the input and send input as a
single OW 5Jj_ij6,�L to maximum matching
process, we get 77 possible segmentations. This
number grows exponentially with the length of
input sentence. Throwing away space character
means we are losing important information so we
keep that intact to our use.
After pre-processing a series of modules further
process the input string and convert the OWs into a
sequence of words. This is summarized in Figure
2 and explained below.
Each OW is sent to a module which deals with
space omission errors. This module extracts all
possible morpheme segmentations out of an OW.
Ten best segmentations of these are selected based
on minimum-word heuristic. This heuristic prefers
segmentations with minimum number of mor-
phemes. Such a heuristic is important to prevent
the search space to explode. We observed that us-
ing 10-best segmentations proved to be sufficient
in most cases as OW normally encapsulates two or
three Urdu words but as a heuristic we also added a
feature which increases this number of 10-best
segmentations to 15, 20, 25-best and so on depend-
ing upon number of characters in an OW. Ten best
segmentations for each OW are merged with the
extracted segmentations of other OWs. Up till here
we have successfully resolved all space omission
errors and the input sentence has been segmented
into morphemes. The 10n (where ‘n’ is No. of
OWs) segmentations are then passed on to space
insertion error removal module. This module has
several sub-modules that handle different linguistic
phenomena like reduplication, affixation, abbrevia-
tions and compounding.
</bodyText>
<page confidence="0.996731">
533
</page>
<bodyText confidence="0.99861">
The reduplication identification module employs
single edit distance algorithm to see if adjacent
morphemes are at single edit-distance of each oth-
er. If the edit distance is less than two, then the
reduplication is identified and marked.
</bodyText>
<figureCaption confidence="0.972958">
Figure 2: Urdu Word Segmentation Process
</figureCaption>
<bodyText confidence="0.986769166666666">
For example the module will correctly recognize
consecutively occurring OWs R%M3 and R�M3 as a
case of reduplication. Reduplication is also ap-
plied earlier in space omission error module as
there may also be a case of reduplication within a
single OW. This module handles such cases, by
dividing words in halves and identifying possible
reduplications. Thus, if the words are written
without space, e.g. R M3R%M3 (innocent) they are
still identified and tagged as reduplicated words
R%M3 and R�M3.
This list of words is then fed into an automaton
which recognizes the sequence of abbreviations
generated by transliterating English letters.
A complete affix list is compiled, and in the next
stage the short listed word sequences are processed
through a process which looks through this list to
determine if any of the OWs may be combined.
Part of speech information of stem is also used to
confirm if OWs can be merged.
Urdu compounds are finally identified. This is
done by using a compound list generated through
the corpus. As compounding is arbitrary, where
speakers are not certain in many cases that a se-
quence of morphemes form a single compound or
not, the segmentation process leaves this level to
the discretion of the user. Whichever compounds
are listed in a compound lexicon are treated as a
single compound word. Those not listed are not
tagged as compounds. User may enhance this list
arbitrarily. The lexicon is initialized with a list of
non-controversial compound, as verified from pub-
lished dictionaries.
Eventually, all the segmentations are re-ranked.
We used three different re-ranking methods name-
ly minimum-word heuristic, unigram and bi-gram
based sequence probabilities, comparative analysis.
Based on the segmentation process, the output se-
quence contains the following tagging. As dis-
cussed earlier, the word segmentation may be
defined based on this tagging by the individual
application using this process.
</bodyText>
<table confidence="0.992554631578947">
Phenomenon Tags Examples
Word &lt;W&gt;&lt;/W&gt; &lt;W&gt;=S61&lt;/W&gt;
Root &lt;R&gt;&lt;/R&gt; &lt;W&gt;&lt;R&gt;&amp;quot;.,9rc&lt;/R&gt;
&lt;S&gt;_U*&lt;/S&gt;&lt;/W&gt;
Suffix &lt;S&gt;&lt;/S&gt; &lt;W&gt;&lt;R&gt;&amp;quot;&gt;._&gt;&lt;/R&gt;
&lt;S&gt;��+~~&lt;/S&gt;&lt;/W&gt;
Prefix &lt;P&gt;&lt;/P&gt; &lt;W&gt;&lt;P&gt;~3&lt;/P&gt;
&lt;R&gt;,y4�T�,&lt;/R&gt;&lt;/W&gt;
XY Com- &lt;C1&gt;&lt;/C1&gt; &lt;C1&gt;&lt;W&gt;UL8)1&lt;/W&gt;
pounds &lt;W&gt;V&lt;/W&gt;&lt;/C1&gt;
X-e-Y Com- &lt;C2&gt;&lt;/C2&gt; &lt;C2&gt;&lt;W&gt;yE9&lt;/W&gt;
pounds &lt;W&gt;,5,k1&lt;/W&gt;&lt;/C2&gt;
X-o-Y Com- &lt;C3&gt;&lt;/C3&gt; &lt;C3&gt;&lt;W&gt;~}5&lt;/W&gt;
pounds &lt;W&gt;9&lt;/W&gt;
&lt;W&gt;W1%j&lt;/W&gt;&lt;/C3&gt;
Reduplication &lt;Rd&gt;&lt;/Rd&gt; &lt;Rd&gt;&lt;W&gt;O�M&gt;&lt;/W&gt;
&lt;W&gt;NLM&gt;&lt;/W&gt;&lt;/Rd&gt;
Abbreviations &lt;A&gt;&lt;/A&gt; &lt;A&gt;&lt;W&gt;,sy&lt;/W&gt;
&lt;W&gt;,p._w&lt;/W&gt; &lt;/A&gt;
</table>
<figureCaption confidence="0.978206">
Figure 3: Urdu Word Segmentation Tag Set
</figureCaption>
<bodyText confidence="0.999301">
A regular word is tagged using &lt;w&gt; ...&lt;/w&gt; pair.
Roots, suffixes and prefixes are also tagged within
a word. Reduplication, compounding and abbrevia-
tions are all considered to be multi-word tags and
relevant words are grouped within these tags.
Three different kind of compounding is separately
tagged.
</bodyText>
<figure confidence="0.913115833333333">
Diacritic Removal / Tokenization
Space Omission Error Removal
Check for Reduplication within an OW
Lexical Look-ups for Spelling Variations
Maximum Matching Module
Ranking-based on Min-Word Heuristic
Space Insertion Error Removal
Reduplication Handling
English Abbreviation Handling
Affixation Handling
Compound Word Tagging
N-Gram Based Ranking
</figure>
<page confidence="0.99506">
534
</page>
<sectionHeader confidence="0.999906" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.996977555555555">
The algorithm was tested on a very small, manual-
ly segmented corpus of 2367 words. The corpus
we selected contained 404 segmentation errors
with 221 cases of space omissions and 183 cases of
space insertions. In space insertion category there
were 66 cases of affixation, 63 cases of compound-
ing, 32 cases of reduplication and 22 cases of ab-
breviations. The results for all three techniques are
shown below:
</bodyText>
<table confidence="0.987496166666667">
Categories Errors %ages
Affixation 59/66 89.39
Reduplication 27/32 84.37
Abbreviations 19/22 86.36
Compounds 28/63 44.44
Total 133/183 72.67
</table>
<tableCaption confidence="0.855471">
Table 6: Percentages of Number of Errors Detected
in Different Categories of Space Insertion Error
</tableCaption>
<bodyText confidence="0.998877363636363">
There were 221 cases of space omission errors
where multiple words were written in a continuum.
Given below is a table that shows how many of
these were correctly identified by each of the used
techniques. Clearly, statistical techniques outper-
form a simple minimum number of words heuris-
tic. Bigrams are likely to produce better results if
the training corpus is improved. Our training cor-
pus contained manually segmented 70K words.
The bigram probabilities are obtained using
SRILM-Toolkit (Stolcke, 2002).
</bodyText>
<table confidence="0.982545">
Categories Errors %ages
Maximum Matching 186/221 84.16
Unigram 214/221 96.83
Bigram 209/221 94.5
</table>
<tableCaption confidence="0.745601">
Table 7: %age of No. of Errors Detected in Space
Omission with Different Ranking Techniques
</tableCaption>
<bodyText confidence="0.968289666666667">
Following table gives cumulative results for cor-
rectly identified space omission and insertion er-
rors.
</bodyText>
<table confidence="0.96489675">
Categories Errors %ages
Maximum Matching 323/404 79.95
Unigram 347/404 85.8
Bigram 339/404 83.9
</table>
<tableCaption confidence="0.99949">
Table 8: %age of No. of Errors Detected Cumula-
</tableCaption>
<sectionHeader confidence="0.347796" genericHeader="conclusions">
tively
</sectionHeader>
<bodyText confidence="0.9960028">
Final table counts total number of words (redupli-
cation, compounds and abbreviations cases are in-
clusive) in test corpus and total number of
correctly identified words after running the entire
segmentation process.
</bodyText>
<table confidence="0.98778625">
Categories Detected %ages
Maximum Matching 2209/2367 93.3
Unigram 2269/2367 95.8
Bigram 2266/2367 95.7
</table>
<tableCaption confidence="0.99954">
Table 9: Percentage of Correctly Detected Words
</tableCaption>
<sectionHeader confidence="0.997747" genericHeader="acknowledgments">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999971617647059">
This work presents a preliminary effort on word
segmentation problem in Urdu. It is a multi-
dimensional problem. Each dimension requires a
deeper study and analysis. Each sub-problem has
been touched in this work and a basic solution for
all has been devised. However to improve on re-
sults each of these modules require a separate
analysis and study. Statistics is only used in rank-
ing of segmentations. In future work bi-gram sta-
tistics can be used to merge morphemes. More data
can be tagged to find out joining probabilities for
the affixes that occur as free morpheme. Such
analysis will reveal whether an affix is more in-
clined towards joining or occurs freely more fre-
quently. Similarly a corpus can be tagged on
compounds. For each morpheme its probability to
occur in compound can be calculated. If two or
more morphemes with higher compounding proba-
bilities co-occur they can be joined together. Simi-
larly corpus can be tagged for abbreviations.
Ranking of segmentations and affix merging can
be improved if POS tags are also involved with
bigram probabilities. Use of POS tags with n-gram
technique is proven to be very helpful in solving
unknown word problems. Our model does not ex-
plicitly handle unknown words. Currently the max-
imum matching module splits an unknown OW
into smaller Urdu morphemes. For example
(% :./.-%9(Kolesnikov) is erroneously split into
(X%�XJ�&amp;X�-%�. More serious problems occur in
cases when OW is a mixture of known and un-
known words. For example in case _&lt;L6%S,y,9
(“Fraser must go”). All these are to be addressed in
future work.
</bodyText>
<page confidence="0.997557">
535
</page>
<sectionHeader confidence="0.998343" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928555555556">
Andreas, S. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language
Processing, Denver, Colorado.
Aroonmanakul, W. 2002. Collocation and Thai
Word Segmentation. In proceeding of SNLPOrien-
tal COCOSDA.
Blum, A. 1997. Empirical Support for Winnow and
Weighted-Majority Algorithm: Results on a Ca-
lendar Scheduling Domain, Machine Learning,
26:5-23.
Charoenpornsawat, P., Kijsirikul, B. 1998. Fea-
ture-Based Thai Unknown Word Boundary Identi-
fication Using Winnow. In Proceedings of the
1998 IEEE Asia-Pacific Conference on Circuits
and Systems (APCCAS’98).
Gu, P. and Mao, Y. 1994. The adjacent matching
algorithm of Chinese automatic word segmentation
and its implementation in the QHFY Chinese-
English system. In International Conference on
Chinese Computing, Singapore.
Hussain, S. 2003. www. LICT4D . asia / Fonts /
Nafees_Nastalique. In the Proceedings of 12th
AMIC Annual Conference on E-Worlds: Govern-
ments, Business and Civil Society, Asian Media
Information Center, Singapore. Also available at
http://www.crulp.org/Publication/papers/2003/ww
w.LICT4D.asia.pdf.
Hussain, S. 2004. Letter to Sound Rules for Urdu
Text to Speech System. In the Proceedings of
Workshop on Computational Approaches to Arabic
Script-based Languages, COLING 2004, Geneva,
Switzerland, 2004.
Krawtrakul, A., Thumkanon. C., Poovorawan. Y.
and Suktarachan. M. 1997. Automatic Thai Un-
known Word Recognition. In Proceedings of the
natural language Processing Pacific Rim Sympo-
sium.
Li, B.Y., S. Lin, C.F. Sun, and M.S. Sun. 1991. A
maximum-matching word segmentation algorithm
using corpus tags for disambiguation. In
ROCLING IV, pages: 135-146, Taipei. ROCLING
Liang, N. 1986. A written Chinese automatic seg-
mentation system-CDWS. In Journal of Chinese
Information Processing, 1(1):44-52.
Meknavin. S., Charenpornsawat. P. and Kijsirikul.
B. 1997. Feature-based Thai Words Segmentation.
NLPRS, Incorporating SNLP.
Naseem, T., Hussain, S. 2007. Spelling Error
Trends in Urdu. In the Proceedings of Conference
on Language Technology (CLT07), University of
Peshawar, Pakistan.
Nie, J., Jin W., and Hannan, M. 1994. A hybrid
approach to unknown word detection and segmen-
tation of Chinese. In International Conference on
Chinese Computing, Singapore.
Phissamay, P., Dalolay,V., Chanhsililath, C., Sili-
masak, O. Hussain, S., and Durrani, N. 2007. Syl-
labification of Lao Script for Line Breaking. In
PAN Localization Working Papers 2004-2007. .
Poowarawan, Y., 1986. Dictionary-based Thai Syl-
lable Separation. In Proceedings of the Ninth Elec-
tronics Engineering Conference
Rarunrom, S., 1991. Dictionary-based Thai Word
Separation. Senior Project Report.
Sornlertlamvanich, V. 1995. Word Segmentation
for Thai in a Machine Translation System (in
Thai), Papers on Natural Language Processing,
NECTEC, Thailand
Wong, P., Chan, C. 1996. Chinese Word Segmen-
tation based on Maximum Matching and Word
Binding Force. In Proceedings of COLING 96, pp.
200-203.
</reference>
<page confidence="0.998537">
536
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.854085">
<title confidence="0.999815">Urdu Word Segmentation</title>
<author confidence="0.963242">Nadir Durrani Sarmad Hussain</author>
<affiliation confidence="0.9139385">Institute for NLP Center for Research in Urdu Language Processing Universität Stuttgart National University of Computer and Emerging Sciences</affiliation>
<abstract confidence="0.9985748125">Word Segmentation is the foremost obligatory task in almost all the NLP applications where the initial phase requires tokenization of input into words. Urdu is amongst the Asian languages that face word segmentation challenge. However, unlike other Asian languages, word segmentation in Urdu not only has space omission errors but also space insertion errors. This paper discusses how orthographic and linguistic features in Urdu trigger these two problems. It also discusses the work that has been done to tokenize input text. We employ a hybrid solution that performs an n-gram ranking on top of rule based maximum matching heuristic. Our best technique gives an error detection of 85.8% and overall accuracy of 95.8%. Further issues and possible future directions are also discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Andreas</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<marker>Andreas, 2002</marker>
<rawString>Andreas, S. 2002. SRILM - an extensible language modeling toolkit. In Intl. Conf. Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Aroonmanakul</author>
</authors>
<title>Collocation and Thai Word Segmentation. In proceeding of SNLPOriental COCOSDA.</title>
<date>2002</date>
<contexts>
<context position="16843" citStr="Aroonmanakul, 2002" startWordPosition="2799" endWordPosition="2800"> rule based techniques, which also use additional linguistic information for generating intermediate solutions which are then eventually mapped onto words. For example, rule based techniques have also been applied to languages like Thai and Lao to determine syllables, before syllables are eventually mapped onto words, e.g. see (Phissamy et al., 2007). There has been an increasing application of statistical methods, including n-grams, to solve word segmentation. These techniques are based at letters, syllables and words, and use contextual information to resolve segmentation ambiguities, e.g. (Aroonmanakul, 2002; Krawtrakul et al., 1997). The limitation of statistical methods is that they only use immediate context and long distance dependencies cannot be directly handled. Also the performance is based on training corpus. Nevertheless, statistical methods are considered to be very effective to solve segmentation ambiguities. Finally, another class of segmentation techniques applies several types of features, e.g. Winnow and RIPPER algorithms (Meknavin et al., 1997; Blum 1997). The idea is to learn several sources of features that characterize the context in which each word tends to occur. Then these </context>
</contexts>
<marker>Aroonmanakul, 2002</marker>
<rawString>Aroonmanakul, W. 2002. Collocation and Thai Word Segmentation. In proceeding of SNLPOriental COCOSDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
</authors>
<title>Empirical Support for Winnow and Weighted-Majority Algorithm: Results on a Calendar Scheduling Domain,</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>26--5</pages>
<contexts>
<context position="17316" citStr="Blum 1997" startWordPosition="2868" endWordPosition="2869">s are based at letters, syllables and words, and use contextual information to resolve segmentation ambiguities, e.g. (Aroonmanakul, 2002; Krawtrakul et al., 1997). The limitation of statistical methods is that they only use immediate context and long distance dependencies cannot be directly handled. Also the performance is based on training corpus. Nevertheless, statistical methods are considered to be very effective to solve segmentation ambiguities. Finally, another class of segmentation techniques applies several types of features, e.g. Winnow and RIPPER algorithms (Meknavin et al., 1997; Blum 1997). The idea is to learn several sources of features that characterize the context in which each word tends to occur. Then these features are combined to remove the segmentation ambiguities (Charoenpornsawat and Kijsirikul 1998). 5 Segmentation Model for Urdu Although many other languages share the same problem of word boundary identification for language processing, Urdu problem is unique due to its cursive script and its irregular use of space to create proper shaping. Though other languages only have space omission challenge, Urdu has both omission and insertion problems further confounding t</context>
</contexts>
<marker>Blum, 1997</marker>
<rawString>Blum, A. 1997. Empirical Support for Winnow and Weighted-Majority Algorithm: Results on a Calendar Scheduling Domain, Machine Learning, 26:5-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Charoenpornsawat</author>
<author>B Kijsirikul</author>
</authors>
<title>Feature-Based Thai Unknown Word Boundary Identification Using Winnow.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 IEEE Asia-Pacific Conference on Circuits and Systems (APCCAS’98).</booktitle>
<contexts>
<context position="17542" citStr="Charoenpornsawat and Kijsirikul 1998" startWordPosition="2902" endWordPosition="2905">ods is that they only use immediate context and long distance dependencies cannot be directly handled. Also the performance is based on training corpus. Nevertheless, statistical methods are considered to be very effective to solve segmentation ambiguities. Finally, another class of segmentation techniques applies several types of features, e.g. Winnow and RIPPER algorithms (Meknavin et al., 1997; Blum 1997). The idea is to learn several sources of features that characterize the context in which each word tends to occur. Then these features are combined to remove the segmentation ambiguities (Charoenpornsawat and Kijsirikul 1998). 5 Segmentation Model for Urdu Although many other languages share the same problem of word boundary identification for language processing, Urdu problem is unique due to its cursive script and its irregular use of space to create proper shaping. Though other languages only have space omission challenge, Urdu has both omission and insertion problems further confounding the issue. We employ a combination of techniques to investigate an effective algorithm to achieve Urdu segmentation. These techniques are incorporated based on knowledge of Urdu linguistic and writing system specific informatio</context>
</contexts>
<marker>Charoenpornsawat, Kijsirikul, 1998</marker>
<rawString>Charoenpornsawat, P., Kijsirikul, B. 1998. Feature-Based Thai Unknown Word Boundary Identification Using Winnow. In Proceedings of the 1998 IEEE Asia-Pacific Conference on Circuits and Systems (APCCAS’98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gu</author>
<author>Y Mao</author>
</authors>
<title>The adjacent matching algorithm of Chinese automatic word segmentation and its implementation in the QHFY ChineseEnglish system.</title>
<date>1994</date>
<booktitle>In International Conference on Chinese Computing,</booktitle>
<contexts>
<context position="16176" citStr="Gu and Mao, 1994" startWordPosition="2697" endWordPosition="2700"> another rule based technique that was proposed to solve the shortcomings of longest matching. It generates all possible segmentations out of a given sequence of characters using dynamic programming. It then selects the best segmentation based on some heuristics. Most popularly used heuristic selects the segmentation with minimum number of words. This heuristic fails when alternatives have same number of words. Some additional heuristics are then often applied, including longest match (Sornlertlamvanich, 1995). Many variants of maximum matching have been applied (Liang, 1986; Li et al., 1991; Gu and Mao, 1994; Nie et al., 1994). There is a third category of rule based techniques, which also use additional linguistic information for generating intermediate solutions which are then eventually mapped onto words. For example, rule based techniques have also been applied to languages like Thai and Lao to determine syllables, before syllables are eventually mapped onto words, e.g. see (Phissamy et al., 2007). There has been an increasing application of statistical methods, including n-grams, to solve word segmentation. These techniques are based at letters, syllables and words, and use contextual inform</context>
</contexts>
<marker>Gu, Mao, 1994</marker>
<rawString>Gu, P. and Mao, Y. 1994. The adjacent matching algorithm of Chinese automatic word segmentation and its implementation in the QHFY ChineseEnglish system. In International Conference on Chinese Computing, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hussain</author>
</authors>
<title>www. LICT4D . asia / Fonts / Nafees_Nastalique.</title>
<date>2003</date>
<booktitle>In the Proceedings of 12th AMIC Annual Conference on E-Worlds: Governments, Business and</booktitle>
<institution>Civil Society, Asian Media Information Center, Singapore.</institution>
<note>Also available at http://www.crulp.org/Publication/papers/2003/ww w.LICT4D.asia.pdf.</note>
<contexts>
<context position="1855" citStr="Hussain, 2003" startWordPosition="282" endWordPosition="283">entify word boundaries, though with some complications, e.g. the word “e.g.” uses a period in between and thus the period does not indicate a word boundary. However, many Asian languages like Thai, Khmer, Lao and Dzongkha do not have word boundaries and thus do not use white space to consistently mark word endings. This makes the process of tokenization of input into words for such languages very challenging. Urdu is spoken by more than 100 million people, mostly in Pakistan and India1. It is an Indo-Aryan language, written using Arabic script from right to left, and Nastalique writing style (Hussain, 2003). 1 Ethnologue.com http://www.ethnologue.com/14/show language.asp?code=UR D Nastalique is a cursive writing system, which also does not have a concept of space. Thus, though space is used in typing the language, it serves other purposes, as discussed later in this paper. This entails that space cannot be used as a reliable delimiter for words. Therefore, Urdu shares the word segmentation challenge for language processing, like other Asian languages. This paper explains the problem of word segmentation in Urdu. It gives details of work done to investigate linguistic typology of words and motiva</context>
</contexts>
<marker>Hussain, 2003</marker>
<rawString>Hussain, S. 2003. www. LICT4D . asia / Fonts / Nafees_Nastalique. In the Proceedings of 12th AMIC Annual Conference on E-Worlds: Governments, Business and Civil Society, Asian Media Information Center, Singapore. Also available at http://www.crulp.org/Publication/papers/2003/ww w.LICT4D.asia.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hussain</author>
</authors>
<title>Letter to Sound Rules for Urdu Text to Speech System.</title>
<date>2004</date>
<booktitle>In the Proceedings of Workshop on Computational Approaches to Arabic Script-based Languages, COLING 2004,</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="8159" citStr="Hussain, 2004" startWordPosition="1360" endWordPosition="1361"> may be considered as acceptable alternatives, and thus Urdu word segmentation system would need to deal with both forms and consider them equivalent. This process is productively applicable and 529 not limited to a few pre-determined cases. Additional complication in the process arises from the fact that in some cases (last two cases in Table 2) the spellings also change when two words are written in combined form, due to the way these characters are encoded. Urdu considers Ls and , both logically same characters at a certain level, though with different shapes to indicated different vowels (Hussain, 2004). In combined form they render the same shape. However, Unicode terms , as a nonjoiner with no medial shape. Thus, the Urdu writers use Ls to generate the medial position of , in combined form. 3.2 Space Insertion When multiple morphemes are juxtaposed within a word, many of them tend to retain their shaping as separate ligatures. If ending characters are joiners, space is usually inserted by writers to prevent them from joining and thus to retain the separate ligature identity. This causes an extra space within a word. Though this creates the visually acceptable form, it creates two tokens fr</context>
</contexts>
<marker>Hussain, 2004</marker>
<rawString>Hussain, S. 2004. Letter to Sound Rules for Urdu Text to Speech System. In the Proceedings of Workshop on Computational Approaches to Arabic Script-based Languages, COLING 2004, Geneva, Switzerland, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Poovorawan</author>
<author>M Suktarachan</author>
</authors>
<title>Automatic Thai Unknown Word Recognition.</title>
<date>1997</date>
<booktitle>In Proceedings of the natural language Processing Pacific Rim Symposium.</booktitle>
<marker>Poovorawan, Suktarachan, 1997</marker>
<rawString>Krawtrakul, A., Thumkanon. C., Poovorawan. Y. and Suktarachan. M. 1997. Automatic Thai Unknown Word Recognition. In Proceedings of the natural language Processing Pacific Rim Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Y Li</author>
<author>S Lin</author>
<author>C F Sun</author>
<author>M S Sun</author>
</authors>
<title>A maximum-matching word segmentation algorithm using corpus tags for disambiguation.</title>
<date>1991</date>
<booktitle>In ROCLING IV,</booktitle>
<pages>135--146</pages>
<publisher>ROCLING</publisher>
<location>Taipei.</location>
<contexts>
<context position="16158" citStr="Li et al., 1991" startWordPosition="2693" endWordPosition="2696">ximum matching is another rule based technique that was proposed to solve the shortcomings of longest matching. It generates all possible segmentations out of a given sequence of characters using dynamic programming. It then selects the best segmentation based on some heuristics. Most popularly used heuristic selects the segmentation with minimum number of words. This heuristic fails when alternatives have same number of words. Some additional heuristics are then often applied, including longest match (Sornlertlamvanich, 1995). Many variants of maximum matching have been applied (Liang, 1986; Li et al., 1991; Gu and Mao, 1994; Nie et al., 1994). There is a third category of rule based techniques, which also use additional linguistic information for generating intermediate solutions which are then eventually mapped onto words. For example, rule based techniques have also been applied to languages like Thai and Lao to determine syllables, before syllables are eventually mapped onto words, e.g. see (Phissamy et al., 2007). There has been an increasing application of statistical methods, including n-grams, to solve word segmentation. These techniques are based at letters, syllables and words, and use</context>
</contexts>
<marker>Li, Lin, Sun, Sun, 1991</marker>
<rawString>Li, B.Y., S. Lin, C.F. Sun, and M.S. Sun. 1991. A maximum-matching word segmentation algorithm using corpus tags for disambiguation. In ROCLING IV, pages: 135-146, Taipei. ROCLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Liang</author>
</authors>
<title>A written Chinese automatic segmentation system-CDWS.</title>
<date>1986</date>
<booktitle>In Journal of Chinese Information Processing,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="16141" citStr="Liang, 1986" startWordPosition="2691" endWordPosition="2692"> intended. Maximum matching is another rule based technique that was proposed to solve the shortcomings of longest matching. It generates all possible segmentations out of a given sequence of characters using dynamic programming. It then selects the best segmentation based on some heuristics. Most popularly used heuristic selects the segmentation with minimum number of words. This heuristic fails when alternatives have same number of words. Some additional heuristics are then often applied, including longest match (Sornlertlamvanich, 1995). Many variants of maximum matching have been applied (Liang, 1986; Li et al., 1991; Gu and Mao, 1994; Nie et al., 1994). There is a third category of rule based techniques, which also use additional linguistic information for generating intermediate solutions which are then eventually mapped onto words. For example, rule based techniques have also been applied to languages like Thai and Lao to determine syllables, before syllables are eventually mapped onto words, e.g. see (Phissamy et al., 2007). There has been an increasing application of statistical methods, including n-grams, to solve word segmentation. These techniques are based at letters, syllables a</context>
</contexts>
<marker>Liang, 1986</marker>
<rawString>Liang, N. 1986. A written Chinese automatic segmentation system-CDWS. In Journal of Chinese Information Processing, 1(1):44-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Charenpornsawat</author>
<author>B Kijsirikul</author>
</authors>
<title>Feature-based Thai Words Segmentation. NLPRS, Incorporating SNLP.</title>
<date>1997</date>
<marker>Charenpornsawat, Kijsirikul, 1997</marker>
<rawString>Meknavin. S., Charenpornsawat. P. and Kijsirikul. B. 1997. Feature-based Thai Words Segmentation. NLPRS, Incorporating SNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Naseem</author>
<author>S Hussain</author>
</authors>
<title>Spelling Error Trends in Urdu.</title>
<date>2007</date>
<booktitle>In the Proceedings of Conference on Language Technology (CLT07),</booktitle>
<institution>University of Peshawar, Pakistan.</institution>
<contexts>
<context position="10198" citStr="Naseem and Hussain, 2007" startWordPosition="1703" endWordPosition="1706">(ii) in some cases, 2 Though Unicode recommends using Zero Width Non-Joiner character in these context, this is not generally known by Urdu typists and thus not practiced; Further, this character is not available on most Urdu keyboards. to keep two words compounded together from visually merging, (iii) to keep reduplicated words from combining, (iv) to enhance readability of some foreign words written in Urdu, and (v) to keep English letters separate and readable when English abbreviations are transliterated. 3.3 Extent of Segmentation Issues in Urdu In an earlier work on Urdu spell checking (Naseem and Hussain, 2007) report that a significant number of spelling errors3 are due to irregular use of space, as discussed above. The study does a spelling check of an Urdu corpus. The errors reported by the spelling checker are manually analyzed. A total of 975 errors are found and of which 736 errors were due to irregular use of space (75.5%) and 239 errors are non-space-related errors (24.5%). Of the space related errors, majority of errors (672 or 70% of total errors) are due to space omission and 53 errors (5%) were due to space insertion. Thus irregular use of space causes an extremely high percentage of all</context>
</contexts>
<marker>Naseem, Hussain, 2007</marker>
<rawString>Naseem, T., Hussain, S. 2007. Spelling Error Trends in Urdu. In the Proceedings of Conference on Language Technology (CLT07), University of Peshawar, Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nie</author>
<author>W Jin</author>
<author>M Hannan</author>
</authors>
<title>A hybrid approach to unknown word detection and segmentation of Chinese.</title>
<date>1994</date>
<booktitle>In International Conference on Chinese Computing,</booktitle>
<contexts>
<context position="16195" citStr="Nie et al., 1994" startWordPosition="2701" endWordPosition="2704">d technique that was proposed to solve the shortcomings of longest matching. It generates all possible segmentations out of a given sequence of characters using dynamic programming. It then selects the best segmentation based on some heuristics. Most popularly used heuristic selects the segmentation with minimum number of words. This heuristic fails when alternatives have same number of words. Some additional heuristics are then often applied, including longest match (Sornlertlamvanich, 1995). Many variants of maximum matching have been applied (Liang, 1986; Li et al., 1991; Gu and Mao, 1994; Nie et al., 1994). There is a third category of rule based techniques, which also use additional linguistic information for generating intermediate solutions which are then eventually mapped onto words. For example, rule based techniques have also been applied to languages like Thai and Lao to determine syllables, before syllables are eventually mapped onto words, e.g. see (Phissamy et al., 2007). There has been an increasing application of statistical methods, including n-grams, to solve word segmentation. These techniques are based at letters, syllables and words, and use contextual information to resolve se</context>
</contexts>
<marker>Nie, Jin, Hannan, 1994</marker>
<rawString>Nie, J., Jin W., and Hannan, M. 1994. A hybrid approach to unknown word detection and segmentation of Chinese. In International Conference on Chinese Computing, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Phissamay</author>
<author>V Dalolay</author>
<author>C Chanhsililath</author>
<author>O Hussain Silimasak</author>
<author>S</author>
<author>N Durrani</author>
</authors>
<title>Syllabification of Lao Script for Line Breaking.</title>
<date>2007</date>
<booktitle>In PAN Localization Working Papers</booktitle>
<pages>2004--2007</pages>
<marker>Phissamay, Dalolay, Chanhsililath, Silimasak, S, Durrani, 2007</marker>
<rawString>Phissamay, P., Dalolay,V., Chanhsililath, C., Silimasak, O. Hussain, S., and Durrani, N. 2007. Syllabification of Lao Script for Line Breaking. In PAN Localization Working Papers 2004-2007. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Poowarawan</author>
</authors>
<title>Dictionary-based Thai Syllable Separation.</title>
<date>1986</date>
<booktitle>In Proceedings of the Ninth Electronics Engineering Conference</booktitle>
<contexts>
<context position="15052" citStr="Poowarawan, 1986" startWordPosition="2521" endWordPosition="2522">ays clear when to mark it. To develop a more consistent solution, the current work tags the different levels of boundaries, and it is left up to the application provider using the output to decide which tags to translate to word level boundaries. Free morphemes are placed and identified at first level. At second level we identify strings that are clearly lexicalized as a single word. Compounds, reduplication and abbreviations are dealt at third level. 4 Summary of Existing Techniques Rule based techniques have been extensively used for word segmentation. Techniques including longest matching (Poowarawan, 1986; Rarunrom, 1991) try to match longest possible dictionary look-up. If a match is found at nth letter next lookup is performed starting from n+1 index. Longest matching with word binding force is used for Chinese word segmentation (Wong and Chang, 1997). However, the problem with this technique is that it consistently segments a letter sequence the same way, and does not take the context into account. 531 Thus, shorter word sequences are never generated, even where they are intended. Maximum matching is another rule based technique that was proposed to solve the shortcomings of longest matchin</context>
</contexts>
<marker>Poowarawan, 1986</marker>
<rawString>Poowarawan, Y., 1986. Dictionary-based Thai Syllable Separation. In Proceedings of the Ninth Electronics Engineering Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rarunrom</author>
</authors>
<title>Dictionary-based Thai Word Separation. Senior Project Report.</title>
<date>1991</date>
<contexts>
<context position="15069" citStr="Rarunrom, 1991" startWordPosition="2523" endWordPosition="2524">mark it. To develop a more consistent solution, the current work tags the different levels of boundaries, and it is left up to the application provider using the output to decide which tags to translate to word level boundaries. Free morphemes are placed and identified at first level. At second level we identify strings that are clearly lexicalized as a single word. Compounds, reduplication and abbreviations are dealt at third level. 4 Summary of Existing Techniques Rule based techniques have been extensively used for word segmentation. Techniques including longest matching (Poowarawan, 1986; Rarunrom, 1991) try to match longest possible dictionary look-up. If a match is found at nth letter next lookup is performed starting from n+1 index. Longest matching with word binding force is used for Chinese word segmentation (Wong and Chang, 1997). However, the problem with this technique is that it consistently segments a letter sequence the same way, and does not take the context into account. 531 Thus, shorter word sequences are never generated, even where they are intended. Maximum matching is another rule based technique that was proposed to solve the shortcomings of longest matching. It generates a</context>
</contexts>
<marker>Rarunrom, 1991</marker>
<rawString>Rarunrom, S., 1991. Dictionary-based Thai Word Separation. Senior Project Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sornlertlamvanich</author>
</authors>
<title>Word Segmentation for Thai in a Machine Translation System (in</title>
<date>1995</date>
<booktitle>Thai), Papers on Natural Language Processing,</booktitle>
<location>NECTEC, Thailand</location>
<contexts>
<context position="16075" citStr="Sornlertlamvanich, 1995" startWordPosition="2680" endWordPosition="2682">ount. 531 Thus, shorter word sequences are never generated, even where they are intended. Maximum matching is another rule based technique that was proposed to solve the shortcomings of longest matching. It generates all possible segmentations out of a given sequence of characters using dynamic programming. It then selects the best segmentation based on some heuristics. Most popularly used heuristic selects the segmentation with minimum number of words. This heuristic fails when alternatives have same number of words. Some additional heuristics are then often applied, including longest match (Sornlertlamvanich, 1995). Many variants of maximum matching have been applied (Liang, 1986; Li et al., 1991; Gu and Mao, 1994; Nie et al., 1994). There is a third category of rule based techniques, which also use additional linguistic information for generating intermediate solutions which are then eventually mapped onto words. For example, rule based techniques have also been applied to languages like Thai and Lao to determine syllables, before syllables are eventually mapped onto words, e.g. see (Phissamy et al., 2007). There has been an increasing application of statistical methods, including n-grams, to solve wor</context>
</contexts>
<marker>Sornlertlamvanich, 1995</marker>
<rawString>Sornlertlamvanich, V. 1995. Word Segmentation for Thai in a Machine Translation System (in Thai), Papers on Natural Language Processing, NECTEC, Thailand</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wong</author>
<author>C Chan</author>
</authors>
<title>Chinese Word Segmentation based on Maximum Matching and Word Binding Force.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING 96,</booktitle>
<pages>200--203</pages>
<marker>Wong, Chan, 1996</marker>
<rawString>Wong, P., Chan, C. 1996. Chinese Word Segmentation based on Maximum Matching and Word Binding Force. In Proceedings of COLING 96, pp. 200-203.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>