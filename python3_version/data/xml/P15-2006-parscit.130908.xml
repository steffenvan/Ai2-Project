<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023446">
<title confidence="0.961939">
Deep Markov Neural Network for Sequential Data Classification
</title>
<author confidence="0.999551">
Min Yang&apos; Wenting Tu&apos; Wenpeng Yin&apos; Ziyu Lu&apos;
</author>
<affiliation confidence="0.999409">
&apos;Department of Computer Science, The University of Hong Kong, Hong Kong
</affiliation>
<email confidence="0.980252">
{myang,wttu,zylu}@cs.hku.hk
</email>
<note confidence="0.681649">
&apos;Center for Information and Language Processing, University of Munich, Germany
</note>
<email confidence="0.986234">
wenpeng@cis.lmu.de
</email>
<sectionHeader confidence="0.993483" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876833333333">
We present a general framework for incor-
porating sequential data and arbitrary fea-
tures into language modeling. The general
framework consists of two parts: a hidden
Markov component and a recursive neural
network component. We demonstrate the
effectiveness of our model by applying it
to a specific application: predicting topics
and sentiments in dialogues. Experiments
on real data demonstrate that our method
is substantially more accurate than previ-
ous methods.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975378787879">
Processing sequential data is a significant research
challenge for natural language processing. In
the past decades, numerous studies have been
conducted on modeling sequential data. Hidden
Markov Models (HMMs) and its variants are rep-
resentative statistical models of sequential data for
the purposes of classification, segmentation, and
clustering (Rabiner, 1989). For most aforemen-
tioned methods, only the dependencies between
consecutive hidden states are modeled. In natural
language processing, however, we find there are
dependencies locally and at a distance. Conser-
vatively using the most recent history to perform
prediction yields overfitting to short-term trends
and missing important long-term effects. Thus, it
is crucial to explore in depth to capture long-term
temporal dynamics in language use.
Numerous real world learning problems are
best characterized by interactions between mul-
tiple causes or factors. Taking sentiment analy-
sis for dialogues as an example, the topic of the
document and the author’s identity are both valu-
able for mining user’s opinions in the conversa-
tion. Specifically, each participant in the dialogue
usually has specific sentiment polarities towards
different topics. However, most existing sequen-
tial data modeling methods are not capable of in-
corporating the information from both the topic
and the author’s identity. More generally, there
is no sufficiently flexible sequential model that al-
lows incorporating an arbitrary set of features.
In this paper, we present a Deep Markov Neu-
ral Network (DMNN) for incorporating sequential
data and arbitrary features into language model-
ing. Our method learns from general sequential
observations. It is also capable of taking the or-
dering of words into account, and collecting in-
formation from arbitrary features associated with
the context. Comparing to traditional HMM-based
method, it explores deeply into the structure of
sentences, and is more flexible in taking exter-
nal features into account. On the other hand, it
doesn’t suffer from the training difficulties of re-
current neural networks, such as the vanishing gra-
dient problem.
The general framework consists of two parts:
a hidden Markov component and a neural net-
work component. In the training phase, the hid-
den Markov model is trained on the sequential ob-
servation, resulting in transition probabilities and
hidden states at each time step. Then, the neural
network is trained, taking words, features and hid-
den state at the previous time step as input, to pre-
dict the hidden states at the present time step. The
procedure is reversed in the testing phase: the neu-
ral network predicts the hidden states using words
and features, then the hidden Markov model pre-
dicts the observation using hidden states.
A key insight of our method is to use hid-
den states as an intermediate representation, as
a bridge to connect sentences and observations.
By using hidden states, we can deal with arbi-
trary observation, without worrying about the is-
sue of discretization and normalization. Hidden
states are robust with respect to the random noise
in the observation. Unlike recurrent neural net-
</bodyText>
<page confidence="0.981535">
32
</page>
<bodyText confidence="0.9384497">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 32–37,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
work which connects networks between consecu-
tive time steps, the recursive neural network in our
framework connects to the previous time step by
using its hidden states. In the training phase, since
hidden states are inferred by the hidden Markov
model, the training of recursive neural networks
at each time step can be performed separately,
preventing the difficulty of learning an extremely
deep neural network.
We demonstrate the effectiveness of our model
by applying it to a specific application: predicting
topics and sentiments in dialogues. In this exam-
ple, the sequential observation includes topics and
sentiments. The feature includes the identity of
the author. Experiments on real data demonstrate
that our method is substantially more accurate than
previous methods.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999946228571429">
Modeling sequential data is an active research field
(Lewis and Gale, 1994; Jain et al., 2000; Ra-
biner, 1989; Baldi and Brunak, 2001; Kum et al.,
2005). The paper proposed by Kum et al. (2005)
describes most of the existing techniques for se-
quential data modeling. Hidden Markov Mod-
els (HMMs) is one of the most successful models
for sequential data that is best known for speech
recognition (Rabiner, 1989). Recently, HMMs
have been applied to a variety of applications out-
side of speech recognition, such as handwriting
recognition (Nag et al., 1986; Kundu and Bahl,
1988) and fault-detection (Smyth, 1994). The
variants and extensions of HMMs also include
language models (Guyon and Pereira, 1995) and
econometrics (Garcia and Perron, 1996).
In order to properly capture more complex lin-
guistic phenomena, a variety of neural networks
have been proposed, such as neural probabilistic
language model (Bengio et al., 2006), recurrent
neural network (Mikolov et al., 2010) and recur-
sive neural tensor network (Socher et al., 2013).
As opposed to the work that only focuses on the
context of the sequential data, some studies have
been proposed to incorporate more general fea-
tures associated with the context. Ghahramani and
Jordan (1997) proposes a factorial HMMs method
and it has been successfully utilized in natural lan-
guage processing (Duh, 2005), computer vision
(Wang and Ji, 2005) and speech processing (Gael
et al., 2009). However, exact inference and param-
eter estimation in factorial HMMs is intractable,
thus the learning algorithm is difficult to imple-
ment and is limited to the study of real-valued data
sets.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="method">
3 The DMNN Model
</sectionHeader>
<bodyText confidence="0.999949">
In this section, we describe our general framework
for incorporating sequential data and an arbitrary
set of features into language modeling.
</bodyText>
<subsectionHeader confidence="0.964294">
3.1 Generative model
</subsectionHeader>
<bodyText confidence="0.999690857142857">
Given a time sequence t = 1, 2, 3,. .. , n, we as-
sociate each time slice with an observation (st, ut)
and a state label yt. Here, st represents the sen-
tence at time t, and ut represents additional fea-
tures. Additional features may include the author
of the sentence, the bag-of-word features and other
semantic features. The label yt is the item that we
want to predict. It might be the topic of the sen-
tence, or the sentiment of the author.
Given tuples (st, ut, yt), it is natural to build a
supervised classification model to predict yt. Re-
current neural networks have been shown effective
in modeling temporal NLP data. However, due to
the depth of the time sequence, training a single
RNN is difficult. When the time sequence length
n is large, the RNN model suffers from many prac-
tical problems, including the vanishing gradient is-
sue which makes the training process inefficient.
We propose a Deep Markov Neural Network
(DMNN) model. The DMNN model introduces
a hidden state variable Ht for each time slice. It
serves as an intermediate layer connecting the la-
bel yt and the observation (st, ut). These hidden
variables disentangle the correlation between neu-
ral networks for each sentence, but preserving time
series dependence. The time series dependence is
modeled by a Markov chain. In particular, we as-
sume that there is a labeling matrix L such that
</bodyText>
<equation confidence="0.992708">
P(yt = ijHt = j) = Lij (1)
</equation>
<bodyText confidence="0.945411">
and a transition matrix T such that
</bodyText>
<equation confidence="0.99746">
P(Ht+1 = ijHt = j) = Tij (2)
</equation>
<bodyText confidence="0.962957125">
These two equations establish the relation be-
tween the hidden state and the labels. On the
other hand, we use a neural network model M to
model the relation between the hidden states and
the observations. The neural network model takes
(Ht−1, st, ut) as input, and predict Ht as its out-
put. In particular, we use a logistic model to define
the probability:
</bodyText>
<page confidence="0.768406">
33
</page>
<equation confidence="0.9978485">
P(Ht = i|Ht−1, st, ut) a (3)
exp((wih, φ(Ht−1)) + (wiu, ϕ(ut)) + (wisN(st) + b))
</equation>
<bodyText confidence="0.99948725">
The vectors wh, wu, ws are linear combination
coefficients to be estimated. The functions O, cp
and function N turn Ht−1, ut and st into fea-
turized vectors. Among these functions, we rec-
ommend choosing O(Ht−1) to be a binary vector
whose Ht−1-th coordinate is one and all other co-
ordinates are zeros. Both function cp and function
N are modeled by deep neural networks.
Since the sentence st has varied lengths and
distinct structures, choosing an appropriate neural
network to extract the sentence-level feature is a
challenge task. In this paper, we choose N to be
the recursive autoencoder (Socher et al., 2011a),
which explicitly takes structure of the sentence
into account. The network for defining cp can be
a standard fully connect neural network.
</bodyText>
<subsectionHeader confidence="0.994553">
3.2 Estimating Model Parameters
</subsectionHeader>
<bodyText confidence="0.999901576923077">
There are two sets of parameters to be estimated:
the parameters L, T for the Markov chain model,
and the parameters wh, wu, ws, cp, N for the deep
neural networks. The training is performed in two
phases. In the first phase, the hidden states {Ht}
are estimated based on the labels {yt}. The emis-
sion matrix L and the transition matrix T are es-
timated at the same time. This step can be done
by using the Baum-Welch algorithm (Baum et al.,
1970; Baum, 1972) for learning hidden Markov
models.
When the hidden states {Ht} are obtained, the
second phase estimates the remaining parameters
for the neural network model in a supervised pre-
diction problem. First, we use available sentences
to train the structure of the recursive neural net-
work N. This step can be done without using other
information besides {st}. After the structure of N
is given, the remaining task is to train a supervised
prediction model to predict the hidden state Ht for
each time slice. In this final step, the parameters to
be estimated are wh, wu, wsand the weight coeffi-
cients in neural networks N and cp. By maximiz-
ing the log-likelihood of the prediction, all model
parameters can be estimated by stochastic gradient
descent.
</bodyText>
<subsectionHeader confidence="0.990767">
3.3 Prediction
</subsectionHeader>
<bodyText confidence="0.999689714285714">
The prediction procedure is a reverse of the train-
ing procedure. For prediction, we only have the
sentence st and the additional feature ut. By equa-
tion (3), we use (s1, u1) to predict H1, then use
(H1, s2, u2) to predict H2. This procedure contin-
ues until we have reached Hn. Note that each Ht
is a random variable. Equation (3) yields
</bodyText>
<equation confidence="0.998815">
�P(Ht = i|s, u) = P(Ht = i|st, ut, Ht−1 = j)
7
· P(Ht−1 = j|s, u) (4)
</equation>
<bodyText confidence="0.9996022">
This recursive formula suggests inferring the
probability distribution P(Ht|s, u) one by one,
starting from t = 1 and terminate at t = n. After
P(Ht|s, u) is available, we can infer the probabil-
ity distribution of yt as
</bodyText>
<equation confidence="0.9916194">
�P(yt = i|s, u) = P(yt = i|Ht = j)P(Ht = j|s, u)
7
�
= Li,7P(Ht = j|s, u) (5)
7
</equation>
<bodyText confidence="0.992373">
which gives the prediction for the label of interest.
</bodyText>
<sectionHeader confidence="0.7647625" genericHeader="method">
3.4 Application: Sentiment analysis in
conversation
</sectionHeader>
<bodyText confidence="0.999971566666667">
Sentiment analysis for dialogues is a typical se-
quential data modeling problem.The sentiments
and topics expressed in a conversation affect
the interaction between dialogue participants
(Suin Kim, 2012). For example, given a user say
that “I have had a high fever for 3 days”, the user
may write back positive-sentiment response like “I
hope you feel better soon”, or it could be negative-
sentiment content when the response is “Sorry, but
you cannot join us today” (Hasegawa et al., 2013).
Incorporating the session’s sequential information
into sentiment analysis may improve the predic-
tion accuracy. Meanwhile, each participate in the
dialogue usually has specific sentiment polarities
towards different topics.
In this paper, the sequential labels available to
the framework include topics and sentiments. In
the training dataset, topics are obtained by run-
ning an LDA model, while the sentiment labels are
manually labeled. The feature includes the iden-
tity of the author. In the training phase, the hid-
den Markov model is trained on the sequential la-
bels, resulting in transition probabilities and hid-
den states at each time step. Then, the recursive
autoencoders (Socher et al., 2011a) is trained, tak-
ing words, the identity of the author and hidden
state at the previous time step as input, to predict
the hidden states at the present time step. The pro-
cedure is reversed in the testing phase: the neu-
ral network predicts the hidden states using words
</bodyText>
<page confidence="0.995763">
34
</page>
<bodyText confidence="0.999241666666667">
and the identity of the author, then the hidden
Markov model predicts the observation using hid-
den states.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9993995">
To evaluate our model, we conduct experiments
for sentiment analysis in conversations.
</bodyText>
<subsectionHeader confidence="0.948435">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999864435897436">
We conduct experiments on both English and Chi-
nese datasets. The detailed properties of the
datasets are described as follow.
Twitter conversation (Twitter): The original
dataset is a collection of about 1.3 million conver-
sations drawn from Twitter by Ritter et al. (2010).
Each conversation contains between 2 and 243
posts. In our experiments, we filter the data by
keeping only the conversations of five or more
tweets. This results in 64,068 conversations con-
taining 542,866 tweets.
Sina Weibo conversation (Sina): since there is
no authoritative publicly available Chinese short-
text conversation corpus, we write a web crawler
to grab tweets from Sina Weibo, which is the
most popular Twitter-like microblogging website
in China1. Following the strategy used in (Rit-
ter et al., 2010), we crawled Sina Weibo for a 3
months period from September 2013 to Novem-
ber 2013. Filtering the conversations that contain
less than five posts, we get a Chinese conversa-
tion corpus with 5,921 conversations containing
37,282 tweets.
For both datasets, we set the ground truth of sen-
timent classification of tweets by using human an-
notation. Specifically, we randomly select 1000
conversations from each datasets, and then invite
three researchers who work on natural language
processing to label sentiment tag of each tweet
(i.e., positive, negative or neutral) manually. From
3 responses for each tweet, we measure the agree-
ment as the number of people who submitted the
same response. We measure the performance of
our framework using the tweets that satisfy at least
2 out of 3 agreement.
For both datasets, data preprocessing is per-
formed. The words about time, numeral words,
pronoun and punctuation are removed as they are
unrelated to the sentiment analysis task.
</bodyText>
<footnote confidence="0.995938">
1http://weibo.com
</footnote>
<table confidence="0.99857">
Dataset SVM NBSVM RAE Mesnil’s DMNN
Twitter 0.572 0.624 0.639 0.650 0.682
Sina 0.548 0.612 0.598 0.626 0.652
</table>
<tableCaption confidence="0.998557">
Table 1: Three-way classification accuracy
</tableCaption>
<subsectionHeader confidence="0.621408">
4.2 Baseline methods
</subsectionHeader>
<bodyText confidence="0.999676730769231">
To evaluate the effectiveness of our framework
on the application of sentiment analysis, we com-
pare our approach with several baseline methods,
which we describe below:
SVM: Support Vector Machine is widely-used
baseline method to build sentiment classifiers
(Pang et al., 2002). In our experiment, 5000 words
with greatest information gain are chosen as fea-
tures, and we use the LibLinear2 to implement
SVM.
NBSVM: This is a state-of-the-art performer on
many sentiment classification datasets (Wang and
Manning, 2012). The model is run using the pub-
licly available code3.
RAE: Recursive Autoencoder (Socher et al.,
2011b) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. The RAE model is run using the
publicly available code4 and we follow the same
setting as in (Socher et al., 2011b).
Mesnil’s method: This method is proposed in
(Mesnil et al., 2014), which achieves the strongest
results on movie reviews recently. It is a ensem-
ble of the generative technique and the discrimi-
native technique. We run this algorithm with pub-
licly available code 5.
</bodyText>
<subsectionHeader confidence="0.999704">
4.3 Experiment results
</subsectionHeader>
<bodyText confidence="0.999401083333333">
In our HMMs component, the number of hidden
states is 80. We randomly initialize the matrix
of state transition probabilities and the initial state
distribution between 0 and 1. The emission prob-
abilities are determined by Gaussian distributions.
In our recursive autoencoders component, we rep-
resent each words using 100-dimensional vectors.
The hyperparameter used for weighing reconstruc-
tion and cross-entropy error is 0.1.
For each dataset, we use 800 conversations as
the training data and the remaining are used for
testing. We summarize the experiment results in
</bodyText>
<footnote confidence="0.97516275">
2http://www.csie.ntu.edu.tw/~cjlin/liblinear/
3http://nlp.stanford.edu/~sidaw
4https://github.com/sancha/jrae/zipball/stable
35 5https://github.com/mesnilgr/iclr15.
</footnote>
<tableCaption confidence="0.621284">
Table 1. According to Table 1, the proposed ap-
</tableCaption>
<bodyText confidence="0.996437181818182">
proach significantly and consistently outperforms
other methods on both datasets. This verifies the
effectiveness of the proposed approach. For exam-
ple, the overall accuracy of our algorithm is 3.2%
higher than Mesnil’s method and 11.0% higher
than SVM on Twitter conversations dataset. For
the Sina Weibo dataset, we observe similar results.
The advantage of our model comes from its capa-
bility of exploring sequential information and in-
corporating an arbitrary number of factors of the
corpus.
</bodyText>
<sectionHeader confidence="0.970842" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99993625">
In this paper, we present a general framework
for incorporating sequential data into language
modeling. We demonstrate the effectiveness of
our method by applying it to a specific appli-
cation: predicting topics and sentiments in dia-
logues. Experiments on real data demonstrate that
our method is substantially more accurate than
previous methods.
</bodyText>
<sectionHeader confidence="0.976957" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.823218454545455">
Pierre Baldi and Søren Brunak. 2001. Bioinformatics:
the machine learning approach. MIT press.
Leonard E Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
ring in the statistical analysis of probabilistic func-
tions of markov chains. The annals of mathematical
statistics, pages 164–171.
Leonard E Baum. 1972. An equality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of markov processes. In-
equalities, 3:1–8.
</bodyText>
<reference confidence="0.998153491071429">
Yoshua Bengio, Holger Schwenk, Jean-Sébastien
Senécal, Fréderic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137–186.
Springer.
Kevin Duh. 2005. Jointly labeling multiple sequences:
A factorial hmm approach. In Proceedings of the
ACL Student Research Workshop, pages 19–24. As-
sociation for Computational Linguistics.
Jurgen V Gael, Yee W Teh, and Zoubin Ghahramani.
2009. The infinite factorial hidden markov model.
In Advances in Neural Information Processing Sys-
tems, pages 1697–1704.
René Garcia and Pierre Perron. 1996. An analysis of
the real interest rate under regime shifts. The Review
of Economics and Statistics, pages 111–125.
Zoubin Ghahramani and Michael I Jordan. 1997. Fac-
torial hidden markov models. Machine learning,
29(2-3):245–273.
Isabelle Guyon and Fernando Pereira. 1995. Design
of a linguistic postprocessor using variable memory
length markov models. In Document Analysis and
Recognition, 1995., Proceedings of the Third Inter-
national Conference on, volume 1, pages 454–457.
IEEE.
Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga,
and Masashi Toyoda. 2013. Predicting and eliciting
addressee’s emotion in online dialogue. In ACL (1),
pages 964–972.
Anil K Jain, Robert P. W. Duin, and Jianchang Mao.
2000. Statistical pattern recognition: A review.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 22(1):4–37.
Hye-Chung Monica Kum, Susan Paulsen, and Wei
Wang. 2005. Comparative study of sequential pat-
tern mining models. In Foundations of Data Mining
and knowledge Discovery, pages 43–70. Springer.
Amlan Kundu and Paramrir Bahl. 1988. Recognition
of handwritten script: a hidden markov model based
approach. In Acoustics, Speech, and Signal Process-
ing, 1988. ICASSP-88., 1988 International Confer-
ence on, pages 928–931. IEEE.
David D Lewis and William A Gale. 1994. A se-
quential algorithm for training text classifiers. In
Proceedings of the 17th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 3–12. Springer-Verlag
New York, Inc.
Grégoire Mesnil, Marc’Aurelio Ranzato, Tomas
Mikolov, and Yoshua Bengio. 2014. Ensemble
of generative and discriminative techniques for sen-
timent analysis of movie reviews. arXiv preprint
arXiv:1412.5335.
Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
R Nag, K Wong, and Frank Fallside. 1986. Script
recognition using hidden markov models. In Acous-
tics, Speech, and Signal Processing, IEEE Interna-
tional Conference on ICASSP’86., volume 11, pages
2071–2074. IEEE.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
Lawrence Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
36 tion. Proceedings of the IEEE, 77(2):257–286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.
Padhraic Smyth. 1994. Hidden markov models for
fault detection in dynamic systems. Pattern recog-
nition, 27(1):149–164.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011a.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-Supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP).
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642. Citeseer.
Alice Oh Suin Kim, JinYeong Bak. 2012. Discover-
ing emotion influence patterns in online social net-
work conversations. In SIGWEB ACM Special In-
terest Group on Hypertext, Hypermedia, and Web.
ACM.
Peng Wang and Qiang Ji. 2005. Multi-view face track-
ing with factorial and switching hmm. In Applica-
tion of Computer Vision, 2005. WACV/MOTIONS’05
Volume 1. Seventh IEEE Workshops on, volume 1,
pages 401–406. IEEE.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90–94. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999611">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.906353">
<title confidence="0.996191">Deep Markov Neural Network for Sequential Data Classification</title>
<author confidence="0.955231">of Computer Science</author>
<author confidence="0.955231">The University of Hong Kong</author>
<author confidence="0.955231">Hong</author>
<affiliation confidence="0.980672">for Information and Language Processing, University of Munich,</affiliation>
<email confidence="0.998072">wenpeng@cis.lmu.de</email>
<abstract confidence="0.996933076923077">We present a general framework for incorporating sequential data and arbitrary features into language modeling. The general framework consists of two parts: a hidden Markov component and a recursive neural network component. We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. Experiments on real data demonstrate that our method is substantially more accurate than previous methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-Sébastien Senécal</author>
<author>Fréderic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5927" citStr="Bengio et al., 2006" startWordPosition="903" endWordPosition="906">the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully utilized in natural language processing (Duh, 2005), computer vision (Wang and Ji, 2005) and speech processing (Gael et al., 2009). However, exact inference and parameter estimation in factorial HMMs is intractable, thus t</context>
</contexts>
<marker>Bengio, Schwenk, Senécal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-Sébastien Senécal, Fréderic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Jointly labeling multiple sequences: A factorial hmm approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>pages</pages>
<contexts>
<context position="6356" citStr="Duh, 2005" startWordPosition="974" endWordPosition="975">). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully utilized in natural language processing (Duh, 2005), computer vision (Wang and Ji, 2005) and speech processing (Gael et al., 2009). However, exact inference and parameter estimation in factorial HMMs is intractable, thus the learning algorithm is difficult to implement and is limited to the study of real-valued data sets. 3 The DMNN Model In this section, we describe our general framework for incorporating sequential data and an arbitrary set of features into language modeling. 3.1 Generative model Given a time sequence t = 1, 2, 3,. .. , n, we associate each time slice with an observation (st, ut) and a state label yt. Here, st represents the</context>
</contexts>
<marker>Duh, 2005</marker>
<rawString>Kevin Duh. 2005. Jointly labeling multiple sequences: A factorial hmm approach. In Proceedings of the ACL Student Research Workshop, pages 19–24. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen V Gael</author>
<author>Yee W Teh</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>The infinite factorial hidden markov model.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1697--1704</pages>
<contexts>
<context position="6435" citStr="Gael et al., 2009" startWordPosition="985" endWordPosition="988">riety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully utilized in natural language processing (Duh, 2005), computer vision (Wang and Ji, 2005) and speech processing (Gael et al., 2009). However, exact inference and parameter estimation in factorial HMMs is intractable, thus the learning algorithm is difficult to implement and is limited to the study of real-valued data sets. 3 The DMNN Model In this section, we describe our general framework for incorporating sequential data and an arbitrary set of features into language modeling. 3.1 Generative model Given a time sequence t = 1, 2, 3,. .. , n, we associate each time slice with an observation (st, ut) and a state label yt. Here, st represents the sentence at time t, and ut represents additional features. Additional features</context>
</contexts>
<marker>Gael, Teh, Ghahramani, 2009</marker>
<rawString>Jurgen V Gael, Yee W Teh, and Zoubin Ghahramani. 2009. The infinite factorial hidden markov model. In Advances in Neural Information Processing Systems, pages 1697–1704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>René Garcia</author>
<author>Pierre Perron</author>
</authors>
<title>An analysis of the real interest rate under regime shifts. The Review of Economics and Statistics,</title>
<date>1996</date>
<pages>111--125</pages>
<contexts>
<context position="5747" citStr="Garcia and Perron, 1996" startWordPosition="875" endWordPosition="878"> Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully utilized in natural language processing (D</context>
</contexts>
<marker>Garcia, Perron, 1996</marker>
<rawString>René Garcia and Pierre Perron. 1996. An analysis of the real interest rate under regime shifts. The Review of Economics and Statistics, pages 111–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Factorial hidden markov models.</title>
<date>1997</date>
<booktitle>Machine learning,</booktitle>
<pages>29--2</pages>
<contexts>
<context position="6242" citStr="Ghahramani and Jordan (1997)" startWordPosition="954" endWordPosition="957">The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully utilized in natural language processing (Duh, 2005), computer vision (Wang and Ji, 2005) and speech processing (Gael et al., 2009). However, exact inference and parameter estimation in factorial HMMs is intractable, thus the learning algorithm is difficult to implement and is limited to the study of real-valued data sets. 3 The DMNN Model In this section, we describe our general framework for incorporating sequential data and an arbitrary set of features into language modeling. 3.1 Generative model Given a time sequence t = 1, 2, 3</context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Zoubin Ghahramani and Michael I Jordan. 1997. Factorial hidden markov models. Machine learning, 29(2-3):245–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Fernando Pereira</author>
</authors>
<title>Design of a linguistic postprocessor using variable memory length markov models. In Document Analysis and Recognition,</title>
<date>1995</date>
<booktitle>Proceedings of the Third International Conference on,</booktitle>
<volume>1</volume>
<pages>454--457</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5704" citStr="Guyon and Pereira, 1995" startWordPosition="869" endWordPosition="872">Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully</context>
</contexts>
<marker>Guyon, Pereira, 1995</marker>
<rawString>Isabelle Guyon and Fernando Pereira. 1995. Design of a linguistic postprocessor using variable memory length markov models. In Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on, volume 1, pages 454–457. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takayuki Hasegawa</author>
<author>Nobuhiro Kaji</author>
<author>Naoki Yoshinaga</author>
<author>Masashi Toyoda</author>
</authors>
<title>Predicting and eliciting addressee’s emotion in online dialogue.</title>
<date>2013</date>
<journal>In ACL</journal>
<volume>1</volume>
<pages>964--972</pages>
<contexts>
<context position="12014" citStr="Hasegawa et al., 2013" startWordPosition="1968" endWordPosition="1971"> = Li,7P(Ht = j|s, u) (5) 7 which gives the prediction for the label of interest. 3.4 Application: Sentiment analysis in conversation Sentiment analysis for dialogues is a typical sequential data modeling problem.The sentiments and topics expressed in a conversation affect the interaction between dialogue participants (Suin Kim, 2012). For example, given a user say that “I have had a high fever for 3 days”, the user may write back positive-sentiment response like “I hope you feel better soon”, or it could be negativesentiment content when the response is “Sorry, but you cannot join us today” (Hasegawa et al., 2013). Incorporating the session’s sequential information into sentiment analysis may improve the prediction accuracy. Meanwhile, each participate in the dialogue usually has specific sentiment polarities towards different topics. In this paper, the sequential labels available to the framework include topics and sentiments. In the training dataset, topics are obtained by running an LDA model, while the sentiment labels are manually labeled. The feature includes the identity of the author. In the training phase, the hidden Markov model is trained on the sequential labels, resulting in transition pro</context>
</contexts>
<marker>Hasegawa, Kaji, Yoshinaga, Toyoda, 2013</marker>
<rawString>Takayuki Hasegawa, Nobuhiro Kaji, Naoki Yoshinaga, and Masashi Toyoda. 2013. Predicting and eliciting addressee’s emotion in online dialogue. In ACL (1), pages 964–972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil K Jain</author>
<author>Robert P W Duin</author>
<author>Jianchang Mao</author>
</authors>
<title>Statistical pattern recognition: A review. Pattern Analysis and Machine Intelligence,</title>
<date>2000</date>
<journal>IEEE Transactions on,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5097" citStr="Jain et al., 2000" startWordPosition="770" endWordPosition="773">ursive neural networks at each time step can be performed separately, preventing the difficulty of learning an extremely deep neural network. We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. In this example, the sequential observation includes topics and sentiments. The feature includes the identity of the author. Experiments on real data demonstrate that our method is substantially more accurate than previous methods. 2 Related work Modeling sequential data is an active research field (Lewis and Gale, 1994; Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira</context>
</contexts>
<marker>Jain, Duin, Mao, 2000</marker>
<rawString>Anil K Jain, Robert P. W. Duin, and Jianchang Mao. 2000. Statistical pattern recognition: A review. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(1):4–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hye-Chung Monica Kum</author>
<author>Susan Paulsen</author>
<author>Wei Wang</author>
</authors>
<title>Comparative study of sequential pattern mining models.</title>
<date>2005</date>
<booktitle>In Foundations of Data Mining and knowledge Discovery,</booktitle>
<pages>43--70</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5155" citStr="Kum et al., 2005" startWordPosition="781" endWordPosition="784">separately, preventing the difficulty of learning an extremely deep neural network. We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. In this example, the sequential observation includes topics and sentiments. The feature includes the identity of the author. Experiments on real data demonstrate that our method is substantially more accurate than previous methods. 2 Related work Modeling sequential data is an active research field (Lewis and Gale, 1994; Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In ord</context>
</contexts>
<marker>Kum, Paulsen, Wang, 2005</marker>
<rawString>Hye-Chung Monica Kum, Susan Paulsen, and Wei Wang. 2005. Comparative study of sequential pattern mining models. In Foundations of Data Mining and knowledge Discovery, pages 43–70. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amlan Kundu</author>
<author>Paramrir Bahl</author>
</authors>
<title>Recognition of handwritten script: a hidden markov model based approach.</title>
<date>1988</date>
<booktitle>In Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on,</booktitle>
<pages>928--931</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5578" citStr="Kundu and Bahl, 1988" startWordPosition="851" endWordPosition="854">accurate than previous methods. 2 Related work Modeling sequential data is an active research field (Lewis and Gale, 1994; Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general fe</context>
</contexts>
<marker>Kundu, Bahl, 1988</marker>
<rawString>Amlan Kundu and Paramrir Bahl. 1988. Recognition of handwritten script: a hidden markov model based approach. In Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on, pages 928–931. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>3--12</pages>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc.</location>
<contexts>
<context position="5078" citStr="Lewis and Gale, 1994" startWordPosition="766" endWordPosition="769">l, the training of recursive neural networks at each time step can be performed separately, preventing the difficulty of learning an extremely deep neural network. We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. In this example, the sequential observation includes topics and sentiments. The feature includes the identity of the author. Experiments on real data demonstrate that our method is substantially more accurate than previous methods. 2 Related work Modeling sequential data is an active research field (Lewis and Gale, 1994; Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D Lewis and William A Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 3–12. Springer-Verlag New York, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grégoire Mesnil</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335.</title>
<date>2014</date>
<contexts>
<context position="16070" citStr="Mesnil et al., 2014" startWordPosition="2611" endWordPosition="2614">periment, 5000 words with greatest information gain are chosen as features, and we use the LibLinear2 to implement SVM. NBSVM: This is a state-of-the-art performer on many sentiment classification datasets (Wang and Manning, 2012). The model is run using the publicly available code3. RAE: Recursive Autoencoder (Socher et al., 2011b) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. The RAE model is run using the publicly available code4 and we follow the same setting as in (Socher et al., 2011b). Mesnil’s method: This method is proposed in (Mesnil et al., 2014), which achieves the strongest results on movie reviews recently. It is a ensemble of the generative technique and the discriminative technique. We run this algorithm with publicly available code 5. 4.3 Experiment results In our HMMs component, the number of hidden states is 80. We randomly initialize the matrix of state transition probabilities and the initial state distribution between 0 and 1. The emission probabilities are determined by Gaussian distributions. In our recursive autoencoders component, we represent each words using 100-dimensional vectors. The hyperparameter used for weighin</context>
</contexts>
<marker>Mesnil, Ranzato, Mikolov, Bengio, 2014</marker>
<rawString>Grégoire Mesnil, Marc’Aurelio Ranzato, Tomas Mikolov, and Yoshua Bengio. 2014. Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafiát</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafiát, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nag</author>
<author>K Wong</author>
<author>Frank Fallside</author>
</authors>
<title>Script recognition using hidden markov models.</title>
<date>1986</date>
<booktitle>In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP’86.,</booktitle>
<volume>11</volume>
<pages>2071--2074</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5555" citStr="Nag et al., 1986" startWordPosition="847" endWordPosition="850">ubstantially more accurate than previous methods. 2 Related work Modeling sequential data is an active research field (Lewis and Gale, 1994; Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to inco</context>
</contexts>
<marker>Nag, Wong, Fallside, 1986</marker>
<rawString>R Nag, K Wong, and Frank Fallside. 1986. Script recognition using hidden markov models. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP’86., volume 11, pages 2071–2074. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15439" citStr="Pang et al., 2002" startWordPosition="2510" endWordPosition="2513">cessing is performed. The words about time, numeral words, pronoun and punctuation are removed as they are unrelated to the sentiment analysis task. 1http://weibo.com Dataset SVM NBSVM RAE Mesnil’s DMNN Twitter 0.572 0.624 0.639 0.650 0.682 Sina 0.548 0.612 0.598 0.626 0.652 Table 1: Three-way classification accuracy 4.2 Baseline methods To evaluate the effectiveness of our framework on the application of sentiment analysis, we compare our approach with several baseline methods, which we describe below: SVM: Support Vector Machine is widely-used baseline method to build sentiment classifiers (Pang et al., 2002). In our experiment, 5000 words with greatest information gain are chosen as features, and we use the LibLinear2 to implement SVM. NBSVM: This is a state-of-the-art performer on many sentiment classification datasets (Wang and Manning, 2012). The model is run using the publicly available code3. RAE: Recursive Autoencoder (Socher et al., 2011b) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. The RAE model is run using the publicly available code4 and we follow the same setting as in (Socher et al., 2011b). Mesnil’s method: This method is pr</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recogni36 tion.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="1160" citStr="Rabiner, 1989" startWordPosition="160" endWordPosition="161">strate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. Experiments on real data demonstrate that our method is substantially more accurate than previous methods. 1 Introduction Processing sequential data is a significant research challenge for natural language processing. In the past decades, numerous studies have been conducted on modeling sequential data. Hidden Markov Models (HMMs) and its variants are representative statistical models of sequential data for the purposes of classification, segmentation, and clustering (Rabiner, 1989). For most aforementioned methods, only the dependencies between consecutive hidden states are modeled. In natural language processing, however, we find there are dependencies locally and at a distance. Conservatively using the most recent history to perform prediction yields overfitting to short-term trends and missing important long-term effects. Thus, it is crucial to explore in depth to capture long-term temporal dynamics in language use. Numerous real world learning problems are best characterized by interactions between multiple causes or factors. Taking sentiment analysis for dialogues </context>
<context position="5112" citStr="Rabiner, 1989" startWordPosition="774" endWordPosition="776">rks at each time step can be performed separately, preventing the difficulty of learning an extremely deep neural network. We demonstrate the effectiveness of our model by applying it to a specific application: predicting topics and sentiments in dialogues. In this example, the sequential observation includes topics and sentiments. The feature includes the identity of the author. Experiments on real data demonstrate that our method is substantially more accurate than previous methods. 2 Related work Modeling sequential data is an active research field (Lewis and Gale, 1994; Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and eco</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recogni36 tion. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>Bill Dolan</author>
</authors>
<title>Unsupervised modeling of twitter conversations.</title>
<date>2010</date>
<contexts>
<context position="13481" citStr="Ritter et al. (2010)" startWordPosition="2202" endWordPosition="2205">resent time step. The procedure is reversed in the testing phase: the neural network predicts the hidden states using words 34 and the identity of the author, then the hidden Markov model predicts the observation using hidden states. 4 Experiments To evaluate our model, we conduct experiments for sentiment analysis in conversations. 4.1 Datasets We conduct experiments on both English and Chinese datasets. The detailed properties of the datasets are described as follow. Twitter conversation (Twitter): The original dataset is a collection of about 1.3 million conversations drawn from Twitter by Ritter et al. (2010). Each conversation contains between 2 and 243 posts. In our experiments, we filter the data by keeping only the conversations of five or more tweets. This results in 64,068 conversations containing 542,866 tweets. Sina Weibo conversation (Sina): since there is no authoritative publicly available Chinese shorttext conversation corpus, we write a web crawler to grab tweets from Sina Weibo, which is the most popular Twitter-like microblogging website in China1. Following the strategy used in (Ritter et al., 2010), we crawled Sina Weibo for a 3 months period from September 2013 to November 2013. </context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2010</marker>
<rawString>Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsupervised modeling of twitter conversations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
</authors>
<title>Hidden markov models for fault detection in dynamic systems.</title>
<date>1994</date>
<booktitle>Pattern recognition,</booktitle>
<pages>27--1</pages>
<contexts>
<context position="5612" citStr="Smyth, 1994" startWordPosition="857" endWordPosition="858">ork Modeling sequential data is an active research field (Lewis and Gale, 1994; Jain et al., 2000; Rabiner, 1989; Baldi and Brunak, 2001; Kum et al., 2005). The paper proposed by Kum et al. (2005) describes most of the existing techniques for sequential data modeling. Hidden Markov Models (HMMs) is one of the most successful models for sequential data that is best known for speech recognition (Rabiner, 1989). Recently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context</context>
</contexts>
<marker>Smyth, 1994</marker>
<rawString>Padhraic Smyth. 1994. Hidden markov models for fault detection in dynamic systems. Pattern recognition, 27(1):149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9318" citStr="Socher et al., 2011" startWordPosition="1491" endWordPosition="1494">+ b)) The vectors wh, wu, ws are linear combination coefficients to be estimated. The functions O, cp and function N turn Ht−1, ut and st into featurized vectors. Among these functions, we recommend choosing O(Ht−1) to be a binary vector whose Ht−1-th coordinate is one and all other coordinates are zeros. Both function cp and function N are modeled by deep neural networks. Since the sentence st has varied lengths and distinct structures, choosing an appropriate neural network to extract the sentence-level feature is a challenge task. In this paper, we choose N to be the recursive autoencoder (Socher et al., 2011a), which explicitly takes structure of the sentence into account. The network for defining cp can be a standard fully connect neural network. 3.2 Estimating Model Parameters There are two sets of parameters to be estimated: the parameters L, T for the Markov chain model, and the parameters wh, wu, ws, cp, N for the deep neural networks. The training is performed in two phases. In the first phase, the hidden states {Ht} are estimated based on the labels {yt}. The emission matrix L and the transition matrix T are estimated at the same time. This step can be done by using the Baum-Welch algorith</context>
<context position="12715" citStr="Socher et al., 2011" startWordPosition="2075" endWordPosition="2078">mprove the prediction accuracy. Meanwhile, each participate in the dialogue usually has specific sentiment polarities towards different topics. In this paper, the sequential labels available to the framework include topics and sentiments. In the training dataset, topics are obtained by running an LDA model, while the sentiment labels are manually labeled. The feature includes the identity of the author. In the training phase, the hidden Markov model is trained on the sequential labels, resulting in transition probabilities and hidden states at each time step. Then, the recursive autoencoders (Socher et al., 2011a) is trained, taking words, the identity of the author and hidden state at the previous time step as input, to predict the hidden states at the present time step. The procedure is reversed in the testing phase: the neural network predicts the hidden states using words 34 and the identity of the author, then the hidden Markov model predicts the observation using hidden states. 4 Experiments To evaluate our model, we conduct experiments for sentiment analysis in conversations. 4.1 Datasets We conduct experiments on both English and Chinese datasets. The detailed properties of the datasets are d</context>
<context position="15782" citStr="Socher et al., 2011" startWordPosition="2564" endWordPosition="2567"> evaluate the effectiveness of our framework on the application of sentiment analysis, we compare our approach with several baseline methods, which we describe below: SVM: Support Vector Machine is widely-used baseline method to build sentiment classifiers (Pang et al., 2002). In our experiment, 5000 words with greatest information gain are chosen as features, and we use the LibLinear2 to implement SVM. NBSVM: This is a state-of-the-art performer on many sentiment classification datasets (Wang and Manning, 2012). The model is run using the publicly available code3. RAE: Recursive Autoencoder (Socher et al., 2011b) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. The RAE model is run using the publicly available code4 and we follow the same setting as in (Socher et al., 2011b). Mesnil’s method: This method is proposed in (Mesnil et al., 2014), which achieves the strongest results on movie reviews recently. It is a ensemble of the generative technique and the discriminative technique. We run this algorithm with publicly available code 5. 4.3 Experiment results In our HMMs component, the number of hidden states is 80. We randomly initialize the matri</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011a. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="9318" citStr="Socher et al., 2011" startWordPosition="1491" endWordPosition="1494">+ b)) The vectors wh, wu, ws are linear combination coefficients to be estimated. The functions O, cp and function N turn Ht−1, ut and st into featurized vectors. Among these functions, we recommend choosing O(Ht−1) to be a binary vector whose Ht−1-th coordinate is one and all other coordinates are zeros. Both function cp and function N are modeled by deep neural networks. Since the sentence st has varied lengths and distinct structures, choosing an appropriate neural network to extract the sentence-level feature is a challenge task. In this paper, we choose N to be the recursive autoencoder (Socher et al., 2011a), which explicitly takes structure of the sentence into account. The network for defining cp can be a standard fully connect neural network. 3.2 Estimating Model Parameters There are two sets of parameters to be estimated: the parameters L, T for the Markov chain model, and the parameters wh, wu, ws, cp, N for the deep neural networks. The training is performed in two phases. In the first phase, the hidden states {Ht} are estimated based on the labels {yt}. The emission matrix L and the transition matrix T are estimated at the same time. This step can be done by using the Baum-Welch algorith</context>
<context position="12715" citStr="Socher et al., 2011" startWordPosition="2075" endWordPosition="2078">mprove the prediction accuracy. Meanwhile, each participate in the dialogue usually has specific sentiment polarities towards different topics. In this paper, the sequential labels available to the framework include topics and sentiments. In the training dataset, topics are obtained by running an LDA model, while the sentiment labels are manually labeled. The feature includes the identity of the author. In the training phase, the hidden Markov model is trained on the sequential labels, resulting in transition probabilities and hidden states at each time step. Then, the recursive autoencoders (Socher et al., 2011a) is trained, taking words, the identity of the author and hidden state at the previous time step as input, to predict the hidden states at the present time step. The procedure is reversed in the testing phase: the neural network predicts the hidden states using words 34 and the identity of the author, then the hidden Markov model predicts the observation using hidden states. 4 Experiments To evaluate our model, we conduct experiments for sentiment analysis in conversations. 4.1 Datasets We conduct experiments on both English and Chinese datasets. The detailed properties of the datasets are d</context>
<context position="15782" citStr="Socher et al., 2011" startWordPosition="2564" endWordPosition="2567"> evaluate the effectiveness of our framework on the application of sentiment analysis, we compare our approach with several baseline methods, which we describe below: SVM: Support Vector Machine is widely-used baseline method to build sentiment classifiers (Pang et al., 2002). In our experiment, 5000 words with greatest information gain are chosen as features, and we use the LibLinear2 to implement SVM. NBSVM: This is a state-of-the-art performer on many sentiment classification datasets (Wang and Manning, 2012). The model is run using the publicly available code3. RAE: Recursive Autoencoder (Socher et al., 2011b) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. The RAE model is run using the publicly available code4 and we follow the same setting as in (Socher et al., 2011b). Mesnil’s method: This method is proposed in (Mesnil et al., 2014), which achieves the strongest results on movie reviews recently. It is a ensemble of the generative technique and the discriminative technique. We run this algorithm with publicly available code 5. 4.3 Experiment results In our HMMs component, the number of hidden states is 80. We randomly initialize the matri</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6034" citStr="Socher et al., 2013" startWordPosition="920" endWordPosition="923">ecently, HMMs have been applied to a variety of applications outside of speech recognition, such as handwriting recognition (Nag et al., 1986; Kundu and Bahl, 1988) and fault-detection (Smyth, 1994). The variants and extensions of HMMs also include language models (Guyon and Pereira, 1995) and econometrics (Garcia and Perron, 1996). In order to properly capture more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully utilized in natural language processing (Duh, 2005), computer vision (Wang and Ji, 2005) and speech processing (Gael et al., 2009). However, exact inference and parameter estimation in factorial HMMs is intractable, thus the learning algorithm is difficult to implement and is limited to the study of real-valued data sets. 3 The</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Oh Suin Kim</author>
<author>JinYeong Bak</author>
</authors>
<title>Discovering emotion influence patterns in online social network conversations.</title>
<date>2012</date>
<booktitle>In SIGWEB ACM Special Interest Group on Hypertext, Hypermedia, and Web.</booktitle>
<publisher>ACM.</publisher>
<marker>Kim, Bak, 2012</marker>
<rawString>Alice Oh Suin Kim, JinYeong Bak. 2012. Discovering emotion influence patterns in online social network conversations. In SIGWEB ACM Special Interest Group on Hypertext, Hypermedia, and Web. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Wang</author>
<author>Qiang Ji</author>
</authors>
<title>Multi-view face tracking with factorial and switching hmm.</title>
<date>2005</date>
<booktitle>In Application of Computer Vision,</booktitle>
<volume>1</volume>
<pages>401--406</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6393" citStr="Wang and Ji, 2005" startWordPosition="978" endWordPosition="981">re more complex linguistic phenomena, a variety of neural networks have been proposed, such as neural probabilistic language model (Bengio et al., 2006), recurrent neural network (Mikolov et al., 2010) and recursive neural tensor network (Socher et al., 2013). As opposed to the work that only focuses on the context of the sequential data, some studies have been proposed to incorporate more general features associated with the context. Ghahramani and Jordan (1997) proposes a factorial HMMs method and it has been successfully utilized in natural language processing (Duh, 2005), computer vision (Wang and Ji, 2005) and speech processing (Gael et al., 2009). However, exact inference and parameter estimation in factorial HMMs is intractable, thus the learning algorithm is difficult to implement and is limited to the study of real-valued data sets. 3 The DMNN Model In this section, we describe our general framework for incorporating sequential data and an arbitrary set of features into language modeling. 3.1 Generative model Given a time sequence t = 1, 2, 3,. .. , n, we associate each time slice with an observation (st, ut) and a state label yt. Here, st represents the sentence at time t, and ut represent</context>
</contexts>
<marker>Wang, Ji, 2005</marker>
<rawString>Peng Wang and Qiang Ji. 2005. Multi-view face tracking with factorial and switching hmm. In Application of Computer Vision, 2005. WACV/MOTIONS’05 Volume 1. Seventh IEEE Workshops on, volume 1, pages 401–406. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2,</booktitle>
<pages>90--94</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15680" citStr="Wang and Manning, 2012" startWordPosition="2547" endWordPosition="2550">.682 Sina 0.548 0.612 0.598 0.626 0.652 Table 1: Three-way classification accuracy 4.2 Baseline methods To evaluate the effectiveness of our framework on the application of sentiment analysis, we compare our approach with several baseline methods, which we describe below: SVM: Support Vector Machine is widely-used baseline method to build sentiment classifiers (Pang et al., 2002). In our experiment, 5000 words with greatest information gain are chosen as features, and we use the LibLinear2 to implement SVM. NBSVM: This is a state-of-the-art performer on many sentiment classification datasets (Wang and Manning, 2012). The model is run using the publicly available code3. RAE: Recursive Autoencoder (Socher et al., 2011b) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. The RAE model is run using the publicly available code4 and we follow the same setting as in (Socher et al., 2011b). Mesnil’s method: This method is proposed in (Mesnil et al., 2014), which achieves the strongest results on movie reviews recently. It is a ensemble of the generative technique and the discriminative technique. We run this algorithm with publicly available code 5. 4.3 Experim</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida Wang and Christopher D Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2, pages 90–94. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>