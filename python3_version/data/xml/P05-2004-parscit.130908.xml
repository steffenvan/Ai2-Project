<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044780">
<title confidence="0.973224">
Jointly Labeling Multiple Sequences: A Factorial HMM Approach
</title>
<author confidence="0.999368">
Kevin Duh
</author>
<affiliation confidence="0.999828">
Department of Electrical Engineering
University of Washington, USA
</affiliation>
<email confidence="0.998363">
duh@ee.washington.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999882">
We present new statistical models for
jointly labeling multiple sequences and
apply them to the combined task of part-
of-speech tagging and noun phrase chunk-
ing. The model is based on the Factorial
Hidden Markov Model (FHMM) with dis-
tributed hidden states representing part-
of-speech and noun phrase sequences. We
demonstrate that this joint labeling ap-
proach, by enabling information sharing
between tagging/chunking subtasks, out-
performs the traditional method of tag-
ging and chunking in succession. Fur-
ther, we extend this into a novel model,
Switching FHMM, to allow for explicit
modeling of cross-sequence dependencies
based on linguistic knowledge. We report
tagging/chunking accuracies for varying
dataset sizes and show that our approach
is relatively robust to data sparsity.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912581395349">
Traditionally, various sequence labeling problems in
natural language processing are solved by the cas-
cading of well-defined subtasks, each extracting spe-
cific knowledge. For instance, the problem of in-
formation extraction from sentences may be broken
into several stages: First, part-of-speech (POS) tag-
ging is performed on the sequence of word tokens.
This result is then utilized in noun-phrase and verb-
phrase chunking. Finally, a higher-level analyzer
extracts relevant information based on knowledge
gleaned in previous subtasks.
The decomposition of problems into well-defined
subtasks is useful but sometimes leads to unneces-
sary errors. The problem is that errors in earlier
subtasks will propagate to downstream subtasks, ul-
timately deteriorating overall performance. There-
fore, a method that allows the joint labeling of sub-
tasks is desired. Two major advantages arise from
simultaneous labeling: First, there is more robust-
ness against error propagation. This is especially
relevant if we use probabilities in our models. Cas-
cading subtasks inherently “throws away” the prob-
ability at each stage; joint labeling preserves the un-
certainty. Second, information between simultane-
ous subtasks can be shared to further improve ac-
curacy. For instance, it is possible that knowing a
certain noun phrase chunk may help the model infer
POS tags more accurately, and vice versa.
In this paper, we propose a solution to the
joint labeling problem by representing multiple se-
quences in a single Factorial Hidden Markov Model
(FHMM) (Ghahramani and Jordan, 1997). The
FHMM generalizes hidden Markov models (HMM)
by allowing separate hidden state sequences. In our
case, these hidden state sequences represent the POS
tags and phrase chunk labels. The links between the
two hidden sequences model dependencies between
tags and chunks. Together the hidden sequences
generate an observed word sequence, and the task of
the tagger/chunker is to invert this process and infer
the original tags and chunks.
Previous work on joint tagging/chunking has
shown promising results. For example, Xun et
</bodyText>
<page confidence="0.992048">
19
</page>
<note confidence="0.690101">
Proceedings of the ACL Student Research Workshop, pages 19–24,
</note>
<page confidence="0.485235">
Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics
</page>
<figureCaption confidence="0.942752">
Figure 1: Baseline FHMM. The two hidden se-
quences y1:t and z1:t can represent tags and chunks,
respectively. Together they generate x1:t, the ob-
served word sequence.
</figureCaption>
<bodyText confidence="0.999761222222222">
al. (2000) uses a POS tagger to output an N-best list
of tags, then a Viterbi search to find the chunk se-
quence that maximizes the joint tag/chunk probabil-
ity. Florian and Ngai (2001) extends transformation-
based learning tagger to a joint tagger/chunker by
modifying the objective function such that a trans-
formation rule is evaluated on the classification
of all simultaneous subtasks. Our work is most
similar in spirit to Dynamic Conditional Random
Fields (DCRF) (Sutton et al., 2004), which also
models tagging and chunking in a factorial frame-
work. Some main differences between our model
and DCRF may be described as 1) directed graphical
model vs. undirected graphical model, and 2) gener-
ative model vs. conditional model. The main advan-
tage of FHMM over DCRF is that FHMM requires
considerably less computation and exact inference is
easily achievable for FHMM and its variants.
The paper is structured as follows: Section 2 de-
scribes in detail the FHMM. Section 3 presents a
new model, the Switching FHMM, which represents
cross-sequence dependencies more effectively than
FHMMs. Section 4 discusses the task and data and
Section 5 presents various experimental results Sec-
tion 6 discusses future work and concludes.
variables (e.g. tags, chunks). Then we define the
FHMM as the probabilistic model:
</bodyText>
<equation confidence="0.9964625">
p(x1:T, y1:T, z1:T) (1)
p(xt|yt, zt)p(yt|yt−1, zt)p(zt|zt−1)
</equation>
<bodyText confidence="0.999887625">
where π0 = p(x0|y0,z0)p(y0|z0)p(z0). Viewed
as a generative process, we can say that the
chunk model p(zt|zt−1) generates chunks depend-
ing on the previous chunk label, the tag model
p(yt|yt−1,zt) generates tags based on the previ-
ous tag and current chunk, and the word model
p(xt|yt, zt) generates words using the tag and chunk
at the same time-step.
This equation corresponds to the graphical model
of Figure 1. Although the original FHMM de-
veloped by Ghahramani (1997) does not explicitly
model the dependencies between the two hidden
state sequences, here we add the edges between the
y and z nodes to reflect the interaction between tag
and chunk sequences. Note that the FHMM can be
collapsed into a hidden Markov model where the
hidden state is the cross-product of the distributed
states y and z. Despite this equivalence, the FHMM
is advantageous because it requires the estimation of
substantiatially fewer parameters.
FHMM parameters can be calculated via maxi-
mum likelihood (ML) estimation if the values of the
hidden states are available in the training data. Oth-
erwise, parameters must be learned using approx-
imate inference algorithms (e.g. Gibbs sampling,
variational inference), since exact Expectation-
Maximization (EM) algorithm is computationally
intractable (Ghahramani and Jordan, 1997). Given
a test sentence, inference of the corresponding
tag/chunk sequence is found by the Viterbi algo-
rithm, which finds the tag/chunk sequence that max-
imizes the joint probability, i.e.
</bodyText>
<equation confidence="0.9535975">
T
ri
t=2
= π0
2 Factorial HMM arg max p(x1:T, y1:T, z1:T) (2)
y1:T ,z1:T
</equation>
<subsectionHeader confidence="0.993094">
2.1 Basic Factorial HMM
</subsectionHeader>
<bodyText confidence="0.9997904">
A Factorial Hidden Markov Model (FHMM) is a
hidden Markov model with a distributed state rep-
resentation. Let x1:T be a length T sequence of ob-
served random variables (e.g. words) and y1:T and
z1:T be the corresponding sequences of hidden state
</bodyText>
<subsectionHeader confidence="0.99988">
2.2 Adding Cross-Sequence Dependencies
</subsectionHeader>
<bodyText confidence="0.999973">
Many other structures exist in the FHMM frame-
work. Statistical modeling often involves the it-
erative process of finding the best set of depen-
dencies that characterizes the data effectively. As
shown in Figures 2(a), 2(b), and 2(c), dependen-
</bodyText>
<page confidence="0.967653">
20
</page>
<bodyText confidence="0.999725714285714">
cies can be added between the yt and zt−1, be-
tween zt and yt−1, or both. The model in Fig. 2(a)
corresponds to changing the tag model in Eq. 1 to
p(yt|yt−1, zt, zt−1); Fig. 2(b) corresponds to chang-
ing the chunk model to p(zt|zt−1, yt−1); Fig. 2(c),
corresponds to changing both tag and chunk models,
leading to the probability model:
</bodyText>
<equation confidence="0.99093525">
T
H p(xt|yt, zt)p(yt|yt−1, zt, zt−1)p(zt|zt−1, yt−1)
t=1
(3)
</equation>
<bodyText confidence="0.99986925">
We name the models in Figs. 2(a) and 2(b) as
FHMM-T and FHMM-C due to the added depen-
dencies to the tag and chunk models, respectively.
The model of Fig. 2(c) and Eq. 3 will be referred to
as FHMM-CT. Intuitively, the added dependencies
will improve the predictive power across chunk and
tag sequences, provided that enough training data
are available for robust parameter estimation.
</bodyText>
<figureCaption confidence="0.816267">
Figure 2: FHMMs with additional cross-sequence
dependencies. The models will be referred to as (a)
FHMM-T, (b) FHMM-C, and (c) FHMM-CT.
</figureCaption>
<sectionHeader confidence="0.853407" genericHeader="method">
3 Switching Factorial HMM
</sectionHeader>
<bodyText confidence="0.999982291666667">
A reasonable question to ask is, “How exactly does
the chunk sequence interact with the tag sequence?”
The approach of adding dependencies in Section 2.2
acknowledges the existence of cross-sequence inter-
actions but does not explicitly specify the type of
interaction. It relies on statistical learning to find
the salient dependencies, but such an approach is
feasable only when sufficient data are available for
parameter estimation.
To answer the question, we consider how the
chunk sequence affects the generative process for
tags: First, we can expect that the unigram distri-
bution of tags changes depending on whether the
chunk is a noun phrase or verb phrase. (In a noun
phrase, nouns and adjective tags are more com-
mon; in a verb phrase, verbs and adverb tags are
more frequent.) Similarly, a bigram distribution
p(yt|yt−1) describing tag transition probabilities dif-
fers depending on the bigram’s location in the chunk
sequence, such as whether it is within a noun phrase,
verb phrase, or at a phrase boundary. In other words,
the chunk sequence interacts with tags by switching
the particular generative process for tags. We model
this interaction explicitly using a Switching FHMM:
</bodyText>
<equation confidence="0.9990005">
p(x1:T, y1:T, z1:T) (4)
T
= H p(xt|yt, zt)pa(yt|yt−1)p,C3(zt|zt−1)
t=1
</equation>
<bodyText confidence="0.999353323529412">
In this new model, the chunk and tag are now gen-
erated by bigram distributions parameterized by α
and Q. For different values of α (or Q), we have
different distributions for p(yt|yt−1) (or p(zt|zt−1)).
The crucial aspect of the model lies in a function
α = f(z1:t), which summarizes information in z1:t
that is relevant for the generation of y, and a func-
tion Q = g(y1:t), which captures information in y1:t
that is relevant to the generation of z.
In general, the functions f(·) and g(·) partition
the space of all tag or chunk sequences into sev-
eral equivalence classes, such that all instances of
an equivalence class give rise to the same genera-
tive model for the cross sequence. For instance, all
consecutive chunk labels that indicate a noun phrase
can be mapped to one equivalence class, while labels
that indicate verb phrase can be mapped to another.
The mapping can be specified manually or learned
automatically. Section 5 discusses a linguistically-
motivated mapping that is used for the experiments.
Once the mappings are defined, the parameters
pa(yt|yt−1) and pa(zt|zt−1) are obtained via max-
imum likelihood estimation in a fashion similar to
that of the FHMM. The only exception is that now
the training data are partitioned according to the
mappings, and each α- and Q- specific generative
model is estimated separately. Inference of the tags
and chunks for a test sentence proceeds similarly to
FHMM inference. We call this model a Switching
FHMM since the distribution of a hidden sequence
”switches” dynamically depending on the values of
the other hidden sequence.
An idea related to the Switching FHMM is the
Bayesian Multinet (Geiger and Heckerman, 1996;
</bodyText>
<figure confidence="0.89285">
(a) (b) (c)
</figure>
<page confidence="0.996192">
21
</page>
<bodyText confidence="0.999911833333333">
Bilmes, 2000), which allows the dynamic switching
of conditional variables. It can be used to implement
switching from a higher-order model to a lower-
order model, a form of backoff smoothing for deal-
ing with data sparsity. The Switching FHMM differs
in that it switches among models of the same order,
but these models represent different generative pro-
cesses. The result is that the model no longer re-
quires a time-homogenous assumption for state tran-
sitions; rather, the transition probabilities change
dynamically depending on the influence across se-
quences.
</bodyText>
<sectionHeader confidence="0.961949" genericHeader="method">
4 POS Tagging and NP Chunking
</sectionHeader>
<subsectionHeader confidence="0.965885">
4.1 The Tasks
</subsectionHeader>
<bodyText confidence="0.999786">
POS tagging is the task of assigning words the
correct part-of-speech, and is often the first stage
of various natural language processing tasks. As
a result, POS tagging has been one of the most
active areas of research, and many statistical and
rule-based approach have been tried. The most
notable of these include the trigram HMM tagger
(Brants, 2000), maximum entropy tagger (Ratna-
parkhi, 1996), transformation-based tagger (Brill,
1995), and cyclic dependency networks (Toutanova
et al., 2003).
Accuracy numbers for POS tagging are often re-
ported in the range of 95% to 97%. Although
this may seem high, note that a tagger with 97%
accuracy has only a 63% chance of getting all
tags in a 15-word sentence correct, whereas a 98%
accurate tagger has 74% (Manning and Sch¨utze,
1999). Therefore, small improvements can be sig-
nificant, especially if downstream processing re-
quires correctly-tagged sentences. One of the most
difficult problems with POS tagging is the handling
of out-of-vocabulary words.
Noun-phrase (NP) chunking is the task of finding
the non-recursive (base) noun-phrases of sentences.
This segmentation task can be achieved by assign-
ing words in a sentence to one of three tokens: B for
“Begin-NP”, I for “Inside-NP”, or O for “Outside-
NP” (Ramshaw and Marcus, 1995). The “Begin-
NP” token is used in the case when an NP chunk
is immediately followed by another NP chunk. The
state-of-the-art chunkers report F1 scores of 93%-
94% and accuracies of 87%-97%. See, for exam-
ple, NP chunkers utilizing conditional random fields
(Sha and Pereira, 2003) and support vector machines
(Kudo and Matsumoto, 2001).
</bodyText>
<subsectionHeader confidence="0.965774">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.998957">
The data comes from the CoNLL 2000 shared task
(Sang and Buchholz, 2000), which consists of sen-
tences from the Penn Treebank Wall Street Journal
corpus (Marcus et al., 1993). The training set con-
tains a total of 8936 sentences with 19k unique vo-
cabulary. The test set contains 2012 sentences and
8k vocabulary. The out-of-vocabulary rate is 7%.
There are 45 different POS tags and 3 different
NP labels in the original data. An example sentence
with POS and NP tags is shown in Table 1.
</bodyText>
<table confidence="0.807639666666667">
The move could pose a challenge
DT NN MD VB DT NN
I I O O I I
</table>
<tableCaption confidence="0.9744495">
Table 1: Example sentence with POS tags (2nd row) and NP
labels (3rd row). For NP, I = Inside-NP, O=Outside-NP.
</tableCaption>
<sectionHeader confidence="0.997858" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999866375">
We report two sets of experiments. Experiment 1
compares several FHMMs with cascaded HMMs
and demonstrates the benefit of joint labeling. Ex-
periment 2 evaluates the Switching FHMM for
various training dataset sizes and shows its ro-
bustness against data sparsity. All models are
implemented using the Graphical Models Toolkit
(GMTK) (Bilmes and Zweig, 2002).
</bodyText>
<subsectionHeader confidence="0.938316">
5.1 Ezp1: FHMM vs Cascaded HMMs
</subsectionHeader>
<bodyText confidence="0.999870076923077">
We compare the four FHMMs of Section 2 to the
traditional approach of cascading HMMs in succes-
sion, and compare their POS and NP accuracies in
Table 2. In this table, the first row “Oracle HMM”
is an oracle experiment which shows what NP accu-
racies can be achieved if perfectly correct POS tags
are available in a cascaded approach. The second
row “Cascaded HMM” represents the traditional ap-
proach of doing POS tagging and NP chunking in
succession; i.e. an NP chunker is applied to the out-
put of a POS tagger that is 94.17% accurate. The
next four rows show the results of joint labeling us-
ing various FHMMs. The final row “DCRF” are
</bodyText>
<page confidence="0.991525">
22
</page>
<bodyText confidence="0.998018710526316">
comparable results from Dynamic Conditional Ran-
dom Fields (Sutton et al., 2004).
There are several observations: First, it is im-
portant to note that FHMM outperforms the cas-
caded HMM in terms of NP accuracy for all but one
model. For instance, FHMM-CT achieves an NP
accuracy of 95.93%, significantly higher than both
the cascaded HMM (93.90%) and the oracle HMM
(94.67%). This confirms our hypothesis that joint la-
beling helps prevent POS errors from propagating to
NP chunking. Second, the fact that several FHMM
models achieve NP accuracies higher than the ora-
cle HMM implies that information sharing between
POS and NP sequences gives even more benefit than
having only perfectly correct POS tags. Thirdly, the
fact that the most complex model (FHMM-CT) per-
forms best suggests that it is important to avoid data
sparsity problems, as it requires more parameters to
be estimated in training.
Finally, it should be noted that although the DCRF
outperforms the FHMM in this experiment, the
DCRF uses significantly more word features (e.g.
capitalization, existence in a list of proper nouns,
etc.) and a larger context (previous and next 3
tags), whereas the FHMM considers the word as its
sole feature, and the previous tag as its only con-
text. Further work is required to see whether the
addition of these features in the FHMM’s genera-
tive framework will achieve accuracies close to that
of DCRF. The take-home message is that, in light
of the computational advantages of generative mod-
els, the FHMM should not be dismissed as a poten-
tial solution for joint labeling. In fact, recent results
in the discriminative training of FHMMs (Bach and
Jordan, 2005) has shown promising results in speech
processing and it is likely that such advanced tech-
niques, among others, may improve the FHMM’s
performance to state-of-the-art results.
</bodyText>
<subsectionHeader confidence="0.997102">
5.2 Ezp2: Switching FHMM and Data Sparsity
</subsectionHeader>
<bodyText confidence="0.999121">
We now compare the Switching FHMM to the best
model of Experiment 1 (FHMM-CT) for varying
amounts of training data. The Switching FHMM
uses the following α and Q mapping. The mapping
α = f(z1:t) partitions the space of chunk history z1:t
into five equivalence classes based on the two most
recent chunk labels:
</bodyText>
<table confidence="0.9995965">
Model POS NP
Oracle HMM – 94.67
Cascaded HMM 94.17 93.90
Baseline FHMM 93.82 93.56
FHMM-T 93.73 94.07
FHMM-C 94.16 95.76
FHMM-CT 94.15 95.93
DCRF 98.92 97.36
</table>
<tableCaption confidence="0.841329714285714">
Table 2: POS and NP Accuracy for Cascaded HMM
and FHMM Models.
Class1. {z1:t : zt−1 = I, zt = I}
Class2. {z1:t : zt−1 = O, zt = O}
Class3. {z1:t : zt−1 = {I, B}, zt = O}
Class4. {z1:t : zt−1 = O, zt = {I, B}}
Class5. {z1:t : (zt−1, zt) = {(I, B), (B, I)}}
</tableCaption>
<bodyText confidence="0.997577875">
Class1 and Class2 are cases where the tag is located
strictly inside or outside an NP chunk. Class3 and
Class4 are situations where the tag is leaving or en-
tering an NP, and Class5 is when the tag transits be-
tween consecutive NP chunks. Class-specific tag bi-
grams pα(yt|yt−1) are trained by dividing the train-
ing data according to the mapping. On the other
hand, the mapping Q = g(y1:t) is not used to en-
sure a single point of comparison with FHMM-CT;
we use FHMM-CT’s chunk model p(zt|zt−1, yt−1)
in place of p,3(zt|zt−1).
The POS and NP accuracies are plotted in Figures
3 and 4. We report accuracies based on the aver-
age of five different random subsets of the training
data for datasets of sizes 1000, 3000, 5000, and 7000
sentences. Note that for the Switching FHMM, POS
and NP accuracy remains relatively constant despite
the reduction in data size. This suggests that a more
explicit model for cross sequence interaction is es-
sential especially in the case of insufficient train-
ing data. Also, for the very small datasize of 1000,
the accuracies for Cascaded HMM are 84% for POS
and 70% for NP, suggesting that the general FHMM
framework is still beneficial.
</bodyText>
<sectionHeader confidence="0.997623" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999954">
We have demonstrated that joint labeling with an
FHMM can outperform the traditional approach of
cascading tagging and chunking in NLP. The new
Switching FHMM generalizes the FHMM by allow-
</bodyText>
<page confidence="0.991727">
23
</page>
<figure confidence="0.962553">
Number of training sentences
</figure>
<figureCaption confidence="0.9999785">
Figure 3: POS Accuracy for varying data sizes
Figure 4: NP Accuracy for varying data sizes
</figureCaption>
<bodyText confidence="0.99995625">
ing dynamically changing generative models and is
a promising approach for modeling the type of inter-
actions between hidden state sequences.
Three directions for future research are planned:
First, we will augment the FHMM such that its ac-
curacies are competitive with state-of-the-art taggers
and chunkers. This includes adding word features to
improve accuracy on OOV words, augmenting the
context from bigram to trigram, and applying ad-
vanced smoothing techniques. Second, we plan to
examine the Switching FHMM further, especially in
terms of automatic construction of the α and Q func-
tion. A promising approach is to learn the mappings
using decision trees or random forests, which has re-
cently achieved good results in a similar problem in
language modeling (Xu and Jelinek, 2004). Finally,
we plan to integrate the tagger/chunker in an end-
to-end system, such as a Factored Language Model
(Bilmes and Kirchhoff, 2003), to measure the over-
all merit of joint labeling.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993162">
The author would like to thank Katrin Kirchhoff, Jeff Bilmes,
and Gang Ji for insightful discussions, Chris Bartels for support
on GMTK, and the two anonymous reviewers for their construc-
tive comments. Also, the author gratefully acknowledges sup-
port from NSF and CIA under NSF Grant No. IIS-0326276.
</bodyText>
<sectionHeader confidence="0.989414" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998747591836735">
Francis Bach and Michael Jordan. 2005. Discriminative train-
ing of hidden Markov models for multiple pitch tracking. In
Proc. Intl. Conf. Acoustics, Speech, Signal Processing.
J. Bilmes and K. Kirchhoff. 2003. Factored language models
and generalized parallel backoff. In Proc. of HLT/NACCL.
J. Bilmes and G. Zweig. 2002. The Graphical Models Toolkit:
An open source software system for speech and time-series
processing. In Intl. Conf. on Acoustics, Speech, Signal Proc.
Jeff Bilmes. 2000. Dynamic bayesian multi-networks. In The
16th Conference on Uncertainty in Artificial Intelligence.
Thorsten Brants. 2000. TnT – a statistical part-of-speech tag-
ger. In Proceedings of the Applied NLP.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part of
speech tagging. Computational Linguistics, 21(4):543–565.
Radu Florian and Grace Ngai. 2001. Multidimensional
transformation-based learning. In Proc. CoNLL.
D. Geiger and D. Heckerman. 1996. Knowledge representation
and inference in similarity netwrosk and Bayesian multinets.
Artificial Intelligence, 82:45–74.
Z. Ghahramani and M. I. Jordan. 1997. Factorial hidden
Markov models. Machine Learning, 29:245–275.
T. Kudo and Y. Matsumoto. 2001. Chunking with support vec-
tor machines. In Proceedings ofNAACL-2001.
C. D. Manning and H. Sch¨utze, 1999. Foundations ofStatistical
Natural Language Processing, chapter 10. MIT Press.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The Penn
Treebank. Computational Linguistics, 19:313–330.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using
transformation-based learning. In Proceedings of the Third
Workshop on Very Large Corpora (ACL-95).
A. Ratnaparkhi. 1996. A maximum entropy model for part-of-
speech tagging. In Proceedings ofEMNLP-1996.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to
the CoNLL-2000 shared task: Chunking. In Proc. CoNLL.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of HLT-NAACL.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dy-
namic conditional random fields. In Intl. Conf. Machine
Learning (ICML 2004).
K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic depen-
dency network. In Proc. of HLT-NAACL.
Peng Xu and Frederick Jelinek. 2004. Random forests in lan-
guage modeling. In Proc. EMNLP.
E. Xun, C. Huang, and M. Zhou. 2000. A unified statistical
model for the identification of English BaseNP. In Proc.
ACL.
</reference>
<figure confidence="0.99891704">
FHMM−CT
Switch FHMM
86
1000 2000 3000 4000 5000 6000 7000 8000 9000
95
94
93
92
91
90
89
88
87
POS Accuracy
96
95.5
95
94.5
94
FHMM−CT
Switch FHMM
93.5
1000 2000 3000 4000 5000 6000 7000 8000 9000
Number of training sentences
NP Accuracy
</figure>
<page confidence="0.973859">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.596067">
<title confidence="0.999003">Jointly Labeling Multiple Sequences: A Factorial HMM Approach</title>
<author confidence="0.999975">Kevin Duh</author>
<affiliation confidence="0.8016435">Department of Electrical Engineering University of Washington, USA</affiliation>
<email confidence="0.999793">duh@ee.washington.edu</email>
<abstract confidence="0.999479761904762">We present new statistical models for jointly labeling multiple sequences and apply them to the combined task of partof-speech tagging and noun phrase chunking. The model is based on the Factorial Hidden Markov Model (FHMM) with distributed hidden states representing partof-speech and noun phrase sequences. We demonstrate that this joint labeling approach, by enabling information sharing between tagging/chunking subtasks, outperforms the traditional method of tagging and chunking in succession. Further, we extend this into a novel model, Switching FHMM, to allow for explicit modeling of cross-sequence dependencies based on linguistic knowledge. We report tagging/chunking accuracies for varying dataset sizes and show that our approach is relatively robust to data sparsity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francis Bach</author>
<author>Michael Jordan</author>
</authors>
<title>Discriminative training of hidden Markov models for multiple pitch tracking.</title>
<date>2005</date>
<booktitle>In Proc. Intl. Conf. Acoustics, Speech, Signal Processing.</booktitle>
<contexts>
<context position="16357" citStr="Bach and Jordan, 2005" startWordPosition="2646" endWordPosition="2649">.g. capitalization, existence in a list of proper nouns, etc.) and a larger context (previous and next 3 tags), whereas the FHMM considers the word as its sole feature, and the previous tag as its only context. Further work is required to see whether the addition of these features in the FHMM’s generative framework will achieve accuracies close to that of DCRF. The take-home message is that, in light of the computational advantages of generative models, the FHMM should not be dismissed as a potential solution for joint labeling. In fact, recent results in the discriminative training of FHMMs (Bach and Jordan, 2005) has shown promising results in speech processing and it is likely that such advanced techniques, among others, may improve the FHMM’s performance to state-of-the-art results. 5.2 Ezp2: Switching FHMM and Data Sparsity We now compare the Switching FHMM to the best model of Experiment 1 (FHMM-CT) for varying amounts of training data. The Switching FHMM uses the following α and Q mapping. The mapping α = f(z1:t) partitions the space of chunk history z1:t into five equivalence classes based on the two most recent chunk labels: Model POS NP Oracle HMM – 94.67 Cascaded HMM 94.17 93.90 Baseline FHMM</context>
</contexts>
<marker>Bach, Jordan, 2005</marker>
<rawString>Francis Bach and Michael Jordan. 2005. Discriminative training of hidden Markov models for multiple pitch tracking. In Proc. Intl. Conf. Acoustics, Speech, Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>K Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NACCL.</booktitle>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>J. Bilmes and K. Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proc. of HLT/NACCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bilmes</author>
<author>G Zweig</author>
</authors>
<title>The Graphical Models Toolkit: An open source software system for speech and time-series processing.</title>
<date>2002</date>
<booktitle>In Intl. Conf. on Acoustics, Speech, Signal Proc.</booktitle>
<contexts>
<context position="14026" citStr="Bilmes and Zweig, 2002" startWordPosition="2247" endWordPosition="2250">e original data. An example sentence with POS and NP tags is shown in Table 1. The move could pose a challenge DT NN MD VB DT NN I I O O I I Table 1: Example sentence with POS tags (2nd row) and NP labels (3rd row). For NP, I = Inside-NP, O=Outside-NP. 5 Experiments We report two sets of experiments. Experiment 1 compares several FHMMs with cascaded HMMs and demonstrates the benefit of joint labeling. Experiment 2 evaluates the Switching FHMM for various training dataset sizes and shows its robustness against data sparsity. All models are implemented using the Graphical Models Toolkit (GMTK) (Bilmes and Zweig, 2002). 5.1 Ezp1: FHMM vs Cascaded HMMs We compare the four FHMMs of Section 2 to the traditional approach of cascading HMMs in succession, and compare their POS and NP accuracies in Table 2. In this table, the first row “Oracle HMM” is an oracle experiment which shows what NP accuracies can be achieved if perfectly correct POS tags are available in a cascaded approach. The second row “Cascaded HMM” represents the traditional approach of doing POS tagging and NP chunking in succession; i.e. an NP chunker is applied to the output of a POS tagger that is 94.17% accurate. The next four rows show the re</context>
</contexts>
<marker>Bilmes, Zweig, 2002</marker>
<rawString>J. Bilmes and G. Zweig. 2002. The Graphical Models Toolkit: An open source software system for speech and time-series processing. In Intl. Conf. on Acoustics, Speech, Signal Proc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Bilmes</author>
</authors>
<title>Dynamic bayesian multi-networks.</title>
<date>2000</date>
<booktitle>In The 16th Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="10769" citStr="Bilmes, 2000" startWordPosition="1707" endWordPosition="1708">tained via maximum likelihood estimation in a fashion similar to that of the FHMM. The only exception is that now the training data are partitioned according to the mappings, and each α- and Q- specific generative model is estimated separately. Inference of the tags and chunks for a test sentence proceeds similarly to FHMM inference. We call this model a Switching FHMM since the distribution of a hidden sequence ”switches” dynamically depending on the values of the other hidden sequence. An idea related to the Switching FHMM is the Bayesian Multinet (Geiger and Heckerman, 1996; (a) (b) (c) 21 Bilmes, 2000), which allows the dynamic switching of conditional variables. It can be used to implement switching from a higher-order model to a lowerorder model, a form of backoff smoothing for dealing with data sparsity. The Switching FHMM differs in that it switches among models of the same order, but these models represent different generative processes. The result is that the model no longer requires a time-homogenous assumption for state transitions; rather, the transition probabilities change dynamically depending on the influence across sequences. 4 POS Tagging and NP Chunking 4.1 The Tasks POS tag</context>
</contexts>
<marker>Bilmes, 2000</marker>
<rawString>Jeff Bilmes. 2000. Dynamic bayesian multi-networks. In The 16th Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Applied NLP.</booktitle>
<contexts>
<context position="11717" citStr="Brants, 2000" startWordPosition="1862" endWordPosition="1863">e processes. The result is that the model no longer requires a time-homogenous assumption for state transitions; rather, the transition probabilities change dynamically depending on the influence across sequences. 4 POS Tagging and NP Chunking 4.1 The Tasks POS tagging is the task of assigning words the correct part-of-speech, and is often the first stage of various natural language processing tasks. As a result, POS tagging has been one of the most active areas of research, and many statistical and rule-based approach have been tried. The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratnaparkhi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al., 2003). Accuracy numbers for POS tagging are often reported in the range of 95% to 97%. Although this may seem high, note that a tagger with 97% accuracy has only a 63% chance of getting all tags in a 15-word sentence correct, whereas a 98% accurate tagger has 74% (Manning and Sch¨utze, 1999). Therefore, small improvements can be significant, especially if downstream processing requires correctly-tagged sentences. One of the most difficult problems with POS </context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000. TnT – a statistical part-of-speech tagger. In Proceedings of the Applied NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="11804" citStr="Brill, 1995" startWordPosition="1872" endWordPosition="1873">n for state transitions; rather, the transition probabilities change dynamically depending on the influence across sequences. 4 POS Tagging and NP Chunking 4.1 The Tasks POS tagging is the task of assigning words the correct part-of-speech, and is often the first stage of various natural language processing tasks. As a result, POS tagging has been one of the most active areas of research, and many statistical and rule-based approach have been tried. The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratnaparkhi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al., 2003). Accuracy numbers for POS tagging are often reported in the range of 95% to 97%. Although this may seem high, note that a tagger with 97% accuracy has only a 63% chance of getting all tags in a 15-word sentence correct, whereas a 98% accurate tagger has 74% (Manning and Sch¨utze, 1999). Therefore, small improvements can be significant, especially if downstream processing requires correctly-tagged sentences. One of the most difficult problems with POS tagging is the handling of out-of-vocabulary words. Noun-phrase (NP) chunking is the ta</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Grace Ngai</author>
</authors>
<title>Multidimensional transformation-based learning.</title>
<date>2001</date>
<booktitle>In Proc. CoNLL.</booktitle>
<contexts>
<context position="3552" citStr="Florian and Ngai (2001)" startWordPosition="535" endWordPosition="538">d infer the original tags and chunks. Previous work on joint tagging/chunking has shown promising results. For example, Xun et 19 Proceedings of the ACL Student Research Workshop, pages 19–24, Ann Arbor, Michigan, June 2005. c�2005 Association for Computational Linguistics Figure 1: Baseline FHMM. The two hidden sequences y1:t and z1:t can represent tags and chunks, respectively. Together they generate x1:t, the observed word sequence. al. (2000) uses a POS tagger to output an N-best list of tags, then a Viterbi search to find the chunk sequence that maximizes the joint tag/chunk probability. Florian and Ngai (2001) extends transformationbased learning tagger to a joint tagger/chunker by modifying the objective function such that a transformation rule is evaluated on the classification of all simultaneous subtasks. Our work is most similar in spirit to Dynamic Conditional Random Fields (DCRF) (Sutton et al., 2004), which also models tagging and chunking in a factorial framework. Some main differences between our model and DCRF may be described as 1) directed graphical model vs. undirected graphical model, and 2) generative model vs. conditional model. The main advantage of FHMM over DCRF is that FHMM req</context>
</contexts>
<marker>Florian, Ngai, 2001</marker>
<rawString>Radu Florian and Grace Ngai. 2001. Multidimensional transformation-based learning. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Geiger</author>
<author>D Heckerman</author>
</authors>
<title>Knowledge representation and inference in similarity netwrosk and Bayesian multinets.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>82--45</pages>
<contexts>
<context position="10739" citStr="Geiger and Heckerman, 1996" startWordPosition="1699" endWordPosition="1702">rameters pa(yt|yt−1) and pa(zt|zt−1) are obtained via maximum likelihood estimation in a fashion similar to that of the FHMM. The only exception is that now the training data are partitioned according to the mappings, and each α- and Q- specific generative model is estimated separately. Inference of the tags and chunks for a test sentence proceeds similarly to FHMM inference. We call this model a Switching FHMM since the distribution of a hidden sequence ”switches” dynamically depending on the values of the other hidden sequence. An idea related to the Switching FHMM is the Bayesian Multinet (Geiger and Heckerman, 1996; (a) (b) (c) 21 Bilmes, 2000), which allows the dynamic switching of conditional variables. It can be used to implement switching from a higher-order model to a lowerorder model, a form of backoff smoothing for dealing with data sparsity. The Switching FHMM differs in that it switches among models of the same order, but these models represent different generative processes. The result is that the model no longer requires a time-homogenous assumption for state transitions; rather, the transition probabilities change dynamically depending on the influence across sequences. 4 POS Tagging and NP </context>
</contexts>
<marker>Geiger, Heckerman, 1996</marker>
<rawString>D. Geiger and D. Heckerman. 1996. Knowledge representation and inference in similarity netwrosk and Bayesian multinets. Artificial Intelligence, 82:45–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Ghahramani</author>
<author>M I Jordan</author>
</authors>
<date>1997</date>
<booktitle>Factorial hidden Markov models. Machine Learning,</booktitle>
<pages>29--245</pages>
<contexts>
<context position="2528" citStr="Ghahramani and Jordan, 1997" startWordPosition="373" endWordPosition="376">stness against error propagation. This is especially relevant if we use probabilities in our models. Cascading subtasks inherently “throws away” the probability at each stage; joint labeling preserves the uncertainty. Second, information between simultaneous subtasks can be shared to further improve accuracy. For instance, it is possible that knowing a certain noun phrase chunk may help the model infer POS tags more accurately, and vice versa. In this paper, we propose a solution to the joint labeling problem by representing multiple sequences in a single Factorial Hidden Markov Model (FHMM) (Ghahramani and Jordan, 1997). The FHMM generalizes hidden Markov models (HMM) by allowing separate hidden state sequences. In our case, these hidden state sequences represent the POS tags and phrase chunk labels. The links between the two hidden sequences model dependencies between tags and chunks. Together the hidden sequences generate an observed word sequence, and the task of the tagger/chunker is to invert this process and infer the original tags and chunks. Previous work on joint tagging/chunking has shown promising results. For example, Xun et 19 Proceedings of the ACL Student Research Workshop, pages 19–24, Ann Ar</context>
<context position="6045" citStr="Ghahramani and Jordan, 1997" startWordPosition="923" endWordPosition="926">MM can be collapsed into a hidden Markov model where the hidden state is the cross-product of the distributed states y and z. Despite this equivalence, the FHMM is advantageous because it requires the estimation of substantiatially fewer parameters. FHMM parameters can be calculated via maximum likelihood (ML) estimation if the values of the hidden states are available in the training data. Otherwise, parameters must be learned using approximate inference algorithms (e.g. Gibbs sampling, variational inference), since exact ExpectationMaximization (EM) algorithm is computationally intractable (Ghahramani and Jordan, 1997). Given a test sentence, inference of the corresponding tag/chunk sequence is found by the Viterbi algorithm, which finds the tag/chunk sequence that maximizes the joint probability, i.e. T ri t=2 = π0 2 Factorial HMM arg max p(x1:T, y1:T, z1:T) (2) y1:T ,z1:T 2.1 Basic Factorial HMM A Factorial Hidden Markov Model (FHMM) is a hidden Markov model with a distributed state representation. Let x1:T be a length T sequence of observed random variables (e.g. words) and y1:T and z1:T be the corresponding sequences of hidden state 2.2 Adding Cross-Sequence Dependencies Many other structures exist in t</context>
</contexts>
<marker>Ghahramani, Jordan, 1997</marker>
<rawString>Z. Ghahramani and M. I. Jordan. 1997. Factorial hidden Markov models. Machine Learning, 29:245–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL-2001.</booktitle>
<contexts>
<context position="12984" citStr="Kudo and Matsumoto, 2001" startWordPosition="2063" endWordPosition="2066">ry words. Noun-phrase (NP) chunking is the task of finding the non-recursive (base) noun-phrases of sentences. This segmentation task can be achieved by assigning words in a sentence to one of three tokens: B for “Begin-NP”, I for “Inside-NP”, or O for “OutsideNP” (Ramshaw and Marcus, 1995). The “BeginNP” token is used in the case when an NP chunk is immediately followed by another NP chunk. The state-of-the-art chunkers report F1 scores of 93%- 94% and accuracies of 87%-97%. See, for example, NP chunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001). 4.2 Data The data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of sentences from the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). The training set contains a total of 8936 sentences with 19k unique vocabulary. The test set contains 2012 sentences and 8k vocabulary. The out-of-vocabulary rate is 7%. There are 45 different POS tags and 3 different NP labels in the original data. An example sentence with POS and NP tags is shown in Table 1. The move could pose a challenge DT NN MD VB DT NN I I O O I I Table 1: Example sentence with POS tags </context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with support vector machines. In Proceedings ofNAACL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations ofStatistical Natural Language Processing, chapter 10.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. D. Manning and H. Sch¨utze, 1999. Foundations ofStatistical Natural Language Processing, chapter 10. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="13168" citStr="Marcus et al., 1993" startWordPosition="2095" endWordPosition="2098"> of three tokens: B for “Begin-NP”, I for “Inside-NP”, or O for “OutsideNP” (Ramshaw and Marcus, 1995). The “BeginNP” token is used in the case when an NP chunk is immediately followed by another NP chunk. The state-of-the-art chunkers report F1 scores of 93%- 94% and accuracies of 87%-97%. See, for example, NP chunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001). 4.2 Data The data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of sentences from the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). The training set contains a total of 8936 sentences with 19k unique vocabulary. The test set contains 2012 sentences and 8k vocabulary. The out-of-vocabulary rate is 7%. There are 45 different POS tags and 3 different NP labels in the original data. An example sentence with POS and NP tags is shown in Table 1. The move could pose a challenge DT NN MD VB DT NN I I O O I I Table 1: Example sentence with POS tags (2nd row) and NP labels (3rd row). For NP, I = Inside-NP, O=Outside-NP. 5 Experiments We report two sets of experiments. Experiment 1 compares several FHMMs with cascaded HMMs and demo</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora (ACL-95).</booktitle>
<contexts>
<context position="12650" citStr="Ramshaw and Marcus, 1995" startWordPosition="2008" endWordPosition="2011">hance of getting all tags in a 15-word sentence correct, whereas a 98% accurate tagger has 74% (Manning and Sch¨utze, 1999). Therefore, small improvements can be significant, especially if downstream processing requires correctly-tagged sentences. One of the most difficult problems with POS tagging is the handling of out-of-vocabulary words. Noun-phrase (NP) chunking is the task of finding the non-recursive (base) noun-phrases of sentences. This segmentation task can be achieved by assigning words in a sentence to one of three tokens: B for “Begin-NP”, I for “Inside-NP”, or O for “OutsideNP” (Ramshaw and Marcus, 1995). The “BeginNP” token is used in the case when an NP chunk is immediately followed by another NP chunk. The state-of-the-art chunkers report F1 scores of 93%- 94% and accuracies of 87%-97%. See, for example, NP chunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001). 4.2 Data The data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of sentences from the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). The training set contains a total of 8936 sentences with 19k unique vocabulary. </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora (ACL-95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-ofspeech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings ofEMNLP-1996.</booktitle>
<contexts>
<context position="11761" citStr="Ratnaparkhi, 1996" startWordPosition="1867" endWordPosition="1869">el no longer requires a time-homogenous assumption for state transitions; rather, the transition probabilities change dynamically depending on the influence across sequences. 4 POS Tagging and NP Chunking 4.1 The Tasks POS tagging is the task of assigning words the correct part-of-speech, and is often the first stage of various natural language processing tasks. As a result, POS tagging has been one of the most active areas of research, and many statistical and rule-based approach have been tried. The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratnaparkhi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al., 2003). Accuracy numbers for POS tagging are often reported in the range of 95% to 97%. Although this may seem high, note that a tagger with 97% accuracy has only a 63% chance of getting all tags in a 15-word sentence correct, whereas a 98% accurate tagger has 74% (Manning and Sch¨utze, 1999). Therefore, small improvements can be significant, especially if downstream processing requires correctly-tagged sentences. One of the most difficult problems with POS tagging is the handling of out-of-vocabulary</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-ofspeech tagging. In Proceedings ofEMNLP-1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking. In</title>
<date>2000</date>
<booktitle>Proc. CoNLL.</booktitle>
<contexts>
<context position="13067" citStr="Sang and Buchholz, 2000" startWordPosition="2078" endWordPosition="2081">noun-phrases of sentences. This segmentation task can be achieved by assigning words in a sentence to one of three tokens: B for “Begin-NP”, I for “Inside-NP”, or O for “OutsideNP” (Ramshaw and Marcus, 1995). The “BeginNP” token is used in the case when an NP chunk is immediately followed by another NP chunk. The state-of-the-art chunkers report F1 scores of 93%- 94% and accuracies of 87%-97%. See, for example, NP chunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001). 4.2 Data The data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of sentences from the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). The training set contains a total of 8936 sentences with 19k unique vocabulary. The test set contains 2012 sentences and 8k vocabulary. The out-of-vocabulary rate is 7%. There are 45 different POS tags and 3 different NP labels in the original data. An example sentence with POS and NP tags is shown in Table 1. The move could pose a challenge DT NN MD VB DT NN I I O O I I Table 1: Example sentence with POS tags (2nd row) and NP labels (3rd row). For NP, I = Inside-NP, O=Outside-NP. 5 Experimen</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="12929" citStr="Sha and Pereira, 2003" startWordPosition="2055" endWordPosition="2058"> with POS tagging is the handling of out-of-vocabulary words. Noun-phrase (NP) chunking is the task of finding the non-recursive (base) noun-phrases of sentences. This segmentation task can be achieved by assigning words in a sentence to one of three tokens: B for “Begin-NP”, I for “Inside-NP”, or O for “OutsideNP” (Ramshaw and Marcus, 1995). The “BeginNP” token is used in the case when an NP chunk is immediately followed by another NP chunk. The state-of-the-art chunkers report F1 scores of 93%- 94% and accuracies of 87%-97%. See, for example, NP chunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001). 4.2 Data The data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of sentences from the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). The training set contains a total of 8936 sentences with 19k unique vocabulary. The test set contains 2012 sentences and 8k vocabulary. The out-of-vocabulary rate is 7%. There are 45 different POS tags and 3 different NP labels in the original data. An example sentence with POS and NP tags is shown in Table 1. The move could pose a challenge DT NN MD VB DT </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>K Rohanimanesh</author>
<author>A McCallum</author>
</authors>
<title>Dynamic conditional random fields.</title>
<date>2004</date>
<booktitle>In Intl. Conf. Machine Learning (ICML</booktitle>
<contexts>
<context position="3856" citStr="Sutton et al., 2004" startWordPosition="581" endWordPosition="584">hidden sequences y1:t and z1:t can represent tags and chunks, respectively. Together they generate x1:t, the observed word sequence. al. (2000) uses a POS tagger to output an N-best list of tags, then a Viterbi search to find the chunk sequence that maximizes the joint tag/chunk probability. Florian and Ngai (2001) extends transformationbased learning tagger to a joint tagger/chunker by modifying the objective function such that a transformation rule is evaluated on the classification of all simultaneous subtasks. Our work is most similar in spirit to Dynamic Conditional Random Fields (DCRF) (Sutton et al., 2004), which also models tagging and chunking in a factorial framework. Some main differences between our model and DCRF may be described as 1) directed graphical model vs. undirected graphical model, and 2) generative model vs. conditional model. The main advantage of FHMM over DCRF is that FHMM requires considerably less computation and exact inference is easily achievable for FHMM and its variants. The paper is structured as follows: Section 2 describes in detail the FHMM. Section 3 presents a new model, the Switching FHMM, which represents cross-sequence dependencies more effectively than FHMMs</context>
<context position="14778" citStr="Sutton et al., 2004" startWordPosition="2382" endWordPosition="2385">, and compare their POS and NP accuracies in Table 2. In this table, the first row “Oracle HMM” is an oracle experiment which shows what NP accuracies can be achieved if perfectly correct POS tags are available in a cascaded approach. The second row “Cascaded HMM” represents the traditional approach of doing POS tagging and NP chunking in succession; i.e. an NP chunker is applied to the output of a POS tagger that is 94.17% accurate. The next four rows show the results of joint labeling using various FHMMs. The final row “DCRF” are 22 comparable results from Dynamic Conditional Random Fields (Sutton et al., 2004). There are several observations: First, it is important to note that FHMM outperforms the cascaded HMM in terms of NP accuracy for all but one model. For instance, FHMM-CT achieves an NP accuracy of 95.93%, significantly higher than both the cascaded HMM (93.90%) and the oracle HMM (94.67%). This confirms our hypothesis that joint labeling helps prevent POS errors from propagating to NP chunking. Second, the fact that several FHMM models achieve NP accuracies higher than the oracle HMM implies that information sharing between POS and NP sequences gives even more benefit than having only perfe</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dynamic conditional random fields. In Intl. Conf. Machine Learning (ICML 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="11861" citStr="Toutanova et al., 2003" startWordPosition="1878" endWordPosition="1881">n probabilities change dynamically depending on the influence across sequences. 4 POS Tagging and NP Chunking 4.1 The Tasks POS tagging is the task of assigning words the correct part-of-speech, and is often the first stage of various natural language processing tasks. As a result, POS tagging has been one of the most active areas of research, and many statistical and rule-based approach have been tried. The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratnaparkhi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al., 2003). Accuracy numbers for POS tagging are often reported in the range of 95% to 97%. Although this may seem high, note that a tagger with 97% accuracy has only a 63% chance of getting all tags in a 15-word sentence correct, whereas a 98% accurate tagger has 74% (Manning and Sch¨utze, 1999). Therefore, small improvements can be significant, especially if downstream processing requires correctly-tagged sentences. One of the most difficult problems with POS tagging is the handling of out-of-vocabulary words. Noun-phrase (NP) chunking is the task of finding the non-recursive (base) noun-phrases of se</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Random forests in language modeling.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP.</booktitle>
<marker>Xu, Jelinek, 2004</marker>
<rawString>Peng Xu and Frederick Jelinek. 2004. Random forests in language modeling. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Xun</author>
<author>C Huang</author>
<author>M Zhou</author>
</authors>
<title>A unified statistical model for the identification of English BaseNP. In</title>
<date>2000</date>
<booktitle>Proc. ACL.</booktitle>
<marker>Xun, Huang, Zhou, 2000</marker>
<rawString>E. Xun, C. Huang, and M. Zhou. 2000. A unified statistical model for the identification of English BaseNP. In Proc. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>