<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010968">
<title confidence="0.994944">
Finding Terms in Corpora for Many Languages with the Sketch Engine
</title>
<author confidence="0.971956">
Adam Kilgarriff
</author>
<affiliation confidence="0.870444">
Lexical Computing Ltd., United Kingdom
</affiliation>
<email confidence="0.924233">
adam.kilgarriff@sketchengine.co.uk
</email>
<author confidence="0.902338">
Miloˇs Jakub´ıˇcek and Vojtˇech Kov´aˇr and Pavel Rychl´y and Vit Suchomel
</author>
<affiliation confidence="0.9661755">
Masaryk University, Czech Republic
Lexical Computing Ltd., United Kingdom
</affiliation>
<email confidence="0.968078">
{xjakub, xkovar3, pary, xsuchom2}@fi.muni.cz
</email>
<sectionHeader confidence="0.997139" genericHeader="abstract">
1 Overview
</sectionHeader>
<bodyText confidence="0.9956125">
Term candidates for a domain, in a language,
can be found by
</bodyText>
<listItem confidence="0.998336545454545">
• taking a corpus for the domain, and a refer-
ence corpus for the language
• identifying the grammatical shape of a term
in the language
• tokenising, lemmatising and POS-tagging
both corpora
• identifying (and counting) the items in each
corpus which match the grammatical shape
• for each item in the domain corpus, compar-
ing its frequency with its frequency in the
refence corpus.
</listItem>
<bodyText confidence="0.999858461538462">
Then, the items with the highest frequency in the
domain corpus in comparison to the reference cor-
pus will be the top term candidates.
None of the steps above are unusual or innova-
tive for NLP (see, e. g., (Aker et al., 2013), (Go-
jun et al., 2012)). However it is far from trivial
to implement them all, for numerous languages,
in an environment that makes it easy for non-
programmers to find the terms in a domain. This
is what we have done in the Sketch Engine (Kil-
garriff et al., 2004), and will demonstrate. In this
abstract we describe how we addressed each of the
stages above.
</bodyText>
<sectionHeader confidence="0.831253" genericHeader="method">
2 The reference corpus
</sectionHeader>
<bodyText confidence="0.998260222222222">
Lexical Computing Ltd. (LCL) has been build-
ing reference corpora for over a decade. Corpora
are available for, currently, sixty languages. They
were collected by LCL from the web. For the
world’s major languages (and some others), these
are in the billions of words, gathered using Spider-
Ling (Suchomel and Pomik´alek, 2012) and form-
ing the TenTen corpus family (Jakub´ıˇcek et al.,
2013).
</bodyText>
<sectionHeader confidence="0.945893" genericHeader="method">
3 The domain corpus
</sectionHeader>
<bodyText confidence="0.999894461538462">
There are two situations: either the user already
has a corpus for the domain they are interested in,
or they do not. In the first case, there is a web in-
terface for uploading and indexing the corpus in
the Sketch Engine. In the second, we offer Web-
BootCaT (Baroni et al., 2006), a procedure for
sending queries of ‘seed terms’ to a commercial
search engine; gathering the pages that the search
engine identifies; and cleaning, deduplicating and
indexing them as a corpus (Baroni and Bernardini,
2004). (The question “how well does it work?”
is not easy to answer, but anecdotal evidence over
ten years suggests: remarkably well.)
</bodyText>
<sectionHeader confidence="0.988719" genericHeader="method">
4 Grammatical shape
</sectionHeader>
<bodyText confidence="0.999992">
We make the simplifying assumption that terms
are noun phrases (in their canonical form, without
leading articles: the term is base station, not the
base stations.) Then the task is to write a noun
phrase grammar for the language.
</bodyText>
<sectionHeader confidence="0.955695" genericHeader="method">
5 Tokenising, lemmatising, POS-tagging
</sectionHeader>
<bodyText confidence="0.999450909090909">
For each language, we need processing tools.
While many in the NLP world make the case for
language-independent tools, and claim that their
tools are usable for any, or at least many, lan-
guages, we are firm believers in the maxim “never
trust NLP tools from people who don’t speak the
language”. While we use language-independent
components in some cases (in particular TreeTag-
ger,1 RFTagger2 and FreeLing3), we collaborate
with NLP experts in the language to ascertain what
the best available tools are, sometimes to assist
</bodyText>
<footnote confidence="0.9995536">
1http://www.cis.uni-muenchen.de/
˜schmid/tools/TreeTagger/
2http://www.cis.uni-muenchen.de/
˜schmid/tools/RFTagger/
3http://nlp.lsi.upc.edu/freeling/
</footnote>
<page confidence="0.983566">
53
</page>
<note confidence="0.6132985">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 53–56,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999624">
in obtaining and customising them, and to verify
that they are producing good quality output. In
most cases these collaborators are also the people
who have written the sketch grammar and the term
grammar for the language.4
</bodyText>
<sectionHeader confidence="0.927848" genericHeader="method">
6 Identifying and counting candidates
</sectionHeader>
<bodyText confidence="0.999938857142857">
Within the Sketch Engine we already have ma-
chinery for shallow parsing, based on a ’Sketch
Grammar’ of regular expressions over part-of-
speech tags, written in CQL (Corpus Query Lan-
guage, an extended version of the formalism de-
veloped in Stuttgart in the 1990s (Schulze and
Christ, 1996)). Our implementation is mature, sta-
ble and fast, processing million-word corpora in
seconds and billion-word corpora in a few hours.
The machinery has most often been used to find
&lt;grammatical-relation, word1, word2&gt; triples for
lexicography and related research. It was straight-
forward to modify it to find, and count, the items
having the appropriate shape for a term.
</bodyText>
<sectionHeader confidence="0.917122" genericHeader="method">
7 Comparing frequencies
</sectionHeader>
<bodyText confidence="0.999989833333333">
The challenge of identifying the best candidate
terms for the domain, given their frequency in
the domain corpus and the reference corpus, is a
variant on the challenge of finding the keywords
in a corpus. As argued in (Kilgarriff, 2009), a
good method is simply to take the ratio of the nor-
malised frequency of the term in the domain cor-
pus to its normalised frequency in a reference cor-
pus. Before taking the ratio, we add a constant,
the ‘simple maths parameter’, firstly, to address
the case where the candidate is absent in the refer-
ence corpus (and we cannot divide by zero), and
secondly, because there is no one right answer:
depending on the user needs and on the nature
of the corpora, the constant can be raised to give
a list with more higher-frequency candidates, or
lowered to give more emphasis to lower-frequency
items.
Candidate terms are then presented to the user
in a sorted list, with the best candidates – those
with the highest domain:reference ratio – at the
top. Each item in the list is clickable: the user can
click to see a concordance for the term, in either
the domain or the reference corpus.
</bodyText>
<footnote confidence="0.9849435">
4Collaborators are typically credited on the ‘info’ page
for a reference corpus on the Sketch Engine website. The
collaborations are also often agreeable and fruitful in research
terms, resulting in many joint publications.
</footnote>
<figureCaption confidence="0.981344">
Figure 2: Term finding results for Japanese, WIPO format.
</figureCaption>
<sectionHeader confidence="0.514506" genericHeader="method">
8 Current status
</sectionHeader>
<bodyText confidence="0.452098">
Languages currently covered by the terminolo-
gy finding system are sumarized in Table 1.
</bodyText>
<table confidence="0.999699272727273">
Language POS tagger Ref. corpus
Chinese simp. Stanford NLP zhTenTen11
Chinese trad. Stanford NLP zhTenTen11
English TreeTagger enTenTen08
French TreeTagger frTenTen12
German RFTagger deTenTen10
Japanese MeCab+Comainu jpTenTen11
Korean HanNanum koTenTen12
Portuguese Freeling ptTenTen11
Russian RFTagger ruTenTen11
Spanish Freeling esTenTen11
</table>
<tableCaption confidence="0.70388425">
Table 1: Terminology support for languages in Sketch En-
gine in January 2014. POS tagger is mentioned as an im-
portant part of the corpus processing chain. The last column
shows the corresponding default reference corpus.
</tableCaption>
<bodyText confidence="0.999966461538462">
The display of term finding results is shown
in Figure 1 for English, for a bootcatted climate-
change corpus. Figure 2 shows a result set for
Japanese in the mobile telecommunications do-
main, prepared for the first users of the sys-
temm, the World Intellectual Property Organisa-
tion (WIPO), using their patents data, with their
preferred display format.
The user can modify various extraction related
options: Keyword reference corpus, term refer-
ence corpus, simple maths parameter, word length
and other word properties, number of top results
to display. The form is shown in Figure 3.
</bodyText>
<sectionHeader confidence="0.983678" genericHeader="method">
9 Current challenges
</sectionHeader>
<subsectionHeader confidence="0.990118">
9.1 Canonical form: lemmas and word forms
</subsectionHeader>
<bodyText confidence="0.9991845">
In English one (almost) always wants to present
each word in the term candidate in its canonical,
</bodyText>
<page confidence="0.998322">
54
</page>
<figureCaption confidence="0.998247">
Figure 1: Term finding result in the Sketch Engine – keywords on the left, multiword terms on the right. The values in paren-
theses represent keyness score and frequency in the focus corpus. The green coloured candidates were used in a WebBootCaT
run to build the corpus. The tickboxes are for specifying seed terms for iterating the corpus-building process.
</figureCaption>
<bodyText confidence="0.9778275">
dictionary form. But in French one does not. The
top term candidate in one of our first experiments,
using a French volcanoes corpus, was nu´ee ar-
dente. The problem here is that ardente is the
feminine form of the adjective, as required by the
fact that nu´ee is a feminine noun. Simply tak-
ing the canonical form of each word (masculine
singular, for adjectives) would flout the rule of
adjective-noun gender agreement. A gender re-
specting lemma turns out necessary in such cases.
Noun lemmas beginning with a capital letter
and gender respecting ending of adjectives had to
be dealt with to correctly extract German phrases.
In most of the languages we have been work-
ing on, there are also some terms which should be
given in the plural: an English example is current
affairs. This is a familiar lexicographic puzzle: for
some words, there are distinct meanings limited to
some part or parts of the paradigm, and this needs
noting. We are currently exploring options for this.
</bodyText>
<figureCaption confidence="0.89561325">
9.2 Versions of processing chains
If the version of the tools used for the reference
corpus is not identical to the version used on the
Figure 3: Term finding settings form
</figureCaption>
<page confidence="0.994029">
55
</page>
<bodyText confidence="0.999956">
domain corpus, it is likely that the candidate list
will be dominated by cases where the two versions
treated the expression differently. Thus the two
analyses of the expression will not match and (in
simple cases), one of the analyses will have fre-
quency zero in each corpus, giving one very high
and one very low ratio. This makes the tool unus-
able if processing chains are not the same.
The reference corpus is processed in batch
mode, and we hope not to upgrade it more than
once a year. The domain corpus is processed
at runtime. Until the development of the term-
finding function, it did not greatly matter if dif-
ferent versions were used. For term-finding, we
have had to look carefully at the tools, separating
each out into an independent module, so that we
can be sure of applying the same versions through-
out. It has been a large task. (It also means that
solutions based on POS-tagging by web services,
where we do not control the web service, are not
viable, since then, an unexpected upgrade to the
web service will break our system.)
</bodyText>
<sectionHeader confidence="0.968784" genericHeader="evaluation">
10 Evaluation
</sectionHeader>
<bodyText confidence="0.99992352">
We have undertaken a first evaluation using the
GENIA corpus (Kim et al., 2003), in which all
terms have been manually identified.5
First, a plain-text version of GENIA was ex-
tracted and loaded into the system. Keyword and
term extraction was performed to obtain the top
2000 keywords and top 1000 multi-word terms.
Terms manually annotated in GENIA as well as
terms extracted by our tool were normalized be-
fore comparison (lower case, spaces and hyphens
removed) and then GENIA terms were looked up
in the extraction results. 61 of the top 100 GE-
NIA terms were found by the system. The terms
not found were not English words: most were
acronyms, e.g. EGR1, STAT-6.
Concerning the domain corpus size: Although
the extraction method works well even with very
small corpora (e.g. the sample environmental cor-
pus in 1 consists of 100,000 words), larger cor-
pora should be employed to cover more terms. An
early version of this extraction tool was used to
help lexicographers compile environment protec-
tion related terminology. A 50 million words cor-
pus was sufficient in that case. (Avinesh et al.,
2012) report 30 million words is enough.
</bodyText>
<footnote confidence="0.7745115">
5GENIA has also been used for evaluating term-finding
systems by (Zhang et al., 2008).
</footnote>
<sectionHeader confidence="0.941314" genericHeader="conclusions">
11 Conclusion
</sectionHeader>
<bodyText confidence="0.9999256">
We have built a system for finding terms in a
domain corpus. It is currently set up for nine lan-
guages. In 2014 we shall extend the coverage of
languages and improve the system according to
further feedback from users.
</bodyText>
<sectionHeader confidence="0.952117" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.995522333333333">
This work has been partly supported by the
Ministry of Education of CR within the LINDAT-
Clarin project LM2010013.
</bodyText>
<sectionHeader confidence="0.998434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999866846153846">
[Aker et al.2013] A. Aker, M. Paramita, and
R. Gaizauskas. 2013. Extracting bilingual ter-
minologies from comparable corpora. In Proc.
ACL, pages 402–411.
[Avinesh et al.2012] PVS Avinesh, D. McCarthy,
D. Glennon, and J. Pomik´alek. 2012. Domain
specific corpora from the web. In Proc. EURALEX.
[Baroni and Bernardini2004] M. Baroni and S. Bernar-
dini. 2004. Bootcat: Bootstrapping corpora and
terms from the web. In Proc. LREC.
[Baroni et al.2006] M. Baroni, A. Kilgarriff,
J. Pomik´alek, and P. Rychl´y. 2006. Webboot-
cat: instant domain-specific corpora to support
human translators. In Proc. EAMT, pages 247–252.
[Gojun et al.2012] A. Gojun, U. Heid, B. Weissbach,
C. Loth, and I. Mingers. 2012. Adapting and evalu-
ating a generic term extraction tool. In Proc. LREC,
pages 651–656.
[Jakub´ıˇcek et al.2013] M. Jakub´ıˇcek, A. Kilgarriff,
V. Kov´aˇr, P. Rychl´y, and V. Suchomel. 2013. The
tenten corpus family. In Proc. Corpus Linguistics.
[Kilgarriff et al.2004] A. Kilgarriff, P. Rychl´y, P. Smrˇz,
and D. Tugwell. 2004. The sketch engine. Proc.
EURALEX, pages 105–116.
[Kilgarriff2009] A. Kilgarriff. 2009. Simple maths for
keywords. In Proc. Corpus Linguistics.
[Kim et al.2003] J-D. Kim, T. Ohta, Y. Tateisi, and
J. Tsujii. 2003. Genia corpusa semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180–i182.
[Schulze and Christ1996] B. M. Schulze and O. Christ.
1996. The CQP user’s manual. Univ. Stuttgart.
[Suchomel and Pomik´alek2012] V. Suchomel and
J. Pomik´alek. 2012. Efficient web crawling for
large text corpora. In Proc. WAC7, pages 39–43.
[Zhang et al.2008] Z. Zhang, J. Iria, C. A. Brewster, and
F. Ciravegna. 2008. A comparative evaluation of
term recognition algorithms. In Proc. LREC, pages
2108–2113.
</reference>
<page confidence="0.998422">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.009431">
<title confidence="0.999863">Finding Terms in Corpora for Many Languages with the Sketch Engine</title>
<author confidence="0.9988">Adam Kilgarriff</author>
<affiliation confidence="0.788729">Lexical Computing Ltd., United</affiliation>
<email confidence="0.956208">adam.kilgarriff@sketchengine.co.uk</email>
<author confidence="0.881784">Jakub´ıˇcek Kov´aˇr Rychl´y</author>
<affiliation confidence="0.952818">Masaryk University, Czech Lexical Computing Ltd., United</affiliation>
<abstract confidence="0.987973890625">xkovar3, pary, 1 Overview Term candidates for a domain, in a language, can be found by • taking a corpus for the domain, and a reference corpus for the language • identifying the grammatical shape of a term in the language • tokenising, lemmatising and POS-tagging both corpora • identifying (and counting) the items in each corpus which match the grammatical shape • for each item in the domain corpus, comparing its frequency with its frequency in the refence corpus. Then, the items with the highest frequency in the domain corpus in comparison to the reference corpus will be the top term candidates. None of the steps above are unusual or innovative for NLP (see, e. g., (Aker et al., 2013), (Gojun et al., 2012)). However it is far from trivial to implement them all, for numerous languages, in an environment that makes it easy for nonprogrammers to find the terms in a domain. This is what we have done in the Sketch Engine (Kilgarriff et al., 2004), and will demonstrate. In this abstract we describe how we addressed each of the stages above. 2 The reference corpus Lexical Computing Ltd. (LCL) has been building reference corpora for over a decade. Corpora are available for, currently, sixty languages. They were collected by LCL from the web. For the world’s major languages (and some others), these are in the billions of words, gathered using Spider- Ling (Suchomel and Pomik´alek, 2012) and forming the TenTen corpus family (Jakub´ıˇcek et al., 2013). 3 The domain corpus There are two situations: either the user already has a corpus for the domain they are interested in, or they do not. In the first case, there is a web interface for uploading and indexing the corpus in the Sketch Engine. In the second, we offer Web- BootCaT (Baroni et al., 2006), a procedure for sending queries of ‘seed terms’ to a commercial search engine; gathering the pages that the search engine identifies; and cleaning, deduplicating and indexing them as a corpus (Baroni and Bernardini, 2004). (The question “how well does it work?” is not easy to answer, but anecdotal evidence over ten years suggests: remarkably well.) 4 Grammatical shape We make the simplifying assumption that terms are noun phrases (in their canonical form, without articles: the term is not Then the task is to write a noun phrase grammar for the language. 5 Tokenising, lemmatising, POS-tagging For each language, we need processing tools. While many in the NLP world make the case for language-independent tools, and claim that their tools are usable for any, or at least many, languages, we are firm believers in the maxim “never trust NLP tools from people who don’t speak the language”. While we use language-independent components in some cases (in particular TreeTagand we collaborate with NLP experts in the language to ascertain what the best available tools are, sometimes to assist ˜schmid/tools/TreeTagger/ ˜schmid/tools/RFTagger/ 53 of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational pages 53–56, Sweden, April 26-30 2014. Association for Computational Linguistics in obtaining and customising them, and to verify that they are producing good quality output. In most cases these collaborators are also the people who have written the sketch grammar and the term for the 6 Identifying and counting candidates Within the Sketch Engine we already have machinery for shallow parsing, based on a ’Sketch Grammar’ of regular expressions over part-ofspeech tags, written in CQL (Corpus Query Language, an extended version of the formalism developed in Stuttgart in the 1990s (Schulze and Christ, 1996)). Our implementation is mature, stable and fast, processing million-word corpora in seconds and billion-word corpora in a few hours. The machinery has most often been used to find word1, for lexicography and related research. It was straightforward to modify it to find, and count, the items having the appropriate shape for a term. 7 Comparing frequencies The challenge of identifying the best candidate terms for the domain, given their frequency in the domain corpus and the reference corpus, is a variant on the challenge of finding the keywords in a corpus. As argued in (Kilgarriff, 2009), a good method is simply to take the ratio of the normalised frequency of the term in the domain corpus to its normalised frequency in a reference corpus. Before taking the ratio, we add a constant, the ‘simple maths parameter’, firstly, to address the case where the candidate is absent in the reference corpus (and we cannot divide by zero), and secondly, because there is no one right answer: depending on the user needs and on the nature of the corpora, the constant can be raised to give a list with more higher-frequency candidates, or lowered to give more emphasis to lower-frequency items. Candidate terms are then presented to the user in a sorted list, with the best candidates – those with the highest domain:reference ratio – at the top. Each item in the list is clickable: the user can click to see a concordance for the term, in either the domain or the reference corpus. are typically credited on the ‘info’ page for a reference corpus on the Sketch Engine website. The collaborations are also often agreeable and fruitful in research terms, resulting in many joint publications. Figure 2: Term finding results for Japanese, WIPO format. 8 Current status Languages currently covered by the terminology finding system are sumarized in Table 1.</abstract>
<note confidence="0.8900135">Language POS tagger Ref. corpus Chinese simp. Stanford NLP zhTenTen11 Chinese trad. Stanford NLP zhTenTen11 English TreeTagger enTenTen08 French TreeTagger frTenTen12 German RFTagger deTenTen10 Japanese MeCab+Comainu jpTenTen11 Korean HanNanum koTenTen12 Portuguese Freeling ptTenTen11 Russian RFTagger ruTenTen11 Spanish Freeling esTenTen11 Table 1: Terminology support for languages in Sketch En-</note>
<abstract confidence="0.991476693181818">gine in January 2014. POS tagger is mentioned as an important part of the corpus processing chain. The last column shows the corresponding default reference corpus. The display of term finding results is shown in Figure 1 for English, for a bootcatted climatechange corpus. Figure 2 shows a result set for Japanese in the mobile telecommunications domain, prepared for the first users of the systemm, the World Intellectual Property Organisation (WIPO), using their patents data, with their preferred display format. The user can modify various extraction related options: Keyword reference corpus, term reference corpus, simple maths parameter, word length and other word properties, number of top results to display. The form is shown in Figure 3. 9 Current challenges 9.1 Canonical form: lemmas and word forms In English one (almost) always wants to present each word in the term candidate in its canonical, 54 Figure 1: Term finding result in the Sketch Engine – keywords on the left, multiword terms on the right. The values in parentheses represent keyness score and frequency in the focus corpus. The green coloured candidates were used in a WebBootCaT run to build the corpus. The tickboxes are for specifying seed terms for iterating the corpus-building process. dictionary form. But in French one does not. The top term candidate in one of our first experiments, a French volcanoes corpus, was ar-The problem here is that the feminine form of the adjective, as required by the that a feminine noun. Simply tak-ing the canonical form of each word (masculine singular, for adjectives) would flout the rule of adjective-noun gender agreement. A gender re-specting lemma turns out necessary in such cases. Noun lemmas beginning with a capital letter and gender respecting ending of adjectives had to be dealt with to correctly extract German phrases. In most of the languages we have been work-ing on, there are also some terms which should be in the plural: an English example is This is a familiar lexicographic puzzle: for some words, there are distinct meanings limited to some part or parts of the paradigm, and this needs noting. We are currently exploring options for this. 9.2 Versions of processing chains If the version of the tools used for the reference corpus is not identical to the version used on the Figure 3: Term finding settings form 55 domain corpus, it is likely that the candidate list will be dominated by cases where the two versions treated the expression differently. Thus the two analyses of the expression will not match and (in simple cases), one of the analyses will have frequency zero in each corpus, giving one very high and one very low ratio. This makes the tool unusable if processing chains are not the same. The reference corpus is processed in batch mode, and we hope not to upgrade it more than once a year. The domain corpus is processed at runtime. Until the development of the termfinding function, it did not greatly matter if different versions were used. For term-finding, we have had to look carefully at the tools, separating each out into an independent module, so that we can be sure of applying the same versions throughout. It has been a large task. (It also means that solutions based on POS-tagging by web services, where we do not control the web service, are not viable, since then, an unexpected upgrade to the web service will break our system.) 10 Evaluation We have undertaken a first evaluation using the GENIA corpus (Kim et al., 2003), in which all have been manually First, a plain-text version of GENIA was extracted and loaded into the system. Keyword and term extraction was performed to obtain the top 2000 keywords and top 1000 multi-word terms. Terms manually annotated in GENIA as well as terms extracted by our tool were normalized before comparison (lower case, spaces and hyphens removed) and then GENIA terms were looked up in the extraction results. 61 of the top 100 GE- NIA terms were found by the system. The terms not found were not English words: most were acronyms, e.g. EGR1, STAT-6. Concerning the domain corpus size: Although the extraction method works well even with very small corpora (e.g. the sample environmental corpus in 1 consists of 100,000 words), larger corpora should be employed to cover more terms. An early version of this extraction tool was used to help lexicographers compile environment protection related terminology. A 50 million words corpus was sufficient in that case. (Avinesh et al., 2012) report 30 million words is enough. has also been used for evaluating term-finding systems by (Zhang et al., 2008). 11 Conclusion We have built a system for finding terms in a domain corpus. It is currently set up for nine languages. In 2014 we shall extend the coverage of languages and improve the system according to further feedback from users. Acknowledgement</abstract>
<note confidence="0.9930404">This work has been partly supported by the Ministry of Education of CR within the LINDAT- Clarin project LM2010013. References [Aker et al.2013] A. Aker, M. Paramita, and R. Gaizauskas. 2013. Extracting bilingual terfrom comparable corpora. In pages 402–411. [Avinesh et al.2012] PVS Avinesh, D. McCarthy, D. Glennon, and J. Pomik´alek. 2012. Domain corpora from the web. In [Baroni and Bernardini2004] M. Baroni and S. Bernardini. 2004. Bootcat: Bootstrapping corpora and from the web. In [Baroni et al.2006] M. Baroni, A. Kilgarriff, J. Pomik´alek, and P. Rychl´y. 2006. Webbootcat: instant domain-specific corpora to support translators. In pages 247–252. [Gojun et al.2012] A. Gojun, U. Heid, B. Weissbach, C. Loth, and I. Mingers. 2012. Adapting and evalua generic term extraction tool. In pages 651–656. [Jakub´ıˇcek et al.2013] M. Jakub´ıˇcek, A. Kilgarriff, V. Kov´aˇr, P. Rychl´y, and V. Suchomel. 2013. The corpus family. In Corpus [Kilgarriff et al.2004] A. Kilgarriff, P. Rychl´y, P. Smrˇz, D. Tugwell. 2004. The sketch engine. pages 105–116. [Kilgarriff2009] A. Kilgarriff. 2009. Simple maths for In Corpus [Kim et al.2003] J-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Genia corpusa semantically ancorpus for bio-textmining. 19(suppl 1):i180–i182. [Schulze and Christ1996] B. M. Schulze and O. Christ. The CQP user’s manual. [Suchomel and Pomik´alek2012] V. Suchomel and J. Pomik´alek. 2012. Efficient web crawling for text corpora. In pages 39–43. [Zhang et al.2008] Z. Zhang, J. Iria, C. A. Brewster, and</note>
<abstract confidence="0.537193">F. Ciravegna. 2008. A comparative evaluation of recognition algorithms. In pages 2108–2113.</abstract>
<intro confidence="0.69214">56</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aker</author>
<author>M Paramita</author>
<author>R Gaizauskas</author>
</authors>
<title>Extracting bilingual terminologies from comparable corpora.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>402--411</pages>
<marker>[Aker et al.2013]</marker>
<rawString>A. Aker, M. Paramita, and R. Gaizauskas. 2013. Extracting bilingual terminologies from comparable corpora. In Proc. ACL, pages 402–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PVS Avinesh</author>
<author>D McCarthy</author>
<author>D Glennon</author>
<author>J Pomik´alek</author>
</authors>
<title>Domain specific corpora from the web. In</title>
<date>2012</date>
<booktitle>Proc. EURALEX.</booktitle>
<marker>[Avinesh et al.2012]</marker>
<rawString>PVS Avinesh, D. McCarthy, D. Glennon, and J. Pomik´alek. 2012. Domain specific corpora from the web. In Proc. EURALEX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>S Bernardini</author>
</authors>
<title>Bootcat: Bootstrapping corpora and terms from the web. In</title>
<date>2004</date>
<booktitle>Proc. LREC.</booktitle>
<marker>[Baroni and Bernardini2004]</marker>
<rawString>M. Baroni and S. Bernardini. 2004. Bootcat: Bootstrapping corpora and terms from the web. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Kilgarriff</author>
<author>J Pomik´alek</author>
<author>P Rychl´y</author>
</authors>
<title>Webbootcat: instant domain-specific corpora to support human translators.</title>
<date>2006</date>
<booktitle>In Proc. EAMT,</booktitle>
<pages>247--252</pages>
<marker>[Baroni et al.2006]</marker>
<rawString>M. Baroni, A. Kilgarriff, J. Pomik´alek, and P. Rychl´y. 2006. Webbootcat: instant domain-specific corpora to support human translators. In Proc. EAMT, pages 247–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gojun</author>
<author>U Heid</author>
<author>B Weissbach</author>
<author>C Loth</author>
<author>I Mingers</author>
</authors>
<title>Adapting and evaluating a generic term extraction tool.</title>
<date>2012</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>651--656</pages>
<marker>[Gojun et al.2012]</marker>
<rawString>A. Gojun, U. Heid, B. Weissbach, C. Loth, and I. Mingers. 2012. Adapting and evaluating a generic term extraction tool. In Proc. LREC, pages 651–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jakub´ıˇcek</author>
<author>A Kilgarriff</author>
<author>V Kov´aˇr</author>
<author>P Rychl´y</author>
<author>V Suchomel</author>
</authors>
<title>The tenten corpus family.</title>
<date>2013</date>
<booktitle>In Proc. Corpus Linguistics.</booktitle>
<marker>[Jakub´ıˇcek et al.2013]</marker>
<rawString>M. Jakub´ıˇcek, A. Kilgarriff, V. Kov´aˇr, P. Rychl´y, and V. Suchomel. 2013. The tenten corpus family. In Proc. Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>P Rychl´y</author>
<author>P Smrˇz</author>
<author>D Tugwell</author>
</authors>
<title>The sketch engine.</title>
<date>2004</date>
<booktitle>Proc. EURALEX,</booktitle>
<pages>105--116</pages>
<marker>[Kilgarriff et al.2004]</marker>
<rawString>A. Kilgarriff, P. Rychl´y, P. Smrˇz, and D. Tugwell. 2004. The sketch engine. Proc. EURALEX, pages 105–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Simple maths for keywords.</title>
<date>2009</date>
<booktitle>In Proc. Corpus Linguistics.</booktitle>
<marker>[Kilgarriff2009]</marker>
<rawString>A. Kilgarriff. 2009. Simple maths for keywords. In Proc. Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>1--180</pages>
<marker>[Kim et al.2003]</marker>
<rawString>J-D. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Schulze</author>
<author>O Christ</author>
</authors>
<date>1996</date>
<booktitle>The CQP user’s manual. Univ.</booktitle>
<location>Stuttgart.</location>
<marker>[Schulze and Christ1996]</marker>
<rawString>B. M. Schulze and O. Christ. 1996. The CQP user’s manual. Univ. Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Suchomel</author>
<author>J Pomik´alek</author>
</authors>
<title>Efficient web crawling for large text corpora.</title>
<date>2012</date>
<booktitle>In Proc. WAC7,</booktitle>
<pages>39--43</pages>
<marker>[Suchomel and Pomik´alek2012]</marker>
<rawString>V. Suchomel and J. Pomik´alek. 2012. Efficient web crawling for large text corpora. In Proc. WAC7, pages 39–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhang</author>
<author>J Iria</author>
<author>C A Brewster</author>
<author>F Ciravegna</author>
</authors>
<title>A comparative evaluation of term recognition algorithms.</title>
<date>2008</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>2108--2113</pages>
<marker>[Zhang et al.2008]</marker>
<rawString>Z. Zhang, J. Iria, C. A. Brewster, and F. Ciravegna. 2008. A comparative evaluation of term recognition algorithms. In Proc. LREC, pages 2108–2113.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>